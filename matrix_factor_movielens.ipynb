{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremy/anaconda3/envs/gnn/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = 'movielens-1m'\n",
    "users, items, train_ratings, test_ratings, items_features_tensor, user_features_tensor = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = users.nunique()\n",
    "num_items = items.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.graph_splitters import python_stratified_split\n",
    "\n",
    "def mask_datasets(device, train_ratings, test_ratings, val_ratio=0.1):\n",
    "    col_user = 'userid'\n",
    "    col_item = 'itemid'\n",
    "    col_timestamp = 'timestamp'\n",
    "    train_ratings, val_ratings = python_stratified_split(train_ratings, ratio=1-val_ratio, col_user=col_user, col_item=col_item, col_timestamp=col_timestamp)\n",
    "    \n",
    "    num_users = train_ratings[col_user].max() + 1\n",
    "    num_items = train_ratings[col_item].max() + 1\n",
    "    \n",
    "    # Create rating and mask matrices for train, val and test\n",
    "    rating_matrix_train = np.zeros(shape=(num_users, num_items))\n",
    "    mask_matrix_train = np.zeros(shape=(num_users, num_items))\n",
    "    rating_matrix_val = np.zeros(shape=(num_users, num_items))\n",
    "    mask_matrix_val = np.zeros(shape=(num_users, num_items))\n",
    "    rating_matrix_test = np.zeros(shape=(num_users, num_items))\n",
    "    mask_matrix_test = np.zeros(shape=(num_users, num_items))\n",
    "    \n",
    "    for _, r in train_ratings.iterrows():\n",
    "        rating_matrix_train[int(int(r[0])), int(int(r[1]))] = int(r[2])\n",
    "        mask_matrix_train[int(r[0]), int(r[1])] = 1\n",
    "    \n",
    "    for _, r in val_ratings.iterrows():\n",
    "        rating_matrix_val[int(r[0]), int(r[1])] = int(r[2])\n",
    "        mask_matrix_val[int(r[0]), int(r[1])] = 1\n",
    "    \n",
    "    for _, r in test_ratings.iterrows():\n",
    "        rating_matrix_test[int(r[0]), int(r[1])] = int(r[2])\n",
    "        mask_matrix_test[int(r[0]), int(r[1])] = 1\n",
    "    \n",
    "    rating_matrix_train = torch.tensor(rating_matrix_train).to(device)\n",
    "    mask_matrix_train = torch.tensor(mask_matrix_train).to(device)\n",
    "    rating_matrix_val = torch.tensor(rating_matrix_val).to(device)\n",
    "    mask_matrix_val = torch.tensor(mask_matrix_val).to(device)\n",
    "    rating_matrix_test = torch.tensor(rating_matrix_test).to(device)\n",
    "    mask_matrix_test = torch.tensor(mask_matrix_test).to(device)\n",
    "    \n",
    "    return rating_matrix_train, mask_matrix_train, rating_matrix_val, mask_matrix_val, rating_matrix_test, mask_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_368418/2521653375.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  rating_matrix_train[int(int(r[0])), int(int(r[1]))] = int(r[2])\n",
      "/tmp/ipykernel_368418/2521653375.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mask_matrix_train[int(r[0]), int(r[1])] = 1\n",
      "/tmp/ipykernel_368418/2521653375.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  rating_matrix_val[int(r[0]), int(r[1])] = int(r[2])\n",
      "/tmp/ipykernel_368418/2521653375.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mask_matrix_val[int(r[0]), int(r[1])] = 1\n",
      "/tmp/ipykernel_368418/2521653375.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  rating_matrix_test[int(r[0]), int(r[1])] = int(r[2])\n",
      "/tmp/ipykernel_368418/2521653375.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mask_matrix_test[int(r[0]), int(r[1])] = 1\n"
     ]
    }
   ],
   "source": [
    "rating_matrix_train, mask_matrix_train, \\\n",
    "rating_matrix_val, mask_matrix_val, \\\n",
    "rating_matrix_test, mask_matrix_test = mask_datasets(device, train_ratings, test_ratings, val_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim, rating_matrix):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        # User and item latent factors\n",
    "        self.user_factors = nn.Parameter(torch.randn(num_users, latent_dim, device=device) * 0.01)\n",
    "        self.item_factors = nn.Parameter(torch.randn(num_items, latent_dim, device=device) * 0.01)\n",
    "        \n",
    "        # User and item biases\n",
    "        self.user_bias = nn.Parameter(torch.zeros(num_users, 1, device=device))\n",
    "        self.item_bias = nn.Parameter(torch.zeros(1, num_items, device=device))\n",
    "        \n",
    "        # Global average rating\n",
    "        self.global_bias = nn.Parameter(torch.tensor([rating_matrix[rating_matrix != 0].mean()], device=device))\n",
    "    \n",
    "    def forward(self):\n",
    "        # Compute the predicted rating matrix\n",
    "        interaction = torch.matmul(self.user_factors, self.item_factors.t())\n",
    "        pred_ratings = interaction + self.user_bias + self.item_bias + self.global_bias\n",
    "        return pred_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(pred_ratings, true_ratings, mask, model, alpha, beta):\n",
    "    # Compute the squared error only on observed entries\n",
    "    diff = mask * (true_ratings - pred_ratings)\n",
    "    mse_loss = (diff ** 2).sum()\n",
    "    \n",
    "    # Regularization terms\n",
    "    reg_loss = alpha * torch.norm(model.user_factors, p=2) ** 2 + \\\n",
    "               beta * torch.norm(model.item_factors, p=2) ** 2\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = mse_loss + reg_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Train Loss: 861276.9565, Train RMSE: 1.1091, Val RMSE: 1.1160, Test RMSE: 1.1110\n",
      "Epoch [20/500], Train Loss: 845622.4658, Train RMSE: 1.0990, Val RMSE: 1.1071, Test RMSE: 1.1021\n",
      "Epoch [30/500], Train Loss: 823167.8880, Train RMSE: 1.0843, Val RMSE: 1.0939, Test RMSE: 1.0889\n",
      "Epoch [40/500], Train Loss: 791775.8227, Train RMSE: 1.0634, Val RMSE: 1.0746, Test RMSE: 1.0697\n",
      "Epoch [50/500], Train Loss: 752025.1241, Train RMSE: 1.0364, Val RMSE: 1.0493, Test RMSE: 1.0448\n",
      "Epoch [60/500], Train Loss: 707583.2912, Train RMSE: 1.0053, Val RMSE: 1.0204, Test RMSE: 1.0163\n",
      "Epoch [70/500], Train Loss: 663756.4469, Train RMSE: 0.9736, Val RMSE: 0.9914, Test RMSE: 0.9878\n",
      "Epoch [80/500], Train Loss: 624801.4590, Train RMSE: 0.9446, Val RMSE: 0.9657, Test RMSE: 0.9626\n",
      "Epoch [90/500], Train Loss: 592326.9705, Train RMSE: 0.9197, Val RMSE: 0.9447, Test RMSE: 0.9421\n",
      "Epoch [100/500], Train Loss: 565877.0459, Train RMSE: 0.8990, Val RMSE: 0.9286, Test RMSE: 0.9262\n",
      "Epoch [110/500], Train Loss: 544215.4465, Train RMSE: 0.8816, Val RMSE: 0.9164, Test RMSE: 0.9142\n",
      "Epoch [120/500], Train Loss: 526026.1320, Train RMSE: 0.8667, Val RMSE: 0.9073, Test RMSE: 0.9053\n",
      "Epoch [130/500], Train Loss: 510147.1105, Train RMSE: 0.8535, Val RMSE: 0.9004, Test RMSE: 0.8984\n",
      "Epoch [140/500], Train Loss: 495672.1007, Train RMSE: 0.8413, Val RMSE: 0.8951, Test RMSE: 0.8932\n",
      "Epoch [150/500], Train Loss: 481980.1889, Train RMSE: 0.8296, Val RMSE: 0.8909, Test RMSE: 0.8890\n",
      "Epoch [160/500], Train Loss: 468693.9482, Train RMSE: 0.8181, Val RMSE: 0.8876, Test RMSE: 0.8856\n",
      "Epoch [170/500], Train Loss: 455610.7124, Train RMSE: 0.8066, Val RMSE: 0.8849, Test RMSE: 0.8829\n",
      "Epoch [180/500], Train Loss: 442640.4654, Train RMSE: 0.7950, Val RMSE: 0.8828, Test RMSE: 0.8807\n",
      "Epoch [190/500], Train Loss: 429761.3878, Train RMSE: 0.7833, Val RMSE: 0.8812, Test RMSE: 0.8790\n",
      "Epoch [200/500], Train Loss: 416991.3818, Train RMSE: 0.7716, Val RMSE: 0.8801, Test RMSE: 0.8778\n",
      "Epoch [210/500], Train Loss: 404369.8050, Train RMSE: 0.7598, Val RMSE: 0.8795, Test RMSE: 0.8771\n",
      "Epoch [220/500], Train Loss: 391945.2692, Train RMSE: 0.7480, Val RMSE: 0.8793, Test RMSE: 0.8768\n",
      "Converged at epoch 229\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 64      # Number of latent factors\n",
    "alpha = 0.01         # Regularization parameter for user factors\n",
    "beta = 0.01          # Regularization parameter for item factors\n",
    "num_epochs = 500     # Number of training epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "num_users, num_items = rating_matrix_train.shape\n",
    "model = MatrixFactorization(num_users, num_items, latent_dim, rating_matrix_train).to(device)\n",
    "early_stop = 0\n",
    "best_val_rmse = float('inf')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    pred_ratings = model()\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(pred_ratings, rating_matrix_train, mask_matrix_train, model, alpha, beta)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute RMSE on observed ratings\n",
    "    with torch.no_grad():\n",
    "        train_mse = ((mask_matrix_train * (rating_matrix_train - pred_ratings)) ** 2).sum() / mask_matrix_train.sum()\n",
    "        train_rmse = torch.sqrt(train_mse)\n",
    "\n",
    "        val_mse = ((mask_matrix_val * (rating_matrix_val - pred_ratings)) ** 2).sum() / mask_matrix_val.sum()\n",
    "        val_rmse = torch.sqrt(val_mse)\n",
    "\n",
    "        test_mse = ((mask_matrix_test * (rating_matrix_test - pred_ratings)) ** 2).sum() / mask_matrix_test.sum()\n",
    "        test_rmse = torch.sqrt(test_mse)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "        if early_stop == 10:\n",
    "            print(f'Converged at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}, Train RMSE: {train_rmse.item():.4f}, Val RMSE: {val_rmse.item():.4f}, Test RMSE: {test_rmse.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
