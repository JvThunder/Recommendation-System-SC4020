{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvthunder/anaconda/envs/rec-sys/lib/python3.9/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/jvthunder/anaconda/envs/rec-sys/lib/python3.9/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZNSt15__exception_ptr13exception_ptr9_M_addrefEv\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/jvthunder/anaconda/envs/rec-sys/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from models.light_gcn import LightGCNStack\n",
    "from utils.graph_splitters import python_stratified_split\n",
    "from utils.light_gcn_utils import bpr_loss, evaluate, build_user_item_interactions, get_positive_negative_ratings, recall_at_k, precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = 'goodbooks-10k'\n",
    "users, items, train_ratings, test_ratings, items_features_tensor, user_features_tensor = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 26792, num_items: 7796\n"
     ]
    }
   ],
   "source": [
    "num_users = users['userid'].nunique()\n",
    "num_items = items['itemid'].nunique()\n",
    "print(f\"num_users: {num_users}, num_items: {num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_230304/1560685500.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  train_edge_index = torch.tensor([train_user_ids, train_item_ids], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Create edge index for bipartite graph for train set\n",
    "train_user_ids = train_ratings['userid'].values\n",
    "train_item_ids = train_ratings['itemid'].values + num_users \n",
    "train_edge_index = torch.tensor([train_user_ids, train_item_ids], dtype=torch.long)\n",
    "\n",
    "# Create edge index for bipartite graph for test set\n",
    "test_user_ids = test_ratings['userid'].values  \n",
    "test_item_ids = test_ratings['itemid'].values + num_users  \n",
    "test_edge_index = torch.tensor([test_user_ids, test_item_ids], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_item_dict = build_user_item_interactions(train_ratings)\n",
    "test_user_item_dict = build_user_item_interactions(test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_threshold = 5\n",
    "negative_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_ratings = get_positive_negative_ratings(train_user_item_dict, positive_threshold, negative_threshold)\n",
    "test_user_ratings = get_positive_negative_ratings(test_user_item_dict, positive_threshold, negative_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 384\n",
    "num_nodes = num_users + num_items + 1000\n",
    "no_user_features = user_features_tensor.size(1)\n",
    "no_item_features = items_features_tensor.size(1)\n",
    "\n",
    "num_layers = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "user_features_tensor = user_features_tensor.to(device)\n",
    "items_features_tensor = items_features_tensor.to(device)\n",
    "train_edge_index = train_edge_index.to(device)\n",
    "test_edge_index = test_edge_index.to(device)\n",
    "\n",
    "model = LightGCNStack(num_nodes, no_user_features, no_item_features, embedding_dim, num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model(user_features_tensor, items_features_tensor, train_edge_index)\n",
    "recall = recall_at_k(train_user_ratings, embeddings, k=k, device=device)\n",
    "precision = precision_at_k(train_user_ratings, embeddings, k=k, device=device)\n",
    "\n",
    "print(\"Base recall:\", recall)\n",
    "print(\"Base precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 51756/51756 [01:57<00:00, 441.18it/s, Avg Loss=3.9841]\n"
     ]
    }
   ],
   "source": [
    "calc_metrics_every = 5\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(train_user_ratings, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    embeddings = model(user_features_tensor, items_features_tensor, train_edge_index)\n",
    "\n",
    "    for user_id, pos_items, neg_items in pbar:\n",
    "        no_sample = min(len(pos_items), len(neg_items))\n",
    "        users = torch.tensor([user_id] * no_sample, dtype=torch.long).to(device)\n",
    "        pos_samples = random.sample(pos_items, no_sample)\n",
    "        pos_samples = torch.tensor(pos_samples, dtype=torch.long).to(device)\n",
    "        neg_samples = random.sample(neg_items, no_sample)\n",
    "        neg_samples = torch.tensor(neg_samples, dtype=torch.long).to(device)\n",
    "        \n",
    "        loss = bpr_loss(embeddings, users, pos_samples, neg_samples)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        avg_loss = total_loss.item() / num_batches\n",
    "\n",
    "        pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % calc_metrics_every == 0:\n",
    "        recall = recall_at_k(train_user_ratings, embeddings, k=k, device=device)\n",
    "        precision = precision_at_k(train_user_ratings, embeddings, k=k, device=device)\n",
    "        avg_loss = total_loss / len(train_user_ratings)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Recall@{k}: {recall:.4f}, Precision@{k}: {precision:.4f}')\n",
    "    else:\n",
    "        avg_loss = total_loss / len(train_user_ratings)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/52107 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'movies_features_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(test_user_ratings)\n\u001b[0;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model(user_features_tensor, \u001b[43mmovies_features_tensor\u001b[49m, test_edge_index)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_id, pos_movies, neg_movies \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      8\u001b[0m     no_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pos_movies), \u001b[38;5;28mlen\u001b[39m(neg_movies))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movies_features_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "num_batches = 0\n",
    "pbar = tqdm(test_user_ratings)\n",
    "\n",
    "embeddings = model(user_features_tensor, items_features_tensor, test_edge_index)\n",
    "\n",
    "for user_id, pos_items, neg_items in pbar:\n",
    "    no_sample = min(len(pos_items), len(neg_items))\n",
    "    users = torch.tensor([user_id] * no_sample, dtype=torch.long).to(device)\n",
    "    pos_samples = random.sample(pos_items, no_sample)\n",
    "    pos_samples = torch.tensor(pos_samples, dtype=torch.long).to(device)\n",
    "    neg_samples = random.sample(neg_items, no_sample)\n",
    "    neg_samples = torch.tensor(neg_samples, dtype=torch.long).to(device)\n",
    "    loss = bpr_loss(embeddings, users, pos_samples, neg_samples)\n",
    "    total_loss += loss\n",
    "    num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    # Update progress bar with average loss\n",
    "    pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "recall = recall_at_k(train_user_ratings, embeddings, k=k, device=device)\n",
    "precision = precision_at_k(train_user_ratings, embeddings, k=k, device=device)\n",
    "avg_loss = total_loss / len(test_user_ratings)\n",
    "print(f'Test Loss: {avg_loss:.4f}, Test Recall@{k}: {recall:.4f}, Test Precision@{k}: {precision:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
