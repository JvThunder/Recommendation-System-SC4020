{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models_ncf.neumf import NeuMFEngine\n",
    "from models_ncf.data import SampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neumf_config = {'alias': 'neumf_first_try',\n",
    "                'num_epoch': 10,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 6040,\n",
    "                'num_items': 3706,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16, 64, 32, 16, 8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "                'l2_regularization': 0.0000001,\n",
    "                'weight_init_gaussian': True,\n",
    "                'use_cuda': True,\n",
    "                'use_bachify_eval': True,\n",
    "                'device_id': 0,\n",
    "                'pretrain': False,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_precision0.6391_recall0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_precision0.5606_recall0.2463.model'),\n",
    "                'model_dir': 'checkpoints/{}_Epoch{}_precision{:.4f}_recall{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0 - Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Print the device ID and its name for each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i} - Name: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n",
      "Index(['userId', 'itemId', 'rating', 'real_score', 'negative_samples'], dtype='object')\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6956796646118164\n",
      "[Training Epoch 0] Batch 1, Loss 0.6952062845230103\n",
      "[Training Epoch 0] Batch 2, Loss 0.6947036981582642\n",
      "[Training Epoch 0] Batch 3, Loss 0.6941486597061157\n",
      "[Training Epoch 0] Batch 4, Loss 0.6936460733413696\n",
      "[Training Epoch 0] Batch 5, Loss 0.693114161491394\n",
      "[Training Epoch 0] Batch 6, Loss 0.6926034688949585\n",
      "[Training Epoch 0] Batch 7, Loss 0.6921007037162781\n",
      "[Training Epoch 0] Batch 8, Loss 0.6915513277053833\n",
      "[Training Epoch 0] Batch 9, Loss 0.6911758184432983\n",
      "[Training Epoch 0] Batch 10, Loss 0.6903867721557617\n",
      "[Training Epoch 0] Batch 11, Loss 0.689639151096344\n",
      "[Training Epoch 0] Batch 12, Loss 0.6894201040267944\n",
      "[Training Epoch 0] Batch 13, Loss 0.6888707876205444\n",
      "[Training Epoch 0] Batch 14, Loss 0.6886982321739197\n",
      "[Training Epoch 0] Batch 15, Loss 0.6877202987670898\n",
      "[Training Epoch 0] Batch 16, Loss 0.6879569292068481\n",
      "[Training Epoch 0] Batch 17, Loss 0.6864657402038574\n",
      "[Training Epoch 0] Batch 18, Loss 0.6863647103309631\n",
      "[Training Epoch 0] Batch 19, Loss 0.6859546899795532\n",
      "[Training Epoch 0] Batch 20, Loss 0.6847607493400574\n",
      "[Training Epoch 0] Batch 21, Loss 0.6838151216506958\n",
      "[Training Epoch 0] Batch 22, Loss 0.6837594509124756\n",
      "[Training Epoch 0] Batch 23, Loss 0.6833306550979614\n",
      "[Training Epoch 0] Batch 24, Loss 0.6824445724487305\n",
      "[Training Epoch 0] Batch 25, Loss 0.6824705004692078\n",
      "[Training Epoch 0] Batch 26, Loss 0.6806904077529907\n",
      "[Training Epoch 0] Batch 27, Loss 0.6798340082168579\n",
      "[Training Epoch 0] Batch 28, Loss 0.679349422454834\n",
      "[Training Epoch 0] Batch 29, Loss 0.6778565645217896\n",
      "[Training Epoch 0] Batch 30, Loss 0.6786502599716187\n",
      "[Training Epoch 0] Batch 31, Loss 0.6766906976699829\n",
      "[Training Epoch 0] Batch 32, Loss 0.6759903430938721\n",
      "[Training Epoch 0] Batch 33, Loss 0.6749147176742554\n",
      "[Training Epoch 0] Batch 34, Loss 0.6725236177444458\n",
      "[Training Epoch 0] Batch 35, Loss 0.67364501953125\n",
      "[Training Epoch 0] Batch 36, Loss 0.6718820333480835\n",
      "[Training Epoch 0] Batch 37, Loss 0.6719743013381958\n",
      "[Training Epoch 0] Batch 38, Loss 0.6712138652801514\n",
      "[Training Epoch 0] Batch 39, Loss 0.6691368222236633\n",
      "[Training Epoch 0] Batch 40, Loss 0.6668161153793335\n",
      "[Training Epoch 0] Batch 41, Loss 0.6688162088394165\n",
      "[Training Epoch 0] Batch 42, Loss 0.6663645505905151\n",
      "[Training Epoch 0] Batch 43, Loss 0.6636176705360413\n",
      "[Training Epoch 0] Batch 44, Loss 0.6640107035636902\n",
      "[Training Epoch 0] Batch 45, Loss 0.6594351530075073\n",
      "[Training Epoch 0] Batch 46, Loss 0.6575838327407837\n",
      "[Training Epoch 0] Batch 47, Loss 0.6564788818359375\n",
      "[Training Epoch 0] Batch 48, Loss 0.653395414352417\n",
      "[Training Epoch 0] Batch 49, Loss 0.6570578813552856\n",
      "[Training Epoch 0] Batch 50, Loss 0.6477943658828735\n",
      "[Training Epoch 0] Batch 51, Loss 0.6484296917915344\n",
      "[Training Epoch 0] Batch 52, Loss 0.6465160846710205\n",
      "[Training Epoch 0] Batch 53, Loss 0.6440120935440063\n",
      "[Training Epoch 0] Batch 54, Loss 0.6402682065963745\n",
      "[Training Epoch 0] Batch 55, Loss 0.6406782865524292\n",
      "[Training Epoch 0] Batch 56, Loss 0.6341543793678284\n",
      "[Training Epoch 0] Batch 57, Loss 0.6349883079528809\n",
      "[Training Epoch 0] Batch 58, Loss 0.631150484085083\n",
      "[Training Epoch 0] Batch 59, Loss 0.6244131326675415\n",
      "[Training Epoch 0] Batch 60, Loss 0.62447589635849\n",
      "[Training Epoch 0] Batch 61, Loss 0.6169717907905579\n",
      "[Training Epoch 0] Batch 62, Loss 0.6090627908706665\n",
      "[Training Epoch 0] Batch 63, Loss 0.6096212267875671\n",
      "[Training Epoch 0] Batch 64, Loss 0.60908043384552\n",
      "[Training Epoch 0] Batch 65, Loss 0.6052507162094116\n",
      "[Training Epoch 0] Batch 66, Loss 0.592930793762207\n",
      "[Training Epoch 0] Batch 67, Loss 0.5813473463058472\n",
      "[Training Epoch 0] Batch 68, Loss 0.5868026614189148\n",
      "[Training Epoch 0] Batch 69, Loss 0.588526725769043\n",
      "[Training Epoch 0] Batch 70, Loss 0.5728176832199097\n",
      "[Training Epoch 0] Batch 71, Loss 0.5586175918579102\n",
      "[Training Epoch 0] Batch 72, Loss 0.5596601963043213\n",
      "[Training Epoch 0] Batch 73, Loss 0.5479867458343506\n",
      "[Training Epoch 0] Batch 74, Loss 0.5489041209220886\n",
      "[Training Epoch 0] Batch 75, Loss 0.545335590839386\n",
      "[Training Epoch 0] Batch 76, Loss 0.5518321990966797\n",
      "[Training Epoch 0] Batch 77, Loss 0.525205135345459\n",
      "[Training Epoch 0] Batch 78, Loss 0.5364107489585876\n",
      "[Training Epoch 0] Batch 79, Loss 0.5197497606277466\n",
      "[Training Epoch 0] Batch 80, Loss 0.5026204586029053\n",
      "[Training Epoch 0] Batch 81, Loss 0.5140990018844604\n",
      "[Training Epoch 0] Batch 82, Loss 0.5072033405303955\n",
      "[Training Epoch 0] Batch 83, Loss 0.5101466178894043\n",
      "[Training Epoch 0] Batch 84, Loss 0.522575318813324\n",
      "[Training Epoch 0] Batch 85, Loss 0.489423930644989\n",
      "[Training Epoch 0] Batch 86, Loss 0.5201703310012817\n",
      "[Training Epoch 0] Batch 87, Loss 0.48710453510284424\n",
      "[Training Epoch 0] Batch 88, Loss 0.5099778175354004\n",
      "[Training Epoch 0] Batch 89, Loss 0.4672779440879822\n",
      "[Training Epoch 0] Batch 90, Loss 0.4753068685531616\n",
      "[Training Epoch 0] Batch 91, Loss 0.5172587633132935\n",
      "[Training Epoch 0] Batch 92, Loss 0.5437754392623901\n",
      "[Training Epoch 0] Batch 93, Loss 0.5100719332695007\n",
      "[Training Epoch 0] Batch 94, Loss 0.48935467004776\n",
      "[Training Epoch 0] Batch 95, Loss 0.49514347314834595\n",
      "[Training Epoch 0] Batch 96, Loss 0.49730610847473145\n",
      "[Training Epoch 0] Batch 97, Loss 0.4390135109424591\n",
      "[Training Epoch 0] Batch 98, Loss 0.4717905521392822\n",
      "[Training Epoch 0] Batch 99, Loss 0.5187298059463501\n",
      "[Training Epoch 0] Batch 100, Loss 0.5121468305587769\n",
      "[Training Epoch 0] Batch 101, Loss 0.4374392032623291\n",
      "[Training Epoch 0] Batch 102, Loss 0.47923582792282104\n",
      "[Training Epoch 0] Batch 103, Loss 0.4386078417301178\n",
      "[Training Epoch 0] Batch 104, Loss 0.443134069442749\n",
      "[Training Epoch 0] Batch 105, Loss 0.47349902987480164\n",
      "[Training Epoch 0] Batch 106, Loss 0.468832790851593\n",
      "[Training Epoch 0] Batch 107, Loss 0.48028630018234253\n",
      "[Training Epoch 0] Batch 108, Loss 0.46511730551719666\n",
      "[Training Epoch 0] Batch 109, Loss 0.48012205958366394\n",
      "[Training Epoch 0] Batch 110, Loss 0.4876934289932251\n",
      "[Training Epoch 0] Batch 111, Loss 0.4811505079269409\n",
      "[Training Epoch 0] Batch 112, Loss 0.4710402488708496\n",
      "[Training Epoch 0] Batch 113, Loss 0.48420748114585876\n",
      "[Training Epoch 0] Batch 114, Loss 0.46729129552841187\n",
      "[Training Epoch 0] Batch 115, Loss 0.47014933824539185\n",
      "[Training Epoch 0] Batch 116, Loss 0.46096551418304443\n",
      "[Training Epoch 0] Batch 117, Loss 0.47072654962539673\n",
      "[Training Epoch 0] Batch 118, Loss 0.46863090991973877\n",
      "[Training Epoch 0] Batch 119, Loss 0.45497798919677734\n",
      "[Training Epoch 0] Batch 120, Loss 0.46487224102020264\n",
      "[Training Epoch 0] Batch 121, Loss 0.4464761018753052\n",
      "[Training Epoch 0] Batch 122, Loss 0.5015063285827637\n",
      "[Training Epoch 0] Batch 123, Loss 0.455428808927536\n",
      "[Training Epoch 0] Batch 124, Loss 0.4540742039680481\n",
      "[Training Epoch 0] Batch 125, Loss 0.4786890149116516\n",
      "[Training Epoch 0] Batch 126, Loss 0.4765201210975647\n",
      "[Training Epoch 0] Batch 127, Loss 0.48242872953414917\n",
      "[Training Epoch 0] Batch 128, Loss 0.4668421149253845\n",
      "[Training Epoch 0] Batch 129, Loss 0.46187686920166016\n",
      "[Training Epoch 0] Batch 130, Loss 0.435686856508255\n",
      "[Training Epoch 0] Batch 131, Loss 0.43639862537384033\n",
      "[Training Epoch 0] Batch 132, Loss 0.4375556409358978\n",
      "[Training Epoch 0] Batch 133, Loss 0.42572474479675293\n",
      "[Training Epoch 0] Batch 134, Loss 0.4541371762752533\n",
      "[Training Epoch 0] Batch 135, Loss 0.4682060182094574\n",
      "[Training Epoch 0] Batch 136, Loss 0.4449785351753235\n",
      "[Training Epoch 0] Batch 137, Loss 0.43912601470947266\n",
      "[Training Epoch 0] Batch 138, Loss 0.43951138854026794\n",
      "[Training Epoch 0] Batch 139, Loss 0.45139673352241516\n",
      "[Training Epoch 0] Batch 140, Loss 0.442665159702301\n",
      "[Training Epoch 0] Batch 141, Loss 0.49875426292419434\n",
      "[Training Epoch 0] Batch 142, Loss 0.4546332061290741\n",
      "[Training Epoch 0] Batch 143, Loss 0.4415940046310425\n",
      "[Training Epoch 0] Batch 144, Loss 0.43940553069114685\n",
      "[Training Epoch 0] Batch 145, Loss 0.44227567315101624\n",
      "[Training Epoch 0] Batch 146, Loss 0.44883131980895996\n",
      "[Training Epoch 0] Batch 147, Loss 0.4384026527404785\n",
      "[Training Epoch 0] Batch 148, Loss 0.44976621866226196\n",
      "[Training Epoch 0] Batch 149, Loss 0.4564421474933624\n",
      "[Training Epoch 0] Batch 150, Loss 0.4315651059150696\n",
      "[Training Epoch 0] Batch 151, Loss 0.4142451286315918\n",
      "[Training Epoch 0] Batch 152, Loss 0.4327203631401062\n",
      "[Training Epoch 0] Batch 153, Loss 0.43125343322753906\n",
      "[Training Epoch 0] Batch 154, Loss 0.43811535835266113\n",
      "[Training Epoch 0] Batch 155, Loss 0.4189523458480835\n",
      "[Training Epoch 0] Batch 156, Loss 0.41279473900794983\n",
      "[Training Epoch 0] Batch 157, Loss 0.42497915029525757\n",
      "[Training Epoch 0] Batch 158, Loss 0.4139435291290283\n",
      "[Training Epoch 0] Batch 159, Loss 0.42923882603645325\n",
      "[Training Epoch 0] Batch 160, Loss 0.4348093271255493\n",
      "[Training Epoch 0] Batch 161, Loss 0.4401732385158539\n",
      "[Training Epoch 0] Batch 162, Loss 0.41715043783187866\n",
      "[Training Epoch 0] Batch 163, Loss 0.4048992395401001\n",
      "[Training Epoch 0] Batch 164, Loss 0.42941445112228394\n",
      "[Training Epoch 0] Batch 165, Loss 0.38383549451828003\n",
      "[Training Epoch 0] Batch 166, Loss 0.40684792399406433\n",
      "[Training Epoch 0] Batch 167, Loss 0.3784937560558319\n",
      "[Training Epoch 0] Batch 168, Loss 0.3893996477127075\n",
      "[Training Epoch 0] Batch 169, Loss 0.418950617313385\n",
      "[Training Epoch 0] Batch 170, Loss 0.38859108090400696\n",
      "[Training Epoch 0] Batch 171, Loss 0.4367368817329407\n",
      "[Training Epoch 0] Batch 172, Loss 0.3938644528388977\n",
      "[Training Epoch 0] Batch 173, Loss 0.41480156779289246\n",
      "[Training Epoch 0] Batch 174, Loss 0.406727135181427\n",
      "[Training Epoch 0] Batch 175, Loss 0.39307257533073425\n",
      "[Training Epoch 0] Batch 176, Loss 0.4100644588470459\n",
      "[Training Epoch 0] Batch 177, Loss 0.4052296280860901\n",
      "[Training Epoch 0] Batch 178, Loss 0.3723779320716858\n",
      "[Training Epoch 0] Batch 179, Loss 0.3774605691432953\n",
      "[Training Epoch 0] Batch 180, Loss 0.40054571628570557\n",
      "[Training Epoch 0] Batch 181, Loss 0.4353765845298767\n",
      "[Training Epoch 0] Batch 182, Loss 0.4120168685913086\n",
      "[Training Epoch 0] Batch 183, Loss 0.3953191339969635\n",
      "[Training Epoch 0] Batch 184, Loss 0.4297054409980774\n",
      "[Training Epoch 0] Batch 185, Loss 0.3908962607383728\n",
      "[Training Epoch 0] Batch 186, Loss 0.4195272922515869\n",
      "[Training Epoch 0] Batch 187, Loss 0.40892234444618225\n",
      "[Training Epoch 0] Batch 188, Loss 0.3945731520652771\n",
      "[Training Epoch 0] Batch 189, Loss 0.3942350745201111\n",
      "[Training Epoch 0] Batch 190, Loss 0.3973523676395416\n",
      "[Training Epoch 0] Batch 191, Loss 0.416201114654541\n",
      "[Training Epoch 0] Batch 192, Loss 0.3882169723510742\n",
      "[Training Epoch 0] Batch 193, Loss 0.3805082142353058\n",
      "[Training Epoch 0] Batch 194, Loss 0.39183348417282104\n",
      "[Training Epoch 0] Batch 195, Loss 0.403999388217926\n",
      "[Training Epoch 0] Batch 196, Loss 0.39959532022476196\n",
      "[Training Epoch 0] Batch 197, Loss 0.40789520740509033\n",
      "[Training Epoch 0] Batch 198, Loss 0.38691017031669617\n",
      "[Training Epoch 0] Batch 199, Loss 0.4029264450073242\n",
      "[Training Epoch 0] Batch 200, Loss 0.3818817436695099\n",
      "[Training Epoch 0] Batch 201, Loss 0.41027140617370605\n",
      "[Training Epoch 0] Batch 202, Loss 0.4293021559715271\n",
      "[Training Epoch 0] Batch 203, Loss 0.4197305142879486\n",
      "[Training Epoch 0] Batch 204, Loss 0.38268083333969116\n",
      "[Training Epoch 0] Batch 205, Loss 0.40177422761917114\n",
      "[Training Epoch 0] Batch 206, Loss 0.4173179864883423\n",
      "[Training Epoch 0] Batch 207, Loss 0.3871386647224426\n",
      "[Training Epoch 0] Batch 208, Loss 0.3939428925514221\n",
      "[Training Epoch 0] Batch 209, Loss 0.41360288858413696\n",
      "[Training Epoch 0] Batch 210, Loss 0.3887068033218384\n",
      "[Training Epoch 0] Batch 211, Loss 0.3972787857055664\n",
      "[Training Epoch 0] Batch 212, Loss 0.3823830187320709\n",
      "[Training Epoch 0] Batch 213, Loss 0.41540268063545227\n",
      "[Training Epoch 0] Batch 214, Loss 0.4063894748687744\n",
      "[Training Epoch 0] Batch 215, Loss 0.3615669906139374\n",
      "[Training Epoch 0] Batch 216, Loss 0.38815057277679443\n",
      "[Training Epoch 0] Batch 217, Loss 0.38699352741241455\n",
      "[Training Epoch 0] Batch 218, Loss 0.3684348464012146\n",
      "[Training Epoch 0] Batch 219, Loss 0.39116525650024414\n",
      "[Training Epoch 0] Batch 220, Loss 0.3851860463619232\n",
      "[Training Epoch 0] Batch 221, Loss 0.3645431399345398\n",
      "[Training Epoch 0] Batch 222, Loss 0.36809980869293213\n",
      "[Training Epoch 0] Batch 223, Loss 0.3752514719963074\n",
      "[Training Epoch 0] Batch 224, Loss 0.3704005181789398\n",
      "[Training Epoch 0] Batch 225, Loss 0.3505878448486328\n",
      "[Training Epoch 0] Batch 226, Loss 0.4014950096607208\n",
      "[Training Epoch 0] Batch 227, Loss 0.38352271914482117\n",
      "[Training Epoch 0] Batch 228, Loss 0.36290740966796875\n",
      "[Training Epoch 0] Batch 229, Loss 0.3928346633911133\n",
      "[Training Epoch 0] Batch 230, Loss 0.38647598028182983\n",
      "[Training Epoch 0] Batch 231, Loss 0.3919171094894409\n",
      "[Training Epoch 0] Batch 232, Loss 0.3436799645423889\n",
      "[Training Epoch 0] Batch 233, Loss 0.38733404874801636\n",
      "[Training Epoch 0] Batch 234, Loss 0.37175679206848145\n",
      "[Training Epoch 0] Batch 235, Loss 0.3611547648906708\n",
      "[Training Epoch 0] Batch 236, Loss 0.3992033004760742\n",
      "[Training Epoch 0] Batch 237, Loss 0.4136506915092468\n",
      "[Training Epoch 0] Batch 238, Loss 0.3970930576324463\n",
      "[Training Epoch 0] Batch 239, Loss 0.4035069942474365\n",
      "[Training Epoch 0] Batch 240, Loss 0.3917255103588104\n",
      "[Training Epoch 0] Batch 241, Loss 0.36472320556640625\n",
      "[Training Epoch 0] Batch 242, Loss 0.4154161810874939\n",
      "[Training Epoch 0] Batch 243, Loss 0.35319778323173523\n",
      "[Training Epoch 0] Batch 244, Loss 0.391842782497406\n",
      "[Training Epoch 0] Batch 245, Loss 0.3535430431365967\n",
      "[Training Epoch 0] Batch 246, Loss 0.40130555629730225\n",
      "[Training Epoch 0] Batch 247, Loss 0.39919599890708923\n",
      "[Training Epoch 0] Batch 248, Loss 0.38515591621398926\n",
      "[Training Epoch 0] Batch 249, Loss 0.387739896774292\n",
      "[Training Epoch 0] Batch 250, Loss 0.376437783241272\n",
      "[Training Epoch 0] Batch 251, Loss 0.3802078366279602\n",
      "[Training Epoch 0] Batch 252, Loss 0.38389265537261963\n",
      "[Training Epoch 0] Batch 253, Loss 0.39417117834091187\n",
      "[Training Epoch 0] Batch 254, Loss 0.39008039236068726\n",
      "[Training Epoch 0] Batch 255, Loss 0.40066128969192505\n",
      "[Training Epoch 0] Batch 256, Loss 0.3780978322029114\n",
      "[Training Epoch 0] Batch 257, Loss 0.39127424359321594\n",
      "[Training Epoch 0] Batch 258, Loss 0.4200862646102905\n",
      "[Training Epoch 0] Batch 259, Loss 0.3934289216995239\n",
      "[Training Epoch 0] Batch 260, Loss 0.39042919874191284\n",
      "[Training Epoch 0] Batch 261, Loss 0.4077981114387512\n",
      "[Training Epoch 0] Batch 262, Loss 0.3905786871910095\n",
      "[Training Epoch 0] Batch 263, Loss 0.39109331369400024\n",
      "[Training Epoch 0] Batch 264, Loss 0.378781795501709\n",
      "[Training Epoch 0] Batch 265, Loss 0.3680543303489685\n",
      "[Training Epoch 0] Batch 266, Loss 0.3851326107978821\n",
      "[Training Epoch 0] Batch 267, Loss 0.34109634160995483\n",
      "[Training Epoch 0] Batch 268, Loss 0.3832119107246399\n",
      "[Training Epoch 0] Batch 269, Loss 0.3876045048236847\n",
      "[Training Epoch 0] Batch 270, Loss 0.35270044207572937\n",
      "[Training Epoch 0] Batch 271, Loss 0.39667341113090515\n",
      "[Training Epoch 0] Batch 272, Loss 0.37951964139938354\n",
      "[Training Epoch 0] Batch 273, Loss 0.37110137939453125\n",
      "[Training Epoch 0] Batch 274, Loss 0.3891739249229431\n",
      "[Training Epoch 0] Batch 275, Loss 0.41129595041275024\n",
      "[Training Epoch 0] Batch 276, Loss 0.4194294214248657\n",
      "[Training Epoch 0] Batch 277, Loss 0.3743223547935486\n",
      "[Training Epoch 0] Batch 278, Loss 0.35624414682388306\n",
      "[Training Epoch 0] Batch 279, Loss 0.40693041682243347\n",
      "[Training Epoch 0] Batch 280, Loss 0.39018717408180237\n",
      "[Training Epoch 0] Batch 281, Loss 0.3622013330459595\n",
      "[Training Epoch 0] Batch 282, Loss 0.3849408030509949\n",
      "[Training Epoch 0] Batch 283, Loss 0.39406341314315796\n",
      "[Training Epoch 0] Batch 284, Loss 0.4065866470336914\n",
      "[Training Epoch 0] Batch 285, Loss 0.3815597891807556\n",
      "[Training Epoch 0] Batch 286, Loss 0.36229419708251953\n",
      "[Training Epoch 0] Batch 287, Loss 0.3710762858390808\n",
      "[Training Epoch 0] Batch 288, Loss 0.37835946679115295\n",
      "[Training Epoch 0] Batch 289, Loss 0.3802837133407593\n",
      "[Training Epoch 0] Batch 290, Loss 0.38978034257888794\n",
      "[Training Epoch 0] Batch 291, Loss 0.3734128475189209\n",
      "[Training Epoch 0] Batch 292, Loss 0.36863958835601807\n",
      "[Training Epoch 0] Batch 293, Loss 0.3714565634727478\n",
      "[Training Epoch 0] Batch 294, Loss 0.3966301381587982\n",
      "[Training Epoch 0] Batch 295, Loss 0.3946141302585602\n",
      "[Training Epoch 0] Batch 296, Loss 0.3757656216621399\n",
      "[Training Epoch 0] Batch 297, Loss 0.38711339235305786\n",
      "[Training Epoch 0] Batch 298, Loss 0.37090516090393066\n",
      "[Training Epoch 0] Batch 299, Loss 0.3635348081588745\n",
      "[Training Epoch 0] Batch 300, Loss 0.3730185627937317\n",
      "[Training Epoch 0] Batch 301, Loss 0.4020032584667206\n",
      "[Training Epoch 0] Batch 302, Loss 0.3618263900279999\n",
      "[Training Epoch 0] Batch 303, Loss 0.361147940158844\n",
      "[Training Epoch 0] Batch 304, Loss 0.3443996012210846\n",
      "[Training Epoch 0] Batch 305, Loss 0.36500343680381775\n",
      "[Training Epoch 0] Batch 306, Loss 0.39652371406555176\n",
      "[Training Epoch 0] Batch 307, Loss 0.4083617925643921\n",
      "[Training Epoch 0] Batch 308, Loss 0.39641091227531433\n",
      "[Training Epoch 0] Batch 309, Loss 0.38656455278396606\n",
      "[Training Epoch 0] Batch 310, Loss 0.37161701917648315\n",
      "[Training Epoch 0] Batch 311, Loss 0.3891005218029022\n",
      "[Training Epoch 0] Batch 312, Loss 0.35834363102912903\n",
      "[Training Epoch 0] Batch 313, Loss 0.3606451451778412\n",
      "[Training Epoch 0] Batch 314, Loss 0.38380831480026245\n",
      "[Training Epoch 0] Batch 315, Loss 0.3892894685268402\n",
      "[Training Epoch 0] Batch 316, Loss 0.3697437644004822\n",
      "[Training Epoch 0] Batch 317, Loss 0.41421645879745483\n",
      "[Training Epoch 0] Batch 318, Loss 0.3773877024650574\n",
      "[Training Epoch 0] Batch 319, Loss 0.373288094997406\n",
      "[Training Epoch 0] Batch 320, Loss 0.370001882314682\n",
      "[Training Epoch 0] Batch 321, Loss 0.39782509207725525\n",
      "[Training Epoch 0] Batch 322, Loss 0.3755621612071991\n",
      "[Training Epoch 0] Batch 323, Loss 0.386746346950531\n",
      "[Training Epoch 0] Batch 324, Loss 0.35193854570388794\n",
      "[Training Epoch 0] Batch 325, Loss 0.3692936301231384\n",
      "[Training Epoch 0] Batch 326, Loss 0.3676554560661316\n",
      "[Training Epoch 0] Batch 327, Loss 0.38131457567214966\n",
      "[Training Epoch 0] Batch 328, Loss 0.40369755029678345\n",
      "[Training Epoch 0] Batch 329, Loss 0.37991297245025635\n",
      "[Training Epoch 0] Batch 330, Loss 0.3549480438232422\n",
      "[Training Epoch 0] Batch 331, Loss 0.38279008865356445\n",
      "[Training Epoch 0] Batch 332, Loss 0.3621819019317627\n",
      "[Training Epoch 0] Batch 333, Loss 0.39120548963546753\n",
      "[Training Epoch 0] Batch 334, Loss 0.34592205286026\n",
      "[Training Epoch 0] Batch 335, Loss 0.40414565801620483\n",
      "[Training Epoch 0] Batch 336, Loss 0.38187316060066223\n",
      "[Training Epoch 0] Batch 337, Loss 0.3509823679924011\n",
      "[Training Epoch 0] Batch 338, Loss 0.4085303544998169\n",
      "[Training Epoch 0] Batch 339, Loss 0.38882648944854736\n",
      "[Training Epoch 0] Batch 340, Loss 0.40944862365722656\n",
      "[Training Epoch 0] Batch 341, Loss 0.3644855320453644\n",
      "[Training Epoch 0] Batch 342, Loss 0.3859434723854065\n",
      "[Training Epoch 0] Batch 343, Loss 0.36946624517440796\n",
      "[Training Epoch 0] Batch 344, Loss 0.40565913915634155\n",
      "[Training Epoch 0] Batch 345, Loss 0.3687383532524109\n",
      "[Training Epoch 0] Batch 346, Loss 0.36723002791404724\n",
      "[Training Epoch 0] Batch 347, Loss 0.38421595096588135\n",
      "[Training Epoch 0] Batch 348, Loss 0.37191376090049744\n",
      "[Training Epoch 0] Batch 349, Loss 0.3831218481063843\n",
      "[Training Epoch 0] Batch 350, Loss 0.3688945472240448\n",
      "[Training Epoch 0] Batch 351, Loss 0.3638173043727875\n",
      "[Training Epoch 0] Batch 352, Loss 0.35731571912765503\n",
      "[Training Epoch 0] Batch 353, Loss 0.3837728500366211\n",
      "[Training Epoch 0] Batch 354, Loss 0.38063302636146545\n",
      "[Training Epoch 0] Batch 355, Loss 0.38029390573501587\n",
      "[Training Epoch 0] Batch 356, Loss 0.40751412510871887\n",
      "[Training Epoch 0] Batch 357, Loss 0.3598134517669678\n",
      "[Training Epoch 0] Batch 358, Loss 0.3860895037651062\n",
      "[Training Epoch 0] Batch 359, Loss 0.36730244755744934\n",
      "[Training Epoch 0] Batch 360, Loss 0.367891788482666\n",
      "[Training Epoch 0] Batch 361, Loss 0.3791120648384094\n",
      "[Training Epoch 0] Batch 362, Loss 0.3873135447502136\n",
      "[Training Epoch 0] Batch 363, Loss 0.3776775002479553\n",
      "[Training Epoch 0] Batch 364, Loss 0.34775757789611816\n",
      "[Training Epoch 0] Batch 365, Loss 0.3804795742034912\n",
      "[Training Epoch 0] Batch 366, Loss 0.33562833070755005\n",
      "[Training Epoch 0] Batch 367, Loss 0.3904527425765991\n",
      "[Training Epoch 0] Batch 368, Loss 0.3472365736961365\n",
      "[Training Epoch 0] Batch 369, Loss 0.3569108843803406\n",
      "[Training Epoch 0] Batch 370, Loss 0.3594149351119995\n",
      "[Training Epoch 0] Batch 371, Loss 0.3939775228500366\n",
      "[Training Epoch 0] Batch 372, Loss 0.4030088186264038\n",
      "[Training Epoch 0] Batch 373, Loss 0.3843896985054016\n",
      "[Training Epoch 0] Batch 374, Loss 0.3583022952079773\n",
      "[Training Epoch 0] Batch 375, Loss 0.3692670464515686\n",
      "[Training Epoch 0] Batch 376, Loss 0.37921637296676636\n",
      "[Training Epoch 0] Batch 377, Loss 0.3866216540336609\n",
      "[Training Epoch 0] Batch 378, Loss 0.3935653567314148\n",
      "[Training Epoch 0] Batch 379, Loss 0.33672618865966797\n",
      "[Training Epoch 0] Batch 380, Loss 0.3757505416870117\n",
      "[Training Epoch 0] Batch 381, Loss 0.37651142477989197\n",
      "[Training Epoch 0] Batch 382, Loss 0.3985757529735565\n",
      "[Training Epoch 0] Batch 383, Loss 0.3725317418575287\n",
      "[Training Epoch 0] Batch 384, Loss 0.3804474174976349\n",
      "[Training Epoch 0] Batch 385, Loss 0.3585050702095032\n",
      "[Training Epoch 0] Batch 386, Loss 0.38671979308128357\n",
      "[Training Epoch 0] Batch 387, Loss 0.33747565746307373\n",
      "[Training Epoch 0] Batch 388, Loss 0.37859344482421875\n",
      "[Training Epoch 0] Batch 389, Loss 0.3774099349975586\n",
      "[Training Epoch 0] Batch 390, Loss 0.3670383095741272\n",
      "[Training Epoch 0] Batch 391, Loss 0.36161863803863525\n",
      "[Training Epoch 0] Batch 392, Loss 0.382357656955719\n",
      "[Training Epoch 0] Batch 393, Loss 0.37504813075065613\n",
      "[Training Epoch 0] Batch 394, Loss 0.3755998909473419\n",
      "[Training Epoch 0] Batch 395, Loss 0.37101244926452637\n",
      "[Training Epoch 0] Batch 396, Loss 0.37706470489501953\n",
      "[Training Epoch 0] Batch 397, Loss 0.40003055334091187\n",
      "[Training Epoch 0] Batch 398, Loss 0.33718588948249817\n",
      "[Training Epoch 0] Batch 399, Loss 0.36117058992385864\n",
      "[Training Epoch 0] Batch 400, Loss 0.36288589239120483\n",
      "[Training Epoch 0] Batch 401, Loss 0.3641315698623657\n",
      "[Training Epoch 0] Batch 402, Loss 0.3658885061740875\n",
      "[Training Epoch 0] Batch 403, Loss 0.3675887882709503\n",
      "[Training Epoch 0] Batch 404, Loss 0.4000629782676697\n",
      "[Training Epoch 0] Batch 405, Loss 0.3893436789512634\n",
      "[Training Epoch 0] Batch 406, Loss 0.35745179653167725\n",
      "[Training Epoch 0] Batch 407, Loss 0.4081306457519531\n",
      "[Training Epoch 0] Batch 408, Loss 0.34120550751686096\n",
      "[Training Epoch 0] Batch 409, Loss 0.36780041456222534\n",
      "[Training Epoch 0] Batch 410, Loss 0.38088279962539673\n",
      "[Training Epoch 0] Batch 411, Loss 0.3596220016479492\n",
      "[Training Epoch 0] Batch 412, Loss 0.3599826693534851\n",
      "[Training Epoch 0] Batch 413, Loss 0.383674681186676\n",
      "[Training Epoch 0] Batch 414, Loss 0.3538951277732849\n",
      "[Training Epoch 0] Batch 415, Loss 0.3777330219745636\n",
      "[Training Epoch 0] Batch 416, Loss 0.38306301832199097\n",
      "[Training Epoch 0] Batch 417, Loss 0.4039965867996216\n",
      "[Training Epoch 0] Batch 418, Loss 0.3855111002922058\n",
      "[Training Epoch 0] Batch 419, Loss 0.39477986097335815\n",
      "[Training Epoch 0] Batch 420, Loss 0.3851253390312195\n",
      "[Training Epoch 0] Batch 421, Loss 0.3717913031578064\n",
      "[Training Epoch 0] Batch 422, Loss 0.37574225664138794\n",
      "[Training Epoch 0] Batch 423, Loss 0.3922073245048523\n",
      "[Training Epoch 0] Batch 424, Loss 0.36442264914512634\n",
      "[Training Epoch 0] Batch 425, Loss 0.37964537739753723\n",
      "[Training Epoch 0] Batch 426, Loss 0.38041847944259644\n",
      "[Training Epoch 0] Batch 427, Loss 0.38635241985321045\n",
      "[Training Epoch 0] Batch 428, Loss 0.39561739563941956\n",
      "[Training Epoch 0] Batch 429, Loss 0.3669213652610779\n",
      "[Training Epoch 0] Batch 430, Loss 0.3689100742340088\n",
      "[Training Epoch 0] Batch 431, Loss 0.3775275647640228\n",
      "[Training Epoch 0] Batch 432, Loss 0.3934323489665985\n",
      "[Training Epoch 0] Batch 433, Loss 0.39315083622932434\n",
      "[Training Epoch 0] Batch 434, Loss 0.38551944494247437\n",
      "[Training Epoch 0] Batch 435, Loss 0.34555208683013916\n",
      "[Training Epoch 0] Batch 436, Loss 0.3703773021697998\n",
      "[Training Epoch 0] Batch 437, Loss 0.37143784761428833\n",
      "[Training Epoch 0] Batch 438, Loss 0.39070552587509155\n",
      "[Training Epoch 0] Batch 439, Loss 0.3507559597492218\n",
      "[Training Epoch 0] Batch 440, Loss 0.3841087222099304\n",
      "[Training Epoch 0] Batch 441, Loss 0.3841575086116791\n",
      "[Training Epoch 0] Batch 442, Loss 0.38337162137031555\n",
      "[Training Epoch 0] Batch 443, Loss 0.4020724594593048\n",
      "[Training Epoch 0] Batch 444, Loss 0.36964869499206543\n",
      "[Training Epoch 0] Batch 445, Loss 0.38856109976768494\n",
      "[Training Epoch 0] Batch 446, Loss 0.3514215350151062\n",
      "[Training Epoch 0] Batch 447, Loss 0.4025161862373352\n",
      "[Training Epoch 0] Batch 448, Loss 0.39402616024017334\n",
      "[Training Epoch 0] Batch 449, Loss 0.3893786668777466\n",
      "[Training Epoch 0] Batch 450, Loss 0.3670959770679474\n",
      "[Training Epoch 0] Batch 451, Loss 0.362844854593277\n",
      "[Training Epoch 0] Batch 452, Loss 0.3935902714729309\n",
      "[Training Epoch 0] Batch 453, Loss 0.37819308042526245\n",
      "[Training Epoch 0] Batch 454, Loss 0.38226640224456787\n",
      "[Training Epoch 0] Batch 455, Loss 0.3778969943523407\n",
      "[Training Epoch 0] Batch 456, Loss 0.37880739569664\n",
      "[Training Epoch 0] Batch 457, Loss 0.36886537075042725\n",
      "[Training Epoch 0] Batch 458, Loss 0.3876951038837433\n",
      "[Training Epoch 0] Batch 459, Loss 0.3791384696960449\n",
      "[Training Epoch 0] Batch 460, Loss 0.37562650442123413\n",
      "[Training Epoch 0] Batch 461, Loss 0.33259516954421997\n",
      "[Training Epoch 0] Batch 462, Loss 0.38366979360580444\n",
      "[Training Epoch 0] Batch 463, Loss 0.3544212281703949\n",
      "[Training Epoch 0] Batch 464, Loss 0.36867135763168335\n",
      "[Training Epoch 0] Batch 465, Loss 0.3614984154701233\n",
      "[Training Epoch 0] Batch 466, Loss 0.3781030476093292\n",
      "[Training Epoch 0] Batch 467, Loss 0.3543258309364319\n",
      "[Training Epoch 0] Batch 468, Loss 0.36260586977005005\n",
      "[Training Epoch 0] Batch 469, Loss 0.4041996896266937\n",
      "[Training Epoch 0] Batch 470, Loss 0.3844384253025055\n",
      "[Training Epoch 0] Batch 471, Loss 0.3780365586280823\n",
      "[Training Epoch 0] Batch 472, Loss 0.3689381182193756\n",
      "[Training Epoch 0] Batch 473, Loss 0.36892592906951904\n",
      "[Training Epoch 0] Batch 474, Loss 0.372372031211853\n",
      "[Training Epoch 0] Batch 475, Loss 0.36294132471084595\n",
      "[Training Epoch 0] Batch 476, Loss 0.3897448778152466\n",
      "[Training Epoch 0] Batch 477, Loss 0.3551212549209595\n",
      "[Training Epoch 0] Batch 478, Loss 0.35361623764038086\n",
      "[Training Epoch 0] Batch 479, Loss 0.3707291781902313\n",
      "[Training Epoch 0] Batch 480, Loss 0.37057021260261536\n",
      "[Training Epoch 0] Batch 481, Loss 0.3674378991127014\n",
      "[Training Epoch 0] Batch 482, Loss 0.349592000246048\n",
      "[Training Epoch 0] Batch 483, Loss 0.3489660620689392\n",
      "[Training Epoch 0] Batch 484, Loss 0.37511950731277466\n",
      "[Training Epoch 0] Batch 485, Loss 0.37687060236930847\n",
      "[Training Epoch 0] Batch 486, Loss 0.3748859167098999\n",
      "[Training Epoch 0] Batch 487, Loss 0.3638768196105957\n",
      "[Training Epoch 0] Batch 488, Loss 0.3948338031768799\n",
      "[Training Epoch 0] Batch 489, Loss 0.40538421273231506\n",
      "[Training Epoch 0] Batch 490, Loss 0.36710605025291443\n",
      "[Training Epoch 0] Batch 491, Loss 0.34494394063949585\n",
      "[Training Epoch 0] Batch 492, Loss 0.3945302665233612\n",
      "[Training Epoch 0] Batch 493, Loss 0.38390517234802246\n",
      "[Training Epoch 0] Batch 494, Loss 0.36417728662490845\n",
      "[Training Epoch 0] Batch 495, Loss 0.35737302899360657\n",
      "[Training Epoch 0] Batch 496, Loss 0.36462894082069397\n",
      "[Training Epoch 0] Batch 497, Loss 0.36130666732788086\n",
      "[Training Epoch 0] Batch 498, Loss 0.3630076050758362\n",
      "[Training Epoch 0] Batch 499, Loss 0.3777354061603546\n",
      "[Training Epoch 0] Batch 500, Loss 0.3560318946838379\n",
      "[Training Epoch 0] Batch 501, Loss 0.39053916931152344\n",
      "[Training Epoch 0] Batch 502, Loss 0.37437403202056885\n",
      "[Training Epoch 0] Batch 503, Loss 0.36654889583587646\n",
      "[Training Epoch 0] Batch 504, Loss 0.3370913863182068\n",
      "[Training Epoch 0] Batch 505, Loss 0.3870096802711487\n",
      "[Training Epoch 0] Batch 506, Loss 0.39723896980285645\n",
      "[Training Epoch 0] Batch 507, Loss 0.372760146856308\n",
      "[Training Epoch 0] Batch 508, Loss 0.347974956035614\n",
      "[Training Epoch 0] Batch 509, Loss 0.364097535610199\n",
      "[Training Epoch 0] Batch 510, Loss 0.3920494318008423\n",
      "[Training Epoch 0] Batch 511, Loss 0.3677161633968353\n",
      "[Training Epoch 0] Batch 512, Loss 0.36147284507751465\n",
      "[Training Epoch 0] Batch 513, Loss 0.378889799118042\n",
      "[Training Epoch 0] Batch 514, Loss 0.3830583095550537\n",
      "[Training Epoch 0] Batch 515, Loss 0.37527161836624146\n",
      "[Training Epoch 0] Batch 516, Loss 0.39326828718185425\n",
      "[Training Epoch 0] Batch 517, Loss 0.39012712240219116\n",
      "[Training Epoch 0] Batch 518, Loss 0.3710762858390808\n",
      "[Training Epoch 0] Batch 519, Loss 0.36517584323883057\n",
      "[Training Epoch 0] Batch 520, Loss 0.3601190447807312\n",
      "[Training Epoch 0] Batch 521, Loss 0.37597164511680603\n",
      "[Training Epoch 0] Batch 522, Loss 0.39400148391723633\n",
      "[Training Epoch 0] Batch 523, Loss 0.36063653230667114\n",
      "[Training Epoch 0] Batch 524, Loss 0.37615805864334106\n",
      "[Training Epoch 0] Batch 525, Loss 0.3713439702987671\n",
      "[Training Epoch 0] Batch 526, Loss 0.3348020315170288\n",
      "[Training Epoch 0] Batch 527, Loss 0.368835985660553\n",
      "[Training Epoch 0] Batch 528, Loss 0.36603203415870667\n",
      "[Training Epoch 0] Batch 529, Loss 0.36270660161972046\n",
      "[Training Epoch 0] Batch 530, Loss 0.3543837368488312\n",
      "[Training Epoch 0] Batch 531, Loss 0.34196555614471436\n",
      "[Training Epoch 0] Batch 532, Loss 0.34924134612083435\n",
      "[Training Epoch 0] Batch 533, Loss 0.3828778564929962\n",
      "[Training Epoch 0] Batch 534, Loss 0.3721204400062561\n",
      "[Training Epoch 0] Batch 535, Loss 0.370953232049942\n",
      "[Training Epoch 0] Batch 536, Loss 0.4017540216445923\n",
      "[Training Epoch 0] Batch 537, Loss 0.4031345248222351\n",
      "[Training Epoch 0] Batch 538, Loss 0.3935837149620056\n",
      "[Training Epoch 0] Batch 539, Loss 0.3733775019645691\n",
      "[Training Epoch 0] Batch 540, Loss 0.3747802674770355\n",
      "[Training Epoch 0] Batch 541, Loss 0.35792866349220276\n",
      "[Training Epoch 0] Batch 542, Loss 0.3709602355957031\n",
      "[Training Epoch 0] Batch 543, Loss 0.35778945684432983\n",
      "[Training Epoch 0] Batch 544, Loss 0.36574435234069824\n",
      "[Training Epoch 0] Batch 545, Loss 0.38717618584632874\n",
      "[Training Epoch 0] Batch 546, Loss 0.35334014892578125\n",
      "[Training Epoch 0] Batch 547, Loss 0.3791203796863556\n",
      "[Training Epoch 0] Batch 548, Loss 0.391406774520874\n",
      "[Training Epoch 0] Batch 549, Loss 0.36422866582870483\n",
      "[Training Epoch 0] Batch 550, Loss 0.3805602788925171\n",
      "[Training Epoch 0] Batch 551, Loss 0.37377262115478516\n",
      "[Training Epoch 0] Batch 552, Loss 0.3854202628135681\n",
      "[Training Epoch 0] Batch 553, Loss 0.3495951294898987\n",
      "[Training Epoch 0] Batch 554, Loss 0.38457900285720825\n",
      "[Training Epoch 0] Batch 555, Loss 0.3736785054206848\n",
      "[Training Epoch 0] Batch 556, Loss 0.37378230690956116\n",
      "[Training Epoch 0] Batch 557, Loss 0.3724555969238281\n",
      "[Training Epoch 0] Batch 558, Loss 0.3711800277233124\n",
      "[Training Epoch 0] Batch 559, Loss 0.3356533348560333\n",
      "[Training Epoch 0] Batch 560, Loss 0.38004234433174133\n",
      "[Training Epoch 0] Batch 561, Loss 0.40931594371795654\n",
      "[Training Epoch 0] Batch 562, Loss 0.36386048793792725\n",
      "[Training Epoch 0] Batch 563, Loss 0.36756476759910583\n",
      "[Training Epoch 0] Batch 564, Loss 0.39227086305618286\n",
      "[Training Epoch 0] Batch 565, Loss 0.34297680854797363\n",
      "[Training Epoch 0] Batch 566, Loss 0.35107582807540894\n",
      "[Training Epoch 0] Batch 567, Loss 0.37450751662254333\n",
      "[Training Epoch 0] Batch 568, Loss 0.3984290361404419\n",
      "[Training Epoch 0] Batch 569, Loss 0.3474384546279907\n",
      "[Training Epoch 0] Batch 570, Loss 0.3746100068092346\n",
      "[Training Epoch 0] Batch 571, Loss 0.3595432639122009\n",
      "[Training Epoch 0] Batch 572, Loss 0.3872057795524597\n",
      "[Training Epoch 0] Batch 573, Loss 0.3981643319129944\n",
      "[Training Epoch 0] Batch 574, Loss 0.3778100609779358\n",
      "[Training Epoch 0] Batch 575, Loss 0.38470160961151123\n",
      "[Training Epoch 0] Batch 576, Loss 0.37160149216651917\n",
      "[Training Epoch 0] Batch 577, Loss 0.36400219798088074\n",
      "[Training Epoch 0] Batch 578, Loss 0.3795849680900574\n",
      "[Training Epoch 0] Batch 579, Loss 0.3570212721824646\n",
      "[Training Epoch 0] Batch 580, Loss 0.3835068345069885\n",
      "[Training Epoch 0] Batch 581, Loss 0.3637895882129669\n",
      "[Training Epoch 0] Batch 582, Loss 0.3571197986602783\n",
      "[Training Epoch 0] Batch 583, Loss 0.3820434808731079\n",
      "[Training Epoch 0] Batch 584, Loss 0.3978866636753082\n",
      "[Training Epoch 0] Batch 585, Loss 0.3314210772514343\n",
      "[Training Epoch 0] Batch 586, Loss 0.3555721938610077\n",
      "[Training Epoch 0] Batch 587, Loss 0.39402127265930176\n",
      "[Training Epoch 0] Batch 588, Loss 0.3678341507911682\n",
      "[Training Epoch 0] Batch 589, Loss 0.3619335889816284\n",
      "[Training Epoch 0] Batch 590, Loss 0.3679094910621643\n",
      "[Training Epoch 0] Batch 591, Loss 0.3557546138763428\n",
      "[Training Epoch 0] Batch 592, Loss 0.401782751083374\n",
      "[Training Epoch 0] Batch 593, Loss 0.3679589033126831\n",
      "[Training Epoch 0] Batch 594, Loss 0.3648003339767456\n",
      "[Training Epoch 0] Batch 595, Loss 0.3490350544452667\n",
      "[Training Epoch 0] Batch 596, Loss 0.383994460105896\n",
      "[Training Epoch 0] Batch 597, Loss 0.34326568245887756\n",
      "[Training Epoch 0] Batch 598, Loss 0.36752980947494507\n",
      "[Training Epoch 0] Batch 599, Loss 0.356952428817749\n",
      "[Training Epoch 0] Batch 600, Loss 0.3790012001991272\n",
      "[Training Epoch 0] Batch 601, Loss 0.36000218987464905\n",
      "[Training Epoch 0] Batch 602, Loss 0.3602166175842285\n",
      "[Training Epoch 0] Batch 603, Loss 0.36052268743515015\n",
      "[Training Epoch 0] Batch 604, Loss 0.33053332567214966\n",
      "[Training Epoch 0] Batch 605, Loss 0.4071197211742401\n",
      "[Training Epoch 0] Batch 606, Loss 0.33735644817352295\n",
      "[Training Epoch 0] Batch 607, Loss 0.37017273902893066\n",
      "[Training Epoch 0] Batch 608, Loss 0.34565091133117676\n",
      "[Training Epoch 0] Batch 609, Loss 0.34991455078125\n",
      "[Training Epoch 0] Batch 610, Loss 0.38416576385498047\n",
      "[Training Epoch 0] Batch 611, Loss 0.40245264768600464\n",
      "[Training Epoch 0] Batch 612, Loss 0.34950870275497437\n",
      "[Training Epoch 0] Batch 613, Loss 0.38668060302734375\n",
      "[Training Epoch 0] Batch 614, Loss 0.36523252725601196\n",
      "[Training Epoch 0] Batch 615, Loss 0.3636002540588379\n",
      "[Training Epoch 0] Batch 616, Loss 0.3625691533088684\n",
      "[Training Epoch 0] Batch 617, Loss 0.38154929876327515\n",
      "[Training Epoch 0] Batch 618, Loss 0.37871795892715454\n",
      "[Training Epoch 0] Batch 619, Loss 0.3685658872127533\n",
      "[Training Epoch 0] Batch 620, Loss 0.3919355869293213\n",
      "[Training Epoch 0] Batch 621, Loss 0.3721216320991516\n",
      "[Training Epoch 0] Batch 622, Loss 0.356147825717926\n",
      "[Training Epoch 0] Batch 623, Loss 0.3669520616531372\n",
      "[Training Epoch 0] Batch 624, Loss 0.36039990186691284\n",
      "[Training Epoch 0] Batch 625, Loss 0.38086390495300293\n",
      "[Training Epoch 0] Batch 626, Loss 0.36219221353530884\n",
      "[Training Epoch 0] Batch 627, Loss 0.35430774092674255\n",
      "[Training Epoch 0] Batch 628, Loss 0.374992311000824\n",
      "[Training Epoch 0] Batch 629, Loss 0.37030330300331116\n",
      "[Training Epoch 0] Batch 630, Loss 0.37320637702941895\n",
      "[Training Epoch 0] Batch 631, Loss 0.383430540561676\n",
      "[Training Epoch 0] Batch 632, Loss 0.38524097204208374\n",
      "[Training Epoch 0] Batch 633, Loss 0.34765276312828064\n",
      "[Training Epoch 0] Batch 634, Loss 0.36537662148475647\n",
      "[Training Epoch 0] Batch 635, Loss 0.36688482761383057\n",
      "[Training Epoch 0] Batch 636, Loss 0.3538479208946228\n",
      "[Training Epoch 0] Batch 637, Loss 0.3695579171180725\n",
      "[Training Epoch 0] Batch 638, Loss 0.3641532063484192\n",
      "[Training Epoch 0] Batch 639, Loss 0.3513537049293518\n",
      "[Training Epoch 0] Batch 640, Loss 0.37472060322761536\n",
      "[Training Epoch 0] Batch 641, Loss 0.3700118064880371\n",
      "[Training Epoch 0] Batch 642, Loss 0.38761764764785767\n",
      "[Training Epoch 0] Batch 643, Loss 0.3683531880378723\n",
      "[Training Epoch 0] Batch 644, Loss 0.36735302209854126\n",
      "[Training Epoch 0] Batch 645, Loss 0.36807042360305786\n",
      "[Training Epoch 0] Batch 646, Loss 0.3447967767715454\n",
      "[Training Epoch 0] Batch 647, Loss 0.35446280241012573\n",
      "[Training Epoch 0] Batch 648, Loss 0.3646898567676544\n",
      "[Training Epoch 0] Batch 649, Loss 0.35299789905548096\n",
      "[Training Epoch 0] Batch 650, Loss 0.3964785933494568\n",
      "[Training Epoch 0] Batch 651, Loss 0.3718482255935669\n",
      "[Training Epoch 0] Batch 652, Loss 0.3694837689399719\n",
      "[Training Epoch 0] Batch 653, Loss 0.3623692989349365\n",
      "[Training Epoch 0] Batch 654, Loss 0.38485240936279297\n",
      "[Training Epoch 0] Batch 655, Loss 0.33291688561439514\n",
      "[Training Epoch 0] Batch 656, Loss 0.3312418758869171\n",
      "[Training Epoch 0] Batch 657, Loss 0.37090003490448\n",
      "[Training Epoch 0] Batch 658, Loss 0.3904380798339844\n",
      "[Training Epoch 0] Batch 659, Loss 0.39866694808006287\n",
      "[Training Epoch 0] Batch 660, Loss 0.3663251996040344\n",
      "[Training Epoch 0] Batch 661, Loss 0.35983261466026306\n",
      "[Training Epoch 0] Batch 662, Loss 0.32696211338043213\n",
      "[Training Epoch 0] Batch 663, Loss 0.35874295234680176\n",
      "[Training Epoch 0] Batch 664, Loss 0.35113418102264404\n",
      "[Training Epoch 0] Batch 665, Loss 0.39526301622390747\n",
      "[Training Epoch 0] Batch 666, Loss 0.3539440333843231\n",
      "[Training Epoch 0] Batch 667, Loss 0.33119919896125793\n",
      "[Training Epoch 0] Batch 668, Loss 0.38103723526000977\n",
      "[Training Epoch 0] Batch 669, Loss 0.35953304171562195\n",
      "[Training Epoch 0] Batch 670, Loss 0.3664247691631317\n",
      "[Training Epoch 0] Batch 671, Loss 0.38052770495414734\n",
      "[Training Epoch 0] Batch 672, Loss 0.34005168080329895\n",
      "[Training Epoch 0] Batch 673, Loss 0.3831930160522461\n",
      "[Training Epoch 0] Batch 674, Loss 0.339205801486969\n",
      "[Training Epoch 0] Batch 675, Loss 0.3449420928955078\n",
      "[Training Epoch 0] Batch 676, Loss 0.3322455883026123\n",
      "[Training Epoch 0] Batch 677, Loss 0.361444890499115\n",
      "[Training Epoch 0] Batch 678, Loss 0.3793381452560425\n",
      "[Training Epoch 0] Batch 679, Loss 0.3527452051639557\n",
      "[Training Epoch 0] Batch 680, Loss 0.3457799553871155\n",
      "[Training Epoch 0] Batch 681, Loss 0.3789213299751282\n",
      "[Training Epoch 0] Batch 682, Loss 0.37976765632629395\n",
      "[Training Epoch 0] Batch 683, Loss 0.3535540699958801\n",
      "[Training Epoch 0] Batch 684, Loss 0.3931727409362793\n",
      "[Training Epoch 0] Batch 685, Loss 0.37821274995803833\n",
      "[Training Epoch 0] Batch 686, Loss 0.37708795070648193\n",
      "[Training Epoch 0] Batch 687, Loss 0.4109240770339966\n",
      "[Training Epoch 0] Batch 688, Loss 0.35793519020080566\n",
      "[Training Epoch 0] Batch 689, Loss 0.3420500159263611\n",
      "[Training Epoch 0] Batch 690, Loss 0.3476327061653137\n",
      "[Training Epoch 0] Batch 691, Loss 0.3672926425933838\n",
      "[Training Epoch 0] Batch 692, Loss 0.35039108991622925\n",
      "[Training Epoch 0] Batch 693, Loss 0.367624968290329\n",
      "[Training Epoch 0] Batch 694, Loss 0.3735730051994324\n",
      "[Training Epoch 0] Batch 695, Loss 0.38373273611068726\n",
      "[Training Epoch 0] Batch 696, Loss 0.3679945468902588\n",
      "[Training Epoch 0] Batch 697, Loss 0.3632160425186157\n",
      "[Training Epoch 0] Batch 698, Loss 0.36517444252967834\n",
      "[Training Epoch 0] Batch 699, Loss 0.3730838894844055\n",
      "[Training Epoch 0] Batch 700, Loss 0.36765462160110474\n",
      "[Training Epoch 0] Batch 701, Loss 0.37129974365234375\n",
      "[Training Epoch 0] Batch 702, Loss 0.367226243019104\n",
      "[Training Epoch 0] Batch 703, Loss 0.3372676372528076\n",
      "[Training Epoch 0] Batch 704, Loss 0.36970508098602295\n",
      "[Training Epoch 0] Batch 705, Loss 0.4059516191482544\n",
      "[Training Epoch 0] Batch 706, Loss 0.34989649057388306\n",
      "[Training Epoch 0] Batch 707, Loss 0.372067928314209\n",
      "[Training Epoch 0] Batch 708, Loss 0.3756850063800812\n",
      "[Training Epoch 0] Batch 709, Loss 0.34517455101013184\n",
      "[Training Epoch 0] Batch 710, Loss 0.3442607820034027\n",
      "[Training Epoch 0] Batch 711, Loss 0.3690880537033081\n",
      "[Training Epoch 0] Batch 712, Loss 0.36823201179504395\n",
      "[Training Epoch 0] Batch 713, Loss 0.3683333694934845\n",
      "[Training Epoch 0] Batch 714, Loss 0.3705442547798157\n",
      "[Training Epoch 0] Batch 715, Loss 0.34911006689071655\n",
      "[Training Epoch 0] Batch 716, Loss 0.361171692609787\n",
      "[Training Epoch 0] Batch 717, Loss 0.3495646119117737\n",
      "[Training Epoch 0] Batch 718, Loss 0.36422795057296753\n",
      "[Training Epoch 0] Batch 719, Loss 0.3556414842605591\n",
      "[Training Epoch 0] Batch 720, Loss 0.37314996123313904\n",
      "[Training Epoch 0] Batch 721, Loss 0.3730444312095642\n",
      "[Training Epoch 0] Batch 722, Loss 0.37351667881011963\n",
      "[Training Epoch 0] Batch 723, Loss 0.34306955337524414\n",
      "[Training Epoch 0] Batch 724, Loss 0.38168665766716003\n",
      "[Training Epoch 0] Batch 725, Loss 0.37738466262817383\n",
      "[Training Epoch 0] Batch 726, Loss 0.3785550594329834\n",
      "[Training Epoch 0] Batch 727, Loss 0.3627662658691406\n",
      "[Training Epoch 0] Batch 728, Loss 0.40874892473220825\n",
      "[Training Epoch 0] Batch 729, Loss 0.3719301223754883\n",
      "[Training Epoch 0] Batch 730, Loss 0.3625996708869934\n",
      "[Training Epoch 0] Batch 731, Loss 0.3603345453739166\n",
      "[Training Epoch 0] Batch 732, Loss 0.3574123978614807\n",
      "[Training Epoch 0] Batch 733, Loss 0.3608882427215576\n",
      "[Training Epoch 0] Batch 734, Loss 0.3424804210662842\n",
      "[Training Epoch 0] Batch 735, Loss 0.34557437896728516\n",
      "[Training Epoch 0] Batch 736, Loss 0.3537096679210663\n",
      "[Training Epoch 0] Batch 737, Loss 0.3785632252693176\n",
      "[Training Epoch 0] Batch 738, Loss 0.36583027243614197\n",
      "[Training Epoch 0] Batch 739, Loss 0.36204007267951965\n",
      "[Training Epoch 0] Batch 740, Loss 0.34517380595207214\n",
      "[Training Epoch 0] Batch 741, Loss 0.4017612040042877\n",
      "[Training Epoch 0] Batch 742, Loss 0.33722853660583496\n",
      "[Training Epoch 0] Batch 743, Loss 0.36267024278640747\n",
      "[Training Epoch 0] Batch 744, Loss 0.36790525913238525\n",
      "[Training Epoch 0] Batch 745, Loss 0.36639392375946045\n",
      "[Training Epoch 0] Batch 746, Loss 0.3548375964164734\n",
      "[Training Epoch 0] Batch 747, Loss 0.3604295253753662\n",
      "[Training Epoch 0] Batch 748, Loss 0.3868541121482849\n",
      "[Training Epoch 0] Batch 749, Loss 0.38993409276008606\n",
      "[Training Epoch 0] Batch 750, Loss 0.33934152126312256\n",
      "[Training Epoch 0] Batch 751, Loss 0.35182297229766846\n",
      "[Training Epoch 0] Batch 752, Loss 0.3444957137107849\n",
      "[Training Epoch 0] Batch 753, Loss 0.37953388690948486\n",
      "[Training Epoch 0] Batch 754, Loss 0.3903110921382904\n",
      "[Training Epoch 0] Batch 755, Loss 0.34531840682029724\n",
      "[Training Epoch 0] Batch 756, Loss 0.38774722814559937\n",
      "[Training Epoch 0] Batch 757, Loss 0.3732920289039612\n",
      "[Training Epoch 0] Batch 758, Loss 0.39370378851890564\n",
      "[Training Epoch 0] Batch 759, Loss 0.3620968461036682\n",
      "[Training Epoch 0] Batch 760, Loss 0.3656931519508362\n",
      "[Training Epoch 0] Batch 761, Loss 0.37929415702819824\n",
      "[Training Epoch 0] Batch 762, Loss 0.36163967847824097\n",
      "[Training Epoch 0] Batch 763, Loss 0.3877319097518921\n",
      "[Training Epoch 0] Batch 764, Loss 0.3566553294658661\n",
      "[Training Epoch 0] Batch 765, Loss 0.36478543281555176\n",
      "[Training Epoch 0] Batch 766, Loss 0.3792787194252014\n",
      "[Training Epoch 0] Batch 767, Loss 0.38891708850860596\n",
      "[Training Epoch 0] Batch 768, Loss 0.38941001892089844\n",
      "[Training Epoch 0] Batch 769, Loss 0.3461657762527466\n",
      "[Training Epoch 0] Batch 770, Loss 0.35310739278793335\n",
      "[Training Epoch 0] Batch 771, Loss 0.3408876359462738\n",
      "[Training Epoch 0] Batch 772, Loss 0.35437196493148804\n",
      "[Training Epoch 0] Batch 773, Loss 0.3643779754638672\n",
      "[Training Epoch 0] Batch 774, Loss 0.39054161310195923\n",
      "[Training Epoch 0] Batch 775, Loss 0.3601803183555603\n",
      "[Training Epoch 0] Batch 776, Loss 0.3650089502334595\n",
      "[Training Epoch 0] Batch 777, Loss 0.3463488817214966\n",
      "[Training Epoch 0] Batch 778, Loss 0.37411946058273315\n",
      "[Training Epoch 0] Batch 779, Loss 0.3821559250354767\n",
      "[Training Epoch 0] Batch 780, Loss 0.3686067461967468\n",
      "[Training Epoch 0] Batch 781, Loss 0.38244619965553284\n",
      "[Training Epoch 0] Batch 782, Loss 0.36070728302001953\n",
      "[Training Epoch 0] Batch 783, Loss 0.3695092797279358\n",
      "[Training Epoch 0] Batch 784, Loss 0.37044793367385864\n",
      "[Training Epoch 0] Batch 785, Loss 0.3606671690940857\n",
      "[Training Epoch 0] Batch 786, Loss 0.36781325936317444\n",
      "[Training Epoch 0] Batch 787, Loss 0.3464067280292511\n",
      "[Training Epoch 0] Batch 788, Loss 0.36109447479248047\n",
      "[Training Epoch 0] Batch 789, Loss 0.34381628036499023\n",
      "[Training Epoch 0] Batch 790, Loss 0.36443012952804565\n",
      "[Training Epoch 0] Batch 791, Loss 0.35129833221435547\n",
      "[Training Epoch 0] Batch 792, Loss 0.3754976689815521\n",
      "[Training Epoch 0] Batch 793, Loss 0.3601861596107483\n",
      "[Training Epoch 0] Batch 794, Loss 0.3531840741634369\n",
      "[Training Epoch 0] Batch 795, Loss 0.37710481882095337\n",
      "[Training Epoch 0] Batch 796, Loss 0.33913546800613403\n",
      "[Training Epoch 0] Batch 797, Loss 0.3418586850166321\n",
      "[Training Epoch 0] Batch 798, Loss 0.3452337980270386\n",
      "[Training Epoch 0] Batch 799, Loss 0.37226977944374084\n",
      "[Training Epoch 0] Batch 800, Loss 0.3700323700904846\n",
      "[Training Epoch 0] Batch 801, Loss 0.35678356885910034\n",
      "[Training Epoch 0] Batch 802, Loss 0.3575722575187683\n",
      "[Training Epoch 0] Batch 803, Loss 0.38206946849823\n",
      "[Training Epoch 0] Batch 804, Loss 0.3454775810241699\n",
      "[Training Epoch 0] Batch 805, Loss 0.3603268265724182\n",
      "[Training Epoch 0] Batch 806, Loss 0.35299181938171387\n",
      "[Training Epoch 0] Batch 807, Loss 0.37315472960472107\n",
      "[Training Epoch 0] Batch 808, Loss 0.3645651638507843\n",
      "[Training Epoch 0] Batch 809, Loss 0.34793126583099365\n",
      "[Training Epoch 0] Batch 810, Loss 0.3338397741317749\n",
      "[Training Epoch 0] Batch 811, Loss 0.37716251611709595\n",
      "[Training Epoch 0] Batch 812, Loss 0.37111812829971313\n",
      "[Training Epoch 0] Batch 813, Loss 0.3752117156982422\n",
      "[Training Epoch 0] Batch 814, Loss 0.3547753393650055\n",
      "[Training Epoch 0] Batch 815, Loss 0.32682251930236816\n",
      "[Training Epoch 0] Batch 816, Loss 0.38717782497406006\n",
      "[Training Epoch 0] Batch 817, Loss 0.337850421667099\n",
      "[Training Epoch 0] Batch 818, Loss 0.3845272958278656\n",
      "[Training Epoch 0] Batch 819, Loss 0.3642706871032715\n",
      "[Training Epoch 0] Batch 820, Loss 0.35223373770713806\n",
      "[Training Epoch 0] Batch 821, Loss 0.33815377950668335\n",
      "[Training Epoch 0] Batch 822, Loss 0.3433091938495636\n",
      "[Training Epoch 0] Batch 823, Loss 0.35070371627807617\n",
      "[Training Epoch 0] Batch 824, Loss 0.3538479804992676\n",
      "[Training Epoch 0] Batch 825, Loss 0.36863163113594055\n",
      "[Training Epoch 0] Batch 826, Loss 0.3667799234390259\n",
      "[Training Epoch 0] Batch 827, Loss 0.3632914125919342\n",
      "[Training Epoch 0] Batch 828, Loss 0.33859968185424805\n",
      "[Training Epoch 0] Batch 829, Loss 0.3652966320514679\n",
      "[Training Epoch 0] Batch 830, Loss 0.4101399779319763\n",
      "[Training Epoch 0] Batch 831, Loss 0.37518781423568726\n",
      "[Training Epoch 0] Batch 832, Loss 0.34610581398010254\n",
      "[Training Epoch 0] Batch 833, Loss 0.36659038066864014\n",
      "[Training Epoch 0] Batch 834, Loss 0.3840809464454651\n",
      "[Training Epoch 0] Batch 835, Loss 0.339824378490448\n",
      "[Training Epoch 0] Batch 836, Loss 0.3564791679382324\n",
      "[Training Epoch 0] Batch 837, Loss 0.38161903619766235\n",
      "[Training Epoch 0] Batch 838, Loss 0.3433055281639099\n",
      "[Training Epoch 0] Batch 839, Loss 0.3430573344230652\n",
      "[Training Epoch 0] Batch 840, Loss 0.3420410752296448\n",
      "[Training Epoch 0] Batch 841, Loss 0.3472924828529358\n",
      "[Training Epoch 0] Batch 842, Loss 0.37051501870155334\n",
      "[Training Epoch 0] Batch 843, Loss 0.3333497941493988\n",
      "[Training Epoch 0] Batch 844, Loss 0.358257919549942\n",
      "[Training Epoch 0] Batch 845, Loss 0.34802281856536865\n",
      "[Training Epoch 0] Batch 846, Loss 0.38385632634162903\n",
      "[Training Epoch 0] Batch 847, Loss 0.37234318256378174\n",
      "[Training Epoch 0] Batch 848, Loss 0.36664867401123047\n",
      "[Training Epoch 0] Batch 849, Loss 0.35049062967300415\n",
      "[Training Epoch 0] Batch 850, Loss 0.3968713879585266\n",
      "[Training Epoch 0] Batch 851, Loss 0.3525075912475586\n",
      "[Training Epoch 0] Batch 852, Loss 0.37254974246025085\n",
      "[Training Epoch 0] Batch 853, Loss 0.36747151613235474\n",
      "[Training Epoch 0] Batch 854, Loss 0.34749001264572144\n",
      "[Training Epoch 0] Batch 855, Loss 0.352103054523468\n",
      "[Training Epoch 0] Batch 856, Loss 0.36670422554016113\n",
      "[Training Epoch 0] Batch 857, Loss 0.3639158308506012\n",
      "[Training Epoch 0] Batch 858, Loss 0.33531612157821655\n",
      "[Training Epoch 0] Batch 859, Loss 0.3663812577724457\n",
      "[Training Epoch 0] Batch 860, Loss 0.34769606590270996\n",
      "[Training Epoch 0] Batch 861, Loss 0.37107551097869873\n",
      "[Training Epoch 0] Batch 862, Loss 0.357749342918396\n",
      "[Training Epoch 0] Batch 863, Loss 0.34701007604599\n",
      "[Training Epoch 0] Batch 864, Loss 0.34537410736083984\n",
      "[Training Epoch 0] Batch 865, Loss 0.38413524627685547\n",
      "[Training Epoch 0] Batch 866, Loss 0.37369033694267273\n",
      "[Training Epoch 0] Batch 867, Loss 0.35201919078826904\n",
      "[Training Epoch 0] Batch 868, Loss 0.3520199656486511\n",
      "[Training Epoch 0] Batch 869, Loss 0.36297130584716797\n",
      "[Training Epoch 0] Batch 870, Loss 0.36550357937812805\n",
      "[Training Epoch 0] Batch 871, Loss 0.38100534677505493\n",
      "[Training Epoch 0] Batch 872, Loss 0.3821263909339905\n",
      "[Training Epoch 0] Batch 873, Loss 0.36149945855140686\n",
      "[Training Epoch 0] Batch 874, Loss 0.35087376832962036\n",
      "[Training Epoch 0] Batch 875, Loss 0.3759475350379944\n",
      "[Training Epoch 0] Batch 876, Loss 0.35246923565864563\n",
      "[Training Epoch 0] Batch 877, Loss 0.3392977714538574\n",
      "[Training Epoch 0] Batch 878, Loss 0.36590704321861267\n",
      "[Training Epoch 0] Batch 879, Loss 0.3437007665634155\n",
      "[Training Epoch 0] Batch 880, Loss 0.37817662954330444\n",
      "[Training Epoch 0] Batch 881, Loss 0.37442874908447266\n",
      "[Training Epoch 0] Batch 882, Loss 0.3562641143798828\n",
      "[Training Epoch 0] Batch 883, Loss 0.3543028235435486\n",
      "[Training Epoch 0] Batch 884, Loss 0.3776206374168396\n",
      "[Training Epoch 0] Batch 885, Loss 0.3879057765007019\n",
      "[Training Epoch 0] Batch 886, Loss 0.36546701192855835\n",
      "[Training Epoch 0] Batch 887, Loss 0.36466366052627563\n",
      "[Training Epoch 0] Batch 888, Loss 0.3292964994907379\n",
      "[Training Epoch 0] Batch 889, Loss 0.38667434453964233\n",
      "[Training Epoch 0] Batch 890, Loss 0.34636223316192627\n",
      "[Training Epoch 0] Batch 891, Loss 0.32729971408843994\n",
      "[Training Epoch 0] Batch 892, Loss 0.3617948293685913\n",
      "[Training Epoch 0] Batch 893, Loss 0.3392755389213562\n",
      "[Training Epoch 0] Batch 894, Loss 0.3593427240848541\n",
      "[Training Epoch 0] Batch 895, Loss 0.37521398067474365\n",
      "[Training Epoch 0] Batch 896, Loss 0.3560219705104828\n",
      "[Training Epoch 0] Batch 897, Loss 0.35648566484451294\n",
      "[Training Epoch 0] Batch 898, Loss 0.36676809191703796\n",
      "[Training Epoch 0] Batch 899, Loss 0.38301074504852295\n",
      "[Training Epoch 0] Batch 900, Loss 0.3811078667640686\n",
      "[Training Epoch 0] Batch 901, Loss 0.3574806749820709\n",
      "[Training Epoch 0] Batch 902, Loss 0.3845115303993225\n",
      "[Training Epoch 0] Batch 903, Loss 0.38782089948654175\n",
      "[Training Epoch 0] Batch 904, Loss 0.34283778071403503\n",
      "[Training Epoch 0] Batch 905, Loss 0.3810886740684509\n",
      "[Training Epoch 0] Batch 906, Loss 0.37077754735946655\n",
      "[Training Epoch 0] Batch 907, Loss 0.3415253758430481\n",
      "[Training Epoch 0] Batch 908, Loss 0.36719807982444763\n",
      "[Training Epoch 0] Batch 909, Loss 0.35037142038345337\n",
      "[Training Epoch 0] Batch 910, Loss 0.34255000948905945\n",
      "[Training Epoch 0] Batch 911, Loss 0.390121191740036\n",
      "[Training Epoch 0] Batch 912, Loss 0.3468469977378845\n",
      "[Training Epoch 0] Batch 913, Loss 0.3451271653175354\n",
      "[Training Epoch 0] Batch 914, Loss 0.3682349920272827\n",
      "[Training Epoch 0] Batch 915, Loss 0.368416965007782\n",
      "[Training Epoch 0] Batch 916, Loss 0.40028560161590576\n",
      "[Training Epoch 0] Batch 917, Loss 0.36734163761138916\n",
      "[Training Epoch 0] Batch 918, Loss 0.3376578390598297\n",
      "[Training Epoch 0] Batch 919, Loss 0.36070311069488525\n",
      "[Training Epoch 0] Batch 920, Loss 0.36077940464019775\n",
      "[Training Epoch 0] Batch 921, Loss 0.3638373613357544\n",
      "[Training Epoch 0] Batch 922, Loss 0.3462870717048645\n",
      "[Training Epoch 0] Batch 923, Loss 0.36910393834114075\n",
      "[Training Epoch 0] Batch 924, Loss 0.3595033884048462\n",
      "[Training Epoch 0] Batch 925, Loss 0.3477301597595215\n",
      "[Training Epoch 0] Batch 926, Loss 0.3623121976852417\n",
      "[Training Epoch 0] Batch 927, Loss 0.37444716691970825\n",
      "[Training Epoch 0] Batch 928, Loss 0.3855564296245575\n",
      "[Training Epoch 0] Batch 929, Loss 0.3697991967201233\n",
      "[Training Epoch 0] Batch 930, Loss 0.35668450593948364\n",
      "[Training Epoch 0] Batch 931, Loss 0.36134403944015503\n",
      "[Training Epoch 0] Batch 932, Loss 0.3457970917224884\n",
      "[Training Epoch 0] Batch 933, Loss 0.381915807723999\n",
      "[Training Epoch 0] Batch 934, Loss 0.3517899215221405\n",
      "[Training Epoch 0] Batch 935, Loss 0.35732728242874146\n",
      "[Training Epoch 0] Batch 936, Loss 0.37398308515548706\n",
      "[Training Epoch 0] Batch 937, Loss 0.37949612736701965\n",
      "[Training Epoch 0] Batch 938, Loss 0.35906505584716797\n",
      "[Training Epoch 0] Batch 939, Loss 0.35061439871788025\n",
      "[Training Epoch 0] Batch 940, Loss 0.365437388420105\n",
      "[Training Epoch 0] Batch 941, Loss 0.35340940952301025\n",
      "[Training Epoch 0] Batch 942, Loss 0.35509467124938965\n",
      "[Training Epoch 0] Batch 943, Loss 0.3556126654148102\n",
      "[Training Epoch 0] Batch 944, Loss 0.3501408100128174\n",
      "[Training Epoch 0] Batch 945, Loss 0.33757829666137695\n",
      "[Training Epoch 0] Batch 946, Loss 0.39621323347091675\n",
      "[Training Epoch 0] Batch 947, Loss 0.36891403794288635\n",
      "[Training Epoch 0] Batch 948, Loss 0.35059234499931335\n",
      "[Training Epoch 0] Batch 949, Loss 0.35305821895599365\n",
      "[Training Epoch 0] Batch 950, Loss 0.3714776337146759\n",
      "[Training Epoch 0] Batch 951, Loss 0.36145204305648804\n",
      "[Training Epoch 0] Batch 952, Loss 0.3946523368358612\n",
      "[Training Epoch 0] Batch 953, Loss 0.367870032787323\n",
      "[Training Epoch 0] Batch 954, Loss 0.34699830412864685\n",
      "[Training Epoch 0] Batch 955, Loss 0.3798471689224243\n",
      "[Training Epoch 0] Batch 956, Loss 0.36886972188949585\n",
      "[Training Epoch 0] Batch 957, Loss 0.3557004928588867\n",
      "[Training Epoch 0] Batch 958, Loss 0.34597063064575195\n",
      "[Training Epoch 0] Batch 959, Loss 0.36761265993118286\n",
      "[Training Epoch 0] Batch 960, Loss 0.3880213797092438\n",
      "[Training Epoch 0] Batch 961, Loss 0.3308711647987366\n",
      "[Training Epoch 0] Batch 962, Loss 0.35171592235565186\n",
      "[Training Epoch 0] Batch 963, Loss 0.35444533824920654\n",
      "[Training Epoch 0] Batch 964, Loss 0.3715403079986572\n",
      "[Training Epoch 0] Batch 965, Loss 0.33440855145454407\n",
      "[Training Epoch 0] Batch 966, Loss 0.34593668580055237\n",
      "[Training Epoch 0] Batch 967, Loss 0.3673169016838074\n",
      "[Training Epoch 0] Batch 968, Loss 0.3292006552219391\n",
      "[Training Epoch 0] Batch 969, Loss 0.3774467408657074\n",
      "[Training Epoch 0] Batch 970, Loss 0.3263756036758423\n",
      "[Training Epoch 0] Batch 971, Loss 0.34558865427970886\n",
      "[Training Epoch 0] Batch 972, Loss 0.36870670318603516\n",
      "[Training Epoch 0] Batch 973, Loss 0.3775695562362671\n",
      "[Training Epoch 0] Batch 974, Loss 0.35450515151023865\n",
      "[Training Epoch 0] Batch 975, Loss 0.3584842085838318\n",
      "[Training Epoch 0] Batch 976, Loss 0.349814236164093\n",
      "[Training Epoch 0] Batch 977, Loss 0.3552651107311249\n",
      "[Training Epoch 0] Batch 978, Loss 0.3888712525367737\n",
      "[Training Epoch 0] Batch 979, Loss 0.3245754837989807\n",
      "[Training Epoch 0] Batch 980, Loss 0.38265669345855713\n",
      "[Training Epoch 0] Batch 981, Loss 0.3735004961490631\n",
      "[Training Epoch 0] Batch 982, Loss 0.34364068508148193\n",
      "[Training Epoch 0] Batch 983, Loss 0.35271644592285156\n",
      "[Training Epoch 0] Batch 984, Loss 0.36205777525901794\n",
      "[Training Epoch 0] Batch 985, Loss 0.35817334055900574\n",
      "[Training Epoch 0] Batch 986, Loss 0.35972335934638977\n",
      "[Training Epoch 0] Batch 987, Loss 0.3520914316177368\n",
      "[Training Epoch 0] Batch 988, Loss 0.34036558866500854\n",
      "[Training Epoch 0] Batch 989, Loss 0.3602336645126343\n",
      "[Training Epoch 0] Batch 990, Loss 0.3722268342971802\n",
      "[Training Epoch 0] Batch 991, Loss 0.35715949535369873\n",
      "[Training Epoch 0] Batch 992, Loss 0.3733408749103546\n",
      "[Training Epoch 0] Batch 993, Loss 0.3331446647644043\n",
      "[Training Epoch 0] Batch 994, Loss 0.3338807225227356\n",
      "[Training Epoch 0] Batch 995, Loss 0.34507620334625244\n",
      "[Training Epoch 0] Batch 996, Loss 0.36988401412963867\n",
      "[Training Epoch 0] Batch 997, Loss 0.35630667209625244\n",
      "[Training Epoch 0] Batch 998, Loss 0.3358681797981262\n",
      "[Training Epoch 0] Batch 999, Loss 0.33240973949432373\n",
      "[Training Epoch 0] Batch 1000, Loss 0.37663447856903076\n",
      "[Training Epoch 0] Batch 1001, Loss 0.3593006432056427\n",
      "[Training Epoch 0] Batch 1002, Loss 0.37567922472953796\n",
      "[Training Epoch 0] Batch 1003, Loss 0.3904523551464081\n",
      "[Training Epoch 0] Batch 1004, Loss 0.36460351943969727\n",
      "[Training Epoch 0] Batch 1005, Loss 0.3844653367996216\n",
      "[Training Epoch 0] Batch 1006, Loss 0.40090489387512207\n",
      "[Training Epoch 0] Batch 1007, Loss 0.3261655569076538\n",
      "[Training Epoch 0] Batch 1008, Loss 0.33827564120292664\n",
      "[Training Epoch 0] Batch 1009, Loss 0.3823922872543335\n",
      "[Training Epoch 0] Batch 1010, Loss 0.39450564980506897\n",
      "[Training Epoch 0] Batch 1011, Loss 0.3784581124782562\n",
      "[Training Epoch 0] Batch 1012, Loss 0.3458537459373474\n",
      "[Training Epoch 0] Batch 1013, Loss 0.34318020939826965\n",
      "[Training Epoch 0] Batch 1014, Loss 0.35135966539382935\n",
      "[Training Epoch 0] Batch 1015, Loss 0.3522080183029175\n",
      "[Training Epoch 0] Batch 1016, Loss 0.3639521598815918\n",
      "[Training Epoch 0] Batch 1017, Loss 0.36559930443763733\n",
      "[Training Epoch 0] Batch 1018, Loss 0.35933464765548706\n",
      "[Training Epoch 0] Batch 1019, Loss 0.360568642616272\n",
      "[Training Epoch 0] Batch 1020, Loss 0.34140145778656006\n",
      "[Training Epoch 0] Batch 1021, Loss 0.3563792407512665\n",
      "[Training Epoch 0] Batch 1022, Loss 0.35123366117477417\n",
      "[Training Epoch 0] Batch 1023, Loss 0.3588670492172241\n",
      "[Training Epoch 0] Batch 1024, Loss 0.38766664266586304\n",
      "[Training Epoch 0] Batch 1025, Loss 0.3603062033653259\n",
      "[Training Epoch 0] Batch 1026, Loss 0.3621487021446228\n",
      "[Training Epoch 0] Batch 1027, Loss 0.3419381380081177\n",
      "[Training Epoch 0] Batch 1028, Loss 0.36830416321754456\n",
      "[Training Epoch 0] Batch 1029, Loss 0.36828288435935974\n",
      "[Training Epoch 0] Batch 1030, Loss 0.37096258997917175\n",
      "[Training Epoch 0] Batch 1031, Loss 0.34576985239982605\n",
      "[Training Epoch 0] Batch 1032, Loss 0.36149704456329346\n",
      "[Training Epoch 0] Batch 1033, Loss 0.3745855689048767\n",
      "[Training Epoch 0] Batch 1034, Loss 0.36913928389549255\n",
      "[Training Epoch 0] Batch 1035, Loss 0.3420221507549286\n",
      "[Training Epoch 0] Batch 1036, Loss 0.3757438063621521\n",
      "[Training Epoch 0] Batch 1037, Loss 0.32468289136886597\n",
      "[Training Epoch 0] Batch 1038, Loss 0.3513021469116211\n",
      "[Training Epoch 0] Batch 1039, Loss 0.36475828289985657\n",
      "[Training Epoch 0] Batch 1040, Loss 0.3721495568752289\n",
      "[Training Epoch 0] Batch 1041, Loss 0.33713293075561523\n",
      "[Training Epoch 0] Batch 1042, Loss 0.3340771794319153\n",
      "[Training Epoch 0] Batch 1043, Loss 0.37134629487991333\n",
      "[Training Epoch 0] Batch 1044, Loss 0.3806474804878235\n",
      "[Training Epoch 0] Batch 1045, Loss 0.34990614652633667\n",
      "[Training Epoch 0] Batch 1046, Loss 0.3520415425300598\n",
      "[Training Epoch 0] Batch 1047, Loss 0.34881582856178284\n",
      "[Training Epoch 0] Batch 1048, Loss 0.35990142822265625\n",
      "[Training Epoch 0] Batch 1049, Loss 0.36177709698677063\n",
      "[Training Epoch 0] Batch 1050, Loss 0.3613359332084656\n",
      "[Training Epoch 0] Batch 1051, Loss 0.3843122720718384\n",
      "[Training Epoch 0] Batch 1052, Loss 0.3581079840660095\n",
      "[Training Epoch 0] Batch 1053, Loss 0.3557144105434418\n",
      "[Training Epoch 0] Batch 1054, Loss 0.34378883242607117\n",
      "[Training Epoch 0] Batch 1055, Loss 0.3646230101585388\n",
      "[Training Epoch 0] Batch 1056, Loss 0.3519209325313568\n",
      "[Training Epoch 0] Batch 1057, Loss 0.3616383969783783\n",
      "[Training Epoch 0] Batch 1058, Loss 0.3495175838470459\n",
      "[Training Epoch 0] Batch 1059, Loss 0.3333064913749695\n",
      "[Training Epoch 0] Batch 1060, Loss 0.34785521030426025\n",
      "[Training Epoch 0] Batch 1061, Loss 0.32496413588523865\n",
      "[Training Epoch 0] Batch 1062, Loss 0.371028333902359\n",
      "[Training Epoch 0] Batch 1063, Loss 0.33341196179389954\n",
      "[Training Epoch 0] Batch 1064, Loss 0.35488685965538025\n",
      "[Training Epoch 0] Batch 1065, Loss 0.3487997055053711\n",
      "[Training Epoch 0] Batch 1066, Loss 0.34327858686447144\n",
      "[Training Epoch 0] Batch 1067, Loss 0.341277539730072\n",
      "[Training Epoch 0] Batch 1068, Loss 0.3659483790397644\n",
      "[Training Epoch 0] Batch 1069, Loss 0.32124975323677063\n",
      "[Training Epoch 0] Batch 1070, Loss 0.3731865882873535\n",
      "[Training Epoch 0] Batch 1071, Loss 0.3594897985458374\n",
      "[Training Epoch 0] Batch 1072, Loss 0.35121166706085205\n",
      "[Training Epoch 0] Batch 1073, Loss 0.31721264123916626\n",
      "[Training Epoch 0] Batch 1074, Loss 0.34629619121551514\n",
      "[Training Epoch 0] Batch 1075, Loss 0.3901006877422333\n",
      "[Training Epoch 0] Batch 1076, Loss 0.37169384956359863\n",
      "[Training Epoch 0] Batch 1077, Loss 0.38514241576194763\n",
      "[Training Epoch 0] Batch 1078, Loss 0.35787564516067505\n",
      "[Training Epoch 0] Batch 1079, Loss 0.4126959443092346\n",
      "[Training Epoch 0] Batch 1080, Loss 0.37208688259124756\n",
      "[Training Epoch 0] Batch 1081, Loss 0.3832348585128784\n",
      "[Training Epoch 0] Batch 1082, Loss 0.3630504012107849\n",
      "[Training Epoch 0] Batch 1083, Loss 0.33047688007354736\n",
      "[Training Epoch 0] Batch 1084, Loss 0.3486948609352112\n",
      "[Training Epoch 0] Batch 1085, Loss 0.3621061444282532\n",
      "[Training Epoch 0] Batch 1086, Loss 0.362283319234848\n",
      "[Training Epoch 0] Batch 1087, Loss 0.3450219929218292\n",
      "[Training Epoch 0] Batch 1088, Loss 0.3586274981498718\n",
      "[Training Epoch 0] Batch 1089, Loss 0.3458407521247864\n",
      "[Training Epoch 0] Batch 1090, Loss 0.34770265221595764\n",
      "[Training Epoch 0] Batch 1091, Loss 0.3834228515625\n",
      "[Training Epoch 0] Batch 1092, Loss 0.3461286425590515\n",
      "[Training Epoch 0] Batch 1093, Loss 0.35701847076416016\n",
      "[Training Epoch 0] Batch 1094, Loss 0.3480207622051239\n",
      "[Training Epoch 0] Batch 1095, Loss 0.37838220596313477\n",
      "[Training Epoch 0] Batch 1096, Loss 0.3455201983451843\n",
      "[Training Epoch 0] Batch 1097, Loss 0.32443326711654663\n",
      "[Training Epoch 0] Batch 1098, Loss 0.3449946641921997\n",
      "[Training Epoch 0] Batch 1099, Loss 0.3696102201938629\n",
      "[Training Epoch 0] Batch 1100, Loss 0.3453633785247803\n",
      "[Training Epoch 0] Batch 1101, Loss 0.35783594846725464\n",
      "[Training Epoch 0] Batch 1102, Loss 0.36787497997283936\n",
      "[Training Epoch 0] Batch 1103, Loss 0.3595919609069824\n",
      "[Training Epoch 0] Batch 1104, Loss 0.3538481593132019\n",
      "[Training Epoch 0] Batch 1105, Loss 0.36617451906204224\n",
      "[Training Epoch 0] Batch 1106, Loss 0.36978989839553833\n",
      "[Training Epoch 0] Batch 1107, Loss 0.3366888463497162\n",
      "[Training Epoch 0] Batch 1108, Loss 0.3389367461204529\n",
      "[Training Epoch 0] Batch 1109, Loss 0.3462293744087219\n",
      "[Training Epoch 0] Batch 1110, Loss 0.3527895510196686\n",
      "[Training Epoch 0] Batch 1111, Loss 0.3495604693889618\n",
      "[Training Epoch 0] Batch 1112, Loss 0.3215990960597992\n",
      "[Training Epoch 0] Batch 1113, Loss 0.35922902822494507\n",
      "[Training Epoch 0] Batch 1114, Loss 0.3310542404651642\n",
      "[Training Epoch 0] Batch 1115, Loss 0.324802428483963\n",
      "[Training Epoch 0] Batch 1116, Loss 0.34595948457717896\n",
      "[Training Epoch 0] Batch 1117, Loss 0.35041412711143494\n",
      "[Training Epoch 0] Batch 1118, Loss 0.3660937249660492\n",
      "[Training Epoch 0] Batch 1119, Loss 0.3550722002983093\n",
      "[Training Epoch 0] Batch 1120, Loss 0.31878894567489624\n",
      "[Training Epoch 0] Batch 1121, Loss 0.38381969928741455\n",
      "[Training Epoch 0] Batch 1122, Loss 0.3550432324409485\n",
      "[Training Epoch 0] Batch 1123, Loss 0.35139429569244385\n",
      "[Training Epoch 0] Batch 1124, Loss 0.3742388188838959\n",
      "[Training Epoch 0] Batch 1125, Loss 0.35334551334381104\n",
      "[Training Epoch 0] Batch 1126, Loss 0.3595881462097168\n",
      "[Training Epoch 0] Batch 1127, Loss 0.33586764335632324\n",
      "[Training Epoch 0] Batch 1128, Loss 0.3479582965373993\n",
      "[Training Epoch 0] Batch 1129, Loss 0.3512330651283264\n",
      "[Training Epoch 0] Batch 1130, Loss 0.3395475149154663\n",
      "[Training Epoch 0] Batch 1131, Loss 0.3315061628818512\n",
      "[Training Epoch 0] Batch 1132, Loss 0.3477245569229126\n",
      "[Training Epoch 0] Batch 1133, Loss 0.3744155764579773\n",
      "[Training Epoch 0] Batch 1134, Loss 0.36908528208732605\n",
      "[Training Epoch 0] Batch 1135, Loss 0.36759841442108154\n",
      "[Training Epoch 0] Batch 1136, Loss 0.3520047664642334\n",
      "[Training Epoch 0] Batch 1137, Loss 0.35272443294525146\n",
      "[Training Epoch 0] Batch 1138, Loss 0.34563949704170227\n",
      "[Training Epoch 0] Batch 1139, Loss 0.36899733543395996\n",
      "[Training Epoch 0] Batch 1140, Loss 0.3456103801727295\n",
      "[Training Epoch 0] Batch 1141, Loss 0.32380181550979614\n",
      "[Training Epoch 0] Batch 1142, Loss 0.3502040207386017\n",
      "[Training Epoch 0] Batch 1143, Loss 0.35946691036224365\n",
      "[Training Epoch 0] Batch 1144, Loss 0.34307944774627686\n",
      "[Training Epoch 0] Batch 1145, Loss 0.3396850526332855\n",
      "[Training Epoch 0] Batch 1146, Loss 0.3210410475730896\n",
      "[Training Epoch 0] Batch 1147, Loss 0.36734551191329956\n",
      "[Training Epoch 0] Batch 1148, Loss 0.3799419403076172\n",
      "[Training Epoch 0] Batch 1149, Loss 0.33753103017807007\n",
      "[Training Epoch 0] Batch 1150, Loss 0.3558661937713623\n",
      "[Training Epoch 0] Batch 1151, Loss 0.33929288387298584\n",
      "[Training Epoch 0] Batch 1152, Loss 0.3482903838157654\n",
      "[Training Epoch 0] Batch 1153, Loss 0.35115939378738403\n",
      "[Training Epoch 0] Batch 1154, Loss 0.3402848243713379\n",
      "[Training Epoch 0] Batch 1155, Loss 0.3491377532482147\n",
      "[Training Epoch 0] Batch 1156, Loss 0.35134971141815186\n",
      "[Training Epoch 0] Batch 1157, Loss 0.3686824142932892\n",
      "[Training Epoch 0] Batch 1158, Loss 0.32351624965667725\n",
      "[Training Epoch 0] Batch 1159, Loss 0.3405008614063263\n",
      "[Training Epoch 0] Batch 1160, Loss 0.3850206136703491\n",
      "[Training Epoch 0] Batch 1161, Loss 0.3549186587333679\n",
      "[Training Epoch 0] Batch 1162, Loss 0.32582977414131165\n",
      "[Training Epoch 0] Batch 1163, Loss 0.31983673572540283\n",
      "[Training Epoch 0] Batch 1164, Loss 0.3572176694869995\n",
      "[Training Epoch 0] Batch 1165, Loss 0.3461045026779175\n",
      "[Training Epoch 0] Batch 1166, Loss 0.3460211753845215\n",
      "[Training Epoch 0] Batch 1167, Loss 0.36715683341026306\n",
      "[Training Epoch 0] Batch 1168, Loss 0.3640259802341461\n",
      "[Training Epoch 0] Batch 1169, Loss 0.3114588260650635\n",
      "[Training Epoch 0] Batch 1170, Loss 0.3624708652496338\n",
      "[Training Epoch 0] Batch 1171, Loss 0.3517894148826599\n",
      "[Training Epoch 0] Batch 1172, Loss 0.3581653833389282\n",
      "[Training Epoch 0] Batch 1173, Loss 0.3636624813079834\n",
      "[Training Epoch 0] Batch 1174, Loss 0.342157244682312\n",
      "[Training Epoch 0] Batch 1175, Loss 0.34643682837486267\n",
      "[Training Epoch 0] Batch 1176, Loss 0.3341541886329651\n",
      "[Training Epoch 0] Batch 1177, Loss 0.33096635341644287\n",
      "[Training Epoch 0] Batch 1178, Loss 0.3354185223579407\n",
      "[Training Epoch 0] Batch 1179, Loss 0.3316822052001953\n",
      "[Training Epoch 0] Batch 1180, Loss 0.37094542384147644\n",
      "[Training Epoch 0] Batch 1181, Loss 0.38049548864364624\n",
      "[Training Epoch 0] Batch 1182, Loss 0.36700892448425293\n",
      "[Training Epoch 0] Batch 1183, Loss 0.3210218846797943\n",
      "[Training Epoch 0] Batch 1184, Loss 0.32047218084335327\n",
      "[Training Epoch 0] Batch 1185, Loss 0.3481724262237549\n",
      "[Training Epoch 0] Batch 1186, Loss 0.3431283235549927\n",
      "[Training Epoch 0] Batch 1187, Loss 0.3395160436630249\n",
      "[Training Epoch 0] Batch 1188, Loss 0.3596498966217041\n",
      "[Training Epoch 0] Batch 1189, Loss 0.33790504932403564\n",
      "[Training Epoch 0] Batch 1190, Loss 0.3424561619758606\n",
      "[Training Epoch 0] Batch 1191, Loss 0.3462996184825897\n",
      "[Training Epoch 0] Batch 1192, Loss 0.3618665933609009\n",
      "[Training Epoch 0] Batch 1193, Loss 0.31900566816329956\n",
      "[Training Epoch 0] Batch 1194, Loss 0.358950138092041\n",
      "[Training Epoch 0] Batch 1195, Loss 0.3464556336402893\n",
      "[Training Epoch 0] Batch 1196, Loss 0.355857253074646\n",
      "[Training Epoch 0] Batch 1197, Loss 0.3699708878993988\n",
      "[Training Epoch 0] Batch 1198, Loss 0.33202865719795227\n",
      "[Training Epoch 0] Batch 1199, Loss 0.3545641601085663\n",
      "[Training Epoch 0] Batch 1200, Loss 0.354401558637619\n",
      "[Training Epoch 0] Batch 1201, Loss 0.3225412368774414\n",
      "[Training Epoch 0] Batch 1202, Loss 0.3346025049686432\n",
      "[Training Epoch 0] Batch 1203, Loss 0.3607114255428314\n",
      "[Training Epoch 0] Batch 1204, Loss 0.31645599007606506\n",
      "[Training Epoch 0] Batch 1205, Loss 0.3509145975112915\n",
      "[Training Epoch 0] Batch 1206, Loss 0.35013332962989807\n",
      "[Training Epoch 0] Batch 1207, Loss 0.341270387172699\n",
      "[Training Epoch 0] Batch 1208, Loss 0.3626266419887543\n",
      "[Training Epoch 0] Batch 1209, Loss 0.3491706848144531\n",
      "[Training Epoch 0] Batch 1210, Loss 0.3352561593055725\n",
      "[Training Epoch 0] Batch 1211, Loss 0.34176182746887207\n",
      "[Training Epoch 0] Batch 1212, Loss 0.34176644682884216\n",
      "[Training Epoch 0] Batch 1213, Loss 0.3346176743507385\n",
      "[Training Epoch 0] Batch 1214, Loss 0.3510506749153137\n",
      "[Training Epoch 0] Batch 1215, Loss 0.3432232439517975\n",
      "[Training Epoch 0] Batch 1216, Loss 0.33944517374038696\n",
      "[Training Epoch 0] Batch 1217, Loss 0.3365812301635742\n",
      "[Training Epoch 0] Batch 1218, Loss 0.33423328399658203\n",
      "[Training Epoch 0] Batch 1219, Loss 0.37281447649002075\n",
      "[Training Epoch 0] Batch 1220, Loss 0.32914847135543823\n",
      "[Training Epoch 0] Batch 1221, Loss 0.35720202326774597\n",
      "[Training Epoch 0] Batch 1222, Loss 0.37651658058166504\n",
      "[Training Epoch 0] Batch 1223, Loss 0.3438701629638672\n",
      "[Training Epoch 0] Batch 1224, Loss 0.36986616253852844\n",
      "[Training Epoch 0] Batch 1225, Loss 0.3415222465991974\n",
      "[Training Epoch 0] Batch 1226, Loss 0.3497360348701477\n",
      "[Training Epoch 0] Batch 1227, Loss 0.3475049138069153\n",
      "[Training Epoch 0] Batch 1228, Loss 0.3303467631340027\n",
      "[Training Epoch 0] Batch 1229, Loss 0.33163270354270935\n",
      "[Training Epoch 0] Batch 1230, Loss 0.3397652506828308\n",
      "[Training Epoch 0] Batch 1231, Loss 0.3452284634113312\n",
      "[Training Epoch 0] Batch 1232, Loss 0.33863458037376404\n",
      "[Training Epoch 0] Batch 1233, Loss 0.34524673223495483\n",
      "[Training Epoch 0] Batch 1234, Loss 0.31907397508621216\n",
      "[Training Epoch 0] Batch 1235, Loss 0.3630520701408386\n",
      "[Training Epoch 0] Batch 1236, Loss 0.3647313117980957\n",
      "[Training Epoch 0] Batch 1237, Loss 0.3538956940174103\n",
      "[Training Epoch 0] Batch 1238, Loss 0.3748515844345093\n",
      "[Training Epoch 0] Batch 1239, Loss 0.358565092086792\n",
      "[Training Epoch 0] Batch 1240, Loss 0.3379078209400177\n",
      "[Training Epoch 0] Batch 1241, Loss 0.38748228549957275\n",
      "[Training Epoch 0] Batch 1242, Loss 0.3482183516025543\n",
      "[Training Epoch 0] Batch 1243, Loss 0.3141564726829529\n",
      "[Training Epoch 0] Batch 1244, Loss 0.3430495262145996\n",
      "[Training Epoch 0] Batch 1245, Loss 0.3380551338195801\n",
      "[Training Epoch 0] Batch 1246, Loss 0.3450447916984558\n",
      "[Training Epoch 0] Batch 1247, Loss 0.3449222147464752\n",
      "[Training Epoch 0] Batch 1248, Loss 0.35765624046325684\n",
      "[Training Epoch 0] Batch 1249, Loss 0.33463847637176514\n",
      "[Training Epoch 0] Batch 1250, Loss 0.34634751081466675\n",
      "[Training Epoch 0] Batch 1251, Loss 0.3502122759819031\n",
      "[Training Epoch 0] Batch 1252, Loss 0.3458670973777771\n",
      "[Training Epoch 0] Batch 1253, Loss 0.38901910185813904\n",
      "[Training Epoch 0] Batch 1254, Loss 0.3490322232246399\n",
      "[Training Epoch 0] Batch 1255, Loss 0.35336750745773315\n",
      "[Training Epoch 0] Batch 1256, Loss 0.3269050717353821\n",
      "[Training Epoch 0] Batch 1257, Loss 0.3393682837486267\n",
      "[Training Epoch 0] Batch 1258, Loss 0.3527236580848694\n",
      "[Training Epoch 0] Batch 1259, Loss 0.34776580333709717\n",
      "[Training Epoch 0] Batch 1260, Loss 0.3573349118232727\n",
      "[Training Epoch 0] Batch 1261, Loss 0.37142446637153625\n",
      "[Training Epoch 0] Batch 1262, Loss 0.3545168936252594\n",
      "[Training Epoch 0] Batch 1263, Loss 0.32964348793029785\n",
      "[Training Epoch 0] Batch 1264, Loss 0.3479821979999542\n",
      "[Training Epoch 0] Batch 1265, Loss 0.3584073483943939\n",
      "[Training Epoch 0] Batch 1266, Loss 0.37073269486427307\n",
      "[Training Epoch 0] Batch 1267, Loss 0.34807664155960083\n",
      "[Training Epoch 0] Batch 1268, Loss 0.3214811086654663\n",
      "[Training Epoch 0] Batch 1269, Loss 0.32917284965515137\n",
      "[Training Epoch 0] Batch 1270, Loss 0.3329331874847412\n",
      "[Training Epoch 0] Batch 1271, Loss 0.3607962131500244\n",
      "[Training Epoch 0] Batch 1272, Loss 0.3684689998626709\n",
      "[Training Epoch 0] Batch 1273, Loss 0.3718208074569702\n",
      "[Training Epoch 0] Batch 1274, Loss 0.34869450330734253\n",
      "[Training Epoch 0] Batch 1275, Loss 0.35310685634613037\n",
      "[Training Epoch 0] Batch 1276, Loss 0.3289533853530884\n",
      "[Training Epoch 0] Batch 1277, Loss 0.3206946849822998\n",
      "[Training Epoch 0] Batch 1278, Loss 0.36749160289764404\n",
      "[Training Epoch 0] Batch 1279, Loss 0.3608856797218323\n",
      "[Training Epoch 0] Batch 1280, Loss 0.3704648017883301\n",
      "[Training Epoch 0] Batch 1281, Loss 0.3552998900413513\n",
      "[Training Epoch 0] Batch 1282, Loss 0.32352688908576965\n",
      "[Training Epoch 0] Batch 1283, Loss 0.34420549869537354\n",
      "[Training Epoch 0] Batch 1284, Loss 0.35469380021095276\n",
      "[Training Epoch 0] Batch 1285, Loss 0.35455888509750366\n",
      "[Training Epoch 0] Batch 1286, Loss 0.359771728515625\n",
      "[Training Epoch 0] Batch 1287, Loss 0.34472525119781494\n",
      "[Training Epoch 0] Batch 1288, Loss 0.353674054145813\n",
      "[Training Epoch 0] Batch 1289, Loss 0.3465197682380676\n",
      "[Training Epoch 0] Batch 1290, Loss 0.316183865070343\n",
      "[Training Epoch 0] Batch 1291, Loss 0.3418983221054077\n",
      "[Training Epoch 0] Batch 1292, Loss 0.3510783910751343\n",
      "[Training Epoch 0] Batch 1293, Loss 0.34047436714172363\n",
      "[Training Epoch 0] Batch 1294, Loss 0.34198975563049316\n",
      "[Training Epoch 0] Batch 1295, Loss 0.3696244955062866\n",
      "[Training Epoch 0] Batch 1296, Loss 0.3217574954032898\n",
      "[Training Epoch 0] Batch 1297, Loss 0.3148455023765564\n",
      "[Training Epoch 0] Batch 1298, Loss 0.35912758111953735\n",
      "[Training Epoch 0] Batch 1299, Loss 0.34170830249786377\n",
      "[Training Epoch 0] Batch 1300, Loss 0.3448101282119751\n",
      "[Training Epoch 0] Batch 1301, Loss 0.3414266109466553\n",
      "[Training Epoch 0] Batch 1302, Loss 0.3468480110168457\n",
      "[Training Epoch 0] Batch 1303, Loss 0.33536529541015625\n",
      "[Training Epoch 0] Batch 1304, Loss 0.3521934151649475\n",
      "[Training Epoch 0] Batch 1305, Loss 0.3598233163356781\n",
      "[Training Epoch 0] Batch 1306, Loss 0.3305685520172119\n",
      "[Training Epoch 0] Batch 1307, Loss 0.33454516530036926\n",
      "[Training Epoch 0] Batch 1308, Loss 0.3579481244087219\n",
      "[Training Epoch 0] Batch 1309, Loss 0.3450433015823364\n",
      "[Training Epoch 0] Batch 1310, Loss 0.34705859422683716\n",
      "[Training Epoch 0] Batch 1311, Loss 0.32695484161376953\n",
      "[Training Epoch 0] Batch 1312, Loss 0.3588274121284485\n",
      "[Training Epoch 0] Batch 1313, Loss 0.36016303300857544\n",
      "[Training Epoch 0] Batch 1314, Loss 0.39028605818748474\n",
      "[Training Epoch 0] Batch 1315, Loss 0.3236766457557678\n",
      "[Training Epoch 0] Batch 1316, Loss 0.3555293083190918\n",
      "[Training Epoch 0] Batch 1317, Loss 0.348180890083313\n",
      "[Training Epoch 0] Batch 1318, Loss 0.33392274379730225\n",
      "[Training Epoch 0] Batch 1319, Loss 0.38187482953071594\n",
      "[Training Epoch 0] Batch 1320, Loss 0.34067752957344055\n",
      "[Training Epoch 0] Batch 1321, Loss 0.33760446310043335\n",
      "[Training Epoch 0] Batch 1322, Loss 0.3208562135696411\n",
      "[Training Epoch 0] Batch 1323, Loss 0.3344205617904663\n",
      "[Training Epoch 0] Batch 1324, Loss 0.34857600927352905\n",
      "[Training Epoch 0] Batch 1325, Loss 0.3513490855693817\n",
      "[Training Epoch 0] Batch 1326, Loss 0.3619350790977478\n",
      "[Training Epoch 0] Batch 1327, Loss 0.3409663438796997\n",
      "[Training Epoch 0] Batch 1328, Loss 0.38278618454933167\n",
      "[Training Epoch 0] Batch 1329, Loss 0.356783390045166\n",
      "[Training Epoch 0] Batch 1330, Loss 0.35231080651283264\n",
      "[Training Epoch 0] Batch 1331, Loss 0.3254450261592865\n",
      "[Training Epoch 0] Batch 1332, Loss 0.3786221146583557\n",
      "[Training Epoch 0] Batch 1333, Loss 0.37469708919525146\n",
      "[Training Epoch 0] Batch 1334, Loss 0.34349435567855835\n",
      "[Training Epoch 0] Batch 1335, Loss 0.35001474618911743\n",
      "[Training Epoch 0] Batch 1336, Loss 0.3422219455242157\n",
      "[Training Epoch 0] Batch 1337, Loss 0.33954066038131714\n",
      "[Training Epoch 0] Batch 1338, Loss 0.35693424940109253\n",
      "[Training Epoch 0] Batch 1339, Loss 0.3474012017250061\n",
      "[Training Epoch 0] Batch 1340, Loss 0.32958024740219116\n",
      "[Training Epoch 0] Batch 1341, Loss 0.3524918556213379\n",
      "[Training Epoch 0] Batch 1342, Loss 0.375520259141922\n",
      "[Training Epoch 0] Batch 1343, Loss 0.34418952465057373\n",
      "[Training Epoch 0] Batch 1344, Loss 0.31165146827697754\n",
      "[Training Epoch 0] Batch 1345, Loss 0.32916274666786194\n",
      "[Training Epoch 0] Batch 1346, Loss 0.3715211749076843\n",
      "[Training Epoch 0] Batch 1347, Loss 0.35446006059646606\n",
      "[Training Epoch 0] Batch 1348, Loss 0.34718263149261475\n",
      "[Training Epoch 0] Batch 1349, Loss 0.3412901759147644\n",
      "[Training Epoch 0] Batch 1350, Loss 0.37457919120788574\n",
      "[Training Epoch 0] Batch 1351, Loss 0.33907806873321533\n",
      "[Training Epoch 0] Batch 1352, Loss 0.3477693796157837\n",
      "[Training Epoch 0] Batch 1353, Loss 0.37083369493484497\n",
      "[Training Epoch 0] Batch 1354, Loss 0.3527042865753174\n",
      "[Training Epoch 0] Batch 1355, Loss 0.33113980293273926\n",
      "[Training Epoch 0] Batch 1356, Loss 0.3259854018688202\n",
      "[Training Epoch 0] Batch 1357, Loss 0.3351325988769531\n",
      "[Training Epoch 0] Batch 1358, Loss 0.3161046802997589\n",
      "[Training Epoch 0] Batch 1359, Loss 0.336579829454422\n",
      "[Training Epoch 0] Batch 1360, Loss 0.35776734352111816\n",
      "[Training Epoch 0] Batch 1361, Loss 0.32282787561416626\n",
      "[Training Epoch 0] Batch 1362, Loss 0.31935232877731323\n",
      "[Training Epoch 0] Batch 1363, Loss 0.37264806032180786\n",
      "[Training Epoch 0] Batch 1364, Loss 0.39059197902679443\n",
      "[Training Epoch 0] Batch 1365, Loss 0.3624436855316162\n",
      "[Training Epoch 0] Batch 1366, Loss 0.352603018283844\n",
      "[Training Epoch 0] Batch 1367, Loss 0.3267688751220703\n",
      "[Training Epoch 0] Batch 1368, Loss 0.3255320191383362\n",
      "[Training Epoch 0] Batch 1369, Loss 0.34739425778388977\n",
      "[Training Epoch 0] Batch 1370, Loss 0.3306952118873596\n",
      "[Training Epoch 0] Batch 1371, Loss 0.3637259602546692\n",
      "[Training Epoch 0] Batch 1372, Loss 0.3419795036315918\n",
      "[Training Epoch 0] Batch 1373, Loss 0.3458177447319031\n",
      "[Training Epoch 0] Batch 1374, Loss 0.31661123037338257\n",
      "[Training Epoch 0] Batch 1375, Loss 0.3271007239818573\n",
      "[Training Epoch 0] Batch 1376, Loss 0.35502463579177856\n",
      "[Training Epoch 0] Batch 1377, Loss 0.3137989044189453\n",
      "[Training Epoch 0] Batch 1378, Loss 0.326123982667923\n",
      "[Training Epoch 0] Batch 1379, Loss 0.32120174169540405\n",
      "[Training Epoch 0] Batch 1380, Loss 0.31675225496292114\n",
      "[Training Epoch 0] Batch 1381, Loss 0.3710341453552246\n",
      "[Training Epoch 0] Batch 1382, Loss 0.3481157124042511\n",
      "[Training Epoch 0] Batch 1383, Loss 0.3438788950443268\n",
      "[Training Epoch 0] Batch 1384, Loss 0.37159448862075806\n",
      "[Training Epoch 0] Batch 1385, Loss 0.34479472041130066\n",
      "[Training Epoch 0] Batch 1386, Loss 0.36455902457237244\n",
      "[Training Epoch 0] Batch 1387, Loss 0.3418446183204651\n",
      "[Training Epoch 0] Batch 1388, Loss 0.3604133129119873\n",
      "[Training Epoch 0] Batch 1389, Loss 0.3322305977344513\n",
      "[Training Epoch 0] Batch 1390, Loss 0.3473297953605652\n",
      "[Training Epoch 0] Batch 1391, Loss 0.36684679985046387\n",
      "[Training Epoch 0] Batch 1392, Loss 0.3492822051048279\n",
      "[Training Epoch 0] Batch 1393, Loss 0.3311360478401184\n",
      "[Training Epoch 0] Batch 1394, Loss 0.3274577260017395\n",
      "[Training Epoch 0] Batch 1395, Loss 0.33376753330230713\n",
      "[Training Epoch 0] Batch 1396, Loss 0.37336575984954834\n",
      "[Training Epoch 0] Batch 1397, Loss 0.37056201696395874\n",
      "[Training Epoch 0] Batch 1398, Loss 0.3496568202972412\n",
      "[Training Epoch 0] Batch 1399, Loss 0.3467044234275818\n",
      "[Training Epoch 0] Batch 1400, Loss 0.3286672830581665\n",
      "[Training Epoch 0] Batch 1401, Loss 0.3541193902492523\n",
      "[Training Epoch 0] Batch 1402, Loss 0.3383193612098694\n",
      "[Training Epoch 0] Batch 1403, Loss 0.3524632453918457\n",
      "[Training Epoch 0] Batch 1404, Loss 0.33901435136795044\n",
      "[Training Epoch 0] Batch 1405, Loss 0.34746143221855164\n",
      "[Training Epoch 0] Batch 1406, Loss 0.37627896666526794\n",
      "[Training Epoch 0] Batch 1407, Loss 0.3303232192993164\n",
      "[Training Epoch 0] Batch 1408, Loss 0.35244423151016235\n",
      "[Training Epoch 0] Batch 1409, Loss 0.34790635108947754\n",
      "[Training Epoch 0] Batch 1410, Loss 0.34354379773139954\n",
      "[Training Epoch 0] Batch 1411, Loss 0.32775014638900757\n",
      "[Training Epoch 0] Batch 1412, Loss 0.3732684552669525\n",
      "[Training Epoch 0] Batch 1413, Loss 0.37161391973495483\n",
      "[Training Epoch 0] Batch 1414, Loss 0.36723634600639343\n",
      "[Training Epoch 0] Batch 1415, Loss 0.32374298572540283\n",
      "[Training Epoch 0] Batch 1416, Loss 0.3458985984325409\n",
      "[Training Epoch 0] Batch 1417, Loss 0.3493737578392029\n",
      "[Training Epoch 0] Batch 1418, Loss 0.32780128717422485\n",
      "[Training Epoch 0] Batch 1419, Loss 0.3427172303199768\n",
      "[Training Epoch 0] Batch 1420, Loss 0.35601335763931274\n",
      "[Training Epoch 0] Batch 1421, Loss 0.3549971580505371\n",
      "[Training Epoch 0] Batch 1422, Loss 0.329710453748703\n",
      "[Training Epoch 0] Batch 1423, Loss 0.3600636124610901\n",
      "[Training Epoch 0] Batch 1424, Loss 0.36261314153671265\n",
      "[Training Epoch 0] Batch 1425, Loss 0.3474319577217102\n",
      "[Training Epoch 0] Batch 1426, Loss 0.3534395098686218\n",
      "[Training Epoch 0] Batch 1427, Loss 0.3480348289012909\n",
      "[Training Epoch 0] Batch 1428, Loss 0.3222128748893738\n",
      "[Training Epoch 0] Batch 1429, Loss 0.3246224820613861\n",
      "[Training Epoch 0] Batch 1430, Loss 0.33603498339653015\n",
      "[Training Epoch 0] Batch 1431, Loss 0.3897510766983032\n",
      "[Training Epoch 0] Batch 1432, Loss 0.3488934338092804\n",
      "[Training Epoch 0] Batch 1433, Loss 0.33182060718536377\n",
      "[Training Epoch 0] Batch 1434, Loss 0.3462178111076355\n",
      "[Training Epoch 0] Batch 1435, Loss 0.3412279486656189\n",
      "[Training Epoch 0] Batch 1436, Loss 0.34765326976776123\n",
      "[Training Epoch 0] Batch 1437, Loss 0.35213378071784973\n",
      "[Training Epoch 0] Batch 1438, Loss 0.32668960094451904\n",
      "[Training Epoch 0] Batch 1439, Loss 0.3691680431365967\n",
      "[Training Epoch 0] Batch 1440, Loss 0.32409387826919556\n",
      "[Training Epoch 0] Batch 1441, Loss 0.3373354375362396\n",
      "[Training Epoch 0] Batch 1442, Loss 0.33472365140914917\n",
      "[Training Epoch 0] Batch 1443, Loss 0.34824949502944946\n",
      "[Training Epoch 0] Batch 1444, Loss 0.365694522857666\n",
      "[Training Epoch 0] Batch 1445, Loss 0.3474440574645996\n",
      "[Training Epoch 0] Batch 1446, Loss 0.34119877219200134\n",
      "[Training Epoch 0] Batch 1447, Loss 0.33654800057411194\n",
      "[Training Epoch 0] Batch 1448, Loss 0.3097366690635681\n",
      "[Training Epoch 0] Batch 1449, Loss 0.34887897968292236\n",
      "[Training Epoch 0] Batch 1450, Loss 0.35874098539352417\n",
      "[Training Epoch 0] Batch 1451, Loss 0.32965517044067383\n",
      "[Training Epoch 0] Batch 1452, Loss 0.3633958697319031\n",
      "[Training Epoch 0] Batch 1453, Loss 0.3622491657733917\n",
      "[Training Epoch 0] Batch 1454, Loss 0.3408825695514679\n",
      "[Training Epoch 0] Batch 1455, Loss 0.3519698679447174\n",
      "[Training Epoch 0] Batch 1456, Loss 0.3831901550292969\n",
      "[Training Epoch 0] Batch 1457, Loss 0.3497694730758667\n",
      "[Training Epoch 0] Batch 1458, Loss 0.3343273997306824\n",
      "[Training Epoch 0] Batch 1459, Loss 0.3452991247177124\n",
      "[Training Epoch 0] Batch 1460, Loss 0.35260316729545593\n",
      "[Training Epoch 0] Batch 1461, Loss 0.34928447008132935\n",
      "[Training Epoch 0] Batch 1462, Loss 0.3290242850780487\n",
      "[Training Epoch 0] Batch 1463, Loss 0.34615540504455566\n",
      "[Training Epoch 0] Batch 1464, Loss 0.34906554222106934\n",
      "[Training Epoch 0] Batch 1465, Loss 0.34444278478622437\n",
      "[Training Epoch 0] Batch 1466, Loss 0.35442131757736206\n",
      "[Training Epoch 0] Batch 1467, Loss 0.3646261692047119\n",
      "[Training Epoch 0] Batch 1468, Loss 0.3508574962615967\n",
      "[Training Epoch 0] Batch 1469, Loss 0.36308082938194275\n",
      "[Training Epoch 0] Batch 1470, Loss 0.34768742322921753\n",
      "[Training Epoch 0] Batch 1471, Loss 0.3394393026828766\n",
      "[Training Epoch 0] Batch 1472, Loss 0.3513956367969513\n",
      "[Training Epoch 0] Batch 1473, Loss 0.3541150689125061\n",
      "[Training Epoch 0] Batch 1474, Loss 0.35133159160614014\n",
      "[Training Epoch 0] Batch 1475, Loss 0.3454357981681824\n",
      "[Training Epoch 0] Batch 1476, Loss 0.33829009532928467\n",
      "[Training Epoch 0] Batch 1477, Loss 0.3366140127182007\n",
      "[Training Epoch 0] Batch 1478, Loss 0.37784144282341003\n",
      "[Training Epoch 0] Batch 1479, Loss 0.35528939962387085\n",
      "[Training Epoch 0] Batch 1480, Loss 0.3754245638847351\n",
      "[Training Epoch 0] Batch 1481, Loss 0.32812467217445374\n",
      "[Training Epoch 0] Batch 1482, Loss 0.3383841812610626\n",
      "[Training Epoch 0] Batch 1483, Loss 0.3612458109855652\n",
      "[Training Epoch 0] Batch 1484, Loss 0.3505345284938812\n",
      "[Training Epoch 0] Batch 1485, Loss 0.3526148796081543\n",
      "[Training Epoch 0] Batch 1486, Loss 0.35245782136917114\n",
      "[Training Epoch 0] Batch 1487, Loss 0.33575472235679626\n",
      "[Training Epoch 0] Batch 1488, Loss 0.3362088203430176\n",
      "[Training Epoch 0] Batch 1489, Loss 0.35470613837242126\n",
      "[Training Epoch 0] Batch 1490, Loss 0.3393336832523346\n",
      "[Training Epoch 0] Batch 1491, Loss 0.34337738156318665\n",
      "[Training Epoch 0] Batch 1492, Loss 0.3335498571395874\n",
      "[Training Epoch 0] Batch 1493, Loss 0.35795825719833374\n",
      "[Training Epoch 0] Batch 1494, Loss 0.37996867299079895\n",
      "[Training Epoch 0] Batch 1495, Loss 0.34090423583984375\n",
      "[Training Epoch 0] Batch 1496, Loss 0.3706355690956116\n",
      "[Training Epoch 0] Batch 1497, Loss 0.3434979021549225\n",
      "[Training Epoch 0] Batch 1498, Loss 0.30814725160598755\n",
      "[Training Epoch 0] Batch 1499, Loss 0.35321110486984253\n",
      "[Training Epoch 0] Batch 1500, Loss 0.354654997587204\n",
      "[Training Epoch 0] Batch 1501, Loss 0.3468002676963806\n",
      "[Training Epoch 0] Batch 1502, Loss 0.34701669216156006\n",
      "[Training Epoch 0] Batch 1503, Loss 0.3338972330093384\n",
      "[Training Epoch 0] Batch 1504, Loss 0.34023573994636536\n",
      "[Training Epoch 0] Batch 1505, Loss 0.33205053210258484\n",
      "[Training Epoch 0] Batch 1506, Loss 0.33368217945098877\n",
      "[Training Epoch 0] Batch 1507, Loss 0.34081190824508667\n",
      "[Training Epoch 0] Batch 1508, Loss 0.33793652057647705\n",
      "[Training Epoch 0] Batch 1509, Loss 0.3369940519332886\n",
      "[Training Epoch 0] Batch 1510, Loss 0.32370710372924805\n",
      "[Training Epoch 0] Batch 1511, Loss 0.32985812425613403\n",
      "[Training Epoch 0] Batch 1512, Loss 0.3505357801914215\n",
      "[Training Epoch 0] Batch 1513, Loss 0.3548155725002289\n",
      "[Training Epoch 0] Batch 1514, Loss 0.3469415307044983\n",
      "[Training Epoch 0] Batch 1515, Loss 0.3556060194969177\n",
      "[Training Epoch 0] Batch 1516, Loss 0.34985625743865967\n",
      "[Training Epoch 0] Batch 1517, Loss 0.36137205362319946\n",
      "[Training Epoch 0] Batch 1518, Loss 0.33554375171661377\n",
      "[Training Epoch 0] Batch 1519, Loss 0.34795108437538147\n",
      "[Training Epoch 0] Batch 1520, Loss 0.35817503929138184\n",
      "[Training Epoch 0] Batch 1521, Loss 0.34638896584510803\n",
      "[Training Epoch 0] Batch 1522, Loss 0.3294052481651306\n",
      "[Training Epoch 0] Batch 1523, Loss 0.38383010029792786\n",
      "[Training Epoch 0] Batch 1524, Loss 0.31353530287742615\n",
      "[Training Epoch 0] Batch 1525, Loss 0.33754146099090576\n",
      "[Training Epoch 0] Batch 1526, Loss 0.31159377098083496\n",
      "[Training Epoch 0] Batch 1527, Loss 0.3236624598503113\n",
      "[Training Epoch 0] Batch 1528, Loss 0.35036054253578186\n",
      "[Training Epoch 0] Batch 1529, Loss 0.31976577639579773\n",
      "[Training Epoch 0] Batch 1530, Loss 0.3459763526916504\n",
      "[Training Epoch 0] Batch 1531, Loss 0.34778133034706116\n",
      "[Training Epoch 0] Batch 1532, Loss 0.3532440662384033\n",
      "[Training Epoch 0] Batch 1533, Loss 0.328954815864563\n",
      "[Training Epoch 0] Batch 1534, Loss 0.3811289668083191\n",
      "[Training Epoch 0] Batch 1535, Loss 0.355018675327301\n",
      "[Training Epoch 0] Batch 1536, Loss 0.3614363670349121\n",
      "[Training Epoch 0] Batch 1537, Loss 0.3596806228160858\n",
      "[Training Epoch 0] Batch 1538, Loss 0.34272539615631104\n",
      "[Training Epoch 0] Batch 1539, Loss 0.35613465309143066\n",
      "[Training Epoch 0] Batch 1540, Loss 0.3414839506149292\n",
      "[Training Epoch 0] Batch 1541, Loss 0.3288385272026062\n",
      "[Training Epoch 0] Batch 1542, Loss 0.3457390069961548\n",
      "[Training Epoch 0] Batch 1543, Loss 0.36468201875686646\n",
      "[Training Epoch 0] Batch 1544, Loss 0.3376007080078125\n",
      "[Training Epoch 0] Batch 1545, Loss 0.34039074182510376\n",
      "[Training Epoch 0] Batch 1546, Loss 0.368588924407959\n",
      "[Training Epoch 0] Batch 1547, Loss 0.33759886026382446\n",
      "[Training Epoch 0] Batch 1548, Loss 0.33670419454574585\n",
      "[Training Epoch 0] Batch 1549, Loss 0.327187180519104\n",
      "[Training Epoch 0] Batch 1550, Loss 0.3579559326171875\n",
      "[Training Epoch 0] Batch 1551, Loss 0.3266158699989319\n",
      "[Training Epoch 0] Batch 1552, Loss 0.34421873092651367\n",
      "[Training Epoch 0] Batch 1553, Loss 0.3161899149417877\n",
      "[Training Epoch 0] Batch 1554, Loss 0.3223594129085541\n",
      "[Training Epoch 0] Batch 1555, Loss 0.3405555784702301\n",
      "[Training Epoch 0] Batch 1556, Loss 0.33635181188583374\n",
      "[Training Epoch 0] Batch 1557, Loss 0.311615526676178\n",
      "[Training Epoch 0] Batch 1558, Loss 0.37253537774086\n",
      "[Training Epoch 0] Batch 1559, Loss 0.32236313819885254\n",
      "[Training Epoch 0] Batch 1560, Loss 0.34087073802948\n",
      "[Training Epoch 0] Batch 1561, Loss 0.3350200057029724\n",
      "[Training Epoch 0] Batch 1562, Loss 0.3510667383670807\n",
      "[Training Epoch 0] Batch 1563, Loss 0.37125301361083984\n",
      "[Training Epoch 0] Batch 1564, Loss 0.32023507356643677\n",
      "[Training Epoch 0] Batch 1565, Loss 0.3405252695083618\n",
      "[Training Epoch 0] Batch 1566, Loss 0.3395085632801056\n",
      "[Training Epoch 0] Batch 1567, Loss 0.376290500164032\n",
      "[Training Epoch 0] Batch 1568, Loss 0.32670679688453674\n",
      "[Training Epoch 0] Batch 1569, Loss 0.3184358775615692\n",
      "[Training Epoch 0] Batch 1570, Loss 0.34891125559806824\n",
      "[Training Epoch 0] Batch 1571, Loss 0.35867446660995483\n",
      "[Training Epoch 0] Batch 1572, Loss 0.33313286304473877\n",
      "[Training Epoch 0] Batch 1573, Loss 0.31292033195495605\n",
      "[Training Epoch 0] Batch 1574, Loss 0.34832626581192017\n",
      "[Training Epoch 0] Batch 1575, Loss 0.3327624797821045\n",
      "[Training Epoch 0] Batch 1576, Loss 0.3791009485721588\n",
      "[Training Epoch 0] Batch 1577, Loss 0.37750911712646484\n",
      "[Training Epoch 0] Batch 1578, Loss 0.34466353058815\n",
      "[Training Epoch 0] Batch 1579, Loss 0.3260507583618164\n",
      "[Training Epoch 0] Batch 1580, Loss 0.32440614700317383\n",
      "[Training Epoch 0] Batch 1581, Loss 0.3486063778400421\n",
      "[Training Epoch 0] Batch 1582, Loss 0.33355945348739624\n",
      "[Training Epoch 0] Batch 1583, Loss 0.3098132014274597\n",
      "[Training Epoch 0] Batch 1584, Loss 0.33492204546928406\n",
      "[Training Epoch 0] Batch 1585, Loss 0.3182094693183899\n",
      "[Training Epoch 0] Batch 1586, Loss 0.3479527235031128\n",
      "[Training Epoch 0] Batch 1587, Loss 0.33591562509536743\n",
      "[Training Epoch 0] Batch 1588, Loss 0.3114817142486572\n",
      "[Training Epoch 0] Batch 1589, Loss 0.3568440079689026\n",
      "[Training Epoch 0] Batch 1590, Loss 0.34036433696746826\n",
      "[Training Epoch 0] Batch 1591, Loss 0.34738361835479736\n",
      "[Training Epoch 0] Batch 1592, Loss 0.3589707016944885\n",
      "[Training Epoch 0] Batch 1593, Loss 0.30496907234191895\n",
      "[Training Epoch 0] Batch 1594, Loss 0.3419607877731323\n",
      "[Training Epoch 0] Batch 1595, Loss 0.33791548013687134\n",
      "[Training Epoch 0] Batch 1596, Loss 0.3714436888694763\n",
      "[Training Epoch 0] Batch 1597, Loss 0.3444399833679199\n",
      "[Training Epoch 0] Batch 1598, Loss 0.33798831701278687\n",
      "[Training Epoch 0] Batch 1599, Loss 0.36416229605674744\n",
      "[Training Epoch 0] Batch 1600, Loss 0.33285653591156006\n",
      "[Training Epoch 0] Batch 1601, Loss 0.3710746169090271\n",
      "[Training Epoch 0] Batch 1602, Loss 0.3570217490196228\n",
      "[Training Epoch 0] Batch 1603, Loss 0.35098129510879517\n",
      "[Training Epoch 0] Batch 1604, Loss 0.32933756709098816\n",
      "[Training Epoch 0] Batch 1605, Loss 0.332385390996933\n",
      "[Training Epoch 0] Batch 1606, Loss 0.3289405107498169\n",
      "[Training Epoch 0] Batch 1607, Loss 0.34369030594825745\n",
      "[Training Epoch 0] Batch 1608, Loss 0.32559579610824585\n",
      "[Training Epoch 0] Batch 1609, Loss 0.3289757966995239\n",
      "[Training Epoch 0] Batch 1610, Loss 0.3137383759021759\n",
      "[Training Epoch 0] Batch 1611, Loss 0.33172908425331116\n",
      "[Training Epoch 0] Batch 1612, Loss 0.3556988835334778\n",
      "[Training Epoch 0] Batch 1613, Loss 0.3436010479927063\n",
      "[Training Epoch 0] Batch 1614, Loss 0.3154759705066681\n",
      "[Training Epoch 0] Batch 1615, Loss 0.33344215154647827\n",
      "[Training Epoch 0] Batch 1616, Loss 0.3436860144138336\n",
      "[Training Epoch 0] Batch 1617, Loss 0.34665000438690186\n",
      "[Training Epoch 0] Batch 1618, Loss 0.3316589593887329\n",
      "[Training Epoch 0] Batch 1619, Loss 0.35056251287460327\n",
      "[Training Epoch 0] Batch 1620, Loss 0.3354472219944\n",
      "[Training Epoch 0] Batch 1621, Loss 0.3552795350551605\n",
      "[Training Epoch 0] Batch 1622, Loss 0.3446396291255951\n",
      "[Training Epoch 0] Batch 1623, Loss 0.33860939741134644\n",
      "[Training Epoch 0] Batch 1624, Loss 0.3360785245895386\n",
      "[Training Epoch 0] Batch 1625, Loss 0.3729862570762634\n",
      "[Training Epoch 0] Batch 1626, Loss 0.3257792592048645\n",
      "[Training Epoch 0] Batch 1627, Loss 0.3446468710899353\n",
      "[Training Epoch 0] Batch 1628, Loss 0.3111337125301361\n",
      "[Training Epoch 0] Batch 1629, Loss 0.33281177282333374\n",
      "[Training Epoch 0] Batch 1630, Loss 0.35405805706977844\n",
      "[Training Epoch 0] Batch 1631, Loss 0.35028132796287537\n",
      "[Training Epoch 0] Batch 1632, Loss 0.32400861382484436\n",
      "[Training Epoch 0] Batch 1633, Loss 0.32559871673583984\n",
      "[Training Epoch 0] Batch 1634, Loss 0.3437921106815338\n",
      "[Training Epoch 0] Batch 1635, Loss 0.3237622380256653\n",
      "[Training Epoch 0] Batch 1636, Loss 0.35123327374458313\n",
      "[Training Epoch 0] Batch 1637, Loss 0.3472825884819031\n",
      "[Training Epoch 0] Batch 1638, Loss 0.33098268508911133\n",
      "[Training Epoch 0] Batch 1639, Loss 0.3406020402908325\n",
      "[Training Epoch 0] Batch 1640, Loss 0.3620058298110962\n",
      "[Training Epoch 0] Batch 1641, Loss 0.3115364611148834\n",
      "[Training Epoch 0] Batch 1642, Loss 0.34909874200820923\n",
      "[Training Epoch 0] Batch 1643, Loss 0.3530465066432953\n",
      "[Training Epoch 0] Batch 1644, Loss 0.33445918560028076\n",
      "[Training Epoch 0] Batch 1645, Loss 0.3313756287097931\n",
      "[Training Epoch 0] Batch 1646, Loss 0.32658666372299194\n",
      "[Training Epoch 0] Batch 1647, Loss 0.34721171855926514\n",
      "[Training Epoch 0] Batch 1648, Loss 0.32270294427871704\n",
      "[Training Epoch 0] Batch 1649, Loss 0.32393741607666016\n",
      "[Training Epoch 0] Batch 1650, Loss 0.3569585084915161\n",
      "[Training Epoch 0] Batch 1651, Loss 0.3399781882762909\n",
      "[Training Epoch 0] Batch 1652, Loss 0.3352612853050232\n",
      "[Training Epoch 0] Batch 1653, Loss 0.33225691318511963\n",
      "[Training Epoch 0] Batch 1654, Loss 0.3183852434158325\n",
      "[Training Epoch 0] Batch 1655, Loss 0.3471320867538452\n",
      "[Training Epoch 0] Batch 1656, Loss 0.3066122531890869\n",
      "[Training Epoch 0] Batch 1657, Loss 0.32851117849349976\n",
      "[Training Epoch 0] Batch 1658, Loss 0.3188352584838867\n",
      "[Training Epoch 0] Batch 1659, Loss 0.3244754672050476\n",
      "[Training Epoch 0] Batch 1660, Loss 0.3168509602546692\n",
      "[Training Epoch 0] Batch 1661, Loss 0.33607298135757446\n",
      "[Training Epoch 0] Batch 1662, Loss 0.34652021527290344\n",
      "[Training Epoch 0] Batch 1663, Loss 0.3296707570552826\n",
      "[Training Epoch 0] Batch 1664, Loss 0.3661782145500183\n",
      "[Training Epoch 0] Batch 1665, Loss 0.32878726720809937\n",
      "[Training Epoch 0] Batch 1666, Loss 0.3494093418121338\n",
      "[Training Epoch 0] Batch 1667, Loss 0.3500651717185974\n",
      "[Training Epoch 0] Batch 1668, Loss 0.35999709367752075\n",
      "[Training Epoch 0] Batch 1669, Loss 0.33399125933647156\n",
      "[Training Epoch 0] Batch 1670, Loss 0.3398018479347229\n",
      "[Training Epoch 0] Batch 1671, Loss 0.30892282724380493\n",
      "[Training Epoch 0] Batch 1672, Loss 0.34099483489990234\n",
      "[Training Epoch 0] Batch 1673, Loss 0.3435795307159424\n",
      "[Training Epoch 0] Batch 1674, Loss 0.3480657935142517\n",
      "[Training Epoch 0] Batch 1675, Loss 0.33761370182037354\n",
      "[Training Epoch 0] Batch 1676, Loss 0.3548666834831238\n",
      "[Training Epoch 0] Batch 1677, Loss 0.3571222126483917\n",
      "[Training Epoch 0] Batch 1678, Loss 0.2973834276199341\n",
      "[Training Epoch 0] Batch 1679, Loss 0.3441905677318573\n",
      "[Training Epoch 0] Batch 1680, Loss 0.32334184646606445\n",
      "[Training Epoch 0] Batch 1681, Loss 0.34640276432037354\n",
      "[Training Epoch 0] Batch 1682, Loss 0.35752883553504944\n",
      "[Training Epoch 0] Batch 1683, Loss 0.3271387219429016\n",
      "[Training Epoch 0] Batch 1684, Loss 0.31174927949905396\n",
      "[Training Epoch 0] Batch 1685, Loss 0.3395804464817047\n",
      "[Training Epoch 0] Batch 1686, Loss 0.369355171918869\n",
      "[Training Epoch 0] Batch 1687, Loss 0.30978360772132874\n",
      "[Training Epoch 0] Batch 1688, Loss 0.33140289783477783\n",
      "[Training Epoch 0] Batch 1689, Loss 0.35245999693870544\n",
      "[Training Epoch 0] Batch 1690, Loss 0.3059312701225281\n",
      "[Training Epoch 0] Batch 1691, Loss 0.32028359174728394\n",
      "[Training Epoch 0] Batch 1692, Loss 0.3425725996494293\n",
      "[Training Epoch 0] Batch 1693, Loss 0.3304458558559418\n",
      "[Training Epoch 0] Batch 1694, Loss 0.32091179490089417\n",
      "[Training Epoch 0] Batch 1695, Loss 0.35118675231933594\n",
      "[Training Epoch 0] Batch 1696, Loss 0.3284454345703125\n",
      "[Training Epoch 0] Batch 1697, Loss 0.3157675266265869\n",
      "[Training Epoch 0] Batch 1698, Loss 0.3578135073184967\n",
      "[Training Epoch 0] Batch 1699, Loss 0.3645436763763428\n",
      "[Training Epoch 0] Batch 1700, Loss 0.30861443281173706\n",
      "[Training Epoch 0] Batch 1701, Loss 0.3384288549423218\n",
      "[Training Epoch 0] Batch 1702, Loss 0.3364420235157013\n",
      "[Training Epoch 0] Batch 1703, Loss 0.34525781869888306\n",
      "[Training Epoch 0] Batch 1704, Loss 0.31607386469841003\n",
      "[Training Epoch 0] Batch 1705, Loss 0.30761080980300903\n",
      "[Training Epoch 0] Batch 1706, Loss 0.31628185510635376\n",
      "[Training Epoch 0] Batch 1707, Loss 0.32130783796310425\n",
      "[Training Epoch 0] Batch 1708, Loss 0.31590303778648376\n",
      "[Training Epoch 0] Batch 1709, Loss 0.3613032102584839\n",
      "[Training Epoch 0] Batch 1710, Loss 0.3275081515312195\n",
      "[Training Epoch 0] Batch 1711, Loss 0.3391726016998291\n",
      "[Training Epoch 0] Batch 1712, Loss 0.3469734191894531\n",
      "[Training Epoch 0] Batch 1713, Loss 0.3462732434272766\n",
      "[Training Epoch 0] Batch 1714, Loss 0.338786780834198\n",
      "[Training Epoch 0] Batch 1715, Loss 0.3075787425041199\n",
      "[Training Epoch 0] Batch 1716, Loss 0.3498234152793884\n",
      "[Training Epoch 0] Batch 1717, Loss 0.32467830181121826\n",
      "[Training Epoch 0] Batch 1718, Loss 0.33094894886016846\n",
      "[Training Epoch 0] Batch 1719, Loss 0.3267240822315216\n",
      "[Training Epoch 0] Batch 1720, Loss 0.34089821577072144\n",
      "[Training Epoch 0] Batch 1721, Loss 0.3547322154045105\n",
      "[Training Epoch 0] Batch 1722, Loss 0.3420567512512207\n",
      "[Training Epoch 0] Batch 1723, Loss 0.30650901794433594\n",
      "[Training Epoch 0] Batch 1724, Loss 0.3425099551677704\n",
      "[Training Epoch 0] Batch 1725, Loss 0.32997429370880127\n",
      "[Training Epoch 0] Batch 1726, Loss 0.34390419721603394\n",
      "[Training Epoch 0] Batch 1727, Loss 0.32357197999954224\n",
      "[Training Epoch 0] Batch 1728, Loss 0.3779818117618561\n",
      "[Training Epoch 0] Batch 1729, Loss 0.3266200125217438\n",
      "[Training Epoch 0] Batch 1730, Loss 0.30122268199920654\n",
      "[Training Epoch 0] Batch 1731, Loss 0.3407442271709442\n",
      "[Training Epoch 0] Batch 1732, Loss 0.3173280358314514\n",
      "[Training Epoch 0] Batch 1733, Loss 0.33566269278526306\n",
      "[Training Epoch 0] Batch 1734, Loss 0.32958510518074036\n",
      "[Training Epoch 0] Batch 1735, Loss 0.35490670800209045\n",
      "[Training Epoch 0] Batch 1736, Loss 0.3238183856010437\n",
      "[Training Epoch 0] Batch 1737, Loss 0.35119855403900146\n",
      "[Training Epoch 0] Batch 1738, Loss 0.3492633104324341\n",
      "[Training Epoch 0] Batch 1739, Loss 0.34737446904182434\n",
      "[Training Epoch 0] Batch 1740, Loss 0.3482242822647095\n",
      "[Training Epoch 0] Batch 1741, Loss 0.3193369507789612\n",
      "[Training Epoch 0] Batch 1742, Loss 0.32969677448272705\n",
      "[Training Epoch 0] Batch 1743, Loss 0.3325324058532715\n",
      "[Training Epoch 0] Batch 1744, Loss 0.3471483588218689\n",
      "[Training Epoch 0] Batch 1745, Loss 0.33792078495025635\n",
      "[Training Epoch 0] Batch 1746, Loss 0.331180214881897\n",
      "[Training Epoch 0] Batch 1747, Loss 0.38410288095474243\n",
      "[Training Epoch 0] Batch 1748, Loss 0.35072359442710876\n",
      "[Training Epoch 0] Batch 1749, Loss 0.3363296389579773\n",
      "[Training Epoch 0] Batch 1750, Loss 0.35404932498931885\n",
      "[Training Epoch 0] Batch 1751, Loss 0.3354741632938385\n",
      "[Training Epoch 0] Batch 1752, Loss 0.33628326654434204\n",
      "[Training Epoch 0] Batch 1753, Loss 0.36323675513267517\n",
      "[Training Epoch 0] Batch 1754, Loss 0.34496837854385376\n",
      "[Training Epoch 0] Batch 1755, Loss 0.3571930229663849\n",
      "[Training Epoch 0] Batch 1756, Loss 0.33938950300216675\n",
      "[Training Epoch 0] Batch 1757, Loss 0.32743674516677856\n",
      "[Training Epoch 0] Batch 1758, Loss 0.3386417329311371\n",
      "[Training Epoch 0] Batch 1759, Loss 0.3394978642463684\n",
      "[Training Epoch 0] Batch 1760, Loss 0.34721529483795166\n",
      "[Training Epoch 0] Batch 1761, Loss 0.3497096002101898\n",
      "[Training Epoch 0] Batch 1762, Loss 0.33101820945739746\n",
      "[Training Epoch 0] Batch 1763, Loss 0.3825017511844635\n",
      "[Training Epoch 0] Batch 1764, Loss 0.3435114324092865\n",
      "[Training Epoch 0] Batch 1765, Loss 0.33919769525527954\n",
      "[Training Epoch 0] Batch 1766, Loss 0.3144822418689728\n",
      "[Training Epoch 0] Batch 1767, Loss 0.30466747283935547\n",
      "[Training Epoch 0] Batch 1768, Loss 0.33651167154312134\n",
      "[Training Epoch 0] Batch 1769, Loss 0.33497896790504456\n",
      "[Training Epoch 0] Batch 1770, Loss 0.3443959653377533\n",
      "[Training Epoch 0] Batch 1771, Loss 0.34136873483657837\n",
      "[Training Epoch 0] Batch 1772, Loss 0.31383079290390015\n",
      "[Training Epoch 0] Batch 1773, Loss 0.32999399304389954\n",
      "[Training Epoch 0] Batch 1774, Loss 0.3649330735206604\n",
      "[Training Epoch 0] Batch 1775, Loss 0.30342742800712585\n",
      "[Training Epoch 0] Batch 1776, Loss 0.31159713864326477\n",
      "[Training Epoch 0] Batch 1777, Loss 0.3145914077758789\n",
      "[Training Epoch 0] Batch 1778, Loss 0.3308871388435364\n",
      "[Training Epoch 0] Batch 1779, Loss 0.3070143461227417\n",
      "[Training Epoch 0] Batch 1780, Loss 0.34108999371528625\n",
      "[Training Epoch 0] Batch 1781, Loss 0.3477635979652405\n",
      "[Training Epoch 0] Batch 1782, Loss 0.3521729111671448\n",
      "[Training Epoch 0] Batch 1783, Loss 0.3300017714500427\n",
      "[Training Epoch 0] Batch 1784, Loss 0.3397634029388428\n",
      "[Training Epoch 0] Batch 1785, Loss 0.3363890051841736\n",
      "[Training Epoch 0] Batch 1786, Loss 0.35572105646133423\n",
      "[Training Epoch 0] Batch 1787, Loss 0.3279498219490051\n",
      "[Training Epoch 0] Batch 1788, Loss 0.32952797412872314\n",
      "[Training Epoch 0] Batch 1789, Loss 0.3544182777404785\n",
      "[Training Epoch 0] Batch 1790, Loss 0.35505250096321106\n",
      "[Training Epoch 0] Batch 1791, Loss 0.33809220790863037\n",
      "[Training Epoch 0] Batch 1792, Loss 0.3316017985343933\n",
      "[Training Epoch 0] Batch 1793, Loss 0.3462948799133301\n",
      "[Training Epoch 0] Batch 1794, Loss 0.3391726613044739\n",
      "[Training Epoch 0] Batch 1795, Loss 0.350864052772522\n",
      "[Training Epoch 0] Batch 1796, Loss 0.33524084091186523\n",
      "[Training Epoch 0] Batch 1797, Loss 0.33303511142730713\n",
      "[Training Epoch 0] Batch 1798, Loss 0.3281598389148712\n",
      "[Training Epoch 0] Batch 1799, Loss 0.3381420373916626\n",
      "[Training Epoch 0] Batch 1800, Loss 0.3642449975013733\n",
      "[Training Epoch 0] Batch 1801, Loss 0.34041398763656616\n",
      "[Training Epoch 0] Batch 1802, Loss 0.36997902393341064\n",
      "[Training Epoch 0] Batch 1803, Loss 0.32864174246788025\n",
      "[Training Epoch 0] Batch 1804, Loss 0.34450703859329224\n",
      "[Training Epoch 0] Batch 1805, Loss 0.3809521794319153\n",
      "[Training Epoch 0] Batch 1806, Loss 0.3609325885772705\n",
      "[Training Epoch 0] Batch 1807, Loss 0.3665362000465393\n",
      "[Training Epoch 0] Batch 1808, Loss 0.31732338666915894\n",
      "[Training Epoch 0] Batch 1809, Loss 0.3219994306564331\n",
      "[Training Epoch 0] Batch 1810, Loss 0.3356781601905823\n",
      "[Training Epoch 0] Batch 1811, Loss 0.32606402039527893\n",
      "[Training Epoch 0] Batch 1812, Loss 0.34874650835990906\n",
      "[Training Epoch 0] Batch 1813, Loss 0.33210521936416626\n",
      "[Training Epoch 0] Batch 1814, Loss 0.33637332916259766\n",
      "[Training Epoch 0] Batch 1815, Loss 0.3449127972126007\n",
      "[Training Epoch 0] Batch 1816, Loss 0.3213544487953186\n",
      "[Training Epoch 0] Batch 1817, Loss 0.3197806179523468\n",
      "[Training Epoch 0] Batch 1818, Loss 0.36985668540000916\n",
      "[Training Epoch 0] Batch 1819, Loss 0.3221416771411896\n",
      "[Training Epoch 0] Batch 1820, Loss 0.33997178077697754\n",
      "[Training Epoch 0] Batch 1821, Loss 0.3352452218532562\n",
      "[Training Epoch 0] Batch 1822, Loss 0.30528727173805237\n",
      "[Training Epoch 0] Batch 1823, Loss 0.3121194541454315\n",
      "[Training Epoch 0] Batch 1824, Loss 0.353380411863327\n",
      "[Training Epoch 0] Batch 1825, Loss 0.3495529890060425\n",
      "[Training Epoch 0] Batch 1826, Loss 0.32413631677627563\n",
      "[Training Epoch 0] Batch 1827, Loss 0.34839069843292236\n",
      "[Training Epoch 0] Batch 1828, Loss 0.35798317193984985\n",
      "[Training Epoch 0] Batch 1829, Loss 0.29805681109428406\n",
      "[Training Epoch 0] Batch 1830, Loss 0.3466107249259949\n",
      "[Training Epoch 0] Batch 1831, Loss 0.3513481318950653\n",
      "[Training Epoch 0] Batch 1832, Loss 0.3236573338508606\n",
      "[Training Epoch 0] Batch 1833, Loss 0.3485414385795593\n",
      "[Training Epoch 0] Batch 1834, Loss 0.37164196372032166\n",
      "[Training Epoch 0] Batch 1835, Loss 0.339629203081131\n",
      "[Training Epoch 0] Batch 1836, Loss 0.32324713468551636\n",
      "[Training Epoch 0] Batch 1837, Loss 0.32693299651145935\n",
      "[Training Epoch 0] Batch 1838, Loss 0.3407469391822815\n",
      "[Training Epoch 0] Batch 1839, Loss 0.344969779253006\n",
      "[Training Epoch 0] Batch 1840, Loss 0.35215944051742554\n",
      "[Training Epoch 0] Batch 1841, Loss 0.31912973523139954\n",
      "[Training Epoch 0] Batch 1842, Loss 0.3269246816635132\n",
      "[Training Epoch 0] Batch 1843, Loss 0.33334410190582275\n",
      "[Training Epoch 0] Batch 1844, Loss 0.31516581773757935\n",
      "[Training Epoch 0] Batch 1845, Loss 0.33629417419433594\n",
      "[Training Epoch 0] Batch 1846, Loss 0.3213180899620056\n",
      "[Training Epoch 0] Batch 1847, Loss 0.3246307075023651\n",
      "[Training Epoch 0] Batch 1848, Loss 0.36403703689575195\n",
      "[Training Epoch 0] Batch 1849, Loss 0.33007562160491943\n",
      "[Training Epoch 0] Batch 1850, Loss 0.346280038356781\n",
      "[Training Epoch 0] Batch 1851, Loss 0.3201049566268921\n",
      "[Training Epoch 0] Batch 1852, Loss 0.3089704215526581\n",
      "[Training Epoch 0] Batch 1853, Loss 0.36731261014938354\n",
      "[Training Epoch 0] Batch 1854, Loss 0.3210529386997223\n",
      "[Training Epoch 0] Batch 1855, Loss 0.35281217098236084\n",
      "[Training Epoch 0] Batch 1856, Loss 0.3608212471008301\n",
      "[Training Epoch 0] Batch 1857, Loss 0.34923115372657776\n",
      "[Training Epoch 0] Batch 1858, Loss 0.2942691743373871\n",
      "[Training Epoch 0] Batch 1859, Loss 0.30230069160461426\n",
      "[Training Epoch 0] Batch 1860, Loss 0.32866916060447693\n",
      "[Training Epoch 0] Batch 1861, Loss 0.34921741485595703\n",
      "[Training Epoch 0] Batch 1862, Loss 0.32689252495765686\n",
      "[Training Epoch 0] Batch 1863, Loss 0.3193713426589966\n",
      "[Training Epoch 0] Batch 1864, Loss 0.3448365330696106\n",
      "[Training Epoch 0] Batch 1865, Loss 0.3692392408847809\n",
      "[Training Epoch 0] Batch 1866, Loss 0.33421429991722107\n",
      "[Training Epoch 0] Batch 1867, Loss 0.31566113233566284\n",
      "[Training Epoch 0] Batch 1868, Loss 0.3266354501247406\n",
      "[Training Epoch 0] Batch 1869, Loss 0.3141551911830902\n",
      "[Training Epoch 0] Batch 1870, Loss 0.3107951581478119\n",
      "[Training Epoch 0] Batch 1871, Loss 0.30737102031707764\n",
      "[Training Epoch 0] Batch 1872, Loss 0.34933000802993774\n",
      "[Training Epoch 0] Batch 1873, Loss 0.31862401962280273\n",
      "[Training Epoch 0] Batch 1874, Loss 0.34088510274887085\n",
      "[Training Epoch 0] Batch 1875, Loss 0.36101585626602173\n",
      "[Training Epoch 0] Batch 1876, Loss 0.2991509735584259\n",
      "[Training Epoch 0] Batch 1877, Loss 0.32779958844184875\n",
      "[Training Epoch 0] Batch 1878, Loss 0.3190121650695801\n",
      "[Training Epoch 0] Batch 1879, Loss 0.36218851804733276\n",
      "[Training Epoch 0] Batch 1880, Loss 0.3422030210494995\n",
      "[Training Epoch 0] Batch 1881, Loss 0.31202226877212524\n",
      "[Training Epoch 0] Batch 1882, Loss 0.33109721541404724\n",
      "[Training Epoch 0] Batch 1883, Loss 0.3430507183074951\n",
      "[Training Epoch 0] Batch 1884, Loss 0.35133886337280273\n",
      "[Training Epoch 0] Batch 1885, Loss 0.3124326467514038\n",
      "[Training Epoch 0] Batch 1886, Loss 0.3000393807888031\n",
      "[Training Epoch 0] Batch 1887, Loss 0.3215429484844208\n",
      "[Training Epoch 0] Batch 1888, Loss 0.35844185948371887\n",
      "[Training Epoch 0] Batch 1889, Loss 0.3358803391456604\n",
      "[Training Epoch 0] Batch 1890, Loss 0.3407971262931824\n",
      "[Training Epoch 0] Batch 1891, Loss 0.3501543700695038\n",
      "[Training Epoch 0] Batch 1892, Loss 0.3413540720939636\n",
      "[Training Epoch 0] Batch 1893, Loss 0.3383304476737976\n",
      "[Training Epoch 0] Batch 1894, Loss 0.3033749461174011\n",
      "[Training Epoch 0] Batch 1895, Loss 0.32856401801109314\n",
      "[Training Epoch 0] Batch 1896, Loss 0.342138409614563\n",
      "[Training Epoch 0] Batch 1897, Loss 0.34700775146484375\n",
      "[Training Epoch 0] Batch 1898, Loss 0.3705174922943115\n",
      "[Training Epoch 0] Batch 1899, Loss 0.3650209903717041\n",
      "[Training Epoch 0] Batch 1900, Loss 0.3417016267776489\n",
      "[Training Epoch 0] Batch 1901, Loss 0.3383673429489136\n",
      "[Training Epoch 0] Batch 1902, Loss 0.35236239433288574\n",
      "[Training Epoch 0] Batch 1903, Loss 0.34715622663497925\n",
      "[Training Epoch 0] Batch 1904, Loss 0.3151942491531372\n",
      "[Training Epoch 0] Batch 1905, Loss 0.3152655363082886\n",
      "[Training Epoch 0] Batch 1906, Loss 0.33750832080841064\n",
      "[Training Epoch 0] Batch 1907, Loss 0.31079578399658203\n",
      "[Training Epoch 0] Batch 1908, Loss 0.3425511121749878\n",
      "[Training Epoch 0] Batch 1909, Loss 0.3276393413543701\n",
      "[Training Epoch 0] Batch 1910, Loss 0.3250845670700073\n",
      "[Training Epoch 0] Batch 1911, Loss 0.3093565106391907\n",
      "[Training Epoch 0] Batch 1912, Loss 0.33081865310668945\n",
      "[Training Epoch 0] Batch 1913, Loss 0.31607672572135925\n",
      "[Training Epoch 0] Batch 1914, Loss 0.3527591824531555\n",
      "[Training Epoch 0] Batch 1915, Loss 0.32394614815711975\n",
      "[Training Epoch 0] Batch 1916, Loss 0.35115861892700195\n",
      "[Training Epoch 0] Batch 1917, Loss 0.3587653934955597\n",
      "[Training Epoch 0] Batch 1918, Loss 0.32651519775390625\n",
      "[Training Epoch 0] Batch 1919, Loss 0.3230692744255066\n",
      "[Training Epoch 0] Batch 1920, Loss 0.3683755695819855\n",
      "[Training Epoch 0] Batch 1921, Loss 0.3377753794193268\n",
      "[Training Epoch 0] Batch 1922, Loss 0.3091672658920288\n",
      "[Training Epoch 0] Batch 1923, Loss 0.34517019987106323\n",
      "[Training Epoch 0] Batch 1924, Loss 0.3350161910057068\n",
      "[Training Epoch 0] Batch 1925, Loss 0.3454476594924927\n",
      "[Training Epoch 0] Batch 1926, Loss 0.3172081708908081\n",
      "[Training Epoch 0] Batch 1927, Loss 0.35454076528549194\n",
      "[Training Epoch 0] Batch 1928, Loss 0.3402053117752075\n",
      "[Training Epoch 0] Batch 1929, Loss 0.3296859860420227\n",
      "[Training Epoch 0] Batch 1930, Loss 0.3306574821472168\n",
      "[Training Epoch 0] Batch 1931, Loss 0.3297463655471802\n",
      "[Training Epoch 0] Batch 1932, Loss 0.3404616713523865\n",
      "[Training Epoch 0] Batch 1933, Loss 0.3462288975715637\n",
      "[Training Epoch 0] Batch 1934, Loss 0.3276556134223938\n",
      "[Training Epoch 0] Batch 1935, Loss 0.32441896200180054\n",
      "[Training Epoch 0] Batch 1936, Loss 0.33833959698677063\n",
      "[Training Epoch 0] Batch 1937, Loss 0.3352371156215668\n",
      "[Training Epoch 0] Batch 1938, Loss 0.29144909977912903\n",
      "[Training Epoch 0] Batch 1939, Loss 0.3607507050037384\n",
      "[Training Epoch 0] Batch 1940, Loss 0.3361782431602478\n",
      "[Training Epoch 0] Batch 1941, Loss 0.33989855647087097\n",
      "[Training Epoch 0] Batch 1942, Loss 0.3337029814720154\n",
      "[Training Epoch 0] Batch 1943, Loss 0.3439125418663025\n",
      "[Training Epoch 0] Batch 1944, Loss 0.3456457257270813\n",
      "[Training Epoch 0] Batch 1945, Loss 0.30392986536026\n",
      "[Training Epoch 0] Batch 1946, Loss 0.3038436770439148\n",
      "[Training Epoch 0] Batch 1947, Loss 0.3499647378921509\n",
      "[Training Epoch 0] Batch 1948, Loss 0.34528619050979614\n",
      "[Training Epoch 0] Batch 1949, Loss 0.3544086217880249\n",
      "[Training Epoch 0] Batch 1950, Loss 0.3762969374656677\n",
      "[Training Epoch 0] Batch 1951, Loss 0.3298681676387787\n",
      "[Training Epoch 0] Batch 1952, Loss 0.34449735283851624\n",
      "[Training Epoch 0] Batch 1953, Loss 0.325539231300354\n",
      "[Training Epoch 0] Batch 1954, Loss 0.33420342206954956\n",
      "[Training Epoch 0] Batch 1955, Loss 0.32490435242652893\n",
      "[Training Epoch 0] Batch 1956, Loss 0.34296655654907227\n",
      "[Training Epoch 0] Batch 1957, Loss 0.36788517236709595\n",
      "[Training Epoch 0] Batch 1958, Loss 0.3322470188140869\n",
      "[Training Epoch 0] Batch 1959, Loss 0.3174043893814087\n",
      "[Training Epoch 0] Batch 1960, Loss 0.3113778233528137\n",
      "[Training Epoch 0] Batch 1961, Loss 0.3791525959968567\n",
      "[Training Epoch 0] Batch 1962, Loss 0.31994765996932983\n",
      "[Training Epoch 0] Batch 1963, Loss 0.3342474699020386\n",
      "[Training Epoch 0] Batch 1964, Loss 0.332145094871521\n",
      "[Training Epoch 0] Batch 1965, Loss 0.33017081022262573\n",
      "[Training Epoch 0] Batch 1966, Loss 0.32139813899993896\n",
      "[Training Epoch 0] Batch 1967, Loss 0.29835614562034607\n",
      "[Training Epoch 0] Batch 1968, Loss 0.33246752619743347\n",
      "[Training Epoch 0] Batch 1969, Loss 0.3608776926994324\n",
      "[Training Epoch 0] Batch 1970, Loss 0.3381815552711487\n",
      "[Training Epoch 0] Batch 1971, Loss 0.3255104720592499\n",
      "[Training Epoch 0] Batch 1972, Loss 0.3345232903957367\n",
      "[Training Epoch 0] Batch 1973, Loss 0.3264888525009155\n",
      "[Training Epoch 0] Batch 1974, Loss 0.3199685513973236\n",
      "[Training Epoch 0] Batch 1975, Loss 0.3638001084327698\n",
      "[Training Epoch 0] Batch 1976, Loss 0.31306129693984985\n",
      "[Training Epoch 0] Batch 1977, Loss 0.31665268540382385\n",
      "[Training Epoch 0] Batch 1978, Loss 0.3503264784812927\n",
      "[Training Epoch 0] Batch 1979, Loss 0.3192902207374573\n",
      "[Training Epoch 0] Batch 1980, Loss 0.3368510603904724\n",
      "[Training Epoch 0] Batch 1981, Loss 0.35581761598587036\n",
      "[Training Epoch 0] Batch 1982, Loss 0.3308202028274536\n",
      "[Training Epoch 0] Batch 1983, Loss 0.3458664119243622\n",
      "[Training Epoch 0] Batch 1984, Loss 0.34513646364212036\n",
      "[Training Epoch 0] Batch 1985, Loss 0.3098040521144867\n",
      "[Training Epoch 0] Batch 1986, Loss 0.34868937730789185\n",
      "[Training Epoch 0] Batch 1987, Loss 0.3232707679271698\n",
      "[Training Epoch 0] Batch 1988, Loss 0.30441969633102417\n",
      "[Training Epoch 0] Batch 1989, Loss 0.33528631925582886\n",
      "[Training Epoch 0] Batch 1990, Loss 0.35024493932724\n",
      "[Training Epoch 0] Batch 1991, Loss 0.35064369440078735\n",
      "[Training Epoch 0] Batch 1992, Loss 0.32654768228530884\n",
      "[Training Epoch 0] Batch 1993, Loss 0.3295276463031769\n",
      "[Training Epoch 0] Batch 1994, Loss 0.3393312096595764\n",
      "[Training Epoch 0] Batch 1995, Loss 0.3446701467037201\n",
      "[Training Epoch 0] Batch 1996, Loss 0.3454974293708801\n",
      "[Training Epoch 0] Batch 1997, Loss 0.31838124990463257\n",
      "[Training Epoch 0] Batch 1998, Loss 0.34276852011680603\n",
      "[Training Epoch 0] Batch 1999, Loss 0.36700624227523804\n",
      "[Training Epoch 0] Batch 2000, Loss 0.36050713062286377\n",
      "[Training Epoch 0] Batch 2001, Loss 0.34210601449012756\n",
      "[Training Epoch 0] Batch 2002, Loss 0.3278502821922302\n",
      "[Training Epoch 0] Batch 2003, Loss 0.32499682903289795\n",
      "[Training Epoch 0] Batch 2004, Loss 0.3509271740913391\n",
      "[Training Epoch 0] Batch 2005, Loss 0.31555449962615967\n",
      "[Training Epoch 0] Batch 2006, Loss 0.359908789396286\n",
      "[Training Epoch 0] Batch 2007, Loss 0.346842885017395\n",
      "[Training Epoch 0] Batch 2008, Loss 0.316584974527359\n",
      "[Training Epoch 0] Batch 2009, Loss 0.31642937660217285\n",
      "[Training Epoch 0] Batch 2010, Loss 0.3184705972671509\n",
      "[Training Epoch 0] Batch 2011, Loss 0.30095815658569336\n",
      "[Training Epoch 0] Batch 2012, Loss 0.35611671209335327\n",
      "[Training Epoch 0] Batch 2013, Loss 0.3804301619529724\n",
      "[Training Epoch 0] Batch 2014, Loss 0.3589392900466919\n",
      "[Training Epoch 0] Batch 2015, Loss 0.3392636775970459\n",
      "[Training Epoch 0] Batch 2016, Loss 0.31773751974105835\n",
      "[Training Epoch 0] Batch 2017, Loss 0.3244713544845581\n",
      "[Training Epoch 0] Batch 2018, Loss 0.3203449249267578\n",
      "[Training Epoch 0] Batch 2019, Loss 0.34847795963287354\n",
      "[Training Epoch 0] Batch 2020, Loss 0.34574681520462036\n",
      "[Training Epoch 0] Batch 2021, Loss 0.3429452180862427\n",
      "[Training Epoch 0] Batch 2022, Loss 0.36286604404449463\n",
      "[Training Epoch 0] Batch 2023, Loss 0.3349180221557617\n",
      "[Training Epoch 0] Batch 2024, Loss 0.3600298762321472\n",
      "[Training Epoch 0] Batch 2025, Loss 0.3335326910018921\n",
      "[Training Epoch 0] Batch 2026, Loss 0.321464478969574\n",
      "[Training Epoch 0] Batch 2027, Loss 0.3199426233768463\n",
      "[Training Epoch 0] Batch 2028, Loss 0.3522637188434601\n",
      "[Training Epoch 0] Batch 2029, Loss 0.32256969809532166\n",
      "[Training Epoch 0] Batch 2030, Loss 0.3341333866119385\n",
      "[Training Epoch 0] Batch 2031, Loss 0.3026497960090637\n",
      "[Training Epoch 0] Batch 2032, Loss 0.331813246011734\n",
      "[Training Epoch 0] Batch 2033, Loss 0.3388230502605438\n",
      "[Training Epoch 0] Batch 2034, Loss 0.31633150577545166\n",
      "[Training Epoch 0] Batch 2035, Loss 0.340503990650177\n",
      "[Training Epoch 0] Batch 2036, Loss 0.3315490782260895\n",
      "[Training Epoch 0] Batch 2037, Loss 0.34273409843444824\n",
      "[Training Epoch 0] Batch 2038, Loss 0.3256112039089203\n",
      "[Training Epoch 0] Batch 2039, Loss 0.3417370915412903\n",
      "[Training Epoch 0] Batch 2040, Loss 0.34077462553977966\n",
      "[Training Epoch 0] Batch 2041, Loss 0.3529720902442932\n",
      "[Training Epoch 0] Batch 2042, Loss 0.34561294317245483\n",
      "[Training Epoch 0] Batch 2043, Loss 0.29257696866989136\n",
      "[Training Epoch 0] Batch 2044, Loss 0.3029741644859314\n",
      "[Training Epoch 0] Batch 2045, Loss 0.33589673042297363\n",
      "[Training Epoch 0] Batch 2046, Loss 0.33891183137893677\n",
      "[Training Epoch 0] Batch 2047, Loss 0.3174111843109131\n",
      "[Training Epoch 0] Batch 2048, Loss 0.3151080906391144\n",
      "[Training Epoch 0] Batch 2049, Loss 0.33340317010879517\n",
      "[Training Epoch 0] Batch 2050, Loss 0.342340350151062\n",
      "[Training Epoch 0] Batch 2051, Loss 0.35290443897247314\n",
      "[Training Epoch 0] Batch 2052, Loss 0.3118955194950104\n",
      "[Training Epoch 0] Batch 2053, Loss 0.3012321889400482\n",
      "[Training Epoch 0] Batch 2054, Loss 0.31517088413238525\n",
      "[Training Epoch 0] Batch 2055, Loss 0.3185262084007263\n",
      "[Training Epoch 0] Batch 2056, Loss 0.3397875130176544\n",
      "[Training Epoch 0] Batch 2057, Loss 0.3350580930709839\n",
      "[Training Epoch 0] Batch 2058, Loss 0.3199945092201233\n",
      "[Training Epoch 0] Batch 2059, Loss 0.3041152060031891\n",
      "[Training Epoch 0] Batch 2060, Loss 0.32162970304489136\n",
      "[Training Epoch 0] Batch 2061, Loss 0.3764662742614746\n",
      "[Training Epoch 0] Batch 2062, Loss 0.31432241201400757\n",
      "[Training Epoch 0] Batch 2063, Loss 0.333719402551651\n",
      "[Training Epoch 0] Batch 2064, Loss 0.33578479290008545\n",
      "[Training Epoch 0] Batch 2065, Loss 0.34433621168136597\n",
      "[Training Epoch 0] Batch 2066, Loss 0.3655892312526703\n",
      "[Training Epoch 0] Batch 2067, Loss 0.34964674711227417\n",
      "[Training Epoch 0] Batch 2068, Loss 0.33003145456314087\n",
      "[Training Epoch 0] Batch 2069, Loss 0.3382166922092438\n",
      "[Training Epoch 0] Batch 2070, Loss 0.3312762677669525\n",
      "[Training Epoch 0] Batch 2071, Loss 0.32857292890548706\n",
      "[Training Epoch 0] Batch 2072, Loss 0.34296971559524536\n",
      "[Training Epoch 0] Batch 2073, Loss 0.307298868894577\n",
      "[Training Epoch 0] Batch 2074, Loss 0.35035836696624756\n",
      "[Training Epoch 0] Batch 2075, Loss 0.2989237904548645\n",
      "[Training Epoch 0] Batch 2076, Loss 0.33303213119506836\n",
      "[Training Epoch 0] Batch 2077, Loss 0.3304957449436188\n",
      "[Training Epoch 0] Batch 2078, Loss 0.33446311950683594\n",
      "[Training Epoch 0] Batch 2079, Loss 0.344127893447876\n",
      "[Training Epoch 0] Batch 2080, Loss 0.3510581851005554\n",
      "[Training Epoch 0] Batch 2081, Loss 0.34924012422561646\n",
      "[Training Epoch 0] Batch 2082, Loss 0.3479650020599365\n",
      "[Training Epoch 0] Batch 2083, Loss 0.34521180391311646\n",
      "[Training Epoch 0] Batch 2084, Loss 0.340290904045105\n",
      "[Training Epoch 0] Batch 2085, Loss 0.333212673664093\n",
      "[Training Epoch 0] Batch 2086, Loss 0.3410317897796631\n",
      "[Training Epoch 0] Batch 2087, Loss 0.34380730986595154\n",
      "[Training Epoch 0] Batch 2088, Loss 0.3191934823989868\n",
      "[Training Epoch 0] Batch 2089, Loss 0.34376218914985657\n",
      "[Training Epoch 0] Batch 2090, Loss 0.33135199546813965\n",
      "[Training Epoch 0] Batch 2091, Loss 0.3342024087905884\n",
      "[Training Epoch 0] Batch 2092, Loss 0.35054123401641846\n",
      "[Training Epoch 0] Batch 2093, Loss 0.3320653736591339\n",
      "[Training Epoch 0] Batch 2094, Loss 0.3422420620918274\n",
      "[Training Epoch 0] Batch 2095, Loss 0.3392927646636963\n",
      "[Training Epoch 0] Batch 2096, Loss 0.33996108174324036\n",
      "[Training Epoch 0] Batch 2097, Loss 0.34982746839523315\n",
      "[Training Epoch 0] Batch 2098, Loss 0.34605228900909424\n",
      "[Training Epoch 0] Batch 2099, Loss 0.32325279712677\n",
      "[Training Epoch 0] Batch 2100, Loss 0.31291601061820984\n",
      "[Training Epoch 0] Batch 2101, Loss 0.2989504337310791\n",
      "[Training Epoch 0] Batch 2102, Loss 0.32179784774780273\n",
      "[Training Epoch 0] Batch 2103, Loss 0.34484291076660156\n",
      "[Training Epoch 0] Batch 2104, Loss 0.3176411986351013\n",
      "[Training Epoch 0] Batch 2105, Loss 0.3524201512336731\n",
      "[Training Epoch 0] Batch 2106, Loss 0.31530439853668213\n",
      "[Training Epoch 0] Batch 2107, Loss 0.322768896818161\n",
      "[Training Epoch 0] Batch 2108, Loss 0.3396812081336975\n",
      "[Training Epoch 0] Batch 2109, Loss 0.3473218083381653\n",
      "[Training Epoch 0] Batch 2110, Loss 0.35478049516677856\n",
      "[Training Epoch 0] Batch 2111, Loss 0.33821597695350647\n",
      "[Training Epoch 0] Batch 2112, Loss 0.32365167140960693\n",
      "[Training Epoch 0] Batch 2113, Loss 0.3372765779495239\n",
      "[Training Epoch 0] Batch 2114, Loss 0.3165592551231384\n",
      "[Training Epoch 0] Batch 2115, Loss 0.3254450857639313\n",
      "[Training Epoch 0] Batch 2116, Loss 0.3722534477710724\n",
      "[Training Epoch 0] Batch 2117, Loss 0.33577561378479004\n",
      "[Training Epoch 0] Batch 2118, Loss 0.2965618371963501\n",
      "[Training Epoch 0] Batch 2119, Loss 0.3390142321586609\n",
      "[Training Epoch 0] Batch 2120, Loss 0.3258005976676941\n",
      "[Training Epoch 0] Batch 2121, Loss 0.35581135749816895\n",
      "[Training Epoch 0] Batch 2122, Loss 0.3487187623977661\n",
      "[Training Epoch 0] Batch 2123, Loss 0.3177884519100189\n",
      "[Training Epoch 0] Batch 2124, Loss 0.33421748876571655\n",
      "[Training Epoch 0] Batch 2125, Loss 0.29982680082321167\n",
      "[Training Epoch 0] Batch 2126, Loss 0.3330346941947937\n",
      "[Training Epoch 0] Batch 2127, Loss 0.33967000246047974\n",
      "[Training Epoch 0] Batch 2128, Loss 0.3270016312599182\n",
      "[Training Epoch 0] Batch 2129, Loss 0.3480667471885681\n",
      "[Training Epoch 0] Batch 2130, Loss 0.347819447517395\n",
      "[Training Epoch 0] Batch 2131, Loss 0.34741178154945374\n",
      "[Training Epoch 0] Batch 2132, Loss 0.3129834532737732\n",
      "[Training Epoch 0] Batch 2133, Loss 0.3029874861240387\n",
      "[Training Epoch 0] Batch 2134, Loss 0.30933815240859985\n",
      "[Training Epoch 0] Batch 2135, Loss 0.34523969888687134\n",
      "[Training Epoch 0] Batch 2136, Loss 0.353604257106781\n",
      "[Training Epoch 0] Batch 2137, Loss 0.3350108861923218\n",
      "[Training Epoch 0] Batch 2138, Loss 0.31832507252693176\n",
      "[Training Epoch 0] Batch 2139, Loss 0.35596001148223877\n",
      "[Training Epoch 0] Batch 2140, Loss 0.33985981345176697\n",
      "[Training Epoch 0] Batch 2141, Loss 0.3492560386657715\n",
      "[Training Epoch 0] Batch 2142, Loss 0.33378052711486816\n",
      "[Training Epoch 0] Batch 2143, Loss 0.33328378200531006\n",
      "[Training Epoch 0] Batch 2144, Loss 0.33958810567855835\n",
      "[Training Epoch 0] Batch 2145, Loss 0.3579765558242798\n",
      "[Training Epoch 0] Batch 2146, Loss 0.3684872090816498\n",
      "[Training Epoch 0] Batch 2147, Loss 0.3143160939216614\n",
      "[Training Epoch 0] Batch 2148, Loss 0.35917699337005615\n",
      "[Training Epoch 0] Batch 2149, Loss 0.32437416911125183\n",
      "[Training Epoch 0] Batch 2150, Loss 0.33446764945983887\n",
      "[Training Epoch 0] Batch 2151, Loss 0.31244412064552307\n",
      "[Training Epoch 0] Batch 2152, Loss 0.3422151803970337\n",
      "[Training Epoch 0] Batch 2153, Loss 0.31722491979599\n",
      "[Training Epoch 0] Batch 2154, Loss 0.3565387725830078\n",
      "[Training Epoch 0] Batch 2155, Loss 0.3429650664329529\n",
      "[Training Epoch 0] Batch 2156, Loss 0.3113229274749756\n",
      "[Training Epoch 0] Batch 2157, Loss 0.347708135843277\n",
      "[Training Epoch 0] Batch 2158, Loss 0.31914466619491577\n",
      "[Training Epoch 0] Batch 2159, Loss 0.3284604251384735\n",
      "[Training Epoch 0] Batch 2160, Loss 0.33122310042381287\n",
      "[Training Epoch 0] Batch 2161, Loss 0.3089933693408966\n",
      "[Training Epoch 0] Batch 2162, Loss 0.33889031410217285\n",
      "[Training Epoch 0] Batch 2163, Loss 0.3308054804801941\n",
      "[Training Epoch 0] Batch 2164, Loss 0.3398768901824951\n",
      "[Training Epoch 0] Batch 2165, Loss 0.31681954860687256\n",
      "[Training Epoch 0] Batch 2166, Loss 0.34256958961486816\n",
      "[Training Epoch 0] Batch 2167, Loss 0.30691564083099365\n",
      "[Training Epoch 0] Batch 2168, Loss 0.3179435431957245\n",
      "[Training Epoch 0] Batch 2169, Loss 0.3490836024284363\n",
      "[Training Epoch 0] Batch 2170, Loss 0.33424749970436096\n",
      "[Training Epoch 0] Batch 2171, Loss 0.33205729722976685\n",
      "[Training Epoch 0] Batch 2172, Loss 0.3396402597427368\n",
      "[Training Epoch 0] Batch 2173, Loss 0.34290146827697754\n",
      "[Training Epoch 0] Batch 2174, Loss 0.2935275435447693\n",
      "[Training Epoch 0] Batch 2175, Loss 0.3318384289741516\n",
      "[Training Epoch 0] Batch 2176, Loss 0.3312700688838959\n",
      "[Training Epoch 0] Batch 2177, Loss 0.30703890323638916\n",
      "[Training Epoch 0] Batch 2178, Loss 0.3242279887199402\n",
      "[Training Epoch 0] Batch 2179, Loss 0.3191153109073639\n",
      "[Training Epoch 0] Batch 2180, Loss 0.3219846487045288\n",
      "[Training Epoch 0] Batch 2181, Loss 0.3366570472717285\n",
      "[Training Epoch 0] Batch 2182, Loss 0.3203035891056061\n",
      "[Training Epoch 0] Batch 2183, Loss 0.3410084545612335\n",
      "[Training Epoch 0] Batch 2184, Loss 0.3358001410961151\n",
      "[Training Epoch 0] Batch 2185, Loss 0.32157188653945923\n",
      "[Training Epoch 0] Batch 2186, Loss 0.3201458752155304\n",
      "[Training Epoch 0] Batch 2187, Loss 0.3383246660232544\n",
      "[Training Epoch 0] Batch 2188, Loss 0.32253289222717285\n",
      "[Training Epoch 0] Batch 2189, Loss 0.3456801772117615\n",
      "[Training Epoch 0] Batch 2190, Loss 0.3227517902851105\n",
      "[Training Epoch 0] Batch 2191, Loss 0.3123447000980377\n",
      "[Training Epoch 0] Batch 2192, Loss 0.3148046135902405\n",
      "[Training Epoch 0] Batch 2193, Loss 0.33218514919281006\n",
      "[Training Epoch 0] Batch 2194, Loss 0.3381050229072571\n",
      "[Training Epoch 0] Batch 2195, Loss 0.33781561255455017\n",
      "[Training Epoch 0] Batch 2196, Loss 0.3344755172729492\n",
      "[Training Epoch 0] Batch 2197, Loss 0.327237069606781\n",
      "[Training Epoch 0] Batch 2198, Loss 0.3274439871311188\n",
      "[Training Epoch 0] Batch 2199, Loss 0.3258631229400635\n",
      "[Training Epoch 0] Batch 2200, Loss 0.33115512132644653\n",
      "[Training Epoch 0] Batch 2201, Loss 0.33396798372268677\n",
      "[Training Epoch 0] Batch 2202, Loss 0.2976742088794708\n",
      "[Training Epoch 0] Batch 2203, Loss 0.33250871300697327\n",
      "[Training Epoch 0] Batch 2204, Loss 0.3343818187713623\n",
      "[Training Epoch 0] Batch 2205, Loss 0.30988332629203796\n",
      "[Training Epoch 0] Batch 2206, Loss 0.3592380881309509\n",
      "[Training Epoch 0] Batch 2207, Loss 0.3313613533973694\n",
      "[Training Epoch 0] Batch 2208, Loss 0.33896440267562866\n",
      "[Training Epoch 0] Batch 2209, Loss 0.3374813497066498\n",
      "[Training Epoch 0] Batch 2210, Loss 0.3250100612640381\n",
      "[Training Epoch 0] Batch 2211, Loss 0.307211697101593\n",
      "[Training Epoch 0] Batch 2212, Loss 0.35340797901153564\n",
      "[Training Epoch 0] Batch 2213, Loss 0.31996089220046997\n",
      "[Training Epoch 0] Batch 2214, Loss 0.3377004563808441\n",
      "[Training Epoch 0] Batch 2215, Loss 0.3447534441947937\n",
      "[Training Epoch 0] Batch 2216, Loss 0.3150034248828888\n",
      "[Training Epoch 0] Batch 2217, Loss 0.34737515449523926\n",
      "[Training Epoch 0] Batch 2218, Loss 0.3396098017692566\n",
      "[Training Epoch 0] Batch 2219, Loss 0.3217608332633972\n",
      "[Training Epoch 0] Batch 2220, Loss 0.34443944692611694\n",
      "[Training Epoch 0] Batch 2221, Loss 0.3318042755126953\n",
      "[Training Epoch 0] Batch 2222, Loss 0.3148350715637207\n",
      "[Training Epoch 0] Batch 2223, Loss 0.31807446479797363\n",
      "[Training Epoch 0] Batch 2224, Loss 0.30928295850753784\n",
      "[Training Epoch 0] Batch 2225, Loss 0.33818519115448\n",
      "[Training Epoch 0] Batch 2226, Loss 0.3138349652290344\n",
      "[Training Epoch 0] Batch 2227, Loss 0.3122948408126831\n",
      "[Training Epoch 0] Batch 2228, Loss 0.3283262550830841\n",
      "[Training Epoch 0] Batch 2229, Loss 0.35703474283218384\n",
      "[Training Epoch 0] Batch 2230, Loss 0.32045120000839233\n",
      "[Training Epoch 0] Batch 2231, Loss 0.3451824486255646\n",
      "[Training Epoch 0] Batch 2232, Loss 0.3415217995643616\n",
      "[Training Epoch 0] Batch 2233, Loss 0.34237605333328247\n",
      "[Training Epoch 0] Batch 2234, Loss 0.34342798590660095\n",
      "[Training Epoch 0] Batch 2235, Loss 0.32029256224632263\n",
      "[Training Epoch 0] Batch 2236, Loss 0.3248313367366791\n",
      "[Training Epoch 0] Batch 2237, Loss 0.3261428475379944\n",
      "[Training Epoch 0] Batch 2238, Loss 0.32308056950569153\n",
      "[Training Epoch 0] Batch 2239, Loss 0.2871635854244232\n",
      "[Training Epoch 0] Batch 2240, Loss 0.31821608543395996\n",
      "[Training Epoch 0] Batch 2241, Loss 0.3103150427341461\n",
      "[Training Epoch 0] Batch 2242, Loss 0.31370070576667786\n",
      "[Training Epoch 0] Batch 2243, Loss 0.34571611881256104\n",
      "[Training Epoch 0] Batch 2244, Loss 0.31435224413871765\n",
      "[Training Epoch 0] Batch 2245, Loss 0.33561718463897705\n",
      "[Training Epoch 0] Batch 2246, Loss 0.33238205313682556\n",
      "[Training Epoch 0] Batch 2247, Loss 0.322383850812912\n",
      "[Training Epoch 0] Batch 2248, Loss 0.3577975332736969\n",
      "[Training Epoch 0] Batch 2249, Loss 0.3057559132575989\n",
      "[Training Epoch 0] Batch 2250, Loss 0.34188827872276306\n",
      "[Training Epoch 0] Batch 2251, Loss 0.3325149416923523\n",
      "[Training Epoch 0] Batch 2252, Loss 0.31144633889198303\n",
      "[Training Epoch 0] Batch 2253, Loss 0.32881584763526917\n",
      "[Training Epoch 0] Batch 2254, Loss 0.3454033136367798\n",
      "[Training Epoch 0] Batch 2255, Loss 0.3496401906013489\n",
      "[Training Epoch 0] Batch 2256, Loss 0.32432523369789124\n",
      "[Training Epoch 0] Batch 2257, Loss 0.3405340909957886\n",
      "[Training Epoch 0] Batch 2258, Loss 0.3085477352142334\n",
      "[Training Epoch 0] Batch 2259, Loss 0.37965500354766846\n",
      "[Training Epoch 0] Batch 2260, Loss 0.3253489136695862\n",
      "[Training Epoch 0] Batch 2261, Loss 0.3128540515899658\n",
      "[Training Epoch 0] Batch 2262, Loss 0.3540685772895813\n",
      "[Training Epoch 0] Batch 2263, Loss 0.3538813591003418\n",
      "[Training Epoch 0] Batch 2264, Loss 0.3411591649055481\n",
      "[Training Epoch 0] Batch 2265, Loss 0.2954825162887573\n",
      "[Training Epoch 0] Batch 2266, Loss 0.32691240310668945\n",
      "[Training Epoch 0] Batch 2267, Loss 0.33981412649154663\n",
      "[Training Epoch 0] Batch 2268, Loss 0.3348606526851654\n",
      "[Training Epoch 0] Batch 2269, Loss 0.35843461751937866\n",
      "[Training Epoch 0] Batch 2270, Loss 0.30076727271080017\n",
      "[Training Epoch 0] Batch 2271, Loss 0.32875537872314453\n",
      "[Training Epoch 0] Batch 2272, Loss 0.3361433148384094\n",
      "[Training Epoch 0] Batch 2273, Loss 0.35588836669921875\n",
      "[Training Epoch 0] Batch 2274, Loss 0.35131561756134033\n",
      "[Training Epoch 0] Batch 2275, Loss 0.31339603662490845\n",
      "[Training Epoch 0] Batch 2276, Loss 0.33205458521842957\n",
      "[Training Epoch 0] Batch 2277, Loss 0.32842734456062317\n",
      "[Training Epoch 0] Batch 2278, Loss 0.3306129276752472\n",
      "[Training Epoch 0] Batch 2279, Loss 0.3384004831314087\n",
      "[Training Epoch 0] Batch 2280, Loss 0.3207034468650818\n",
      "[Training Epoch 0] Batch 2281, Loss 0.35429149866104126\n",
      "[Training Epoch 0] Batch 2282, Loss 0.3493462800979614\n",
      "[Training Epoch 0] Batch 2283, Loss 0.3285464644432068\n",
      "[Training Epoch 0] Batch 2284, Loss 0.339866042137146\n",
      "[Training Epoch 0] Batch 2285, Loss 0.35264408588409424\n",
      "[Training Epoch 0] Batch 2286, Loss 0.3286561965942383\n",
      "[Training Epoch 0] Batch 2287, Loss 0.3119399845600128\n",
      "[Training Epoch 0] Batch 2288, Loss 0.34855329990386963\n",
      "[Training Epoch 0] Batch 2289, Loss 0.2976949214935303\n",
      "[Training Epoch 0] Batch 2290, Loss 0.3071722388267517\n",
      "[Training Epoch 0] Batch 2291, Loss 0.3361646831035614\n",
      "[Training Epoch 0] Batch 2292, Loss 0.3506547808647156\n",
      "[Training Epoch 0] Batch 2293, Loss 0.31988465785980225\n",
      "[Training Epoch 0] Batch 2294, Loss 0.3472852110862732\n",
      "[Training Epoch 0] Batch 2295, Loss 0.33739081025123596\n",
      "[Training Epoch 0] Batch 2296, Loss 0.3254460096359253\n",
      "[Training Epoch 0] Batch 2297, Loss 0.344679057598114\n",
      "[Training Epoch 0] Batch 2298, Loss 0.3288467526435852\n",
      "[Training Epoch 0] Batch 2299, Loss 0.33948570489883423\n",
      "[Training Epoch 0] Batch 2300, Loss 0.3448752164840698\n",
      "[Training Epoch 0] Batch 2301, Loss 0.2906164228916168\n",
      "[Training Epoch 0] Batch 2302, Loss 0.3315613865852356\n",
      "[Training Epoch 0] Batch 2303, Loss 0.32621073722839355\n",
      "[Training Epoch 0] Batch 2304, Loss 0.33261892199516296\n",
      "[Training Epoch 0] Batch 2305, Loss 0.3360668420791626\n",
      "[Training Epoch 0] Batch 2306, Loss 0.3352593779563904\n",
      "[Training Epoch 0] Batch 2307, Loss 0.3249810039997101\n",
      "[Training Epoch 0] Batch 2308, Loss 0.31741946935653687\n",
      "[Training Epoch 0] Batch 2309, Loss 0.3581139147281647\n",
      "[Training Epoch 0] Batch 2310, Loss 0.3363220989704132\n",
      "[Training Epoch 0] Batch 2311, Loss 0.35613465309143066\n",
      "[Training Epoch 0] Batch 2312, Loss 0.3222385346889496\n",
      "[Training Epoch 0] Batch 2313, Loss 0.3297705352306366\n",
      "[Training Epoch 0] Batch 2314, Loss 0.3848320543766022\n",
      "[Training Epoch 0] Batch 2315, Loss 0.34009480476379395\n",
      "[Training Epoch 0] Batch 2316, Loss 0.32302021980285645\n",
      "[Training Epoch 0] Batch 2317, Loss 0.3400782346725464\n",
      "[Training Epoch 0] Batch 2318, Loss 0.32341268658638\n",
      "[Training Epoch 0] Batch 2319, Loss 0.32699158787727356\n",
      "[Training Epoch 0] Batch 2320, Loss 0.3400183320045471\n",
      "[Training Epoch 0] Batch 2321, Loss 0.3323265314102173\n",
      "[Training Epoch 0] Batch 2322, Loss 0.32011061906814575\n",
      "[Training Epoch 0] Batch 2323, Loss 0.33238840103149414\n",
      "[Training Epoch 0] Batch 2324, Loss 0.330135703086853\n",
      "[Training Epoch 0] Batch 2325, Loss 0.351851224899292\n",
      "[Training Epoch 0] Batch 2326, Loss 0.3256048560142517\n",
      "[Training Epoch 0] Batch 2327, Loss 0.32031065225601196\n",
      "[Training Epoch 0] Batch 2328, Loss 0.33053967356681824\n",
      "[Training Epoch 0] Batch 2329, Loss 0.3376673460006714\n",
      "[Training Epoch 0] Batch 2330, Loss 0.3490108251571655\n",
      "[Training Epoch 0] Batch 2331, Loss 0.3404226303100586\n",
      "[Training Epoch 0] Batch 2332, Loss 0.3222643733024597\n",
      "[Training Epoch 0] Batch 2333, Loss 0.31969016790390015\n",
      "[Training Epoch 0] Batch 2334, Loss 0.31042763590812683\n",
      "[Training Epoch 0] Batch 2335, Loss 0.33179813623428345\n",
      "[Training Epoch 0] Batch 2336, Loss 0.3241662383079529\n",
      "[Training Epoch 0] Batch 2337, Loss 0.3207959532737732\n",
      "[Training Epoch 0] Batch 2338, Loss 0.30438271164894104\n",
      "[Training Epoch 0] Batch 2339, Loss 0.32266587018966675\n",
      "[Training Epoch 0] Batch 2340, Loss 0.3599078059196472\n",
      "[Training Epoch 0] Batch 2341, Loss 0.3681376576423645\n",
      "[Training Epoch 0] Batch 2342, Loss 0.33731454610824585\n",
      "[Training Epoch 0] Batch 2343, Loss 0.3459950089454651\n",
      "[Training Epoch 0] Batch 2344, Loss 0.30104100704193115\n",
      "[Training Epoch 0] Batch 2345, Loss 0.32667967677116394\n",
      "[Training Epoch 0] Batch 2346, Loss 0.30805057287216187\n",
      "[Training Epoch 0] Batch 2347, Loss 0.32236286997795105\n",
      "[Training Epoch 0] Batch 2348, Loss 0.347057580947876\n",
      "[Training Epoch 0] Batch 2349, Loss 0.3110266327857971\n",
      "[Training Epoch 0] Batch 2350, Loss 0.3134554624557495\n",
      "[Training Epoch 0] Batch 2351, Loss 0.32682478427886963\n",
      "[Training Epoch 0] Batch 2352, Loss 0.32382631301879883\n",
      "[Training Epoch 0] Batch 2353, Loss 0.32378172874450684\n",
      "[Training Epoch 0] Batch 2354, Loss 0.3427170515060425\n",
      "[Training Epoch 0] Batch 2355, Loss 0.3449018597602844\n",
      "[Training Epoch 0] Batch 2356, Loss 0.33993959426879883\n",
      "[Training Epoch 0] Batch 2357, Loss 0.36329230666160583\n",
      "[Training Epoch 0] Batch 2358, Loss 0.35083192586898804\n",
      "[Training Epoch 0] Batch 2359, Loss 0.315118670463562\n",
      "[Training Epoch 0] Batch 2360, Loss 0.32648080587387085\n",
      "[Training Epoch 0] Batch 2361, Loss 0.3650388717651367\n",
      "[Training Epoch 0] Batch 2362, Loss 0.3435937762260437\n",
      "[Training Epoch 0] Batch 2363, Loss 0.32417452335357666\n",
      "[Training Epoch 0] Batch 2364, Loss 0.32786667346954346\n",
      "[Training Epoch 0] Batch 2365, Loss 0.33941733837127686\n",
      "[Training Epoch 0] Batch 2366, Loss 0.2823818325996399\n",
      "[Training Epoch 0] Batch 2367, Loss 0.34909671545028687\n",
      "[Training Epoch 0] Batch 2368, Loss 0.35736292600631714\n",
      "[Training Epoch 0] Batch 2369, Loss 0.32445141673088074\n",
      "[Training Epoch 0] Batch 2370, Loss 0.3383631110191345\n",
      "[Training Epoch 0] Batch 2371, Loss 0.3246685862541199\n",
      "[Training Epoch 0] Batch 2372, Loss 0.32171303033828735\n",
      "[Training Epoch 0] Batch 2373, Loss 0.36647310853004456\n",
      "[Training Epoch 0] Batch 2374, Loss 0.3203158378601074\n",
      "[Training Epoch 0] Batch 2375, Loss 0.2965439558029175\n",
      "[Training Epoch 0] Batch 2376, Loss 0.3459068238735199\n",
      "[Training Epoch 0] Batch 2377, Loss 0.31413963437080383\n",
      "[Training Epoch 0] Batch 2378, Loss 0.31476008892059326\n",
      "[Training Epoch 0] Batch 2379, Loss 0.3179681897163391\n",
      "[Training Epoch 0] Batch 2380, Loss 0.34049510955810547\n",
      "[Training Epoch 0] Batch 2381, Loss 0.3346335291862488\n",
      "[Training Epoch 0] Batch 2382, Loss 0.32113614678382874\n",
      "[Training Epoch 0] Batch 2383, Loss 0.29813718795776367\n",
      "[Training Epoch 0] Batch 2384, Loss 0.3173936605453491\n",
      "[Training Epoch 0] Batch 2385, Loss 0.2863035202026367\n",
      "[Training Epoch 0] Batch 2386, Loss 0.3357575535774231\n",
      "[Training Epoch 0] Batch 2387, Loss 0.31614089012145996\n",
      "[Training Epoch 0] Batch 2388, Loss 0.28993070125579834\n",
      "[Training Epoch 0] Batch 2389, Loss 0.3144676089286804\n",
      "[Training Epoch 0] Batch 2390, Loss 0.36526140570640564\n",
      "[Training Epoch 0] Batch 2391, Loss 0.3477121889591217\n",
      "[Training Epoch 0] Batch 2392, Loss 0.3317388892173767\n",
      "[Training Epoch 0] Batch 2393, Loss 0.35246092081069946\n",
      "[Training Epoch 0] Batch 2394, Loss 0.3305722177028656\n",
      "[Training Epoch 0] Batch 2395, Loss 0.32915061712265015\n",
      "[Training Epoch 0] Batch 2396, Loss 0.3591814339160919\n",
      "[Training Epoch 0] Batch 2397, Loss 0.36810845136642456\n",
      "[Training Epoch 0] Batch 2398, Loss 0.32848596572875977\n",
      "[Training Epoch 0] Batch 2399, Loss 0.3248289227485657\n",
      "[Training Epoch 0] Batch 2400, Loss 0.33382436633110046\n",
      "[Training Epoch 0] Batch 2401, Loss 0.3258029818534851\n",
      "[Training Epoch 0] Batch 2402, Loss 0.33954399824142456\n",
      "[Training Epoch 0] Batch 2403, Loss 0.31927359104156494\n",
      "[Training Epoch 0] Batch 2404, Loss 0.31017282605171204\n",
      "[Training Epoch 0] Batch 2405, Loss 0.3257243037223816\n",
      "[Training Epoch 0] Batch 2406, Loss 0.3249257802963257\n",
      "[Training Epoch 0] Batch 2407, Loss 0.3300369381904602\n",
      "[Training Epoch 0] Batch 2408, Loss 0.34062132239341736\n",
      "[Training Epoch 0] Batch 2409, Loss 0.35292375087738037\n",
      "[Training Epoch 0] Batch 2410, Loss 0.32029837369918823\n",
      "[Training Epoch 0] Batch 2411, Loss 0.3435781002044678\n",
      "[Training Epoch 0] Batch 2412, Loss 0.3308156728744507\n",
      "[Training Epoch 0] Batch 2413, Loss 0.3206061124801636\n",
      "[Training Epoch 0] Batch 2414, Loss 0.320773720741272\n",
      "[Training Epoch 0] Batch 2415, Loss 0.3462521731853485\n",
      "[Training Epoch 0] Batch 2416, Loss 0.3240419030189514\n",
      "[Training Epoch 0] Batch 2417, Loss 0.3174973726272583\n",
      "[Training Epoch 0] Batch 2418, Loss 0.35295289754867554\n",
      "[Training Epoch 0] Batch 2419, Loss 0.33382749557495117\n",
      "[Training Epoch 0] Batch 2420, Loss 0.3182939887046814\n",
      "[Training Epoch 0] Batch 2421, Loss 0.3127294182777405\n",
      "[Training Epoch 0] Batch 2422, Loss 0.35269150137901306\n",
      "[Training Epoch 0] Batch 2423, Loss 0.32611700892448425\n",
      "[Training Epoch 0] Batch 2424, Loss 0.3399154841899872\n",
      "[Training Epoch 0] Batch 2425, Loss 0.3233329951763153\n",
      "[Training Epoch 0] Batch 2426, Loss 0.3211497664451599\n",
      "[Training Epoch 0] Batch 2427, Loss 0.3144958019256592\n",
      "[Training Epoch 0] Batch 2428, Loss 0.3246777057647705\n",
      "[Training Epoch 0] Batch 2429, Loss 0.3732529282569885\n",
      "[Training Epoch 0] Batch 2430, Loss 0.32791846990585327\n",
      "[Training Epoch 0] Batch 2431, Loss 0.3341987133026123\n",
      "[Training Epoch 0] Batch 2432, Loss 0.3133954405784607\n",
      "[Training Epoch 0] Batch 2433, Loss 0.3264997601509094\n",
      "[Training Epoch 0] Batch 2434, Loss 0.3310156464576721\n",
      "[Training Epoch 0] Batch 2435, Loss 0.3403947353363037\n",
      "[Training Epoch 0] Batch 2436, Loss 0.31137514114379883\n",
      "[Training Epoch 0] Batch 2437, Loss 0.2947656810283661\n",
      "[Training Epoch 0] Batch 2438, Loss 0.32006680965423584\n",
      "[Training Epoch 0] Batch 2439, Loss 0.3133556842803955\n",
      "[Training Epoch 0] Batch 2440, Loss 0.3297017812728882\n",
      "[Training Epoch 0] Batch 2441, Loss 0.32318371534347534\n",
      "[Training Epoch 0] Batch 2442, Loss 0.3337700366973877\n",
      "[Training Epoch 0] Batch 2443, Loss 0.3431239128112793\n",
      "[Training Epoch 0] Batch 2444, Loss 0.31749227643013\n",
      "[Training Epoch 0] Batch 2445, Loss 0.31528985500335693\n",
      "[Training Epoch 0] Batch 2446, Loss 0.3094213902950287\n",
      "[Training Epoch 0] Batch 2447, Loss 0.34306418895721436\n",
      "[Training Epoch 0] Batch 2448, Loss 0.34291839599609375\n",
      "[Training Epoch 0] Batch 2449, Loss 0.3158966898918152\n",
      "[Training Epoch 0] Batch 2450, Loss 0.34379348158836365\n",
      "[Training Epoch 0] Batch 2451, Loss 0.3728952407836914\n",
      "[Training Epoch 0] Batch 2452, Loss 0.33816131949424744\n",
      "[Training Epoch 0] Batch 2453, Loss 0.35534244775772095\n",
      "[Training Epoch 0] Batch 2454, Loss 0.35568392276763916\n",
      "[Training Epoch 0] Batch 2455, Loss 0.3197048306465149\n",
      "[Training Epoch 0] Batch 2456, Loss 0.3364105224609375\n",
      "[Training Epoch 0] Batch 2457, Loss 0.36004722118377686\n",
      "[Training Epoch 0] Batch 2458, Loss 0.3416273593902588\n",
      "[Training Epoch 0] Batch 2459, Loss 0.3462715446949005\n",
      "[Training Epoch 0] Batch 2460, Loss 0.32249706983566284\n",
      "[Training Epoch 0] Batch 2461, Loss 0.32395970821380615\n",
      "[Training Epoch 0] Batch 2462, Loss 0.3046041429042816\n",
      "[Training Epoch 0] Batch 2463, Loss 0.31862014532089233\n",
      "[Training Epoch 0] Batch 2464, Loss 0.3250829577445984\n",
      "[Training Epoch 0] Batch 2465, Loss 0.32477307319641113\n",
      "[Training Epoch 0] Batch 2466, Loss 0.3540366291999817\n",
      "[Training Epoch 0] Batch 2467, Loss 0.31963926553726196\n",
      "[Training Epoch 0] Batch 2468, Loss 0.31132903695106506\n",
      "[Training Epoch 0] Batch 2469, Loss 0.3206394910812378\n",
      "[Training Epoch 0] Batch 2470, Loss 0.32060134410858154\n",
      "[Training Epoch 0] Batch 2471, Loss 0.3521016240119934\n",
      "[Training Epoch 0] Batch 2472, Loss 0.3490521013736725\n",
      "[Training Epoch 0] Batch 2473, Loss 0.32478946447372437\n",
      "[Training Epoch 0] Batch 2474, Loss 0.32838621735572815\n",
      "[Training Epoch 0] Batch 2475, Loss 0.342115581035614\n",
      "[Training Epoch 0] Batch 2476, Loss 0.3322575092315674\n",
      "[Training Epoch 0] Batch 2477, Loss 0.2795054316520691\n",
      "[Training Epoch 0] Batch 2478, Loss 0.3360621929168701\n",
      "[Training Epoch 0] Batch 2479, Loss 0.3107530474662781\n",
      "[Training Epoch 0] Batch 2480, Loss 0.3104155957698822\n",
      "[Training Epoch 0] Batch 2481, Loss 0.3222620487213135\n",
      "[Training Epoch 0] Batch 2482, Loss 0.32268667221069336\n",
      "[Training Epoch 0] Batch 2483, Loss 0.3266470730304718\n",
      "[Training Epoch 0] Batch 2484, Loss 0.3214878439903259\n",
      "[Training Epoch 0] Batch 2485, Loss 0.33490070700645447\n",
      "[Training Epoch 0] Batch 2486, Loss 0.30206039547920227\n",
      "[Training Epoch 0] Batch 2487, Loss 0.33231520652770996\n",
      "[Training Epoch 0] Batch 2488, Loss 0.3251103162765503\n",
      "[Training Epoch 0] Batch 2489, Loss 0.3136807680130005\n",
      "[Training Epoch 0] Batch 2490, Loss 0.32649973034858704\n",
      "[Training Epoch 0] Batch 2491, Loss 0.3298525810241699\n",
      "[Training Epoch 0] Batch 2492, Loss 0.346744179725647\n",
      "[Training Epoch 0] Batch 2493, Loss 0.32284820079803467\n",
      "[Training Epoch 0] Batch 2494, Loss 0.32424265146255493\n",
      "[Training Epoch 0] Batch 2495, Loss 0.3283591866493225\n",
      "[Training Epoch 0] Batch 2496, Loss 0.30654430389404297\n",
      "[Training Epoch 0] Batch 2497, Loss 0.32509350776672363\n",
      "[Training Epoch 0] Batch 2498, Loss 0.31548139452934265\n",
      "[Training Epoch 0] Batch 2499, Loss 0.31320738792419434\n",
      "[Training Epoch 0] Batch 2500, Loss 0.35319983959198\n",
      "[Training Epoch 0] Batch 2501, Loss 0.3164898753166199\n",
      "[Training Epoch 0] Batch 2502, Loss 0.330996572971344\n",
      "[Training Epoch 0] Batch 2503, Loss 0.3146202564239502\n",
      "[Training Epoch 0] Batch 2504, Loss 0.3272431790828705\n",
      "[Training Epoch 0] Batch 2505, Loss 0.34246161580085754\n",
      "[Training Epoch 0] Batch 2506, Loss 0.33510035276412964\n",
      "[Training Epoch 0] Batch 2507, Loss 0.3016963005065918\n",
      "[Training Epoch 0] Batch 2508, Loss 0.3034343421459198\n",
      "[Training Epoch 0] Batch 2509, Loss 0.33538374304771423\n",
      "[Training Epoch 0] Batch 2510, Loss 0.31393975019454956\n",
      "[Training Epoch 0] Batch 2511, Loss 0.34010159969329834\n",
      "[Training Epoch 0] Batch 2512, Loss 0.3188450336456299\n",
      "[Training Epoch 0] Batch 2513, Loss 0.35158878564834595\n",
      "[Training Epoch 0] Batch 2514, Loss 0.3394532799720764\n",
      "[Training Epoch 0] Batch 2515, Loss 0.30734193325042725\n",
      "[Training Epoch 0] Batch 2516, Loss 0.3166011869907379\n",
      "[Training Epoch 0] Batch 2517, Loss 0.33123090863227844\n",
      "[Training Epoch 0] Batch 2518, Loss 0.338074266910553\n",
      "[Training Epoch 0] Batch 2519, Loss 0.3276180326938629\n",
      "[Training Epoch 0] Batch 2520, Loss 0.2971508502960205\n",
      "[Training Epoch 0] Batch 2521, Loss 0.3644143044948578\n",
      "[Training Epoch 0] Batch 2522, Loss 0.3544926047325134\n",
      "[Training Epoch 0] Batch 2523, Loss 0.327986478805542\n",
      "[Training Epoch 0] Batch 2524, Loss 0.3180088996887207\n",
      "[Training Epoch 0] Batch 2525, Loss 0.34088897705078125\n",
      "[Training Epoch 0] Batch 2526, Loss 0.3078596889972687\n",
      "[Training Epoch 0] Batch 2527, Loss 0.3053942918777466\n",
      "[Training Epoch 0] Batch 2528, Loss 0.30908507108688354\n",
      "[Training Epoch 0] Batch 2529, Loss 0.32951509952545166\n",
      "[Training Epoch 0] Batch 2530, Loss 0.3154563009738922\n",
      "[Training Epoch 0] Batch 2531, Loss 0.32849106192588806\n",
      "[Training Epoch 0] Batch 2532, Loss 0.3334693908691406\n",
      "[Training Epoch 0] Batch 2533, Loss 0.3300555348396301\n",
      "[Training Epoch 0] Batch 2534, Loss 0.345919668674469\n",
      "[Training Epoch 0] Batch 2535, Loss 0.3332165479660034\n",
      "[Training Epoch 0] Batch 2536, Loss 0.33310529589653015\n",
      "[Training Epoch 0] Batch 2537, Loss 0.31874820590019226\n",
      "[Training Epoch 0] Batch 2538, Loss 0.3037679195404053\n",
      "[Training Epoch 0] Batch 2539, Loss 0.33054596185684204\n",
      "[Training Epoch 0] Batch 2540, Loss 0.3449808955192566\n",
      "[Training Epoch 0] Batch 2541, Loss 0.35989686846733093\n",
      "[Training Epoch 0] Batch 2542, Loss 0.31341591477394104\n",
      "[Training Epoch 0] Batch 2543, Loss 0.34362196922302246\n",
      "[Training Epoch 0] Batch 2544, Loss 0.2942495048046112\n",
      "[Training Epoch 0] Batch 2545, Loss 0.29915815591812134\n",
      "[Training Epoch 0] Batch 2546, Loss 0.3388848304748535\n",
      "[Training Epoch 0] Batch 2547, Loss 0.3434610366821289\n",
      "[Training Epoch 0] Batch 2548, Loss 0.33310991525650024\n",
      "[Training Epoch 0] Batch 2549, Loss 0.31884902715682983\n",
      "[Training Epoch 0] Batch 2550, Loss 0.30784422159194946\n",
      "[Training Epoch 0] Batch 2551, Loss 0.31054598093032837\n",
      "[Training Epoch 0] Batch 2552, Loss 0.3342188000679016\n",
      "[Training Epoch 0] Batch 2553, Loss 0.3352591395378113\n",
      "[Training Epoch 0] Batch 2554, Loss 0.3059455156326294\n",
      "[Training Epoch 0] Batch 2555, Loss 0.3181178569793701\n",
      "[Training Epoch 0] Batch 2556, Loss 0.3149929642677307\n",
      "[Training Epoch 0] Batch 2557, Loss 0.3372557461261749\n",
      "[Training Epoch 0] Batch 2558, Loss 0.3397204279899597\n",
      "[Training Epoch 0] Batch 2559, Loss 0.3545818626880646\n",
      "[Training Epoch 0] Batch 2560, Loss 0.3065119981765747\n",
      "[Training Epoch 0] Batch 2561, Loss 0.33952635526657104\n",
      "[Training Epoch 0] Batch 2562, Loss 0.33438295125961304\n",
      "[Training Epoch 0] Batch 2563, Loss 0.29881447553634644\n",
      "[Training Epoch 0] Batch 2564, Loss 0.3521628975868225\n",
      "[Training Epoch 0] Batch 2565, Loss 0.317493736743927\n",
      "[Training Epoch 0] Batch 2566, Loss 0.3359977602958679\n",
      "[Training Epoch 0] Batch 2567, Loss 0.3245633840560913\n",
      "[Training Epoch 0] Batch 2568, Loss 0.3253088891506195\n",
      "[Training Epoch 0] Batch 2569, Loss 0.31945329904556274\n",
      "[Training Epoch 0] Batch 2570, Loss 0.3381878137588501\n",
      "[Training Epoch 0] Batch 2571, Loss 0.3260415196418762\n",
      "[Training Epoch 0] Batch 2572, Loss 0.3228178322315216\n",
      "[Training Epoch 0] Batch 2573, Loss 0.3523913025856018\n",
      "[Training Epoch 0] Batch 2574, Loss 0.3272985816001892\n",
      "[Training Epoch 0] Batch 2575, Loss 0.3606519103050232\n",
      "[Training Epoch 0] Batch 2576, Loss 0.3359171748161316\n",
      "[Training Epoch 0] Batch 2577, Loss 0.3423398435115814\n",
      "[Training Epoch 0] Batch 2578, Loss 0.31465816497802734\n",
      "[Training Epoch 0] Batch 2579, Loss 0.33596283197402954\n",
      "[Training Epoch 0] Batch 2580, Loss 0.3125600516796112\n",
      "[Training Epoch 0] Batch 2581, Loss 0.3214443325996399\n",
      "[Training Epoch 0] Batch 2582, Loss 0.3235417604446411\n",
      "[Training Epoch 0] Batch 2583, Loss 0.32662931084632874\n",
      "[Training Epoch 0] Batch 2584, Loss 0.33103781938552856\n",
      "[Training Epoch 0] Batch 2585, Loss 0.3058919608592987\n",
      "[Training Epoch 0] Batch 2586, Loss 0.3406919240951538\n",
      "[Training Epoch 0] Batch 2587, Loss 0.34269505739212036\n",
      "[Training Epoch 0] Batch 2588, Loss 0.347140371799469\n",
      "[Training Epoch 0] Batch 2589, Loss 0.33139288425445557\n",
      "[Training Epoch 0] Batch 2590, Loss 0.2981199622154236\n",
      "[Training Epoch 0] Batch 2591, Loss 0.3248751163482666\n",
      "[Training Epoch 0] Batch 2592, Loss 0.32768866419792175\n",
      "[Training Epoch 0] Batch 2593, Loss 0.3386583924293518\n",
      "[Training Epoch 0] Batch 2594, Loss 0.3459417521953583\n",
      "[Training Epoch 0] Batch 2595, Loss 0.32365551590919495\n",
      "[Training Epoch 0] Batch 2596, Loss 0.3345993757247925\n",
      "[Training Epoch 0] Batch 2597, Loss 0.30127790570259094\n",
      "[Training Epoch 0] Batch 2598, Loss 0.3383789360523224\n",
      "[Training Epoch 0] Batch 2599, Loss 0.3361499309539795\n",
      "[Training Epoch 0] Batch 2600, Loss 0.3129909336566925\n",
      "[Training Epoch 0] Batch 2601, Loss 0.3470708727836609\n",
      "[Training Epoch 0] Batch 2602, Loss 0.3334358334541321\n",
      "[Training Epoch 0] Batch 2603, Loss 0.31478410959243774\n",
      "[Training Epoch 0] Batch 2604, Loss 0.33340680599212646\n",
      "[Training Epoch 0] Batch 2605, Loss 0.31769245862960815\n",
      "[Training Epoch 0] Batch 2606, Loss 0.3137383460998535\n",
      "[Training Epoch 0] Batch 2607, Loss 0.3280322551727295\n",
      "[Training Epoch 0] Batch 2608, Loss 0.3711029291152954\n",
      "[Training Epoch 0] Batch 2609, Loss 0.30683356523513794\n",
      "[Training Epoch 0] Batch 2610, Loss 0.34121304750442505\n",
      "[Training Epoch 0] Batch 2611, Loss 0.32683488726615906\n",
      "[Training Epoch 0] Batch 2612, Loss 0.33689481019973755\n",
      "[Training Epoch 0] Batch 2613, Loss 0.3174734115600586\n",
      "[Training Epoch 0] Batch 2614, Loss 0.2908596694469452\n",
      "[Training Epoch 0] Batch 2615, Loss 0.3427658975124359\n",
      "[Training Epoch 0] Batch 2616, Loss 0.3261745870113373\n",
      "[Training Epoch 0] Batch 2617, Loss 0.3126443326473236\n",
      "[Training Epoch 0] Batch 2618, Loss 0.314059317111969\n",
      "[Training Epoch 0] Batch 2619, Loss 0.3194235563278198\n",
      "[Training Epoch 0] Batch 2620, Loss 0.3440168797969818\n",
      "[Training Epoch 0] Batch 2621, Loss 0.3078361749649048\n",
      "[Training Epoch 0] Batch 2622, Loss 0.33124840259552\n",
      "[Training Epoch 0] Batch 2623, Loss 0.3274914622306824\n",
      "[Training Epoch 0] Batch 2624, Loss 0.30804532766342163\n",
      "[Training Epoch 0] Batch 2625, Loss 0.31767183542251587\n",
      "[Training Epoch 0] Batch 2626, Loss 0.33209285140037537\n",
      "[Training Epoch 0] Batch 2627, Loss 0.3326394855976105\n",
      "[Training Epoch 0] Batch 2628, Loss 0.3096955120563507\n",
      "[Training Epoch 0] Batch 2629, Loss 0.31813371181488037\n",
      "[Training Epoch 0] Batch 2630, Loss 0.3373333215713501\n",
      "[Training Epoch 0] Batch 2631, Loss 0.324617475271225\n",
      "[Training Epoch 0] Batch 2632, Loss 0.32958751916885376\n",
      "[Training Epoch 0] Batch 2633, Loss 0.33363160490989685\n",
      "[Training Epoch 0] Batch 2634, Loss 0.35442984104156494\n",
      "[Training Epoch 0] Batch 2635, Loss 0.3219549357891083\n",
      "[Training Epoch 0] Batch 2636, Loss 0.33464205265045166\n",
      "[Training Epoch 0] Batch 2637, Loss 0.3223159909248352\n",
      "[Training Epoch 0] Batch 2638, Loss 0.33186036348342896\n",
      "[Training Epoch 0] Batch 2639, Loss 0.3154750466346741\n",
      "[Training Epoch 0] Batch 2640, Loss 0.3427823483943939\n",
      "[Training Epoch 0] Batch 2641, Loss 0.3032758831977844\n",
      "[Training Epoch 0] Batch 2642, Loss 0.32449042797088623\n",
      "[Training Epoch 0] Batch 2643, Loss 0.32951676845550537\n",
      "[Training Epoch 0] Batch 2644, Loss 0.3032483458518982\n",
      "[Training Epoch 0] Batch 2645, Loss 0.3303288519382477\n",
      "[Training Epoch 0] Batch 2646, Loss 0.3101658225059509\n",
      "[Training Epoch 0] Batch 2647, Loss 0.32553333044052124\n",
      "[Training Epoch 0] Batch 2648, Loss 0.29500219225883484\n",
      "[Training Epoch 0] Batch 2649, Loss 0.3219892978668213\n",
      "[Training Epoch 0] Batch 2650, Loss 0.3562650680541992\n",
      "[Training Epoch 0] Batch 2651, Loss 0.2856842875480652\n",
      "[Training Epoch 0] Batch 2652, Loss 0.3489643633365631\n",
      "[Training Epoch 0] Batch 2653, Loss 0.3205794394016266\n",
      "[Training Epoch 0] Batch 2654, Loss 0.31023576855659485\n",
      "[Training Epoch 0] Batch 2655, Loss 0.3342011868953705\n",
      "[Training Epoch 0] Batch 2656, Loss 0.31747543811798096\n",
      "[Training Epoch 0] Batch 2657, Loss 0.32192090153694153\n",
      "[Training Epoch 0] Batch 2658, Loss 0.3203471899032593\n",
      "[Training Epoch 0] Batch 2659, Loss 0.34547096490859985\n",
      "[Training Epoch 0] Batch 2660, Loss 0.31268495321273804\n",
      "[Training Epoch 0] Batch 2661, Loss 0.3599015176296234\n",
      "[Training Epoch 0] Batch 2662, Loss 0.3349931240081787\n",
      "[Training Epoch 0] Batch 2663, Loss 0.3506806790828705\n",
      "[Training Epoch 0] Batch 2664, Loss 0.33620697259902954\n",
      "[Training Epoch 0] Batch 2665, Loss 0.331373929977417\n",
      "[Training Epoch 0] Batch 2666, Loss 0.3173076808452606\n",
      "[Training Epoch 0] Batch 2667, Loss 0.3369196653366089\n",
      "[Training Epoch 0] Batch 2668, Loss 0.32946842908859253\n",
      "[Training Epoch 0] Batch 2669, Loss 0.3291616439819336\n",
      "[Training Epoch 0] Batch 2670, Loss 0.3405977785587311\n",
      "[Training Epoch 0] Batch 2671, Loss 0.3186621069908142\n",
      "[Training Epoch 0] Batch 2672, Loss 0.31426799297332764\n",
      "[Training Epoch 0] Batch 2673, Loss 0.3118089735507965\n",
      "[Training Epoch 0] Batch 2674, Loss 0.3111572861671448\n",
      "[Training Epoch 0] Batch 2675, Loss 0.31474947929382324\n",
      "[Training Epoch 0] Batch 2676, Loss 0.3481709957122803\n",
      "[Training Epoch 0] Batch 2677, Loss 0.3207724094390869\n",
      "[Training Epoch 0] Batch 2678, Loss 0.3470500111579895\n",
      "[Training Epoch 0] Batch 2679, Loss 0.3258311152458191\n",
      "[Training Epoch 0] Batch 2680, Loss 0.35558533668518066\n",
      "[Training Epoch 0] Batch 2681, Loss 0.2985861897468567\n",
      "[Training Epoch 0] Batch 2682, Loss 0.31254440546035767\n",
      "[Training Epoch 0] Batch 2683, Loss 0.3427204489707947\n",
      "[Training Epoch 0] Batch 2684, Loss 0.33191418647766113\n",
      "[Training Epoch 0] Batch 2685, Loss 0.3508533835411072\n",
      "[Training Epoch 0] Batch 2686, Loss 0.3191037178039551\n",
      "[Training Epoch 0] Batch 2687, Loss 0.3133407235145569\n",
      "[Training Epoch 0] Batch 2688, Loss 0.354705810546875\n",
      "[Training Epoch 0] Batch 2689, Loss 0.3108659088611603\n",
      "[Training Epoch 0] Batch 2690, Loss 0.34695762395858765\n",
      "[Training Epoch 0] Batch 2691, Loss 0.3360624611377716\n",
      "[Training Epoch 0] Batch 2692, Loss 0.32668444514274597\n",
      "[Training Epoch 0] Batch 2693, Loss 0.33347535133361816\n",
      "[Training Epoch 0] Batch 2694, Loss 0.3122282028198242\n",
      "[Training Epoch 0] Batch 2695, Loss 0.30839505791664124\n",
      "[Training Epoch 0] Batch 2696, Loss 0.34027835726737976\n",
      "[Training Epoch 0] Batch 2697, Loss 0.33594003319740295\n",
      "[Training Epoch 0] Batch 2698, Loss 0.31839150190353394\n",
      "[Training Epoch 0] Batch 2699, Loss 0.29434633255004883\n",
      "[Training Epoch 0] Batch 2700, Loss 0.33993154764175415\n",
      "[Training Epoch 0] Batch 2701, Loss 0.3357096314430237\n",
      "[Training Epoch 0] Batch 2702, Loss 0.3585912585258484\n",
      "[Training Epoch 0] Batch 2703, Loss 0.30161842703819275\n",
      "[Training Epoch 0] Batch 2704, Loss 0.3141076862812042\n",
      "[Training Epoch 0] Batch 2705, Loss 0.3324563503265381\n",
      "[Training Epoch 0] Batch 2706, Loss 0.33452922105789185\n",
      "[Training Epoch 0] Batch 2707, Loss 0.34574517607688904\n",
      "[Training Epoch 0] Batch 2708, Loss 0.30668914318084717\n",
      "[Training Epoch 0] Batch 2709, Loss 0.297699511051178\n",
      "[Training Epoch 0] Batch 2710, Loss 0.3155195116996765\n",
      "[Training Epoch 0] Batch 2711, Loss 0.3309081792831421\n",
      "[Training Epoch 0] Batch 2712, Loss 0.3242781162261963\n",
      "[Training Epoch 0] Batch 2713, Loss 0.31308430433273315\n",
      "[Training Epoch 0] Batch 2714, Loss 0.31226882338523865\n",
      "[Training Epoch 0] Batch 2715, Loss 0.30879902839660645\n",
      "[Training Epoch 0] Batch 2716, Loss 0.3225371837615967\n",
      "[Training Epoch 0] Batch 2717, Loss 0.2944408357143402\n",
      "[Training Epoch 0] Batch 2718, Loss 0.3245149254798889\n",
      "[Training Epoch 0] Batch 2719, Loss 0.33734622597694397\n",
      "[Training Epoch 0] Batch 2720, Loss 0.31698012351989746\n",
      "[Training Epoch 0] Batch 2721, Loss 0.32520315051078796\n",
      "[Training Epoch 0] Batch 2722, Loss 0.32441556453704834\n",
      "[Training Epoch 0] Batch 2723, Loss 0.3249019384384155\n",
      "[Training Epoch 0] Batch 2724, Loss 0.3181314468383789\n",
      "[Training Epoch 0] Batch 2725, Loss 0.3183576464653015\n",
      "[Training Epoch 0] Batch 2726, Loss 0.32149630784988403\n",
      "[Training Epoch 0] Batch 2727, Loss 0.3544021248817444\n",
      "[Training Epoch 0] Batch 2728, Loss 0.31808555126190186\n",
      "[Training Epoch 0] Batch 2729, Loss 0.29358863830566406\n",
      "[Training Epoch 0] Batch 2730, Loss 0.3469920754432678\n",
      "[Training Epoch 0] Batch 2731, Loss 0.3379422426223755\n",
      "[Training Epoch 0] Batch 2732, Loss 0.3275558352470398\n",
      "[Training Epoch 0] Batch 2733, Loss 0.3216707110404968\n",
      "[Training Epoch 0] Batch 2734, Loss 0.3341373801231384\n",
      "[Training Epoch 0] Batch 2735, Loss 0.3505439758300781\n",
      "[Training Epoch 0] Batch 2736, Loss 0.3610357642173767\n",
      "[Training Epoch 0] Batch 2737, Loss 0.33283108472824097\n",
      "[Training Epoch 0] Batch 2738, Loss 0.3101115822792053\n",
      "[Training Epoch 0] Batch 2739, Loss 0.3421916365623474\n",
      "[Training Epoch 0] Batch 2740, Loss 0.33392533659935\n",
      "[Training Epoch 0] Batch 2741, Loss 0.33954939246177673\n",
      "[Training Epoch 0] Batch 2742, Loss 0.33019357919692993\n",
      "[Training Epoch 0] Batch 2743, Loss 0.3210798501968384\n",
      "[Training Epoch 0] Batch 2744, Loss 0.3228415250778198\n",
      "[Training Epoch 0] Batch 2745, Loss 0.3264203667640686\n",
      "[Training Epoch 0] Batch 2746, Loss 0.3131200671195984\n",
      "[Training Epoch 0] Batch 2747, Loss 0.30867958068847656\n",
      "[Training Epoch 0] Batch 2748, Loss 0.2903780937194824\n",
      "[Training Epoch 0] Batch 2749, Loss 0.3142079710960388\n",
      "[Training Epoch 0] Batch 2750, Loss 0.35121816396713257\n",
      "[Training Epoch 0] Batch 2751, Loss 0.32199451327323914\n",
      "[Training Epoch 0] Batch 2752, Loss 0.3009123206138611\n",
      "[Training Epoch 0] Batch 2753, Loss 0.3283660411834717\n",
      "[Training Epoch 0] Batch 2754, Loss 0.31156784296035767\n",
      "[Training Epoch 0] Batch 2755, Loss 0.34976887702941895\n",
      "[Training Epoch 0] Batch 2756, Loss 0.3219132423400879\n",
      "[Training Epoch 0] Batch 2757, Loss 0.32031744718551636\n",
      "[Training Epoch 0] Batch 2758, Loss 0.357780396938324\n",
      "[Training Epoch 0] Batch 2759, Loss 0.34154295921325684\n",
      "[Training Epoch 0] Batch 2760, Loss 0.3283205032348633\n",
      "[Training Epoch 0] Batch 2761, Loss 0.3526638150215149\n",
      "[Training Epoch 0] Batch 2762, Loss 0.3220812976360321\n",
      "[Training Epoch 0] Batch 2763, Loss 0.33590441942214966\n",
      "[Training Epoch 0] Batch 2764, Loss 0.3233950734138489\n",
      "[Training Epoch 0] Batch 2765, Loss 0.32178062200546265\n",
      "[Training Epoch 0] Batch 2766, Loss 0.32645654678344727\n",
      "[Training Epoch 0] Batch 2767, Loss 0.3232676386833191\n",
      "[Training Epoch 0] Batch 2768, Loss 0.30164259672164917\n",
      "[Training Epoch 0] Batch 2769, Loss 0.3425493836402893\n",
      "[Training Epoch 0] Batch 2770, Loss 0.31514355540275574\n",
      "[Training Epoch 0] Batch 2771, Loss 0.3373381495475769\n",
      "[Training Epoch 0] Batch 2772, Loss 0.30829137563705444\n",
      "[Training Epoch 0] Batch 2773, Loss 0.3307156264781952\n",
      "[Training Epoch 0] Batch 2774, Loss 0.3529220223426819\n",
      "[Training Epoch 0] Batch 2775, Loss 0.352683961391449\n",
      "[Training Epoch 0] Batch 2776, Loss 0.3234459459781647\n",
      "[Training Epoch 0] Batch 2777, Loss 0.3338702917098999\n",
      "[Training Epoch 0] Batch 2778, Loss 0.3235957622528076\n",
      "[Training Epoch 0] Batch 2779, Loss 0.34404438734054565\n",
      "[Training Epoch 0] Batch 2780, Loss 0.31758779287338257\n",
      "[Training Epoch 0] Batch 2781, Loss 0.33826568722724915\n",
      "[Training Epoch 0] Batch 2782, Loss 0.32835274934768677\n",
      "[Training Epoch 0] Batch 2783, Loss 0.3197993338108063\n",
      "[Training Epoch 0] Batch 2784, Loss 0.34613993763923645\n",
      "[Training Epoch 0] Batch 2785, Loss 0.31676310300827026\n",
      "[Training Epoch 0] Batch 2786, Loss 0.31603777408599854\n",
      "[Training Epoch 0] Batch 2787, Loss 0.3159170150756836\n",
      "[Training Epoch 0] Batch 2788, Loss 0.3434479832649231\n",
      "[Training Epoch 0] Batch 2789, Loss 0.35566335916519165\n",
      "[Training Epoch 0] Batch 2790, Loss 0.3399774432182312\n",
      "[Training Epoch 0] Batch 2791, Loss 0.32347625494003296\n",
      "[Training Epoch 0] Batch 2792, Loss 0.320762574672699\n",
      "[Training Epoch 0] Batch 2793, Loss 0.313993901014328\n",
      "[Training Epoch 0] Batch 2794, Loss 0.3206424117088318\n",
      "[Training Epoch 0] Batch 2795, Loss 0.3114861249923706\n",
      "[Training Epoch 0] Batch 2796, Loss 0.32395094633102417\n",
      "[Training Epoch 0] Batch 2797, Loss 0.2820846438407898\n",
      "[Training Epoch 0] Batch 2798, Loss 0.3261348307132721\n",
      "[Training Epoch 0] Batch 2799, Loss 0.30253151059150696\n",
      "[Training Epoch 0] Batch 2800, Loss 0.32478067278862\n",
      "[Training Epoch 0] Batch 2801, Loss 0.3080691397190094\n",
      "[Training Epoch 0] Batch 2802, Loss 0.3283112049102783\n",
      "[Training Epoch 0] Batch 2803, Loss 0.30725088715553284\n",
      "[Training Epoch 0] Batch 2804, Loss 0.3267974257469177\n",
      "[Training Epoch 0] Batch 2805, Loss 0.3429957628250122\n",
      "[Training Epoch 0] Batch 2806, Loss 0.307939350605011\n",
      "[Training Epoch 0] Batch 2807, Loss 0.3331078886985779\n",
      "[Training Epoch 0] Batch 2808, Loss 0.32191184163093567\n",
      "[Training Epoch 0] Batch 2809, Loss 0.30333858728408813\n",
      "[Training Epoch 0] Batch 2810, Loss 0.3285425305366516\n",
      "[Training Epoch 0] Batch 2811, Loss 0.33140408992767334\n",
      "[Training Epoch 0] Batch 2812, Loss 0.31447935104370117\n",
      "[Training Epoch 0] Batch 2813, Loss 0.3332354426383972\n",
      "[Training Epoch 0] Batch 2814, Loss 0.3060322403907776\n",
      "[Training Epoch 0] Batch 2815, Loss 0.3014683425426483\n",
      "[Training Epoch 0] Batch 2816, Loss 0.31899863481521606\n",
      "[Training Epoch 0] Batch 2817, Loss 0.3260630965232849\n",
      "[Training Epoch 0] Batch 2818, Loss 0.31844401359558105\n",
      "[Training Epoch 0] Batch 2819, Loss 0.310513436794281\n",
      "[Training Epoch 0] Batch 2820, Loss 0.3196521997451782\n",
      "[Training Epoch 0] Batch 2821, Loss 0.3247021436691284\n",
      "[Training Epoch 0] Batch 2822, Loss 0.2997695803642273\n",
      "[Training Epoch 0] Batch 2823, Loss 0.34867867827415466\n",
      "[Training Epoch 0] Batch 2824, Loss 0.32702332735061646\n",
      "[Training Epoch 0] Batch 2825, Loss 0.3091113567352295\n",
      "[Training Epoch 0] Batch 2826, Loss 0.33748215436935425\n",
      "[Training Epoch 0] Batch 2827, Loss 0.32582446932792664\n",
      "[Training Epoch 0] Batch 2828, Loss 0.30508720874786377\n",
      "[Training Epoch 0] Batch 2829, Loss 0.2968655228614807\n",
      "[Training Epoch 0] Batch 2830, Loss 0.3149903416633606\n",
      "[Training Epoch 0] Batch 2831, Loss 0.3130493760108948\n",
      "[Training Epoch 0] Batch 2832, Loss 0.31050223112106323\n",
      "[Training Epoch 0] Batch 2833, Loss 0.3198074698448181\n",
      "[Training Epoch 0] Batch 2834, Loss 0.32509645819664\n",
      "[Training Epoch 0] Batch 2835, Loss 0.26975518465042114\n",
      "[Training Epoch 0] Batch 2836, Loss 0.30554330348968506\n",
      "[Training Epoch 0] Batch 2837, Loss 0.32349368929862976\n",
      "[Training Epoch 0] Batch 2838, Loss 0.2903580963611603\n",
      "[Training Epoch 0] Batch 2839, Loss 0.324135959148407\n",
      "[Training Epoch 0] Batch 2840, Loss 0.32628172636032104\n",
      "[Training Epoch 0] Batch 2841, Loss 0.29638585448265076\n",
      "[Training Epoch 0] Batch 2842, Loss 0.3065797984600067\n",
      "[Training Epoch 0] Batch 2843, Loss 0.33342695236206055\n",
      "[Training Epoch 0] Batch 2844, Loss 0.3274068534374237\n",
      "[Training Epoch 0] Batch 2845, Loss 0.32337483763694763\n",
      "[Training Epoch 0] Batch 2846, Loss 0.3112134635448456\n",
      "[Training Epoch 0] Batch 2847, Loss 0.31901854276657104\n",
      "[Training Epoch 0] Batch 2848, Loss 0.3183947801589966\n",
      "[Training Epoch 0] Batch 2849, Loss 0.33137619495391846\n",
      "[Training Epoch 0] Batch 2850, Loss 0.3280053436756134\n",
      "[Training Epoch 0] Batch 2851, Loss 0.3273012340068817\n",
      "[Training Epoch 0] Batch 2852, Loss 0.3235720098018646\n",
      "[Training Epoch 0] Batch 2853, Loss 0.3243224322795868\n",
      "[Training Epoch 0] Batch 2854, Loss 0.32620230317115784\n",
      "[Training Epoch 0] Batch 2855, Loss 0.31065160036087036\n",
      "[Training Epoch 0] Batch 2856, Loss 0.3457743525505066\n",
      "[Training Epoch 0] Batch 2857, Loss 0.3286876976490021\n",
      "[Training Epoch 0] Batch 2858, Loss 0.3305481970310211\n",
      "[Training Epoch 0] Batch 2859, Loss 0.3168554902076721\n",
      "[Training Epoch 0] Batch 2860, Loss 0.3170846104621887\n",
      "[Training Epoch 0] Batch 2861, Loss 0.31567126512527466\n",
      "[Training Epoch 0] Batch 2862, Loss 0.3254534602165222\n",
      "[Training Epoch 0] Batch 2863, Loss 0.3118591904640198\n",
      "[Training Epoch 0] Batch 2864, Loss 0.3443771004676819\n",
      "[Training Epoch 0] Batch 2865, Loss 0.3522356152534485\n",
      "[Training Epoch 0] Batch 2866, Loss 0.3285646438598633\n",
      "[Training Epoch 0] Batch 2867, Loss 0.35457026958465576\n",
      "[Training Epoch 0] Batch 2868, Loss 0.302171528339386\n",
      "[Training Epoch 0] Batch 2869, Loss 0.32882946729660034\n",
      "[Training Epoch 0] Batch 2870, Loss 0.3055291771888733\n",
      "[Training Epoch 0] Batch 2871, Loss 0.3334003686904907\n",
      "[Training Epoch 0] Batch 2872, Loss 0.3198704719543457\n",
      "[Training Epoch 0] Batch 2873, Loss 0.3192360997200012\n",
      "[Training Epoch 0] Batch 2874, Loss 0.3211366832256317\n",
      "[Training Epoch 0] Batch 2875, Loss 0.3405115604400635\n",
      "[Training Epoch 0] Batch 2876, Loss 0.2998000681400299\n",
      "[Training Epoch 0] Batch 2877, Loss 0.31650102138519287\n",
      "[Training Epoch 0] Batch 2878, Loss 0.32654139399528503\n",
      "[Training Epoch 0] Batch 2879, Loss 0.33795908093452454\n",
      "[Training Epoch 0] Batch 2880, Loss 0.3349279761314392\n",
      "[Training Epoch 0] Batch 2881, Loss 0.3161095976829529\n",
      "[Training Epoch 0] Batch 2882, Loss 0.2973204255104065\n",
      "[Training Epoch 0] Batch 2883, Loss 0.3516117334365845\n",
      "[Training Epoch 0] Batch 2884, Loss 0.31719157099723816\n",
      "[Training Epoch 0] Batch 2885, Loss 0.3167743682861328\n",
      "[Training Epoch 0] Batch 2886, Loss 0.35497963428497314\n",
      "[Training Epoch 0] Batch 2887, Loss 0.29726284742355347\n",
      "[Training Epoch 0] Batch 2888, Loss 0.3166981041431427\n",
      "[Training Epoch 0] Batch 2889, Loss 0.31388384103775024\n",
      "[Training Epoch 0] Batch 2890, Loss 0.3280501961708069\n",
      "[Training Epoch 0] Batch 2891, Loss 0.33359554409980774\n",
      "[Training Epoch 0] Batch 2892, Loss 0.32758545875549316\n",
      "[Training Epoch 0] Batch 2893, Loss 0.3127269148826599\n",
      "[Training Epoch 0] Batch 2894, Loss 0.3371509313583374\n",
      "[Training Epoch 0] Batch 2895, Loss 0.30612319707870483\n",
      "[Training Epoch 0] Batch 2896, Loss 0.29402440786361694\n",
      "[Training Epoch 0] Batch 2897, Loss 0.3258466124534607\n",
      "[Training Epoch 0] Batch 2898, Loss 0.30930036306381226\n",
      "[Training Epoch 0] Batch 2899, Loss 0.3132001757621765\n",
      "[Training Epoch 0] Batch 2900, Loss 0.33802294731140137\n",
      "[Training Epoch 0] Batch 2901, Loss 0.32560989260673523\n",
      "[Training Epoch 0] Batch 2902, Loss 0.2988384962081909\n",
      "[Training Epoch 0] Batch 2903, Loss 0.2913181483745575\n",
      "[Training Epoch 0] Batch 2904, Loss 0.329288125038147\n",
      "[Training Epoch 0] Batch 2905, Loss 0.3089043200016022\n",
      "[Training Epoch 0] Batch 2906, Loss 0.3057924509048462\n",
      "[Training Epoch 0] Batch 2907, Loss 0.30044400691986084\n",
      "[Training Epoch 0] Batch 2908, Loss 0.3098132312297821\n",
      "[Training Epoch 0] Batch 2909, Loss 0.33072778582572937\n",
      "[Training Epoch 0] Batch 2910, Loss 0.288433313369751\n",
      "[Training Epoch 0] Batch 2911, Loss 0.32541894912719727\n",
      "[Training Epoch 0] Batch 2912, Loss 0.3098646402359009\n",
      "[Training Epoch 0] Batch 2913, Loss 0.3497248888015747\n",
      "[Training Epoch 0] Batch 2914, Loss 0.30930081009864807\n",
      "[Training Epoch 0] Batch 2915, Loss 0.3288879990577698\n",
      "[Training Epoch 0] Batch 2916, Loss 0.33726590871810913\n",
      "[Training Epoch 0] Batch 2917, Loss 0.2967755198478699\n",
      "[Training Epoch 0] Batch 2918, Loss 0.33604443073272705\n",
      "[Training Epoch 0] Batch 2919, Loss 0.32137930393218994\n",
      "[Training Epoch 0] Batch 2920, Loss 0.325900137424469\n",
      "[Training Epoch 0] Batch 2921, Loss 0.3303226828575134\n",
      "[Training Epoch 0] Batch 2922, Loss 0.31749311089515686\n",
      "[Training Epoch 0] Batch 2923, Loss 0.31869828701019287\n",
      "[Training Epoch 0] Batch 2924, Loss 0.32499417662620544\n",
      "[Training Epoch 0] Batch 2925, Loss 0.35434800386428833\n",
      "[Training Epoch 0] Batch 2926, Loss 0.35544854402542114\n",
      "[Training Epoch 0] Batch 2927, Loss 0.30625230073928833\n",
      "[Training Epoch 0] Batch 2928, Loss 0.29570233821868896\n",
      "[Training Epoch 0] Batch 2929, Loss 0.3366851210594177\n",
      "[Training Epoch 0] Batch 2930, Loss 0.30827435851097107\n",
      "[Training Epoch 0] Batch 2931, Loss 0.32251936197280884\n",
      "[Training Epoch 0] Batch 2932, Loss 0.3628794550895691\n",
      "[Training Epoch 0] Batch 2933, Loss 0.33968207240104675\n",
      "[Training Epoch 0] Batch 2934, Loss 0.3425602614879608\n",
      "[Training Epoch 0] Batch 2935, Loss 0.2904113531112671\n",
      "[Training Epoch 0] Batch 2936, Loss 0.30725008249282837\n",
      "[Training Epoch 0] Batch 2937, Loss 0.29051780700683594\n",
      "[Training Epoch 0] Batch 2938, Loss 0.31038233637809753\n",
      "[Training Epoch 0] Batch 2939, Loss 0.3121752142906189\n",
      "[Training Epoch 0] Batch 2940, Loss 0.35002705454826355\n",
      "[Training Epoch 0] Batch 2941, Loss 0.3276522159576416\n",
      "[Training Epoch 0] Batch 2942, Loss 0.30921533703804016\n",
      "[Training Epoch 0] Batch 2943, Loss 0.3536147177219391\n",
      "[Training Epoch 0] Batch 2944, Loss 0.3056291341781616\n",
      "[Training Epoch 0] Batch 2945, Loss 0.30277159810066223\n",
      "[Training Epoch 0] Batch 2946, Loss 0.3307623267173767\n",
      "[Training Epoch 0] Batch 2947, Loss 0.32301825284957886\n",
      "[Training Epoch 0] Batch 2948, Loss 0.33272793889045715\n",
      "[Training Epoch 0] Batch 2949, Loss 0.32566869258880615\n",
      "[Training Epoch 0] Batch 2950, Loss 0.3192622661590576\n",
      "[Training Epoch 0] Batch 2951, Loss 0.3543156087398529\n",
      "[Training Epoch 0] Batch 2952, Loss 0.3421497642993927\n",
      "[Training Epoch 0] Batch 2953, Loss 0.31440532207489014\n",
      "[Training Epoch 0] Batch 2954, Loss 0.2952742278575897\n",
      "[Training Epoch 0] Batch 2955, Loss 0.3122018575668335\n",
      "[Training Epoch 0] Batch 2956, Loss 0.3577319383621216\n",
      "[Training Epoch 0] Batch 2957, Loss 0.33704814314842224\n",
      "[Training Epoch 0] Batch 2958, Loss 0.32458850741386414\n",
      "[Training Epoch 0] Batch 2959, Loss 0.29799020290374756\n",
      "[Training Epoch 0] Batch 2960, Loss 0.32681703567504883\n",
      "[Training Epoch 0] Batch 2961, Loss 0.32507896423339844\n",
      "[Training Epoch 0] Batch 2962, Loss 0.3321228325366974\n",
      "[Training Epoch 0] Batch 2963, Loss 0.31922829151153564\n",
      "[Training Epoch 0] Batch 2964, Loss 0.3221757709980011\n",
      "[Training Epoch 0] Batch 2965, Loss 0.295054167509079\n",
      "[Training Epoch 0] Batch 2966, Loss 0.2995831072330475\n",
      "[Training Epoch 0] Batch 2967, Loss 0.3284820318222046\n",
      "[Training Epoch 0] Batch 2968, Loss 0.36336520314216614\n",
      "[Training Epoch 0] Batch 2969, Loss 0.3185040056705475\n",
      "[Training Epoch 0] Batch 2970, Loss 0.32127395272254944\n",
      "[Training Epoch 0] Batch 2971, Loss 0.33162781596183777\n",
      "[Training Epoch 0] Batch 2972, Loss 0.32538628578186035\n",
      "[Training Epoch 0] Batch 2973, Loss 0.31243419647216797\n",
      "[Training Epoch 0] Batch 2974, Loss 0.3199792504310608\n",
      "[Training Epoch 0] Batch 2975, Loss 0.35871022939682007\n",
      "[Training Epoch 0] Batch 2976, Loss 0.3204524517059326\n",
      "[Training Epoch 0] Batch 2977, Loss 0.33639705181121826\n",
      "[Training Epoch 0] Batch 2978, Loss 0.3118722438812256\n",
      "[Training Epoch 0] Batch 2979, Loss 0.3203161358833313\n",
      "[Training Epoch 0] Batch 2980, Loss 0.3201208710670471\n",
      "[Training Epoch 0] Batch 2981, Loss 0.3001086711883545\n",
      "[Training Epoch 0] Batch 2982, Loss 0.29750558733940125\n",
      "[Training Epoch 0] Batch 2983, Loss 0.31575214862823486\n",
      "[Training Epoch 0] Batch 2984, Loss 0.3270567059516907\n",
      "[Training Epoch 0] Batch 2985, Loss 0.3151257336139679\n",
      "[Training Epoch 0] Batch 2986, Loss 0.3383660912513733\n",
      "[Training Epoch 0] Batch 2987, Loss 0.3295252025127411\n",
      "[Training Epoch 0] Batch 2988, Loss 0.3340720534324646\n",
      "[Training Epoch 0] Batch 2989, Loss 0.3492338955402374\n",
      "[Training Epoch 0] Batch 2990, Loss 0.34945279359817505\n",
      "[Training Epoch 0] Batch 2991, Loss 0.31918200850486755\n",
      "[Training Epoch 0] Batch 2992, Loss 0.29715263843536377\n",
      "[Training Epoch 0] Batch 2993, Loss 0.3374905586242676\n",
      "[Training Epoch 0] Batch 2994, Loss 0.35596734285354614\n",
      "[Training Epoch 0] Batch 2995, Loss 0.325677752494812\n",
      "[Training Epoch 0] Batch 2996, Loss 0.33361780643463135\n",
      "[Training Epoch 0] Batch 2997, Loss 0.3291952610015869\n",
      "[Training Epoch 0] Batch 2998, Loss 0.33050960302352905\n",
      "[Training Epoch 0] Batch 2999, Loss 0.3464856743812561\n",
      "[Training Epoch 0] Batch 3000, Loss 0.3171641230583191\n",
      "[Training Epoch 0] Batch 3001, Loss 0.32741451263427734\n",
      "[Training Epoch 0] Batch 3002, Loss 0.33925333619117737\n",
      "[Training Epoch 0] Batch 3003, Loss 0.3304106295108795\n",
      "[Training Epoch 0] Batch 3004, Loss 0.2935570478439331\n",
      "[Training Epoch 0] Batch 3005, Loss 0.3158538341522217\n",
      "[Training Epoch 0] Batch 3006, Loss 0.3113518953323364\n",
      "[Training Epoch 0] Batch 3007, Loss 0.30293598771095276\n",
      "[Training Epoch 0] Batch 3008, Loss 0.30712825059890747\n",
      "[Training Epoch 0] Batch 3009, Loss 0.34299421310424805\n",
      "[Training Epoch 0] Batch 3010, Loss 0.32786989212036133\n",
      "[Training Epoch 0] Batch 3011, Loss 0.33136385679244995\n",
      "[Training Epoch 0] Batch 3012, Loss 0.30640798807144165\n",
      "[Training Epoch 0] Batch 3013, Loss 0.3426879644393921\n",
      "[Training Epoch 0] Batch 3014, Loss 0.3326253890991211\n",
      "[Training Epoch 0] Batch 3015, Loss 0.3243827819824219\n",
      "[Training Epoch 0] Batch 3016, Loss 0.33116117119789124\n",
      "[Training Epoch 0] Batch 3017, Loss 0.3235882520675659\n",
      "[Training Epoch 0] Batch 3018, Loss 0.3204385042190552\n",
      "[Training Epoch 0] Batch 3019, Loss 0.331854909658432\n",
      "[Training Epoch 0] Batch 3020, Loss 0.32396700978279114\n",
      "[Training Epoch 0] Batch 3021, Loss 0.308685839176178\n",
      "[Training Epoch 0] Batch 3022, Loss 0.30979034304618835\n",
      "[Training Epoch 0] Batch 3023, Loss 0.2916536331176758\n",
      "[Training Epoch 0] Batch 3024, Loss 0.32299718260765076\n",
      "[Training Epoch 0] Batch 3025, Loss 0.30875256657600403\n",
      "[Training Epoch 0] Batch 3026, Loss 0.3298862874507904\n",
      "[Training Epoch 0] Batch 3027, Loss 0.33499211072921753\n",
      "[Training Epoch 0] Batch 3028, Loss 0.29514145851135254\n",
      "[Training Epoch 0] Batch 3029, Loss 0.2981051206588745\n",
      "[Training Epoch 0] Batch 3030, Loss 0.3334823250770569\n",
      "[Training Epoch 0] Batch 3031, Loss 0.3364511728286743\n",
      "[Training Epoch 0] Batch 3032, Loss 0.3309786021709442\n",
      "[Training Epoch 0] Batch 3033, Loss 0.32511505484580994\n",
      "[Training Epoch 0] Batch 3034, Loss 0.3293016254901886\n",
      "[Training Epoch 0] Batch 3035, Loss 0.3422881066799164\n",
      "[Training Epoch 0] Batch 3036, Loss 0.31749314069747925\n",
      "[Training Epoch 0] Batch 3037, Loss 0.32080399990081787\n",
      "[Training Epoch 0] Batch 3038, Loss 0.3164985477924347\n",
      "[Training Epoch 0] Batch 3039, Loss 0.3127659559249878\n",
      "[Training Epoch 0] Batch 3040, Loss 0.30136579275131226\n",
      "[Training Epoch 0] Batch 3041, Loss 0.31871187686920166\n",
      "[Training Epoch 0] Batch 3042, Loss 0.29558753967285156\n",
      "[Training Epoch 0] Batch 3043, Loss 0.31995803117752075\n",
      "[Training Epoch 0] Batch 3044, Loss 0.31962037086486816\n",
      "[Training Epoch 0] Batch 3045, Loss 0.3190445303916931\n",
      "[Training Epoch 0] Batch 3046, Loss 0.2869763672351837\n",
      "[Training Epoch 0] Batch 3047, Loss 0.2999424338340759\n",
      "[Training Epoch 0] Batch 3048, Loss 0.30277618765830994\n",
      "[Training Epoch 0] Batch 3049, Loss 0.29891571402549744\n",
      "[Training Epoch 0] Batch 3050, Loss 0.3284919261932373\n",
      "[Training Epoch 0] Batch 3051, Loss 0.3202196955680847\n",
      "[Training Epoch 0] Batch 3052, Loss 0.3406166434288025\n",
      "[Training Epoch 0] Batch 3053, Loss 0.32091382145881653\n",
      "[Training Epoch 0] Batch 3054, Loss 0.30407804250717163\n",
      "[Training Epoch 0] Batch 3055, Loss 0.28792011737823486\n",
      "[Training Epoch 0] Batch 3056, Loss 0.32007044553756714\n",
      "[Training Epoch 0] Batch 3057, Loss 0.3442695140838623\n",
      "[Training Epoch 0] Batch 3058, Loss 0.33244210481643677\n",
      "[Training Epoch 0] Batch 3059, Loss 0.31250301003456116\n",
      "[Training Epoch 0] Batch 3060, Loss 0.34121519327163696\n",
      "[Training Epoch 0] Batch 3061, Loss 0.32228583097457886\n",
      "[Training Epoch 0] Batch 3062, Loss 0.30654412508010864\n",
      "[Training Epoch 0] Batch 3063, Loss 0.3001580238342285\n",
      "[Training Epoch 0] Batch 3064, Loss 0.31290021538734436\n",
      "[Training Epoch 0] Batch 3065, Loss 0.2987944483757019\n",
      "[Training Epoch 0] Batch 3066, Loss 0.3048170804977417\n",
      "[Training Epoch 0] Batch 3067, Loss 0.33142200112342834\n",
      "[Training Epoch 0] Batch 3068, Loss 0.31697869300842285\n",
      "[Training Epoch 0] Batch 3069, Loss 0.2929537296295166\n",
      "[Training Epoch 0] Batch 3070, Loss 0.3233444094657898\n",
      "[Training Epoch 0] Batch 3071, Loss 0.3378657102584839\n",
      "[Training Epoch 0] Batch 3072, Loss 0.3330869674682617\n",
      "[Training Epoch 0] Batch 3073, Loss 0.29495444893836975\n",
      "[Training Epoch 0] Batch 3074, Loss 0.32859769463539124\n",
      "[Training Epoch 0] Batch 3075, Loss 0.3414318263530731\n",
      "[Training Epoch 0] Batch 3076, Loss 0.3253120183944702\n",
      "[Training Epoch 0] Batch 3077, Loss 0.3628823757171631\n",
      "[Training Epoch 0] Batch 3078, Loss 0.35168302059173584\n",
      "[Training Epoch 0] Batch 3079, Loss 0.30912137031555176\n",
      "[Training Epoch 0] Batch 3080, Loss 0.30570918321609497\n",
      "[Training Epoch 0] Batch 3081, Loss 0.30894380807876587\n",
      "[Training Epoch 0] Batch 3082, Loss 0.32198286056518555\n",
      "[Training Epoch 0] Batch 3083, Loss 0.33442822098731995\n",
      "[Training Epoch 0] Batch 3084, Loss 0.3269892632961273\n",
      "[Training Epoch 0] Batch 3085, Loss 0.31740260124206543\n",
      "[Training Epoch 0] Batch 3086, Loss 0.3057553768157959\n",
      "[Training Epoch 0] Batch 3087, Loss 0.3165768086910248\n",
      "[Training Epoch 0] Batch 3088, Loss 0.33753955364227295\n",
      "[Training Epoch 0] Batch 3089, Loss 0.31192636489868164\n",
      "[Training Epoch 0] Batch 3090, Loss 0.35408297181129456\n",
      "[Training Epoch 0] Batch 3091, Loss 0.3178727626800537\n",
      "[Training Epoch 0] Batch 3092, Loss 0.31275224685668945\n",
      "[Training Epoch 0] Batch 3093, Loss 0.30653446912765503\n",
      "[Training Epoch 0] Batch 3094, Loss 0.3409885764122009\n",
      "[Training Epoch 0] Batch 3095, Loss 0.32919082045555115\n",
      "[Training Epoch 0] Batch 3096, Loss 0.3089403212070465\n",
      "[Training Epoch 0] Batch 3097, Loss 0.3393070697784424\n",
      "[Training Epoch 0] Batch 3098, Loss 0.29100093245506287\n",
      "[Training Epoch 0] Batch 3099, Loss 0.32846155762672424\n",
      "[Training Epoch 0] Batch 3100, Loss 0.3173559308052063\n",
      "[Training Epoch 0] Batch 3101, Loss 0.34607791900634766\n",
      "[Training Epoch 0] Batch 3102, Loss 0.3092115521430969\n",
      "[Training Epoch 0] Batch 3103, Loss 0.33534061908721924\n",
      "[Training Epoch 0] Batch 3104, Loss 0.331636518239975\n",
      "[Training Epoch 0] Batch 3105, Loss 0.32110509276390076\n",
      "[Training Epoch 0] Batch 3106, Loss 0.3147459030151367\n",
      "[Training Epoch 0] Batch 3107, Loss 0.3423115611076355\n",
      "[Training Epoch 0] Batch 3108, Loss 0.3154921531677246\n",
      "[Training Epoch 0] Batch 3109, Loss 0.32552576065063477\n",
      "[Training Epoch 0] Batch 3110, Loss 0.368800550699234\n",
      "[Training Epoch 0] Batch 3111, Loss 0.3560086488723755\n",
      "[Training Epoch 0] Batch 3112, Loss 0.36352038383483887\n",
      "[Training Epoch 0] Batch 3113, Loss 0.31397074460983276\n",
      "[Training Epoch 0] Batch 3114, Loss 0.33476707339286804\n",
      "[Training Epoch 0] Batch 3115, Loss 0.30457592010498047\n",
      "[Training Epoch 0] Batch 3116, Loss 0.3037601113319397\n",
      "[Training Epoch 0] Batch 3117, Loss 0.3385990858078003\n",
      "[Training Epoch 0] Batch 3118, Loss 0.32870179414749146\n",
      "[Training Epoch 0] Batch 3119, Loss 0.31091415882110596\n",
      "[Training Epoch 0] Batch 3120, Loss 0.31081998348236084\n",
      "[Training Epoch 0] Batch 3121, Loss 0.3145553469657898\n",
      "[Training Epoch 0] Batch 3122, Loss 0.32463276386260986\n",
      "[Training Epoch 0] Batch 3123, Loss 0.34680211544036865\n",
      "[Training Epoch 0] Batch 3124, Loss 0.3169083893299103\n",
      "[Training Epoch 0] Batch 3125, Loss 0.3325869143009186\n",
      "[Training Epoch 0] Batch 3126, Loss 0.32950106263160706\n",
      "[Training Epoch 0] Batch 3127, Loss 0.29732298851013184\n",
      "[Training Epoch 0] Batch 3128, Loss 0.3119136095046997\n",
      "[Training Epoch 0] Batch 3129, Loss 0.32481369376182556\n",
      "[Training Epoch 0] Batch 3130, Loss 0.3089173436164856\n",
      "[Training Epoch 0] Batch 3131, Loss 0.34787845611572266\n",
      "[Training Epoch 0] Batch 3132, Loss 0.29593390226364136\n",
      "[Training Epoch 0] Batch 3133, Loss 0.364461213350296\n",
      "[Training Epoch 0] Batch 3134, Loss 0.31227967143058777\n",
      "[Training Epoch 0] Batch 3135, Loss 0.3322673439979553\n",
      "[Training Epoch 0] Batch 3136, Loss 0.3089935779571533\n",
      "[Training Epoch 0] Batch 3137, Loss 0.3174899220466614\n",
      "[Training Epoch 0] Batch 3138, Loss 0.34372442960739136\n",
      "[Training Epoch 0] Batch 3139, Loss 0.31196027994155884\n",
      "[Training Epoch 0] Batch 3140, Loss 0.30859068036079407\n",
      "[Training Epoch 0] Batch 3141, Loss 0.3187709152698517\n",
      "[Training Epoch 0] Batch 3142, Loss 0.3500439524650574\n",
      "[Training Epoch 0] Batch 3143, Loss 0.32631438970565796\n",
      "[Training Epoch 0] Batch 3144, Loss 0.3421945571899414\n",
      "[Training Epoch 0] Batch 3145, Loss 0.33065012097358704\n",
      "[Training Epoch 0] Batch 3146, Loss 0.32952117919921875\n",
      "[Training Epoch 0] Batch 3147, Loss 0.32839488983154297\n",
      "[Training Epoch 0] Batch 3148, Loss 0.34985026717185974\n",
      "[Training Epoch 0] Batch 3149, Loss 0.3142208158969879\n",
      "[Training Epoch 0] Batch 3150, Loss 0.3285215198993683\n",
      "[Training Epoch 0] Batch 3151, Loss 0.32328933477401733\n",
      "[Training Epoch 0] Batch 3152, Loss 0.3238387107849121\n",
      "[Training Epoch 0] Batch 3153, Loss 0.3541313409805298\n",
      "[Training Epoch 0] Batch 3154, Loss 0.3519434630870819\n",
      "[Training Epoch 0] Batch 3155, Loss 0.3272324204444885\n",
      "[Training Epoch 0] Batch 3156, Loss 0.31356704235076904\n",
      "[Training Epoch 0] Batch 3157, Loss 0.35419413447380066\n",
      "[Training Epoch 0] Batch 3158, Loss 0.31015485525131226\n",
      "[Training Epoch 0] Batch 3159, Loss 0.3279775083065033\n",
      "[Training Epoch 0] Batch 3160, Loss 0.3164513409137726\n",
      "[Training Epoch 0] Batch 3161, Loss 0.3475489616394043\n",
      "[Training Epoch 0] Batch 3162, Loss 0.3267328143119812\n",
      "[Training Epoch 0] Batch 3163, Loss 0.33563679456710815\n",
      "[Training Epoch 0] Batch 3164, Loss 0.336789071559906\n",
      "[Training Epoch 0] Batch 3165, Loss 0.3515063226222992\n",
      "[Training Epoch 0] Batch 3166, Loss 0.3241978585720062\n",
      "[Training Epoch 0] Batch 3167, Loss 0.3491445779800415\n",
      "[Training Epoch 0] Batch 3168, Loss 0.2954305410385132\n",
      "[Training Epoch 0] Batch 3169, Loss 0.35136735439300537\n",
      "[Training Epoch 0] Batch 3170, Loss 0.3069370985031128\n",
      "[Training Epoch 0] Batch 3171, Loss 0.3000800311565399\n",
      "[Training Epoch 0] Batch 3172, Loss 0.3265334963798523\n",
      "[Training Epoch 0] Batch 3173, Loss 0.3271212875843048\n",
      "[Training Epoch 0] Batch 3174, Loss 0.33504611253738403\n",
      "[Training Epoch 0] Batch 3175, Loss 0.32904720306396484\n",
      "[Training Epoch 0] Batch 3176, Loss 0.3061009645462036\n",
      "[Training Epoch 0] Batch 3177, Loss 0.3407799005508423\n",
      "[Training Epoch 0] Batch 3178, Loss 0.33742228150367737\n",
      "[Training Epoch 0] Batch 3179, Loss 0.31493428349494934\n",
      "[Training Epoch 0] Batch 3180, Loss 0.32443204522132874\n",
      "[Training Epoch 0] Batch 3181, Loss 0.2959436774253845\n",
      "[Training Epoch 0] Batch 3182, Loss 0.3072744905948639\n",
      "[Training Epoch 0] Batch 3183, Loss 0.31917375326156616\n",
      "[Training Epoch 0] Batch 3184, Loss 0.30469316244125366\n",
      "[Training Epoch 0] Batch 3185, Loss 0.32646381855010986\n",
      "[Training Epoch 0] Batch 3186, Loss 0.33458828926086426\n",
      "[Training Epoch 0] Batch 3187, Loss 0.31079941987991333\n",
      "[Training Epoch 0] Batch 3188, Loss 0.30730265378952026\n",
      "[Training Epoch 0] Batch 3189, Loss 0.3369346559047699\n",
      "[Training Epoch 0] Batch 3190, Loss 0.2992570698261261\n",
      "[Training Epoch 0] Batch 3191, Loss 0.3622976243495941\n",
      "[Training Epoch 0] Batch 3192, Loss 0.3176376223564148\n",
      "[Training Epoch 0] Batch 3193, Loss 0.33471179008483887\n",
      "[Training Epoch 0] Batch 3194, Loss 0.31710565090179443\n",
      "[Training Epoch 0] Batch 3195, Loss 0.3246637284755707\n",
      "[Training Epoch 0] Batch 3196, Loss 0.30724263191223145\n",
      "[Training Epoch 0] Batch 3197, Loss 0.3344500660896301\n",
      "[Training Epoch 0] Batch 3198, Loss 0.3161579668521881\n",
      "[Training Epoch 0] Batch 3199, Loss 0.30910980701446533\n",
      "[Training Epoch 0] Batch 3200, Loss 0.344920814037323\n",
      "[Training Epoch 0] Batch 3201, Loss 0.3448631763458252\n",
      "[Training Epoch 0] Batch 3202, Loss 0.2940433621406555\n",
      "[Training Epoch 0] Batch 3203, Loss 0.30713704228401184\n",
      "[Training Epoch 0] Batch 3204, Loss 0.32563310861587524\n",
      "[Training Epoch 0] Batch 3205, Loss 0.3211398720741272\n",
      "[Training Epoch 0] Batch 3206, Loss 0.32376354932785034\n",
      "[Training Epoch 0] Batch 3207, Loss 0.3046506345272064\n",
      "[Training Epoch 0] Batch 3208, Loss 0.3277958333492279\n",
      "[Training Epoch 0] Batch 3209, Loss 0.33488738536834717\n",
      "[Training Epoch 0] Batch 3210, Loss 0.28978708386421204\n",
      "[Training Epoch 0] Batch 3211, Loss 0.3086729347705841\n",
      "[Training Epoch 0] Batch 3212, Loss 0.3327244520187378\n",
      "[Training Epoch 0] Batch 3213, Loss 0.317025750875473\n",
      "[Training Epoch 0] Batch 3214, Loss 0.3214118480682373\n",
      "[Training Epoch 0] Batch 3215, Loss 0.3158677816390991\n",
      "[Training Epoch 0] Batch 3216, Loss 0.3279208838939667\n",
      "[Training Epoch 0] Batch 3217, Loss 0.2665903568267822\n",
      "[Training Epoch 0] Batch 3218, Loss 0.30857276916503906\n",
      "[Training Epoch 0] Batch 3219, Loss 0.31038588285446167\n",
      "[Training Epoch 0] Batch 3220, Loss 0.2994166612625122\n",
      "[Training Epoch 0] Batch 3221, Loss 0.31641727685928345\n",
      "[Training Epoch 0] Batch 3222, Loss 0.3237777352333069\n",
      "[Training Epoch 0] Batch 3223, Loss 0.2915031909942627\n",
      "[Training Epoch 0] Batch 3224, Loss 0.2941274642944336\n",
      "[Training Epoch 0] Batch 3225, Loss 0.3013450801372528\n",
      "[Training Epoch 0] Batch 3226, Loss 0.33206599950790405\n",
      "[Training Epoch 0] Batch 3227, Loss 0.32687655091285706\n",
      "[Training Epoch 0] Batch 3228, Loss 0.3208363950252533\n",
      "[Training Epoch 0] Batch 3229, Loss 0.34754979610443115\n",
      "[Training Epoch 0] Batch 3230, Loss 0.3172699511051178\n",
      "[Training Epoch 0] Batch 3231, Loss 0.3202143907546997\n",
      "[Training Epoch 0] Batch 3232, Loss 0.3060477077960968\n",
      "[Training Epoch 0] Batch 3233, Loss 0.2900054156780243\n",
      "[Training Epoch 0] Batch 3234, Loss 0.3266822099685669\n",
      "[Training Epoch 0] Batch 3235, Loss 0.3080957233905792\n",
      "[Training Epoch 0] Batch 3236, Loss 0.31162914633750916\n",
      "[Training Epoch 0] Batch 3237, Loss 0.31246843934059143\n",
      "[Training Epoch 0] Batch 3238, Loss 0.3329290449619293\n",
      "[Training Epoch 0] Batch 3239, Loss 0.26847660541534424\n",
      "[Training Epoch 0] Batch 3240, Loss 0.30803176760673523\n",
      "[Training Epoch 0] Batch 3241, Loss 0.318494588136673\n",
      "[Training Epoch 0] Batch 3242, Loss 0.28834056854248047\n",
      "[Training Epoch 0] Batch 3243, Loss 0.33664670586586\n",
      "[Training Epoch 0] Batch 3244, Loss 0.3389127850532532\n",
      "[Training Epoch 0] Batch 3245, Loss 0.32232552766799927\n",
      "[Training Epoch 0] Batch 3246, Loss 0.32452288269996643\n",
      "[Training Epoch 0] Batch 3247, Loss 0.3427353501319885\n",
      "[Training Epoch 0] Batch 3248, Loss 0.3031357526779175\n",
      "[Training Epoch 0] Batch 3249, Loss 0.34123748540878296\n",
      "[Training Epoch 0] Batch 3250, Loss 0.32668399810791016\n",
      "[Training Epoch 0] Batch 3251, Loss 0.3281511962413788\n",
      "[Training Epoch 0] Batch 3252, Loss 0.33233270049095154\n",
      "[Training Epoch 0] Batch 3253, Loss 0.33181893825531006\n",
      "[Training Epoch 0] Batch 3254, Loss 0.29085367918014526\n",
      "[Training Epoch 0] Batch 3255, Loss 0.3237183392047882\n",
      "[Training Epoch 0] Batch 3256, Loss 0.3252590298652649\n",
      "[Training Epoch 0] Batch 3257, Loss 0.3084317147731781\n",
      "[Training Epoch 0] Batch 3258, Loss 0.31801077723503113\n",
      "[Training Epoch 0] Batch 3259, Loss 0.3347000777721405\n",
      "[Training Epoch 0] Batch 3260, Loss 0.314270555973053\n",
      "[Training Epoch 0] Batch 3261, Loss 0.3136445879936218\n",
      "[Training Epoch 0] Batch 3262, Loss 0.3408818244934082\n",
      "[Training Epoch 0] Batch 3263, Loss 0.31165623664855957\n",
      "[Training Epoch 0] Batch 3264, Loss 0.3313491940498352\n",
      "[Training Epoch 0] Batch 3265, Loss 0.3156850337982178\n",
      "[Training Epoch 0] Batch 3266, Loss 0.3311627507209778\n",
      "[Training Epoch 0] Batch 3267, Loss 0.3010939657688141\n",
      "[Training Epoch 0] Batch 3268, Loss 0.32057851552963257\n",
      "[Training Epoch 0] Batch 3269, Loss 0.33187681436538696\n",
      "[Training Epoch 0] Batch 3270, Loss 0.3464268147945404\n",
      "[Training Epoch 0] Batch 3271, Loss 0.35133206844329834\n",
      "[Training Epoch 0] Batch 3272, Loss 0.3596855103969574\n",
      "[Training Epoch 0] Batch 3273, Loss 0.31996458768844604\n",
      "[Training Epoch 0] Batch 3274, Loss 0.3326992988586426\n",
      "[Training Epoch 0] Batch 3275, Loss 0.3251645565032959\n",
      "[Training Epoch 0] Batch 3276, Loss 0.3268486261367798\n",
      "[Training Epoch 0] Batch 3277, Loss 0.32993677258491516\n",
      "[Training Epoch 0] Batch 3278, Loss 0.2964077591896057\n",
      "[Training Epoch 0] Batch 3279, Loss 0.3020205497741699\n",
      "[Training Epoch 0] Batch 3280, Loss 0.36716243624687195\n",
      "[Training Epoch 0] Batch 3281, Loss 0.31020408868789673\n",
      "[Training Epoch 0] Batch 3282, Loss 0.3388235569000244\n",
      "[Training Epoch 0] Batch 3283, Loss 0.31344103813171387\n",
      "[Training Epoch 0] Batch 3284, Loss 0.324833482503891\n",
      "[Training Epoch 0] Batch 3285, Loss 0.3413246273994446\n",
      "[Training Epoch 0] Batch 3286, Loss 0.3060106635093689\n",
      "[Training Epoch 0] Batch 3287, Loss 0.32606738805770874\n",
      "[Training Epoch 0] Batch 3288, Loss 0.3370967507362366\n",
      "[Training Epoch 0] Batch 3289, Loss 0.3022424876689911\n",
      "[Training Epoch 0] Batch 3290, Loss 0.3412032127380371\n",
      "[Training Epoch 0] Batch 3291, Loss 0.3213241696357727\n",
      "[Training Epoch 0] Batch 3292, Loss 0.3310717046260834\n",
      "[Training Epoch 0] Batch 3293, Loss 0.3324134945869446\n",
      "[Training Epoch 0] Batch 3294, Loss 0.34814274311065674\n",
      "[Training Epoch 0] Batch 3295, Loss 0.34195202589035034\n",
      "[Training Epoch 0] Batch 3296, Loss 0.3196133077144623\n",
      "[Training Epoch 0] Batch 3297, Loss 0.3256469964981079\n",
      "[Training Epoch 0] Batch 3298, Loss 0.32794755697250366\n",
      "[Training Epoch 0] Batch 3299, Loss 0.32331088185310364\n",
      "[Training Epoch 0] Batch 3300, Loss 0.3098098933696747\n",
      "[Training Epoch 0] Batch 3301, Loss 0.31770986318588257\n",
      "[Training Epoch 0] Batch 3302, Loss 0.3128071427345276\n",
      "[Training Epoch 0] Batch 3303, Loss 0.3015230596065521\n",
      "[Training Epoch 0] Batch 3304, Loss 0.29826995730400085\n",
      "[Training Epoch 0] Batch 3305, Loss 0.31130534410476685\n",
      "[Training Epoch 0] Batch 3306, Loss 0.3296758532524109\n",
      "[Training Epoch 0] Batch 3307, Loss 0.30341005325317383\n",
      "[Training Epoch 0] Batch 3308, Loss 0.3443259596824646\n",
      "[Training Epoch 0] Batch 3309, Loss 0.2978737950325012\n",
      "[Training Epoch 0] Batch 3310, Loss 0.31995680928230286\n",
      "[Training Epoch 0] Batch 3311, Loss 0.3632414638996124\n",
      "[Training Epoch 0] Batch 3312, Loss 0.3186867833137512\n",
      "[Training Epoch 0] Batch 3313, Loss 0.3096388578414917\n",
      "[Training Epoch 0] Batch 3314, Loss 0.32842355966567993\n",
      "[Training Epoch 0] Batch 3315, Loss 0.303962379693985\n",
      "[Training Epoch 0] Batch 3316, Loss 0.3091557025909424\n",
      "[Training Epoch 0] Batch 3317, Loss 0.3195056915283203\n",
      "[Training Epoch 0] Batch 3318, Loss 0.32128623127937317\n",
      "[Training Epoch 0] Batch 3319, Loss 0.3282979428768158\n",
      "[Training Epoch 0] Batch 3320, Loss 0.3118807077407837\n",
      "[Training Epoch 0] Batch 3321, Loss 0.3033602237701416\n",
      "[Training Epoch 0] Batch 3322, Loss 0.32909923791885376\n",
      "[Training Epoch 0] Batch 3323, Loss 0.29974955320358276\n",
      "[Training Epoch 0] Batch 3324, Loss 0.3443858325481415\n",
      "[Training Epoch 0] Batch 3325, Loss 0.3210082948207855\n",
      "[Training Epoch 0] Batch 3326, Loss 0.34861668944358826\n",
      "[Training Epoch 0] Batch 3327, Loss 0.3046764135360718\n",
      "[Training Epoch 0] Batch 3328, Loss 0.3392033576965332\n",
      "[Training Epoch 0] Batch 3329, Loss 0.3160904049873352\n",
      "[Training Epoch 0] Batch 3330, Loss 0.3299374580383301\n",
      "[Training Epoch 0] Batch 3331, Loss 0.29540160298347473\n",
      "[Training Epoch 0] Batch 3332, Loss 0.27855658531188965\n",
      "[Training Epoch 0] Batch 3333, Loss 0.29893073439598083\n",
      "[Training Epoch 0] Batch 3334, Loss 0.3359406590461731\n",
      "[Training Epoch 0] Batch 3335, Loss 0.3091970682144165\n",
      "[Training Epoch 0] Batch 3336, Loss 0.3248600959777832\n",
      "[Training Epoch 0] Batch 3337, Loss 0.32349932193756104\n",
      "[Training Epoch 0] Batch 3338, Loss 0.3042343258857727\n",
      "[Training Epoch 0] Batch 3339, Loss 0.3305184543132782\n",
      "[Training Epoch 0] Batch 3340, Loss 0.3009675443172455\n",
      "[Training Epoch 0] Batch 3341, Loss 0.34078672528266907\n",
      "[Training Epoch 0] Batch 3342, Loss 0.33646637201309204\n",
      "[Training Epoch 0] Batch 3343, Loss 0.34340953826904297\n",
      "[Training Epoch 0] Batch 3344, Loss 0.32678043842315674\n",
      "[Training Epoch 0] Batch 3345, Loss 0.32258209586143494\n",
      "[Training Epoch 0] Batch 3346, Loss 0.33099299669265747\n",
      "[Training Epoch 0] Batch 3347, Loss 0.31434503197669983\n",
      "[Training Epoch 0] Batch 3348, Loss 0.3455037474632263\n",
      "[Training Epoch 0] Batch 3349, Loss 0.33566296100616455\n",
      "[Training Epoch 0] Batch 3350, Loss 0.3235240876674652\n",
      "[Training Epoch 0] Batch 3351, Loss 0.3199027180671692\n",
      "[Training Epoch 0] Batch 3352, Loss 0.3368486166000366\n",
      "[Training Epoch 0] Batch 3353, Loss 0.3217281699180603\n",
      "[Training Epoch 0] Batch 3354, Loss 0.3139423131942749\n",
      "[Training Epoch 0] Batch 3355, Loss 0.3446298837661743\n",
      "[Training Epoch 0] Batch 3356, Loss 0.3143290579319\n",
      "[Training Epoch 0] Batch 3357, Loss 0.31489717960357666\n",
      "[Training Epoch 0] Batch 3358, Loss 0.3324941396713257\n",
      "[Training Epoch 0] Batch 3359, Loss 0.3197445869445801\n",
      "[Training Epoch 0] Batch 3360, Loss 0.31466051936149597\n",
      "[Training Epoch 0] Batch 3361, Loss 0.3317386507987976\n",
      "[Training Epoch 0] Batch 3362, Loss 0.3120124340057373\n",
      "[Training Epoch 0] Batch 3363, Loss 0.3300001323223114\n",
      "[Training Epoch 0] Batch 3364, Loss 0.29931002855300903\n",
      "[Training Epoch 0] Batch 3365, Loss 0.33334535360336304\n",
      "[Training Epoch 0] Batch 3366, Loss 0.31829944252967834\n",
      "[Training Epoch 0] Batch 3367, Loss 0.3300044536590576\n",
      "[Training Epoch 0] Batch 3368, Loss 0.3320002555847168\n",
      "[Training Epoch 0] Batch 3369, Loss 0.3495637774467468\n",
      "[Training Epoch 0] Batch 3370, Loss 0.32900696992874146\n",
      "[Training Epoch 0] Batch 3371, Loss 0.30946362018585205\n",
      "[Training Epoch 0] Batch 3372, Loss 0.33336108922958374\n",
      "[Training Epoch 0] Batch 3373, Loss 0.31042706966400146\n",
      "[Training Epoch 0] Batch 3374, Loss 0.35331815481185913\n",
      "[Training Epoch 0] Batch 3375, Loss 0.30793121457099915\n",
      "[Training Epoch 0] Batch 3376, Loss 0.34259527921676636\n",
      "[Training Epoch 0] Batch 3377, Loss 0.30642956495285034\n",
      "[Training Epoch 0] Batch 3378, Loss 0.27308547496795654\n",
      "[Training Epoch 0] Batch 3379, Loss 0.32991498708724976\n",
      "[Training Epoch 0] Batch 3380, Loss 0.31041255593299866\n",
      "[Training Epoch 0] Batch 3381, Loss 0.3165055513381958\n",
      "[Training Epoch 0] Batch 3382, Loss 0.30751001834869385\n",
      "[Training Epoch 0] Batch 3383, Loss 0.31059107184410095\n",
      "[Training Epoch 0] Batch 3384, Loss 0.3045898973941803\n",
      "[Training Epoch 0] Batch 3385, Loss 0.3249576687812805\n",
      "[Training Epoch 0] Batch 3386, Loss 0.32493048906326294\n",
      "[Training Epoch 0] Batch 3387, Loss 0.31872010231018066\n",
      "[Training Epoch 0] Batch 3388, Loss 0.3410455584526062\n",
      "[Training Epoch 0] Batch 3389, Loss 0.32423144578933716\n",
      "[Training Epoch 0] Batch 3390, Loss 0.31800132989883423\n",
      "[Training Epoch 0] Batch 3391, Loss 0.34612274169921875\n",
      "[Training Epoch 0] Batch 3392, Loss 0.3211313486099243\n",
      "[Training Epoch 0] Batch 3393, Loss 0.31776154041290283\n",
      "[Training Epoch 0] Batch 3394, Loss 0.3273336589336395\n",
      "[Training Epoch 0] Batch 3395, Loss 0.3207847476005554\n",
      "[Training Epoch 0] Batch 3396, Loss 0.34039831161499023\n",
      "[Training Epoch 0] Batch 3397, Loss 0.32937049865722656\n",
      "[Training Epoch 0] Batch 3398, Loss 0.3067866861820221\n",
      "[Training Epoch 0] Batch 3399, Loss 0.31166255474090576\n",
      "[Training Epoch 0] Batch 3400, Loss 0.3337559401988983\n",
      "[Training Epoch 0] Batch 3401, Loss 0.3376437723636627\n",
      "[Training Epoch 0] Batch 3402, Loss 0.2986111640930176\n",
      "[Training Epoch 0] Batch 3403, Loss 0.30481502413749695\n",
      "[Training Epoch 0] Batch 3404, Loss 0.32733839750289917\n",
      "[Training Epoch 0] Batch 3405, Loss 0.300731897354126\n",
      "[Training Epoch 0] Batch 3406, Loss 0.3384655714035034\n",
      "[Training Epoch 0] Batch 3407, Loss 0.26162171363830566\n",
      "[Training Epoch 0] Batch 3408, Loss 0.3265018165111542\n",
      "[Training Epoch 0] Batch 3409, Loss 0.3086567521095276\n",
      "[Training Epoch 0] Batch 3410, Loss 0.33689141273498535\n",
      "[Training Epoch 0] Batch 3411, Loss 0.3014654517173767\n",
      "[Training Epoch 0] Batch 3412, Loss 0.29977163672447205\n",
      "[Training Epoch 0] Batch 3413, Loss 0.34083378314971924\n",
      "[Training Epoch 0] Batch 3414, Loss 0.31722670793533325\n",
      "[Training Epoch 0] Batch 3415, Loss 0.3431808352470398\n",
      "[Training Epoch 0] Batch 3416, Loss 0.3327946364879608\n",
      "[Training Epoch 0] Batch 3417, Loss 0.3225855231285095\n",
      "[Training Epoch 0] Batch 3418, Loss 0.3298380374908447\n",
      "[Training Epoch 0] Batch 3419, Loss 0.33374953269958496\n",
      "[Training Epoch 0] Batch 3420, Loss 0.3229483962059021\n",
      "[Training Epoch 0] Batch 3421, Loss 0.3219870328903198\n",
      "[Training Epoch 0] Batch 3422, Loss 0.3002072870731354\n",
      "[Training Epoch 0] Batch 3423, Loss 0.33096742630004883\n",
      "[Training Epoch 0] Batch 3424, Loss 0.3107326030731201\n",
      "[Training Epoch 0] Batch 3425, Loss 0.3190150260925293\n",
      "[Training Epoch 0] Batch 3426, Loss 0.3102289140224457\n",
      "[Training Epoch 0] Batch 3427, Loss 0.3377189636230469\n",
      "[Training Epoch 0] Batch 3428, Loss 0.2952154874801636\n",
      "[Training Epoch 0] Batch 3429, Loss 0.306186705827713\n",
      "[Training Epoch 0] Batch 3430, Loss 0.3314182162284851\n",
      "[Training Epoch 0] Batch 3431, Loss 0.3302174210548401\n",
      "[Training Epoch 0] Batch 3432, Loss 0.3115084767341614\n",
      "[Training Epoch 0] Batch 3433, Loss 0.29187220335006714\n",
      "[Training Epoch 0] Batch 3434, Loss 0.337436318397522\n",
      "[Training Epoch 0] Batch 3435, Loss 0.3104456067085266\n",
      "[Training Epoch 0] Batch 3436, Loss 0.3066105842590332\n",
      "[Training Epoch 0] Batch 3437, Loss 0.33331140875816345\n",
      "[Training Epoch 0] Batch 3438, Loss 0.3302316665649414\n",
      "[Training Epoch 0] Batch 3439, Loss 0.3215269446372986\n",
      "[Training Epoch 0] Batch 3440, Loss 0.3117467164993286\n",
      "[Training Epoch 0] Batch 3441, Loss 0.3338795006275177\n",
      "[Training Epoch 0] Batch 3442, Loss 0.2935522794723511\n",
      "[Training Epoch 0] Batch 3443, Loss 0.3268197476863861\n",
      "[Training Epoch 0] Batch 3444, Loss 0.2921912372112274\n",
      "[Training Epoch 0] Batch 3445, Loss 0.31640759110450745\n",
      "[Training Epoch 0] Batch 3446, Loss 0.32045453786849976\n",
      "[Training Epoch 0] Batch 3447, Loss 0.3299553394317627\n",
      "[Training Epoch 0] Batch 3448, Loss 0.3225255608558655\n",
      "[Training Epoch 0] Batch 3449, Loss 0.31730175018310547\n",
      "[Training Epoch 0] Batch 3450, Loss 0.3340516686439514\n",
      "[Training Epoch 0] Batch 3451, Loss 0.3003534972667694\n",
      "[Training Epoch 0] Batch 3452, Loss 0.3060328960418701\n",
      "[Training Epoch 0] Batch 3453, Loss 0.3165154457092285\n",
      "[Training Epoch 0] Batch 3454, Loss 0.3188219666481018\n",
      "[Training Epoch 0] Batch 3455, Loss 0.2969898283481598\n",
      "[Training Epoch 0] Batch 3456, Loss 0.3436678647994995\n",
      "[Training Epoch 0] Batch 3457, Loss 0.3241233229637146\n",
      "[Training Epoch 0] Batch 3458, Loss 0.33517396450042725\n",
      "[Training Epoch 0] Batch 3459, Loss 0.32416027784347534\n",
      "[Training Epoch 0] Batch 3460, Loss 0.29477643966674805\n",
      "[Training Epoch 0] Batch 3461, Loss 0.34865862131118774\n",
      "[Training Epoch 0] Batch 3462, Loss 0.32806509733200073\n",
      "[Training Epoch 0] Batch 3463, Loss 0.30867505073547363\n",
      "[Training Epoch 0] Batch 3464, Loss 0.3201485276222229\n",
      "[Training Epoch 0] Batch 3465, Loss 0.30877166986465454\n",
      "[Training Epoch 0] Batch 3466, Loss 0.2899351119995117\n",
      "[Training Epoch 0] Batch 3467, Loss 0.3300023674964905\n",
      "[Training Epoch 0] Batch 3468, Loss 0.2870393395423889\n",
      "[Training Epoch 0] Batch 3469, Loss 0.339529424905777\n",
      "[Training Epoch 0] Batch 3470, Loss 0.30932700634002686\n",
      "[Training Epoch 0] Batch 3471, Loss 0.3113212585449219\n",
      "[Training Epoch 0] Batch 3472, Loss 0.32417917251586914\n",
      "[Training Epoch 0] Batch 3473, Loss 0.3449873626232147\n",
      "[Training Epoch 0] Batch 3474, Loss 0.32629624009132385\n",
      "[Training Epoch 0] Batch 3475, Loss 0.3318464457988739\n",
      "[Training Epoch 0] Batch 3476, Loss 0.30510950088500977\n",
      "[Training Epoch 0] Batch 3477, Loss 0.32713449001312256\n",
      "[Training Epoch 0] Batch 3478, Loss 0.3117963969707489\n",
      "[Training Epoch 0] Batch 3479, Loss 0.31651610136032104\n",
      "[Training Epoch 0] Batch 3480, Loss 0.304320752620697\n",
      "[Training Epoch 0] Batch 3481, Loss 0.3448832631111145\n",
      "[Training Epoch 0] Batch 3482, Loss 0.31474924087524414\n",
      "[Training Epoch 0] Batch 3483, Loss 0.3064700961112976\n",
      "[Training Epoch 0] Batch 3484, Loss 0.29567867517471313\n",
      "[Training Epoch 0] Batch 3485, Loss 0.33770543336868286\n",
      "[Training Epoch 0] Batch 3486, Loss 0.32162657380104065\n",
      "[Training Epoch 0] Batch 3487, Loss 0.33937162160873413\n",
      "[Training Epoch 0] Batch 3488, Loss 0.30211758613586426\n",
      "[Training Epoch 0] Batch 3489, Loss 0.31072041392326355\n",
      "[Training Epoch 0] Batch 3490, Loss 0.31358540058135986\n",
      "[Training Epoch 0] Batch 3491, Loss 0.28963756561279297\n",
      "[Training Epoch 0] Batch 3492, Loss 0.3110728859901428\n",
      "[Training Epoch 0] Batch 3493, Loss 0.3237084746360779\n",
      "[Training Epoch 0] Batch 3494, Loss 0.32873448729515076\n",
      "[Training Epoch 0] Batch 3495, Loss 0.30069196224212646\n",
      "[Training Epoch 0] Batch 3496, Loss 0.32856401801109314\n",
      "[Training Epoch 0] Batch 3497, Loss 0.32332679629325867\n",
      "[Training Epoch 0] Batch 3498, Loss 0.30402129888534546\n",
      "[Training Epoch 0] Batch 3499, Loss 0.3426695466041565\n",
      "[Training Epoch 0] Batch 3500, Loss 0.3178054690361023\n",
      "[Training Epoch 0] Batch 3501, Loss 0.303838312625885\n",
      "[Training Epoch 0] Batch 3502, Loss 0.3186787962913513\n",
      "[Training Epoch 0] Batch 3503, Loss 0.29593825340270996\n",
      "[Training Epoch 0] Batch 3504, Loss 0.30145928263664246\n",
      "[Training Epoch 0] Batch 3505, Loss 0.3021228313446045\n",
      "[Training Epoch 0] Batch 3506, Loss 0.32798266410827637\n",
      "[Training Epoch 0] Batch 3507, Loss 0.30463993549346924\n",
      "[Training Epoch 0] Batch 3508, Loss 0.32849207520484924\n",
      "[Training Epoch 0] Batch 3509, Loss 0.3402828276157379\n",
      "[Training Epoch 0] Batch 3510, Loss 0.3007678687572479\n",
      "[Training Epoch 0] Batch 3511, Loss 0.3116968274116516\n",
      "[Training Epoch 0] Batch 3512, Loss 0.30380043387413025\n",
      "[Training Epoch 0] Batch 3513, Loss 0.33410000801086426\n",
      "[Training Epoch 0] Batch 3514, Loss 0.30017489194869995\n",
      "[Training Epoch 0] Batch 3515, Loss 0.3292555809020996\n",
      "[Training Epoch 0] Batch 3516, Loss 0.3241252303123474\n",
      "[Training Epoch 0] Batch 3517, Loss 0.29760709404945374\n",
      "[Training Epoch 0] Batch 3518, Loss 0.3369044065475464\n",
      "[Training Epoch 0] Batch 3519, Loss 0.30674636363983154\n",
      "[Training Epoch 0] Batch 3520, Loss 0.32639145851135254\n",
      "[Training Epoch 0] Batch 3521, Loss 0.31915828585624695\n",
      "[Training Epoch 0] Batch 3522, Loss 0.3289888799190521\n",
      "[Training Epoch 0] Batch 3523, Loss 0.34282875061035156\n",
      "[Training Epoch 0] Batch 3524, Loss 0.334763765335083\n",
      "[Training Epoch 0] Batch 3525, Loss 0.3241036534309387\n",
      "[Training Epoch 0] Batch 3526, Loss 0.29359501600265503\n",
      "[Training Epoch 0] Batch 3527, Loss 0.3035762906074524\n",
      "[Training Epoch 0] Batch 3528, Loss 0.3311956524848938\n",
      "[Training Epoch 0] Batch 3529, Loss 0.3218733072280884\n",
      "[Training Epoch 0] Batch 3530, Loss 0.3016700744628906\n",
      "[Training Epoch 0] Batch 3531, Loss 0.32538700103759766\n",
      "[Training Epoch 0] Batch 3532, Loss 0.3097010850906372\n",
      "[Training Epoch 0] Batch 3533, Loss 0.3445214331150055\n",
      "[Training Epoch 0] Batch 3534, Loss 0.30764204263687134\n",
      "[Training Epoch 0] Batch 3535, Loss 0.3267419934272766\n",
      "[Training Epoch 0] Batch 3536, Loss 0.31953978538513184\n",
      "[Training Epoch 0] Batch 3537, Loss 0.33018025755882263\n",
      "[Training Epoch 0] Batch 3538, Loss 0.3346967101097107\n",
      "[Training Epoch 0] Batch 3539, Loss 0.3346942663192749\n",
      "[Training Epoch 0] Batch 3540, Loss 0.3043034076690674\n",
      "[Training Epoch 0] Batch 3541, Loss 0.3205019235610962\n",
      "[Training Epoch 0] Batch 3542, Loss 0.32842427492141724\n",
      "[Training Epoch 0] Batch 3543, Loss 0.27988243103027344\n",
      "[Training Epoch 0] Batch 3544, Loss 0.3584606647491455\n",
      "[Training Epoch 0] Batch 3545, Loss 0.31179291009902954\n",
      "[Training Epoch 0] Batch 3546, Loss 0.3303135335445404\n",
      "[Training Epoch 0] Batch 3547, Loss 0.33124953508377075\n",
      "[Training Epoch 0] Batch 3548, Loss 0.32227301597595215\n",
      "[Training Epoch 0] Batch 3549, Loss 0.28865742683410645\n",
      "[Training Epoch 0] Batch 3550, Loss 0.3298015594482422\n",
      "[Training Epoch 0] Batch 3551, Loss 0.33951127529144287\n",
      "[Training Epoch 0] Batch 3552, Loss 0.32441139221191406\n",
      "[Training Epoch 0] Batch 3553, Loss 0.335786908864975\n",
      "[Training Epoch 0] Batch 3554, Loss 0.2834700644016266\n",
      "[Training Epoch 0] Batch 3555, Loss 0.3431173264980316\n",
      "[Training Epoch 0] Batch 3556, Loss 0.3377053141593933\n",
      "[Training Epoch 0] Batch 3557, Loss 0.3516266345977783\n",
      "[Training Epoch 0] Batch 3558, Loss 0.3217755854129791\n",
      "[Training Epoch 0] Batch 3559, Loss 0.3224695920944214\n",
      "[Training Epoch 0] Batch 3560, Loss 0.3198949098587036\n",
      "[Training Epoch 0] Batch 3561, Loss 0.3195839524269104\n",
      "[Training Epoch 0] Batch 3562, Loss 0.355811208486557\n",
      "[Training Epoch 0] Batch 3563, Loss 0.3200879991054535\n",
      "[Training Epoch 0] Batch 3564, Loss 0.3347245454788208\n",
      "[Training Epoch 0] Batch 3565, Loss 0.3036288619041443\n",
      "[Training Epoch 0] Batch 3566, Loss 0.30886805057525635\n",
      "[Training Epoch 0] Batch 3567, Loss 0.30100488662719727\n",
      "[Training Epoch 0] Batch 3568, Loss 0.3041720986366272\n",
      "[Training Epoch 0] Batch 3569, Loss 0.34161049127578735\n",
      "[Training Epoch 0] Batch 3570, Loss 0.2951808571815491\n",
      "[Training Epoch 0] Batch 3571, Loss 0.33010923862457275\n",
      "[Training Epoch 0] Batch 3572, Loss 0.31364524364471436\n",
      "[Training Epoch 0] Batch 3573, Loss 0.346733033657074\n",
      "[Training Epoch 0] Batch 3574, Loss 0.3118237555027008\n",
      "[Training Epoch 0] Batch 3575, Loss 0.34479963779449463\n",
      "[Training Epoch 0] Batch 3576, Loss 0.3207266926765442\n",
      "[Training Epoch 0] Batch 3577, Loss 0.3102327585220337\n",
      "[Training Epoch 0] Batch 3578, Loss 0.31920093297958374\n",
      "[Training Epoch 0] Batch 3579, Loss 0.36087530851364136\n",
      "[Training Epoch 0] Batch 3580, Loss 0.2750411331653595\n",
      "[Training Epoch 0] Batch 3581, Loss 0.30879390239715576\n",
      "[Training Epoch 0] Batch 3582, Loss 0.3132841885089874\n",
      "[Training Epoch 0] Batch 3583, Loss 0.3223055601119995\n",
      "[Training Epoch 0] Batch 3584, Loss 0.33578813076019287\n",
      "[Training Epoch 0] Batch 3585, Loss 0.3206830322742462\n",
      "[Training Epoch 0] Batch 3586, Loss 0.31209197640419006\n",
      "[Training Epoch 0] Batch 3587, Loss 0.3171818256378174\n",
      "[Training Epoch 0] Batch 3588, Loss 0.3116838335990906\n",
      "[Training Epoch 0] Batch 3589, Loss 0.3221093714237213\n",
      "[Training Epoch 0] Batch 3590, Loss 0.307731032371521\n",
      "[Training Epoch 0] Batch 3591, Loss 0.315944105386734\n",
      "[Training Epoch 0] Batch 3592, Loss 0.3168110251426697\n",
      "[Training Epoch 0] Batch 3593, Loss 0.3384939730167389\n",
      "[Training Epoch 0] Batch 3594, Loss 0.3523426055908203\n",
      "[Training Epoch 0] Batch 3595, Loss 0.3289719223976135\n",
      "[Training Epoch 0] Batch 3596, Loss 0.2976214587688446\n",
      "[Training Epoch 0] Batch 3597, Loss 0.32361268997192383\n",
      "[Training Epoch 0] Batch 3598, Loss 0.2973092198371887\n",
      "[Training Epoch 0] Batch 3599, Loss 0.29786768555641174\n",
      "[Training Epoch 0] Batch 3600, Loss 0.2846345901489258\n",
      "[Training Epoch 0] Batch 3601, Loss 0.33340710401535034\n",
      "[Training Epoch 0] Batch 3602, Loss 0.32700037956237793\n",
      "[Training Epoch 0] Batch 3603, Loss 0.3073785901069641\n",
      "[Training Epoch 0] Batch 3604, Loss 0.3337642252445221\n",
      "[Training Epoch 0] Batch 3605, Loss 0.30611294507980347\n",
      "[Training Epoch 0] Batch 3606, Loss 0.34544679522514343\n",
      "[Training Epoch 0] Batch 3607, Loss 0.31864094734191895\n",
      "[Training Epoch 0] Batch 3608, Loss 0.3323574662208557\n",
      "[Training Epoch 0] Batch 3609, Loss 0.32148775458335876\n",
      "[Training Epoch 0] Batch 3610, Loss 0.29423755407333374\n",
      "[Training Epoch 0] Batch 3611, Loss 0.3337734639644623\n",
      "[Training Epoch 0] Batch 3612, Loss 0.3124251961708069\n",
      "[Training Epoch 0] Batch 3613, Loss 0.3053181767463684\n",
      "[Training Epoch 0] Batch 3614, Loss 0.2997112274169922\n",
      "[Training Epoch 0] Batch 3615, Loss 0.31241005659103394\n",
      "[Training Epoch 0] Batch 3616, Loss 0.3202618360519409\n",
      "[Training Epoch 0] Batch 3617, Loss 0.30961179733276367\n",
      "[Training Epoch 0] Batch 3618, Loss 0.29656529426574707\n",
      "[Training Epoch 0] Batch 3619, Loss 0.35690292716026306\n",
      "[Training Epoch 0] Batch 3620, Loss 0.32052794098854065\n",
      "[Training Epoch 0] Batch 3621, Loss 0.3113984167575836\n",
      "[Training Epoch 0] Batch 3622, Loss 0.3073068857192993\n",
      "[Training Epoch 0] Batch 3623, Loss 0.2981308102607727\n",
      "[Training Epoch 0] Batch 3624, Loss 0.29162824153900146\n",
      "[Training Epoch 0] Batch 3625, Loss 0.30159470438957214\n",
      "[Training Epoch 0] Batch 3626, Loss 0.32676494121551514\n",
      "[Training Epoch 0] Batch 3627, Loss 0.3226598799228668\n",
      "[Training Epoch 0] Batch 3628, Loss 0.31720778346061707\n",
      "[Training Epoch 0] Batch 3629, Loss 0.33324134349823\n",
      "[Training Epoch 0] Batch 3630, Loss 0.3222678005695343\n",
      "[Training Epoch 0] Batch 3631, Loss 0.3218374252319336\n",
      "[Training Epoch 0] Batch 3632, Loss 0.3135620355606079\n",
      "[Training Epoch 0] Batch 3633, Loss 0.32321101427078247\n",
      "[Training Epoch 0] Batch 3634, Loss 0.31931036710739136\n",
      "[Training Epoch 0] Batch 3635, Loss 0.3121181130409241\n",
      "[Training Epoch 0] Batch 3636, Loss 0.3024289608001709\n",
      "[Training Epoch 0] Batch 3637, Loss 0.3355885148048401\n",
      "[Training Epoch 0] Batch 3638, Loss 0.3181888461112976\n",
      "[Training Epoch 0] Batch 3639, Loss 0.3139519691467285\n",
      "[Training Epoch 0] Batch 3640, Loss 0.2933841347694397\n",
      "[Training Epoch 0] Batch 3641, Loss 0.29272550344467163\n",
      "[Training Epoch 0] Batch 3642, Loss 0.34268826246261597\n",
      "[Training Epoch 0] Batch 3643, Loss 0.32458171248435974\n",
      "[Training Epoch 0] Batch 3644, Loss 0.3097626566886902\n",
      "[Training Epoch 0] Batch 3645, Loss 0.3021582365036011\n",
      "[Training Epoch 0] Batch 3646, Loss 0.3172203302383423\n",
      "[Training Epoch 0] Batch 3647, Loss 0.3163474202156067\n",
      "[Training Epoch 0] Batch 3648, Loss 0.30145400762557983\n",
      "[Training Epoch 0] Batch 3649, Loss 0.3105839490890503\n",
      "[Training Epoch 0] Batch 3650, Loss 0.2962089478969574\n",
      "[Training Epoch 0] Batch 3651, Loss 0.3153391480445862\n",
      "[Training Epoch 0] Batch 3652, Loss 0.30520087480545044\n",
      "[Training Epoch 0] Batch 3653, Loss 0.28995177149772644\n",
      "[Training Epoch 0] Batch 3654, Loss 0.32507020235061646\n",
      "[Training Epoch 0] Batch 3655, Loss 0.32105475664138794\n",
      "[Training Epoch 0] Batch 3656, Loss 0.3330608606338501\n",
      "[Training Epoch 0] Batch 3657, Loss 0.3227214515209198\n",
      "[Training Epoch 0] Batch 3658, Loss 0.32019782066345215\n",
      "[Training Epoch 0] Batch 3659, Loss 0.3468833267688751\n",
      "[Training Epoch 0] Batch 3660, Loss 0.3089236617088318\n",
      "[Training Epoch 0] Batch 3661, Loss 0.3261762857437134\n",
      "[Training Epoch 0] Batch 3662, Loss 0.33911576867103577\n",
      "[Training Epoch 0] Batch 3663, Loss 0.3116661012172699\n",
      "[Training Epoch 0] Batch 3664, Loss 0.33148807287216187\n",
      "[Training Epoch 0] Batch 3665, Loss 0.33482539653778076\n",
      "[Training Epoch 0] Batch 3666, Loss 0.3509715497493744\n",
      "[Training Epoch 0] Batch 3667, Loss 0.3299819231033325\n",
      "[Training Epoch 0] Batch 3668, Loss 0.3545483350753784\n",
      "[Training Epoch 0] Batch 3669, Loss 0.3205792307853699\n",
      "[Training Epoch 0] Batch 3670, Loss 0.3142428398132324\n",
      "[Training Epoch 0] Batch 3671, Loss 0.32185298204421997\n",
      "[Training Epoch 0] Batch 3672, Loss 0.31033816933631897\n",
      "[Training Epoch 0] Batch 3673, Loss 0.32957470417022705\n",
      "[Training Epoch 0] Batch 3674, Loss 0.3062743842601776\n",
      "[Training Epoch 0] Batch 3675, Loss 0.33427098393440247\n",
      "[Training Epoch 0] Batch 3676, Loss 0.31132352352142334\n",
      "[Training Epoch 0] Batch 3677, Loss 0.30714935064315796\n",
      "[Training Epoch 0] Batch 3678, Loss 0.34518980979919434\n",
      "[Training Epoch 0] Batch 3679, Loss 0.32003188133239746\n",
      "[Training Epoch 0] Batch 3680, Loss 0.3081570267677307\n",
      "[Training Epoch 0] Batch 3681, Loss 0.3111368417739868\n",
      "[Training Epoch 0] Batch 3682, Loss 0.29706281423568726\n",
      "[Training Epoch 0] Batch 3683, Loss 0.31438422203063965\n",
      "[Training Epoch 0] Batch 3684, Loss 0.3318096995353699\n",
      "[Training Epoch 0] Batch 3685, Loss 0.30884239077568054\n",
      "[Training Epoch 0] Batch 3686, Loss 0.319675087928772\n",
      "[Training Epoch 0] Batch 3687, Loss 0.29927805066108704\n",
      "[Training Epoch 0] Batch 3688, Loss 0.3358451724052429\n",
      "[Training Epoch 0] Batch 3689, Loss 0.31490659713745117\n",
      "[Training Epoch 0] Batch 3690, Loss 0.31224408745765686\n",
      "[Training Epoch 0] Batch 3691, Loss 0.35139554738998413\n",
      "[Training Epoch 0] Batch 3692, Loss 0.3430198132991791\n",
      "[Training Epoch 0] Batch 3693, Loss 0.3369232416152954\n",
      "[Training Epoch 0] Batch 3694, Loss 0.3095109462738037\n",
      "[Training Epoch 0] Batch 3695, Loss 0.3418220281600952\n",
      "[Training Epoch 0] Batch 3696, Loss 0.3308368921279907\n",
      "[Training Epoch 0] Batch 3697, Loss 0.326129287481308\n",
      "[Training Epoch 0] Batch 3698, Loss 0.32526570558547974\n",
      "[Training Epoch 0] Batch 3699, Loss 0.33317965269088745\n",
      "[Training Epoch 0] Batch 3700, Loss 0.3354807496070862\n",
      "[Training Epoch 0] Batch 3701, Loss 0.31777942180633545\n",
      "[Training Epoch 0] Batch 3702, Loss 0.33009573817253113\n",
      "[Training Epoch 0] Batch 3703, Loss 0.3052116334438324\n",
      "[Training Epoch 0] Batch 3704, Loss 0.31511953473091125\n",
      "[Training Epoch 0] Batch 3705, Loss 0.3220887780189514\n",
      "[Training Epoch 0] Batch 3706, Loss 0.31470754742622375\n",
      "[Training Epoch 0] Batch 3707, Loss 0.3406396508216858\n",
      "[Training Epoch 0] Batch 3708, Loss 0.29802241921424866\n",
      "[Training Epoch 0] Batch 3709, Loss 0.31731826066970825\n",
      "[Training Epoch 0] Batch 3710, Loss 0.3112087547779083\n",
      "[Training Epoch 0] Batch 3711, Loss 0.3042409121990204\n",
      "[Training Epoch 0] Batch 3712, Loss 0.2961384952068329\n",
      "[Training Epoch 0] Batch 3713, Loss 0.31896746158599854\n",
      "[Training Epoch 0] Batch 3714, Loss 0.2983403205871582\n",
      "[Training Epoch 0] Batch 3715, Loss 0.3339099884033203\n",
      "[Training Epoch 0] Batch 3716, Loss 0.3032875061035156\n",
      "[Training Epoch 0] Batch 3717, Loss 0.32108187675476074\n",
      "[Training Epoch 0] Batch 3718, Loss 0.32442206144332886\n",
      "[Training Epoch 0] Batch 3719, Loss 0.32190120220184326\n",
      "[Training Epoch 0] Batch 3720, Loss 0.3411140441894531\n",
      "[Training Epoch 0] Batch 3721, Loss 0.29217371344566345\n",
      "[Training Epoch 0] Batch 3722, Loss 0.3202986717224121\n",
      "[Training Epoch 0] Batch 3723, Loss 0.3043380379676819\n",
      "[Training Epoch 0] Batch 3724, Loss 0.3355432152748108\n",
      "[Training Epoch 0] Batch 3725, Loss 0.29436635971069336\n",
      "[Training Epoch 0] Batch 3726, Loss 0.33469244837760925\n",
      "[Training Epoch 0] Batch 3727, Loss 0.3184625506401062\n",
      "[Training Epoch 0] Batch 3728, Loss 0.2950838506221771\n",
      "[Training Epoch 0] Batch 3729, Loss 0.31693780422210693\n",
      "[Training Epoch 0] Batch 3730, Loss 0.2938086986541748\n",
      "[Training Epoch 0] Batch 3731, Loss 0.3388717770576477\n",
      "[Training Epoch 0] Batch 3732, Loss 0.3289942145347595\n",
      "[Training Epoch 0] Batch 3733, Loss 0.333871066570282\n",
      "[Training Epoch 0] Batch 3734, Loss 0.3134954571723938\n",
      "[Training Epoch 0] Batch 3735, Loss 0.30011337995529175\n",
      "[Training Epoch 0] Batch 3736, Loss 0.3167988061904907\n",
      "[Training Epoch 0] Batch 3737, Loss 0.3038631081581116\n",
      "[Training Epoch 0] Batch 3738, Loss 0.30408424139022827\n",
      "[Training Epoch 0] Batch 3739, Loss 0.28584539890289307\n",
      "[Training Epoch 0] Batch 3740, Loss 0.3036152422428131\n",
      "[Training Epoch 0] Batch 3741, Loss 0.3032405972480774\n",
      "[Training Epoch 0] Batch 3742, Loss 0.2873639464378357\n",
      "[Training Epoch 0] Batch 3743, Loss 0.2909480929374695\n",
      "[Training Epoch 0] Batch 3744, Loss 0.3263016641139984\n",
      "[Training Epoch 0] Batch 3745, Loss 0.3424474895000458\n",
      "[Training Epoch 0] Batch 3746, Loss 0.313320517539978\n",
      "[Training Epoch 0] Batch 3747, Loss 0.295432984828949\n",
      "[Training Epoch 0] Batch 3748, Loss 0.33590590953826904\n",
      "[Training Epoch 0] Batch 3749, Loss 0.3265020251274109\n",
      "[Training Epoch 0] Batch 3750, Loss 0.28020554780960083\n",
      "[Training Epoch 0] Batch 3751, Loss 0.3003177046775818\n",
      "[Training Epoch 0] Batch 3752, Loss 0.33982565999031067\n",
      "[Training Epoch 0] Batch 3753, Loss 0.29367613792419434\n",
      "[Training Epoch 0] Batch 3754, Loss 0.3037356734275818\n",
      "[Training Epoch 0] Batch 3755, Loss 0.32876056432724\n",
      "[Training Epoch 0] Batch 3756, Loss 0.321724534034729\n",
      "[Training Epoch 0] Batch 3757, Loss 0.32287538051605225\n",
      "[Training Epoch 0] Batch 3758, Loss 0.3146950304508209\n",
      "[Training Epoch 0] Batch 3759, Loss 0.3145095705986023\n",
      "[Training Epoch 0] Batch 3760, Loss 0.3103620111942291\n",
      "[Training Epoch 0] Batch 3761, Loss 0.29648709297180176\n",
      "[Training Epoch 0] Batch 3762, Loss 0.3301035165786743\n",
      "[Training Epoch 0] Batch 3763, Loss 0.340840220451355\n",
      "[Training Epoch 0] Batch 3764, Loss 0.31341925263404846\n",
      "[Training Epoch 0] Batch 3765, Loss 0.2778003215789795\n",
      "[Training Epoch 0] Batch 3766, Loss 0.32600167393684387\n",
      "[Training Epoch 0] Batch 3767, Loss 0.3134481906890869\n",
      "[Training Epoch 0] Batch 3768, Loss 0.30924031138420105\n",
      "[Training Epoch 0] Batch 3769, Loss 0.2970063090324402\n",
      "[Training Epoch 0] Batch 3770, Loss 0.34331512451171875\n",
      "[Training Epoch 0] Batch 3771, Loss 0.3117941915988922\n",
      "[Training Epoch 0] Batch 3772, Loss 0.29239606857299805\n",
      "[Training Epoch 0] Batch 3773, Loss 0.3374171853065491\n",
      "[Training Epoch 0] Batch 3774, Loss 0.3317359685897827\n",
      "[Training Epoch 0] Batch 3775, Loss 0.3225940465927124\n",
      "[Training Epoch 0] Batch 3776, Loss 0.2879939377307892\n",
      "[Training Epoch 0] Batch 3777, Loss 0.3111528158187866\n",
      "[Training Epoch 0] Batch 3778, Loss 0.29823094606399536\n",
      "[Training Epoch 0] Batch 3779, Loss 0.3261566758155823\n",
      "[Training Epoch 0] Batch 3780, Loss 0.29506024718284607\n",
      "[Training Epoch 0] Batch 3781, Loss 0.34186285734176636\n",
      "[Training Epoch 0] Batch 3782, Loss 0.3248860538005829\n",
      "[Training Epoch 0] Batch 3783, Loss 0.33004432916641235\n",
      "[Training Epoch 0] Batch 3784, Loss 0.29644346237182617\n",
      "[Training Epoch 0] Batch 3785, Loss 0.3031928539276123\n",
      "[Training Epoch 0] Batch 3786, Loss 0.33497297763824463\n",
      "[Training Epoch 0] Batch 3787, Loss 0.33519619703292847\n",
      "[Training Epoch 0] Batch 3788, Loss 0.31826773285865784\n",
      "[Training Epoch 0] Batch 3789, Loss 0.30601966381073\n",
      "[Training Epoch 0] Batch 3790, Loss 0.3090217709541321\n",
      "[Training Epoch 0] Batch 3791, Loss 0.32192331552505493\n",
      "[Training Epoch 0] Batch 3792, Loss 0.3142760992050171\n",
      "[Training Epoch 0] Batch 3793, Loss 0.3228003978729248\n",
      "[Training Epoch 0] Batch 3794, Loss 0.3171461522579193\n",
      "[Training Epoch 0] Batch 3795, Loss 0.3306688666343689\n",
      "[Training Epoch 0] Batch 3796, Loss 0.33148109912872314\n",
      "[Training Epoch 0] Batch 3797, Loss 0.3377838730812073\n",
      "[Training Epoch 0] Batch 3798, Loss 0.32551005482673645\n",
      "[Training Epoch 0] Batch 3799, Loss 0.28080934286117554\n",
      "[Training Epoch 0] Batch 3800, Loss 0.32891514897346497\n",
      "[Training Epoch 0] Batch 3801, Loss 0.31653547286987305\n",
      "[Training Epoch 0] Batch 3802, Loss 0.29082146286964417\n",
      "[Training Epoch 0] Batch 3803, Loss 0.28794580698013306\n",
      "[Training Epoch 0] Batch 3804, Loss 0.3123925030231476\n",
      "[Training Epoch 0] Batch 3805, Loss 0.2899278402328491\n",
      "[Training Epoch 0] Batch 3806, Loss 0.31755709648132324\n",
      "[Training Epoch 0] Batch 3807, Loss 0.32228636741638184\n",
      "[Training Epoch 0] Batch 3808, Loss 0.3197746276855469\n",
      "[Training Epoch 0] Batch 3809, Loss 0.3060947060585022\n",
      "[Training Epoch 0] Batch 3810, Loss 0.284929096698761\n",
      "[Training Epoch 0] Batch 3811, Loss 0.334432989358902\n",
      "[Training Epoch 0] Batch 3812, Loss 0.30653706192970276\n",
      "[Training Epoch 0] Batch 3813, Loss 0.3102019131183624\n",
      "[Training Epoch 0] Batch 3814, Loss 0.31204038858413696\n",
      "[Training Epoch 0] Batch 3815, Loss 0.3446342349052429\n",
      "[Training Epoch 0] Batch 3816, Loss 0.33387520909309387\n",
      "[Training Epoch 0] Batch 3817, Loss 0.288704514503479\n",
      "[Training Epoch 0] Batch 3818, Loss 0.30964037775993347\n",
      "[Training Epoch 0] Batch 3819, Loss 0.3037783205509186\n",
      "[Training Epoch 0] Batch 3820, Loss 0.27674466371536255\n",
      "[Training Epoch 0] Batch 3821, Loss 0.29004526138305664\n",
      "[Training Epoch 0] Batch 3822, Loss 0.32343798875808716\n",
      "[Training Epoch 0] Batch 3823, Loss 0.3031633496284485\n",
      "[Training Epoch 0] Batch 3824, Loss 0.31581300497055054\n",
      "[Training Epoch 0] Batch 3825, Loss 0.28031837940216064\n",
      "[Training Epoch 0] Batch 3826, Loss 0.3137810230255127\n",
      "[Training Epoch 0] Batch 3827, Loss 0.31265074014663696\n",
      "[Training Epoch 0] Batch 3828, Loss 0.31456518173217773\n",
      "[Training Epoch 0] Batch 3829, Loss 0.31088390946388245\n",
      "[Training Epoch 0] Batch 3830, Loss 0.3140811026096344\n",
      "[Training Epoch 0] Batch 3831, Loss 0.30888426303863525\n",
      "[Training Epoch 0] Batch 3832, Loss 0.3220040798187256\n",
      "[Training Epoch 0] Batch 3833, Loss 0.30484944581985474\n",
      "[Training Epoch 0] Batch 3834, Loss 0.311910480260849\n",
      "[Training Epoch 0] Batch 3835, Loss 0.31690508127212524\n",
      "[Training Epoch 0] Batch 3836, Loss 0.3049824833869934\n",
      "[Training Epoch 0] Batch 3837, Loss 0.32646217942237854\n",
      "[Training Epoch 0] Batch 3838, Loss 0.3368944227695465\n",
      "[Training Epoch 0] Batch 3839, Loss 0.30746036767959595\n",
      "[Training Epoch 0] Batch 3840, Loss 0.31550133228302\n",
      "[Training Epoch 0] Batch 3841, Loss 0.32942524552345276\n",
      "[Training Epoch 0] Batch 3842, Loss 0.27327150106430054\n",
      "[Training Epoch 0] Batch 3843, Loss 0.3454875946044922\n",
      "[Training Epoch 0] Batch 3844, Loss 0.31657499074935913\n",
      "[Training Epoch 0] Batch 3845, Loss 0.3195244073867798\n",
      "[Training Epoch 0] Batch 3846, Loss 0.31261327862739563\n",
      "[Training Epoch 0] Batch 3847, Loss 0.30748605728149414\n",
      "[Training Epoch 0] Batch 3848, Loss 0.33174562454223633\n",
      "[Training Epoch 0] Batch 3849, Loss 0.3147125244140625\n",
      "[Training Epoch 0] Batch 3850, Loss 0.31464076042175293\n",
      "[Training Epoch 0] Batch 3851, Loss 0.29261523485183716\n",
      "[Training Epoch 0] Batch 3852, Loss 0.32706698775291443\n",
      "[Training Epoch 0] Batch 3853, Loss 0.32362300157546997\n",
      "[Training Epoch 0] Batch 3854, Loss 0.30965375900268555\n",
      "[Training Epoch 0] Batch 3855, Loss 0.3308483362197876\n",
      "[Training Epoch 0] Batch 3856, Loss 0.3332001268863678\n",
      "[Training Epoch 0] Batch 3857, Loss 0.30969342589378357\n",
      "[Training Epoch 0] Batch 3858, Loss 0.31614065170288086\n",
      "[Training Epoch 0] Batch 3859, Loss 0.3253228962421417\n",
      "[Training Epoch 0] Batch 3860, Loss 0.33477360010147095\n",
      "[Training Epoch 0] Batch 3861, Loss 0.2956256568431854\n",
      "[Training Epoch 0] Batch 3862, Loss 0.3052813410758972\n",
      "[Training Epoch 0] Batch 3863, Loss 0.3080388903617859\n",
      "[Training Epoch 0] Batch 3864, Loss 0.3160765767097473\n",
      "[Training Epoch 0] Batch 3865, Loss 0.3008483052253723\n",
      "[Training Epoch 0] Batch 3866, Loss 0.3116872310638428\n",
      "[Training Epoch 0] Batch 3867, Loss 0.3136255741119385\n",
      "[Training Epoch 0] Batch 3868, Loss 0.29582157731056213\n",
      "[Training Epoch 0] Batch 3869, Loss 0.31291067600250244\n",
      "[Training Epoch 0] Batch 3870, Loss 0.285918653011322\n",
      "[Training Epoch 0] Batch 3871, Loss 0.29409259557724\n",
      "[Training Epoch 0] Batch 3872, Loss 0.29053574800491333\n",
      "[Training Epoch 0] Batch 3873, Loss 0.30550581216812134\n",
      "[Training Epoch 0] Batch 3874, Loss 0.3232029676437378\n",
      "[Training Epoch 0] Batch 3875, Loss 0.30377888679504395\n",
      "[Training Epoch 0] Batch 3876, Loss 0.30134767293930054\n",
      "[Training Epoch 0] Batch 3877, Loss 0.3089439868927002\n",
      "[Training Epoch 0] Batch 3878, Loss 0.32026517391204834\n",
      "[Training Epoch 0] Batch 3879, Loss 0.31265342235565186\n",
      "[Training Epoch 0] Batch 3880, Loss 0.30429697036743164\n",
      "[Training Epoch 0] Batch 3881, Loss 0.30731818079948425\n",
      "[Training Epoch 0] Batch 3882, Loss 0.3175068497657776\n",
      "[Training Epoch 0] Batch 3883, Loss 0.31497901678085327\n",
      "[Training Epoch 0] Batch 3884, Loss 0.2999332547187805\n",
      "[Training Epoch 0] Batch 3885, Loss 0.3096194863319397\n",
      "[Training Epoch 0] Batch 3886, Loss 0.31245917081832886\n",
      "[Training Epoch 0] Batch 3887, Loss 0.30914372205734253\n",
      "[Training Epoch 0] Batch 3888, Loss 0.3078193664550781\n",
      "[Training Epoch 0] Batch 3889, Loss 0.32453492283821106\n",
      "[Training Epoch 0] Batch 3890, Loss 0.32960858941078186\n",
      "[Training Epoch 0] Batch 3891, Loss 0.3247372508049011\n",
      "[Training Epoch 0] Batch 3892, Loss 0.31310921907424927\n",
      "[Training Epoch 0] Batch 3893, Loss 0.309005469083786\n",
      "[Training Epoch 0] Batch 3894, Loss 0.3057147264480591\n",
      "[Training Epoch 0] Batch 3895, Loss 0.31898659467697144\n",
      "[Training Epoch 0] Batch 3896, Loss 0.3244604170322418\n",
      "[Training Epoch 0] Batch 3897, Loss 0.31816405057907104\n",
      "[Training Epoch 0] Batch 3898, Loss 0.3149067759513855\n",
      "[Training Epoch 0] Batch 3899, Loss 0.30875205993652344\n",
      "[Training Epoch 0] Batch 3900, Loss 0.30350059270858765\n",
      "[Training Epoch 0] Batch 3901, Loss 0.32141369581222534\n",
      "[Training Epoch 0] Batch 3902, Loss 0.2921577990055084\n",
      "[Training Epoch 0] Batch 3903, Loss 0.33104389905929565\n",
      "[Training Epoch 0] Batch 3904, Loss 0.2990613281726837\n",
      "[Training Epoch 0] Batch 3905, Loss 0.33889853954315186\n",
      "[Training Epoch 0] Batch 3906, Loss 0.3359123468399048\n",
      "[Training Epoch 0] Batch 3907, Loss 0.3103103041648865\n",
      "[Training Epoch 0] Batch 3908, Loss 0.3222106695175171\n",
      "[Training Epoch 0] Batch 3909, Loss 0.3039711117744446\n",
      "[Training Epoch 0] Batch 3910, Loss 0.33253204822540283\n",
      "[Training Epoch 0] Batch 3911, Loss 0.2859782576560974\n",
      "[Training Epoch 0] Batch 3912, Loss 0.2981365919113159\n",
      "[Training Epoch 0] Batch 3913, Loss 0.3205673098564148\n",
      "[Training Epoch 0] Batch 3914, Loss 0.30427736043930054\n",
      "[Training Epoch 0] Batch 3915, Loss 0.3158496618270874\n",
      "[Training Epoch 0] Batch 3916, Loss 0.31338897347450256\n",
      "[Training Epoch 0] Batch 3917, Loss 0.323950856924057\n",
      "[Training Epoch 0] Batch 3918, Loss 0.3027293384075165\n",
      "[Training Epoch 0] Batch 3919, Loss 0.31514349579811096\n",
      "[Training Epoch 0] Batch 3920, Loss 0.2966004014015198\n",
      "[Training Epoch 0] Batch 3921, Loss 0.3112351596355438\n",
      "[Training Epoch 0] Batch 3922, Loss 0.31273385882377625\n",
      "[Training Epoch 0] Batch 3923, Loss 0.29018548130989075\n",
      "[Training Epoch 0] Batch 3924, Loss 0.31577068567276\n",
      "[Training Epoch 0] Batch 3925, Loss 0.3299068808555603\n",
      "[Training Epoch 0] Batch 3926, Loss 0.3215923607349396\n",
      "[Training Epoch 0] Batch 3927, Loss 0.3145444393157959\n",
      "[Training Epoch 0] Batch 3928, Loss 0.333194762468338\n",
      "[Training Epoch 0] Batch 3929, Loss 0.2895382344722748\n",
      "[Training Epoch 0] Batch 3930, Loss 0.31313955783843994\n",
      "[Training Epoch 0] Batch 3931, Loss 0.31811416149139404\n",
      "[Training Epoch 0] Batch 3932, Loss 0.3503342568874359\n",
      "[Training Epoch 0] Batch 3933, Loss 0.3309965133666992\n",
      "[Training Epoch 0] Batch 3934, Loss 0.3265252709388733\n",
      "[Training Epoch 0] Batch 3935, Loss 0.3139280080795288\n",
      "[Training Epoch 0] Batch 3936, Loss 0.326556533575058\n",
      "[Training Epoch 0] Batch 3937, Loss 0.3091237545013428\n",
      "[Training Epoch 0] Batch 3938, Loss 0.3426736295223236\n",
      "[Training Epoch 0] Batch 3939, Loss 0.32576635479927063\n",
      "[Training Epoch 0] Batch 3940, Loss 0.32659417390823364\n",
      "[Training Epoch 0] Batch 3941, Loss 0.34138137102127075\n",
      "[Training Epoch 0] Batch 3942, Loss 0.29500770568847656\n",
      "[Training Epoch 0] Batch 3943, Loss 0.3449539542198181\n",
      "[Training Epoch 0] Batch 3944, Loss 0.29002493619918823\n",
      "[Training Epoch 0] Batch 3945, Loss 0.3257310390472412\n",
      "[Training Epoch 0] Batch 3946, Loss 0.29817384481430054\n",
      "[Training Epoch 0] Batch 3947, Loss 0.3041912913322449\n",
      "[Training Epoch 0] Batch 3948, Loss 0.3126925528049469\n",
      "[Training Epoch 0] Batch 3949, Loss 0.3281501531600952\n",
      "[Training Epoch 0] Batch 3950, Loss 0.3107762336730957\n",
      "[Training Epoch 0] Batch 3951, Loss 0.3191118836402893\n",
      "[Training Epoch 0] Batch 3952, Loss 0.28773751854896545\n",
      "[Training Epoch 0] Batch 3953, Loss 0.3145279884338379\n",
      "[Training Epoch 0] Batch 3954, Loss 0.31359368562698364\n",
      "[Training Epoch 0] Batch 3955, Loss 0.2970874607563019\n",
      "[Training Epoch 0] Batch 3956, Loss 0.3163602352142334\n",
      "[Training Epoch 0] Batch 3957, Loss 0.3044511377811432\n",
      "[Training Epoch 0] Batch 3958, Loss 0.32683712244033813\n",
      "[Training Epoch 0] Batch 3959, Loss 0.25279051065444946\n",
      "[Training Epoch 0] Batch 3960, Loss 0.32575663924217224\n",
      "[Training Epoch 0] Batch 3961, Loss 0.3071313500404358\n",
      "[Training Epoch 0] Batch 3962, Loss 0.3250349164009094\n",
      "[Training Epoch 0] Batch 3963, Loss 0.31237852573394775\n",
      "[Training Epoch 0] Batch 3964, Loss 0.3120776116847992\n",
      "[Training Epoch 0] Batch 3965, Loss 0.3390388786792755\n",
      "[Training Epoch 0] Batch 3966, Loss 0.3281775116920471\n",
      "[Training Epoch 0] Batch 3967, Loss 0.33208945393562317\n",
      "[Training Epoch 0] Batch 3968, Loss 0.2845308780670166\n",
      "[Training Epoch 0] Batch 3969, Loss 0.3199937045574188\n",
      "[Training Epoch 0] Batch 3970, Loss 0.3139680325984955\n",
      "[Training Epoch 0] Batch 3971, Loss 0.32215768098831177\n",
      "[Training Epoch 0] Batch 3972, Loss 0.3088489770889282\n",
      "[Training Epoch 0] Batch 3973, Loss 0.32615238428115845\n",
      "[Training Epoch 0] Batch 3974, Loss 0.3240523338317871\n",
      "[Training Epoch 0] Batch 3975, Loss 0.2999304533004761\n",
      "[Training Epoch 0] Batch 3976, Loss 0.33661192655563354\n",
      "[Training Epoch 0] Batch 3977, Loss 0.29595687985420227\n",
      "[Training Epoch 0] Batch 3978, Loss 0.3219269812107086\n",
      "[Training Epoch 0] Batch 3979, Loss 0.33525708317756653\n",
      "[Training Epoch 0] Batch 3980, Loss 0.31089743971824646\n",
      "[Training Epoch 0] Batch 3981, Loss 0.3357703387737274\n",
      "[Training Epoch 0] Batch 3982, Loss 0.27760839462280273\n",
      "[Training Epoch 0] Batch 3983, Loss 0.30260157585144043\n",
      "[Training Epoch 0] Batch 3984, Loss 0.29700589179992676\n",
      "[Training Epoch 0] Batch 3985, Loss 0.300851047039032\n",
      "[Training Epoch 0] Batch 3986, Loss 0.35568967461586\n",
      "[Training Epoch 0] Batch 3987, Loss 0.30074143409729004\n",
      "[Training Epoch 0] Batch 3988, Loss 0.32123199105262756\n",
      "[Training Epoch 0] Batch 3989, Loss 0.3113429546356201\n",
      "[Training Epoch 0] Batch 3990, Loss 0.2944314181804657\n",
      "[Training Epoch 0] Batch 3991, Loss 0.33254703879356384\n",
      "[Training Epoch 0] Batch 3992, Loss 0.3393786549568176\n",
      "[Training Epoch 0] Batch 3993, Loss 0.27881544828414917\n",
      "[Training Epoch 0] Batch 3994, Loss 0.3374437689781189\n",
      "[Training Epoch 0] Batch 3995, Loss 0.33159899711608887\n",
      "[Training Epoch 0] Batch 3996, Loss 0.3300709128379822\n",
      "[Training Epoch 0] Batch 3997, Loss 0.30067750811576843\n",
      "[Training Epoch 0] Batch 3998, Loss 0.29679667949676514\n",
      "[Training Epoch 0] Batch 3999, Loss 0.3092424273490906\n",
      "[Training Epoch 0] Batch 4000, Loss 0.3108369708061218\n",
      "[Training Epoch 0] Batch 4001, Loss 0.33142557740211487\n",
      "[Training Epoch 0] Batch 4002, Loss 0.3262300491333008\n",
      "[Training Epoch 0] Batch 4003, Loss 0.31939080357551575\n",
      "[Training Epoch 0] Batch 4004, Loss 0.3079372048377991\n",
      "[Training Epoch 0] Batch 4005, Loss 0.3266792893409729\n",
      "[Training Epoch 0] Batch 4006, Loss 0.3331253230571747\n",
      "[Training Epoch 0] Batch 4007, Loss 0.3053649961948395\n",
      "[Training Epoch 0] Batch 4008, Loss 0.3179313540458679\n",
      "[Training Epoch 0] Batch 4009, Loss 0.29532212018966675\n",
      "[Training Epoch 0] Batch 4010, Loss 0.29228460788726807\n",
      "[Training Epoch 0] Batch 4011, Loss 0.3216415047645569\n",
      "[Training Epoch 0] Batch 4012, Loss 0.34058529138565063\n",
      "[Training Epoch 0] Batch 4013, Loss 0.3198179006576538\n",
      "[Training Epoch 0] Batch 4014, Loss 0.3172951340675354\n",
      "[Training Epoch 0] Batch 4015, Loss 0.32098814845085144\n",
      "[Training Epoch 0] Batch 4016, Loss 0.3140104413032532\n",
      "[Training Epoch 0] Batch 4017, Loss 0.3109120726585388\n",
      "[Training Epoch 0] Batch 4018, Loss 0.3438296318054199\n",
      "[Training Epoch 0] Batch 4019, Loss 0.33764171600341797\n",
      "[Training Epoch 0] Batch 4020, Loss 0.323180615901947\n",
      "[Training Epoch 0] Batch 4021, Loss 0.2888694107532501\n",
      "[Training Epoch 0] Batch 4022, Loss 0.30907416343688965\n",
      "[Training Epoch 0] Batch 4023, Loss 0.34573620557785034\n",
      "[Training Epoch 0] Batch 4024, Loss 0.3212810754776001\n",
      "[Training Epoch 0] Batch 4025, Loss 0.3432258367538452\n",
      "[Training Epoch 0] Batch 4026, Loss 0.2873873710632324\n",
      "[Training Epoch 0] Batch 4027, Loss 0.31566867232322693\n",
      "[Training Epoch 0] Batch 4028, Loss 0.334121972322464\n",
      "[Training Epoch 0] Batch 4029, Loss 0.32295751571655273\n",
      "[Training Epoch 0] Batch 4030, Loss 0.30931705236434937\n",
      "[Training Epoch 0] Batch 4031, Loss 0.3245903253555298\n",
      "[Training Epoch 0] Batch 4032, Loss 0.2779179513454437\n",
      "[Training Epoch 0] Batch 4033, Loss 0.3412134647369385\n",
      "[Training Epoch 0] Batch 4034, Loss 0.3468441963195801\n",
      "[Training Epoch 0] Batch 4035, Loss 0.3366636335849762\n",
      "[Training Epoch 0] Batch 4036, Loss 0.3185257911682129\n",
      "[Training Epoch 0] Batch 4037, Loss 0.3700089454650879\n",
      "[Training Epoch 0] Batch 4038, Loss 0.30277979373931885\n",
      "[Training Epoch 0] Batch 4039, Loss 0.3078012466430664\n",
      "[Training Epoch 0] Batch 4040, Loss 0.3076481819152832\n",
      "[Training Epoch 0] Batch 4041, Loss 0.299913227558136\n",
      "[Training Epoch 0] Batch 4042, Loss 0.3543739914894104\n",
      "[Training Epoch 0] Batch 4043, Loss 0.30530720949172974\n",
      "[Training Epoch 0] Batch 4044, Loss 0.3192524313926697\n",
      "[Training Epoch 0] Batch 4045, Loss 0.32676756381988525\n",
      "[Training Epoch 0] Batch 4046, Loss 0.2846808433532715\n",
      "[Training Epoch 0] Batch 4047, Loss 0.3432635962963104\n",
      "[Training Epoch 0] Batch 4048, Loss 0.2984273433685303\n",
      "[Training Epoch 0] Batch 4049, Loss 0.32849758863449097\n",
      "[Training Epoch 0] Batch 4050, Loss 0.3291219472885132\n",
      "[Training Epoch 0] Batch 4051, Loss 0.3220909833908081\n",
      "[Training Epoch 0] Batch 4052, Loss 0.2902369499206543\n",
      "[Training Epoch 0] Batch 4053, Loss 0.29638218879699707\n",
      "[Training Epoch 0] Batch 4054, Loss 0.2961796522140503\n",
      "[Training Epoch 0] Batch 4055, Loss 0.2894574999809265\n",
      "[Training Epoch 0] Batch 4056, Loss 0.32220789790153503\n",
      "[Training Epoch 0] Batch 4057, Loss 0.30631446838378906\n",
      "[Training Epoch 0] Batch 4058, Loss 0.3118513524532318\n",
      "[Training Epoch 0] Batch 4059, Loss 0.3105486035346985\n",
      "[Training Epoch 0] Batch 4060, Loss 0.296841561794281\n",
      "[Training Epoch 0] Batch 4061, Loss 0.3198041319847107\n",
      "[Training Epoch 0] Batch 4062, Loss 0.3062271773815155\n",
      "[Training Epoch 0] Batch 4063, Loss 0.30656394362449646\n",
      "[Training Epoch 0] Batch 4064, Loss 0.3205823600292206\n",
      "[Training Epoch 0] Batch 4065, Loss 0.3107333481311798\n",
      "[Training Epoch 0] Batch 4066, Loss 0.3179168701171875\n",
      "[Training Epoch 0] Batch 4067, Loss 0.316761314868927\n",
      "[Training Epoch 0] Batch 4068, Loss 0.29542067646980286\n",
      "[Training Epoch 0] Batch 4069, Loss 0.3351283371448517\n",
      "[Training Epoch 0] Batch 4070, Loss 0.33838745951652527\n",
      "[Training Epoch 0] Batch 4071, Loss 0.31088975071907043\n",
      "[Training Epoch 0] Batch 4072, Loss 0.30339515209198\n",
      "[Training Epoch 0] Batch 4073, Loss 0.34508317708969116\n",
      "[Training Epoch 0] Batch 4074, Loss 0.32006800174713135\n",
      "[Training Epoch 0] Batch 4075, Loss 0.31447356939315796\n",
      "[Training Epoch 0] Batch 4076, Loss 0.3383367657661438\n",
      "[Training Epoch 0] Batch 4077, Loss 0.2852507531642914\n",
      "[Training Epoch 0] Batch 4078, Loss 0.32263314723968506\n",
      "[Training Epoch 0] Batch 4079, Loss 0.303316593170166\n",
      "[Training Epoch 0] Batch 4080, Loss 0.3277472257614136\n",
      "[Training Epoch 0] Batch 4081, Loss 0.31693845987319946\n",
      "[Training Epoch 0] Batch 4082, Loss 0.29859036207199097\n",
      "[Training Epoch 0] Batch 4083, Loss 0.2797909379005432\n",
      "[Training Epoch 0] Batch 4084, Loss 0.323020339012146\n",
      "[Training Epoch 0] Batch 4085, Loss 0.31269875168800354\n",
      "[Training Epoch 0] Batch 4086, Loss 0.26626014709472656\n",
      "[Training Epoch 0] Batch 4087, Loss 0.30043572187423706\n",
      "[Training Epoch 0] Batch 4088, Loss 0.3211207389831543\n",
      "[Training Epoch 0] Batch 4089, Loss 0.3188346028327942\n",
      "[Training Epoch 0] Batch 4090, Loss 0.3237275183200836\n",
      "[Training Epoch 0] Batch 4091, Loss 0.2704448997974396\n",
      "[Training Epoch 0] Batch 4092, Loss 0.3205726444721222\n",
      "[Training Epoch 0] Batch 4093, Loss 0.3074150085449219\n",
      "[Training Epoch 0] Batch 4094, Loss 0.3201484978199005\n",
      "[Training Epoch 0] Batch 4095, Loss 0.3277857005596161\n",
      "[Training Epoch 0] Batch 4096, Loss 0.2900904715061188\n",
      "[Training Epoch 0] Batch 4097, Loss 0.29552435874938965\n",
      "[Training Epoch 0] Batch 4098, Loss 0.30385851860046387\n",
      "[Training Epoch 0] Batch 4099, Loss 0.31444239616394043\n",
      "[Training Epoch 0] Batch 4100, Loss 0.3154056966304779\n",
      "[Training Epoch 0] Batch 4101, Loss 0.3285430669784546\n",
      "[Training Epoch 0] Batch 4102, Loss 0.2930338382720947\n",
      "[Training Epoch 0] Batch 4103, Loss 0.32812023162841797\n",
      "[Training Epoch 0] Batch 4104, Loss 0.3084588646888733\n",
      "[Training Epoch 0] Batch 4105, Loss 0.3114347755908966\n",
      "[Training Epoch 0] Batch 4106, Loss 0.3035261631011963\n",
      "[Training Epoch 0] Batch 4107, Loss 0.32306045293807983\n",
      "[Training Epoch 0] Batch 4108, Loss 0.31180816888809204\n",
      "[Training Epoch 0] Batch 4109, Loss 0.3032678961753845\n",
      "[Training Epoch 0] Batch 4110, Loss 0.3125547170639038\n",
      "[Training Epoch 0] Batch 4111, Loss 0.290511816740036\n",
      "[Training Epoch 0] Batch 4112, Loss 0.2863498330116272\n",
      "[Training Epoch 0] Batch 4113, Loss 0.3308119475841522\n",
      "[Training Epoch 0] Batch 4114, Loss 0.30572575330734253\n",
      "[Training Epoch 0] Batch 4115, Loss 0.2874777913093567\n",
      "[Training Epoch 0] Batch 4116, Loss 0.3169138729572296\n",
      "[Training Epoch 0] Batch 4117, Loss 0.3217077851295471\n",
      "[Training Epoch 0] Batch 4118, Loss 0.29364216327667236\n",
      "[Training Epoch 0] Batch 4119, Loss 0.3160232901573181\n",
      "[Training Epoch 0] Batch 4120, Loss 0.29844433069229126\n",
      "[Training Epoch 0] Batch 4121, Loss 0.3122519254684448\n",
      "[Training Epoch 0] Batch 4122, Loss 0.3005647659301758\n",
      "[Training Epoch 0] Batch 4123, Loss 0.31078293919563293\n",
      "[Training Epoch 0] Batch 4124, Loss 0.32990026473999023\n",
      "[Training Epoch 0] Batch 4125, Loss 0.3084948658943176\n",
      "[Training Epoch 0] Batch 4126, Loss 0.35121431946754456\n",
      "[Training Epoch 0] Batch 4127, Loss 0.29179543256759644\n",
      "[Training Epoch 0] Batch 4128, Loss 0.3282889723777771\n",
      "[Training Epoch 0] Batch 4129, Loss 0.31439703702926636\n",
      "[Training Epoch 0] Batch 4130, Loss 0.3048112094402313\n",
      "[Training Epoch 0] Batch 4131, Loss 0.28087475895881653\n",
      "[Training Epoch 0] Batch 4132, Loss 0.2997699975967407\n",
      "[Training Epoch 0] Batch 4133, Loss 0.30848050117492676\n",
      "[Training Epoch 0] Batch 4134, Loss 0.33471977710723877\n",
      "[Training Epoch 0] Batch 4135, Loss 0.33266592025756836\n",
      "[Training Epoch 0] Batch 4136, Loss 0.3086395561695099\n",
      "[Training Epoch 0] Batch 4137, Loss 0.3321227431297302\n",
      "[Training Epoch 0] Batch 4138, Loss 0.34308838844299316\n",
      "[Training Epoch 0] Batch 4139, Loss 0.2810995578765869\n",
      "[Training Epoch 0] Batch 4140, Loss 0.30737918615341187\n",
      "[Training Epoch 0] Batch 4141, Loss 0.3139238953590393\n",
      "[Training Epoch 0] Batch 4142, Loss 0.2983505129814148\n",
      "[Training Epoch 0] Batch 4143, Loss 0.35806795954704285\n",
      "[Training Epoch 0] Batch 4144, Loss 0.278026819229126\n",
      "[Training Epoch 0] Batch 4145, Loss 0.31624042987823486\n",
      "[Training Epoch 0] Batch 4146, Loss 0.30207592248916626\n",
      "[Training Epoch 0] Batch 4147, Loss 0.3036014437675476\n",
      "[Training Epoch 0] Batch 4148, Loss 0.3259783387184143\n",
      "[Training Epoch 0] Batch 4149, Loss 0.3373730480670929\n",
      "[Training Epoch 0] Batch 4150, Loss 0.33247214555740356\n",
      "[Training Epoch 0] Batch 4151, Loss 0.3138163089752197\n",
      "[Training Epoch 0] Batch 4152, Loss 0.34411516785621643\n",
      "[Training Epoch 0] Batch 4153, Loss 0.3201630711555481\n",
      "[Training Epoch 0] Batch 4154, Loss 0.2822856307029724\n",
      "[Training Epoch 0] Batch 4155, Loss 0.29185691475868225\n",
      "[Training Epoch 0] Batch 4156, Loss 0.31528693437576294\n",
      "[Training Epoch 0] Batch 4157, Loss 0.32389456033706665\n",
      "[Training Epoch 0] Batch 4158, Loss 0.3175554573535919\n",
      "[Training Epoch 0] Batch 4159, Loss 0.3100138306617737\n",
      "[Training Epoch 0] Batch 4160, Loss 0.34486448764801025\n",
      "[Training Epoch 0] Batch 4161, Loss 0.33987918496131897\n",
      "[Training Epoch 0] Batch 4162, Loss 0.3011094927787781\n",
      "[Training Epoch 0] Batch 4163, Loss 0.3356183171272278\n",
      "[Training Epoch 0] Batch 4164, Loss 0.28660231828689575\n",
      "[Training Epoch 0] Batch 4165, Loss 0.2805594801902771\n",
      "[Training Epoch 0] Batch 4166, Loss 0.3399532735347748\n",
      "[Training Epoch 0] Batch 4167, Loss 0.32100117206573486\n",
      "[Training Epoch 0] Batch 4168, Loss 0.3410002589225769\n",
      "[Training Epoch 0] Batch 4169, Loss 0.3274572491645813\n",
      "[Training Epoch 0] Batch 4170, Loss 0.33871009945869446\n",
      "[Training Epoch 0] Batch 4171, Loss 0.305660605430603\n",
      "[Training Epoch 0] Batch 4172, Loss 0.2880392372608185\n",
      "[Training Epoch 0] Batch 4173, Loss 0.29777538776397705\n",
      "[Training Epoch 0] Batch 4174, Loss 0.308679461479187\n",
      "[Training Epoch 0] Batch 4175, Loss 0.3244195580482483\n",
      "[Training Epoch 0] Batch 4176, Loss 0.35081011056900024\n",
      "[Training Epoch 0] Batch 4177, Loss 0.3125791549682617\n",
      "[Training Epoch 0] Batch 4178, Loss 0.29702484607696533\n",
      "[Training Epoch 0] Batch 4179, Loss 0.2957146167755127\n",
      "[Training Epoch 0] Batch 4180, Loss 0.318287193775177\n",
      "[Training Epoch 0] Batch 4181, Loss 0.30602404475212097\n",
      "[Training Epoch 0] Batch 4182, Loss 0.35518333315849304\n",
      "[Training Epoch 0] Batch 4183, Loss 0.3092668950557709\n",
      "[Training Epoch 0] Batch 4184, Loss 0.33381932973861694\n",
      "[Training Epoch 0] Batch 4185, Loss 0.3377815783023834\n",
      "[Training Epoch 0] Batch 4186, Loss 0.3134934604167938\n",
      "[Training Epoch 0] Batch 4187, Loss 0.32890790700912476\n",
      "[Training Epoch 0] Batch 4188, Loss 0.29040616750717163\n",
      "[Training Epoch 0] Batch 4189, Loss 0.2862371802330017\n",
      "[Training Epoch 0] Batch 4190, Loss 0.335629403591156\n",
      "[Training Epoch 0] Batch 4191, Loss 0.32028844952583313\n",
      "[Training Epoch 0] Batch 4192, Loss 0.2923903465270996\n",
      "[Training Epoch 0] Batch 4193, Loss 0.3127795457839966\n",
      "[Training Epoch 0] Batch 4194, Loss 0.3089854121208191\n",
      "[Training Epoch 0] Batch 4195, Loss 0.3094387352466583\n",
      "[Training Epoch 0] Batch 4196, Loss 0.31068456172943115\n",
      "[Training Epoch 0] Batch 4197, Loss 0.3198074400424957\n",
      "[Training Epoch 0] Batch 4198, Loss 0.3143077492713928\n",
      "[Training Epoch 0] Batch 4199, Loss 0.31921517848968506\n",
      "[Training Epoch 0] Batch 4200, Loss 0.317036896944046\n",
      "[Training Epoch 0] Batch 4201, Loss 0.29709357023239136\n",
      "[Training Epoch 0] Batch 4202, Loss 0.3128769099712372\n",
      "[Training Epoch 0] Batch 4203, Loss 0.3108055293560028\n",
      "[Training Epoch 0] Batch 4204, Loss 0.3244327902793884\n",
      "[Training Epoch 0] Batch 4205, Loss 0.29935288429260254\n",
      "[Training Epoch 0] Batch 4206, Loss 0.3226251006126404\n",
      "[Training Epoch 0] Batch 4207, Loss 0.3053553104400635\n",
      "[Training Epoch 0] Batch 4208, Loss 0.3099249005317688\n",
      "[Training Epoch 0] Batch 4209, Loss 0.290851891040802\n",
      "[Training Epoch 0] Batch 4210, Loss 0.31788522005081177\n",
      "[Training Epoch 0] Batch 4211, Loss 0.2930822968482971\n",
      "[Training Epoch 0] Batch 4212, Loss 0.30698472261428833\n",
      "[Training Epoch 0] Batch 4213, Loss 0.299999475479126\n",
      "[Training Epoch 0] Batch 4214, Loss 0.31274884939193726\n",
      "[Training Epoch 0] Batch 4215, Loss 0.2987895607948303\n",
      "[Training Epoch 0] Batch 4216, Loss 0.3145253658294678\n",
      "[Training Epoch 0] Batch 4217, Loss 0.31447675824165344\n",
      "[Training Epoch 0] Batch 4218, Loss 0.3432038724422455\n",
      "[Training Epoch 0] Batch 4219, Loss 0.3223734498023987\n",
      "[Training Epoch 0] Batch 4220, Loss 0.28309106826782227\n",
      "[Training Epoch 0] Batch 4221, Loss 0.3012534976005554\n",
      "[Training Epoch 0] Batch 4222, Loss 0.2960069477558136\n",
      "[Training Epoch 0] Batch 4223, Loss 0.3169964849948883\n",
      "[Training Epoch 0] Batch 4224, Loss 0.31367915868759155\n",
      "[Training Epoch 0] Batch 4225, Loss 0.3072536587715149\n",
      "[Training Epoch 0] Batch 4226, Loss 0.3198304772377014\n",
      "[Training Epoch 0] Batch 4227, Loss 0.31606268882751465\n",
      "[Training Epoch 0] Batch 4228, Loss 0.3092740476131439\n",
      "[Training Epoch 0] Batch 4229, Loss 0.33772245049476624\n",
      "[Training Epoch 0] Batch 4230, Loss 0.3098369240760803\n",
      "[Training Epoch 0] Batch 4231, Loss 0.3636739253997803\n",
      "[Training Epoch 0] Batch 4232, Loss 0.2886925935745239\n",
      "[Training Epoch 0] Batch 4233, Loss 0.30117085576057434\n",
      "[Training Epoch 0] Batch 4234, Loss 0.3336656093597412\n",
      "[Training Epoch 0] Batch 4235, Loss 0.3164041042327881\n",
      "[Training Epoch 0] Batch 4236, Loss 0.33335769176483154\n",
      "[Training Epoch 0] Batch 4237, Loss 0.2790367007255554\n",
      "[Training Epoch 0] Batch 4238, Loss 0.31717735528945923\n",
      "[Training Epoch 0] Batch 4239, Loss 0.29698446393013\n",
      "[Training Epoch 0] Batch 4240, Loss 0.3178790807723999\n",
      "[Training Epoch 0] Batch 4241, Loss 0.30690091848373413\n",
      "[Training Epoch 0] Batch 4242, Loss 0.30864712595939636\n",
      "[Training Epoch 0] Batch 4243, Loss 0.3107050955295563\n",
      "[Training Epoch 0] Batch 4244, Loss 0.31374603509902954\n",
      "[Training Epoch 0] Batch 4245, Loss 0.31505173444747925\n",
      "[Training Epoch 0] Batch 4246, Loss 0.31527820229530334\n",
      "[Training Epoch 0] Batch 4247, Loss 0.31349462270736694\n",
      "[Training Epoch 0] Batch 4248, Loss 0.31865358352661133\n",
      "[Training Epoch 0] Batch 4249, Loss 0.32045358419418335\n",
      "[Training Epoch 0] Batch 4250, Loss 0.322452187538147\n",
      "[Training Epoch 0] Batch 4251, Loss 0.3288980722427368\n",
      "[Training Epoch 0] Batch 4252, Loss 0.31145021319389343\n",
      "[Training Epoch 0] Batch 4253, Loss 0.33759230375289917\n",
      "[Training Epoch 0] Batch 4254, Loss 0.30398786067962646\n",
      "[Training Epoch 0] Batch 4255, Loss 0.3164970874786377\n",
      "[Training Epoch 0] Batch 4256, Loss 0.29829826951026917\n",
      "[Training Epoch 0] Batch 4257, Loss 0.3128929138183594\n",
      "[Training Epoch 0] Batch 4258, Loss 0.3074833154678345\n",
      "[Training Epoch 0] Batch 4259, Loss 0.3105429410934448\n",
      "[Training Epoch 0] Batch 4260, Loss 0.30728957056999207\n",
      "[Training Epoch 0] Batch 4261, Loss 0.3342697024345398\n",
      "[Training Epoch 0] Batch 4262, Loss 0.33323371410369873\n",
      "[Training Epoch 0] Batch 4263, Loss 0.306255042552948\n",
      "[Training Epoch 0] Batch 4264, Loss 0.31990307569503784\n",
      "[Training Epoch 0] Batch 4265, Loss 0.30793383717536926\n",
      "[Training Epoch 0] Batch 4266, Loss 0.3261955678462982\n",
      "[Training Epoch 0] Batch 4267, Loss 0.31900754570961\n",
      "[Training Epoch 0] Batch 4268, Loss 0.31774353981018066\n",
      "[Training Epoch 0] Batch 4269, Loss 0.26428329944610596\n",
      "[Training Epoch 0] Batch 4270, Loss 0.3166467547416687\n",
      "[Training Epoch 0] Batch 4271, Loss 0.339959979057312\n",
      "[Training Epoch 0] Batch 4272, Loss 0.30269044637680054\n",
      "[Training Epoch 0] Batch 4273, Loss 0.3555412292480469\n",
      "[Training Epoch 0] Batch 4274, Loss 0.3009803295135498\n",
      "[Training Epoch 0] Batch 4275, Loss 0.31251993775367737\n",
      "[Training Epoch 0] Batch 4276, Loss 0.3138996362686157\n",
      "[Training Epoch 0] Batch 4277, Loss 0.3096219301223755\n",
      "[Training Epoch 0] Batch 4278, Loss 0.3061743378639221\n",
      "[Training Epoch 0] Batch 4279, Loss 0.2904825210571289\n",
      "[Training Epoch 0] Batch 4280, Loss 0.31268829107284546\n",
      "[Training Epoch 0] Batch 4281, Loss 0.3015763461589813\n",
      "[Training Epoch 0] Batch 4282, Loss 0.3322238326072693\n",
      "[Training Epoch 0] Batch 4283, Loss 0.32960712909698486\n",
      "[Training Epoch 0] Batch 4284, Loss 0.3009599447250366\n",
      "[Training Epoch 0] Batch 4285, Loss 0.3377789258956909\n",
      "[Training Epoch 0] Batch 4286, Loss 0.35842227935791016\n",
      "[Training Epoch 0] Batch 4287, Loss 0.33058324456214905\n",
      "[Training Epoch 0] Batch 4288, Loss 0.3132818043231964\n",
      "[Training Epoch 0] Batch 4289, Loss 0.30298617482185364\n",
      "[Training Epoch 0] Batch 4290, Loss 0.3403264880180359\n",
      "[Training Epoch 0] Batch 4291, Loss 0.3068515658378601\n",
      "[Training Epoch 0] Batch 4292, Loss 0.29026007652282715\n",
      "[Training Epoch 0] Batch 4293, Loss 0.3143153488636017\n",
      "[Training Epoch 0] Batch 4294, Loss 0.283977210521698\n",
      "[Training Epoch 0] Batch 4295, Loss 0.29002106189727783\n",
      "[Training Epoch 0] Batch 4296, Loss 0.33115193247795105\n",
      "[Training Epoch 0] Batch 4297, Loss 0.3139954209327698\n",
      "[Training Epoch 0] Batch 4298, Loss 0.296109676361084\n",
      "[Training Epoch 0] Batch 4299, Loss 0.28949403762817383\n",
      "[Training Epoch 0] Batch 4300, Loss 0.31155791878700256\n",
      "[Training Epoch 0] Batch 4301, Loss 0.32234513759613037\n",
      "[Training Epoch 0] Batch 4302, Loss 0.32922035455703735\n",
      "[Training Epoch 0] Batch 4303, Loss 0.38526976108551025\n",
      "[Training Epoch 0] Batch 4304, Loss 0.3275565505027771\n",
      "[Training Epoch 0] Batch 4305, Loss 0.3224882185459137\n",
      "[Training Epoch 0] Batch 4306, Loss 0.32140016555786133\n",
      "[Training Epoch 0] Batch 4307, Loss 0.3068203628063202\n",
      "[Training Epoch 0] Batch 4308, Loss 0.33273762464523315\n",
      "[Training Epoch 0] Batch 4309, Loss 0.28319481015205383\n",
      "[Training Epoch 0] Batch 4310, Loss 0.31361332535743713\n",
      "[Training Epoch 0] Batch 4311, Loss 0.3192894756793976\n",
      "[Training Epoch 0] Batch 4312, Loss 0.3125620484352112\n",
      "[Training Epoch 0] Batch 4313, Loss 0.2925780117511749\n",
      "[Training Epoch 0] Batch 4314, Loss 0.3269403874874115\n",
      "[Training Epoch 0] Batch 4315, Loss 0.28925010561943054\n",
      "[Training Epoch 0] Batch 4316, Loss 0.320358544588089\n",
      "[Training Epoch 0] Batch 4317, Loss 0.3190356194972992\n",
      "[Training Epoch 0] Batch 4318, Loss 0.31043118238449097\n",
      "[Training Epoch 0] Batch 4319, Loss 0.3032773733139038\n",
      "[Training Epoch 0] Batch 4320, Loss 0.32802295684814453\n",
      "[Training Epoch 0] Batch 4321, Loss 0.3203235864639282\n",
      "[Training Epoch 0] Batch 4322, Loss 0.3321720361709595\n",
      "[Training Epoch 0] Batch 4323, Loss 0.32748109102249146\n",
      "[Training Epoch 0] Batch 4324, Loss 0.3230073153972626\n",
      "[Training Epoch 0] Batch 4325, Loss 0.29177266359329224\n",
      "[Training Epoch 0] Batch 4326, Loss 0.3359445333480835\n",
      "[Training Epoch 0] Batch 4327, Loss 0.3235141932964325\n",
      "[Training Epoch 0] Batch 4328, Loss 0.30090242624282837\n",
      "[Training Epoch 0] Batch 4329, Loss 0.2940894663333893\n",
      "[Training Epoch 0] Batch 4330, Loss 0.32577162981033325\n",
      "[Training Epoch 0] Batch 4331, Loss 0.338486909866333\n",
      "[Training Epoch 0] Batch 4332, Loss 0.32914453744888306\n",
      "[Training Epoch 0] Batch 4333, Loss 0.33077964186668396\n",
      "[Training Epoch 0] Batch 4334, Loss 0.3040815591812134\n",
      "[Training Epoch 0] Batch 4335, Loss 0.33911415934562683\n",
      "[Training Epoch 0] Batch 4336, Loss 0.31822943687438965\n",
      "[Training Epoch 0] Batch 4337, Loss 0.31258827447891235\n",
      "[Training Epoch 0] Batch 4338, Loss 0.314757376909256\n",
      "[Training Epoch 0] Batch 4339, Loss 0.2801271677017212\n",
      "[Training Epoch 0] Batch 4340, Loss 0.33227646350860596\n",
      "[Training Epoch 0] Batch 4341, Loss 0.2772749066352844\n",
      "[Training Epoch 0] Batch 4342, Loss 0.2950979471206665\n",
      "[Training Epoch 0] Batch 4343, Loss 0.305697500705719\n",
      "[Training Epoch 0] Batch 4344, Loss 0.33976054191589355\n",
      "[Training Epoch 0] Batch 4345, Loss 0.3140122592449188\n",
      "[Training Epoch 0] Batch 4346, Loss 0.3255642056465149\n",
      "[Training Epoch 0] Batch 4347, Loss 0.3270651698112488\n",
      "[Training Epoch 0] Batch 4348, Loss 0.3066437244415283\n",
      "[Training Epoch 0] Batch 4349, Loss 0.30614131689071655\n",
      "[Training Epoch 0] Batch 4350, Loss 0.3296995759010315\n",
      "[Training Epoch 0] Batch 4351, Loss 0.3042486906051636\n",
      "[Training Epoch 0] Batch 4352, Loss 0.3070070743560791\n",
      "[Training Epoch 0] Batch 4353, Loss 0.31424060463905334\n",
      "[Training Epoch 0] Batch 4354, Loss 0.3004700243473053\n",
      "[Training Epoch 0] Batch 4355, Loss 0.3076619505882263\n",
      "[Training Epoch 0] Batch 4356, Loss 0.3223700523376465\n",
      "[Training Epoch 0] Batch 4357, Loss 0.33328819274902344\n",
      "[Training Epoch 0] Batch 4358, Loss 0.3105480372905731\n",
      "[Training Epoch 0] Batch 4359, Loss 0.30286121368408203\n",
      "[Training Epoch 0] Batch 4360, Loss 0.3195541501045227\n",
      "[Training Epoch 0] Batch 4361, Loss 0.28601765632629395\n",
      "[Training Epoch 0] Batch 4362, Loss 0.3343014717102051\n",
      "[Training Epoch 0] Batch 4363, Loss 0.3073618412017822\n",
      "[Training Epoch 0] Batch 4364, Loss 0.31833577156066895\n",
      "[Training Epoch 0] Batch 4365, Loss 0.3138277530670166\n",
      "[Training Epoch 0] Batch 4366, Loss 0.3148401379585266\n",
      "[Training Epoch 0] Batch 4367, Loss 0.31017830967903137\n",
      "[Training Epoch 0] Batch 4368, Loss 0.311870813369751\n",
      "[Training Epoch 0] Batch 4369, Loss 0.3227143883705139\n",
      "[Training Epoch 0] Batch 4370, Loss 0.2805161774158478\n",
      "[Training Epoch 0] Batch 4371, Loss 0.3048369288444519\n",
      "[Training Epoch 0] Batch 4372, Loss 0.3031608462333679\n",
      "[Training Epoch 0] Batch 4373, Loss 0.3004888892173767\n",
      "[Training Epoch 0] Batch 4374, Loss 0.3022421896457672\n",
      "[Training Epoch 0] Batch 4375, Loss 0.3183091878890991\n",
      "[Training Epoch 0] Batch 4376, Loss 0.31329619884490967\n",
      "[Training Epoch 0] Batch 4377, Loss 0.32058826088905334\n",
      "[Training Epoch 0] Batch 4378, Loss 0.31049829721450806\n",
      "[Training Epoch 0] Batch 4379, Loss 0.33553338050842285\n",
      "[Training Epoch 0] Batch 4380, Loss 0.31546902656555176\n",
      "[Training Epoch 0] Batch 4381, Loss 0.3000238835811615\n",
      "[Training Epoch 0] Batch 4382, Loss 0.3185446858406067\n",
      "[Training Epoch 0] Batch 4383, Loss 0.29185616970062256\n",
      "[Training Epoch 0] Batch 4384, Loss 0.3116670846939087\n",
      "[Training Epoch 0] Batch 4385, Loss 0.30573517084121704\n",
      "[Training Epoch 0] Batch 4386, Loss 0.3216460943222046\n",
      "[Training Epoch 0] Batch 4387, Loss 0.31714725494384766\n",
      "[Training Epoch 0] Batch 4388, Loss 0.3074835538864136\n",
      "[Training Epoch 0] Batch 4389, Loss 0.32551467418670654\n",
      "[Training Epoch 0] Batch 4390, Loss 0.30958831310272217\n",
      "[Training Epoch 0] Batch 4391, Loss 0.32176709175109863\n",
      "[Training Epoch 0] Batch 4392, Loss 0.3043835163116455\n",
      "[Training Epoch 0] Batch 4393, Loss 0.300555020570755\n",
      "[Training Epoch 0] Batch 4394, Loss 0.3267253041267395\n",
      "[Training Epoch 0] Batch 4395, Loss 0.2943890690803528\n",
      "[Training Epoch 0] Batch 4396, Loss 0.294946551322937\n",
      "[Training Epoch 0] Batch 4397, Loss 0.30932843685150146\n",
      "[Training Epoch 0] Batch 4398, Loss 0.34594762325286865\n",
      "[Training Epoch 0] Batch 4399, Loss 0.2932373285293579\n",
      "[Training Epoch 0] Batch 4400, Loss 0.3208928406238556\n",
      "[Training Epoch 0] Batch 4401, Loss 0.30887866020202637\n",
      "[Training Epoch 0] Batch 4402, Loss 0.3281034529209137\n",
      "[Training Epoch 0] Batch 4403, Loss 0.3328888416290283\n",
      "[Training Epoch 0] Batch 4404, Loss 0.28507715463638306\n",
      "[Training Epoch 0] Batch 4405, Loss 0.3088163733482361\n",
      "[Training Epoch 0] Batch 4406, Loss 0.29911860823631287\n",
      "[Training Epoch 0] Batch 4407, Loss 0.3136901557445526\n",
      "[Training Epoch 0] Batch 4408, Loss 0.2977595925331116\n",
      "[Training Epoch 0] Batch 4409, Loss 0.32448312640190125\n",
      "[Training Epoch 0] Batch 4410, Loss 0.30520665645599365\n",
      "[Training Epoch 0] Batch 4411, Loss 0.3122602105140686\n",
      "[Training Epoch 0] Batch 4412, Loss 0.30546438694000244\n",
      "[Training Epoch 0] Batch 4413, Loss 0.30340468883514404\n",
      "[Training Epoch 0] Batch 4414, Loss 0.3206261396408081\n",
      "[Training Epoch 0] Batch 4415, Loss 0.30400145053863525\n",
      "[Training Epoch 0] Batch 4416, Loss 0.31429654359817505\n",
      "[Training Epoch 0] Batch 4417, Loss 0.30512022972106934\n",
      "[Training Epoch 0] Batch 4418, Loss 0.32995808124542236\n",
      "[Training Epoch 0] Batch 4419, Loss 0.33671271800994873\n",
      "[Training Epoch 0] Batch 4420, Loss 0.3183749318122864\n",
      "[Training Epoch 0] Batch 4421, Loss 0.32344257831573486\n",
      "[Training Epoch 0] Batch 4422, Loss 0.28695225715637207\n",
      "[Training Epoch 0] Batch 4423, Loss 0.27377039194107056\n",
      "[Training Epoch 0] Batch 4424, Loss 0.3225887417793274\n",
      "[Training Epoch 0] Batch 4425, Loss 0.3076896369457245\n",
      "[Training Epoch 0] Batch 4426, Loss 0.29558467864990234\n",
      "[Training Epoch 0] Batch 4427, Loss 0.3181629478931427\n",
      "[Training Epoch 0] Batch 4428, Loss 0.32042813301086426\n",
      "[Training Epoch 0] Batch 4429, Loss 0.30273985862731934\n",
      "[Training Epoch 0] Batch 4430, Loss 0.3161527216434479\n",
      "[Training Epoch 0] Batch 4431, Loss 0.33477967977523804\n",
      "[Training Epoch 0] Batch 4432, Loss 0.30819571018218994\n",
      "[Training Epoch 0] Batch 4433, Loss 0.291698157787323\n",
      "[Training Epoch 0] Batch 4434, Loss 0.328335165977478\n",
      "[Training Epoch 0] Batch 4435, Loss 0.3120746612548828\n",
      "[Training Epoch 0] Batch 4436, Loss 0.322357714176178\n",
      "[Training Epoch 0] Batch 4437, Loss 0.3032799959182739\n",
      "[Training Epoch 0] Batch 4438, Loss 0.32632970809936523\n",
      "[Training Epoch 0] Batch 4439, Loss 0.2890338897705078\n",
      "[Training Epoch 0] Batch 4440, Loss 0.31136512756347656\n",
      "[Training Epoch 0] Batch 4441, Loss 0.29527658224105835\n",
      "[Training Epoch 0] Batch 4442, Loss 0.3191494941711426\n",
      "[Training Epoch 0] Batch 4443, Loss 0.33978426456451416\n",
      "[Training Epoch 0] Batch 4444, Loss 0.2802562713623047\n",
      "[Training Epoch 0] Batch 4445, Loss 0.32410866022109985\n",
      "[Training Epoch 0] Batch 4446, Loss 0.31860172748565674\n",
      "[Training Epoch 0] Batch 4447, Loss 0.32397156953811646\n",
      "[Training Epoch 0] Batch 4448, Loss 0.33591219782829285\n",
      "[Training Epoch 0] Batch 4449, Loss 0.31223803758621216\n",
      "[Training Epoch 0] Batch 4450, Loss 0.32085949182510376\n",
      "[Training Epoch 0] Batch 4451, Loss 0.3190639019012451\n",
      "[Training Epoch 0] Batch 4452, Loss 0.3068079352378845\n",
      "[Training Epoch 0] Batch 4453, Loss 0.3207390308380127\n",
      "[Training Epoch 0] Batch 4454, Loss 0.32262736558914185\n",
      "[Training Epoch 0] Batch 4455, Loss 0.3399738073348999\n",
      "[Training Epoch 0] Batch 4456, Loss 0.30676233768463135\n",
      "[Training Epoch 0] Batch 4457, Loss 0.29107528924942017\n",
      "[Training Epoch 0] Batch 4458, Loss 0.30453038215637207\n",
      "[Training Epoch 0] Batch 4459, Loss 0.326570600271225\n",
      "[Training Epoch 0] Batch 4460, Loss 0.3318575620651245\n",
      "[Training Epoch 0] Batch 4461, Loss 0.3281325697898865\n",
      "[Training Epoch 0] Batch 4462, Loss 0.32775986194610596\n",
      "[Training Epoch 0] Batch 4463, Loss 0.32002246379852295\n",
      "[Training Epoch 0] Batch 4464, Loss 0.3081704378128052\n",
      "[Training Epoch 0] Batch 4465, Loss 0.30737820267677307\n",
      "[Training Epoch 0] Batch 4466, Loss 0.3148358464241028\n",
      "[Training Epoch 0] Batch 4467, Loss 0.314889132976532\n",
      "[Training Epoch 0] Batch 4468, Loss 0.3505493402481079\n",
      "[Training Epoch 0] Batch 4469, Loss 0.3040616512298584\n",
      "[Training Epoch 0] Batch 4470, Loss 0.3011699318885803\n",
      "[Training Epoch 0] Batch 4471, Loss 0.30721431970596313\n",
      "[Training Epoch 0] Batch 4472, Loss 0.3113386929035187\n",
      "[Training Epoch 0] Batch 4473, Loss 0.29567331075668335\n",
      "[Training Epoch 0] Batch 4474, Loss 0.30606240034103394\n",
      "[Training Epoch 0] Batch 4475, Loss 0.30419617891311646\n",
      "[Training Epoch 0] Batch 4476, Loss 0.3162161707878113\n",
      "[Training Epoch 0] Batch 4477, Loss 0.3340396285057068\n",
      "[Training Epoch 0] Batch 4478, Loss 0.30817529559135437\n",
      "[Training Epoch 0] Batch 4479, Loss 0.33274751901626587\n",
      "[Training Epoch 0] Batch 4480, Loss 0.307268887758255\n",
      "[Training Epoch 0] Batch 4481, Loss 0.2761989235877991\n",
      "[Training Epoch 0] Batch 4482, Loss 0.3141167163848877\n",
      "[Training Epoch 0] Batch 4483, Loss 0.3073125183582306\n",
      "[Training Epoch 0] Batch 4484, Loss 0.32696759700775146\n",
      "[Training Epoch 0] Batch 4485, Loss 0.32135188579559326\n",
      "[Training Epoch 0] Batch 4486, Loss 0.32759565114974976\n",
      "[Training Epoch 0] Batch 4487, Loss 0.31377387046813965\n",
      "[Training Epoch 0] Batch 4488, Loss 0.35257649421691895\n",
      "[Training Epoch 0] Batch 4489, Loss 0.30521321296691895\n",
      "[Training Epoch 0] Batch 4490, Loss 0.3347456455230713\n",
      "[Training Epoch 0] Batch 4491, Loss 0.31279367208480835\n",
      "[Training Epoch 0] Batch 4492, Loss 0.2964478135108948\n",
      "[Training Epoch 0] Batch 4493, Loss 0.29186081886291504\n",
      "[Training Epoch 0] Batch 4494, Loss 0.34159693121910095\n",
      "[Training Epoch 0] Batch 4495, Loss 0.30620336532592773\n",
      "[Training Epoch 0] Batch 4496, Loss 0.3057263493537903\n",
      "[Training Epoch 0] Batch 4497, Loss 0.3040732145309448\n",
      "[Training Epoch 0] Batch 4498, Loss 0.32194948196411133\n",
      "[Training Epoch 0] Batch 4499, Loss 0.32746750116348267\n",
      "[Training Epoch 0] Batch 4500, Loss 0.3155892491340637\n",
      "[Training Epoch 0] Batch 4501, Loss 0.3276229500770569\n",
      "[Training Epoch 0] Batch 4502, Loss 0.29873257875442505\n",
      "[Training Epoch 0] Batch 4503, Loss 0.30256038904190063\n",
      "[Training Epoch 0] Batch 4504, Loss 0.3410916328430176\n",
      "[Training Epoch 0] Batch 4505, Loss 0.30939310789108276\n",
      "[Training Epoch 0] Batch 4506, Loss 0.2953638434410095\n",
      "[Training Epoch 0] Batch 4507, Loss 0.29746872186660767\n",
      "[Training Epoch 0] Batch 4508, Loss 0.3085607886314392\n",
      "[Training Epoch 0] Batch 4509, Loss 0.3040684163570404\n",
      "[Training Epoch 0] Batch 4510, Loss 0.32673537731170654\n",
      "[Training Epoch 0] Batch 4511, Loss 0.30132097005844116\n",
      "[Training Epoch 0] Batch 4512, Loss 0.31440263986587524\n",
      "[Training Epoch 0] Batch 4513, Loss 0.31003454327583313\n",
      "[Training Epoch 0] Batch 4514, Loss 0.29855021834373474\n",
      "[Training Epoch 0] Batch 4515, Loss 0.329047828912735\n",
      "[Training Epoch 0] Batch 4516, Loss 0.3104565739631653\n",
      "[Training Epoch 0] Batch 4517, Loss 0.3020106554031372\n",
      "[Training Epoch 0] Batch 4518, Loss 0.3141118884086609\n",
      "[Training Epoch 0] Batch 4519, Loss 0.28959131240844727\n",
      "[Training Epoch 0] Batch 4520, Loss 0.3416166305541992\n",
      "[Training Epoch 0] Batch 4521, Loss 0.314613938331604\n",
      "[Training Epoch 0] Batch 4522, Loss 0.33379000425338745\n",
      "[Training Epoch 0] Batch 4523, Loss 0.3030022978782654\n",
      "[Training Epoch 0] Batch 4524, Loss 0.33952516317367554\n",
      "[Training Epoch 0] Batch 4525, Loss 0.309755802154541\n",
      "[Training Epoch 0] Batch 4526, Loss 0.34550654888153076\n",
      "[Training Epoch 0] Batch 4527, Loss 0.3136572241783142\n",
      "[Training Epoch 0] Batch 4528, Loss 0.2998617887496948\n",
      "[Training Epoch 0] Batch 4529, Loss 0.30707013607025146\n",
      "[Training Epoch 0] Batch 4530, Loss 0.2993474006652832\n",
      "[Training Epoch 0] Batch 4531, Loss 0.30491769313812256\n",
      "[Training Epoch 0] Batch 4532, Loss 0.3117784857749939\n",
      "[Training Epoch 0] Batch 4533, Loss 0.30171769857406616\n",
      "[Training Epoch 0] Batch 4534, Loss 0.32926198840141296\n",
      "[Training Epoch 0] Batch 4535, Loss 0.3124382495880127\n",
      "[Training Epoch 0] Batch 4536, Loss 0.3144739866256714\n",
      "[Training Epoch 0] Batch 4537, Loss 0.2752186357975006\n",
      "[Training Epoch 0] Batch 4538, Loss 0.3087206482887268\n",
      "[Training Epoch 0] Batch 4539, Loss 0.3006933331489563\n",
      "[Training Epoch 0] Batch 4540, Loss 0.28205037117004395\n",
      "[Training Epoch 0] Batch 4541, Loss 0.3093563914299011\n",
      "[Training Epoch 0] Batch 4542, Loss 0.3136671185493469\n",
      "[Training Epoch 0] Batch 4543, Loss 0.32108253240585327\n",
      "[Training Epoch 0] Batch 4544, Loss 0.3428928256034851\n",
      "[Training Epoch 0] Batch 4545, Loss 0.2906534671783447\n",
      "[Training Epoch 0] Batch 4546, Loss 0.2984463572502136\n",
      "[Training Epoch 0] Batch 4547, Loss 0.3243159353733063\n",
      "[Training Epoch 0] Batch 4548, Loss 0.33295774459838867\n",
      "[Training Epoch 0] Batch 4549, Loss 0.30330923199653625\n",
      "[Training Epoch 0] Batch 4550, Loss 0.3337180018424988\n",
      "[Training Epoch 0] Batch 4551, Loss 0.3126140832901001\n",
      "[Training Epoch 0] Batch 4552, Loss 0.30216506123542786\n",
      "[Training Epoch 0] Batch 4553, Loss 0.3234345316886902\n",
      "[Training Epoch 0] Batch 4554, Loss 0.2985665202140808\n",
      "[Training Epoch 0] Batch 4555, Loss 0.3199179172515869\n",
      "[Training Epoch 0] Batch 4556, Loss 0.29014188051223755\n",
      "[Training Epoch 0] Batch 4557, Loss 0.33315104246139526\n",
      "[Training Epoch 0] Batch 4558, Loss 0.29110705852508545\n",
      "[Training Epoch 0] Batch 4559, Loss 0.3262398838996887\n",
      "[Training Epoch 0] Batch 4560, Loss 0.3246508240699768\n",
      "[Training Epoch 0] Batch 4561, Loss 0.3106663227081299\n",
      "[Training Epoch 0] Batch 4562, Loss 0.3085328936576843\n",
      "[Training Epoch 0] Batch 4563, Loss 0.3095968961715698\n",
      "[Training Epoch 0] Batch 4564, Loss 0.30341821908950806\n",
      "[Training Epoch 0] Batch 4565, Loss 0.29182693362236023\n",
      "[Training Epoch 0] Batch 4566, Loss 0.3061548173427582\n",
      "[Training Epoch 0] Batch 4567, Loss 0.3010426163673401\n",
      "[Training Epoch 0] Batch 4568, Loss 0.3267267644405365\n",
      "[Training Epoch 0] Batch 4569, Loss 0.33061155676841736\n",
      "[Training Epoch 0] Batch 4570, Loss 0.31685441732406616\n",
      "[Training Epoch 0] Batch 4571, Loss 0.2788243293762207\n",
      "[Training Epoch 0] Batch 4572, Loss 0.32304948568344116\n",
      "[Training Epoch 0] Batch 4573, Loss 0.29551953077316284\n",
      "[Training Epoch 0] Batch 4574, Loss 0.2999493181705475\n",
      "[Training Epoch 0] Batch 4575, Loss 0.3106454610824585\n",
      "[Training Epoch 0] Batch 4576, Loss 0.33154597878456116\n",
      "[Training Epoch 0] Batch 4577, Loss 0.2716318964958191\n",
      "[Training Epoch 0] Batch 4578, Loss 0.3162444531917572\n",
      "[Training Epoch 0] Batch 4579, Loss 0.3042900562286377\n",
      "[Training Epoch 0] Batch 4580, Loss 0.30294978618621826\n",
      "[Training Epoch 0] Batch 4581, Loss 0.306044340133667\n",
      "[Training Epoch 0] Batch 4582, Loss 0.2842859625816345\n",
      "[Training Epoch 0] Batch 4583, Loss 0.31323888897895813\n",
      "[Training Epoch 0] Batch 4584, Loss 0.3354969322681427\n",
      "[Training Epoch 0] Batch 4585, Loss 0.3248252868652344\n",
      "[Training Epoch 0] Batch 4586, Loss 0.3639189302921295\n",
      "[Training Epoch 0] Batch 4587, Loss 0.3030710220336914\n",
      "[Training Epoch 0] Batch 4588, Loss 0.30207186937332153\n",
      "[Training Epoch 0] Batch 4589, Loss 0.3005521893501282\n",
      "[Training Epoch 0] Batch 4590, Loss 0.3278871178627014\n",
      "[Training Epoch 0] Batch 4591, Loss 0.32762622833251953\n",
      "[Training Epoch 0] Batch 4592, Loss 0.3182470202445984\n",
      "[Training Epoch 0] Batch 4593, Loss 0.296406090259552\n",
      "[Training Epoch 0] Batch 4594, Loss 0.3084298372268677\n",
      "[Training Epoch 0] Batch 4595, Loss 0.32083821296691895\n",
      "[Training Epoch 0] Batch 4596, Loss 0.31725263595581055\n",
      "[Training Epoch 0] Batch 4597, Loss 0.3202196955680847\n",
      "[Training Epoch 0] Batch 4598, Loss 0.2976992726325989\n",
      "[Training Epoch 0] Batch 4599, Loss 0.3204377293586731\n",
      "[Training Epoch 0] Batch 4600, Loss 0.33882397413253784\n",
      "[Training Epoch 0] Batch 4601, Loss 0.3151865601539612\n",
      "[Training Epoch 0] Batch 4602, Loss 0.3004959225654602\n",
      "[Training Epoch 0] Batch 4603, Loss 0.30012068152427673\n",
      "[Training Epoch 0] Batch 4604, Loss 0.32316142320632935\n",
      "[Training Epoch 0] Batch 4605, Loss 0.3004001975059509\n",
      "[Training Epoch 0] Batch 4606, Loss 0.29151278734207153\n",
      "[Training Epoch 0] Batch 4607, Loss 0.2890625596046448\n",
      "[Training Epoch 0] Batch 4608, Loss 0.3029285669326782\n",
      "[Training Epoch 0] Batch 4609, Loss 0.2927500307559967\n",
      "[Training Epoch 0] Batch 4610, Loss 0.30972087383270264\n",
      "[Training Epoch 0] Batch 4611, Loss 0.29964327812194824\n",
      "[Training Epoch 0] Batch 4612, Loss 0.2916328012943268\n",
      "[Training Epoch 0] Batch 4613, Loss 0.3021342158317566\n",
      "[Training Epoch 0] Batch 4614, Loss 0.325604110956192\n",
      "[Training Epoch 0] Batch 4615, Loss 0.288309782743454\n",
      "[Training Epoch 0] Batch 4616, Loss 0.3371245563030243\n",
      "[Training Epoch 0] Batch 4617, Loss 0.3528423011302948\n",
      "[Training Epoch 0] Batch 4618, Loss 0.29885971546173096\n",
      "[Training Epoch 0] Batch 4619, Loss 0.320882111787796\n",
      "[Training Epoch 0] Batch 4620, Loss 0.31164371967315674\n",
      "[Training Epoch 0] Batch 4621, Loss 0.2960188388824463\n",
      "[Training Epoch 0] Batch 4622, Loss 0.28908586502075195\n",
      "[Training Epoch 0] Batch 4623, Loss 0.2986200451850891\n",
      "[Training Epoch 0] Batch 4624, Loss 0.3070880174636841\n",
      "[Training Epoch 0] Batch 4625, Loss 0.3172491788864136\n",
      "[Training Epoch 0] Batch 4626, Loss 0.32664841413497925\n",
      "[Training Epoch 0] Batch 4627, Loss 0.30119597911834717\n",
      "[Training Epoch 0] Batch 4628, Loss 0.32894521951675415\n",
      "[Training Epoch 0] Batch 4629, Loss 0.3275938630104065\n",
      "[Training Epoch 0] Batch 4630, Loss 0.305602490901947\n",
      "[Training Epoch 0] Batch 4631, Loss 0.29064345359802246\n",
      "[Training Epoch 0] Batch 4632, Loss 0.31191128492355347\n",
      "[Training Epoch 0] Batch 4633, Loss 0.3303569257259369\n",
      "[Training Epoch 0] Batch 4634, Loss 0.33751797676086426\n",
      "[Training Epoch 0] Batch 4635, Loss 0.28633636236190796\n",
      "[Training Epoch 0] Batch 4636, Loss 0.3174345791339874\n",
      "[Training Epoch 0] Batch 4637, Loss 0.31473979353904724\n",
      "[Training Epoch 0] Batch 4638, Loss 0.32958126068115234\n",
      "[Training Epoch 0] Batch 4639, Loss 0.3364472985267639\n",
      "[Training Epoch 0] Batch 4640, Loss 0.31046468019485474\n",
      "[Training Epoch 0] Batch 4641, Loss 0.3265714645385742\n",
      "[Training Epoch 0] Batch 4642, Loss 0.2979145050048828\n",
      "[Training Epoch 0] Batch 4643, Loss 0.3016742467880249\n",
      "[Training Epoch 0] Batch 4644, Loss 0.3031081259250641\n",
      "[Training Epoch 0] Batch 4645, Loss 0.31959646940231323\n",
      "[Training Epoch 0] Batch 4646, Loss 0.3026059865951538\n",
      "[Training Epoch 0] Batch 4647, Loss 0.31216394901275635\n",
      "[Training Epoch 0] Batch 4648, Loss 0.32285410165786743\n",
      "[Training Epoch 0] Batch 4649, Loss 0.3193259835243225\n",
      "[Training Epoch 0] Batch 4650, Loss 0.2972192168235779\n",
      "[Training Epoch 0] Batch 4651, Loss 0.3014139235019684\n",
      "[Training Epoch 0] Batch 4652, Loss 0.3044283986091614\n",
      "[Training Epoch 0] Batch 4653, Loss 0.3243231773376465\n",
      "[Training Epoch 0] Batch 4654, Loss 0.29361692070961\n",
      "[Training Epoch 0] Batch 4655, Loss 0.3512302339076996\n",
      "[Training Epoch 0] Batch 4656, Loss 0.29915252327919006\n",
      "[Training Epoch 0] Batch 4657, Loss 0.27923476696014404\n",
      "[Training Epoch 0] Batch 4658, Loss 0.31672078371047974\n",
      "[Training Epoch 0] Batch 4659, Loss 0.28812912106513977\n",
      "[Training Epoch 0] Batch 4660, Loss 0.29458463191986084\n",
      "[Training Epoch 0] Batch 4661, Loss 0.32476121187210083\n",
      "[Training Epoch 0] Batch 4662, Loss 0.325954794883728\n",
      "[Training Epoch 0] Batch 4663, Loss 0.30628883838653564\n",
      "[Training Epoch 0] Batch 4664, Loss 0.31751763820648193\n",
      "[Training Epoch 0] Batch 4665, Loss 0.30923789739608765\n",
      "[Training Epoch 0] Batch 4666, Loss 0.31403788924217224\n",
      "[Training Epoch 0] Batch 4667, Loss 0.2938634157180786\n",
      "[Training Epoch 0] Batch 4668, Loss 0.31458425521850586\n",
      "[Training Epoch 0] Batch 4669, Loss 0.2872885763645172\n",
      "[Training Epoch 0] Batch 4670, Loss 0.3148820400238037\n",
      "[Training Epoch 0] Batch 4671, Loss 0.3000352382659912\n",
      "[Training Epoch 0] Batch 4672, Loss 0.30866485834121704\n",
      "[Training Epoch 0] Batch 4673, Loss 0.3085451126098633\n",
      "[Training Epoch 0] Batch 4674, Loss 0.33146876096725464\n",
      "[Training Epoch 0] Batch 4675, Loss 0.3533157408237457\n",
      "[Training Epoch 0] Batch 4676, Loss 0.3601677417755127\n",
      "[Training Epoch 0] Batch 4677, Loss 0.320940762758255\n",
      "[Training Epoch 0] Batch 4678, Loss 0.28533634543418884\n",
      "[Training Epoch 0] Batch 4679, Loss 0.3051576614379883\n",
      "[Training Epoch 0] Batch 4680, Loss 0.3211415410041809\n",
      "[Training Epoch 0] Batch 4681, Loss 0.3287510871887207\n",
      "[Training Epoch 0] Batch 4682, Loss 0.29459434747695923\n",
      "[Training Epoch 0] Batch 4683, Loss 0.31885290145874023\n",
      "[Training Epoch 0] Batch 4684, Loss 0.31260377168655396\n",
      "[Training Epoch 0] Batch 4685, Loss 0.2927348017692566\n",
      "[Training Epoch 0] Batch 4686, Loss 0.2948492467403412\n",
      "[Training Epoch 0] Batch 4687, Loss 0.2892489731311798\n",
      "[Training Epoch 0] Batch 4688, Loss 0.3087800443172455\n",
      "[Training Epoch 0] Batch 4689, Loss 0.2849276661872864\n",
      "[Training Epoch 0] Batch 4690, Loss 0.30021464824676514\n",
      "[Training Epoch 0] Batch 4691, Loss 0.31398677825927734\n",
      "[Training Epoch 0] Batch 4692, Loss 0.2977561354637146\n",
      "[Training Epoch 0] Batch 4693, Loss 0.30498695373535156\n",
      "[Training Epoch 0] Batch 4694, Loss 0.29742270708084106\n",
      "[Training Epoch 0] Batch 4695, Loss 0.32580384612083435\n",
      "[Training Epoch 0] Batch 4696, Loss 0.2812674343585968\n",
      "[Training Epoch 0] Batch 4697, Loss 0.34084776043891907\n",
      "[Training Epoch 0] Batch 4698, Loss 0.32811465859413147\n",
      "[Training Epoch 0] Batch 4699, Loss 0.32697463035583496\n",
      "[Training Epoch 0] Batch 4700, Loss 0.29044297337532043\n",
      "[Training Epoch 0] Batch 4701, Loss 0.2854141294956207\n",
      "[Training Epoch 0] Batch 4702, Loss 0.3021690249443054\n",
      "[Training Epoch 0] Batch 4703, Loss 0.33982837200164795\n",
      "[Training Epoch 0] Batch 4704, Loss 0.32172632217407227\n",
      "[Training Epoch 0] Batch 4705, Loss 0.294128954410553\n",
      "[Training Epoch 0] Batch 4706, Loss 0.32843321561813354\n",
      "[Training Epoch 0] Batch 4707, Loss 0.2851535975933075\n",
      "[Training Epoch 0] Batch 4708, Loss 0.32468825578689575\n",
      "[Training Epoch 0] Batch 4709, Loss 0.3143514096736908\n",
      "[Training Epoch 0] Batch 4710, Loss 0.32540059089660645\n",
      "[Training Epoch 0] Batch 4711, Loss 0.3092745542526245\n",
      "[Training Epoch 0] Batch 4712, Loss 0.32115769386291504\n",
      "[Training Epoch 0] Batch 4713, Loss 0.25937891006469727\n",
      "[Training Epoch 0] Batch 4714, Loss 0.33523884415626526\n",
      "[Training Epoch 0] Batch 4715, Loss 0.2980806231498718\n",
      "[Training Epoch 0] Batch 4716, Loss 0.32356831431388855\n",
      "[Training Epoch 0] Batch 4717, Loss 0.3390955626964569\n",
      "[Training Epoch 0] Batch 4718, Loss 0.32986992597579956\n",
      "[Training Epoch 0] Batch 4719, Loss 0.3295634984970093\n",
      "[Training Epoch 0] Batch 4720, Loss 0.3217339515686035\n",
      "[Training Epoch 0] Batch 4721, Loss 0.2889893054962158\n",
      "[Training Epoch 0] Batch 4722, Loss 0.299716979265213\n",
      "[Training Epoch 0] Batch 4723, Loss 0.3305118978023529\n",
      "[Training Epoch 0] Batch 4724, Loss 0.29676204919815063\n",
      "[Training Epoch 0] Batch 4725, Loss 0.3398611545562744\n",
      "[Training Epoch 0] Batch 4726, Loss 0.3226845860481262\n",
      "[Training Epoch 0] Batch 4727, Loss 0.311481773853302\n",
      "[Training Epoch 0] Batch 4728, Loss 0.30478373169898987\n",
      "[Training Epoch 0] Batch 4729, Loss 0.3118066191673279\n",
      "[Training Epoch 0] Batch 4730, Loss 0.3121626377105713\n",
      "[Training Epoch 0] Batch 4731, Loss 0.26582223176956177\n",
      "[Training Epoch 0] Batch 4732, Loss 0.3217635154724121\n",
      "[Training Epoch 0] Batch 4733, Loss 0.30904558300971985\n",
      "[Training Epoch 0] Batch 4734, Loss 0.3127448558807373\n",
      "[Training Epoch 0] Batch 4735, Loss 0.32989072799682617\n",
      "[Training Epoch 0] Batch 4736, Loss 0.33583804965019226\n",
      "[Training Epoch 0] Batch 4737, Loss 0.2960616946220398\n",
      "[Training Epoch 0] Batch 4738, Loss 0.30351391434669495\n",
      "[Training Epoch 0] Batch 4739, Loss 0.3147868514060974\n",
      "[Training Epoch 0] Batch 4740, Loss 0.303779274225235\n",
      "[Training Epoch 0] Batch 4741, Loss 0.3026115894317627\n",
      "[Training Epoch 0] Batch 4742, Loss 0.32183563709259033\n",
      "[Training Epoch 0] Batch 4743, Loss 0.28818395733833313\n",
      "[Training Epoch 0] Batch 4744, Loss 0.28281885385513306\n",
      "[Training Epoch 0] Batch 4745, Loss 0.3316912353038788\n",
      "[Training Epoch 0] Batch 4746, Loss 0.33650660514831543\n",
      "[Training Epoch 0] Batch 4747, Loss 0.2903467118740082\n",
      "[Training Epoch 0] Batch 4748, Loss 0.3110843598842621\n",
      "[Training Epoch 0] Batch 4749, Loss 0.3182588219642639\n",
      "[Training Epoch 0] Batch 4750, Loss 0.34219157695770264\n",
      "[Training Epoch 0] Batch 4751, Loss 0.27298009395599365\n",
      "[Training Epoch 0] Batch 4752, Loss 0.33530041575431824\n",
      "[Training Epoch 0] Batch 4753, Loss 0.3172914981842041\n",
      "[Training Epoch 0] Batch 4754, Loss 0.31684958934783936\n",
      "[Training Epoch 0] Batch 4755, Loss 0.2946355938911438\n",
      "[Training Epoch 0] Batch 4756, Loss 0.308750718832016\n",
      "[Training Epoch 0] Batch 4757, Loss 0.30724233388900757\n",
      "[Training Epoch 0] Batch 4758, Loss 0.299258828163147\n",
      "[Training Epoch 0] Batch 4759, Loss 0.32109469175338745\n",
      "[Training Epoch 0] Batch 4760, Loss 0.29632219672203064\n",
      "[Training Epoch 0] Batch 4761, Loss 0.30672281980514526\n",
      "[Training Epoch 0] Batch 4762, Loss 0.30689746141433716\n",
      "[Training Epoch 0] Batch 4763, Loss 0.2993859052658081\n",
      "[Training Epoch 0] Batch 4764, Loss 0.31644582748413086\n",
      "[Training Epoch 0] Batch 4765, Loss 0.3408348560333252\n",
      "[Training Epoch 0] Batch 4766, Loss 0.30412450432777405\n",
      "[Training Epoch 0] Batch 4767, Loss 0.2879760265350342\n",
      "[Training Epoch 0] Batch 4768, Loss 0.3224501609802246\n",
      "[Training Epoch 0] Batch 4769, Loss 0.2988688349723816\n",
      "[Training Epoch 0] Batch 4770, Loss 0.34226539731025696\n",
      "[Training Epoch 0] Batch 4771, Loss 0.3282372057437897\n",
      "[Training Epoch 0] Batch 4772, Loss 0.3237544894218445\n",
      "[Training Epoch 0] Batch 4773, Loss 0.3040270209312439\n",
      "[Training Epoch 0] Batch 4774, Loss 0.31165555119514465\n",
      "[Training Epoch 0] Batch 4775, Loss 0.3012155294418335\n",
      "[Training Epoch 0] Batch 4776, Loss 0.3081005811691284\n",
      "[Training Epoch 0] Batch 4777, Loss 0.3228244185447693\n",
      "[Training Epoch 0] Batch 4778, Loss 0.2938973307609558\n",
      "[Training Epoch 0] Batch 4779, Loss 0.33266401290893555\n",
      "[Training Epoch 0] Batch 4780, Loss 0.285290002822876\n",
      "[Training Epoch 0] Batch 4781, Loss 0.2953391671180725\n",
      "[Training Epoch 0] Batch 4782, Loss 0.3007681369781494\n",
      "[Training Epoch 0] Batch 4783, Loss 0.33865174651145935\n",
      "[Training Epoch 0] Batch 4784, Loss 0.29284048080444336\n",
      "[Training Epoch 0] Batch 4785, Loss 0.32717636227607727\n",
      "[Training Epoch 0] Batch 4786, Loss 0.3153507113456726\n",
      "[Training Epoch 0] Batch 4787, Loss 0.2843402028083801\n",
      "[Training Epoch 0] Batch 4788, Loss 0.2892307639122009\n",
      "[Training Epoch 0] Batch 4789, Loss 0.32249075174331665\n",
      "[Training Epoch 0] Batch 4790, Loss 0.3109293580055237\n",
      "[Training Epoch 0] Batch 4791, Loss 0.32316213846206665\n",
      "[Training Epoch 0] Batch 4792, Loss 0.3180791735649109\n",
      "[Training Epoch 0] Batch 4793, Loss 0.2737042009830475\n",
      "[Training Epoch 0] Batch 4794, Loss 0.3206555247306824\n",
      "[Training Epoch 0] Batch 4795, Loss 0.2919749617576599\n",
      "[Training Epoch 0] Batch 4796, Loss 0.28631195425987244\n",
      "[Training Epoch 0] Batch 4797, Loss 0.29458874464035034\n",
      "[Training Epoch 0] Batch 4798, Loss 0.3292493522167206\n",
      "[Training Epoch 0] Batch 4799, Loss 0.31633710861206055\n",
      "[Training Epoch 0] Batch 4800, Loss 0.3179185092449188\n",
      "[Training Epoch 0] Batch 4801, Loss 0.30339768528938293\n",
      "[Training Epoch 0] Batch 4802, Loss 0.3056770861148834\n",
      "[Training Epoch 0] Batch 4803, Loss 0.3093487024307251\n",
      "[Training Epoch 0] Batch 4804, Loss 0.3030191957950592\n",
      "[Training Epoch 0] Batch 4805, Loss 0.31865638494491577\n",
      "[Training Epoch 0] Batch 4806, Loss 0.2906101942062378\n",
      "[Training Epoch 0] Batch 4807, Loss 0.31091737747192383\n",
      "[Training Epoch 0] Batch 4808, Loss 0.32992252707481384\n",
      "[Training Epoch 0] Batch 4809, Loss 0.31634512543678284\n",
      "[Training Epoch 0] Batch 4810, Loss 0.30553245544433594\n",
      "[Training Epoch 0] Batch 4811, Loss 0.3256363570690155\n",
      "[Training Epoch 0] Batch 4812, Loss 0.28992414474487305\n",
      "[Training Epoch 0] Batch 4813, Loss 0.2916311025619507\n",
      "[Training Epoch 0] Batch 4814, Loss 0.3052697479724884\n",
      "[Training Epoch 0] Batch 4815, Loss 0.31114763021469116\n",
      "[Training Epoch 0] Batch 4816, Loss 0.3207572102546692\n",
      "[Training Epoch 0] Batch 4817, Loss 0.2981802225112915\n",
      "[Training Epoch 0] Batch 4818, Loss 0.31204789876937866\n",
      "[Training Epoch 0] Batch 4819, Loss 0.29616597294807434\n",
      "[Training Epoch 0] Batch 4820, Loss 0.32514381408691406\n",
      "[Training Epoch 0] Batch 4821, Loss 0.2856013774871826\n",
      "[Training Epoch 0] Batch 4822, Loss 0.30019670724868774\n",
      "[Training Epoch 0] Batch 4823, Loss 0.33164459466934204\n",
      "[Training Epoch 0] Batch 4824, Loss 0.29651618003845215\n",
      "[Training Epoch 0] Batch 4825, Loss 0.33354806900024414\n",
      "[Training Epoch 0] Batch 4826, Loss 0.32584652304649353\n",
      "[Training Epoch 0] Batch 4827, Loss 0.3176869750022888\n",
      "[Training Epoch 0] Batch 4828, Loss 0.3128477931022644\n",
      "[Training Epoch 0] Batch 4829, Loss 0.30188584327697754\n",
      "[Training Epoch 0] Batch 4830, Loss 0.28662052750587463\n",
      "[Training Epoch 0] Batch 4831, Loss 0.33971071243286133\n",
      "[Training Epoch 0] Batch 4832, Loss 0.30586904287338257\n",
      "[Training Epoch 0] Batch 4833, Loss 0.30232006311416626\n",
      "[Training Epoch 0] Batch 4834, Loss 0.3201456665992737\n",
      "[Training Epoch 0] Batch 4835, Loss 0.31740251183509827\n",
      "[Training Epoch 0] Batch 4836, Loss 0.3323279023170471\n",
      "[Training Epoch 0] Batch 4837, Loss 0.3201671242713928\n",
      "[Training Epoch 0] Batch 4838, Loss 0.28767746686935425\n",
      "[Training Epoch 0] Batch 4839, Loss 0.3123641014099121\n",
      "[Training Epoch 0] Batch 4840, Loss 0.2980419397354126\n",
      "[Training Epoch 0] Batch 4841, Loss 0.32164430618286133\n",
      "[Training Epoch 0] Batch 4842, Loss 0.30901697278022766\n",
      "[Training Epoch 0] Batch 4843, Loss 0.28321999311447144\n",
      "[Training Epoch 0] Batch 4844, Loss 0.31464874744415283\n",
      "[Training Epoch 0] Batch 4845, Loss 0.31834912300109863\n",
      "[Training Epoch 0] Batch 4846, Loss 0.29742056131362915\n",
      "[Training Epoch 0] Batch 4847, Loss 0.28032711148262024\n",
      "[Training Epoch 0] Batch 4848, Loss 0.2860397398471832\n",
      "[Training Epoch 0] Batch 4849, Loss 0.309712290763855\n",
      "[Training Epoch 0] Batch 4850, Loss 0.2834551930427551\n",
      "[Training Epoch 0] Batch 4851, Loss 0.3249582350254059\n",
      "[Training Epoch 0] Batch 4852, Loss 0.2892075181007385\n",
      "[Training Epoch 0] Batch 4853, Loss 0.32777392864227295\n",
      "[Training Epoch 0] Batch 4854, Loss 0.3994547724723816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 584/584 [00:00<00:00, 1286.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6040\n",
      "Length of test_items: 6040\n",
      "Length of test_preds: 6040\n",
      "[Evaluating Epoch 0] Precision = 0.2594, Recall = 1.0000\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3224395513534546\n",
      "[Training Epoch 1] Batch 1, Loss 0.3075719475746155\n",
      "[Training Epoch 1] Batch 2, Loss 0.2836116552352905\n",
      "[Training Epoch 1] Batch 3, Loss 0.2968325614929199\n",
      "[Training Epoch 1] Batch 4, Loss 0.31246185302734375\n",
      "[Training Epoch 1] Batch 5, Loss 0.3001883327960968\n",
      "[Training Epoch 1] Batch 6, Loss 0.28948545455932617\n",
      "[Training Epoch 1] Batch 7, Loss 0.3070160746574402\n",
      "[Training Epoch 1] Batch 8, Loss 0.29410380125045776\n",
      "[Training Epoch 1] Batch 9, Loss 0.3113848865032196\n",
      "[Training Epoch 1] Batch 10, Loss 0.2875472605228424\n",
      "[Training Epoch 1] Batch 11, Loss 0.287916898727417\n",
      "[Training Epoch 1] Batch 12, Loss 0.29227519035339355\n",
      "[Training Epoch 1] Batch 13, Loss 0.34022900462150574\n",
      "[Training Epoch 1] Batch 14, Loss 0.31858116388320923\n",
      "[Training Epoch 1] Batch 15, Loss 0.3133339583873749\n",
      "[Training Epoch 1] Batch 16, Loss 0.29950082302093506\n",
      "[Training Epoch 1] Batch 17, Loss 0.29507604241371155\n",
      "[Training Epoch 1] Batch 18, Loss 0.3032381236553192\n",
      "[Training Epoch 1] Batch 19, Loss 0.31699061393737793\n",
      "[Training Epoch 1] Batch 20, Loss 0.3032971918582916\n",
      "[Training Epoch 1] Batch 21, Loss 0.30318161845207214\n",
      "[Training Epoch 1] Batch 22, Loss 0.346861869096756\n",
      "[Training Epoch 1] Batch 23, Loss 0.3091707229614258\n",
      "[Training Epoch 1] Batch 24, Loss 0.3142715096473694\n",
      "[Training Epoch 1] Batch 25, Loss 0.2977451682090759\n",
      "[Training Epoch 1] Batch 26, Loss 0.3049546778202057\n",
      "[Training Epoch 1] Batch 27, Loss 0.3276251554489136\n",
      "[Training Epoch 1] Batch 28, Loss 0.2966935634613037\n",
      "[Training Epoch 1] Batch 29, Loss 0.3094322085380554\n",
      "[Training Epoch 1] Batch 30, Loss 0.2998594641685486\n",
      "[Training Epoch 1] Batch 31, Loss 0.2851054072380066\n",
      "[Training Epoch 1] Batch 32, Loss 0.3149759769439697\n",
      "[Training Epoch 1] Batch 33, Loss 0.31503838300704956\n",
      "[Training Epoch 1] Batch 34, Loss 0.29512491822242737\n",
      "[Training Epoch 1] Batch 35, Loss 0.3142396807670593\n",
      "[Training Epoch 1] Batch 36, Loss 0.3200805187225342\n",
      "[Training Epoch 1] Batch 37, Loss 0.29151350259780884\n",
      "[Training Epoch 1] Batch 38, Loss 0.31487786769866943\n",
      "[Training Epoch 1] Batch 39, Loss 0.31277787685394287\n",
      "[Training Epoch 1] Batch 40, Loss 0.2833634614944458\n",
      "[Training Epoch 1] Batch 41, Loss 0.29800304770469666\n",
      "[Training Epoch 1] Batch 42, Loss 0.29967373609542847\n",
      "[Training Epoch 1] Batch 43, Loss 0.312076210975647\n",
      "[Training Epoch 1] Batch 44, Loss 0.2992090582847595\n",
      "[Training Epoch 1] Batch 45, Loss 0.2832152843475342\n",
      "[Training Epoch 1] Batch 46, Loss 0.3249177634716034\n",
      "[Training Epoch 1] Batch 47, Loss 0.3236754536628723\n",
      "[Training Epoch 1] Batch 48, Loss 0.29743048548698425\n",
      "[Training Epoch 1] Batch 49, Loss 0.3196718692779541\n",
      "[Training Epoch 1] Batch 50, Loss 0.2990293502807617\n",
      "[Training Epoch 1] Batch 51, Loss 0.30397528409957886\n",
      "[Training Epoch 1] Batch 52, Loss 0.2955441474914551\n",
      "[Training Epoch 1] Batch 53, Loss 0.3039672374725342\n",
      "[Training Epoch 1] Batch 54, Loss 0.32005438208580017\n",
      "[Training Epoch 1] Batch 55, Loss 0.30560123920440674\n",
      "[Training Epoch 1] Batch 56, Loss 0.2837883234024048\n",
      "[Training Epoch 1] Batch 57, Loss 0.29861313104629517\n",
      "[Training Epoch 1] Batch 58, Loss 0.2898051142692566\n",
      "[Training Epoch 1] Batch 59, Loss 0.3119407892227173\n",
      "[Training Epoch 1] Batch 60, Loss 0.29996204376220703\n",
      "[Training Epoch 1] Batch 61, Loss 0.29119545221328735\n",
      "[Training Epoch 1] Batch 62, Loss 0.2709032893180847\n",
      "[Training Epoch 1] Batch 63, Loss 0.300129771232605\n",
      "[Training Epoch 1] Batch 64, Loss 0.3199167847633362\n",
      "[Training Epoch 1] Batch 65, Loss 0.2815578281879425\n",
      "[Training Epoch 1] Batch 66, Loss 0.27238839864730835\n",
      "[Training Epoch 1] Batch 67, Loss 0.32133904099464417\n",
      "[Training Epoch 1] Batch 68, Loss 0.2873830199241638\n",
      "[Training Epoch 1] Batch 69, Loss 0.30839627981185913\n",
      "[Training Epoch 1] Batch 70, Loss 0.30508047342300415\n",
      "[Training Epoch 1] Batch 71, Loss 0.2615202069282532\n",
      "[Training Epoch 1] Batch 72, Loss 0.31903988122940063\n",
      "[Training Epoch 1] Batch 73, Loss 0.31220585107803345\n",
      "[Training Epoch 1] Batch 74, Loss 0.2748814523220062\n",
      "[Training Epoch 1] Batch 75, Loss 0.2849140167236328\n",
      "[Training Epoch 1] Batch 76, Loss 0.3387882113456726\n",
      "[Training Epoch 1] Batch 77, Loss 0.2603164613246918\n",
      "[Training Epoch 1] Batch 78, Loss 0.299295037984848\n",
      "[Training Epoch 1] Batch 79, Loss 0.3309485912322998\n",
      "[Training Epoch 1] Batch 80, Loss 0.2871462106704712\n",
      "[Training Epoch 1] Batch 81, Loss 0.2908168435096741\n",
      "[Training Epoch 1] Batch 82, Loss 0.3171144127845764\n",
      "[Training Epoch 1] Batch 83, Loss 0.29012981057167053\n",
      "[Training Epoch 1] Batch 84, Loss 0.30234208703041077\n",
      "[Training Epoch 1] Batch 85, Loss 0.28442662954330444\n",
      "[Training Epoch 1] Batch 86, Loss 0.3147736191749573\n",
      "[Training Epoch 1] Batch 87, Loss 0.31483444571495056\n",
      "[Training Epoch 1] Batch 88, Loss 0.2977084815502167\n",
      "[Training Epoch 1] Batch 89, Loss 0.33731067180633545\n",
      "[Training Epoch 1] Batch 90, Loss 0.2945311963558197\n",
      "[Training Epoch 1] Batch 91, Loss 0.29352083802223206\n",
      "[Training Epoch 1] Batch 92, Loss 0.31499195098876953\n",
      "[Training Epoch 1] Batch 93, Loss 0.3038199543952942\n",
      "[Training Epoch 1] Batch 94, Loss 0.3384838104248047\n",
      "[Training Epoch 1] Batch 95, Loss 0.28662222623825073\n",
      "[Training Epoch 1] Batch 96, Loss 0.2822074890136719\n",
      "[Training Epoch 1] Batch 97, Loss 0.2872430384159088\n",
      "[Training Epoch 1] Batch 98, Loss 0.3196178078651428\n",
      "[Training Epoch 1] Batch 99, Loss 0.28156617283821106\n",
      "[Training Epoch 1] Batch 100, Loss 0.2734031677246094\n",
      "[Training Epoch 1] Batch 101, Loss 0.33232244849205017\n",
      "[Training Epoch 1] Batch 102, Loss 0.2949753403663635\n",
      "[Training Epoch 1] Batch 103, Loss 0.3091190457344055\n",
      "[Training Epoch 1] Batch 104, Loss 0.32730042934417725\n",
      "[Training Epoch 1] Batch 105, Loss 0.3333982825279236\n",
      "[Training Epoch 1] Batch 106, Loss 0.30909252166748047\n",
      "[Training Epoch 1] Batch 107, Loss 0.32112789154052734\n",
      "[Training Epoch 1] Batch 108, Loss 0.30391642451286316\n",
      "[Training Epoch 1] Batch 109, Loss 0.2972293496131897\n",
      "[Training Epoch 1] Batch 110, Loss 0.29178065061569214\n",
      "[Training Epoch 1] Batch 111, Loss 0.3174680769443512\n",
      "[Training Epoch 1] Batch 112, Loss 0.2866562604904175\n",
      "[Training Epoch 1] Batch 113, Loss 0.3223536014556885\n",
      "[Training Epoch 1] Batch 114, Loss 0.3016202449798584\n",
      "[Training Epoch 1] Batch 115, Loss 0.2857394218444824\n",
      "[Training Epoch 1] Batch 116, Loss 0.32491156458854675\n",
      "[Training Epoch 1] Batch 117, Loss 0.28498390316963196\n",
      "[Training Epoch 1] Batch 118, Loss 0.30100560188293457\n",
      "[Training Epoch 1] Batch 119, Loss 0.3111785352230072\n",
      "[Training Epoch 1] Batch 120, Loss 0.27913540601730347\n",
      "[Training Epoch 1] Batch 121, Loss 0.27848708629608154\n",
      "[Training Epoch 1] Batch 122, Loss 0.2908380329608917\n",
      "[Training Epoch 1] Batch 123, Loss 0.3403112292289734\n",
      "[Training Epoch 1] Batch 124, Loss 0.30076926946640015\n",
      "[Training Epoch 1] Batch 125, Loss 0.2912231683731079\n",
      "[Training Epoch 1] Batch 126, Loss 0.2870093584060669\n",
      "[Training Epoch 1] Batch 127, Loss 0.26064208149909973\n",
      "[Training Epoch 1] Batch 128, Loss 0.314901739358902\n",
      "[Training Epoch 1] Batch 129, Loss 0.2817602753639221\n",
      "[Training Epoch 1] Batch 130, Loss 0.31719475984573364\n",
      "[Training Epoch 1] Batch 131, Loss 0.3285871744155884\n",
      "[Training Epoch 1] Batch 132, Loss 0.31668975949287415\n",
      "[Training Epoch 1] Batch 133, Loss 0.2843959331512451\n",
      "[Training Epoch 1] Batch 134, Loss 0.3097906708717346\n",
      "[Training Epoch 1] Batch 135, Loss 0.28730830550193787\n",
      "[Training Epoch 1] Batch 136, Loss 0.29144641757011414\n",
      "[Training Epoch 1] Batch 137, Loss 0.28553083539009094\n",
      "[Training Epoch 1] Batch 138, Loss 0.2757629454135895\n",
      "[Training Epoch 1] Batch 139, Loss 0.3358060121536255\n",
      "[Training Epoch 1] Batch 140, Loss 0.3141336739063263\n",
      "[Training Epoch 1] Batch 141, Loss 0.3098808526992798\n",
      "[Training Epoch 1] Batch 142, Loss 0.3328482508659363\n",
      "[Training Epoch 1] Batch 143, Loss 0.3157026469707489\n",
      "[Training Epoch 1] Batch 144, Loss 0.2715165913105011\n",
      "[Training Epoch 1] Batch 145, Loss 0.2840992212295532\n",
      "[Training Epoch 1] Batch 146, Loss 0.31670695543289185\n",
      "[Training Epoch 1] Batch 147, Loss 0.320890337228775\n",
      "[Training Epoch 1] Batch 148, Loss 0.3166974186897278\n",
      "[Training Epoch 1] Batch 149, Loss 0.31086933612823486\n",
      "[Training Epoch 1] Batch 150, Loss 0.29585689306259155\n",
      "[Training Epoch 1] Batch 151, Loss 0.30789875984191895\n",
      "[Training Epoch 1] Batch 152, Loss 0.3001555800437927\n",
      "[Training Epoch 1] Batch 153, Loss 0.3101840019226074\n",
      "[Training Epoch 1] Batch 154, Loss 0.30413782596588135\n",
      "[Training Epoch 1] Batch 155, Loss 0.3405574560165405\n",
      "[Training Epoch 1] Batch 156, Loss 0.3034723401069641\n",
      "[Training Epoch 1] Batch 157, Loss 0.30743277072906494\n",
      "[Training Epoch 1] Batch 158, Loss 0.2947835326194763\n",
      "[Training Epoch 1] Batch 159, Loss 0.28849130868911743\n",
      "[Training Epoch 1] Batch 160, Loss 0.301416277885437\n",
      "[Training Epoch 1] Batch 161, Loss 0.31044262647628784\n",
      "[Training Epoch 1] Batch 162, Loss 0.29601263999938965\n",
      "[Training Epoch 1] Batch 163, Loss 0.2996668517589569\n",
      "[Training Epoch 1] Batch 164, Loss 0.28967252373695374\n",
      "[Training Epoch 1] Batch 165, Loss 0.29063963890075684\n",
      "[Training Epoch 1] Batch 166, Loss 0.28507840633392334\n",
      "[Training Epoch 1] Batch 167, Loss 0.276009202003479\n",
      "[Training Epoch 1] Batch 168, Loss 0.3097609877586365\n",
      "[Training Epoch 1] Batch 169, Loss 0.30196911096572876\n",
      "[Training Epoch 1] Batch 170, Loss 0.31726938486099243\n",
      "[Training Epoch 1] Batch 171, Loss 0.3005385994911194\n",
      "[Training Epoch 1] Batch 172, Loss 0.29613620042800903\n",
      "[Training Epoch 1] Batch 173, Loss 0.31990480422973633\n",
      "[Training Epoch 1] Batch 174, Loss 0.2788603901863098\n",
      "[Training Epoch 1] Batch 175, Loss 0.3077735900878906\n",
      "[Training Epoch 1] Batch 176, Loss 0.29290854930877686\n",
      "[Training Epoch 1] Batch 177, Loss 0.30784428119659424\n",
      "[Training Epoch 1] Batch 178, Loss 0.30094027519226074\n",
      "[Training Epoch 1] Batch 179, Loss 0.2960260808467865\n",
      "[Training Epoch 1] Batch 180, Loss 0.294536292552948\n",
      "[Training Epoch 1] Batch 181, Loss 0.31860047578811646\n",
      "[Training Epoch 1] Batch 182, Loss 0.3016657531261444\n",
      "[Training Epoch 1] Batch 183, Loss 0.3135121464729309\n",
      "[Training Epoch 1] Batch 184, Loss 0.29539716243743896\n",
      "[Training Epoch 1] Batch 185, Loss 0.2972656190395355\n",
      "[Training Epoch 1] Batch 186, Loss 0.31827014684677124\n",
      "[Training Epoch 1] Batch 187, Loss 0.31496375799179077\n",
      "[Training Epoch 1] Batch 188, Loss 0.3145288825035095\n",
      "[Training Epoch 1] Batch 189, Loss 0.2914416491985321\n",
      "[Training Epoch 1] Batch 190, Loss 0.2971675992012024\n",
      "[Training Epoch 1] Batch 191, Loss 0.31279152631759644\n",
      "[Training Epoch 1] Batch 192, Loss 0.2955954670906067\n",
      "[Training Epoch 1] Batch 193, Loss 0.311740905046463\n",
      "[Training Epoch 1] Batch 194, Loss 0.3081178069114685\n",
      "[Training Epoch 1] Batch 195, Loss 0.3032947778701782\n",
      "[Training Epoch 1] Batch 196, Loss 0.33726340532302856\n",
      "[Training Epoch 1] Batch 197, Loss 0.2979753017425537\n",
      "[Training Epoch 1] Batch 198, Loss 0.3359103798866272\n",
      "[Training Epoch 1] Batch 199, Loss 0.3134257197380066\n",
      "[Training Epoch 1] Batch 200, Loss 0.2853243947029114\n",
      "[Training Epoch 1] Batch 201, Loss 0.3359447121620178\n",
      "[Training Epoch 1] Batch 202, Loss 0.2823448181152344\n",
      "[Training Epoch 1] Batch 203, Loss 0.2978861629962921\n",
      "[Training Epoch 1] Batch 204, Loss 0.31645405292510986\n",
      "[Training Epoch 1] Batch 205, Loss 0.30810585618019104\n",
      "[Training Epoch 1] Batch 206, Loss 0.2988436222076416\n",
      "[Training Epoch 1] Batch 207, Loss 0.29570117592811584\n",
      "[Training Epoch 1] Batch 208, Loss 0.3119524121284485\n",
      "[Training Epoch 1] Batch 209, Loss 0.2965187430381775\n",
      "[Training Epoch 1] Batch 210, Loss 0.27891993522644043\n",
      "[Training Epoch 1] Batch 211, Loss 0.30591079592704773\n",
      "[Training Epoch 1] Batch 212, Loss 0.3004370331764221\n",
      "[Training Epoch 1] Batch 213, Loss 0.3265625238418579\n",
      "[Training Epoch 1] Batch 214, Loss 0.27407336235046387\n",
      "[Training Epoch 1] Batch 215, Loss 0.3566799759864807\n",
      "[Training Epoch 1] Batch 216, Loss 0.3051421642303467\n",
      "[Training Epoch 1] Batch 217, Loss 0.3063242435455322\n",
      "[Training Epoch 1] Batch 218, Loss 0.3140353560447693\n",
      "[Training Epoch 1] Batch 219, Loss 0.3009965419769287\n",
      "[Training Epoch 1] Batch 220, Loss 0.32666122913360596\n",
      "[Training Epoch 1] Batch 221, Loss 0.3028608560562134\n",
      "[Training Epoch 1] Batch 222, Loss 0.31492114067077637\n",
      "[Training Epoch 1] Batch 223, Loss 0.33143678307533264\n",
      "[Training Epoch 1] Batch 224, Loss 0.3219453990459442\n",
      "[Training Epoch 1] Batch 225, Loss 0.3021843433380127\n",
      "[Training Epoch 1] Batch 226, Loss 0.30060040950775146\n",
      "[Training Epoch 1] Batch 227, Loss 0.29282286763191223\n",
      "[Training Epoch 1] Batch 228, Loss 0.3490379750728607\n",
      "[Training Epoch 1] Batch 229, Loss 0.26605671644210815\n",
      "[Training Epoch 1] Batch 230, Loss 0.3155480921268463\n",
      "[Training Epoch 1] Batch 231, Loss 0.2651079595088959\n",
      "[Training Epoch 1] Batch 232, Loss 0.3134588599205017\n",
      "[Training Epoch 1] Batch 233, Loss 0.3044653534889221\n",
      "[Training Epoch 1] Batch 234, Loss 0.32110947370529175\n",
      "[Training Epoch 1] Batch 235, Loss 0.3022882342338562\n",
      "[Training Epoch 1] Batch 236, Loss 0.28867679834365845\n",
      "[Training Epoch 1] Batch 237, Loss 0.30845704674720764\n",
      "[Training Epoch 1] Batch 238, Loss 0.3191637396812439\n",
      "[Training Epoch 1] Batch 239, Loss 0.3256593346595764\n",
      "[Training Epoch 1] Batch 240, Loss 0.31415730714797974\n",
      "[Training Epoch 1] Batch 241, Loss 0.28692224621772766\n",
      "[Training Epoch 1] Batch 242, Loss 0.3039471507072449\n",
      "[Training Epoch 1] Batch 243, Loss 0.28084176778793335\n",
      "[Training Epoch 1] Batch 244, Loss 0.286080002784729\n",
      "[Training Epoch 1] Batch 245, Loss 0.29430538415908813\n",
      "[Training Epoch 1] Batch 246, Loss 0.30691245198249817\n",
      "[Training Epoch 1] Batch 247, Loss 0.30838075280189514\n",
      "[Training Epoch 1] Batch 248, Loss 0.3009940981864929\n",
      "[Training Epoch 1] Batch 249, Loss 0.30459558963775635\n",
      "[Training Epoch 1] Batch 250, Loss 0.33520394563674927\n",
      "[Training Epoch 1] Batch 251, Loss 0.2877170741558075\n",
      "[Training Epoch 1] Batch 252, Loss 0.3139384686946869\n",
      "[Training Epoch 1] Batch 253, Loss 0.33053839206695557\n",
      "[Training Epoch 1] Batch 254, Loss 0.2927820086479187\n",
      "[Training Epoch 1] Batch 255, Loss 0.3290344178676605\n",
      "[Training Epoch 1] Batch 256, Loss 0.28889524936676025\n",
      "[Training Epoch 1] Batch 257, Loss 0.31422609090805054\n",
      "[Training Epoch 1] Batch 258, Loss 0.3137829601764679\n",
      "[Training Epoch 1] Batch 259, Loss 0.3481665253639221\n",
      "[Training Epoch 1] Batch 260, Loss 0.294405996799469\n",
      "[Training Epoch 1] Batch 261, Loss 0.30260005593299866\n",
      "[Training Epoch 1] Batch 262, Loss 0.2963826060295105\n",
      "[Training Epoch 1] Batch 263, Loss 0.2984886169433594\n",
      "[Training Epoch 1] Batch 264, Loss 0.28814125061035156\n",
      "[Training Epoch 1] Batch 265, Loss 0.3105393052101135\n",
      "[Training Epoch 1] Batch 266, Loss 0.29630225896835327\n",
      "[Training Epoch 1] Batch 267, Loss 0.2760612368583679\n",
      "[Training Epoch 1] Batch 268, Loss 0.2964155077934265\n",
      "[Training Epoch 1] Batch 269, Loss 0.31565940380096436\n",
      "[Training Epoch 1] Batch 270, Loss 0.29755327105522156\n",
      "[Training Epoch 1] Batch 271, Loss 0.3025175631046295\n",
      "[Training Epoch 1] Batch 272, Loss 0.2959234118461609\n",
      "[Training Epoch 1] Batch 273, Loss 0.302781343460083\n",
      "[Training Epoch 1] Batch 274, Loss 0.2928994297981262\n",
      "[Training Epoch 1] Batch 275, Loss 0.3061743974685669\n",
      "[Training Epoch 1] Batch 276, Loss 0.29761332273483276\n",
      "[Training Epoch 1] Batch 277, Loss 0.2684568166732788\n",
      "[Training Epoch 1] Batch 278, Loss 0.3267327547073364\n",
      "[Training Epoch 1] Batch 279, Loss 0.3136891722679138\n",
      "[Training Epoch 1] Batch 280, Loss 0.2842910885810852\n",
      "[Training Epoch 1] Batch 281, Loss 0.303649365901947\n",
      "[Training Epoch 1] Batch 282, Loss 0.3169037401676178\n",
      "[Training Epoch 1] Batch 283, Loss 0.3098040819168091\n",
      "[Training Epoch 1] Batch 284, Loss 0.28968164324760437\n",
      "[Training Epoch 1] Batch 285, Loss 0.30329203605651855\n",
      "[Training Epoch 1] Batch 286, Loss 0.3029002547264099\n",
      "[Training Epoch 1] Batch 287, Loss 0.28514352440834045\n",
      "[Training Epoch 1] Batch 288, Loss 0.30168259143829346\n",
      "[Training Epoch 1] Batch 289, Loss 0.3062571883201599\n",
      "[Training Epoch 1] Batch 290, Loss 0.2774505019187927\n",
      "[Training Epoch 1] Batch 291, Loss 0.29502230882644653\n",
      "[Training Epoch 1] Batch 292, Loss 0.2875741720199585\n",
      "[Training Epoch 1] Batch 293, Loss 0.3524414300918579\n",
      "[Training Epoch 1] Batch 294, Loss 0.2552632689476013\n",
      "[Training Epoch 1] Batch 295, Loss 0.28679776191711426\n",
      "[Training Epoch 1] Batch 296, Loss 0.28813910484313965\n",
      "[Training Epoch 1] Batch 297, Loss 0.3514263331890106\n",
      "[Training Epoch 1] Batch 298, Loss 0.31207168102264404\n",
      "[Training Epoch 1] Batch 299, Loss 0.30668696761131287\n",
      "[Training Epoch 1] Batch 300, Loss 0.3103522062301636\n",
      "[Training Epoch 1] Batch 301, Loss 0.3022072911262512\n",
      "[Training Epoch 1] Batch 302, Loss 0.3085361421108246\n",
      "[Training Epoch 1] Batch 303, Loss 0.30170127749443054\n",
      "[Training Epoch 1] Batch 304, Loss 0.29692476987838745\n",
      "[Training Epoch 1] Batch 305, Loss 0.3088010549545288\n",
      "[Training Epoch 1] Batch 306, Loss 0.3053300976753235\n",
      "[Training Epoch 1] Batch 307, Loss 0.2985134720802307\n",
      "[Training Epoch 1] Batch 308, Loss 0.2943894565105438\n",
      "[Training Epoch 1] Batch 309, Loss 0.28527307510375977\n",
      "[Training Epoch 1] Batch 310, Loss 0.2823612689971924\n",
      "[Training Epoch 1] Batch 311, Loss 0.2939079999923706\n",
      "[Training Epoch 1] Batch 312, Loss 0.3183690309524536\n",
      "[Training Epoch 1] Batch 313, Loss 0.3027428686618805\n",
      "[Training Epoch 1] Batch 314, Loss 0.3026609420776367\n",
      "[Training Epoch 1] Batch 315, Loss 0.32006916403770447\n",
      "[Training Epoch 1] Batch 316, Loss 0.3274848461151123\n",
      "[Training Epoch 1] Batch 317, Loss 0.2911704480648041\n",
      "[Training Epoch 1] Batch 318, Loss 0.2997199296951294\n",
      "[Training Epoch 1] Batch 319, Loss 0.30007055401802063\n",
      "[Training Epoch 1] Batch 320, Loss 0.2941412925720215\n",
      "[Training Epoch 1] Batch 321, Loss 0.3245577812194824\n",
      "[Training Epoch 1] Batch 322, Loss 0.3266780376434326\n",
      "[Training Epoch 1] Batch 323, Loss 0.29071903228759766\n",
      "[Training Epoch 1] Batch 324, Loss 0.3099512457847595\n",
      "[Training Epoch 1] Batch 325, Loss 0.2863989472389221\n",
      "[Training Epoch 1] Batch 326, Loss 0.31141477823257446\n",
      "[Training Epoch 1] Batch 327, Loss 0.3143038749694824\n",
      "[Training Epoch 1] Batch 328, Loss 0.29589948058128357\n",
      "[Training Epoch 1] Batch 329, Loss 0.3096894919872284\n",
      "[Training Epoch 1] Batch 330, Loss 0.30227649211883545\n",
      "[Training Epoch 1] Batch 331, Loss 0.26981353759765625\n",
      "[Training Epoch 1] Batch 332, Loss 0.31401342153549194\n",
      "[Training Epoch 1] Batch 333, Loss 0.3073212504386902\n",
      "[Training Epoch 1] Batch 334, Loss 0.29316359758377075\n",
      "[Training Epoch 1] Batch 335, Loss 0.31070053577423096\n",
      "[Training Epoch 1] Batch 336, Loss 0.31935757398605347\n",
      "[Training Epoch 1] Batch 337, Loss 0.2851755917072296\n",
      "[Training Epoch 1] Batch 338, Loss 0.3015913963317871\n",
      "[Training Epoch 1] Batch 339, Loss 0.3173500597476959\n",
      "[Training Epoch 1] Batch 340, Loss 0.30837422609329224\n",
      "[Training Epoch 1] Batch 341, Loss 0.28979671001434326\n",
      "[Training Epoch 1] Batch 342, Loss 0.305208683013916\n",
      "[Training Epoch 1] Batch 343, Loss 0.29934415221214294\n",
      "[Training Epoch 1] Batch 344, Loss 0.3134537637233734\n",
      "[Training Epoch 1] Batch 345, Loss 0.3104862570762634\n",
      "[Training Epoch 1] Batch 346, Loss 0.30876874923706055\n",
      "[Training Epoch 1] Batch 347, Loss 0.30371996760368347\n",
      "[Training Epoch 1] Batch 348, Loss 0.30569207668304443\n",
      "[Training Epoch 1] Batch 349, Loss 0.3381474018096924\n",
      "[Training Epoch 1] Batch 350, Loss 0.3083517253398895\n",
      "[Training Epoch 1] Batch 351, Loss 0.2863314151763916\n",
      "[Training Epoch 1] Batch 352, Loss 0.2864118218421936\n",
      "[Training Epoch 1] Batch 353, Loss 0.29837626218795776\n",
      "[Training Epoch 1] Batch 354, Loss 0.298949271440506\n",
      "[Training Epoch 1] Batch 355, Loss 0.29591962695121765\n",
      "[Training Epoch 1] Batch 356, Loss 0.2978411614894867\n",
      "[Training Epoch 1] Batch 357, Loss 0.33345651626586914\n",
      "[Training Epoch 1] Batch 358, Loss 0.3108559548854828\n",
      "[Training Epoch 1] Batch 359, Loss 0.29146942496299744\n",
      "[Training Epoch 1] Batch 360, Loss 0.29610294103622437\n",
      "[Training Epoch 1] Batch 361, Loss 0.28687310218811035\n",
      "[Training Epoch 1] Batch 362, Loss 0.3086452782154083\n",
      "[Training Epoch 1] Batch 363, Loss 0.3064558804035187\n",
      "[Training Epoch 1] Batch 364, Loss 0.2945023775100708\n",
      "[Training Epoch 1] Batch 365, Loss 0.2875221371650696\n",
      "[Training Epoch 1] Batch 366, Loss 0.2978506088256836\n",
      "[Training Epoch 1] Batch 367, Loss 0.3010932207107544\n",
      "[Training Epoch 1] Batch 368, Loss 0.29990237951278687\n",
      "[Training Epoch 1] Batch 369, Loss 0.30722394585609436\n",
      "[Training Epoch 1] Batch 370, Loss 0.31982845067977905\n",
      "[Training Epoch 1] Batch 371, Loss 0.3135872781276703\n",
      "[Training Epoch 1] Batch 372, Loss 0.29250118136405945\n",
      "[Training Epoch 1] Batch 373, Loss 0.3231488764286041\n",
      "[Training Epoch 1] Batch 374, Loss 0.31971514225006104\n",
      "[Training Epoch 1] Batch 375, Loss 0.2761336863040924\n",
      "[Training Epoch 1] Batch 376, Loss 0.3423048257827759\n",
      "[Training Epoch 1] Batch 377, Loss 0.3094564378261566\n",
      "[Training Epoch 1] Batch 378, Loss 0.28900736570358276\n",
      "[Training Epoch 1] Batch 379, Loss 0.3088146150112152\n",
      "[Training Epoch 1] Batch 380, Loss 0.31433209776878357\n",
      "[Training Epoch 1] Batch 381, Loss 0.27719494700431824\n",
      "[Training Epoch 1] Batch 382, Loss 0.30720964074134827\n",
      "[Training Epoch 1] Batch 383, Loss 0.33501875400543213\n",
      "[Training Epoch 1] Batch 384, Loss 0.30647972226142883\n",
      "[Training Epoch 1] Batch 385, Loss 0.3019444942474365\n",
      "[Training Epoch 1] Batch 386, Loss 0.27551600337028503\n",
      "[Training Epoch 1] Batch 387, Loss 0.3115823566913605\n",
      "[Training Epoch 1] Batch 388, Loss 0.29446548223495483\n",
      "[Training Epoch 1] Batch 389, Loss 0.32946789264678955\n",
      "[Training Epoch 1] Batch 390, Loss 0.3175813555717468\n",
      "[Training Epoch 1] Batch 391, Loss 0.3101493716239929\n",
      "[Training Epoch 1] Batch 392, Loss 0.2932201623916626\n",
      "[Training Epoch 1] Batch 393, Loss 0.2970781922340393\n",
      "[Training Epoch 1] Batch 394, Loss 0.3058225214481354\n",
      "[Training Epoch 1] Batch 395, Loss 0.29645663499832153\n",
      "[Training Epoch 1] Batch 396, Loss 0.35366249084472656\n",
      "[Training Epoch 1] Batch 397, Loss 0.3160761892795563\n",
      "[Training Epoch 1] Batch 398, Loss 0.2982034683227539\n",
      "[Training Epoch 1] Batch 399, Loss 0.2857964038848877\n",
      "[Training Epoch 1] Batch 400, Loss 0.3277132511138916\n",
      "[Training Epoch 1] Batch 401, Loss 0.2988824248313904\n",
      "[Training Epoch 1] Batch 402, Loss 0.3573790192604065\n",
      "[Training Epoch 1] Batch 403, Loss 0.31205135583877563\n",
      "[Training Epoch 1] Batch 404, Loss 0.294438898563385\n",
      "[Training Epoch 1] Batch 405, Loss 0.30670881271362305\n",
      "[Training Epoch 1] Batch 406, Loss 0.31237179040908813\n",
      "[Training Epoch 1] Batch 407, Loss 0.32232344150543213\n",
      "[Training Epoch 1] Batch 408, Loss 0.30253541469573975\n",
      "[Training Epoch 1] Batch 409, Loss 0.31056246161460876\n",
      "[Training Epoch 1] Batch 410, Loss 0.3338068127632141\n",
      "[Training Epoch 1] Batch 411, Loss 0.2887647747993469\n",
      "[Training Epoch 1] Batch 412, Loss 0.28490152955055237\n",
      "[Training Epoch 1] Batch 413, Loss 0.28140246868133545\n",
      "[Training Epoch 1] Batch 414, Loss 0.32560640573501587\n",
      "[Training Epoch 1] Batch 415, Loss 0.288759708404541\n",
      "[Training Epoch 1] Batch 416, Loss 0.3071785867214203\n",
      "[Training Epoch 1] Batch 417, Loss 0.31089091300964355\n",
      "[Training Epoch 1] Batch 418, Loss 0.3007696270942688\n",
      "[Training Epoch 1] Batch 419, Loss 0.31096217036247253\n",
      "[Training Epoch 1] Batch 420, Loss 0.33163982629776\n",
      "[Training Epoch 1] Batch 421, Loss 0.30541515350341797\n",
      "[Training Epoch 1] Batch 422, Loss 0.31431031227111816\n",
      "[Training Epoch 1] Batch 423, Loss 0.2791440784931183\n",
      "[Training Epoch 1] Batch 424, Loss 0.31985610723495483\n",
      "[Training Epoch 1] Batch 425, Loss 0.2912483513355255\n",
      "[Training Epoch 1] Batch 426, Loss 0.2829500138759613\n",
      "[Training Epoch 1] Batch 427, Loss 0.2744157314300537\n",
      "[Training Epoch 1] Batch 428, Loss 0.2935715317726135\n",
      "[Training Epoch 1] Batch 429, Loss 0.3397928476333618\n",
      "[Training Epoch 1] Batch 430, Loss 0.32042282819747925\n",
      "[Training Epoch 1] Batch 431, Loss 0.28975003957748413\n",
      "[Training Epoch 1] Batch 432, Loss 0.2957151234149933\n",
      "[Training Epoch 1] Batch 433, Loss 0.3203519582748413\n",
      "[Training Epoch 1] Batch 434, Loss 0.29737281799316406\n",
      "[Training Epoch 1] Batch 435, Loss 0.29519885778427124\n",
      "[Training Epoch 1] Batch 436, Loss 0.317859947681427\n",
      "[Training Epoch 1] Batch 437, Loss 0.2998020052909851\n",
      "[Training Epoch 1] Batch 438, Loss 0.280330091714859\n",
      "[Training Epoch 1] Batch 439, Loss 0.3265407681465149\n",
      "[Training Epoch 1] Batch 440, Loss 0.30526795983314514\n",
      "[Training Epoch 1] Batch 441, Loss 0.2928139567375183\n",
      "[Training Epoch 1] Batch 442, Loss 0.2894260883331299\n",
      "[Training Epoch 1] Batch 443, Loss 0.28490325808525085\n",
      "[Training Epoch 1] Batch 444, Loss 0.29700005054473877\n",
      "[Training Epoch 1] Batch 445, Loss 0.2900420129299164\n",
      "[Training Epoch 1] Batch 446, Loss 0.32519271969795227\n",
      "[Training Epoch 1] Batch 447, Loss 0.2911054491996765\n",
      "[Training Epoch 1] Batch 448, Loss 0.30125439167022705\n",
      "[Training Epoch 1] Batch 449, Loss 0.33956238627433777\n",
      "[Training Epoch 1] Batch 450, Loss 0.2970885634422302\n",
      "[Training Epoch 1] Batch 451, Loss 0.31014445424079895\n",
      "[Training Epoch 1] Batch 452, Loss 0.2925201654434204\n",
      "[Training Epoch 1] Batch 453, Loss 0.290272057056427\n",
      "[Training Epoch 1] Batch 454, Loss 0.33104729652404785\n",
      "[Training Epoch 1] Batch 455, Loss 0.30067431926727295\n",
      "[Training Epoch 1] Batch 456, Loss 0.2916218340396881\n",
      "[Training Epoch 1] Batch 457, Loss 0.3037544786930084\n",
      "[Training Epoch 1] Batch 458, Loss 0.2983245849609375\n",
      "[Training Epoch 1] Batch 459, Loss 0.3123476207256317\n",
      "[Training Epoch 1] Batch 460, Loss 0.29275935888290405\n",
      "[Training Epoch 1] Batch 461, Loss 0.31716740131378174\n",
      "[Training Epoch 1] Batch 462, Loss 0.31135374307632446\n",
      "[Training Epoch 1] Batch 463, Loss 0.2645513713359833\n",
      "[Training Epoch 1] Batch 464, Loss 0.2864975929260254\n",
      "[Training Epoch 1] Batch 465, Loss 0.31576621532440186\n",
      "[Training Epoch 1] Batch 466, Loss 0.300927996635437\n",
      "[Training Epoch 1] Batch 467, Loss 0.29306426644325256\n",
      "[Training Epoch 1] Batch 468, Loss 0.31059956550598145\n",
      "[Training Epoch 1] Batch 469, Loss 0.2981371581554413\n",
      "[Training Epoch 1] Batch 470, Loss 0.2755758464336395\n",
      "[Training Epoch 1] Batch 471, Loss 0.29251644015312195\n",
      "[Training Epoch 1] Batch 472, Loss 0.27750927209854126\n",
      "[Training Epoch 1] Batch 473, Loss 0.30072706937789917\n",
      "[Training Epoch 1] Batch 474, Loss 0.2580849528312683\n",
      "[Training Epoch 1] Batch 475, Loss 0.28099343180656433\n",
      "[Training Epoch 1] Batch 476, Loss 0.29605746269226074\n",
      "[Training Epoch 1] Batch 477, Loss 0.3305639326572418\n",
      "[Training Epoch 1] Batch 478, Loss 0.2900635004043579\n",
      "[Training Epoch 1] Batch 479, Loss 0.3190886378288269\n",
      "[Training Epoch 1] Batch 480, Loss 0.29647523164749146\n",
      "[Training Epoch 1] Batch 481, Loss 0.2952943444252014\n",
      "[Training Epoch 1] Batch 482, Loss 0.26984527707099915\n",
      "[Training Epoch 1] Batch 483, Loss 0.3275552988052368\n",
      "[Training Epoch 1] Batch 484, Loss 0.29344815015792847\n",
      "[Training Epoch 1] Batch 485, Loss 0.29457685351371765\n",
      "[Training Epoch 1] Batch 486, Loss 0.2886396646499634\n",
      "[Training Epoch 1] Batch 487, Loss 0.2827792167663574\n",
      "[Training Epoch 1] Batch 488, Loss 0.28507310152053833\n",
      "[Training Epoch 1] Batch 489, Loss 0.3159002661705017\n",
      "[Training Epoch 1] Batch 490, Loss 0.3175320029258728\n",
      "[Training Epoch 1] Batch 491, Loss 0.28812023997306824\n",
      "[Training Epoch 1] Batch 492, Loss 0.32711905241012573\n",
      "[Training Epoch 1] Batch 493, Loss 0.2812269628047943\n",
      "[Training Epoch 1] Batch 494, Loss 0.3159952759742737\n",
      "[Training Epoch 1] Batch 495, Loss 0.2925259470939636\n",
      "[Training Epoch 1] Batch 496, Loss 0.30980390310287476\n",
      "[Training Epoch 1] Batch 497, Loss 0.29846152663230896\n",
      "[Training Epoch 1] Batch 498, Loss 0.29010292887687683\n",
      "[Training Epoch 1] Batch 499, Loss 0.2995585501194\n",
      "[Training Epoch 1] Batch 500, Loss 0.2795179486274719\n",
      "[Training Epoch 1] Batch 501, Loss 0.29840362071990967\n",
      "[Training Epoch 1] Batch 502, Loss 0.3069056272506714\n",
      "[Training Epoch 1] Batch 503, Loss 0.3321545124053955\n",
      "[Training Epoch 1] Batch 504, Loss 0.31343963742256165\n",
      "[Training Epoch 1] Batch 505, Loss 0.29925698041915894\n",
      "[Training Epoch 1] Batch 506, Loss 0.29053062200546265\n",
      "[Training Epoch 1] Batch 507, Loss 0.30774205923080444\n",
      "[Training Epoch 1] Batch 508, Loss 0.3144477605819702\n",
      "[Training Epoch 1] Batch 509, Loss 0.289547324180603\n",
      "[Training Epoch 1] Batch 510, Loss 0.31033024191856384\n",
      "[Training Epoch 1] Batch 511, Loss 0.3311516046524048\n",
      "[Training Epoch 1] Batch 512, Loss 0.30441027879714966\n",
      "[Training Epoch 1] Batch 513, Loss 0.3091849982738495\n",
      "[Training Epoch 1] Batch 514, Loss 0.3107551336288452\n",
      "[Training Epoch 1] Batch 515, Loss 0.30473992228507996\n",
      "[Training Epoch 1] Batch 516, Loss 0.34215599298477173\n",
      "[Training Epoch 1] Batch 517, Loss 0.34555602073669434\n",
      "[Training Epoch 1] Batch 518, Loss 0.29613086581230164\n",
      "[Training Epoch 1] Batch 519, Loss 0.3252878487110138\n",
      "[Training Epoch 1] Batch 520, Loss 0.3339495062828064\n",
      "[Training Epoch 1] Batch 521, Loss 0.28166985511779785\n",
      "[Training Epoch 1] Batch 522, Loss 0.2816463112831116\n",
      "[Training Epoch 1] Batch 523, Loss 0.2984866499900818\n",
      "[Training Epoch 1] Batch 524, Loss 0.28208741545677185\n",
      "[Training Epoch 1] Batch 525, Loss 0.29226282238960266\n",
      "[Training Epoch 1] Batch 526, Loss 0.2926241159439087\n",
      "[Training Epoch 1] Batch 527, Loss 0.3010631203651428\n",
      "[Training Epoch 1] Batch 528, Loss 0.29750025272369385\n",
      "[Training Epoch 1] Batch 529, Loss 0.29339736700057983\n",
      "[Training Epoch 1] Batch 530, Loss 0.3166079521179199\n",
      "[Training Epoch 1] Batch 531, Loss 0.310271680355072\n",
      "[Training Epoch 1] Batch 532, Loss 0.3047603964805603\n",
      "[Training Epoch 1] Batch 533, Loss 0.2800690829753876\n",
      "[Training Epoch 1] Batch 534, Loss 0.3193250298500061\n",
      "[Training Epoch 1] Batch 535, Loss 0.3180237412452698\n",
      "[Training Epoch 1] Batch 536, Loss 0.3106151223182678\n",
      "[Training Epoch 1] Batch 537, Loss 0.2958749830722809\n",
      "[Training Epoch 1] Batch 538, Loss 0.288594126701355\n",
      "[Training Epoch 1] Batch 539, Loss 0.31027403473854065\n",
      "[Training Epoch 1] Batch 540, Loss 0.26104846596717834\n",
      "[Training Epoch 1] Batch 541, Loss 0.3335946202278137\n",
      "[Training Epoch 1] Batch 542, Loss 0.30070075392723083\n",
      "[Training Epoch 1] Batch 543, Loss 0.2927795648574829\n",
      "[Training Epoch 1] Batch 544, Loss 0.2898353338241577\n",
      "[Training Epoch 1] Batch 545, Loss 0.34514257311820984\n",
      "[Training Epoch 1] Batch 546, Loss 0.31492358446121216\n",
      "[Training Epoch 1] Batch 547, Loss 0.31647318601608276\n",
      "[Training Epoch 1] Batch 548, Loss 0.2618870735168457\n",
      "[Training Epoch 1] Batch 549, Loss 0.2926965355873108\n",
      "[Training Epoch 1] Batch 550, Loss 0.29070883989334106\n",
      "[Training Epoch 1] Batch 551, Loss 0.3177042603492737\n",
      "[Training Epoch 1] Batch 552, Loss 0.3219415545463562\n",
      "[Training Epoch 1] Batch 553, Loss 0.3318568170070648\n",
      "[Training Epoch 1] Batch 554, Loss 0.2871410846710205\n",
      "[Training Epoch 1] Batch 555, Loss 0.2782237231731415\n",
      "[Training Epoch 1] Batch 556, Loss 0.3217168152332306\n",
      "[Training Epoch 1] Batch 557, Loss 0.2808956503868103\n",
      "[Training Epoch 1] Batch 558, Loss 0.28157252073287964\n",
      "[Training Epoch 1] Batch 559, Loss 0.2859255373477936\n",
      "[Training Epoch 1] Batch 560, Loss 0.3028402328491211\n",
      "[Training Epoch 1] Batch 561, Loss 0.27960699796676636\n",
      "[Training Epoch 1] Batch 562, Loss 0.29247820377349854\n",
      "[Training Epoch 1] Batch 563, Loss 0.29047757387161255\n",
      "[Training Epoch 1] Batch 564, Loss 0.32392534613609314\n",
      "[Training Epoch 1] Batch 565, Loss 0.31897270679473877\n",
      "[Training Epoch 1] Batch 566, Loss 0.2965511679649353\n",
      "[Training Epoch 1] Batch 567, Loss 0.3084477186203003\n",
      "[Training Epoch 1] Batch 568, Loss 0.3113977313041687\n",
      "[Training Epoch 1] Batch 569, Loss 0.29479217529296875\n",
      "[Training Epoch 1] Batch 570, Loss 0.2963966727256775\n",
      "[Training Epoch 1] Batch 571, Loss 0.312319815158844\n",
      "[Training Epoch 1] Batch 572, Loss 0.2903822064399719\n",
      "[Training Epoch 1] Batch 573, Loss 0.3152947723865509\n",
      "[Training Epoch 1] Batch 574, Loss 0.2813694179058075\n",
      "[Training Epoch 1] Batch 575, Loss 0.2943049967288971\n",
      "[Training Epoch 1] Batch 576, Loss 0.3286328911781311\n",
      "[Training Epoch 1] Batch 577, Loss 0.26412534713745117\n",
      "[Training Epoch 1] Batch 578, Loss 0.3209424614906311\n",
      "[Training Epoch 1] Batch 579, Loss 0.2681136131286621\n",
      "[Training Epoch 1] Batch 580, Loss 0.3085227310657501\n",
      "[Training Epoch 1] Batch 581, Loss 0.3250465989112854\n",
      "[Training Epoch 1] Batch 582, Loss 0.3260871171951294\n",
      "[Training Epoch 1] Batch 583, Loss 0.31367844343185425\n",
      "[Training Epoch 1] Batch 584, Loss 0.3168795108795166\n",
      "[Training Epoch 1] Batch 585, Loss 0.28668272495269775\n",
      "[Training Epoch 1] Batch 586, Loss 0.30088871717453003\n",
      "[Training Epoch 1] Batch 587, Loss 0.31379854679107666\n",
      "[Training Epoch 1] Batch 588, Loss 0.31478261947631836\n",
      "[Training Epoch 1] Batch 589, Loss 0.29945600032806396\n",
      "[Training Epoch 1] Batch 590, Loss 0.2953219711780548\n",
      "[Training Epoch 1] Batch 591, Loss 0.26625722646713257\n",
      "[Training Epoch 1] Batch 592, Loss 0.3185007572174072\n",
      "[Training Epoch 1] Batch 593, Loss 0.28942131996154785\n",
      "[Training Epoch 1] Batch 594, Loss 0.282999187707901\n",
      "[Training Epoch 1] Batch 595, Loss 0.3001016080379486\n",
      "[Training Epoch 1] Batch 596, Loss 0.33163100481033325\n",
      "[Training Epoch 1] Batch 597, Loss 0.3176044225692749\n",
      "[Training Epoch 1] Batch 598, Loss 0.3140363097190857\n",
      "[Training Epoch 1] Batch 599, Loss 0.3059919774532318\n",
      "[Training Epoch 1] Batch 600, Loss 0.28705066442489624\n",
      "[Training Epoch 1] Batch 601, Loss 0.3412490487098694\n",
      "[Training Epoch 1] Batch 602, Loss 0.29369375109672546\n",
      "[Training Epoch 1] Batch 603, Loss 0.3238375186920166\n",
      "[Training Epoch 1] Batch 604, Loss 0.28949642181396484\n",
      "[Training Epoch 1] Batch 605, Loss 0.30841130018234253\n",
      "[Training Epoch 1] Batch 606, Loss 0.2945977747440338\n",
      "[Training Epoch 1] Batch 607, Loss 0.30574631690979004\n",
      "[Training Epoch 1] Batch 608, Loss 0.3028256893157959\n",
      "[Training Epoch 1] Batch 609, Loss 0.29049214720726013\n",
      "[Training Epoch 1] Batch 610, Loss 0.2815040647983551\n",
      "[Training Epoch 1] Batch 611, Loss 0.3089168071746826\n",
      "[Training Epoch 1] Batch 612, Loss 0.29551416635513306\n",
      "[Training Epoch 1] Batch 613, Loss 0.26760637760162354\n",
      "[Training Epoch 1] Batch 614, Loss 0.288504958152771\n",
      "[Training Epoch 1] Batch 615, Loss 0.31700316071510315\n",
      "[Training Epoch 1] Batch 616, Loss 0.2871869206428528\n",
      "[Training Epoch 1] Batch 617, Loss 0.2841939628124237\n",
      "[Training Epoch 1] Batch 618, Loss 0.29033344984054565\n",
      "[Training Epoch 1] Batch 619, Loss 0.3109024167060852\n",
      "[Training Epoch 1] Batch 620, Loss 0.31208521127700806\n",
      "[Training Epoch 1] Batch 621, Loss 0.2977864444255829\n",
      "[Training Epoch 1] Batch 622, Loss 0.30302754044532776\n",
      "[Training Epoch 1] Batch 623, Loss 0.30211836099624634\n",
      "[Training Epoch 1] Batch 624, Loss 0.3245164155960083\n",
      "[Training Epoch 1] Batch 625, Loss 0.27514782547950745\n",
      "[Training Epoch 1] Batch 626, Loss 0.2778942584991455\n",
      "[Training Epoch 1] Batch 627, Loss 0.3037037253379822\n",
      "[Training Epoch 1] Batch 628, Loss 0.3039094805717468\n",
      "[Training Epoch 1] Batch 629, Loss 0.30510544776916504\n",
      "[Training Epoch 1] Batch 630, Loss 0.3130643367767334\n",
      "[Training Epoch 1] Batch 631, Loss 0.32806891202926636\n",
      "[Training Epoch 1] Batch 632, Loss 0.3188362717628479\n",
      "[Training Epoch 1] Batch 633, Loss 0.27696362137794495\n",
      "[Training Epoch 1] Batch 634, Loss 0.34050777554512024\n",
      "[Training Epoch 1] Batch 635, Loss 0.29081296920776367\n",
      "[Training Epoch 1] Batch 636, Loss 0.2994900345802307\n",
      "[Training Epoch 1] Batch 637, Loss 0.2947675287723541\n",
      "[Training Epoch 1] Batch 638, Loss 0.28521600365638733\n",
      "[Training Epoch 1] Batch 639, Loss 0.29249107837677\n",
      "[Training Epoch 1] Batch 640, Loss 0.27569475769996643\n",
      "[Training Epoch 1] Batch 641, Loss 0.31805381178855896\n",
      "[Training Epoch 1] Batch 642, Loss 0.3303665518760681\n",
      "[Training Epoch 1] Batch 643, Loss 0.30571097135543823\n",
      "[Training Epoch 1] Batch 644, Loss 0.30253440141677856\n",
      "[Training Epoch 1] Batch 645, Loss 0.3105579912662506\n",
      "[Training Epoch 1] Batch 646, Loss 0.28708386421203613\n",
      "[Training Epoch 1] Batch 647, Loss 0.3322202265262604\n",
      "[Training Epoch 1] Batch 648, Loss 0.2692522704601288\n",
      "[Training Epoch 1] Batch 649, Loss 0.299363911151886\n",
      "[Training Epoch 1] Batch 650, Loss 0.305386483669281\n",
      "[Training Epoch 1] Batch 651, Loss 0.308540016412735\n",
      "[Training Epoch 1] Batch 652, Loss 0.28714799880981445\n",
      "[Training Epoch 1] Batch 653, Loss 0.29712405800819397\n",
      "[Training Epoch 1] Batch 654, Loss 0.26545625925064087\n",
      "[Training Epoch 1] Batch 655, Loss 0.28523215651512146\n",
      "[Training Epoch 1] Batch 656, Loss 0.3217014670372009\n",
      "[Training Epoch 1] Batch 657, Loss 0.3264860212802887\n",
      "[Training Epoch 1] Batch 658, Loss 0.266554594039917\n",
      "[Training Epoch 1] Batch 659, Loss 0.3037757873535156\n",
      "[Training Epoch 1] Batch 660, Loss 0.2916511297225952\n",
      "[Training Epoch 1] Batch 661, Loss 0.3125936985015869\n",
      "[Training Epoch 1] Batch 662, Loss 0.32659855484962463\n",
      "[Training Epoch 1] Batch 663, Loss 0.28983116149902344\n",
      "[Training Epoch 1] Batch 664, Loss 0.29226523637771606\n",
      "[Training Epoch 1] Batch 665, Loss 0.30160048604011536\n",
      "[Training Epoch 1] Batch 666, Loss 0.32330459356307983\n",
      "[Training Epoch 1] Batch 667, Loss 0.31873685121536255\n",
      "[Training Epoch 1] Batch 668, Loss 0.29591313004493713\n",
      "[Training Epoch 1] Batch 669, Loss 0.32328319549560547\n",
      "[Training Epoch 1] Batch 670, Loss 0.3188900947570801\n",
      "[Training Epoch 1] Batch 671, Loss 0.2844442129135132\n",
      "[Training Epoch 1] Batch 672, Loss 0.30251359939575195\n",
      "[Training Epoch 1] Batch 673, Loss 0.30043402314186096\n",
      "[Training Epoch 1] Batch 674, Loss 0.2831988036632538\n",
      "[Training Epoch 1] Batch 675, Loss 0.29557400941848755\n",
      "[Training Epoch 1] Batch 676, Loss 0.29885852336883545\n",
      "[Training Epoch 1] Batch 677, Loss 0.2890380620956421\n",
      "[Training Epoch 1] Batch 678, Loss 0.3155219852924347\n",
      "[Training Epoch 1] Batch 679, Loss 0.325522780418396\n",
      "[Training Epoch 1] Batch 680, Loss 0.31512877345085144\n",
      "[Training Epoch 1] Batch 681, Loss 0.3347330689430237\n",
      "[Training Epoch 1] Batch 682, Loss 0.28923386335372925\n",
      "[Training Epoch 1] Batch 683, Loss 0.3024110794067383\n",
      "[Training Epoch 1] Batch 684, Loss 0.325697124004364\n",
      "[Training Epoch 1] Batch 685, Loss 0.3134986162185669\n",
      "[Training Epoch 1] Batch 686, Loss 0.30124741792678833\n",
      "[Training Epoch 1] Batch 687, Loss 0.28219175338745117\n",
      "[Training Epoch 1] Batch 688, Loss 0.29536572098731995\n",
      "[Training Epoch 1] Batch 689, Loss 0.2875380516052246\n",
      "[Training Epoch 1] Batch 690, Loss 0.2963539958000183\n",
      "[Training Epoch 1] Batch 691, Loss 0.30347949266433716\n",
      "[Training Epoch 1] Batch 692, Loss 0.29921895265579224\n",
      "[Training Epoch 1] Batch 693, Loss 0.29619860649108887\n",
      "[Training Epoch 1] Batch 694, Loss 0.29556891322135925\n",
      "[Training Epoch 1] Batch 695, Loss 0.3278457224369049\n",
      "[Training Epoch 1] Batch 696, Loss 0.3109146058559418\n",
      "[Training Epoch 1] Batch 697, Loss 0.3159668445587158\n",
      "[Training Epoch 1] Batch 698, Loss 0.2863989770412445\n",
      "[Training Epoch 1] Batch 699, Loss 0.28293636441230774\n",
      "[Training Epoch 1] Batch 700, Loss 0.32590606808662415\n",
      "[Training Epoch 1] Batch 701, Loss 0.29304802417755127\n",
      "[Training Epoch 1] Batch 702, Loss 0.3280598521232605\n",
      "[Training Epoch 1] Batch 703, Loss 0.29089730978012085\n",
      "[Training Epoch 1] Batch 704, Loss 0.2761814594268799\n",
      "[Training Epoch 1] Batch 705, Loss 0.286072701215744\n",
      "[Training Epoch 1] Batch 706, Loss 0.31568437814712524\n",
      "[Training Epoch 1] Batch 707, Loss 0.30128031969070435\n",
      "[Training Epoch 1] Batch 708, Loss 0.280295729637146\n",
      "[Training Epoch 1] Batch 709, Loss 0.30333471298217773\n",
      "[Training Epoch 1] Batch 710, Loss 0.2904300391674042\n",
      "[Training Epoch 1] Batch 711, Loss 0.3206273019313812\n",
      "[Training Epoch 1] Batch 712, Loss 0.3341710865497589\n",
      "[Training Epoch 1] Batch 713, Loss 0.31601470708847046\n",
      "[Training Epoch 1] Batch 714, Loss 0.30886805057525635\n",
      "[Training Epoch 1] Batch 715, Loss 0.27864181995391846\n",
      "[Training Epoch 1] Batch 716, Loss 0.3419169485569\n",
      "[Training Epoch 1] Batch 717, Loss 0.26248836517333984\n",
      "[Training Epoch 1] Batch 718, Loss 0.3013576865196228\n",
      "[Training Epoch 1] Batch 719, Loss 0.2880937159061432\n",
      "[Training Epoch 1] Batch 720, Loss 0.31160351634025574\n",
      "[Training Epoch 1] Batch 721, Loss 0.3335282802581787\n",
      "[Training Epoch 1] Batch 722, Loss 0.2800649106502533\n",
      "[Training Epoch 1] Batch 723, Loss 0.31920650601387024\n",
      "[Training Epoch 1] Batch 724, Loss 0.2746322453022003\n",
      "[Training Epoch 1] Batch 725, Loss 0.3109973669052124\n",
      "[Training Epoch 1] Batch 726, Loss 0.25340914726257324\n",
      "[Training Epoch 1] Batch 727, Loss 0.31827759742736816\n",
      "[Training Epoch 1] Batch 728, Loss 0.29135310649871826\n",
      "[Training Epoch 1] Batch 729, Loss 0.2964841425418854\n",
      "[Training Epoch 1] Batch 730, Loss 0.2971939146518707\n",
      "[Training Epoch 1] Batch 731, Loss 0.3183656334877014\n",
      "[Training Epoch 1] Batch 732, Loss 0.3089279532432556\n",
      "[Training Epoch 1] Batch 733, Loss 0.2913056015968323\n",
      "[Training Epoch 1] Batch 734, Loss 0.31361764669418335\n",
      "[Training Epoch 1] Batch 735, Loss 0.2854582369327545\n",
      "[Training Epoch 1] Batch 736, Loss 0.2898220419883728\n",
      "[Training Epoch 1] Batch 737, Loss 0.2709555923938751\n",
      "[Training Epoch 1] Batch 738, Loss 0.28665459156036377\n",
      "[Training Epoch 1] Batch 739, Loss 0.291012167930603\n",
      "[Training Epoch 1] Batch 740, Loss 0.30519622564315796\n",
      "[Training Epoch 1] Batch 741, Loss 0.29842203855514526\n",
      "[Training Epoch 1] Batch 742, Loss 0.312448114156723\n",
      "[Training Epoch 1] Batch 743, Loss 0.2902597188949585\n",
      "[Training Epoch 1] Batch 744, Loss 0.2646583318710327\n",
      "[Training Epoch 1] Batch 745, Loss 0.3023677468299866\n",
      "[Training Epoch 1] Batch 746, Loss 0.294815331697464\n",
      "[Training Epoch 1] Batch 747, Loss 0.28486764430999756\n",
      "[Training Epoch 1] Batch 748, Loss 0.3051915168762207\n",
      "[Training Epoch 1] Batch 749, Loss 0.3154942989349365\n",
      "[Training Epoch 1] Batch 750, Loss 0.2810945510864258\n",
      "[Training Epoch 1] Batch 751, Loss 0.29114267230033875\n",
      "[Training Epoch 1] Batch 752, Loss 0.2989407479763031\n",
      "[Training Epoch 1] Batch 753, Loss 0.3007964491844177\n",
      "[Training Epoch 1] Batch 754, Loss 0.29961445927619934\n",
      "[Training Epoch 1] Batch 755, Loss 0.2578223645687103\n",
      "[Training Epoch 1] Batch 756, Loss 0.3071180284023285\n",
      "[Training Epoch 1] Batch 757, Loss 0.2864301800727844\n",
      "[Training Epoch 1] Batch 758, Loss 0.3074381351470947\n",
      "[Training Epoch 1] Batch 759, Loss 0.3249136805534363\n",
      "[Training Epoch 1] Batch 760, Loss 0.3209110498428345\n",
      "[Training Epoch 1] Batch 761, Loss 0.301746666431427\n",
      "[Training Epoch 1] Batch 762, Loss 0.31243109703063965\n",
      "[Training Epoch 1] Batch 763, Loss 0.3085797429084778\n",
      "[Training Epoch 1] Batch 764, Loss 0.3108590245246887\n",
      "[Training Epoch 1] Batch 765, Loss 0.29433169960975647\n",
      "[Training Epoch 1] Batch 766, Loss 0.3053829073905945\n",
      "[Training Epoch 1] Batch 767, Loss 0.2722744345664978\n",
      "[Training Epoch 1] Batch 768, Loss 0.3137034773826599\n",
      "[Training Epoch 1] Batch 769, Loss 0.30777645111083984\n",
      "[Training Epoch 1] Batch 770, Loss 0.3030780553817749\n",
      "[Training Epoch 1] Batch 771, Loss 0.30166828632354736\n",
      "[Training Epoch 1] Batch 772, Loss 0.304490327835083\n",
      "[Training Epoch 1] Batch 773, Loss 0.30275067687034607\n",
      "[Training Epoch 1] Batch 774, Loss 0.28531765937805176\n",
      "[Training Epoch 1] Batch 775, Loss 0.3176632225513458\n",
      "[Training Epoch 1] Batch 776, Loss 0.29157882928848267\n",
      "[Training Epoch 1] Batch 777, Loss 0.31973251700401306\n",
      "[Training Epoch 1] Batch 778, Loss 0.29612722992897034\n",
      "[Training Epoch 1] Batch 779, Loss 0.30985283851623535\n",
      "[Training Epoch 1] Batch 780, Loss 0.29136350750923157\n",
      "[Training Epoch 1] Batch 781, Loss 0.2850942015647888\n",
      "[Training Epoch 1] Batch 782, Loss 0.31750917434692383\n",
      "[Training Epoch 1] Batch 783, Loss 0.26347818970680237\n",
      "[Training Epoch 1] Batch 784, Loss 0.3179088234901428\n",
      "[Training Epoch 1] Batch 785, Loss 0.29815173149108887\n",
      "[Training Epoch 1] Batch 786, Loss 0.2946118414402008\n",
      "[Training Epoch 1] Batch 787, Loss 0.3051720857620239\n",
      "[Training Epoch 1] Batch 788, Loss 0.2969456911087036\n",
      "[Training Epoch 1] Batch 789, Loss 0.2854495644569397\n",
      "[Training Epoch 1] Batch 790, Loss 0.3166695237159729\n",
      "[Training Epoch 1] Batch 791, Loss 0.3142942786216736\n",
      "[Training Epoch 1] Batch 792, Loss 0.3321250081062317\n",
      "[Training Epoch 1] Batch 793, Loss 0.2961779832839966\n",
      "[Training Epoch 1] Batch 794, Loss 0.3066381812095642\n",
      "[Training Epoch 1] Batch 795, Loss 0.3111870586872101\n",
      "[Training Epoch 1] Batch 796, Loss 0.28300634026527405\n",
      "[Training Epoch 1] Batch 797, Loss 0.2934780716896057\n",
      "[Training Epoch 1] Batch 798, Loss 0.351750910282135\n",
      "[Training Epoch 1] Batch 799, Loss 0.30186352133750916\n",
      "[Training Epoch 1] Batch 800, Loss 0.2729478180408478\n",
      "[Training Epoch 1] Batch 801, Loss 0.2979069948196411\n",
      "[Training Epoch 1] Batch 802, Loss 0.2918521463871002\n",
      "[Training Epoch 1] Batch 803, Loss 0.3091137111186981\n",
      "[Training Epoch 1] Batch 804, Loss 0.2978493273258209\n",
      "[Training Epoch 1] Batch 805, Loss 0.29706287384033203\n",
      "[Training Epoch 1] Batch 806, Loss 0.32416072487831116\n",
      "[Training Epoch 1] Batch 807, Loss 0.28605586290359497\n",
      "[Training Epoch 1] Batch 808, Loss 0.3500096797943115\n",
      "[Training Epoch 1] Batch 809, Loss 0.3267957866191864\n",
      "[Training Epoch 1] Batch 810, Loss 0.31493139266967773\n",
      "[Training Epoch 1] Batch 811, Loss 0.2981380820274353\n",
      "[Training Epoch 1] Batch 812, Loss 0.30329930782318115\n",
      "[Training Epoch 1] Batch 813, Loss 0.2951529026031494\n",
      "[Training Epoch 1] Batch 814, Loss 0.2789625823497772\n",
      "[Training Epoch 1] Batch 815, Loss 0.298686683177948\n",
      "[Training Epoch 1] Batch 816, Loss 0.30844056606292725\n",
      "[Training Epoch 1] Batch 817, Loss 0.311585932970047\n",
      "[Training Epoch 1] Batch 818, Loss 0.2907871901988983\n",
      "[Training Epoch 1] Batch 819, Loss 0.28210216760635376\n",
      "[Training Epoch 1] Batch 820, Loss 0.2640761137008667\n",
      "[Training Epoch 1] Batch 821, Loss 0.2910599410533905\n",
      "[Training Epoch 1] Batch 822, Loss 0.2958559989929199\n",
      "[Training Epoch 1] Batch 823, Loss 0.28817519545555115\n",
      "[Training Epoch 1] Batch 824, Loss 0.3039591312408447\n",
      "[Training Epoch 1] Batch 825, Loss 0.2827700972557068\n",
      "[Training Epoch 1] Batch 826, Loss 0.3064543604850769\n",
      "[Training Epoch 1] Batch 827, Loss 0.2950563132762909\n",
      "[Training Epoch 1] Batch 828, Loss 0.3208009898662567\n",
      "[Training Epoch 1] Batch 829, Loss 0.28152143955230713\n",
      "[Training Epoch 1] Batch 830, Loss 0.32907629013061523\n",
      "[Training Epoch 1] Batch 831, Loss 0.2935028672218323\n",
      "[Training Epoch 1] Batch 832, Loss 0.28846633434295654\n",
      "[Training Epoch 1] Batch 833, Loss 0.287008136510849\n",
      "[Training Epoch 1] Batch 834, Loss 0.32801729440689087\n",
      "[Training Epoch 1] Batch 835, Loss 0.29667478799819946\n",
      "[Training Epoch 1] Batch 836, Loss 0.2955161929130554\n",
      "[Training Epoch 1] Batch 837, Loss 0.28696492314338684\n",
      "[Training Epoch 1] Batch 838, Loss 0.3085681200027466\n",
      "[Training Epoch 1] Batch 839, Loss 0.298215389251709\n",
      "[Training Epoch 1] Batch 840, Loss 0.3040565848350525\n",
      "[Training Epoch 1] Batch 841, Loss 0.33366093039512634\n",
      "[Training Epoch 1] Batch 842, Loss 0.3026169240474701\n",
      "[Training Epoch 1] Batch 843, Loss 0.28589189052581787\n",
      "[Training Epoch 1] Batch 844, Loss 0.27498236298561096\n",
      "[Training Epoch 1] Batch 845, Loss 0.29926082491874695\n",
      "[Training Epoch 1] Batch 846, Loss 0.3050540089607239\n",
      "[Training Epoch 1] Batch 847, Loss 0.29175490140914917\n",
      "[Training Epoch 1] Batch 848, Loss 0.3056839108467102\n",
      "[Training Epoch 1] Batch 849, Loss 0.28036975860595703\n",
      "[Training Epoch 1] Batch 850, Loss 0.2517789602279663\n",
      "[Training Epoch 1] Batch 851, Loss 0.3072011470794678\n",
      "[Training Epoch 1] Batch 852, Loss 0.27709951996803284\n",
      "[Training Epoch 1] Batch 853, Loss 0.30426961183547974\n",
      "[Training Epoch 1] Batch 854, Loss 0.31221574544906616\n",
      "[Training Epoch 1] Batch 855, Loss 0.2829993963241577\n",
      "[Training Epoch 1] Batch 856, Loss 0.2972261905670166\n",
      "[Training Epoch 1] Batch 857, Loss 0.3180202543735504\n",
      "[Training Epoch 1] Batch 858, Loss 0.31362834572792053\n",
      "[Training Epoch 1] Batch 859, Loss 0.31012821197509766\n",
      "[Training Epoch 1] Batch 860, Loss 0.313194215297699\n",
      "[Training Epoch 1] Batch 861, Loss 0.29073864221572876\n",
      "[Training Epoch 1] Batch 862, Loss 0.2996976971626282\n",
      "[Training Epoch 1] Batch 863, Loss 0.30523502826690674\n",
      "[Training Epoch 1] Batch 864, Loss 0.3279320299625397\n",
      "[Training Epoch 1] Batch 865, Loss 0.3114616572856903\n",
      "[Training Epoch 1] Batch 866, Loss 0.31084173917770386\n",
      "[Training Epoch 1] Batch 867, Loss 0.2635452449321747\n",
      "[Training Epoch 1] Batch 868, Loss 0.29185745120048523\n",
      "[Training Epoch 1] Batch 869, Loss 0.31427860260009766\n",
      "[Training Epoch 1] Batch 870, Loss 0.2743603587150574\n",
      "[Training Epoch 1] Batch 871, Loss 0.30025944113731384\n",
      "[Training Epoch 1] Batch 872, Loss 0.3001224398612976\n",
      "[Training Epoch 1] Batch 873, Loss 0.2969025671482086\n",
      "[Training Epoch 1] Batch 874, Loss 0.32247620820999146\n",
      "[Training Epoch 1] Batch 875, Loss 0.28760185837745667\n",
      "[Training Epoch 1] Batch 876, Loss 0.28967899084091187\n",
      "[Training Epoch 1] Batch 877, Loss 0.30486804246902466\n",
      "[Training Epoch 1] Batch 878, Loss 0.3168914318084717\n",
      "[Training Epoch 1] Batch 879, Loss 0.2910107970237732\n",
      "[Training Epoch 1] Batch 880, Loss 0.3144899606704712\n",
      "[Training Epoch 1] Batch 881, Loss 0.31099414825439453\n",
      "[Training Epoch 1] Batch 882, Loss 0.2906665802001953\n",
      "[Training Epoch 1] Batch 883, Loss 0.2790908217430115\n",
      "[Training Epoch 1] Batch 884, Loss 0.3105279803276062\n",
      "[Training Epoch 1] Batch 885, Loss 0.2740369439125061\n",
      "[Training Epoch 1] Batch 886, Loss 0.279962956905365\n",
      "[Training Epoch 1] Batch 887, Loss 0.30186742544174194\n",
      "[Training Epoch 1] Batch 888, Loss 0.3266493082046509\n",
      "[Training Epoch 1] Batch 889, Loss 0.28773826360702515\n",
      "[Training Epoch 1] Batch 890, Loss 0.2894420623779297\n",
      "[Training Epoch 1] Batch 891, Loss 0.2808440625667572\n",
      "[Training Epoch 1] Batch 892, Loss 0.2895983159542084\n",
      "[Training Epoch 1] Batch 893, Loss 0.2864436209201813\n",
      "[Training Epoch 1] Batch 894, Loss 0.2964189052581787\n",
      "[Training Epoch 1] Batch 895, Loss 0.3323500156402588\n",
      "[Training Epoch 1] Batch 896, Loss 0.30676722526550293\n",
      "[Training Epoch 1] Batch 897, Loss 0.31204602122306824\n",
      "[Training Epoch 1] Batch 898, Loss 0.28176259994506836\n",
      "[Training Epoch 1] Batch 899, Loss 0.34349262714385986\n",
      "[Training Epoch 1] Batch 900, Loss 0.2914024591445923\n",
      "[Training Epoch 1] Batch 901, Loss 0.3334362208843231\n",
      "[Training Epoch 1] Batch 902, Loss 0.31949958205223083\n",
      "[Training Epoch 1] Batch 903, Loss 0.31394141912460327\n",
      "[Training Epoch 1] Batch 904, Loss 0.2891589403152466\n",
      "[Training Epoch 1] Batch 905, Loss 0.2922087013721466\n",
      "[Training Epoch 1] Batch 906, Loss 0.29275888204574585\n",
      "[Training Epoch 1] Batch 907, Loss 0.2886357009410858\n",
      "[Training Epoch 1] Batch 908, Loss 0.2800045609474182\n",
      "[Training Epoch 1] Batch 909, Loss 0.31525149941444397\n",
      "[Training Epoch 1] Batch 910, Loss 0.3135102391242981\n",
      "[Training Epoch 1] Batch 911, Loss 0.3220246136188507\n",
      "[Training Epoch 1] Batch 912, Loss 0.29285603761672974\n",
      "[Training Epoch 1] Batch 913, Loss 0.30523115396499634\n",
      "[Training Epoch 1] Batch 914, Loss 0.30095601081848145\n",
      "[Training Epoch 1] Batch 915, Loss 0.2958817481994629\n",
      "[Training Epoch 1] Batch 916, Loss 0.3372763991355896\n",
      "[Training Epoch 1] Batch 917, Loss 0.27788206934928894\n",
      "[Training Epoch 1] Batch 918, Loss 0.28478914499282837\n",
      "[Training Epoch 1] Batch 919, Loss 0.3104115128517151\n",
      "[Training Epoch 1] Batch 920, Loss 0.3183669447898865\n",
      "[Training Epoch 1] Batch 921, Loss 0.27497079968452454\n",
      "[Training Epoch 1] Batch 922, Loss 0.31154772639274597\n",
      "[Training Epoch 1] Batch 923, Loss 0.2979121506214142\n",
      "[Training Epoch 1] Batch 924, Loss 0.3289639949798584\n",
      "[Training Epoch 1] Batch 925, Loss 0.29836469888687134\n",
      "[Training Epoch 1] Batch 926, Loss 0.3446888327598572\n",
      "[Training Epoch 1] Batch 927, Loss 0.34148144721984863\n",
      "[Training Epoch 1] Batch 928, Loss 0.3068190813064575\n",
      "[Training Epoch 1] Batch 929, Loss 0.2976013422012329\n",
      "[Training Epoch 1] Batch 930, Loss 0.2883891463279724\n",
      "[Training Epoch 1] Batch 931, Loss 0.30862486362457275\n",
      "[Training Epoch 1] Batch 932, Loss 0.29139867424964905\n",
      "[Training Epoch 1] Batch 933, Loss 0.31711283326148987\n",
      "[Training Epoch 1] Batch 934, Loss 0.31331804394721985\n",
      "[Training Epoch 1] Batch 935, Loss 0.29623299837112427\n",
      "[Training Epoch 1] Batch 936, Loss 0.319931298494339\n",
      "[Training Epoch 1] Batch 937, Loss 0.3367912769317627\n",
      "[Training Epoch 1] Batch 938, Loss 0.29036974906921387\n",
      "[Training Epoch 1] Batch 939, Loss 0.31692221760749817\n",
      "[Training Epoch 1] Batch 940, Loss 0.2961496114730835\n",
      "[Training Epoch 1] Batch 941, Loss 0.3136482536792755\n",
      "[Training Epoch 1] Batch 942, Loss 0.2885139584541321\n",
      "[Training Epoch 1] Batch 943, Loss 0.3231130838394165\n",
      "[Training Epoch 1] Batch 944, Loss 0.2861069440841675\n",
      "[Training Epoch 1] Batch 945, Loss 0.3021849989891052\n",
      "[Training Epoch 1] Batch 946, Loss 0.28999075293540955\n",
      "[Training Epoch 1] Batch 947, Loss 0.3099119961261749\n",
      "[Training Epoch 1] Batch 948, Loss 0.2810865342617035\n",
      "[Training Epoch 1] Batch 949, Loss 0.3021794855594635\n",
      "[Training Epoch 1] Batch 950, Loss 0.2899991273880005\n",
      "[Training Epoch 1] Batch 951, Loss 0.3071724474430084\n",
      "[Training Epoch 1] Batch 952, Loss 0.2824662923812866\n",
      "[Training Epoch 1] Batch 953, Loss 0.32143357396125793\n",
      "[Training Epoch 1] Batch 954, Loss 0.27813518047332764\n",
      "[Training Epoch 1] Batch 955, Loss 0.28517669439315796\n",
      "[Training Epoch 1] Batch 956, Loss 0.27823516726493835\n",
      "[Training Epoch 1] Batch 957, Loss 0.30264812707901\n",
      "[Training Epoch 1] Batch 958, Loss 0.2827762961387634\n",
      "[Training Epoch 1] Batch 959, Loss 0.3061142563819885\n",
      "[Training Epoch 1] Batch 960, Loss 0.3331035375595093\n",
      "[Training Epoch 1] Batch 961, Loss 0.29971784353256226\n",
      "[Training Epoch 1] Batch 962, Loss 0.2935798168182373\n",
      "[Training Epoch 1] Batch 963, Loss 0.28261440992355347\n",
      "[Training Epoch 1] Batch 964, Loss 0.2973203957080841\n",
      "[Training Epoch 1] Batch 965, Loss 0.3036037087440491\n",
      "[Training Epoch 1] Batch 966, Loss 0.29399818181991577\n",
      "[Training Epoch 1] Batch 967, Loss 0.3048518896102905\n",
      "[Training Epoch 1] Batch 968, Loss 0.28939157724380493\n",
      "[Training Epoch 1] Batch 969, Loss 0.2927591800689697\n",
      "[Training Epoch 1] Batch 970, Loss 0.28913313150405884\n",
      "[Training Epoch 1] Batch 971, Loss 0.29336822032928467\n",
      "[Training Epoch 1] Batch 972, Loss 0.29285097122192383\n",
      "[Training Epoch 1] Batch 973, Loss 0.2930825352668762\n",
      "[Training Epoch 1] Batch 974, Loss 0.2730906903743744\n",
      "[Training Epoch 1] Batch 975, Loss 0.3050649166107178\n",
      "[Training Epoch 1] Batch 976, Loss 0.3067338466644287\n",
      "[Training Epoch 1] Batch 977, Loss 0.2924553155899048\n",
      "[Training Epoch 1] Batch 978, Loss 0.2740500569343567\n",
      "[Training Epoch 1] Batch 979, Loss 0.2877470552921295\n",
      "[Training Epoch 1] Batch 980, Loss 0.2888023853302002\n",
      "[Training Epoch 1] Batch 981, Loss 0.2804614305496216\n",
      "[Training Epoch 1] Batch 982, Loss 0.29799091815948486\n",
      "[Training Epoch 1] Batch 983, Loss 0.29616159200668335\n",
      "[Training Epoch 1] Batch 984, Loss 0.2755184769630432\n",
      "[Training Epoch 1] Batch 985, Loss 0.3033737242221832\n",
      "[Training Epoch 1] Batch 986, Loss 0.2745148241519928\n",
      "[Training Epoch 1] Batch 987, Loss 0.30864280462265015\n",
      "[Training Epoch 1] Batch 988, Loss 0.3378549814224243\n",
      "[Training Epoch 1] Batch 989, Loss 0.27155226469039917\n",
      "[Training Epoch 1] Batch 990, Loss 0.32606804370880127\n",
      "[Training Epoch 1] Batch 991, Loss 0.30151939392089844\n",
      "[Training Epoch 1] Batch 992, Loss 0.2961038053035736\n",
      "[Training Epoch 1] Batch 993, Loss 0.31380218267440796\n",
      "[Training Epoch 1] Batch 994, Loss 0.30671390891075134\n",
      "[Training Epoch 1] Batch 995, Loss 0.30703338980674744\n",
      "[Training Epoch 1] Batch 996, Loss 0.2953850030899048\n",
      "[Training Epoch 1] Batch 997, Loss 0.28864896297454834\n",
      "[Training Epoch 1] Batch 998, Loss 0.290573388338089\n",
      "[Training Epoch 1] Batch 999, Loss 0.27871468663215637\n",
      "[Training Epoch 1] Batch 1000, Loss 0.3158858120441437\n",
      "[Training Epoch 1] Batch 1001, Loss 0.30126631259918213\n",
      "[Training Epoch 1] Batch 1002, Loss 0.2928934097290039\n",
      "[Training Epoch 1] Batch 1003, Loss 0.29039788246154785\n",
      "[Training Epoch 1] Batch 1004, Loss 0.3065420389175415\n",
      "[Training Epoch 1] Batch 1005, Loss 0.3017202913761139\n",
      "[Training Epoch 1] Batch 1006, Loss 0.3151181638240814\n",
      "[Training Epoch 1] Batch 1007, Loss 0.30981677770614624\n",
      "[Training Epoch 1] Batch 1008, Loss 0.29060980677604675\n",
      "[Training Epoch 1] Batch 1009, Loss 0.2773743271827698\n",
      "[Training Epoch 1] Batch 1010, Loss 0.2955922484397888\n",
      "[Training Epoch 1] Batch 1011, Loss 0.2868548035621643\n",
      "[Training Epoch 1] Batch 1012, Loss 0.303815096616745\n",
      "[Training Epoch 1] Batch 1013, Loss 0.30688923597335815\n",
      "[Training Epoch 1] Batch 1014, Loss 0.2963639497756958\n",
      "[Training Epoch 1] Batch 1015, Loss 0.27862870693206787\n",
      "[Training Epoch 1] Batch 1016, Loss 0.285930871963501\n",
      "[Training Epoch 1] Batch 1017, Loss 0.31570711731910706\n",
      "[Training Epoch 1] Batch 1018, Loss 0.29601120948791504\n",
      "[Training Epoch 1] Batch 1019, Loss 0.30651870369911194\n",
      "[Training Epoch 1] Batch 1020, Loss 0.27117645740509033\n",
      "[Training Epoch 1] Batch 1021, Loss 0.2779712677001953\n",
      "[Training Epoch 1] Batch 1022, Loss 0.3043850064277649\n",
      "[Training Epoch 1] Batch 1023, Loss 0.30172765254974365\n",
      "[Training Epoch 1] Batch 1024, Loss 0.3150731325149536\n",
      "[Training Epoch 1] Batch 1025, Loss 0.25915634632110596\n",
      "[Training Epoch 1] Batch 1026, Loss 0.3158773183822632\n",
      "[Training Epoch 1] Batch 1027, Loss 0.30858084559440613\n",
      "[Training Epoch 1] Batch 1028, Loss 0.28570419549942017\n",
      "[Training Epoch 1] Batch 1029, Loss 0.305992066860199\n",
      "[Training Epoch 1] Batch 1030, Loss 0.2864580750465393\n",
      "[Training Epoch 1] Batch 1031, Loss 0.3196971118450165\n",
      "[Training Epoch 1] Batch 1032, Loss 0.31571635603904724\n",
      "[Training Epoch 1] Batch 1033, Loss 0.29988083243370056\n",
      "[Training Epoch 1] Batch 1034, Loss 0.2808532118797302\n",
      "[Training Epoch 1] Batch 1035, Loss 0.28772687911987305\n",
      "[Training Epoch 1] Batch 1036, Loss 0.27305734157562256\n",
      "[Training Epoch 1] Batch 1037, Loss 0.32786163687705994\n",
      "[Training Epoch 1] Batch 1038, Loss 0.28451406955718994\n",
      "[Training Epoch 1] Batch 1039, Loss 0.31053805351257324\n",
      "[Training Epoch 1] Batch 1040, Loss 0.3033679127693176\n",
      "[Training Epoch 1] Batch 1041, Loss 0.3190130293369293\n",
      "[Training Epoch 1] Batch 1042, Loss 0.3026294410228729\n",
      "[Training Epoch 1] Batch 1043, Loss 0.3101121187210083\n",
      "[Training Epoch 1] Batch 1044, Loss 0.29457712173461914\n",
      "[Training Epoch 1] Batch 1045, Loss 0.305865615606308\n",
      "[Training Epoch 1] Batch 1046, Loss 0.29928064346313477\n",
      "[Training Epoch 1] Batch 1047, Loss 0.29409918189048767\n",
      "[Training Epoch 1] Batch 1048, Loss 0.3272968530654907\n",
      "[Training Epoch 1] Batch 1049, Loss 0.2871376574039459\n",
      "[Training Epoch 1] Batch 1050, Loss 0.31110578775405884\n",
      "[Training Epoch 1] Batch 1051, Loss 0.3171013593673706\n",
      "[Training Epoch 1] Batch 1052, Loss 0.26456332206726074\n",
      "[Training Epoch 1] Batch 1053, Loss 0.29074516892433167\n",
      "[Training Epoch 1] Batch 1054, Loss 0.32854312658309937\n",
      "[Training Epoch 1] Batch 1055, Loss 0.3005707263946533\n",
      "[Training Epoch 1] Batch 1056, Loss 0.3120037913322449\n",
      "[Training Epoch 1] Batch 1057, Loss 0.31503474712371826\n",
      "[Training Epoch 1] Batch 1058, Loss 0.29840803146362305\n",
      "[Training Epoch 1] Batch 1059, Loss 0.2972768545150757\n",
      "[Training Epoch 1] Batch 1060, Loss 0.29263928532600403\n",
      "[Training Epoch 1] Batch 1061, Loss 0.3061583936214447\n",
      "[Training Epoch 1] Batch 1062, Loss 0.26429346203804016\n",
      "[Training Epoch 1] Batch 1063, Loss 0.2966872453689575\n",
      "[Training Epoch 1] Batch 1064, Loss 0.28360515832901\n",
      "[Training Epoch 1] Batch 1065, Loss 0.30403125286102295\n",
      "[Training Epoch 1] Batch 1066, Loss 0.30243659019470215\n",
      "[Training Epoch 1] Batch 1067, Loss 0.2849535346031189\n",
      "[Training Epoch 1] Batch 1068, Loss 0.31571725010871887\n",
      "[Training Epoch 1] Batch 1069, Loss 0.28543826937675476\n",
      "[Training Epoch 1] Batch 1070, Loss 0.29275086522102356\n",
      "[Training Epoch 1] Batch 1071, Loss 0.3047042191028595\n",
      "[Training Epoch 1] Batch 1072, Loss 0.2809915244579315\n",
      "[Training Epoch 1] Batch 1073, Loss 0.2717837691307068\n",
      "[Training Epoch 1] Batch 1074, Loss 0.27891066670417786\n",
      "[Training Epoch 1] Batch 1075, Loss 0.30269932746887207\n",
      "[Training Epoch 1] Batch 1076, Loss 0.29775798320770264\n",
      "[Training Epoch 1] Batch 1077, Loss 0.3199741244316101\n",
      "[Training Epoch 1] Batch 1078, Loss 0.26591894030570984\n",
      "[Training Epoch 1] Batch 1079, Loss 0.3099333643913269\n",
      "[Training Epoch 1] Batch 1080, Loss 0.29009443521499634\n",
      "[Training Epoch 1] Batch 1081, Loss 0.30190908908843994\n",
      "[Training Epoch 1] Batch 1082, Loss 0.2941209077835083\n",
      "[Training Epoch 1] Batch 1083, Loss 0.3008304834365845\n",
      "[Training Epoch 1] Batch 1084, Loss 0.2938016355037689\n",
      "[Training Epoch 1] Batch 1085, Loss 0.28345856070518494\n",
      "[Training Epoch 1] Batch 1086, Loss 0.31884199380874634\n",
      "[Training Epoch 1] Batch 1087, Loss 0.2685897946357727\n",
      "[Training Epoch 1] Batch 1088, Loss 0.3084352910518646\n",
      "[Training Epoch 1] Batch 1089, Loss 0.3209053874015808\n",
      "[Training Epoch 1] Batch 1090, Loss 0.3187845051288605\n",
      "[Training Epoch 1] Batch 1091, Loss 0.2761313319206238\n",
      "[Training Epoch 1] Batch 1092, Loss 0.32412809133529663\n",
      "[Training Epoch 1] Batch 1093, Loss 0.2832542359828949\n",
      "[Training Epoch 1] Batch 1094, Loss 0.2876582741737366\n",
      "[Training Epoch 1] Batch 1095, Loss 0.3151395916938782\n",
      "[Training Epoch 1] Batch 1096, Loss 0.31803008913993835\n",
      "[Training Epoch 1] Batch 1097, Loss 0.3154245615005493\n",
      "[Training Epoch 1] Batch 1098, Loss 0.32738977670669556\n",
      "[Training Epoch 1] Batch 1099, Loss 0.2894022464752197\n",
      "[Training Epoch 1] Batch 1100, Loss 0.312121719121933\n",
      "[Training Epoch 1] Batch 1101, Loss 0.32218918204307556\n",
      "[Training Epoch 1] Batch 1102, Loss 0.28352051973342896\n",
      "[Training Epoch 1] Batch 1103, Loss 0.3021082878112793\n",
      "[Training Epoch 1] Batch 1104, Loss 0.3117000460624695\n",
      "[Training Epoch 1] Batch 1105, Loss 0.32627713680267334\n",
      "[Training Epoch 1] Batch 1106, Loss 0.3072071075439453\n",
      "[Training Epoch 1] Batch 1107, Loss 0.2736111283302307\n",
      "[Training Epoch 1] Batch 1108, Loss 0.31263893842697144\n",
      "[Training Epoch 1] Batch 1109, Loss 0.28877073526382446\n",
      "[Training Epoch 1] Batch 1110, Loss 0.3085689842700958\n",
      "[Training Epoch 1] Batch 1111, Loss 0.3041951656341553\n",
      "[Training Epoch 1] Batch 1112, Loss 0.3009907603263855\n",
      "[Training Epoch 1] Batch 1113, Loss 0.31854283809661865\n",
      "[Training Epoch 1] Batch 1114, Loss 0.2904449999332428\n",
      "[Training Epoch 1] Batch 1115, Loss 0.30457958579063416\n",
      "[Training Epoch 1] Batch 1116, Loss 0.3179905414581299\n",
      "[Training Epoch 1] Batch 1117, Loss 0.2939508259296417\n",
      "[Training Epoch 1] Batch 1118, Loss 0.3037160038948059\n",
      "[Training Epoch 1] Batch 1119, Loss 0.3194774389266968\n",
      "[Training Epoch 1] Batch 1120, Loss 0.3003913462162018\n",
      "[Training Epoch 1] Batch 1121, Loss 0.28555700182914734\n",
      "[Training Epoch 1] Batch 1122, Loss 0.31690603494644165\n",
      "[Training Epoch 1] Batch 1123, Loss 0.29241982102394104\n",
      "[Training Epoch 1] Batch 1124, Loss 0.31209617853164673\n",
      "[Training Epoch 1] Batch 1125, Loss 0.301760733127594\n",
      "[Training Epoch 1] Batch 1126, Loss 0.30972111225128174\n",
      "[Training Epoch 1] Batch 1127, Loss 0.3095211684703827\n",
      "[Training Epoch 1] Batch 1128, Loss 0.3047473430633545\n",
      "[Training Epoch 1] Batch 1129, Loss 0.29732149839401245\n",
      "[Training Epoch 1] Batch 1130, Loss 0.3182700574398041\n",
      "[Training Epoch 1] Batch 1131, Loss 0.32905519008636475\n",
      "[Training Epoch 1] Batch 1132, Loss 0.29135629534721375\n",
      "[Training Epoch 1] Batch 1133, Loss 0.30538421869277954\n",
      "[Training Epoch 1] Batch 1134, Loss 0.27700284123420715\n",
      "[Training Epoch 1] Batch 1135, Loss 0.320210337638855\n",
      "[Training Epoch 1] Batch 1136, Loss 0.3184930980205536\n",
      "[Training Epoch 1] Batch 1137, Loss 0.3071439266204834\n",
      "[Training Epoch 1] Batch 1138, Loss 0.3255329132080078\n",
      "[Training Epoch 1] Batch 1139, Loss 0.2942577600479126\n",
      "[Training Epoch 1] Batch 1140, Loss 0.2814282178878784\n",
      "[Training Epoch 1] Batch 1141, Loss 0.3052636384963989\n",
      "[Training Epoch 1] Batch 1142, Loss 0.3047114610671997\n",
      "[Training Epoch 1] Batch 1143, Loss 0.2979438006877899\n",
      "[Training Epoch 1] Batch 1144, Loss 0.29671764373779297\n",
      "[Training Epoch 1] Batch 1145, Loss 0.2853977680206299\n",
      "[Training Epoch 1] Batch 1146, Loss 0.2835180163383484\n",
      "[Training Epoch 1] Batch 1147, Loss 0.3238835632801056\n",
      "[Training Epoch 1] Batch 1148, Loss 0.27881085872650146\n",
      "[Training Epoch 1] Batch 1149, Loss 0.3100111484527588\n",
      "[Training Epoch 1] Batch 1150, Loss 0.3149011731147766\n",
      "[Training Epoch 1] Batch 1151, Loss 0.2933692932128906\n",
      "[Training Epoch 1] Batch 1152, Loss 0.28309088945388794\n",
      "[Training Epoch 1] Batch 1153, Loss 0.2927335798740387\n",
      "[Training Epoch 1] Batch 1154, Loss 0.30966871976852417\n",
      "[Training Epoch 1] Batch 1155, Loss 0.3281242549419403\n",
      "[Training Epoch 1] Batch 1156, Loss 0.28723838925361633\n",
      "[Training Epoch 1] Batch 1157, Loss 0.28523513674736023\n",
      "[Training Epoch 1] Batch 1158, Loss 0.31788817048072815\n",
      "[Training Epoch 1] Batch 1159, Loss 0.29769545793533325\n",
      "[Training Epoch 1] Batch 1160, Loss 0.30737704038619995\n",
      "[Training Epoch 1] Batch 1161, Loss 0.3156317472457886\n",
      "[Training Epoch 1] Batch 1162, Loss 0.3142300248146057\n",
      "[Training Epoch 1] Batch 1163, Loss 0.3046414256095886\n",
      "[Training Epoch 1] Batch 1164, Loss 0.29022416472435\n",
      "[Training Epoch 1] Batch 1165, Loss 0.32022958993911743\n",
      "[Training Epoch 1] Batch 1166, Loss 0.3094043731689453\n",
      "[Training Epoch 1] Batch 1167, Loss 0.31462740898132324\n",
      "[Training Epoch 1] Batch 1168, Loss 0.3041427731513977\n",
      "[Training Epoch 1] Batch 1169, Loss 0.28464680910110474\n",
      "[Training Epoch 1] Batch 1170, Loss 0.3038637638092041\n",
      "[Training Epoch 1] Batch 1171, Loss 0.3032863736152649\n",
      "[Training Epoch 1] Batch 1172, Loss 0.2700280547142029\n",
      "[Training Epoch 1] Batch 1173, Loss 0.2693961560726166\n",
      "[Training Epoch 1] Batch 1174, Loss 0.3018474578857422\n",
      "[Training Epoch 1] Batch 1175, Loss 0.3076331317424774\n",
      "[Training Epoch 1] Batch 1176, Loss 0.2836795449256897\n",
      "[Training Epoch 1] Batch 1177, Loss 0.2860589325428009\n",
      "[Training Epoch 1] Batch 1178, Loss 0.2730397582054138\n",
      "[Training Epoch 1] Batch 1179, Loss 0.2846035361289978\n",
      "[Training Epoch 1] Batch 1180, Loss 0.2851865589618683\n",
      "[Training Epoch 1] Batch 1181, Loss 0.31079745292663574\n",
      "[Training Epoch 1] Batch 1182, Loss 0.31016597151756287\n",
      "[Training Epoch 1] Batch 1183, Loss 0.3117176294326782\n",
      "[Training Epoch 1] Batch 1184, Loss 0.3062734007835388\n",
      "[Training Epoch 1] Batch 1185, Loss 0.2830314040184021\n",
      "[Training Epoch 1] Batch 1186, Loss 0.30062052607536316\n",
      "[Training Epoch 1] Batch 1187, Loss 0.27713704109191895\n",
      "[Training Epoch 1] Batch 1188, Loss 0.29725411534309387\n",
      "[Training Epoch 1] Batch 1189, Loss 0.31200534105300903\n",
      "[Training Epoch 1] Batch 1190, Loss 0.32014602422714233\n",
      "[Training Epoch 1] Batch 1191, Loss 0.2869635224342346\n",
      "[Training Epoch 1] Batch 1192, Loss 0.30823761224746704\n",
      "[Training Epoch 1] Batch 1193, Loss 0.3065100908279419\n",
      "[Training Epoch 1] Batch 1194, Loss 0.31416380405426025\n",
      "[Training Epoch 1] Batch 1195, Loss 0.2681314945220947\n",
      "[Training Epoch 1] Batch 1196, Loss 0.31874069571495056\n",
      "[Training Epoch 1] Batch 1197, Loss 0.3337428569793701\n",
      "[Training Epoch 1] Batch 1198, Loss 0.31719261407852173\n",
      "[Training Epoch 1] Batch 1199, Loss 0.2927268147468567\n",
      "[Training Epoch 1] Batch 1200, Loss 0.2900833785533905\n",
      "[Training Epoch 1] Batch 1201, Loss 0.2867027819156647\n",
      "[Training Epoch 1] Batch 1202, Loss 0.30108416080474854\n",
      "[Training Epoch 1] Batch 1203, Loss 0.2848759889602661\n",
      "[Training Epoch 1] Batch 1204, Loss 0.2932824492454529\n",
      "[Training Epoch 1] Batch 1205, Loss 0.30427980422973633\n",
      "[Training Epoch 1] Batch 1206, Loss 0.3143346309661865\n",
      "[Training Epoch 1] Batch 1207, Loss 0.31911903619766235\n",
      "[Training Epoch 1] Batch 1208, Loss 0.3181946575641632\n",
      "[Training Epoch 1] Batch 1209, Loss 0.2844488024711609\n",
      "[Training Epoch 1] Batch 1210, Loss 0.2823534309864044\n",
      "[Training Epoch 1] Batch 1211, Loss 0.3164476752281189\n",
      "[Training Epoch 1] Batch 1212, Loss 0.29330524802207947\n",
      "[Training Epoch 1] Batch 1213, Loss 0.31140804290771484\n",
      "[Training Epoch 1] Batch 1214, Loss 0.2898455858230591\n",
      "[Training Epoch 1] Batch 1215, Loss 0.2736113667488098\n",
      "[Training Epoch 1] Batch 1216, Loss 0.3050749897956848\n",
      "[Training Epoch 1] Batch 1217, Loss 0.30932947993278503\n",
      "[Training Epoch 1] Batch 1218, Loss 0.27109384536743164\n",
      "[Training Epoch 1] Batch 1219, Loss 0.2927555739879608\n",
      "[Training Epoch 1] Batch 1220, Loss 0.2949688732624054\n",
      "[Training Epoch 1] Batch 1221, Loss 0.3002980351448059\n",
      "[Training Epoch 1] Batch 1222, Loss 0.3038536608219147\n",
      "[Training Epoch 1] Batch 1223, Loss 0.32266518473625183\n",
      "[Training Epoch 1] Batch 1224, Loss 0.3088102340698242\n",
      "[Training Epoch 1] Batch 1225, Loss 0.3169540762901306\n",
      "[Training Epoch 1] Batch 1226, Loss 0.27661383152008057\n",
      "[Training Epoch 1] Batch 1227, Loss 0.2952924966812134\n",
      "[Training Epoch 1] Batch 1228, Loss 0.2783447504043579\n",
      "[Training Epoch 1] Batch 1229, Loss 0.27657073736190796\n",
      "[Training Epoch 1] Batch 1230, Loss 0.312709778547287\n",
      "[Training Epoch 1] Batch 1231, Loss 0.28674691915512085\n",
      "[Training Epoch 1] Batch 1232, Loss 0.31879252195358276\n",
      "[Training Epoch 1] Batch 1233, Loss 0.30030137300491333\n",
      "[Training Epoch 1] Batch 1234, Loss 0.2946944832801819\n",
      "[Training Epoch 1] Batch 1235, Loss 0.30725347995758057\n",
      "[Training Epoch 1] Batch 1236, Loss 0.2983413338661194\n",
      "[Training Epoch 1] Batch 1237, Loss 0.26593291759490967\n",
      "[Training Epoch 1] Batch 1238, Loss 0.29656195640563965\n",
      "[Training Epoch 1] Batch 1239, Loss 0.2965872585773468\n",
      "[Training Epoch 1] Batch 1240, Loss 0.29306694865226746\n",
      "[Training Epoch 1] Batch 1241, Loss 0.3089429438114166\n",
      "[Training Epoch 1] Batch 1242, Loss 0.29700368642807007\n",
      "[Training Epoch 1] Batch 1243, Loss 0.27274757623672485\n",
      "[Training Epoch 1] Batch 1244, Loss 0.2989770770072937\n",
      "[Training Epoch 1] Batch 1245, Loss 0.31588587164878845\n",
      "[Training Epoch 1] Batch 1246, Loss 0.2886388897895813\n",
      "[Training Epoch 1] Batch 1247, Loss 0.3095284700393677\n",
      "[Training Epoch 1] Batch 1248, Loss 0.2723381221294403\n",
      "[Training Epoch 1] Batch 1249, Loss 0.3007407486438751\n",
      "[Training Epoch 1] Batch 1250, Loss 0.2867908477783203\n",
      "[Training Epoch 1] Batch 1251, Loss 0.28331896662712097\n",
      "[Training Epoch 1] Batch 1252, Loss 0.2584926187992096\n",
      "[Training Epoch 1] Batch 1253, Loss 0.303570955991745\n",
      "[Training Epoch 1] Batch 1254, Loss 0.29923999309539795\n",
      "[Training Epoch 1] Batch 1255, Loss 0.28338122367858887\n",
      "[Training Epoch 1] Batch 1256, Loss 0.2707344889640808\n",
      "[Training Epoch 1] Batch 1257, Loss 0.3219226002693176\n",
      "[Training Epoch 1] Batch 1258, Loss 0.30851924419403076\n",
      "[Training Epoch 1] Batch 1259, Loss 0.2968056797981262\n",
      "[Training Epoch 1] Batch 1260, Loss 0.30213361978530884\n",
      "[Training Epoch 1] Batch 1261, Loss 0.30626994371414185\n",
      "[Training Epoch 1] Batch 1262, Loss 0.29244258999824524\n",
      "[Training Epoch 1] Batch 1263, Loss 0.3028095066547394\n",
      "[Training Epoch 1] Batch 1264, Loss 0.26809951663017273\n",
      "[Training Epoch 1] Batch 1265, Loss 0.3228115141391754\n",
      "[Training Epoch 1] Batch 1266, Loss 0.28046518564224243\n",
      "[Training Epoch 1] Batch 1267, Loss 0.3075585663318634\n",
      "[Training Epoch 1] Batch 1268, Loss 0.30700409412384033\n",
      "[Training Epoch 1] Batch 1269, Loss 0.3221328854560852\n",
      "[Training Epoch 1] Batch 1270, Loss 0.2793442904949188\n",
      "[Training Epoch 1] Batch 1271, Loss 0.30043452978134155\n",
      "[Training Epoch 1] Batch 1272, Loss 0.2873992323875427\n",
      "[Training Epoch 1] Batch 1273, Loss 0.27585670351982117\n",
      "[Training Epoch 1] Batch 1274, Loss 0.29966074228286743\n",
      "[Training Epoch 1] Batch 1275, Loss 0.3139845132827759\n",
      "[Training Epoch 1] Batch 1276, Loss 0.3021952509880066\n",
      "[Training Epoch 1] Batch 1277, Loss 0.3131241798400879\n",
      "[Training Epoch 1] Batch 1278, Loss 0.26769012212753296\n",
      "[Training Epoch 1] Batch 1279, Loss 0.27764183282852173\n",
      "[Training Epoch 1] Batch 1280, Loss 0.31350964307785034\n",
      "[Training Epoch 1] Batch 1281, Loss 0.28237730264663696\n",
      "[Training Epoch 1] Batch 1282, Loss 0.2807142734527588\n",
      "[Training Epoch 1] Batch 1283, Loss 0.26257944107055664\n",
      "[Training Epoch 1] Batch 1284, Loss 0.2921236455440521\n",
      "[Training Epoch 1] Batch 1285, Loss 0.27554547786712646\n",
      "[Training Epoch 1] Batch 1286, Loss 0.28365403413772583\n",
      "[Training Epoch 1] Batch 1287, Loss 0.2879987061023712\n",
      "[Training Epoch 1] Batch 1288, Loss 0.3163542151451111\n",
      "[Training Epoch 1] Batch 1289, Loss 0.3167293667793274\n",
      "[Training Epoch 1] Batch 1290, Loss 0.31551212072372437\n",
      "[Training Epoch 1] Batch 1291, Loss 0.3174595236778259\n",
      "[Training Epoch 1] Batch 1292, Loss 0.30885884165763855\n",
      "[Training Epoch 1] Batch 1293, Loss 0.3127955198287964\n",
      "[Training Epoch 1] Batch 1294, Loss 0.29468029737472534\n",
      "[Training Epoch 1] Batch 1295, Loss 0.31231993436813354\n",
      "[Training Epoch 1] Batch 1296, Loss 0.27717605233192444\n",
      "[Training Epoch 1] Batch 1297, Loss 0.2912508547306061\n",
      "[Training Epoch 1] Batch 1298, Loss 0.3262936472892761\n",
      "[Training Epoch 1] Batch 1299, Loss 0.2729707956314087\n",
      "[Training Epoch 1] Batch 1300, Loss 0.2953924536705017\n",
      "[Training Epoch 1] Batch 1301, Loss 0.2988860011100769\n",
      "[Training Epoch 1] Batch 1302, Loss 0.2957761287689209\n",
      "[Training Epoch 1] Batch 1303, Loss 0.2769857347011566\n",
      "[Training Epoch 1] Batch 1304, Loss 0.30031251907348633\n",
      "[Training Epoch 1] Batch 1305, Loss 0.2752164304256439\n",
      "[Training Epoch 1] Batch 1306, Loss 0.295380175113678\n",
      "[Training Epoch 1] Batch 1307, Loss 0.3045189380645752\n",
      "[Training Epoch 1] Batch 1308, Loss 0.3026028275489807\n",
      "[Training Epoch 1] Batch 1309, Loss 0.31857284903526306\n",
      "[Training Epoch 1] Batch 1310, Loss 0.30139046907424927\n",
      "[Training Epoch 1] Batch 1311, Loss 0.3225492835044861\n",
      "[Training Epoch 1] Batch 1312, Loss 0.27984923124313354\n",
      "[Training Epoch 1] Batch 1313, Loss 0.3078324794769287\n",
      "[Training Epoch 1] Batch 1314, Loss 0.3043266832828522\n",
      "[Training Epoch 1] Batch 1315, Loss 0.3007408082485199\n",
      "[Training Epoch 1] Batch 1316, Loss 0.27778148651123047\n",
      "[Training Epoch 1] Batch 1317, Loss 0.32786285877227783\n",
      "[Training Epoch 1] Batch 1318, Loss 0.2833091616630554\n",
      "[Training Epoch 1] Batch 1319, Loss 0.2908782958984375\n",
      "[Training Epoch 1] Batch 1320, Loss 0.285586953163147\n",
      "[Training Epoch 1] Batch 1321, Loss 0.30483004450798035\n",
      "[Training Epoch 1] Batch 1322, Loss 0.2910303473472595\n",
      "[Training Epoch 1] Batch 1323, Loss 0.3162088394165039\n",
      "[Training Epoch 1] Batch 1324, Loss 0.2973317503929138\n",
      "[Training Epoch 1] Batch 1325, Loss 0.29700785875320435\n",
      "[Training Epoch 1] Batch 1326, Loss 0.2794252038002014\n",
      "[Training Epoch 1] Batch 1327, Loss 0.3007674217224121\n",
      "[Training Epoch 1] Batch 1328, Loss 0.3003162145614624\n",
      "[Training Epoch 1] Batch 1329, Loss 0.3072006404399872\n",
      "[Training Epoch 1] Batch 1330, Loss 0.30648577213287354\n",
      "[Training Epoch 1] Batch 1331, Loss 0.2931017577648163\n",
      "[Training Epoch 1] Batch 1332, Loss 0.30266788601875305\n",
      "[Training Epoch 1] Batch 1333, Loss 0.3191099464893341\n",
      "[Training Epoch 1] Batch 1334, Loss 0.2880224585533142\n",
      "[Training Epoch 1] Batch 1335, Loss 0.2946951985359192\n",
      "[Training Epoch 1] Batch 1336, Loss 0.32766151428222656\n",
      "[Training Epoch 1] Batch 1337, Loss 0.32299238443374634\n",
      "[Training Epoch 1] Batch 1338, Loss 0.30473798513412476\n",
      "[Training Epoch 1] Batch 1339, Loss 0.318609356880188\n",
      "[Training Epoch 1] Batch 1340, Loss 0.2864421010017395\n",
      "[Training Epoch 1] Batch 1341, Loss 0.29023048281669617\n",
      "[Training Epoch 1] Batch 1342, Loss 0.31175437569618225\n",
      "[Training Epoch 1] Batch 1343, Loss 0.3239949643611908\n",
      "[Training Epoch 1] Batch 1344, Loss 0.3202529549598694\n",
      "[Training Epoch 1] Batch 1345, Loss 0.27761948108673096\n",
      "[Training Epoch 1] Batch 1346, Loss 0.2884644865989685\n",
      "[Training Epoch 1] Batch 1347, Loss 0.3040398359298706\n",
      "[Training Epoch 1] Batch 1348, Loss 0.3063308596611023\n",
      "[Training Epoch 1] Batch 1349, Loss 0.29912158846855164\n",
      "[Training Epoch 1] Batch 1350, Loss 0.3071504831314087\n",
      "[Training Epoch 1] Batch 1351, Loss 0.2976325750350952\n",
      "[Training Epoch 1] Batch 1352, Loss 0.31975874304771423\n",
      "[Training Epoch 1] Batch 1353, Loss 0.3037094473838806\n",
      "[Training Epoch 1] Batch 1354, Loss 0.2932766079902649\n",
      "[Training Epoch 1] Batch 1355, Loss 0.31522136926651\n",
      "[Training Epoch 1] Batch 1356, Loss 0.3047221302986145\n",
      "[Training Epoch 1] Batch 1357, Loss 0.31127211451530457\n",
      "[Training Epoch 1] Batch 1358, Loss 0.31843841075897217\n",
      "[Training Epoch 1] Batch 1359, Loss 0.29889750480651855\n",
      "[Training Epoch 1] Batch 1360, Loss 0.29208773374557495\n",
      "[Training Epoch 1] Batch 1361, Loss 0.31083792448043823\n",
      "[Training Epoch 1] Batch 1362, Loss 0.266933411359787\n",
      "[Training Epoch 1] Batch 1363, Loss 0.27775540947914124\n",
      "[Training Epoch 1] Batch 1364, Loss 0.3019450902938843\n",
      "[Training Epoch 1] Batch 1365, Loss 0.2563347816467285\n",
      "[Training Epoch 1] Batch 1366, Loss 0.2884397804737091\n",
      "[Training Epoch 1] Batch 1367, Loss 0.2805325984954834\n",
      "[Training Epoch 1] Batch 1368, Loss 0.2775893807411194\n",
      "[Training Epoch 1] Batch 1369, Loss 0.28197580575942993\n",
      "[Training Epoch 1] Batch 1370, Loss 0.27659496665000916\n",
      "[Training Epoch 1] Batch 1371, Loss 0.29516416788101196\n",
      "[Training Epoch 1] Batch 1372, Loss 0.3049585223197937\n",
      "[Training Epoch 1] Batch 1373, Loss 0.31692203879356384\n",
      "[Training Epoch 1] Batch 1374, Loss 0.2972751557826996\n",
      "[Training Epoch 1] Batch 1375, Loss 0.3280640244483948\n",
      "[Training Epoch 1] Batch 1376, Loss 0.27486979961395264\n",
      "[Training Epoch 1] Batch 1377, Loss 0.3231966495513916\n",
      "[Training Epoch 1] Batch 1378, Loss 0.31029051542282104\n",
      "[Training Epoch 1] Batch 1379, Loss 0.2930261194705963\n",
      "[Training Epoch 1] Batch 1380, Loss 0.26127052307128906\n",
      "[Training Epoch 1] Batch 1381, Loss 0.2719486653804779\n",
      "[Training Epoch 1] Batch 1382, Loss 0.2857484817504883\n",
      "[Training Epoch 1] Batch 1383, Loss 0.29037272930145264\n",
      "[Training Epoch 1] Batch 1384, Loss 0.3040284216403961\n",
      "[Training Epoch 1] Batch 1385, Loss 0.29314765334129333\n",
      "[Training Epoch 1] Batch 1386, Loss 0.328579843044281\n",
      "[Training Epoch 1] Batch 1387, Loss 0.2890775799751282\n",
      "[Training Epoch 1] Batch 1388, Loss 0.307839572429657\n",
      "[Training Epoch 1] Batch 1389, Loss 0.2852908670902252\n",
      "[Training Epoch 1] Batch 1390, Loss 0.30571308732032776\n",
      "[Training Epoch 1] Batch 1391, Loss 0.2812846302986145\n",
      "[Training Epoch 1] Batch 1392, Loss 0.2940610945224762\n",
      "[Training Epoch 1] Batch 1393, Loss 0.29541894793510437\n",
      "[Training Epoch 1] Batch 1394, Loss 0.280490517616272\n",
      "[Training Epoch 1] Batch 1395, Loss 0.2788674831390381\n",
      "[Training Epoch 1] Batch 1396, Loss 0.2854817807674408\n",
      "[Training Epoch 1] Batch 1397, Loss 0.27735644578933716\n",
      "[Training Epoch 1] Batch 1398, Loss 0.3019283413887024\n",
      "[Training Epoch 1] Batch 1399, Loss 0.3059925138950348\n",
      "[Training Epoch 1] Batch 1400, Loss 0.3000969886779785\n",
      "[Training Epoch 1] Batch 1401, Loss 0.2871784567832947\n",
      "[Training Epoch 1] Batch 1402, Loss 0.28911706805229187\n",
      "[Training Epoch 1] Batch 1403, Loss 0.2852131128311157\n",
      "[Training Epoch 1] Batch 1404, Loss 0.3036426603794098\n",
      "[Training Epoch 1] Batch 1405, Loss 0.3045264482498169\n",
      "[Training Epoch 1] Batch 1406, Loss 0.2995201051235199\n",
      "[Training Epoch 1] Batch 1407, Loss 0.34454119205474854\n",
      "[Training Epoch 1] Batch 1408, Loss 0.309222012758255\n",
      "[Training Epoch 1] Batch 1409, Loss 0.3098626136779785\n",
      "[Training Epoch 1] Batch 1410, Loss 0.2926573157310486\n",
      "[Training Epoch 1] Batch 1411, Loss 0.29161757230758667\n",
      "[Training Epoch 1] Batch 1412, Loss 0.27860260009765625\n",
      "[Training Epoch 1] Batch 1413, Loss 0.3332817554473877\n",
      "[Training Epoch 1] Batch 1414, Loss 0.2999400496482849\n",
      "[Training Epoch 1] Batch 1415, Loss 0.32439830899238586\n",
      "[Training Epoch 1] Batch 1416, Loss 0.30658555030822754\n",
      "[Training Epoch 1] Batch 1417, Loss 0.2970532178878784\n",
      "[Training Epoch 1] Batch 1418, Loss 0.2926735281944275\n",
      "[Training Epoch 1] Batch 1419, Loss 0.29483655095100403\n",
      "[Training Epoch 1] Batch 1420, Loss 0.29616066813468933\n",
      "[Training Epoch 1] Batch 1421, Loss 0.3037581741809845\n",
      "[Training Epoch 1] Batch 1422, Loss 0.2791118621826172\n",
      "[Training Epoch 1] Batch 1423, Loss 0.2880011200904846\n",
      "[Training Epoch 1] Batch 1424, Loss 0.2779058814048767\n",
      "[Training Epoch 1] Batch 1425, Loss 0.29494544863700867\n",
      "[Training Epoch 1] Batch 1426, Loss 0.29574674367904663\n",
      "[Training Epoch 1] Batch 1427, Loss 0.30193138122558594\n",
      "[Training Epoch 1] Batch 1428, Loss 0.29318004846572876\n",
      "[Training Epoch 1] Batch 1429, Loss 0.3035953640937805\n",
      "[Training Epoch 1] Batch 1430, Loss 0.27507123351097107\n",
      "[Training Epoch 1] Batch 1431, Loss 0.29628926515579224\n",
      "[Training Epoch 1] Batch 1432, Loss 0.29028797149658203\n",
      "[Training Epoch 1] Batch 1433, Loss 0.3142043948173523\n",
      "[Training Epoch 1] Batch 1434, Loss 0.2969396710395813\n",
      "[Training Epoch 1] Batch 1435, Loss 0.3015049993991852\n",
      "[Training Epoch 1] Batch 1436, Loss 0.289955198764801\n",
      "[Training Epoch 1] Batch 1437, Loss 0.29121893644332886\n",
      "[Training Epoch 1] Batch 1438, Loss 0.29288190603256226\n",
      "[Training Epoch 1] Batch 1439, Loss 0.3189973831176758\n",
      "[Training Epoch 1] Batch 1440, Loss 0.3515745997428894\n",
      "[Training Epoch 1] Batch 1441, Loss 0.31173187494277954\n",
      "[Training Epoch 1] Batch 1442, Loss 0.3320235013961792\n",
      "[Training Epoch 1] Batch 1443, Loss 0.3007199168205261\n",
      "[Training Epoch 1] Batch 1444, Loss 0.3203031122684479\n",
      "[Training Epoch 1] Batch 1445, Loss 0.3148754835128784\n",
      "[Training Epoch 1] Batch 1446, Loss 0.3054056763648987\n",
      "[Training Epoch 1] Batch 1447, Loss 0.28192752599716187\n",
      "[Training Epoch 1] Batch 1448, Loss 0.3098887801170349\n",
      "[Training Epoch 1] Batch 1449, Loss 0.30267539620399475\n",
      "[Training Epoch 1] Batch 1450, Loss 0.29952535033226013\n",
      "[Training Epoch 1] Batch 1451, Loss 0.3138101398944855\n",
      "[Training Epoch 1] Batch 1452, Loss 0.3071177005767822\n",
      "[Training Epoch 1] Batch 1453, Loss 0.317200630903244\n",
      "[Training Epoch 1] Batch 1454, Loss 0.3227039575576782\n",
      "[Training Epoch 1] Batch 1455, Loss 0.2672889828681946\n",
      "[Training Epoch 1] Batch 1456, Loss 0.3057217299938202\n",
      "[Training Epoch 1] Batch 1457, Loss 0.2965361475944519\n",
      "[Training Epoch 1] Batch 1458, Loss 0.30728551745414734\n",
      "[Training Epoch 1] Batch 1459, Loss 0.2632761001586914\n",
      "[Training Epoch 1] Batch 1460, Loss 0.29127880930900574\n",
      "[Training Epoch 1] Batch 1461, Loss 0.30559542775154114\n",
      "[Training Epoch 1] Batch 1462, Loss 0.30160951614379883\n",
      "[Training Epoch 1] Batch 1463, Loss 0.31602996587753296\n",
      "[Training Epoch 1] Batch 1464, Loss 0.3013457953929901\n",
      "[Training Epoch 1] Batch 1465, Loss 0.2906639575958252\n",
      "[Training Epoch 1] Batch 1466, Loss 0.2604095935821533\n",
      "[Training Epoch 1] Batch 1467, Loss 0.31219205260276794\n",
      "[Training Epoch 1] Batch 1468, Loss 0.2729912996292114\n",
      "[Training Epoch 1] Batch 1469, Loss 0.30937737226486206\n",
      "[Training Epoch 1] Batch 1470, Loss 0.2876526713371277\n",
      "[Training Epoch 1] Batch 1471, Loss 0.28592002391815186\n",
      "[Training Epoch 1] Batch 1472, Loss 0.2745051681995392\n",
      "[Training Epoch 1] Batch 1473, Loss 0.3075019121170044\n",
      "[Training Epoch 1] Batch 1474, Loss 0.28416287899017334\n",
      "[Training Epoch 1] Batch 1475, Loss 0.31630170345306396\n",
      "[Training Epoch 1] Batch 1476, Loss 0.2894947826862335\n",
      "[Training Epoch 1] Batch 1477, Loss 0.2987912893295288\n",
      "[Training Epoch 1] Batch 1478, Loss 0.27928516268730164\n",
      "[Training Epoch 1] Batch 1479, Loss 0.3123759627342224\n",
      "[Training Epoch 1] Batch 1480, Loss 0.30808544158935547\n",
      "[Training Epoch 1] Batch 1481, Loss 0.31482142210006714\n",
      "[Training Epoch 1] Batch 1482, Loss 0.292561799287796\n",
      "[Training Epoch 1] Batch 1483, Loss 0.2815026044845581\n",
      "[Training Epoch 1] Batch 1484, Loss 0.30585795640945435\n",
      "[Training Epoch 1] Batch 1485, Loss 0.3185654282569885\n",
      "[Training Epoch 1] Batch 1486, Loss 0.3055964410305023\n",
      "[Training Epoch 1] Batch 1487, Loss 0.31337642669677734\n",
      "[Training Epoch 1] Batch 1488, Loss 0.30920034646987915\n",
      "[Training Epoch 1] Batch 1489, Loss 0.2777130603790283\n",
      "[Training Epoch 1] Batch 1490, Loss 0.28621917963027954\n",
      "[Training Epoch 1] Batch 1491, Loss 0.2814767360687256\n",
      "[Training Epoch 1] Batch 1492, Loss 0.29703688621520996\n",
      "[Training Epoch 1] Batch 1493, Loss 0.31415706872940063\n",
      "[Training Epoch 1] Batch 1494, Loss 0.29623886942863464\n",
      "[Training Epoch 1] Batch 1495, Loss 0.26738226413726807\n",
      "[Training Epoch 1] Batch 1496, Loss 0.3205283284187317\n",
      "[Training Epoch 1] Batch 1497, Loss 0.29300743341445923\n",
      "[Training Epoch 1] Batch 1498, Loss 0.26570847630500793\n",
      "[Training Epoch 1] Batch 1499, Loss 0.28032004833221436\n",
      "[Training Epoch 1] Batch 1500, Loss 0.3138241767883301\n",
      "[Training Epoch 1] Batch 1501, Loss 0.28532350063323975\n",
      "[Training Epoch 1] Batch 1502, Loss 0.31411561369895935\n",
      "[Training Epoch 1] Batch 1503, Loss 0.2940940856933594\n",
      "[Training Epoch 1] Batch 1504, Loss 0.30801621079444885\n",
      "[Training Epoch 1] Batch 1505, Loss 0.2914268374443054\n",
      "[Training Epoch 1] Batch 1506, Loss 0.28436601161956787\n",
      "[Training Epoch 1] Batch 1507, Loss 0.29088252782821655\n",
      "[Training Epoch 1] Batch 1508, Loss 0.2674950361251831\n",
      "[Training Epoch 1] Batch 1509, Loss 0.30058056116104126\n",
      "[Training Epoch 1] Batch 1510, Loss 0.30756527185440063\n",
      "[Training Epoch 1] Batch 1511, Loss 0.3032069802284241\n",
      "[Training Epoch 1] Batch 1512, Loss 0.3080609440803528\n",
      "[Training Epoch 1] Batch 1513, Loss 0.296317458152771\n",
      "[Training Epoch 1] Batch 1514, Loss 0.31932830810546875\n",
      "[Training Epoch 1] Batch 1515, Loss 0.3141358494758606\n",
      "[Training Epoch 1] Batch 1516, Loss 0.2903033494949341\n",
      "[Training Epoch 1] Batch 1517, Loss 0.3116329312324524\n",
      "[Training Epoch 1] Batch 1518, Loss 0.27297550439834595\n",
      "[Training Epoch 1] Batch 1519, Loss 0.2940358519554138\n",
      "[Training Epoch 1] Batch 1520, Loss 0.3045892119407654\n",
      "[Training Epoch 1] Batch 1521, Loss 0.326748788356781\n",
      "[Training Epoch 1] Batch 1522, Loss 0.272726446390152\n",
      "[Training Epoch 1] Batch 1523, Loss 0.3040161728858948\n",
      "[Training Epoch 1] Batch 1524, Loss 0.2766035795211792\n",
      "[Training Epoch 1] Batch 1525, Loss 0.3050271272659302\n",
      "[Training Epoch 1] Batch 1526, Loss 0.30095013976097107\n",
      "[Training Epoch 1] Batch 1527, Loss 0.31650277972221375\n",
      "[Training Epoch 1] Batch 1528, Loss 0.2828354835510254\n",
      "[Training Epoch 1] Batch 1529, Loss 0.29646429419517517\n",
      "[Training Epoch 1] Batch 1530, Loss 0.2945299744606018\n",
      "[Training Epoch 1] Batch 1531, Loss 0.2950872778892517\n",
      "[Training Epoch 1] Batch 1532, Loss 0.2939741909503937\n",
      "[Training Epoch 1] Batch 1533, Loss 0.3022780418395996\n",
      "[Training Epoch 1] Batch 1534, Loss 0.32396093010902405\n",
      "[Training Epoch 1] Batch 1535, Loss 0.2788380980491638\n",
      "[Training Epoch 1] Batch 1536, Loss 0.3060183525085449\n",
      "[Training Epoch 1] Batch 1537, Loss 0.27414125204086304\n",
      "[Training Epoch 1] Batch 1538, Loss 0.29606425762176514\n",
      "[Training Epoch 1] Batch 1539, Loss 0.2676198482513428\n",
      "[Training Epoch 1] Batch 1540, Loss 0.2836662530899048\n",
      "[Training Epoch 1] Batch 1541, Loss 0.3016241192817688\n",
      "[Training Epoch 1] Batch 1542, Loss 0.2835836410522461\n",
      "[Training Epoch 1] Batch 1543, Loss 0.30926477909088135\n",
      "[Training Epoch 1] Batch 1544, Loss 0.307494193315506\n",
      "[Training Epoch 1] Batch 1545, Loss 0.30788183212280273\n",
      "[Training Epoch 1] Batch 1546, Loss 0.3229115903377533\n",
      "[Training Epoch 1] Batch 1547, Loss 0.31189244985580444\n",
      "[Training Epoch 1] Batch 1548, Loss 0.28665971755981445\n",
      "[Training Epoch 1] Batch 1549, Loss 0.28617483377456665\n",
      "[Training Epoch 1] Batch 1550, Loss 0.28823786973953247\n",
      "[Training Epoch 1] Batch 1551, Loss 0.2742948532104492\n",
      "[Training Epoch 1] Batch 1552, Loss 0.3127342462539673\n",
      "[Training Epoch 1] Batch 1553, Loss 0.26068025827407837\n",
      "[Training Epoch 1] Batch 1554, Loss 0.31820374727249146\n",
      "[Training Epoch 1] Batch 1555, Loss 0.26170212030410767\n",
      "[Training Epoch 1] Batch 1556, Loss 0.3137695789337158\n",
      "[Training Epoch 1] Batch 1557, Loss 0.29792606830596924\n",
      "[Training Epoch 1] Batch 1558, Loss 0.29414522647857666\n",
      "[Training Epoch 1] Batch 1559, Loss 0.32335764169692993\n",
      "[Training Epoch 1] Batch 1560, Loss 0.3018582761287689\n",
      "[Training Epoch 1] Batch 1561, Loss 0.2902071475982666\n",
      "[Training Epoch 1] Batch 1562, Loss 0.3073073625564575\n",
      "[Training Epoch 1] Batch 1563, Loss 0.2847760319709778\n",
      "[Training Epoch 1] Batch 1564, Loss 0.3083847761154175\n",
      "[Training Epoch 1] Batch 1565, Loss 0.30360186100006104\n",
      "[Training Epoch 1] Batch 1566, Loss 0.289694607257843\n",
      "[Training Epoch 1] Batch 1567, Loss 0.2874583601951599\n",
      "[Training Epoch 1] Batch 1568, Loss 0.29846107959747314\n",
      "[Training Epoch 1] Batch 1569, Loss 0.265691339969635\n",
      "[Training Epoch 1] Batch 1570, Loss 0.28096479177474976\n",
      "[Training Epoch 1] Batch 1571, Loss 0.3169756531715393\n",
      "[Training Epoch 1] Batch 1572, Loss 0.29039275646209717\n",
      "[Training Epoch 1] Batch 1573, Loss 0.2999202609062195\n",
      "[Training Epoch 1] Batch 1574, Loss 0.2924589216709137\n",
      "[Training Epoch 1] Batch 1575, Loss 0.3007615804672241\n",
      "[Training Epoch 1] Batch 1576, Loss 0.30134105682373047\n",
      "[Training Epoch 1] Batch 1577, Loss 0.28757768869400024\n",
      "[Training Epoch 1] Batch 1578, Loss 0.28671759366989136\n",
      "[Training Epoch 1] Batch 1579, Loss 0.2775341868400574\n",
      "[Training Epoch 1] Batch 1580, Loss 0.31418031454086304\n",
      "[Training Epoch 1] Batch 1581, Loss 0.29028183221817017\n",
      "[Training Epoch 1] Batch 1582, Loss 0.3234730660915375\n",
      "[Training Epoch 1] Batch 1583, Loss 0.2744440734386444\n",
      "[Training Epoch 1] Batch 1584, Loss 0.26770585775375366\n",
      "[Training Epoch 1] Batch 1585, Loss 0.28832244873046875\n",
      "[Training Epoch 1] Batch 1586, Loss 0.27944397926330566\n",
      "[Training Epoch 1] Batch 1587, Loss 0.30095914006233215\n",
      "[Training Epoch 1] Batch 1588, Loss 0.3027884364128113\n",
      "[Training Epoch 1] Batch 1589, Loss 0.319685697555542\n",
      "[Training Epoch 1] Batch 1590, Loss 0.30902794003486633\n",
      "[Training Epoch 1] Batch 1591, Loss 0.278067946434021\n",
      "[Training Epoch 1] Batch 1592, Loss 0.3079991042613983\n",
      "[Training Epoch 1] Batch 1593, Loss 0.273164302110672\n",
      "[Training Epoch 1] Batch 1594, Loss 0.30182963609695435\n",
      "[Training Epoch 1] Batch 1595, Loss 0.3230462372303009\n",
      "[Training Epoch 1] Batch 1596, Loss 0.3011521100997925\n",
      "[Training Epoch 1] Batch 1597, Loss 0.2856781482696533\n",
      "[Training Epoch 1] Batch 1598, Loss 0.3095940351486206\n",
      "[Training Epoch 1] Batch 1599, Loss 0.28587937355041504\n",
      "[Training Epoch 1] Batch 1600, Loss 0.3025059700012207\n",
      "[Training Epoch 1] Batch 1601, Loss 0.3180693984031677\n",
      "[Training Epoch 1] Batch 1602, Loss 0.2842804789543152\n",
      "[Training Epoch 1] Batch 1603, Loss 0.2875971794128418\n",
      "[Training Epoch 1] Batch 1604, Loss 0.27646106481552124\n",
      "[Training Epoch 1] Batch 1605, Loss 0.30204567313194275\n",
      "[Training Epoch 1] Batch 1606, Loss 0.2694343328475952\n",
      "[Training Epoch 1] Batch 1607, Loss 0.3036312758922577\n",
      "[Training Epoch 1] Batch 1608, Loss 0.3317190706729889\n",
      "[Training Epoch 1] Batch 1609, Loss 0.3204396069049835\n",
      "[Training Epoch 1] Batch 1610, Loss 0.285153329372406\n",
      "[Training Epoch 1] Batch 1611, Loss 0.28721457719802856\n",
      "[Training Epoch 1] Batch 1612, Loss 0.3369614779949188\n",
      "[Training Epoch 1] Batch 1613, Loss 0.31062814593315125\n",
      "[Training Epoch 1] Batch 1614, Loss 0.2777857184410095\n",
      "[Training Epoch 1] Batch 1615, Loss 0.2895655035972595\n",
      "[Training Epoch 1] Batch 1616, Loss 0.32495009899139404\n",
      "[Training Epoch 1] Batch 1617, Loss 0.3160085678100586\n",
      "[Training Epoch 1] Batch 1618, Loss 0.3146619200706482\n",
      "[Training Epoch 1] Batch 1619, Loss 0.28556960821151733\n",
      "[Training Epoch 1] Batch 1620, Loss 0.31903478503227234\n",
      "[Training Epoch 1] Batch 1621, Loss 0.2708158493041992\n",
      "[Training Epoch 1] Batch 1622, Loss 0.2747284471988678\n",
      "[Training Epoch 1] Batch 1623, Loss 0.29617899656295776\n",
      "[Training Epoch 1] Batch 1624, Loss 0.31978943943977356\n",
      "[Training Epoch 1] Batch 1625, Loss 0.32869744300842285\n",
      "[Training Epoch 1] Batch 1626, Loss 0.26854991912841797\n",
      "[Training Epoch 1] Batch 1627, Loss 0.27725303173065186\n",
      "[Training Epoch 1] Batch 1628, Loss 0.29135042428970337\n",
      "[Training Epoch 1] Batch 1629, Loss 0.2924776077270508\n",
      "[Training Epoch 1] Batch 1630, Loss 0.302003413438797\n",
      "[Training Epoch 1] Batch 1631, Loss 0.28433340787887573\n",
      "[Training Epoch 1] Batch 1632, Loss 0.3007766008377075\n",
      "[Training Epoch 1] Batch 1633, Loss 0.2963700294494629\n",
      "[Training Epoch 1] Batch 1634, Loss 0.30107641220092773\n",
      "[Training Epoch 1] Batch 1635, Loss 0.2801351547241211\n",
      "[Training Epoch 1] Batch 1636, Loss 0.2930537462234497\n",
      "[Training Epoch 1] Batch 1637, Loss 0.2860361635684967\n",
      "[Training Epoch 1] Batch 1638, Loss 0.29943129420280457\n",
      "[Training Epoch 1] Batch 1639, Loss 0.30621081590652466\n",
      "[Training Epoch 1] Batch 1640, Loss 0.30327126383781433\n",
      "[Training Epoch 1] Batch 1641, Loss 0.2968621551990509\n",
      "[Training Epoch 1] Batch 1642, Loss 0.2646110951900482\n",
      "[Training Epoch 1] Batch 1643, Loss 0.28637421131134033\n",
      "[Training Epoch 1] Batch 1644, Loss 0.30218154191970825\n",
      "[Training Epoch 1] Batch 1645, Loss 0.28916630148887634\n",
      "[Training Epoch 1] Batch 1646, Loss 0.28966957330703735\n",
      "[Training Epoch 1] Batch 1647, Loss 0.29765886068344116\n",
      "[Training Epoch 1] Batch 1648, Loss 0.3086863160133362\n",
      "[Training Epoch 1] Batch 1649, Loss 0.27502691745758057\n",
      "[Training Epoch 1] Batch 1650, Loss 0.28425323963165283\n",
      "[Training Epoch 1] Batch 1651, Loss 0.3257874548435211\n",
      "[Training Epoch 1] Batch 1652, Loss 0.30233532190322876\n",
      "[Training Epoch 1] Batch 1653, Loss 0.30852335691452026\n",
      "[Training Epoch 1] Batch 1654, Loss 0.29275116324424744\n",
      "[Training Epoch 1] Batch 1655, Loss 0.27661198377609253\n",
      "[Training Epoch 1] Batch 1656, Loss 0.27544689178466797\n",
      "[Training Epoch 1] Batch 1657, Loss 0.3028636872768402\n",
      "[Training Epoch 1] Batch 1658, Loss 0.2942081689834595\n",
      "[Training Epoch 1] Batch 1659, Loss 0.2841172218322754\n",
      "[Training Epoch 1] Batch 1660, Loss 0.30754488706588745\n",
      "[Training Epoch 1] Batch 1661, Loss 0.2854275107383728\n",
      "[Training Epoch 1] Batch 1662, Loss 0.29467707872390747\n",
      "[Training Epoch 1] Batch 1663, Loss 0.2913251221179962\n",
      "[Training Epoch 1] Batch 1664, Loss 0.29776814579963684\n",
      "[Training Epoch 1] Batch 1665, Loss 0.31159111857414246\n",
      "[Training Epoch 1] Batch 1666, Loss 0.26819324493408203\n",
      "[Training Epoch 1] Batch 1667, Loss 0.265987753868103\n",
      "[Training Epoch 1] Batch 1668, Loss 0.26418447494506836\n",
      "[Training Epoch 1] Batch 1669, Loss 0.319696843624115\n",
      "[Training Epoch 1] Batch 1670, Loss 0.26367369294166565\n",
      "[Training Epoch 1] Batch 1671, Loss 0.28461754322052\n",
      "[Training Epoch 1] Batch 1672, Loss 0.32562702894210815\n",
      "[Training Epoch 1] Batch 1673, Loss 0.2788080871105194\n",
      "[Training Epoch 1] Batch 1674, Loss 0.28475838899612427\n",
      "[Training Epoch 1] Batch 1675, Loss 0.30367812514305115\n",
      "[Training Epoch 1] Batch 1676, Loss 0.3050689697265625\n",
      "[Training Epoch 1] Batch 1677, Loss 0.30758747458457947\n",
      "[Training Epoch 1] Batch 1678, Loss 0.28291064500808716\n",
      "[Training Epoch 1] Batch 1679, Loss 0.2801046371459961\n",
      "[Training Epoch 1] Batch 1680, Loss 0.2891613245010376\n",
      "[Training Epoch 1] Batch 1681, Loss 0.30183327198028564\n",
      "[Training Epoch 1] Batch 1682, Loss 0.3045324683189392\n",
      "[Training Epoch 1] Batch 1683, Loss 0.28132110834121704\n",
      "[Training Epoch 1] Batch 1684, Loss 0.30357688665390015\n",
      "[Training Epoch 1] Batch 1685, Loss 0.33849093317985535\n",
      "[Training Epoch 1] Batch 1686, Loss 0.3003755211830139\n",
      "[Training Epoch 1] Batch 1687, Loss 0.2863399386405945\n",
      "[Training Epoch 1] Batch 1688, Loss 0.28464552760124207\n",
      "[Training Epoch 1] Batch 1689, Loss 0.3028663992881775\n",
      "[Training Epoch 1] Batch 1690, Loss 0.3162931799888611\n",
      "[Training Epoch 1] Batch 1691, Loss 0.2689639627933502\n",
      "[Training Epoch 1] Batch 1692, Loss 0.2569636106491089\n",
      "[Training Epoch 1] Batch 1693, Loss 0.30115097761154175\n",
      "[Training Epoch 1] Batch 1694, Loss 0.28270745277404785\n",
      "[Training Epoch 1] Batch 1695, Loss 0.29032349586486816\n",
      "[Training Epoch 1] Batch 1696, Loss 0.31810009479522705\n",
      "[Training Epoch 1] Batch 1697, Loss 0.3018385171890259\n",
      "[Training Epoch 1] Batch 1698, Loss 0.2849392890930176\n",
      "[Training Epoch 1] Batch 1699, Loss 0.3029381036758423\n",
      "[Training Epoch 1] Batch 1700, Loss 0.2898752689361572\n",
      "[Training Epoch 1] Batch 1701, Loss 0.30149364471435547\n",
      "[Training Epoch 1] Batch 1702, Loss 0.29383498430252075\n",
      "[Training Epoch 1] Batch 1703, Loss 0.27918317914009094\n",
      "[Training Epoch 1] Batch 1704, Loss 0.30529582500457764\n",
      "[Training Epoch 1] Batch 1705, Loss 0.3106473982334137\n",
      "[Training Epoch 1] Batch 1706, Loss 0.3344700336456299\n",
      "[Training Epoch 1] Batch 1707, Loss 0.30416685342788696\n",
      "[Training Epoch 1] Batch 1708, Loss 0.31634098291397095\n",
      "[Training Epoch 1] Batch 1709, Loss 0.2927842140197754\n",
      "[Training Epoch 1] Batch 1710, Loss 0.28382134437561035\n",
      "[Training Epoch 1] Batch 1711, Loss 0.2860754132270813\n",
      "[Training Epoch 1] Batch 1712, Loss 0.30866265296936035\n",
      "[Training Epoch 1] Batch 1713, Loss 0.2955021858215332\n",
      "[Training Epoch 1] Batch 1714, Loss 0.2867479920387268\n",
      "[Training Epoch 1] Batch 1715, Loss 0.26435643434524536\n",
      "[Training Epoch 1] Batch 1716, Loss 0.308938592672348\n",
      "[Training Epoch 1] Batch 1717, Loss 0.2874751687049866\n",
      "[Training Epoch 1] Batch 1718, Loss 0.3172130584716797\n",
      "[Training Epoch 1] Batch 1719, Loss 0.3049725294113159\n",
      "[Training Epoch 1] Batch 1720, Loss 0.2626977860927582\n",
      "[Training Epoch 1] Batch 1721, Loss 0.30338627099990845\n",
      "[Training Epoch 1] Batch 1722, Loss 0.3113943636417389\n",
      "[Training Epoch 1] Batch 1723, Loss 0.3033398985862732\n",
      "[Training Epoch 1] Batch 1724, Loss 0.30786776542663574\n",
      "[Training Epoch 1] Batch 1725, Loss 0.30875858664512634\n",
      "[Training Epoch 1] Batch 1726, Loss 0.2875658869743347\n",
      "[Training Epoch 1] Batch 1727, Loss 0.3023480474948883\n",
      "[Training Epoch 1] Batch 1728, Loss 0.3030204772949219\n",
      "[Training Epoch 1] Batch 1729, Loss 0.3188018202781677\n",
      "[Training Epoch 1] Batch 1730, Loss 0.2962985038757324\n",
      "[Training Epoch 1] Batch 1731, Loss 0.28956544399261475\n",
      "[Training Epoch 1] Batch 1732, Loss 0.27711769938468933\n",
      "[Training Epoch 1] Batch 1733, Loss 0.29075103998184204\n",
      "[Training Epoch 1] Batch 1734, Loss 0.29050230979919434\n",
      "[Training Epoch 1] Batch 1735, Loss 0.26767653226852417\n",
      "[Training Epoch 1] Batch 1736, Loss 0.2947198152542114\n",
      "[Training Epoch 1] Batch 1737, Loss 0.2819303274154663\n",
      "[Training Epoch 1] Batch 1738, Loss 0.286304771900177\n",
      "[Training Epoch 1] Batch 1739, Loss 0.31266987323760986\n",
      "[Training Epoch 1] Batch 1740, Loss 0.3075854778289795\n",
      "[Training Epoch 1] Batch 1741, Loss 0.3133310079574585\n",
      "[Training Epoch 1] Batch 1742, Loss 0.30860331654548645\n",
      "[Training Epoch 1] Batch 1743, Loss 0.2983112037181854\n",
      "[Training Epoch 1] Batch 1744, Loss 0.2789839804172516\n",
      "[Training Epoch 1] Batch 1745, Loss 0.2759118378162384\n",
      "[Training Epoch 1] Batch 1746, Loss 0.2685878276824951\n",
      "[Training Epoch 1] Batch 1747, Loss 0.3018011152744293\n",
      "[Training Epoch 1] Batch 1748, Loss 0.313506156206131\n",
      "[Training Epoch 1] Batch 1749, Loss 0.30154162645339966\n",
      "[Training Epoch 1] Batch 1750, Loss 0.2986510097980499\n",
      "[Training Epoch 1] Batch 1751, Loss 0.2890534996986389\n",
      "[Training Epoch 1] Batch 1752, Loss 0.2742820978164673\n",
      "[Training Epoch 1] Batch 1753, Loss 0.2850266098976135\n",
      "[Training Epoch 1] Batch 1754, Loss 0.31633996963500977\n",
      "[Training Epoch 1] Batch 1755, Loss 0.29653820395469666\n",
      "[Training Epoch 1] Batch 1756, Loss 0.2674969434738159\n",
      "[Training Epoch 1] Batch 1757, Loss 0.2997778654098511\n",
      "[Training Epoch 1] Batch 1758, Loss 0.28634387254714966\n",
      "[Training Epoch 1] Batch 1759, Loss 0.3085189163684845\n",
      "[Training Epoch 1] Batch 1760, Loss 0.27911972999572754\n",
      "[Training Epoch 1] Batch 1761, Loss 0.29229241609573364\n",
      "[Training Epoch 1] Batch 1762, Loss 0.29171884059906006\n",
      "[Training Epoch 1] Batch 1763, Loss 0.28362756967544556\n",
      "[Training Epoch 1] Batch 1764, Loss 0.3319614827632904\n",
      "[Training Epoch 1] Batch 1765, Loss 0.3042903542518616\n",
      "[Training Epoch 1] Batch 1766, Loss 0.26182740926742554\n",
      "[Training Epoch 1] Batch 1767, Loss 0.2901701331138611\n",
      "[Training Epoch 1] Batch 1768, Loss 0.30930742621421814\n",
      "[Training Epoch 1] Batch 1769, Loss 0.2861557900905609\n",
      "[Training Epoch 1] Batch 1770, Loss 0.280392587184906\n",
      "[Training Epoch 1] Batch 1771, Loss 0.3067466616630554\n",
      "[Training Epoch 1] Batch 1772, Loss 0.2576184570789337\n",
      "[Training Epoch 1] Batch 1773, Loss 0.29701176285743713\n",
      "[Training Epoch 1] Batch 1774, Loss 0.28100132942199707\n",
      "[Training Epoch 1] Batch 1775, Loss 0.2780895233154297\n",
      "[Training Epoch 1] Batch 1776, Loss 0.306001216173172\n",
      "[Training Epoch 1] Batch 1777, Loss 0.3017203211784363\n",
      "[Training Epoch 1] Batch 1778, Loss 0.27538830041885376\n",
      "[Training Epoch 1] Batch 1779, Loss 0.29303890466690063\n",
      "[Training Epoch 1] Batch 1780, Loss 0.2779441475868225\n",
      "[Training Epoch 1] Batch 1781, Loss 0.2852025628089905\n",
      "[Training Epoch 1] Batch 1782, Loss 0.2594383955001831\n",
      "[Training Epoch 1] Batch 1783, Loss 0.28540685772895813\n",
      "[Training Epoch 1] Batch 1784, Loss 0.3111070692539215\n",
      "[Training Epoch 1] Batch 1785, Loss 0.27542543411254883\n",
      "[Training Epoch 1] Batch 1786, Loss 0.2767525315284729\n",
      "[Training Epoch 1] Batch 1787, Loss 0.27583545446395874\n",
      "[Training Epoch 1] Batch 1788, Loss 0.2817589044570923\n",
      "[Training Epoch 1] Batch 1789, Loss 0.2741760313510895\n",
      "[Training Epoch 1] Batch 1790, Loss 0.2719501256942749\n",
      "[Training Epoch 1] Batch 1791, Loss 0.33198976516723633\n",
      "[Training Epoch 1] Batch 1792, Loss 0.31940680742263794\n",
      "[Training Epoch 1] Batch 1793, Loss 0.30629807710647583\n",
      "[Training Epoch 1] Batch 1794, Loss 0.28820013999938965\n",
      "[Training Epoch 1] Batch 1795, Loss 0.277034193277359\n",
      "[Training Epoch 1] Batch 1796, Loss 0.28298357129096985\n",
      "[Training Epoch 1] Batch 1797, Loss 0.3019939661026001\n",
      "[Training Epoch 1] Batch 1798, Loss 0.2803395986557007\n",
      "[Training Epoch 1] Batch 1799, Loss 0.2877369225025177\n",
      "[Training Epoch 1] Batch 1800, Loss 0.3267894983291626\n",
      "[Training Epoch 1] Batch 1801, Loss 0.26404768228530884\n",
      "[Training Epoch 1] Batch 1802, Loss 0.30903109908103943\n",
      "[Training Epoch 1] Batch 1803, Loss 0.297404408454895\n",
      "[Training Epoch 1] Batch 1804, Loss 0.31057578325271606\n",
      "[Training Epoch 1] Batch 1805, Loss 0.2997053563594818\n",
      "[Training Epoch 1] Batch 1806, Loss 0.32173648476600647\n",
      "[Training Epoch 1] Batch 1807, Loss 0.31184327602386475\n",
      "[Training Epoch 1] Batch 1808, Loss 0.2852884531021118\n",
      "[Training Epoch 1] Batch 1809, Loss 0.3189886510372162\n",
      "[Training Epoch 1] Batch 1810, Loss 0.3179962635040283\n",
      "[Training Epoch 1] Batch 1811, Loss 0.2608090341091156\n",
      "[Training Epoch 1] Batch 1812, Loss 0.31424087285995483\n",
      "[Training Epoch 1] Batch 1813, Loss 0.24452528357505798\n",
      "[Training Epoch 1] Batch 1814, Loss 0.2755955457687378\n",
      "[Training Epoch 1] Batch 1815, Loss 0.279739648103714\n",
      "[Training Epoch 1] Batch 1816, Loss 0.30589452385902405\n",
      "[Training Epoch 1] Batch 1817, Loss 0.27741533517837524\n",
      "[Training Epoch 1] Batch 1818, Loss 0.31263160705566406\n",
      "[Training Epoch 1] Batch 1819, Loss 0.30106574296951294\n",
      "[Training Epoch 1] Batch 1820, Loss 0.3071720004081726\n",
      "[Training Epoch 1] Batch 1821, Loss 0.31102949380874634\n",
      "[Training Epoch 1] Batch 1822, Loss 0.29712799191474915\n",
      "[Training Epoch 1] Batch 1823, Loss 0.30975422263145447\n",
      "[Training Epoch 1] Batch 1824, Loss 0.2980424165725708\n",
      "[Training Epoch 1] Batch 1825, Loss 0.2771803140640259\n",
      "[Training Epoch 1] Batch 1826, Loss 0.3191731870174408\n",
      "[Training Epoch 1] Batch 1827, Loss 0.29961639642715454\n",
      "[Training Epoch 1] Batch 1828, Loss 0.2958005964756012\n",
      "[Training Epoch 1] Batch 1829, Loss 0.27316081523895264\n",
      "[Training Epoch 1] Batch 1830, Loss 0.2760778069496155\n",
      "[Training Epoch 1] Batch 1831, Loss 0.2727205157279968\n",
      "[Training Epoch 1] Batch 1832, Loss 0.2698325216770172\n",
      "[Training Epoch 1] Batch 1833, Loss 0.3059519827365875\n",
      "[Training Epoch 1] Batch 1834, Loss 0.3128082752227783\n",
      "[Training Epoch 1] Batch 1835, Loss 0.3036661744117737\n",
      "[Training Epoch 1] Batch 1836, Loss 0.284985214471817\n",
      "[Training Epoch 1] Batch 1837, Loss 0.254122793674469\n",
      "[Training Epoch 1] Batch 1838, Loss 0.2926807105541229\n",
      "[Training Epoch 1] Batch 1839, Loss 0.26342126727104187\n",
      "[Training Epoch 1] Batch 1840, Loss 0.33749932050704956\n",
      "[Training Epoch 1] Batch 1841, Loss 0.30024880170822144\n",
      "[Training Epoch 1] Batch 1842, Loss 0.30080828070640564\n",
      "[Training Epoch 1] Batch 1843, Loss 0.2932353615760803\n",
      "[Training Epoch 1] Batch 1844, Loss 0.2954224944114685\n",
      "[Training Epoch 1] Batch 1845, Loss 0.27743834257125854\n",
      "[Training Epoch 1] Batch 1846, Loss 0.2815908193588257\n",
      "[Training Epoch 1] Batch 1847, Loss 0.27537623047828674\n",
      "[Training Epoch 1] Batch 1848, Loss 0.26577985286712646\n",
      "[Training Epoch 1] Batch 1849, Loss 0.28843674063682556\n",
      "[Training Epoch 1] Batch 1850, Loss 0.2993093729019165\n",
      "[Training Epoch 1] Batch 1851, Loss 0.3023741543292999\n",
      "[Training Epoch 1] Batch 1852, Loss 0.2953747510910034\n",
      "[Training Epoch 1] Batch 1853, Loss 0.2589750587940216\n",
      "[Training Epoch 1] Batch 1854, Loss 0.3243134617805481\n",
      "[Training Epoch 1] Batch 1855, Loss 0.29599133133888245\n",
      "[Training Epoch 1] Batch 1856, Loss 0.27145326137542725\n",
      "[Training Epoch 1] Batch 1857, Loss 0.3173685073852539\n",
      "[Training Epoch 1] Batch 1858, Loss 0.3002696633338928\n",
      "[Training Epoch 1] Batch 1859, Loss 0.281460702419281\n",
      "[Training Epoch 1] Batch 1860, Loss 0.31333333253860474\n",
      "[Training Epoch 1] Batch 1861, Loss 0.2960139513015747\n",
      "[Training Epoch 1] Batch 1862, Loss 0.2816716432571411\n",
      "[Training Epoch 1] Batch 1863, Loss 0.28270938992500305\n",
      "[Training Epoch 1] Batch 1864, Loss 0.29161596298217773\n",
      "[Training Epoch 1] Batch 1865, Loss 0.3178724944591522\n",
      "[Training Epoch 1] Batch 1866, Loss 0.2890166640281677\n",
      "[Training Epoch 1] Batch 1867, Loss 0.3065432906150818\n",
      "[Training Epoch 1] Batch 1868, Loss 0.2919332683086395\n",
      "[Training Epoch 1] Batch 1869, Loss 0.3291154205799103\n",
      "[Training Epoch 1] Batch 1870, Loss 0.2837734818458557\n",
      "[Training Epoch 1] Batch 1871, Loss 0.2893504202365875\n",
      "[Training Epoch 1] Batch 1872, Loss 0.2874405086040497\n",
      "[Training Epoch 1] Batch 1873, Loss 0.2915932536125183\n",
      "[Training Epoch 1] Batch 1874, Loss 0.2888716161251068\n",
      "[Training Epoch 1] Batch 1875, Loss 0.25405213236808777\n",
      "[Training Epoch 1] Batch 1876, Loss 0.30264589190483093\n",
      "[Training Epoch 1] Batch 1877, Loss 0.28553715348243713\n",
      "[Training Epoch 1] Batch 1878, Loss 0.2850561738014221\n",
      "[Training Epoch 1] Batch 1879, Loss 0.2970801591873169\n",
      "[Training Epoch 1] Batch 1880, Loss 0.300014853477478\n",
      "[Training Epoch 1] Batch 1881, Loss 0.3239959180355072\n",
      "[Training Epoch 1] Batch 1882, Loss 0.3099960386753082\n",
      "[Training Epoch 1] Batch 1883, Loss 0.3111567497253418\n",
      "[Training Epoch 1] Batch 1884, Loss 0.30606281757354736\n",
      "[Training Epoch 1] Batch 1885, Loss 0.30287766456604004\n",
      "[Training Epoch 1] Batch 1886, Loss 0.2891552448272705\n",
      "[Training Epoch 1] Batch 1887, Loss 0.3168700337409973\n",
      "[Training Epoch 1] Batch 1888, Loss 0.3037802577018738\n",
      "[Training Epoch 1] Batch 1889, Loss 0.29020288586616516\n",
      "[Training Epoch 1] Batch 1890, Loss 0.30688199400901794\n",
      "[Training Epoch 1] Batch 1891, Loss 0.2748365104198456\n",
      "[Training Epoch 1] Batch 1892, Loss 0.2864535450935364\n",
      "[Training Epoch 1] Batch 1893, Loss 0.2981836795806885\n",
      "[Training Epoch 1] Batch 1894, Loss 0.28530454635620117\n",
      "[Training Epoch 1] Batch 1895, Loss 0.28208231925964355\n",
      "[Training Epoch 1] Batch 1896, Loss 0.3352644741535187\n",
      "[Training Epoch 1] Batch 1897, Loss 0.26597335934638977\n",
      "[Training Epoch 1] Batch 1898, Loss 0.2737366855144501\n",
      "[Training Epoch 1] Batch 1899, Loss 0.27698007225990295\n",
      "[Training Epoch 1] Batch 1900, Loss 0.2958531677722931\n",
      "[Training Epoch 1] Batch 1901, Loss 0.32185620069503784\n",
      "[Training Epoch 1] Batch 1902, Loss 0.3041014075279236\n",
      "[Training Epoch 1] Batch 1903, Loss 0.28151893615722656\n",
      "[Training Epoch 1] Batch 1904, Loss 0.31505370140075684\n",
      "[Training Epoch 1] Batch 1905, Loss 0.30268198251724243\n",
      "[Training Epoch 1] Batch 1906, Loss 0.2883421778678894\n",
      "[Training Epoch 1] Batch 1907, Loss 0.2807348966598511\n",
      "[Training Epoch 1] Batch 1908, Loss 0.3129855990409851\n",
      "[Training Epoch 1] Batch 1909, Loss 0.28879547119140625\n",
      "[Training Epoch 1] Batch 1910, Loss 0.2835935354232788\n",
      "[Training Epoch 1] Batch 1911, Loss 0.31701794266700745\n",
      "[Training Epoch 1] Batch 1912, Loss 0.3033026456832886\n",
      "[Training Epoch 1] Batch 1913, Loss 0.3347863554954529\n",
      "[Training Epoch 1] Batch 1914, Loss 0.2857331931591034\n",
      "[Training Epoch 1] Batch 1915, Loss 0.31796473264694214\n",
      "[Training Epoch 1] Batch 1916, Loss 0.29929032921791077\n",
      "[Training Epoch 1] Batch 1917, Loss 0.31404614448547363\n",
      "[Training Epoch 1] Batch 1918, Loss 0.2992390990257263\n",
      "[Training Epoch 1] Batch 1919, Loss 0.2757461667060852\n",
      "[Training Epoch 1] Batch 1920, Loss 0.32656997442245483\n",
      "[Training Epoch 1] Batch 1921, Loss 0.27390313148498535\n",
      "[Training Epoch 1] Batch 1922, Loss 0.29634714126586914\n",
      "[Training Epoch 1] Batch 1923, Loss 0.28209543228149414\n",
      "[Training Epoch 1] Batch 1924, Loss 0.2785194516181946\n",
      "[Training Epoch 1] Batch 1925, Loss 0.2722657322883606\n",
      "[Training Epoch 1] Batch 1926, Loss 0.33375829458236694\n",
      "[Training Epoch 1] Batch 1927, Loss 0.29291075468063354\n",
      "[Training Epoch 1] Batch 1928, Loss 0.27105265855789185\n",
      "[Training Epoch 1] Batch 1929, Loss 0.28598785400390625\n",
      "[Training Epoch 1] Batch 1930, Loss 0.2843078374862671\n",
      "[Training Epoch 1] Batch 1931, Loss 0.2964751720428467\n",
      "[Training Epoch 1] Batch 1932, Loss 0.28048837184906006\n",
      "[Training Epoch 1] Batch 1933, Loss 0.27446967363357544\n",
      "[Training Epoch 1] Batch 1934, Loss 0.31738781929016113\n",
      "[Training Epoch 1] Batch 1935, Loss 0.3215741813182831\n",
      "[Training Epoch 1] Batch 1936, Loss 0.3062627911567688\n",
      "[Training Epoch 1] Batch 1937, Loss 0.32117170095443726\n",
      "[Training Epoch 1] Batch 1938, Loss 0.28754425048828125\n",
      "[Training Epoch 1] Batch 1939, Loss 0.3096989393234253\n",
      "[Training Epoch 1] Batch 1940, Loss 0.29823896288871765\n",
      "[Training Epoch 1] Batch 1941, Loss 0.29556965827941895\n",
      "[Training Epoch 1] Batch 1942, Loss 0.28000470995903015\n",
      "[Training Epoch 1] Batch 1943, Loss 0.3168289065361023\n",
      "[Training Epoch 1] Batch 1944, Loss 0.30535104870796204\n",
      "[Training Epoch 1] Batch 1945, Loss 0.2993091344833374\n",
      "[Training Epoch 1] Batch 1946, Loss 0.29769080877304077\n",
      "[Training Epoch 1] Batch 1947, Loss 0.32921141386032104\n",
      "[Training Epoch 1] Batch 1948, Loss 0.2817615866661072\n",
      "[Training Epoch 1] Batch 1949, Loss 0.2892415523529053\n",
      "[Training Epoch 1] Batch 1950, Loss 0.3268240690231323\n",
      "[Training Epoch 1] Batch 1951, Loss 0.30053555965423584\n",
      "[Training Epoch 1] Batch 1952, Loss 0.3125886619091034\n",
      "[Training Epoch 1] Batch 1953, Loss 0.287114679813385\n",
      "[Training Epoch 1] Batch 1954, Loss 0.29456669092178345\n",
      "[Training Epoch 1] Batch 1955, Loss 0.2694002389907837\n",
      "[Training Epoch 1] Batch 1956, Loss 0.29180750250816345\n",
      "[Training Epoch 1] Batch 1957, Loss 0.28149503469467163\n",
      "[Training Epoch 1] Batch 1958, Loss 0.2764795422554016\n",
      "[Training Epoch 1] Batch 1959, Loss 0.3028673529624939\n",
      "[Training Epoch 1] Batch 1960, Loss 0.27312394976615906\n",
      "[Training Epoch 1] Batch 1961, Loss 0.2952170670032501\n",
      "[Training Epoch 1] Batch 1962, Loss 0.28675827383995056\n",
      "[Training Epoch 1] Batch 1963, Loss 0.29430091381073\n",
      "[Training Epoch 1] Batch 1964, Loss 0.3215186595916748\n",
      "[Training Epoch 1] Batch 1965, Loss 0.31147974729537964\n",
      "[Training Epoch 1] Batch 1966, Loss 0.2819614112377167\n",
      "[Training Epoch 1] Batch 1967, Loss 0.29354357719421387\n",
      "[Training Epoch 1] Batch 1968, Loss 0.2713252902030945\n",
      "[Training Epoch 1] Batch 1969, Loss 0.303753525018692\n",
      "[Training Epoch 1] Batch 1970, Loss 0.31330952048301697\n",
      "[Training Epoch 1] Batch 1971, Loss 0.3091651499271393\n",
      "[Training Epoch 1] Batch 1972, Loss 0.2806837856769562\n",
      "[Training Epoch 1] Batch 1973, Loss 0.3002333343029022\n",
      "[Training Epoch 1] Batch 1974, Loss 0.2871326804161072\n",
      "[Training Epoch 1] Batch 1975, Loss 0.29667532444000244\n",
      "[Training Epoch 1] Batch 1976, Loss 0.26680320501327515\n",
      "[Training Epoch 1] Batch 1977, Loss 0.2672687768936157\n",
      "[Training Epoch 1] Batch 1978, Loss 0.28702312707901\n",
      "[Training Epoch 1] Batch 1979, Loss 0.2649293839931488\n",
      "[Training Epoch 1] Batch 1980, Loss 0.31799420714378357\n",
      "[Training Epoch 1] Batch 1981, Loss 0.28669339418411255\n",
      "[Training Epoch 1] Batch 1982, Loss 0.2831210494041443\n",
      "[Training Epoch 1] Batch 1983, Loss 0.2602209448814392\n",
      "[Training Epoch 1] Batch 1984, Loss 0.29235297441482544\n",
      "[Training Epoch 1] Batch 1985, Loss 0.30986958742141724\n",
      "[Training Epoch 1] Batch 1986, Loss 0.30700841546058655\n",
      "[Training Epoch 1] Batch 1987, Loss 0.32482242584228516\n",
      "[Training Epoch 1] Batch 1988, Loss 0.2756027579307556\n",
      "[Training Epoch 1] Batch 1989, Loss 0.2896232008934021\n",
      "[Training Epoch 1] Batch 1990, Loss 0.2740229368209839\n",
      "[Training Epoch 1] Batch 1991, Loss 0.2929069995880127\n",
      "[Training Epoch 1] Batch 1992, Loss 0.31943458318710327\n",
      "[Training Epoch 1] Batch 1993, Loss 0.28897321224212646\n",
      "[Training Epoch 1] Batch 1994, Loss 0.2919612526893616\n",
      "[Training Epoch 1] Batch 1995, Loss 0.28407859802246094\n",
      "[Training Epoch 1] Batch 1996, Loss 0.29498806595802307\n",
      "[Training Epoch 1] Batch 1997, Loss 0.269118070602417\n",
      "[Training Epoch 1] Batch 1998, Loss 0.30912816524505615\n",
      "[Training Epoch 1] Batch 1999, Loss 0.2832352817058563\n",
      "[Training Epoch 1] Batch 2000, Loss 0.2832098603248596\n",
      "[Training Epoch 1] Batch 2001, Loss 0.3030931353569031\n",
      "[Training Epoch 1] Batch 2002, Loss 0.27425575256347656\n",
      "[Training Epoch 1] Batch 2003, Loss 0.3097541928291321\n",
      "[Training Epoch 1] Batch 2004, Loss 0.28279247879981995\n",
      "[Training Epoch 1] Batch 2005, Loss 0.2819778621196747\n",
      "[Training Epoch 1] Batch 2006, Loss 0.31323668360710144\n",
      "[Training Epoch 1] Batch 2007, Loss 0.2646667957305908\n",
      "[Training Epoch 1] Batch 2008, Loss 0.2606031596660614\n",
      "[Training Epoch 1] Batch 2009, Loss 0.29521459341049194\n",
      "[Training Epoch 1] Batch 2010, Loss 0.30483925342559814\n",
      "[Training Epoch 1] Batch 2011, Loss 0.2994789481163025\n",
      "[Training Epoch 1] Batch 2012, Loss 0.3129725754261017\n",
      "[Training Epoch 1] Batch 2013, Loss 0.33345845341682434\n",
      "[Training Epoch 1] Batch 2014, Loss 0.27950623631477356\n",
      "[Training Epoch 1] Batch 2015, Loss 0.3111414909362793\n",
      "[Training Epoch 1] Batch 2016, Loss 0.3122352659702301\n",
      "[Training Epoch 1] Batch 2017, Loss 0.28197380900382996\n",
      "[Training Epoch 1] Batch 2018, Loss 0.30856192111968994\n",
      "[Training Epoch 1] Batch 2019, Loss 0.2722294330596924\n",
      "[Training Epoch 1] Batch 2020, Loss 0.3277578353881836\n",
      "[Training Epoch 1] Batch 2021, Loss 0.25997042655944824\n",
      "[Training Epoch 1] Batch 2022, Loss 0.3187956213951111\n",
      "[Training Epoch 1] Batch 2023, Loss 0.27419787645339966\n",
      "[Training Epoch 1] Batch 2024, Loss 0.33403515815734863\n",
      "[Training Epoch 1] Batch 2025, Loss 0.30191895365715027\n",
      "[Training Epoch 1] Batch 2026, Loss 0.26702073216438293\n",
      "[Training Epoch 1] Batch 2027, Loss 0.32020103931427\n",
      "[Training Epoch 1] Batch 2028, Loss 0.2943945527076721\n",
      "[Training Epoch 1] Batch 2029, Loss 0.31016409397125244\n",
      "[Training Epoch 1] Batch 2030, Loss 0.2857361435890198\n",
      "[Training Epoch 1] Batch 2031, Loss 0.272055059671402\n",
      "[Training Epoch 1] Batch 2032, Loss 0.27353939414024353\n",
      "[Training Epoch 1] Batch 2033, Loss 0.2945719361305237\n",
      "[Training Epoch 1] Batch 2034, Loss 0.27321556210517883\n",
      "[Training Epoch 1] Batch 2035, Loss 0.28063493967056274\n",
      "[Training Epoch 1] Batch 2036, Loss 0.27390360832214355\n",
      "[Training Epoch 1] Batch 2037, Loss 0.2967718243598938\n",
      "[Training Epoch 1] Batch 2038, Loss 0.31254494190216064\n",
      "[Training Epoch 1] Batch 2039, Loss 0.2743591070175171\n",
      "[Training Epoch 1] Batch 2040, Loss 0.29882168769836426\n",
      "[Training Epoch 1] Batch 2041, Loss 0.2917559742927551\n",
      "[Training Epoch 1] Batch 2042, Loss 0.3081475794315338\n",
      "[Training Epoch 1] Batch 2043, Loss 0.31228065490722656\n",
      "[Training Epoch 1] Batch 2044, Loss 0.2693728804588318\n",
      "[Training Epoch 1] Batch 2045, Loss 0.3017366826534271\n",
      "[Training Epoch 1] Batch 2046, Loss 0.29706883430480957\n",
      "[Training Epoch 1] Batch 2047, Loss 0.30267155170440674\n",
      "[Training Epoch 1] Batch 2048, Loss 0.31249159574508667\n",
      "[Training Epoch 1] Batch 2049, Loss 0.2756943106651306\n",
      "[Training Epoch 1] Batch 2050, Loss 0.26893165707588196\n",
      "[Training Epoch 1] Batch 2051, Loss 0.2956092953681946\n",
      "[Training Epoch 1] Batch 2052, Loss 0.3124040365219116\n",
      "[Training Epoch 1] Batch 2053, Loss 0.31750839948654175\n",
      "[Training Epoch 1] Batch 2054, Loss 0.27248093485832214\n",
      "[Training Epoch 1] Batch 2055, Loss 0.3012986481189728\n",
      "[Training Epoch 1] Batch 2056, Loss 0.3021833896636963\n",
      "[Training Epoch 1] Batch 2057, Loss 0.29260367155075073\n",
      "[Training Epoch 1] Batch 2058, Loss 0.2764091491699219\n",
      "[Training Epoch 1] Batch 2059, Loss 0.28135761618614197\n",
      "[Training Epoch 1] Batch 2060, Loss 0.2822968363761902\n",
      "[Training Epoch 1] Batch 2061, Loss 0.3257206976413727\n",
      "[Training Epoch 1] Batch 2062, Loss 0.3013145625591278\n",
      "[Training Epoch 1] Batch 2063, Loss 0.26885467767715454\n",
      "[Training Epoch 1] Batch 2064, Loss 0.325303316116333\n",
      "[Training Epoch 1] Batch 2065, Loss 0.2955667972564697\n",
      "[Training Epoch 1] Batch 2066, Loss 0.30723127722740173\n",
      "[Training Epoch 1] Batch 2067, Loss 0.31094956398010254\n",
      "[Training Epoch 1] Batch 2068, Loss 0.31292396783828735\n",
      "[Training Epoch 1] Batch 2069, Loss 0.30171850323677063\n",
      "[Training Epoch 1] Batch 2070, Loss 0.34229177236557007\n",
      "[Training Epoch 1] Batch 2071, Loss 0.29142558574676514\n",
      "[Training Epoch 1] Batch 2072, Loss 0.30448174476623535\n",
      "[Training Epoch 1] Batch 2073, Loss 0.2995213270187378\n",
      "[Training Epoch 1] Batch 2074, Loss 0.2669789791107178\n",
      "[Training Epoch 1] Batch 2075, Loss 0.28963255882263184\n",
      "[Training Epoch 1] Batch 2076, Loss 0.28284937143325806\n",
      "[Training Epoch 1] Batch 2077, Loss 0.28382572531700134\n",
      "[Training Epoch 1] Batch 2078, Loss 0.2694183588027954\n",
      "[Training Epoch 1] Batch 2079, Loss 0.31253427267074585\n",
      "[Training Epoch 1] Batch 2080, Loss 0.2843928933143616\n",
      "[Training Epoch 1] Batch 2081, Loss 0.2899501621723175\n",
      "[Training Epoch 1] Batch 2082, Loss 0.3024834394454956\n",
      "[Training Epoch 1] Batch 2083, Loss 0.28359222412109375\n",
      "[Training Epoch 1] Batch 2084, Loss 0.3159578740596771\n",
      "[Training Epoch 1] Batch 2085, Loss 0.30673104524612427\n",
      "[Training Epoch 1] Batch 2086, Loss 0.3078633248806\n",
      "[Training Epoch 1] Batch 2087, Loss 0.2957993149757385\n",
      "[Training Epoch 1] Batch 2088, Loss 0.2802632749080658\n",
      "[Training Epoch 1] Batch 2089, Loss 0.2828822731971741\n",
      "[Training Epoch 1] Batch 2090, Loss 0.28886479139328003\n",
      "[Training Epoch 1] Batch 2091, Loss 0.2711092531681061\n",
      "[Training Epoch 1] Batch 2092, Loss 0.29725098609924316\n",
      "[Training Epoch 1] Batch 2093, Loss 0.30157196521759033\n",
      "[Training Epoch 1] Batch 2094, Loss 0.28173285722732544\n",
      "[Training Epoch 1] Batch 2095, Loss 0.28035950660705566\n",
      "[Training Epoch 1] Batch 2096, Loss 0.3246098756790161\n",
      "[Training Epoch 1] Batch 2097, Loss 0.29329603910446167\n",
      "[Training Epoch 1] Batch 2098, Loss 0.3061850070953369\n",
      "[Training Epoch 1] Batch 2099, Loss 0.2918381690979004\n",
      "[Training Epoch 1] Batch 2100, Loss 0.2761191725730896\n",
      "[Training Epoch 1] Batch 2101, Loss 0.2882099747657776\n",
      "[Training Epoch 1] Batch 2102, Loss 0.28611046075820923\n",
      "[Training Epoch 1] Batch 2103, Loss 0.2684585154056549\n",
      "[Training Epoch 1] Batch 2104, Loss 0.279849648475647\n",
      "[Training Epoch 1] Batch 2105, Loss 0.28681719303131104\n",
      "[Training Epoch 1] Batch 2106, Loss 0.3013530969619751\n",
      "[Training Epoch 1] Batch 2107, Loss 0.3126760721206665\n",
      "[Training Epoch 1] Batch 2108, Loss 0.32141128182411194\n",
      "[Training Epoch 1] Batch 2109, Loss 0.2530054450035095\n",
      "[Training Epoch 1] Batch 2110, Loss 0.28110718727111816\n",
      "[Training Epoch 1] Batch 2111, Loss 0.2662890553474426\n",
      "[Training Epoch 1] Batch 2112, Loss 0.2904585599899292\n",
      "[Training Epoch 1] Batch 2113, Loss 0.29253894090652466\n",
      "[Training Epoch 1] Batch 2114, Loss 0.31784093379974365\n",
      "[Training Epoch 1] Batch 2115, Loss 0.2901245355606079\n",
      "[Training Epoch 1] Batch 2116, Loss 0.301131933927536\n",
      "[Training Epoch 1] Batch 2117, Loss 0.29954230785369873\n",
      "[Training Epoch 1] Batch 2118, Loss 0.29623115062713623\n",
      "[Training Epoch 1] Batch 2119, Loss 0.29797303676605225\n",
      "[Training Epoch 1] Batch 2120, Loss 0.29620856046676636\n",
      "[Training Epoch 1] Batch 2121, Loss 0.26016825437545776\n",
      "[Training Epoch 1] Batch 2122, Loss 0.31494447588920593\n",
      "[Training Epoch 1] Batch 2123, Loss 0.29874810576438904\n",
      "[Training Epoch 1] Batch 2124, Loss 0.2894643545150757\n",
      "[Training Epoch 1] Batch 2125, Loss 0.32813748717308044\n",
      "[Training Epoch 1] Batch 2126, Loss 0.28463029861450195\n",
      "[Training Epoch 1] Batch 2127, Loss 0.27842977643013\n",
      "[Training Epoch 1] Batch 2128, Loss 0.25522205233573914\n",
      "[Training Epoch 1] Batch 2129, Loss 0.3068774342536926\n",
      "[Training Epoch 1] Batch 2130, Loss 0.2921466827392578\n",
      "[Training Epoch 1] Batch 2131, Loss 0.31846439838409424\n",
      "[Training Epoch 1] Batch 2132, Loss 0.29296422004699707\n",
      "[Training Epoch 1] Batch 2133, Loss 0.27282780408859253\n",
      "[Training Epoch 1] Batch 2134, Loss 0.3114235997200012\n",
      "[Training Epoch 1] Batch 2135, Loss 0.32861149311065674\n",
      "[Training Epoch 1] Batch 2136, Loss 0.3034557104110718\n",
      "[Training Epoch 1] Batch 2137, Loss 0.30159884691238403\n",
      "[Training Epoch 1] Batch 2138, Loss 0.2772946059703827\n",
      "[Training Epoch 1] Batch 2139, Loss 0.28544971346855164\n",
      "[Training Epoch 1] Batch 2140, Loss 0.33131951093673706\n",
      "[Training Epoch 1] Batch 2141, Loss 0.2759157419204712\n",
      "[Training Epoch 1] Batch 2142, Loss 0.2778782844543457\n",
      "[Training Epoch 1] Batch 2143, Loss 0.28521543741226196\n",
      "[Training Epoch 1] Batch 2144, Loss 0.33591029047966003\n",
      "[Training Epoch 1] Batch 2145, Loss 0.29748955368995667\n",
      "[Training Epoch 1] Batch 2146, Loss 0.3149947226047516\n",
      "[Training Epoch 1] Batch 2147, Loss 0.2957545518875122\n",
      "[Training Epoch 1] Batch 2148, Loss 0.30132758617401123\n",
      "[Training Epoch 1] Batch 2149, Loss 0.2734070420265198\n",
      "[Training Epoch 1] Batch 2150, Loss 0.3272867500782013\n",
      "[Training Epoch 1] Batch 2151, Loss 0.29932600259780884\n",
      "[Training Epoch 1] Batch 2152, Loss 0.3010715842247009\n",
      "[Training Epoch 1] Batch 2153, Loss 0.29274675250053406\n",
      "[Training Epoch 1] Batch 2154, Loss 0.3240751624107361\n",
      "[Training Epoch 1] Batch 2155, Loss 0.27494412660598755\n",
      "[Training Epoch 1] Batch 2156, Loss 0.26109999418258667\n",
      "[Training Epoch 1] Batch 2157, Loss 0.29246920347213745\n",
      "[Training Epoch 1] Batch 2158, Loss 0.28750044107437134\n",
      "[Training Epoch 1] Batch 2159, Loss 0.27809667587280273\n",
      "[Training Epoch 1] Batch 2160, Loss 0.30967098474502563\n",
      "[Training Epoch 1] Batch 2161, Loss 0.3142992854118347\n",
      "[Training Epoch 1] Batch 2162, Loss 0.28614330291748047\n",
      "[Training Epoch 1] Batch 2163, Loss 0.29775556921958923\n",
      "[Training Epoch 1] Batch 2164, Loss 0.2934199273586273\n",
      "[Training Epoch 1] Batch 2165, Loss 0.3218696713447571\n",
      "[Training Epoch 1] Batch 2166, Loss 0.31278860569000244\n",
      "[Training Epoch 1] Batch 2167, Loss 0.26225727796554565\n",
      "[Training Epoch 1] Batch 2168, Loss 0.29012569785118103\n",
      "[Training Epoch 1] Batch 2169, Loss 0.2586350440979004\n",
      "[Training Epoch 1] Batch 2170, Loss 0.2858090400695801\n",
      "[Training Epoch 1] Batch 2171, Loss 0.2981874346733093\n",
      "[Training Epoch 1] Batch 2172, Loss 0.2967006266117096\n",
      "[Training Epoch 1] Batch 2173, Loss 0.2857185900211334\n",
      "[Training Epoch 1] Batch 2174, Loss 0.2964726984500885\n",
      "[Training Epoch 1] Batch 2175, Loss 0.3011445999145508\n",
      "[Training Epoch 1] Batch 2176, Loss 0.30675655603408813\n",
      "[Training Epoch 1] Batch 2177, Loss 0.2915196716785431\n",
      "[Training Epoch 1] Batch 2178, Loss 0.2982540726661682\n",
      "[Training Epoch 1] Batch 2179, Loss 0.30623745918273926\n",
      "[Training Epoch 1] Batch 2180, Loss 0.3076208829879761\n",
      "[Training Epoch 1] Batch 2181, Loss 0.2978571355342865\n",
      "[Training Epoch 1] Batch 2182, Loss 0.3136783838272095\n",
      "[Training Epoch 1] Batch 2183, Loss 0.28773602843284607\n",
      "[Training Epoch 1] Batch 2184, Loss 0.293533056974411\n",
      "[Training Epoch 1] Batch 2185, Loss 0.28111881017684937\n",
      "[Training Epoch 1] Batch 2186, Loss 0.3160606920719147\n",
      "[Training Epoch 1] Batch 2187, Loss 0.29689133167266846\n",
      "[Training Epoch 1] Batch 2188, Loss 0.32800352573394775\n",
      "[Training Epoch 1] Batch 2189, Loss 0.27704131603240967\n",
      "[Training Epoch 1] Batch 2190, Loss 0.2963119149208069\n",
      "[Training Epoch 1] Batch 2191, Loss 0.31273353099823\n",
      "[Training Epoch 1] Batch 2192, Loss 0.2774408459663391\n",
      "[Training Epoch 1] Batch 2193, Loss 0.2886766791343689\n",
      "[Training Epoch 1] Batch 2194, Loss 0.3023131489753723\n",
      "[Training Epoch 1] Batch 2195, Loss 0.2944842278957367\n",
      "[Training Epoch 1] Batch 2196, Loss 0.28579241037368774\n",
      "[Training Epoch 1] Batch 2197, Loss 0.27524465322494507\n",
      "[Training Epoch 1] Batch 2198, Loss 0.29129403829574585\n",
      "[Training Epoch 1] Batch 2199, Loss 0.29604607820510864\n",
      "[Training Epoch 1] Batch 2200, Loss 0.3147164583206177\n",
      "[Training Epoch 1] Batch 2201, Loss 0.28930437564849854\n",
      "[Training Epoch 1] Batch 2202, Loss 0.31357765197753906\n",
      "[Training Epoch 1] Batch 2203, Loss 0.26675528287887573\n",
      "[Training Epoch 1] Batch 2204, Loss 0.3212456703186035\n",
      "[Training Epoch 1] Batch 2205, Loss 0.2925063669681549\n",
      "[Training Epoch 1] Batch 2206, Loss 0.28785520792007446\n",
      "[Training Epoch 1] Batch 2207, Loss 0.2711520195007324\n",
      "[Training Epoch 1] Batch 2208, Loss 0.28535065054893494\n",
      "[Training Epoch 1] Batch 2209, Loss 0.2743588089942932\n",
      "[Training Epoch 1] Batch 2210, Loss 0.2975502014160156\n",
      "[Training Epoch 1] Batch 2211, Loss 0.2727070152759552\n",
      "[Training Epoch 1] Batch 2212, Loss 0.2853641211986542\n",
      "[Training Epoch 1] Batch 2213, Loss 0.2908858060836792\n",
      "[Training Epoch 1] Batch 2214, Loss 0.29044413566589355\n",
      "[Training Epoch 1] Batch 2215, Loss 0.26756152510643005\n",
      "[Training Epoch 1] Batch 2216, Loss 0.2843966484069824\n",
      "[Training Epoch 1] Batch 2217, Loss 0.3175216019153595\n",
      "[Training Epoch 1] Batch 2218, Loss 0.2684435248374939\n",
      "[Training Epoch 1] Batch 2219, Loss 0.29427918791770935\n",
      "[Training Epoch 1] Batch 2220, Loss 0.28896716237068176\n",
      "[Training Epoch 1] Batch 2221, Loss 0.31639185547828674\n",
      "[Training Epoch 1] Batch 2222, Loss 0.2691393792629242\n",
      "[Training Epoch 1] Batch 2223, Loss 0.276431679725647\n",
      "[Training Epoch 1] Batch 2224, Loss 0.3187267780303955\n",
      "[Training Epoch 1] Batch 2225, Loss 0.30974966287612915\n",
      "[Training Epoch 1] Batch 2226, Loss 0.2971050441265106\n",
      "[Training Epoch 1] Batch 2227, Loss 0.30579107999801636\n",
      "[Training Epoch 1] Batch 2228, Loss 0.2922423481941223\n",
      "[Training Epoch 1] Batch 2229, Loss 0.2904624342918396\n",
      "[Training Epoch 1] Batch 2230, Loss 0.2822203040122986\n",
      "[Training Epoch 1] Batch 2231, Loss 0.28973525762557983\n",
      "[Training Epoch 1] Batch 2232, Loss 0.29620450735092163\n",
      "[Training Epoch 1] Batch 2233, Loss 0.30591005086898804\n",
      "[Training Epoch 1] Batch 2234, Loss 0.31119588017463684\n",
      "[Training Epoch 1] Batch 2235, Loss 0.31836530566215515\n",
      "[Training Epoch 1] Batch 2236, Loss 0.2698349356651306\n",
      "[Training Epoch 1] Batch 2237, Loss 0.2779354751110077\n",
      "[Training Epoch 1] Batch 2238, Loss 0.2869410812854767\n",
      "[Training Epoch 1] Batch 2239, Loss 0.27996814250946045\n",
      "[Training Epoch 1] Batch 2240, Loss 0.28230351209640503\n",
      "[Training Epoch 1] Batch 2241, Loss 0.3141918480396271\n",
      "[Training Epoch 1] Batch 2242, Loss 0.2887888550758362\n",
      "[Training Epoch 1] Batch 2243, Loss 0.28722378611564636\n",
      "[Training Epoch 1] Batch 2244, Loss 0.30766037106513977\n",
      "[Training Epoch 1] Batch 2245, Loss 0.2858997583389282\n",
      "[Training Epoch 1] Batch 2246, Loss 0.29506373405456543\n",
      "[Training Epoch 1] Batch 2247, Loss 0.2939741015434265\n",
      "[Training Epoch 1] Batch 2248, Loss 0.30685508251190186\n",
      "[Training Epoch 1] Batch 2249, Loss 0.27477192878723145\n",
      "[Training Epoch 1] Batch 2250, Loss 0.2856575846672058\n",
      "[Training Epoch 1] Batch 2251, Loss 0.2891872823238373\n",
      "[Training Epoch 1] Batch 2252, Loss 0.30631667375564575\n",
      "[Training Epoch 1] Batch 2253, Loss 0.2739728093147278\n",
      "[Training Epoch 1] Batch 2254, Loss 0.29276424646377563\n",
      "[Training Epoch 1] Batch 2255, Loss 0.31363287568092346\n",
      "[Training Epoch 1] Batch 2256, Loss 0.32124340534210205\n",
      "[Training Epoch 1] Batch 2257, Loss 0.2784220576286316\n",
      "[Training Epoch 1] Batch 2258, Loss 0.28269660472869873\n",
      "[Training Epoch 1] Batch 2259, Loss 0.28698039054870605\n",
      "[Training Epoch 1] Batch 2260, Loss 0.27985239028930664\n",
      "[Training Epoch 1] Batch 2261, Loss 0.27101588249206543\n",
      "[Training Epoch 1] Batch 2262, Loss 0.28121453523635864\n",
      "[Training Epoch 1] Batch 2263, Loss 0.28685951232910156\n",
      "[Training Epoch 1] Batch 2264, Loss 0.2895333766937256\n",
      "[Training Epoch 1] Batch 2265, Loss 0.27476805448532104\n",
      "[Training Epoch 1] Batch 2266, Loss 0.296833872795105\n",
      "[Training Epoch 1] Batch 2267, Loss 0.320518434047699\n",
      "[Training Epoch 1] Batch 2268, Loss 0.26275283098220825\n",
      "[Training Epoch 1] Batch 2269, Loss 0.28284162282943726\n",
      "[Training Epoch 1] Batch 2270, Loss 0.274586945772171\n",
      "[Training Epoch 1] Batch 2271, Loss 0.29567569494247437\n",
      "[Training Epoch 1] Batch 2272, Loss 0.2851978838443756\n",
      "[Training Epoch 1] Batch 2273, Loss 0.31195399165153503\n",
      "[Training Epoch 1] Batch 2274, Loss 0.2997293472290039\n",
      "[Training Epoch 1] Batch 2275, Loss 0.2876870036125183\n",
      "[Training Epoch 1] Batch 2276, Loss 0.29878878593444824\n",
      "[Training Epoch 1] Batch 2277, Loss 0.27741098403930664\n",
      "[Training Epoch 1] Batch 2278, Loss 0.309368371963501\n",
      "[Training Epoch 1] Batch 2279, Loss 0.2992507815361023\n",
      "[Training Epoch 1] Batch 2280, Loss 0.2715406119823456\n",
      "[Training Epoch 1] Batch 2281, Loss 0.29471641778945923\n",
      "[Training Epoch 1] Batch 2282, Loss 0.29008740186691284\n",
      "[Training Epoch 1] Batch 2283, Loss 0.29697299003601074\n",
      "[Training Epoch 1] Batch 2284, Loss 0.3229377865791321\n",
      "[Training Epoch 1] Batch 2285, Loss 0.3143947124481201\n",
      "[Training Epoch 1] Batch 2286, Loss 0.28396403789520264\n",
      "[Training Epoch 1] Batch 2287, Loss 0.2684798240661621\n",
      "[Training Epoch 1] Batch 2288, Loss 0.31249791383743286\n",
      "[Training Epoch 1] Batch 2289, Loss 0.3007386326789856\n",
      "[Training Epoch 1] Batch 2290, Loss 0.276075154542923\n",
      "[Training Epoch 1] Batch 2291, Loss 0.28023865818977356\n",
      "[Training Epoch 1] Batch 2292, Loss 0.2958047389984131\n",
      "[Training Epoch 1] Batch 2293, Loss 0.2879578471183777\n",
      "[Training Epoch 1] Batch 2294, Loss 0.26287519931793213\n",
      "[Training Epoch 1] Batch 2295, Loss 0.286754310131073\n",
      "[Training Epoch 1] Batch 2296, Loss 0.3040589392185211\n",
      "[Training Epoch 1] Batch 2297, Loss 0.2832973003387451\n",
      "[Training Epoch 1] Batch 2298, Loss 0.30800920724868774\n",
      "[Training Epoch 1] Batch 2299, Loss 0.3005121648311615\n",
      "[Training Epoch 1] Batch 2300, Loss 0.2816351056098938\n",
      "[Training Epoch 1] Batch 2301, Loss 0.32935479283332825\n",
      "[Training Epoch 1] Batch 2302, Loss 0.2946847379207611\n",
      "[Training Epoch 1] Batch 2303, Loss 0.2821689248085022\n",
      "[Training Epoch 1] Batch 2304, Loss 0.28012919425964355\n",
      "[Training Epoch 1] Batch 2305, Loss 0.2979375123977661\n",
      "[Training Epoch 1] Batch 2306, Loss 0.294447124004364\n",
      "[Training Epoch 1] Batch 2307, Loss 0.29058241844177246\n",
      "[Training Epoch 1] Batch 2308, Loss 0.29365405440330505\n",
      "[Training Epoch 1] Batch 2309, Loss 0.3001915216445923\n",
      "[Training Epoch 1] Batch 2310, Loss 0.2922673523426056\n",
      "[Training Epoch 1] Batch 2311, Loss 0.3150392472743988\n",
      "[Training Epoch 1] Batch 2312, Loss 0.2982265055179596\n",
      "[Training Epoch 1] Batch 2313, Loss 0.3110206127166748\n",
      "[Training Epoch 1] Batch 2314, Loss 0.28905701637268066\n",
      "[Training Epoch 1] Batch 2315, Loss 0.2772040069103241\n",
      "[Training Epoch 1] Batch 2316, Loss 0.3216167092323303\n",
      "[Training Epoch 1] Batch 2317, Loss 0.3022829294204712\n",
      "[Training Epoch 1] Batch 2318, Loss 0.2925046682357788\n",
      "[Training Epoch 1] Batch 2319, Loss 0.30961358547210693\n",
      "[Training Epoch 1] Batch 2320, Loss 0.3045307695865631\n",
      "[Training Epoch 1] Batch 2321, Loss 0.26405853033065796\n",
      "[Training Epoch 1] Batch 2322, Loss 0.27529430389404297\n",
      "[Training Epoch 1] Batch 2323, Loss 0.3111324906349182\n",
      "[Training Epoch 1] Batch 2324, Loss 0.2789301574230194\n",
      "[Training Epoch 1] Batch 2325, Loss 0.2834312319755554\n",
      "[Training Epoch 1] Batch 2326, Loss 0.2823534607887268\n",
      "[Training Epoch 1] Batch 2327, Loss 0.2989024817943573\n",
      "[Training Epoch 1] Batch 2328, Loss 0.261640727519989\n",
      "[Training Epoch 1] Batch 2329, Loss 0.2780458927154541\n",
      "[Training Epoch 1] Batch 2330, Loss 0.29599836468696594\n",
      "[Training Epoch 1] Batch 2331, Loss 0.2893217206001282\n",
      "[Training Epoch 1] Batch 2332, Loss 0.2697296738624573\n",
      "[Training Epoch 1] Batch 2333, Loss 0.262362003326416\n",
      "[Training Epoch 1] Batch 2334, Loss 0.3382332921028137\n",
      "[Training Epoch 1] Batch 2335, Loss 0.29819387197494507\n",
      "[Training Epoch 1] Batch 2336, Loss 0.28624987602233887\n",
      "[Training Epoch 1] Batch 2337, Loss 0.29388225078582764\n",
      "[Training Epoch 1] Batch 2338, Loss 0.26835817098617554\n",
      "[Training Epoch 1] Batch 2339, Loss 0.2823609709739685\n",
      "[Training Epoch 1] Batch 2340, Loss 0.27419817447662354\n",
      "[Training Epoch 1] Batch 2341, Loss 0.26274701952934265\n",
      "[Training Epoch 1] Batch 2342, Loss 0.29626113176345825\n",
      "[Training Epoch 1] Batch 2343, Loss 0.27501797676086426\n",
      "[Training Epoch 1] Batch 2344, Loss 0.3383740782737732\n",
      "[Training Epoch 1] Batch 2345, Loss 0.2978460192680359\n",
      "[Training Epoch 1] Batch 2346, Loss 0.28209662437438965\n",
      "[Training Epoch 1] Batch 2347, Loss 0.2938076853752136\n",
      "[Training Epoch 1] Batch 2348, Loss 0.2936156988143921\n",
      "[Training Epoch 1] Batch 2349, Loss 0.2975603938102722\n",
      "[Training Epoch 1] Batch 2350, Loss 0.2699905335903168\n",
      "[Training Epoch 1] Batch 2351, Loss 0.2872839570045471\n",
      "[Training Epoch 1] Batch 2352, Loss 0.3378761410713196\n",
      "[Training Epoch 1] Batch 2353, Loss 0.29237598180770874\n",
      "[Training Epoch 1] Batch 2354, Loss 0.3043539524078369\n",
      "[Training Epoch 1] Batch 2355, Loss 0.30881762504577637\n",
      "[Training Epoch 1] Batch 2356, Loss 0.2977822422981262\n",
      "[Training Epoch 1] Batch 2357, Loss 0.3182506561279297\n",
      "[Training Epoch 1] Batch 2358, Loss 0.2755838632583618\n",
      "[Training Epoch 1] Batch 2359, Loss 0.3031613230705261\n",
      "[Training Epoch 1] Batch 2360, Loss 0.2869161367416382\n",
      "[Training Epoch 1] Batch 2361, Loss 0.2693282663822174\n",
      "[Training Epoch 1] Batch 2362, Loss 0.2979559004306793\n",
      "[Training Epoch 1] Batch 2363, Loss 0.27300840616226196\n",
      "[Training Epoch 1] Batch 2364, Loss 0.2700223922729492\n",
      "[Training Epoch 1] Batch 2365, Loss 0.30369359254837036\n",
      "[Training Epoch 1] Batch 2366, Loss 0.2995990514755249\n",
      "[Training Epoch 1] Batch 2367, Loss 0.2989922761917114\n",
      "[Training Epoch 1] Batch 2368, Loss 0.24719849228858948\n",
      "[Training Epoch 1] Batch 2369, Loss 0.28334474563598633\n",
      "[Training Epoch 1] Batch 2370, Loss 0.2899842858314514\n",
      "[Training Epoch 1] Batch 2371, Loss 0.29494160413742065\n",
      "[Training Epoch 1] Batch 2372, Loss 0.2982414662837982\n",
      "[Training Epoch 1] Batch 2373, Loss 0.26439860463142395\n",
      "[Training Epoch 1] Batch 2374, Loss 0.358031690120697\n",
      "[Training Epoch 1] Batch 2375, Loss 0.30607467889785767\n",
      "[Training Epoch 1] Batch 2376, Loss 0.29399001598358154\n",
      "[Training Epoch 1] Batch 2377, Loss 0.29976505041122437\n",
      "[Training Epoch 1] Batch 2378, Loss 0.2954934239387512\n",
      "[Training Epoch 1] Batch 2379, Loss 0.3278956115245819\n",
      "[Training Epoch 1] Batch 2380, Loss 0.2730170786380768\n",
      "[Training Epoch 1] Batch 2381, Loss 0.29675421118736267\n",
      "[Training Epoch 1] Batch 2382, Loss 0.2910093367099762\n",
      "[Training Epoch 1] Batch 2383, Loss 0.27762097120285034\n",
      "[Training Epoch 1] Batch 2384, Loss 0.28607362508773804\n",
      "[Training Epoch 1] Batch 2385, Loss 0.31296104192733765\n",
      "[Training Epoch 1] Batch 2386, Loss 0.27502134442329407\n",
      "[Training Epoch 1] Batch 2387, Loss 0.285480260848999\n",
      "[Training Epoch 1] Batch 2388, Loss 0.3003779649734497\n",
      "[Training Epoch 1] Batch 2389, Loss 0.3085291087627411\n",
      "[Training Epoch 1] Batch 2390, Loss 0.2773805856704712\n",
      "[Training Epoch 1] Batch 2391, Loss 0.29722943902015686\n",
      "[Training Epoch 1] Batch 2392, Loss 0.30434489250183105\n",
      "[Training Epoch 1] Batch 2393, Loss 0.2659701704978943\n",
      "[Training Epoch 1] Batch 2394, Loss 0.2923169732093811\n",
      "[Training Epoch 1] Batch 2395, Loss 0.29402050375938416\n",
      "[Training Epoch 1] Batch 2396, Loss 0.3037002384662628\n",
      "[Training Epoch 1] Batch 2397, Loss 0.29390835762023926\n",
      "[Training Epoch 1] Batch 2398, Loss 0.27534398436546326\n",
      "[Training Epoch 1] Batch 2399, Loss 0.2768775224685669\n",
      "[Training Epoch 1] Batch 2400, Loss 0.2800995111465454\n",
      "[Training Epoch 1] Batch 2401, Loss 0.29776114225387573\n",
      "[Training Epoch 1] Batch 2402, Loss 0.2994336485862732\n",
      "[Training Epoch 1] Batch 2403, Loss 0.3233935236930847\n",
      "[Training Epoch 1] Batch 2404, Loss 0.2788947820663452\n",
      "[Training Epoch 1] Batch 2405, Loss 0.3187487721443176\n",
      "[Training Epoch 1] Batch 2406, Loss 0.2861369252204895\n",
      "[Training Epoch 1] Batch 2407, Loss 0.29453325271606445\n",
      "[Training Epoch 1] Batch 2408, Loss 0.2960703670978546\n",
      "[Training Epoch 1] Batch 2409, Loss 0.2864260673522949\n",
      "[Training Epoch 1] Batch 2410, Loss 0.28179147839546204\n",
      "[Training Epoch 1] Batch 2411, Loss 0.31748104095458984\n",
      "[Training Epoch 1] Batch 2412, Loss 0.30101174116134644\n",
      "[Training Epoch 1] Batch 2413, Loss 0.28542062640190125\n",
      "[Training Epoch 1] Batch 2414, Loss 0.28466665744781494\n",
      "[Training Epoch 1] Batch 2415, Loss 0.3205103576183319\n",
      "[Training Epoch 1] Batch 2416, Loss 0.29891645908355713\n",
      "[Training Epoch 1] Batch 2417, Loss 0.28783100843429565\n",
      "[Training Epoch 1] Batch 2418, Loss 0.3102397620677948\n",
      "[Training Epoch 1] Batch 2419, Loss 0.2967865467071533\n",
      "[Training Epoch 1] Batch 2420, Loss 0.3384152054786682\n",
      "[Training Epoch 1] Batch 2421, Loss 0.2808273136615753\n",
      "[Training Epoch 1] Batch 2422, Loss 0.3055194914340973\n",
      "[Training Epoch 1] Batch 2423, Loss 0.27973663806915283\n",
      "[Training Epoch 1] Batch 2424, Loss 0.2889510989189148\n",
      "[Training Epoch 1] Batch 2425, Loss 0.32289451360702515\n",
      "[Training Epoch 1] Batch 2426, Loss 0.268858939409256\n",
      "[Training Epoch 1] Batch 2427, Loss 0.2827429473400116\n",
      "[Training Epoch 1] Batch 2428, Loss 0.2916010320186615\n",
      "[Training Epoch 1] Batch 2429, Loss 0.2965667247772217\n",
      "[Training Epoch 1] Batch 2430, Loss 0.31586503982543945\n",
      "[Training Epoch 1] Batch 2431, Loss 0.2977452278137207\n",
      "[Training Epoch 1] Batch 2432, Loss 0.29771849513053894\n",
      "[Training Epoch 1] Batch 2433, Loss 0.3086310625076294\n",
      "[Training Epoch 1] Batch 2434, Loss 0.3094855546951294\n",
      "[Training Epoch 1] Batch 2435, Loss 0.31833338737487793\n",
      "[Training Epoch 1] Batch 2436, Loss 0.2939416766166687\n",
      "[Training Epoch 1] Batch 2437, Loss 0.3190247714519501\n",
      "[Training Epoch 1] Batch 2438, Loss 0.2981777489185333\n",
      "[Training Epoch 1] Batch 2439, Loss 0.2842215895652771\n",
      "[Training Epoch 1] Batch 2440, Loss 0.27626335620880127\n",
      "[Training Epoch 1] Batch 2441, Loss 0.2927551865577698\n",
      "[Training Epoch 1] Batch 2442, Loss 0.29928451776504517\n",
      "[Training Epoch 1] Batch 2443, Loss 0.29533571004867554\n",
      "[Training Epoch 1] Batch 2444, Loss 0.30616581439971924\n",
      "[Training Epoch 1] Batch 2445, Loss 0.27960485219955444\n",
      "[Training Epoch 1] Batch 2446, Loss 0.3133659362792969\n",
      "[Training Epoch 1] Batch 2447, Loss 0.3012748062610626\n",
      "[Training Epoch 1] Batch 2448, Loss 0.2961843013763428\n",
      "[Training Epoch 1] Batch 2449, Loss 0.2704201340675354\n",
      "[Training Epoch 1] Batch 2450, Loss 0.29895856976509094\n",
      "[Training Epoch 1] Batch 2451, Loss 0.2953566312789917\n",
      "[Training Epoch 1] Batch 2452, Loss 0.26730209589004517\n",
      "[Training Epoch 1] Batch 2453, Loss 0.3050653040409088\n",
      "[Training Epoch 1] Batch 2454, Loss 0.2850615382194519\n",
      "[Training Epoch 1] Batch 2455, Loss 0.3109055161476135\n",
      "[Training Epoch 1] Batch 2456, Loss 0.3324146270751953\n",
      "[Training Epoch 1] Batch 2457, Loss 0.32379698753356934\n",
      "[Training Epoch 1] Batch 2458, Loss 0.3048035204410553\n",
      "[Training Epoch 1] Batch 2459, Loss 0.29066455364227295\n",
      "[Training Epoch 1] Batch 2460, Loss 0.2909488081932068\n",
      "[Training Epoch 1] Batch 2461, Loss 0.2995264232158661\n",
      "[Training Epoch 1] Batch 2462, Loss 0.3008478879928589\n",
      "[Training Epoch 1] Batch 2463, Loss 0.30766382813453674\n",
      "[Training Epoch 1] Batch 2464, Loss 0.29187044501304626\n",
      "[Training Epoch 1] Batch 2465, Loss 0.2904699742794037\n",
      "[Training Epoch 1] Batch 2466, Loss 0.2980765104293823\n",
      "[Training Epoch 1] Batch 2467, Loss 0.3012494742870331\n",
      "[Training Epoch 1] Batch 2468, Loss 0.3028128743171692\n",
      "[Training Epoch 1] Batch 2469, Loss 0.3049566149711609\n",
      "[Training Epoch 1] Batch 2470, Loss 0.2905963957309723\n",
      "[Training Epoch 1] Batch 2471, Loss 0.2767687141895294\n",
      "[Training Epoch 1] Batch 2472, Loss 0.28643935918807983\n",
      "[Training Epoch 1] Batch 2473, Loss 0.3086569309234619\n",
      "[Training Epoch 1] Batch 2474, Loss 0.2973221242427826\n",
      "[Training Epoch 1] Batch 2475, Loss 0.30286064743995667\n",
      "[Training Epoch 1] Batch 2476, Loss 0.2948651909828186\n",
      "[Training Epoch 1] Batch 2477, Loss 0.2928099036216736\n",
      "[Training Epoch 1] Batch 2478, Loss 0.27971088886260986\n",
      "[Training Epoch 1] Batch 2479, Loss 0.3168100118637085\n",
      "[Training Epoch 1] Batch 2480, Loss 0.25908899307250977\n",
      "[Training Epoch 1] Batch 2481, Loss 0.30760395526885986\n",
      "[Training Epoch 1] Batch 2482, Loss 0.2704123258590698\n",
      "[Training Epoch 1] Batch 2483, Loss 0.29011070728302\n",
      "[Training Epoch 1] Batch 2484, Loss 0.3020552694797516\n",
      "[Training Epoch 1] Batch 2485, Loss 0.2850129008293152\n",
      "[Training Epoch 1] Batch 2486, Loss 0.30302995443344116\n",
      "[Training Epoch 1] Batch 2487, Loss 0.304171085357666\n",
      "[Training Epoch 1] Batch 2488, Loss 0.29078420996665955\n",
      "[Training Epoch 1] Batch 2489, Loss 0.3003750443458557\n",
      "[Training Epoch 1] Batch 2490, Loss 0.2838691174983978\n",
      "[Training Epoch 1] Batch 2491, Loss 0.30265963077545166\n",
      "[Training Epoch 1] Batch 2492, Loss 0.28389209508895874\n",
      "[Training Epoch 1] Batch 2493, Loss 0.26802921295166016\n",
      "[Training Epoch 1] Batch 2494, Loss 0.29902130365371704\n",
      "[Training Epoch 1] Batch 2495, Loss 0.2726514935493469\n",
      "[Training Epoch 1] Batch 2496, Loss 0.3217701315879822\n",
      "[Training Epoch 1] Batch 2497, Loss 0.29236966371536255\n",
      "[Training Epoch 1] Batch 2498, Loss 0.29673662781715393\n",
      "[Training Epoch 1] Batch 2499, Loss 0.3108360767364502\n",
      "[Training Epoch 1] Batch 2500, Loss 0.28567880392074585\n",
      "[Training Epoch 1] Batch 2501, Loss 0.2902904748916626\n",
      "[Training Epoch 1] Batch 2502, Loss 0.3207046389579773\n",
      "[Training Epoch 1] Batch 2503, Loss 0.2953971028327942\n",
      "[Training Epoch 1] Batch 2504, Loss 0.28183305263519287\n",
      "[Training Epoch 1] Batch 2505, Loss 0.2547261416912079\n",
      "[Training Epoch 1] Batch 2506, Loss 0.31136393547058105\n",
      "[Training Epoch 1] Batch 2507, Loss 0.3150716722011566\n",
      "[Training Epoch 1] Batch 2508, Loss 0.31507551670074463\n",
      "[Training Epoch 1] Batch 2509, Loss 0.2799009084701538\n",
      "[Training Epoch 1] Batch 2510, Loss 0.32376301288604736\n",
      "[Training Epoch 1] Batch 2511, Loss 0.29013901948928833\n",
      "[Training Epoch 1] Batch 2512, Loss 0.2807115912437439\n",
      "[Training Epoch 1] Batch 2513, Loss 0.30255773663520813\n",
      "[Training Epoch 1] Batch 2514, Loss 0.2692004442214966\n",
      "[Training Epoch 1] Batch 2515, Loss 0.2995239496231079\n",
      "[Training Epoch 1] Batch 2516, Loss 0.31889352202415466\n",
      "[Training Epoch 1] Batch 2517, Loss 0.27759918570518494\n",
      "[Training Epoch 1] Batch 2518, Loss 0.2811869978904724\n",
      "[Training Epoch 1] Batch 2519, Loss 0.30297398567199707\n",
      "[Training Epoch 1] Batch 2520, Loss 0.3029731214046478\n",
      "[Training Epoch 1] Batch 2521, Loss 0.29358425736427307\n",
      "[Training Epoch 1] Batch 2522, Loss 0.2559581398963928\n",
      "[Training Epoch 1] Batch 2523, Loss 0.28885209560394287\n",
      "[Training Epoch 1] Batch 2524, Loss 0.30750173330307007\n",
      "[Training Epoch 1] Batch 2525, Loss 0.2546676993370056\n",
      "[Training Epoch 1] Batch 2526, Loss 0.2923552095890045\n",
      "[Training Epoch 1] Batch 2527, Loss 0.31165117025375366\n",
      "[Training Epoch 1] Batch 2528, Loss 0.2871873378753662\n",
      "[Training Epoch 1] Batch 2529, Loss 0.31929272413253784\n",
      "[Training Epoch 1] Batch 2530, Loss 0.2875105142593384\n",
      "[Training Epoch 1] Batch 2531, Loss 0.3021489679813385\n",
      "[Training Epoch 1] Batch 2532, Loss 0.2978619933128357\n",
      "[Training Epoch 1] Batch 2533, Loss 0.29660335183143616\n",
      "[Training Epoch 1] Batch 2534, Loss 0.3043254315853119\n",
      "[Training Epoch 1] Batch 2535, Loss 0.2837066054344177\n",
      "[Training Epoch 1] Batch 2536, Loss 0.28847450017929077\n",
      "[Training Epoch 1] Batch 2537, Loss 0.30152466893196106\n",
      "[Training Epoch 1] Batch 2538, Loss 0.2728440761566162\n",
      "[Training Epoch 1] Batch 2539, Loss 0.3066578209400177\n",
      "[Training Epoch 1] Batch 2540, Loss 0.26541218161582947\n",
      "[Training Epoch 1] Batch 2541, Loss 0.2962436378002167\n",
      "[Training Epoch 1] Batch 2542, Loss 0.26818034052848816\n",
      "[Training Epoch 1] Batch 2543, Loss 0.28066158294677734\n",
      "[Training Epoch 1] Batch 2544, Loss 0.3009531497955322\n",
      "[Training Epoch 1] Batch 2545, Loss 0.30968791246414185\n",
      "[Training Epoch 1] Batch 2546, Loss 0.2984887361526489\n",
      "[Training Epoch 1] Batch 2547, Loss 0.280092716217041\n",
      "[Training Epoch 1] Batch 2548, Loss 0.2856753468513489\n",
      "[Training Epoch 1] Batch 2549, Loss 0.28916189074516296\n",
      "[Training Epoch 1] Batch 2550, Loss 0.2672628164291382\n",
      "[Training Epoch 1] Batch 2551, Loss 0.2921822965145111\n",
      "[Training Epoch 1] Batch 2552, Loss 0.28126978874206543\n",
      "[Training Epoch 1] Batch 2553, Loss 0.2794696092605591\n",
      "[Training Epoch 1] Batch 2554, Loss 0.30120599269866943\n",
      "[Training Epoch 1] Batch 2555, Loss 0.3240981101989746\n",
      "[Training Epoch 1] Batch 2556, Loss 0.2890283465385437\n",
      "[Training Epoch 1] Batch 2557, Loss 0.2930147051811218\n",
      "[Training Epoch 1] Batch 2558, Loss 0.29724007844924927\n",
      "[Training Epoch 1] Batch 2559, Loss 0.30520468950271606\n",
      "[Training Epoch 1] Batch 2560, Loss 0.29205185174942017\n",
      "[Training Epoch 1] Batch 2561, Loss 0.28764042258262634\n",
      "[Training Epoch 1] Batch 2562, Loss 0.28179293870925903\n",
      "[Training Epoch 1] Batch 2563, Loss 0.2687227725982666\n",
      "[Training Epoch 1] Batch 2564, Loss 0.2709113359451294\n",
      "[Training Epoch 1] Batch 2565, Loss 0.2925662398338318\n",
      "[Training Epoch 1] Batch 2566, Loss 0.3302622437477112\n",
      "[Training Epoch 1] Batch 2567, Loss 0.2891315221786499\n",
      "[Training Epoch 1] Batch 2568, Loss 0.31729835271835327\n",
      "[Training Epoch 1] Batch 2569, Loss 0.3016927242279053\n",
      "[Training Epoch 1] Batch 2570, Loss 0.2707764208316803\n",
      "[Training Epoch 1] Batch 2571, Loss 0.3392859697341919\n",
      "[Training Epoch 1] Batch 2572, Loss 0.2964017987251282\n",
      "[Training Epoch 1] Batch 2573, Loss 0.299157977104187\n",
      "[Training Epoch 1] Batch 2574, Loss 0.29017847776412964\n",
      "[Training Epoch 1] Batch 2575, Loss 0.3006165623664856\n",
      "[Training Epoch 1] Batch 2576, Loss 0.32006117701530457\n",
      "[Training Epoch 1] Batch 2577, Loss 0.27677130699157715\n",
      "[Training Epoch 1] Batch 2578, Loss 0.2778516411781311\n",
      "[Training Epoch 1] Batch 2579, Loss 0.2834259271621704\n",
      "[Training Epoch 1] Batch 2580, Loss 0.3067752420902252\n",
      "[Training Epoch 1] Batch 2581, Loss 0.313214510679245\n",
      "[Training Epoch 1] Batch 2582, Loss 0.29937970638275146\n",
      "[Training Epoch 1] Batch 2583, Loss 0.31924837827682495\n",
      "[Training Epoch 1] Batch 2584, Loss 0.32215991616249084\n",
      "[Training Epoch 1] Batch 2585, Loss 0.27770304679870605\n",
      "[Training Epoch 1] Batch 2586, Loss 0.2618240714073181\n",
      "[Training Epoch 1] Batch 2587, Loss 0.3002862334251404\n",
      "[Training Epoch 1] Batch 2588, Loss 0.2659030556678772\n",
      "[Training Epoch 1] Batch 2589, Loss 0.2699412703514099\n",
      "[Training Epoch 1] Batch 2590, Loss 0.2990984618663788\n",
      "[Training Epoch 1] Batch 2591, Loss 0.29393598437309265\n",
      "[Training Epoch 1] Batch 2592, Loss 0.2983943223953247\n",
      "[Training Epoch 1] Batch 2593, Loss 0.30528199672698975\n",
      "[Training Epoch 1] Batch 2594, Loss 0.293571412563324\n",
      "[Training Epoch 1] Batch 2595, Loss 0.28208136558532715\n",
      "[Training Epoch 1] Batch 2596, Loss 0.3127616047859192\n",
      "[Training Epoch 1] Batch 2597, Loss 0.29962050914764404\n",
      "[Training Epoch 1] Batch 2598, Loss 0.29912981390953064\n",
      "[Training Epoch 1] Batch 2599, Loss 0.29179149866104126\n",
      "[Training Epoch 1] Batch 2600, Loss 0.3146649897098541\n",
      "[Training Epoch 1] Batch 2601, Loss 0.29081976413726807\n",
      "[Training Epoch 1] Batch 2602, Loss 0.307273805141449\n",
      "[Training Epoch 1] Batch 2603, Loss 0.2997303605079651\n",
      "[Training Epoch 1] Batch 2604, Loss 0.28362151980400085\n",
      "[Training Epoch 1] Batch 2605, Loss 0.29970788955688477\n",
      "[Training Epoch 1] Batch 2606, Loss 0.2868295907974243\n",
      "[Training Epoch 1] Batch 2607, Loss 0.2920113205909729\n",
      "[Training Epoch 1] Batch 2608, Loss 0.3134486973285675\n",
      "[Training Epoch 1] Batch 2609, Loss 0.2934802770614624\n",
      "[Training Epoch 1] Batch 2610, Loss 0.2874802052974701\n",
      "[Training Epoch 1] Batch 2611, Loss 0.2864522635936737\n",
      "[Training Epoch 1] Batch 2612, Loss 0.2594797611236572\n",
      "[Training Epoch 1] Batch 2613, Loss 0.26458266377449036\n",
      "[Training Epoch 1] Batch 2614, Loss 0.2849094271659851\n",
      "[Training Epoch 1] Batch 2615, Loss 0.2963184118270874\n",
      "[Training Epoch 1] Batch 2616, Loss 0.28947553038597107\n",
      "[Training Epoch 1] Batch 2617, Loss 0.25820663571357727\n",
      "[Training Epoch 1] Batch 2618, Loss 0.269067645072937\n",
      "[Training Epoch 1] Batch 2619, Loss 0.2803047001361847\n",
      "[Training Epoch 1] Batch 2620, Loss 0.2798505425453186\n",
      "[Training Epoch 1] Batch 2621, Loss 0.2849494218826294\n",
      "[Training Epoch 1] Batch 2622, Loss 0.2889142334461212\n",
      "[Training Epoch 1] Batch 2623, Loss 0.28955936431884766\n",
      "[Training Epoch 1] Batch 2624, Loss 0.25424644351005554\n",
      "[Training Epoch 1] Batch 2625, Loss 0.31103143095970154\n",
      "[Training Epoch 1] Batch 2626, Loss 0.31662312150001526\n",
      "[Training Epoch 1] Batch 2627, Loss 0.2930952310562134\n",
      "[Training Epoch 1] Batch 2628, Loss 0.28153008222579956\n",
      "[Training Epoch 1] Batch 2629, Loss 0.293354332447052\n",
      "[Training Epoch 1] Batch 2630, Loss 0.2708783447742462\n",
      "[Training Epoch 1] Batch 2631, Loss 0.3158737123012543\n",
      "[Training Epoch 1] Batch 2632, Loss 0.2809312045574188\n",
      "[Training Epoch 1] Batch 2633, Loss 0.2821521759033203\n",
      "[Training Epoch 1] Batch 2634, Loss 0.28669872879981995\n",
      "[Training Epoch 1] Batch 2635, Loss 0.29328280687332153\n",
      "[Training Epoch 1] Batch 2636, Loss 0.28593963384628296\n",
      "[Training Epoch 1] Batch 2637, Loss 0.2821539640426636\n",
      "[Training Epoch 1] Batch 2638, Loss 0.3271673321723938\n",
      "[Training Epoch 1] Batch 2639, Loss 0.29546064138412476\n",
      "[Training Epoch 1] Batch 2640, Loss 0.2769134044647217\n",
      "[Training Epoch 1] Batch 2641, Loss 0.3299591541290283\n",
      "[Training Epoch 1] Batch 2642, Loss 0.2842015326023102\n",
      "[Training Epoch 1] Batch 2643, Loss 0.2998794615268707\n",
      "[Training Epoch 1] Batch 2644, Loss 0.29918381571769714\n",
      "[Training Epoch 1] Batch 2645, Loss 0.30247029662132263\n",
      "[Training Epoch 1] Batch 2646, Loss 0.2955012917518616\n",
      "[Training Epoch 1] Batch 2647, Loss 0.2765694558620453\n",
      "[Training Epoch 1] Batch 2648, Loss 0.29752203822135925\n",
      "[Training Epoch 1] Batch 2649, Loss 0.32479357719421387\n",
      "[Training Epoch 1] Batch 2650, Loss 0.2940833568572998\n",
      "[Training Epoch 1] Batch 2651, Loss 0.2791709303855896\n",
      "[Training Epoch 1] Batch 2652, Loss 0.29642146825790405\n",
      "[Training Epoch 1] Batch 2653, Loss 0.30908989906311035\n",
      "[Training Epoch 1] Batch 2654, Loss 0.2636682689189911\n",
      "[Training Epoch 1] Batch 2655, Loss 0.3017866015434265\n",
      "[Training Epoch 1] Batch 2656, Loss 0.29672279953956604\n",
      "[Training Epoch 1] Batch 2657, Loss 0.292344868183136\n",
      "[Training Epoch 1] Batch 2658, Loss 0.2892930507659912\n",
      "[Training Epoch 1] Batch 2659, Loss 0.277670681476593\n",
      "[Training Epoch 1] Batch 2660, Loss 0.29855331778526306\n",
      "[Training Epoch 1] Batch 2661, Loss 0.3061476945877075\n",
      "[Training Epoch 1] Batch 2662, Loss 0.2921815514564514\n",
      "[Training Epoch 1] Batch 2663, Loss 0.29794758558273315\n",
      "[Training Epoch 1] Batch 2664, Loss 0.28160399198532104\n",
      "[Training Epoch 1] Batch 2665, Loss 0.31461256742477417\n",
      "[Training Epoch 1] Batch 2666, Loss 0.3107683062553406\n",
      "[Training Epoch 1] Batch 2667, Loss 0.29387229681015015\n",
      "[Training Epoch 1] Batch 2668, Loss 0.2878038287162781\n",
      "[Training Epoch 1] Batch 2669, Loss 0.274963915348053\n",
      "[Training Epoch 1] Batch 2670, Loss 0.2824040651321411\n",
      "[Training Epoch 1] Batch 2671, Loss 0.29672202467918396\n",
      "[Training Epoch 1] Batch 2672, Loss 0.3094550371170044\n",
      "[Training Epoch 1] Batch 2673, Loss 0.28047406673431396\n",
      "[Training Epoch 1] Batch 2674, Loss 0.29593929648399353\n",
      "[Training Epoch 1] Batch 2675, Loss 0.2879510521888733\n",
      "[Training Epoch 1] Batch 2676, Loss 0.32381609082221985\n",
      "[Training Epoch 1] Batch 2677, Loss 0.2861826419830322\n",
      "[Training Epoch 1] Batch 2678, Loss 0.3031771779060364\n",
      "[Training Epoch 1] Batch 2679, Loss 0.28872373700141907\n",
      "[Training Epoch 1] Batch 2680, Loss 0.2865782082080841\n",
      "[Training Epoch 1] Batch 2681, Loss 0.31928738951683044\n",
      "[Training Epoch 1] Batch 2682, Loss 0.2982882261276245\n",
      "[Training Epoch 1] Batch 2683, Loss 0.3068310022354126\n",
      "[Training Epoch 1] Batch 2684, Loss 0.2997959852218628\n",
      "[Training Epoch 1] Batch 2685, Loss 0.27708613872528076\n",
      "[Training Epoch 1] Batch 2686, Loss 0.2803667485713959\n",
      "[Training Epoch 1] Batch 2687, Loss 0.2832196354866028\n",
      "[Training Epoch 1] Batch 2688, Loss 0.2941705584526062\n",
      "[Training Epoch 1] Batch 2689, Loss 0.3200041651725769\n",
      "[Training Epoch 1] Batch 2690, Loss 0.30284804105758667\n",
      "[Training Epoch 1] Batch 2691, Loss 0.3065091371536255\n",
      "[Training Epoch 1] Batch 2692, Loss 0.2897055149078369\n",
      "[Training Epoch 1] Batch 2693, Loss 0.2976152002811432\n",
      "[Training Epoch 1] Batch 2694, Loss 0.2981458306312561\n",
      "[Training Epoch 1] Batch 2695, Loss 0.3180709183216095\n",
      "[Training Epoch 1] Batch 2696, Loss 0.30435413122177124\n",
      "[Training Epoch 1] Batch 2697, Loss 0.3101087510585785\n",
      "[Training Epoch 1] Batch 2698, Loss 0.29278066754341125\n",
      "[Training Epoch 1] Batch 2699, Loss 0.3025265038013458\n",
      "[Training Epoch 1] Batch 2700, Loss 0.29937678575515747\n",
      "[Training Epoch 1] Batch 2701, Loss 0.3063852787017822\n",
      "[Training Epoch 1] Batch 2702, Loss 0.2956894040107727\n",
      "[Training Epoch 1] Batch 2703, Loss 0.28595170378685\n",
      "[Training Epoch 1] Batch 2704, Loss 0.2941012978553772\n",
      "[Training Epoch 1] Batch 2705, Loss 0.28529930114746094\n",
      "[Training Epoch 1] Batch 2706, Loss 0.29963159561157227\n",
      "[Training Epoch 1] Batch 2707, Loss 0.29391080141067505\n",
      "[Training Epoch 1] Batch 2708, Loss 0.3067832291126251\n",
      "[Training Epoch 1] Batch 2709, Loss 0.27828457951545715\n",
      "[Training Epoch 1] Batch 2710, Loss 0.2746991217136383\n",
      "[Training Epoch 1] Batch 2711, Loss 0.2752429246902466\n",
      "[Training Epoch 1] Batch 2712, Loss 0.2783706486225128\n",
      "[Training Epoch 1] Batch 2713, Loss 0.32710912823677063\n",
      "[Training Epoch 1] Batch 2714, Loss 0.29067960381507874\n",
      "[Training Epoch 1] Batch 2715, Loss 0.2794506549835205\n",
      "[Training Epoch 1] Batch 2716, Loss 0.2893217206001282\n",
      "[Training Epoch 1] Batch 2717, Loss 0.2891830801963806\n",
      "[Training Epoch 1] Batch 2718, Loss 0.25891557335853577\n",
      "[Training Epoch 1] Batch 2719, Loss 0.30438554286956787\n",
      "[Training Epoch 1] Batch 2720, Loss 0.2958044707775116\n",
      "[Training Epoch 1] Batch 2721, Loss 0.3289334774017334\n",
      "[Training Epoch 1] Batch 2722, Loss 0.2837425172328949\n",
      "[Training Epoch 1] Batch 2723, Loss 0.2943423390388489\n",
      "[Training Epoch 1] Batch 2724, Loss 0.2945106625556946\n",
      "[Training Epoch 1] Batch 2725, Loss 0.2726597785949707\n",
      "[Training Epoch 1] Batch 2726, Loss 0.32170575857162476\n",
      "[Training Epoch 1] Batch 2727, Loss 0.27999556064605713\n",
      "[Training Epoch 1] Batch 2728, Loss 0.3176993131637573\n",
      "[Training Epoch 1] Batch 2729, Loss 0.293287068605423\n",
      "[Training Epoch 1] Batch 2730, Loss 0.2990989089012146\n",
      "[Training Epoch 1] Batch 2731, Loss 0.31389230489730835\n",
      "[Training Epoch 1] Batch 2732, Loss 0.2869715988636017\n",
      "[Training Epoch 1] Batch 2733, Loss 0.2853698432445526\n",
      "[Training Epoch 1] Batch 2734, Loss 0.2897069454193115\n",
      "[Training Epoch 1] Batch 2735, Loss 0.31378549337387085\n",
      "[Training Epoch 1] Batch 2736, Loss 0.3120320439338684\n",
      "[Training Epoch 1] Batch 2737, Loss 0.26159167289733887\n",
      "[Training Epoch 1] Batch 2738, Loss 0.2812758982181549\n",
      "[Training Epoch 1] Batch 2739, Loss 0.289047509431839\n",
      "[Training Epoch 1] Batch 2740, Loss 0.30030587315559387\n",
      "[Training Epoch 1] Batch 2741, Loss 0.3104741871356964\n",
      "[Training Epoch 1] Batch 2742, Loss 0.3026807904243469\n",
      "[Training Epoch 1] Batch 2743, Loss 0.2861708402633667\n",
      "[Training Epoch 1] Batch 2744, Loss 0.2949296236038208\n",
      "[Training Epoch 1] Batch 2745, Loss 0.2984621822834015\n",
      "[Training Epoch 1] Batch 2746, Loss 0.3054889142513275\n",
      "[Training Epoch 1] Batch 2747, Loss 0.2798914909362793\n",
      "[Training Epoch 1] Batch 2748, Loss 0.29359352588653564\n",
      "[Training Epoch 1] Batch 2749, Loss 0.3249371647834778\n",
      "[Training Epoch 1] Batch 2750, Loss 0.31060492992401123\n",
      "[Training Epoch 1] Batch 2751, Loss 0.2810094952583313\n",
      "[Training Epoch 1] Batch 2752, Loss 0.2701374590396881\n",
      "[Training Epoch 1] Batch 2753, Loss 0.2996460795402527\n",
      "[Training Epoch 1] Batch 2754, Loss 0.28428998589515686\n",
      "[Training Epoch 1] Batch 2755, Loss 0.306144118309021\n",
      "[Training Epoch 1] Batch 2756, Loss 0.2499743103981018\n",
      "[Training Epoch 1] Batch 2757, Loss 0.2853856682777405\n",
      "[Training Epoch 1] Batch 2758, Loss 0.2922801673412323\n",
      "[Training Epoch 1] Batch 2759, Loss 0.31156444549560547\n",
      "[Training Epoch 1] Batch 2760, Loss 0.2854299545288086\n",
      "[Training Epoch 1] Batch 2761, Loss 0.28721049427986145\n",
      "[Training Epoch 1] Batch 2762, Loss 0.2779073417186737\n",
      "[Training Epoch 1] Batch 2763, Loss 0.3135143518447876\n",
      "[Training Epoch 1] Batch 2764, Loss 0.27420303225517273\n",
      "[Training Epoch 1] Batch 2765, Loss 0.30530112981796265\n",
      "[Training Epoch 1] Batch 2766, Loss 0.30278319120407104\n",
      "[Training Epoch 1] Batch 2767, Loss 0.2877289056777954\n",
      "[Training Epoch 1] Batch 2768, Loss 0.2866772711277008\n",
      "[Training Epoch 1] Batch 2769, Loss 0.29046911001205444\n",
      "[Training Epoch 1] Batch 2770, Loss 0.2493886798620224\n",
      "[Training Epoch 1] Batch 2771, Loss 0.30366501212120056\n",
      "[Training Epoch 1] Batch 2772, Loss 0.2893672585487366\n",
      "[Training Epoch 1] Batch 2773, Loss 0.2849159836769104\n",
      "[Training Epoch 1] Batch 2774, Loss 0.2773347496986389\n",
      "[Training Epoch 1] Batch 2775, Loss 0.29096925258636475\n",
      "[Training Epoch 1] Batch 2776, Loss 0.2834587097167969\n",
      "[Training Epoch 1] Batch 2777, Loss 0.27419131994247437\n",
      "[Training Epoch 1] Batch 2778, Loss 0.26769763231277466\n",
      "[Training Epoch 1] Batch 2779, Loss 0.3121626377105713\n",
      "[Training Epoch 1] Batch 2780, Loss 0.26580867171287537\n",
      "[Training Epoch 1] Batch 2781, Loss 0.3107717037200928\n",
      "[Training Epoch 1] Batch 2782, Loss 0.2853332757949829\n",
      "[Training Epoch 1] Batch 2783, Loss 0.29512926936149597\n",
      "[Training Epoch 1] Batch 2784, Loss 0.27793532609939575\n",
      "[Training Epoch 1] Batch 2785, Loss 0.29532551765441895\n",
      "[Training Epoch 1] Batch 2786, Loss 0.307675302028656\n",
      "[Training Epoch 1] Batch 2787, Loss 0.29994136095046997\n",
      "[Training Epoch 1] Batch 2788, Loss 0.29493454098701477\n",
      "[Training Epoch 1] Batch 2789, Loss 0.3152751922607422\n",
      "[Training Epoch 1] Batch 2790, Loss 0.30360162258148193\n",
      "[Training Epoch 1] Batch 2791, Loss 0.2976665794849396\n",
      "[Training Epoch 1] Batch 2792, Loss 0.2906806468963623\n",
      "[Training Epoch 1] Batch 2793, Loss 0.32531213760375977\n",
      "[Training Epoch 1] Batch 2794, Loss 0.2825985848903656\n",
      "[Training Epoch 1] Batch 2795, Loss 0.29681864380836487\n",
      "[Training Epoch 1] Batch 2796, Loss 0.2793687582015991\n",
      "[Training Epoch 1] Batch 2797, Loss 0.2862595319747925\n",
      "[Training Epoch 1] Batch 2798, Loss 0.28129202127456665\n",
      "[Training Epoch 1] Batch 2799, Loss 0.3044348657131195\n",
      "[Training Epoch 1] Batch 2800, Loss 0.27823489904403687\n",
      "[Training Epoch 1] Batch 2801, Loss 0.30490776896476746\n",
      "[Training Epoch 1] Batch 2802, Loss 0.32271701097488403\n",
      "[Training Epoch 1] Batch 2803, Loss 0.32321175932884216\n",
      "[Training Epoch 1] Batch 2804, Loss 0.288202702999115\n",
      "[Training Epoch 1] Batch 2805, Loss 0.2965170443058014\n",
      "[Training Epoch 1] Batch 2806, Loss 0.29749515652656555\n",
      "[Training Epoch 1] Batch 2807, Loss 0.2662285268306732\n",
      "[Training Epoch 1] Batch 2808, Loss 0.26864928007125854\n",
      "[Training Epoch 1] Batch 2809, Loss 0.29770493507385254\n",
      "[Training Epoch 1] Batch 2810, Loss 0.28942161798477173\n",
      "[Training Epoch 1] Batch 2811, Loss 0.3123739957809448\n",
      "[Training Epoch 1] Batch 2812, Loss 0.2886009216308594\n",
      "[Training Epoch 1] Batch 2813, Loss 0.2827197313308716\n",
      "[Training Epoch 1] Batch 2814, Loss 0.25744181871414185\n",
      "[Training Epoch 1] Batch 2815, Loss 0.2712213099002838\n",
      "[Training Epoch 1] Batch 2816, Loss 0.3175879120826721\n",
      "[Training Epoch 1] Batch 2817, Loss 0.29997456073760986\n",
      "[Training Epoch 1] Batch 2818, Loss 0.2883639633655548\n",
      "[Training Epoch 1] Batch 2819, Loss 0.31599533557891846\n",
      "[Training Epoch 1] Batch 2820, Loss 0.27974361181259155\n",
      "[Training Epoch 1] Batch 2821, Loss 0.28218430280685425\n",
      "[Training Epoch 1] Batch 2822, Loss 0.29645687341690063\n",
      "[Training Epoch 1] Batch 2823, Loss 0.2945301830768585\n",
      "[Training Epoch 1] Batch 2824, Loss 0.2945668399333954\n",
      "[Training Epoch 1] Batch 2825, Loss 0.29230228066444397\n",
      "[Training Epoch 1] Batch 2826, Loss 0.3107466697692871\n",
      "[Training Epoch 1] Batch 2827, Loss 0.26932477951049805\n",
      "[Training Epoch 1] Batch 2828, Loss 0.28962138295173645\n",
      "[Training Epoch 1] Batch 2829, Loss 0.28343629837036133\n",
      "[Training Epoch 1] Batch 2830, Loss 0.27795302867889404\n",
      "[Training Epoch 1] Batch 2831, Loss 0.28380605578422546\n",
      "[Training Epoch 1] Batch 2832, Loss 0.30544140934944153\n",
      "[Training Epoch 1] Batch 2833, Loss 0.2966300845146179\n",
      "[Training Epoch 1] Batch 2834, Loss 0.2930152416229248\n",
      "[Training Epoch 1] Batch 2835, Loss 0.2716238498687744\n",
      "[Training Epoch 1] Batch 2836, Loss 0.31373098492622375\n",
      "[Training Epoch 1] Batch 2837, Loss 0.3148238956928253\n",
      "[Training Epoch 1] Batch 2838, Loss 0.29515159130096436\n",
      "[Training Epoch 1] Batch 2839, Loss 0.2763732075691223\n",
      "[Training Epoch 1] Batch 2840, Loss 0.29731273651123047\n",
      "[Training Epoch 1] Batch 2841, Loss 0.29853731393814087\n",
      "[Training Epoch 1] Batch 2842, Loss 0.28389400243759155\n",
      "[Training Epoch 1] Batch 2843, Loss 0.28131335973739624\n",
      "[Training Epoch 1] Batch 2844, Loss 0.2890160381793976\n",
      "[Training Epoch 1] Batch 2845, Loss 0.29848235845565796\n",
      "[Training Epoch 1] Batch 2846, Loss 0.2989118695259094\n",
      "[Training Epoch 1] Batch 2847, Loss 0.2954642176628113\n",
      "[Training Epoch 1] Batch 2848, Loss 0.2976475954055786\n",
      "[Training Epoch 1] Batch 2849, Loss 0.30740249156951904\n",
      "[Training Epoch 1] Batch 2850, Loss 0.28475868701934814\n",
      "[Training Epoch 1] Batch 2851, Loss 0.3034400939941406\n",
      "[Training Epoch 1] Batch 2852, Loss 0.3192867338657379\n",
      "[Training Epoch 1] Batch 2853, Loss 0.2909408509731293\n",
      "[Training Epoch 1] Batch 2854, Loss 0.2919301390647888\n",
      "[Training Epoch 1] Batch 2855, Loss 0.30033013224601746\n",
      "[Training Epoch 1] Batch 2856, Loss 0.3017610013484955\n",
      "[Training Epoch 1] Batch 2857, Loss 0.29722678661346436\n",
      "[Training Epoch 1] Batch 2858, Loss 0.28319644927978516\n",
      "[Training Epoch 1] Batch 2859, Loss 0.3112298846244812\n",
      "[Training Epoch 1] Batch 2860, Loss 0.29639261960983276\n",
      "[Training Epoch 1] Batch 2861, Loss 0.2682282328605652\n",
      "[Training Epoch 1] Batch 2862, Loss 0.2699849605560303\n",
      "[Training Epoch 1] Batch 2863, Loss 0.3036160171031952\n",
      "[Training Epoch 1] Batch 2864, Loss 0.30142688751220703\n",
      "[Training Epoch 1] Batch 2865, Loss 0.27229198813438416\n",
      "[Training Epoch 1] Batch 2866, Loss 0.28428322076797485\n",
      "[Training Epoch 1] Batch 2867, Loss 0.2874200940132141\n",
      "[Training Epoch 1] Batch 2868, Loss 0.3047444522380829\n",
      "[Training Epoch 1] Batch 2869, Loss 0.30936765670776367\n",
      "[Training Epoch 1] Batch 2870, Loss 0.29597610235214233\n",
      "[Training Epoch 1] Batch 2871, Loss 0.2848978340625763\n",
      "[Training Epoch 1] Batch 2872, Loss 0.2772708535194397\n",
      "[Training Epoch 1] Batch 2873, Loss 0.30115947127342224\n",
      "[Training Epoch 1] Batch 2874, Loss 0.2824355959892273\n",
      "[Training Epoch 1] Batch 2875, Loss 0.31260940432548523\n",
      "[Training Epoch 1] Batch 2876, Loss 0.3227958679199219\n",
      "[Training Epoch 1] Batch 2877, Loss 0.27537721395492554\n",
      "[Training Epoch 1] Batch 2878, Loss 0.26664745807647705\n",
      "[Training Epoch 1] Batch 2879, Loss 0.3064518868923187\n",
      "[Training Epoch 1] Batch 2880, Loss 0.28487199544906616\n",
      "[Training Epoch 1] Batch 2881, Loss 0.3214273154735565\n",
      "[Training Epoch 1] Batch 2882, Loss 0.2935873568058014\n",
      "[Training Epoch 1] Batch 2883, Loss 0.26913565397262573\n",
      "[Training Epoch 1] Batch 2884, Loss 0.2938860356807709\n",
      "[Training Epoch 1] Batch 2885, Loss 0.27862343192100525\n",
      "[Training Epoch 1] Batch 2886, Loss 0.2716829776763916\n",
      "[Training Epoch 1] Batch 2887, Loss 0.2621646523475647\n",
      "[Training Epoch 1] Batch 2888, Loss 0.2689184844493866\n",
      "[Training Epoch 1] Batch 2889, Loss 0.3005779981613159\n",
      "[Training Epoch 1] Batch 2890, Loss 0.31228557229042053\n",
      "[Training Epoch 1] Batch 2891, Loss 0.3367162346839905\n",
      "[Training Epoch 1] Batch 2892, Loss 0.2773091793060303\n",
      "[Training Epoch 1] Batch 2893, Loss 0.266935795545578\n",
      "[Training Epoch 1] Batch 2894, Loss 0.2868087589740753\n",
      "[Training Epoch 1] Batch 2895, Loss 0.2886732220649719\n",
      "[Training Epoch 1] Batch 2896, Loss 0.2648850977420807\n",
      "[Training Epoch 1] Batch 2897, Loss 0.3035922050476074\n",
      "[Training Epoch 1] Batch 2898, Loss 0.27314555644989014\n",
      "[Training Epoch 1] Batch 2899, Loss 0.27362489700317383\n",
      "[Training Epoch 1] Batch 2900, Loss 0.29166674613952637\n",
      "[Training Epoch 1] Batch 2901, Loss 0.26925140619277954\n",
      "[Training Epoch 1] Batch 2902, Loss 0.300554096698761\n",
      "[Training Epoch 1] Batch 2903, Loss 0.2954648733139038\n",
      "[Training Epoch 1] Batch 2904, Loss 0.3115125000476837\n",
      "[Training Epoch 1] Batch 2905, Loss 0.2822396159172058\n",
      "[Training Epoch 1] Batch 2906, Loss 0.31980201601982117\n",
      "[Training Epoch 1] Batch 2907, Loss 0.30297309160232544\n",
      "[Training Epoch 1] Batch 2908, Loss 0.2794537842273712\n",
      "[Training Epoch 1] Batch 2909, Loss 0.2838752269744873\n",
      "[Training Epoch 1] Batch 2910, Loss 0.30108243227005005\n",
      "[Training Epoch 1] Batch 2911, Loss 0.2788255512714386\n",
      "[Training Epoch 1] Batch 2912, Loss 0.3004186153411865\n",
      "[Training Epoch 1] Batch 2913, Loss 0.3108947277069092\n",
      "[Training Epoch 1] Batch 2914, Loss 0.26496267318725586\n",
      "[Training Epoch 1] Batch 2915, Loss 0.3248668313026428\n",
      "[Training Epoch 1] Batch 2916, Loss 0.29518240690231323\n",
      "[Training Epoch 1] Batch 2917, Loss 0.27236831188201904\n",
      "[Training Epoch 1] Batch 2918, Loss 0.29509079456329346\n",
      "[Training Epoch 1] Batch 2919, Loss 0.29007989168167114\n",
      "[Training Epoch 1] Batch 2920, Loss 0.2743309736251831\n",
      "[Training Epoch 1] Batch 2921, Loss 0.28007861971855164\n",
      "[Training Epoch 1] Batch 2922, Loss 0.3257657289505005\n",
      "[Training Epoch 1] Batch 2923, Loss 0.2859170436859131\n",
      "[Training Epoch 1] Batch 2924, Loss 0.2628113031387329\n",
      "[Training Epoch 1] Batch 2925, Loss 0.27921968698501587\n",
      "[Training Epoch 1] Batch 2926, Loss 0.2993619441986084\n",
      "[Training Epoch 1] Batch 2927, Loss 0.3023484945297241\n",
      "[Training Epoch 1] Batch 2928, Loss 0.2914773225784302\n",
      "[Training Epoch 1] Batch 2929, Loss 0.2545752227306366\n",
      "[Training Epoch 1] Batch 2930, Loss 0.2946721911430359\n",
      "[Training Epoch 1] Batch 2931, Loss 0.26585105061531067\n",
      "[Training Epoch 1] Batch 2932, Loss 0.28152745962142944\n",
      "[Training Epoch 1] Batch 2933, Loss 0.25661492347717285\n",
      "[Training Epoch 1] Batch 2934, Loss 0.28745371103286743\n",
      "[Training Epoch 1] Batch 2935, Loss 0.2772163152694702\n",
      "[Training Epoch 1] Batch 2936, Loss 0.2779079079627991\n",
      "[Training Epoch 1] Batch 2937, Loss 0.26981133222579956\n",
      "[Training Epoch 1] Batch 2938, Loss 0.29648324847221375\n",
      "[Training Epoch 1] Batch 2939, Loss 0.29019811749458313\n",
      "[Training Epoch 1] Batch 2940, Loss 0.3060970902442932\n",
      "[Training Epoch 1] Batch 2941, Loss 0.30677351355552673\n",
      "[Training Epoch 1] Batch 2942, Loss 0.30167073011398315\n",
      "[Training Epoch 1] Batch 2943, Loss 0.283061683177948\n",
      "[Training Epoch 1] Batch 2944, Loss 0.29738128185272217\n",
      "[Training Epoch 1] Batch 2945, Loss 0.3200840353965759\n",
      "[Training Epoch 1] Batch 2946, Loss 0.32327473163604736\n",
      "[Training Epoch 1] Batch 2947, Loss 0.296493798494339\n",
      "[Training Epoch 1] Batch 2948, Loss 0.29386308789253235\n",
      "[Training Epoch 1] Batch 2949, Loss 0.30607715249061584\n",
      "[Training Epoch 1] Batch 2950, Loss 0.309413880109787\n",
      "[Training Epoch 1] Batch 2951, Loss 0.27578720450401306\n",
      "[Training Epoch 1] Batch 2952, Loss 0.2733812928199768\n",
      "[Training Epoch 1] Batch 2953, Loss 0.2747346758842468\n",
      "[Training Epoch 1] Batch 2954, Loss 0.2953183650970459\n",
      "[Training Epoch 1] Batch 2955, Loss 0.30694735050201416\n",
      "[Training Epoch 1] Batch 2956, Loss 0.29270315170288086\n",
      "[Training Epoch 1] Batch 2957, Loss 0.288649320602417\n",
      "[Training Epoch 1] Batch 2958, Loss 0.31887757778167725\n",
      "[Training Epoch 1] Batch 2959, Loss 0.29341238737106323\n",
      "[Training Epoch 1] Batch 2960, Loss 0.269412100315094\n",
      "[Training Epoch 1] Batch 2961, Loss 0.28303074836730957\n",
      "[Training Epoch 1] Batch 2962, Loss 0.26102206110954285\n",
      "[Training Epoch 1] Batch 2963, Loss 0.2790308892726898\n",
      "[Training Epoch 1] Batch 2964, Loss 0.2842203378677368\n",
      "[Training Epoch 1] Batch 2965, Loss 0.3197419047355652\n",
      "[Training Epoch 1] Batch 2966, Loss 0.2817930281162262\n",
      "[Training Epoch 1] Batch 2967, Loss 0.31663215160369873\n",
      "[Training Epoch 1] Batch 2968, Loss 0.3121684193611145\n",
      "[Training Epoch 1] Batch 2969, Loss 0.2682836055755615\n",
      "[Training Epoch 1] Batch 2970, Loss 0.2959900200366974\n",
      "[Training Epoch 1] Batch 2971, Loss 0.29269176721572876\n",
      "[Training Epoch 1] Batch 2972, Loss 0.2944350242614746\n",
      "[Training Epoch 1] Batch 2973, Loss 0.29911571741104126\n",
      "[Training Epoch 1] Batch 2974, Loss 0.2832868695259094\n",
      "[Training Epoch 1] Batch 2975, Loss 0.27993881702423096\n",
      "[Training Epoch 1] Batch 2976, Loss 0.30085572600364685\n",
      "[Training Epoch 1] Batch 2977, Loss 0.2938535213470459\n",
      "[Training Epoch 1] Batch 2978, Loss 0.29600760340690613\n",
      "[Training Epoch 1] Batch 2979, Loss 0.28779807686805725\n",
      "[Training Epoch 1] Batch 2980, Loss 0.2935452461242676\n",
      "[Training Epoch 1] Batch 2981, Loss 0.28887510299682617\n",
      "[Training Epoch 1] Batch 2982, Loss 0.29216980934143066\n",
      "[Training Epoch 1] Batch 2983, Loss 0.2568523585796356\n",
      "[Training Epoch 1] Batch 2984, Loss 0.28942006826400757\n",
      "[Training Epoch 1] Batch 2985, Loss 0.27831757068634033\n",
      "[Training Epoch 1] Batch 2986, Loss 0.26900026202201843\n",
      "[Training Epoch 1] Batch 2987, Loss 0.3076639175415039\n",
      "[Training Epoch 1] Batch 2988, Loss 0.27223116159439087\n",
      "[Training Epoch 1] Batch 2989, Loss 0.26939284801483154\n",
      "[Training Epoch 1] Batch 2990, Loss 0.2859680950641632\n",
      "[Training Epoch 1] Batch 2991, Loss 0.2748571038246155\n",
      "[Training Epoch 1] Batch 2992, Loss 0.3196288049221039\n",
      "[Training Epoch 1] Batch 2993, Loss 0.29421889781951904\n",
      "[Training Epoch 1] Batch 2994, Loss 0.3055528402328491\n",
      "[Training Epoch 1] Batch 2995, Loss 0.2892800569534302\n",
      "[Training Epoch 1] Batch 2996, Loss 0.2680884599685669\n",
      "[Training Epoch 1] Batch 2997, Loss 0.2676355242729187\n",
      "[Training Epoch 1] Batch 2998, Loss 0.29662999510765076\n",
      "[Training Epoch 1] Batch 2999, Loss 0.2888658046722412\n",
      "[Training Epoch 1] Batch 3000, Loss 0.2620824873447418\n",
      "[Training Epoch 1] Batch 3001, Loss 0.2875816226005554\n",
      "[Training Epoch 1] Batch 3002, Loss 0.2718653678894043\n",
      "[Training Epoch 1] Batch 3003, Loss 0.2996901571750641\n",
      "[Training Epoch 1] Batch 3004, Loss 0.2782975137233734\n",
      "[Training Epoch 1] Batch 3005, Loss 0.3099476397037506\n",
      "[Training Epoch 1] Batch 3006, Loss 0.29742923378944397\n",
      "[Training Epoch 1] Batch 3007, Loss 0.28600963950157166\n",
      "[Training Epoch 1] Batch 3008, Loss 0.29561829566955566\n",
      "[Training Epoch 1] Batch 3009, Loss 0.3216729760169983\n",
      "[Training Epoch 1] Batch 3010, Loss 0.3047220706939697\n",
      "[Training Epoch 1] Batch 3011, Loss 0.28648948669433594\n",
      "[Training Epoch 1] Batch 3012, Loss 0.28160229325294495\n",
      "[Training Epoch 1] Batch 3013, Loss 0.26679784059524536\n",
      "[Training Epoch 1] Batch 3014, Loss 0.2924402952194214\n",
      "[Training Epoch 1] Batch 3015, Loss 0.2673802375793457\n",
      "[Training Epoch 1] Batch 3016, Loss 0.27021920680999756\n",
      "[Training Epoch 1] Batch 3017, Loss 0.2752479314804077\n",
      "[Training Epoch 1] Batch 3018, Loss 0.2612002193927765\n",
      "[Training Epoch 1] Batch 3019, Loss 0.26958537101745605\n",
      "[Training Epoch 1] Batch 3020, Loss 0.29500898718833923\n",
      "[Training Epoch 1] Batch 3021, Loss 0.28673937916755676\n",
      "[Training Epoch 1] Batch 3022, Loss 0.2731901705265045\n",
      "[Training Epoch 1] Batch 3023, Loss 0.32498157024383545\n",
      "[Training Epoch 1] Batch 3024, Loss 0.2833179831504822\n",
      "[Training Epoch 1] Batch 3025, Loss 0.3041170835494995\n",
      "[Training Epoch 1] Batch 3026, Loss 0.2870197594165802\n",
      "[Training Epoch 1] Batch 3027, Loss 0.2729015350341797\n",
      "[Training Epoch 1] Batch 3028, Loss 0.3148433566093445\n",
      "[Training Epoch 1] Batch 3029, Loss 0.2844897210597992\n",
      "[Training Epoch 1] Batch 3030, Loss 0.26083803176879883\n",
      "[Training Epoch 1] Batch 3031, Loss 0.2914733290672302\n",
      "[Training Epoch 1] Batch 3032, Loss 0.271604984998703\n",
      "[Training Epoch 1] Batch 3033, Loss 0.2725890874862671\n",
      "[Training Epoch 1] Batch 3034, Loss 0.30572637915611267\n",
      "[Training Epoch 1] Batch 3035, Loss 0.3309531807899475\n",
      "[Training Epoch 1] Batch 3036, Loss 0.2773187756538391\n",
      "[Training Epoch 1] Batch 3037, Loss 0.27675294876098633\n",
      "[Training Epoch 1] Batch 3038, Loss 0.2879203259944916\n",
      "[Training Epoch 1] Batch 3039, Loss 0.2900552451610565\n",
      "[Training Epoch 1] Batch 3040, Loss 0.25872817635536194\n",
      "[Training Epoch 1] Batch 3041, Loss 0.28269022703170776\n",
      "[Training Epoch 1] Batch 3042, Loss 0.2672714293003082\n",
      "[Training Epoch 1] Batch 3043, Loss 0.27315348386764526\n",
      "[Training Epoch 1] Batch 3044, Loss 0.2462504655122757\n",
      "[Training Epoch 1] Batch 3045, Loss 0.3044123649597168\n",
      "[Training Epoch 1] Batch 3046, Loss 0.295626163482666\n",
      "[Training Epoch 1] Batch 3047, Loss 0.2849269509315491\n",
      "[Training Epoch 1] Batch 3048, Loss 0.27888333797454834\n",
      "[Training Epoch 1] Batch 3049, Loss 0.27941620349884033\n",
      "[Training Epoch 1] Batch 3050, Loss 0.2815999388694763\n",
      "[Training Epoch 1] Batch 3051, Loss 0.2997145354747772\n",
      "[Training Epoch 1] Batch 3052, Loss 0.3017790913581848\n",
      "[Training Epoch 1] Batch 3053, Loss 0.27293747663497925\n",
      "[Training Epoch 1] Batch 3054, Loss 0.2781563997268677\n",
      "[Training Epoch 1] Batch 3055, Loss 0.2792644500732422\n",
      "[Training Epoch 1] Batch 3056, Loss 0.2866004705429077\n",
      "[Training Epoch 1] Batch 3057, Loss 0.31111961603164673\n",
      "[Training Epoch 1] Batch 3058, Loss 0.29546886682510376\n",
      "[Training Epoch 1] Batch 3059, Loss 0.3038792610168457\n",
      "[Training Epoch 1] Batch 3060, Loss 0.3036916255950928\n",
      "[Training Epoch 1] Batch 3061, Loss 0.268979549407959\n",
      "[Training Epoch 1] Batch 3062, Loss 0.31287747621536255\n",
      "[Training Epoch 1] Batch 3063, Loss 0.28652137517929077\n",
      "[Training Epoch 1] Batch 3064, Loss 0.26756805181503296\n",
      "[Training Epoch 1] Batch 3065, Loss 0.29208824038505554\n",
      "[Training Epoch 1] Batch 3066, Loss 0.2805759310722351\n",
      "[Training Epoch 1] Batch 3067, Loss 0.3023025393486023\n",
      "[Training Epoch 1] Batch 3068, Loss 0.30460548400878906\n",
      "[Training Epoch 1] Batch 3069, Loss 0.3045136332511902\n",
      "[Training Epoch 1] Batch 3070, Loss 0.27789315581321716\n",
      "[Training Epoch 1] Batch 3071, Loss 0.345384418964386\n",
      "[Training Epoch 1] Batch 3072, Loss 0.3047550916671753\n",
      "[Training Epoch 1] Batch 3073, Loss 0.2931060194969177\n",
      "[Training Epoch 1] Batch 3074, Loss 0.27409207820892334\n",
      "[Training Epoch 1] Batch 3075, Loss 0.2850690484046936\n",
      "[Training Epoch 1] Batch 3076, Loss 0.28997743129730225\n",
      "[Training Epoch 1] Batch 3077, Loss 0.28842243552207947\n",
      "[Training Epoch 1] Batch 3078, Loss 0.30227744579315186\n",
      "[Training Epoch 1] Batch 3079, Loss 0.28055238723754883\n",
      "[Training Epoch 1] Batch 3080, Loss 0.2713249623775482\n",
      "[Training Epoch 1] Batch 3081, Loss 0.31643277406692505\n",
      "[Training Epoch 1] Batch 3082, Loss 0.31070253252983093\n",
      "[Training Epoch 1] Batch 3083, Loss 0.30006223917007446\n",
      "[Training Epoch 1] Batch 3084, Loss 0.28426945209503174\n",
      "[Training Epoch 1] Batch 3085, Loss 0.27487242221832275\n",
      "[Training Epoch 1] Batch 3086, Loss 0.3230857849121094\n",
      "[Training Epoch 1] Batch 3087, Loss 0.2970675826072693\n",
      "[Training Epoch 1] Batch 3088, Loss 0.29284971952438354\n",
      "[Training Epoch 1] Batch 3089, Loss 0.294991135597229\n",
      "[Training Epoch 1] Batch 3090, Loss 0.31092584133148193\n",
      "[Training Epoch 1] Batch 3091, Loss 0.2760000228881836\n",
      "[Training Epoch 1] Batch 3092, Loss 0.2786186933517456\n",
      "[Training Epoch 1] Batch 3093, Loss 0.3021702468395233\n",
      "[Training Epoch 1] Batch 3094, Loss 0.32955360412597656\n",
      "[Training Epoch 1] Batch 3095, Loss 0.29405754804611206\n",
      "[Training Epoch 1] Batch 3096, Loss 0.3062744736671448\n",
      "[Training Epoch 1] Batch 3097, Loss 0.29150229692459106\n",
      "[Training Epoch 1] Batch 3098, Loss 0.28771668672561646\n",
      "[Training Epoch 1] Batch 3099, Loss 0.263741672039032\n",
      "[Training Epoch 1] Batch 3100, Loss 0.30125555396080017\n",
      "[Training Epoch 1] Batch 3101, Loss 0.292156457901001\n",
      "[Training Epoch 1] Batch 3102, Loss 0.2918239235877991\n",
      "[Training Epoch 1] Batch 3103, Loss 0.28548362851142883\n",
      "[Training Epoch 1] Batch 3104, Loss 0.29544705152511597\n",
      "[Training Epoch 1] Batch 3105, Loss 0.2938714623451233\n",
      "[Training Epoch 1] Batch 3106, Loss 0.29853129386901855\n",
      "[Training Epoch 1] Batch 3107, Loss 0.3103834390640259\n",
      "[Training Epoch 1] Batch 3108, Loss 0.2634875178337097\n",
      "[Training Epoch 1] Batch 3109, Loss 0.29051756858825684\n",
      "[Training Epoch 1] Batch 3110, Loss 0.2873234152793884\n",
      "[Training Epoch 1] Batch 3111, Loss 0.2918553352355957\n",
      "[Training Epoch 1] Batch 3112, Loss 0.2969933748245239\n",
      "[Training Epoch 1] Batch 3113, Loss 0.31422069668769836\n",
      "[Training Epoch 1] Batch 3114, Loss 0.27717310190200806\n",
      "[Training Epoch 1] Batch 3115, Loss 0.27426213026046753\n",
      "[Training Epoch 1] Batch 3116, Loss 0.3070695400238037\n",
      "[Training Epoch 1] Batch 3117, Loss 0.28879138827323914\n",
      "[Training Epoch 1] Batch 3118, Loss 0.3100242018699646\n",
      "[Training Epoch 1] Batch 3119, Loss 0.3367035686969757\n",
      "[Training Epoch 1] Batch 3120, Loss 0.30911916494369507\n",
      "[Training Epoch 1] Batch 3121, Loss 0.2906273603439331\n",
      "[Training Epoch 1] Batch 3122, Loss 0.3046268820762634\n",
      "[Training Epoch 1] Batch 3123, Loss 0.24873554706573486\n",
      "[Training Epoch 1] Batch 3124, Loss 0.2724522352218628\n",
      "[Training Epoch 1] Batch 3125, Loss 0.29158031940460205\n",
      "[Training Epoch 1] Batch 3126, Loss 0.2627623379230499\n",
      "[Training Epoch 1] Batch 3127, Loss 0.31308645009994507\n",
      "[Training Epoch 1] Batch 3128, Loss 0.3050025701522827\n",
      "[Training Epoch 1] Batch 3129, Loss 0.2627779543399811\n",
      "[Training Epoch 1] Batch 3130, Loss 0.3231604993343353\n",
      "[Training Epoch 1] Batch 3131, Loss 0.28461170196533203\n",
      "[Training Epoch 1] Batch 3132, Loss 0.24120761454105377\n",
      "[Training Epoch 1] Batch 3133, Loss 0.28284770250320435\n",
      "[Training Epoch 1] Batch 3134, Loss 0.2635635733604431\n",
      "[Training Epoch 1] Batch 3135, Loss 0.3020659387111664\n",
      "[Training Epoch 1] Batch 3136, Loss 0.27944666147232056\n",
      "[Training Epoch 1] Batch 3137, Loss 0.31211620569229126\n",
      "[Training Epoch 1] Batch 3138, Loss 0.30913251638412476\n",
      "[Training Epoch 1] Batch 3139, Loss 0.28641918301582336\n",
      "[Training Epoch 1] Batch 3140, Loss 0.30959194898605347\n",
      "[Training Epoch 1] Batch 3141, Loss 0.2582191824913025\n",
      "[Training Epoch 1] Batch 3142, Loss 0.27451467514038086\n",
      "[Training Epoch 1] Batch 3143, Loss 0.2889034152030945\n",
      "[Training Epoch 1] Batch 3144, Loss 0.29772520065307617\n",
      "[Training Epoch 1] Batch 3145, Loss 0.3203558027744293\n",
      "[Training Epoch 1] Batch 3146, Loss 0.3096703886985779\n",
      "[Training Epoch 1] Batch 3147, Loss 0.31148117780685425\n",
      "[Training Epoch 1] Batch 3148, Loss 0.2994832694530487\n",
      "[Training Epoch 1] Batch 3149, Loss 0.29556190967559814\n",
      "[Training Epoch 1] Batch 3150, Loss 0.28721320629119873\n",
      "[Training Epoch 1] Batch 3151, Loss 0.2698368728160858\n",
      "[Training Epoch 1] Batch 3152, Loss 0.28551846742630005\n",
      "[Training Epoch 1] Batch 3153, Loss 0.3072553277015686\n",
      "[Training Epoch 1] Batch 3154, Loss 0.3082753121852875\n",
      "[Training Epoch 1] Batch 3155, Loss 0.30648791790008545\n",
      "[Training Epoch 1] Batch 3156, Loss 0.27930498123168945\n",
      "[Training Epoch 1] Batch 3157, Loss 0.3093496263027191\n",
      "[Training Epoch 1] Batch 3158, Loss 0.28651049733161926\n",
      "[Training Epoch 1] Batch 3159, Loss 0.28059208393096924\n",
      "[Training Epoch 1] Batch 3160, Loss 0.2869197130203247\n",
      "[Training Epoch 1] Batch 3161, Loss 0.2874923646450043\n",
      "[Training Epoch 1] Batch 3162, Loss 0.2917664647102356\n",
      "[Training Epoch 1] Batch 3163, Loss 0.2928723692893982\n",
      "[Training Epoch 1] Batch 3164, Loss 0.2815778851509094\n",
      "[Training Epoch 1] Batch 3165, Loss 0.29803335666656494\n",
      "[Training Epoch 1] Batch 3166, Loss 0.32546335458755493\n",
      "[Training Epoch 1] Batch 3167, Loss 0.28824418783187866\n",
      "[Training Epoch 1] Batch 3168, Loss 0.30422431230545044\n",
      "[Training Epoch 1] Batch 3169, Loss 0.3204631209373474\n",
      "[Training Epoch 1] Batch 3170, Loss 0.2686672508716583\n",
      "[Training Epoch 1] Batch 3171, Loss 0.2942594885826111\n",
      "[Training Epoch 1] Batch 3172, Loss 0.31486016511917114\n",
      "[Training Epoch 1] Batch 3173, Loss 0.27128857374191284\n",
      "[Training Epoch 1] Batch 3174, Loss 0.3124646544456482\n",
      "[Training Epoch 1] Batch 3175, Loss 0.2989843487739563\n",
      "[Training Epoch 1] Batch 3176, Loss 0.26357269287109375\n",
      "[Training Epoch 1] Batch 3177, Loss 0.2769351601600647\n",
      "[Training Epoch 1] Batch 3178, Loss 0.2970789074897766\n",
      "[Training Epoch 1] Batch 3179, Loss 0.28701603412628174\n",
      "[Training Epoch 1] Batch 3180, Loss 0.2677757143974304\n",
      "[Training Epoch 1] Batch 3181, Loss 0.29529470205307007\n",
      "[Training Epoch 1] Batch 3182, Loss 0.2608943581581116\n",
      "[Training Epoch 1] Batch 3183, Loss 0.27645790576934814\n",
      "[Training Epoch 1] Batch 3184, Loss 0.3157632350921631\n",
      "[Training Epoch 1] Batch 3185, Loss 0.315822571516037\n",
      "[Training Epoch 1] Batch 3186, Loss 0.3147532045841217\n",
      "[Training Epoch 1] Batch 3187, Loss 0.3260549306869507\n",
      "[Training Epoch 1] Batch 3188, Loss 0.2979470491409302\n",
      "[Training Epoch 1] Batch 3189, Loss 0.26453036069869995\n",
      "[Training Epoch 1] Batch 3190, Loss 0.27780139446258545\n",
      "[Training Epoch 1] Batch 3191, Loss 0.2692568898200989\n",
      "[Training Epoch 1] Batch 3192, Loss 0.2938491702079773\n",
      "[Training Epoch 1] Batch 3193, Loss 0.3077399730682373\n",
      "[Training Epoch 1] Batch 3194, Loss 0.2539950907230377\n",
      "[Training Epoch 1] Batch 3195, Loss 0.29735177755355835\n",
      "[Training Epoch 1] Batch 3196, Loss 0.26899805665016174\n",
      "[Training Epoch 1] Batch 3197, Loss 0.28241094946861267\n",
      "[Training Epoch 1] Batch 3198, Loss 0.3140687644481659\n",
      "[Training Epoch 1] Batch 3199, Loss 0.3119193911552429\n",
      "[Training Epoch 1] Batch 3200, Loss 0.2911298871040344\n",
      "[Training Epoch 1] Batch 3201, Loss 0.3352678120136261\n",
      "[Training Epoch 1] Batch 3202, Loss 0.2900736927986145\n",
      "[Training Epoch 1] Batch 3203, Loss 0.30020958185195923\n",
      "[Training Epoch 1] Batch 3204, Loss 0.2891862988471985\n",
      "[Training Epoch 1] Batch 3205, Loss 0.2775534391403198\n",
      "[Training Epoch 1] Batch 3206, Loss 0.30568522214889526\n",
      "[Training Epoch 1] Batch 3207, Loss 0.3045830726623535\n",
      "[Training Epoch 1] Batch 3208, Loss 0.28779274225234985\n",
      "[Training Epoch 1] Batch 3209, Loss 0.2782104015350342\n",
      "[Training Epoch 1] Batch 3210, Loss 0.29783064126968384\n",
      "[Training Epoch 1] Batch 3211, Loss 0.2874186635017395\n",
      "[Training Epoch 1] Batch 3212, Loss 0.3113115429878235\n",
      "[Training Epoch 1] Batch 3213, Loss 0.29068151116371155\n",
      "[Training Epoch 1] Batch 3214, Loss 0.28356385231018066\n",
      "[Training Epoch 1] Batch 3215, Loss 0.28082144260406494\n",
      "[Training Epoch 1] Batch 3216, Loss 0.3075863718986511\n",
      "[Training Epoch 1] Batch 3217, Loss 0.26950564980506897\n",
      "[Training Epoch 1] Batch 3218, Loss 0.31978482007980347\n",
      "[Training Epoch 1] Batch 3219, Loss 0.27478423714637756\n",
      "[Training Epoch 1] Batch 3220, Loss 0.27757516503334045\n",
      "[Training Epoch 1] Batch 3221, Loss 0.3039063811302185\n",
      "[Training Epoch 1] Batch 3222, Loss 0.27245014905929565\n",
      "[Training Epoch 1] Batch 3223, Loss 0.28124919533729553\n",
      "[Training Epoch 1] Batch 3224, Loss 0.27363502979278564\n",
      "[Training Epoch 1] Batch 3225, Loss 0.27469342947006226\n",
      "[Training Epoch 1] Batch 3226, Loss 0.2946784496307373\n",
      "[Training Epoch 1] Batch 3227, Loss 0.31884729862213135\n",
      "[Training Epoch 1] Batch 3228, Loss 0.29592564702033997\n",
      "[Training Epoch 1] Batch 3229, Loss 0.2698705792427063\n",
      "[Training Epoch 1] Batch 3230, Loss 0.26576852798461914\n",
      "[Training Epoch 1] Batch 3231, Loss 0.3056645691394806\n",
      "[Training Epoch 1] Batch 3232, Loss 0.31342169642448425\n",
      "[Training Epoch 1] Batch 3233, Loss 0.3119238615036011\n",
      "[Training Epoch 1] Batch 3234, Loss 0.2795121669769287\n",
      "[Training Epoch 1] Batch 3235, Loss 0.2731396555900574\n",
      "[Training Epoch 1] Batch 3236, Loss 0.29237550497055054\n",
      "[Training Epoch 1] Batch 3237, Loss 0.29364147782325745\n",
      "[Training Epoch 1] Batch 3238, Loss 0.2711123824119568\n",
      "[Training Epoch 1] Batch 3239, Loss 0.28851962089538574\n",
      "[Training Epoch 1] Batch 3240, Loss 0.28803175687789917\n",
      "[Training Epoch 1] Batch 3241, Loss 0.3175378143787384\n",
      "[Training Epoch 1] Batch 3242, Loss 0.31910982728004456\n",
      "[Training Epoch 1] Batch 3243, Loss 0.3217102885246277\n",
      "[Training Epoch 1] Batch 3244, Loss 0.2779197692871094\n",
      "[Training Epoch 1] Batch 3245, Loss 0.29659420251846313\n",
      "[Training Epoch 1] Batch 3246, Loss 0.2746252417564392\n",
      "[Training Epoch 1] Batch 3247, Loss 0.2607126832008362\n",
      "[Training Epoch 1] Batch 3248, Loss 0.2965947687625885\n",
      "[Training Epoch 1] Batch 3249, Loss 0.2717663645744324\n",
      "[Training Epoch 1] Batch 3250, Loss 0.2850879430770874\n",
      "[Training Epoch 1] Batch 3251, Loss 0.24929171800613403\n",
      "[Training Epoch 1] Batch 3252, Loss 0.26758092641830444\n",
      "[Training Epoch 1] Batch 3253, Loss 0.29843947291374207\n",
      "[Training Epoch 1] Batch 3254, Loss 0.28540095686912537\n",
      "[Training Epoch 1] Batch 3255, Loss 0.2937675714492798\n",
      "[Training Epoch 1] Batch 3256, Loss 0.2746511995792389\n",
      "[Training Epoch 1] Batch 3257, Loss 0.28022462129592896\n",
      "[Training Epoch 1] Batch 3258, Loss 0.30566203594207764\n",
      "[Training Epoch 1] Batch 3259, Loss 0.2893657684326172\n",
      "[Training Epoch 1] Batch 3260, Loss 0.28356391191482544\n",
      "[Training Epoch 1] Batch 3261, Loss 0.30806219577789307\n",
      "[Training Epoch 1] Batch 3262, Loss 0.3080589771270752\n",
      "[Training Epoch 1] Batch 3263, Loss 0.2820942997932434\n",
      "[Training Epoch 1] Batch 3264, Loss 0.282127320766449\n",
      "[Training Epoch 1] Batch 3265, Loss 0.32131242752075195\n",
      "[Training Epoch 1] Batch 3266, Loss 0.283513605594635\n",
      "[Training Epoch 1] Batch 3267, Loss 0.2986212372779846\n",
      "[Training Epoch 1] Batch 3268, Loss 0.27297598123550415\n",
      "[Training Epoch 1] Batch 3269, Loss 0.2754126191139221\n",
      "[Training Epoch 1] Batch 3270, Loss 0.2934337854385376\n",
      "[Training Epoch 1] Batch 3271, Loss 0.26851150393486023\n",
      "[Training Epoch 1] Batch 3272, Loss 0.2992399334907532\n",
      "[Training Epoch 1] Batch 3273, Loss 0.30296143889427185\n",
      "[Training Epoch 1] Batch 3274, Loss 0.293110191822052\n",
      "[Training Epoch 1] Batch 3275, Loss 0.2827998399734497\n",
      "[Training Epoch 1] Batch 3276, Loss 0.2709801197052002\n",
      "[Training Epoch 1] Batch 3277, Loss 0.2789892554283142\n",
      "[Training Epoch 1] Batch 3278, Loss 0.29475197196006775\n",
      "[Training Epoch 1] Batch 3279, Loss 0.30122530460357666\n",
      "[Training Epoch 1] Batch 3280, Loss 0.30567067861557007\n",
      "[Training Epoch 1] Batch 3281, Loss 0.26676905155181885\n",
      "[Training Epoch 1] Batch 3282, Loss 0.3113584518432617\n",
      "[Training Epoch 1] Batch 3283, Loss 0.2777927815914154\n",
      "[Training Epoch 1] Batch 3284, Loss 0.2685318887233734\n",
      "[Training Epoch 1] Batch 3285, Loss 0.2946547269821167\n",
      "[Training Epoch 1] Batch 3286, Loss 0.26509588956832886\n",
      "[Training Epoch 1] Batch 3287, Loss 0.28776782751083374\n",
      "[Training Epoch 1] Batch 3288, Loss 0.27130061388015747\n",
      "[Training Epoch 1] Batch 3289, Loss 0.29856353998184204\n",
      "[Training Epoch 1] Batch 3290, Loss 0.31030839681625366\n",
      "[Training Epoch 1] Batch 3291, Loss 0.2992859184741974\n",
      "[Training Epoch 1] Batch 3292, Loss 0.29674065113067627\n",
      "[Training Epoch 1] Batch 3293, Loss 0.2742118537425995\n",
      "[Training Epoch 1] Batch 3294, Loss 0.26057204604148865\n",
      "[Training Epoch 1] Batch 3295, Loss 0.28766414523124695\n",
      "[Training Epoch 1] Batch 3296, Loss 0.2931246757507324\n",
      "[Training Epoch 1] Batch 3297, Loss 0.2843076288700104\n",
      "[Training Epoch 1] Batch 3298, Loss 0.27320927381515503\n",
      "[Training Epoch 1] Batch 3299, Loss 0.30846625566482544\n",
      "[Training Epoch 1] Batch 3300, Loss 0.2562718987464905\n",
      "[Training Epoch 1] Batch 3301, Loss 0.2955961227416992\n",
      "[Training Epoch 1] Batch 3302, Loss 0.2693908214569092\n",
      "[Training Epoch 1] Batch 3303, Loss 0.2944486737251282\n",
      "[Training Epoch 1] Batch 3304, Loss 0.3109667897224426\n",
      "[Training Epoch 1] Batch 3305, Loss 0.26364606618881226\n",
      "[Training Epoch 1] Batch 3306, Loss 0.29715031385421753\n",
      "[Training Epoch 1] Batch 3307, Loss 0.27771759033203125\n",
      "[Training Epoch 1] Batch 3308, Loss 0.2958361506462097\n",
      "[Training Epoch 1] Batch 3309, Loss 0.30190667510032654\n",
      "[Training Epoch 1] Batch 3310, Loss 0.27829939126968384\n",
      "[Training Epoch 1] Batch 3311, Loss 0.27194449305534363\n",
      "[Training Epoch 1] Batch 3312, Loss 0.28879430890083313\n",
      "[Training Epoch 1] Batch 3313, Loss 0.2734692692756653\n",
      "[Training Epoch 1] Batch 3314, Loss 0.2751636505126953\n",
      "[Training Epoch 1] Batch 3315, Loss 0.28334373235702515\n",
      "[Training Epoch 1] Batch 3316, Loss 0.2747946083545685\n",
      "[Training Epoch 1] Batch 3317, Loss 0.2873208522796631\n",
      "[Training Epoch 1] Batch 3318, Loss 0.2947598695755005\n",
      "[Training Epoch 1] Batch 3319, Loss 0.2887926399707794\n",
      "[Training Epoch 1] Batch 3320, Loss 0.30162763595581055\n",
      "[Training Epoch 1] Batch 3321, Loss 0.2644340693950653\n",
      "[Training Epoch 1] Batch 3322, Loss 0.2769821584224701\n",
      "[Training Epoch 1] Batch 3323, Loss 0.2660163938999176\n",
      "[Training Epoch 1] Batch 3324, Loss 0.2895203232765198\n",
      "[Training Epoch 1] Batch 3325, Loss 0.27451878786087036\n",
      "[Training Epoch 1] Batch 3326, Loss 0.2622859477996826\n",
      "[Training Epoch 1] Batch 3327, Loss 0.2939411997795105\n",
      "[Training Epoch 1] Batch 3328, Loss 0.2768932282924652\n",
      "[Training Epoch 1] Batch 3329, Loss 0.29063189029693604\n",
      "[Training Epoch 1] Batch 3330, Loss 0.2522304058074951\n",
      "[Training Epoch 1] Batch 3331, Loss 0.2817150354385376\n",
      "[Training Epoch 1] Batch 3332, Loss 0.3018541932106018\n",
      "[Training Epoch 1] Batch 3333, Loss 0.29211556911468506\n",
      "[Training Epoch 1] Batch 3334, Loss 0.3289050757884979\n",
      "[Training Epoch 1] Batch 3335, Loss 0.3089319169521332\n",
      "[Training Epoch 1] Batch 3336, Loss 0.28846460580825806\n",
      "[Training Epoch 1] Batch 3337, Loss 0.29256632924079895\n",
      "[Training Epoch 1] Batch 3338, Loss 0.2766040563583374\n",
      "[Training Epoch 1] Batch 3339, Loss 0.28507789969444275\n",
      "[Training Epoch 1] Batch 3340, Loss 0.2900640368461609\n",
      "[Training Epoch 1] Batch 3341, Loss 0.3126698434352875\n",
      "[Training Epoch 1] Batch 3342, Loss 0.27378714084625244\n",
      "[Training Epoch 1] Batch 3343, Loss 0.2916669249534607\n",
      "[Training Epoch 1] Batch 3344, Loss 0.29499274492263794\n",
      "[Training Epoch 1] Batch 3345, Loss 0.2694917917251587\n",
      "[Training Epoch 1] Batch 3346, Loss 0.2817210555076599\n",
      "[Training Epoch 1] Batch 3347, Loss 0.3153603672981262\n",
      "[Training Epoch 1] Batch 3348, Loss 0.28759437799453735\n",
      "[Training Epoch 1] Batch 3349, Loss 0.28223732113838196\n",
      "[Training Epoch 1] Batch 3350, Loss 0.2730584144592285\n",
      "[Training Epoch 1] Batch 3351, Loss 0.3180334270000458\n",
      "[Training Epoch 1] Batch 3352, Loss 0.28792184591293335\n",
      "[Training Epoch 1] Batch 3353, Loss 0.27162599563598633\n",
      "[Training Epoch 1] Batch 3354, Loss 0.30211877822875977\n",
      "[Training Epoch 1] Batch 3355, Loss 0.2707577645778656\n",
      "[Training Epoch 1] Batch 3356, Loss 0.3042669892311096\n",
      "[Training Epoch 1] Batch 3357, Loss 0.2986255884170532\n",
      "[Training Epoch 1] Batch 3358, Loss 0.2923305630683899\n",
      "[Training Epoch 1] Batch 3359, Loss 0.28997933864593506\n",
      "[Training Epoch 1] Batch 3360, Loss 0.3118661344051361\n",
      "[Training Epoch 1] Batch 3361, Loss 0.2816653847694397\n",
      "[Training Epoch 1] Batch 3362, Loss 0.3136632442474365\n",
      "[Training Epoch 1] Batch 3363, Loss 0.28396743535995483\n",
      "[Training Epoch 1] Batch 3364, Loss 0.300472617149353\n",
      "[Training Epoch 1] Batch 3365, Loss 0.3174888789653778\n",
      "[Training Epoch 1] Batch 3366, Loss 0.3032934069633484\n",
      "[Training Epoch 1] Batch 3367, Loss 0.2985082268714905\n",
      "[Training Epoch 1] Batch 3368, Loss 0.284883052110672\n",
      "[Training Epoch 1] Batch 3369, Loss 0.27072572708129883\n",
      "[Training Epoch 1] Batch 3370, Loss 0.31197845935821533\n",
      "[Training Epoch 1] Batch 3371, Loss 0.29217636585235596\n",
      "[Training Epoch 1] Batch 3372, Loss 0.28671783208847046\n",
      "[Training Epoch 1] Batch 3373, Loss 0.29216575622558594\n",
      "[Training Epoch 1] Batch 3374, Loss 0.2885502576828003\n",
      "[Training Epoch 1] Batch 3375, Loss 0.29325467348098755\n",
      "[Training Epoch 1] Batch 3376, Loss 0.2803221046924591\n",
      "[Training Epoch 1] Batch 3377, Loss 0.279457688331604\n",
      "[Training Epoch 1] Batch 3378, Loss 0.27350136637687683\n",
      "[Training Epoch 1] Batch 3379, Loss 0.30563443899154663\n",
      "[Training Epoch 1] Batch 3380, Loss 0.2792995572090149\n",
      "[Training Epoch 1] Batch 3381, Loss 0.2968448996543884\n",
      "[Training Epoch 1] Batch 3382, Loss 0.32027527689933777\n",
      "[Training Epoch 1] Batch 3383, Loss 0.2943041920661926\n",
      "[Training Epoch 1] Batch 3384, Loss 0.3129366636276245\n",
      "[Training Epoch 1] Batch 3385, Loss 0.31420254707336426\n",
      "[Training Epoch 1] Batch 3386, Loss 0.28427553176879883\n",
      "[Training Epoch 1] Batch 3387, Loss 0.28850990533828735\n",
      "[Training Epoch 1] Batch 3388, Loss 0.29563742876052856\n",
      "[Training Epoch 1] Batch 3389, Loss 0.29279202222824097\n",
      "[Training Epoch 1] Batch 3390, Loss 0.2707482576370239\n",
      "[Training Epoch 1] Batch 3391, Loss 0.2851756811141968\n",
      "[Training Epoch 1] Batch 3392, Loss 0.31379103660583496\n",
      "[Training Epoch 1] Batch 3393, Loss 0.2809827923774719\n",
      "[Training Epoch 1] Batch 3394, Loss 0.28173306584358215\n",
      "[Training Epoch 1] Batch 3395, Loss 0.2989201545715332\n",
      "[Training Epoch 1] Batch 3396, Loss 0.27608048915863037\n",
      "[Training Epoch 1] Batch 3397, Loss 0.3068261444568634\n",
      "[Training Epoch 1] Batch 3398, Loss 0.2952357828617096\n",
      "[Training Epoch 1] Batch 3399, Loss 0.29635652899742126\n",
      "[Training Epoch 1] Batch 3400, Loss 0.2694038152694702\n",
      "[Training Epoch 1] Batch 3401, Loss 0.302698016166687\n",
      "[Training Epoch 1] Batch 3402, Loss 0.2768286466598511\n",
      "[Training Epoch 1] Batch 3403, Loss 0.2756347060203552\n",
      "[Training Epoch 1] Batch 3404, Loss 0.3149675130844116\n",
      "[Training Epoch 1] Batch 3405, Loss 0.2662634551525116\n",
      "[Training Epoch 1] Batch 3406, Loss 0.2758338153362274\n",
      "[Training Epoch 1] Batch 3407, Loss 0.2996997535228729\n",
      "[Training Epoch 1] Batch 3408, Loss 0.2856081426143646\n",
      "[Training Epoch 1] Batch 3409, Loss 0.28426581621170044\n",
      "[Training Epoch 1] Batch 3410, Loss 0.2865307033061981\n",
      "[Training Epoch 1] Batch 3411, Loss 0.3100931644439697\n",
      "[Training Epoch 1] Batch 3412, Loss 0.29007405042648315\n",
      "[Training Epoch 1] Batch 3413, Loss 0.2817850708961487\n",
      "[Training Epoch 1] Batch 3414, Loss 0.3056083023548126\n",
      "[Training Epoch 1] Batch 3415, Loss 0.2830783426761627\n",
      "[Training Epoch 1] Batch 3416, Loss 0.2766166925430298\n",
      "[Training Epoch 1] Batch 3417, Loss 0.29768747091293335\n",
      "[Training Epoch 1] Batch 3418, Loss 0.31088486313819885\n",
      "[Training Epoch 1] Batch 3419, Loss 0.2889329791069031\n",
      "[Training Epoch 1] Batch 3420, Loss 0.30465513467788696\n",
      "[Training Epoch 1] Batch 3421, Loss 0.3094600439071655\n",
      "[Training Epoch 1] Batch 3422, Loss 0.28551924228668213\n",
      "[Training Epoch 1] Batch 3423, Loss 0.24851717054843903\n",
      "[Training Epoch 1] Batch 3424, Loss 0.29181069135665894\n",
      "[Training Epoch 1] Batch 3425, Loss 0.3060908317565918\n",
      "[Training Epoch 1] Batch 3426, Loss 0.29608845710754395\n",
      "[Training Epoch 1] Batch 3427, Loss 0.27644190192222595\n",
      "[Training Epoch 1] Batch 3428, Loss 0.2735833525657654\n",
      "[Training Epoch 1] Batch 3429, Loss 0.2785561680793762\n",
      "[Training Epoch 1] Batch 3430, Loss 0.29325079917907715\n",
      "[Training Epoch 1] Batch 3431, Loss 0.27742087841033936\n",
      "[Training Epoch 1] Batch 3432, Loss 0.2945566773414612\n",
      "[Training Epoch 1] Batch 3433, Loss 0.3128361105918884\n",
      "[Training Epoch 1] Batch 3434, Loss 0.28803297877311707\n",
      "[Training Epoch 1] Batch 3435, Loss 0.29213568568229675\n",
      "[Training Epoch 1] Batch 3436, Loss 0.3117242455482483\n",
      "[Training Epoch 1] Batch 3437, Loss 0.3039821982383728\n",
      "[Training Epoch 1] Batch 3438, Loss 0.2921175956726074\n",
      "[Training Epoch 1] Batch 3439, Loss 0.27796706557273865\n",
      "[Training Epoch 1] Batch 3440, Loss 0.28333258628845215\n",
      "[Training Epoch 1] Batch 3441, Loss 0.29274916648864746\n",
      "[Training Epoch 1] Batch 3442, Loss 0.3185088038444519\n",
      "[Training Epoch 1] Batch 3443, Loss 0.2868349254131317\n",
      "[Training Epoch 1] Batch 3444, Loss 0.2953193187713623\n",
      "[Training Epoch 1] Batch 3445, Loss 0.31739485263824463\n",
      "[Training Epoch 1] Batch 3446, Loss 0.2589932680130005\n",
      "[Training Epoch 1] Batch 3447, Loss 0.3026045262813568\n",
      "[Training Epoch 1] Batch 3448, Loss 0.3089764714241028\n",
      "[Training Epoch 1] Batch 3449, Loss 0.30420976877212524\n",
      "[Training Epoch 1] Batch 3450, Loss 0.2487807273864746\n",
      "[Training Epoch 1] Batch 3451, Loss 0.29980409145355225\n",
      "[Training Epoch 1] Batch 3452, Loss 0.2758258283138275\n",
      "[Training Epoch 1] Batch 3453, Loss 0.2967749834060669\n",
      "[Training Epoch 1] Batch 3454, Loss 0.30697083473205566\n",
      "[Training Epoch 1] Batch 3455, Loss 0.276705801486969\n",
      "[Training Epoch 1] Batch 3456, Loss 0.30231910943984985\n",
      "[Training Epoch 1] Batch 3457, Loss 0.2809888422489166\n",
      "[Training Epoch 1] Batch 3458, Loss 0.2732396125793457\n",
      "[Training Epoch 1] Batch 3459, Loss 0.276533305644989\n",
      "[Training Epoch 1] Batch 3460, Loss 0.27773624658584595\n",
      "[Training Epoch 1] Batch 3461, Loss 0.31601887941360474\n",
      "[Training Epoch 1] Batch 3462, Loss 0.2751951813697815\n",
      "[Training Epoch 1] Batch 3463, Loss 0.29483258724212646\n",
      "[Training Epoch 1] Batch 3464, Loss 0.284440279006958\n",
      "[Training Epoch 1] Batch 3465, Loss 0.30809059739112854\n",
      "[Training Epoch 1] Batch 3466, Loss 0.33095782995224\n",
      "[Training Epoch 1] Batch 3467, Loss 0.28864800930023193\n",
      "[Training Epoch 1] Batch 3468, Loss 0.27971357107162476\n",
      "[Training Epoch 1] Batch 3469, Loss 0.29058778285980225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m sample_generator\u001b[38;5;241m.\u001b[39minstance_a_train_loader(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_negative\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 25\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_an_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m precision, recall \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mevaluate(evaluate_data, epoch_id\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[0;32m     27\u001b[0m engine\u001b[38;5;241m.\u001b[39msave(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malias\u001b[39m\u001b[38;5;124m'\u001b[39m], epoch, precision, recall)\n",
      "File \u001b[1;32md:\\Year 3 Semester 1\\SC4020\\Project 1\\Recommendation-System-SC4020\\models_ncf\\engine.py:42\u001b[0m, in \u001b[0;36mEngine.train_an_epoch\u001b[1;34m(self, train_loader, epoch_id)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     41\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39mLongTensor)\n\u001b[0;32m     44\u001b[0m     user, item, rating \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;241m1\u001b[39m], batch[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Data\n",
    "\n",
    "ml1m_dir = 'ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "# Reindex\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n",
    "# DataLoader for training \n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "\n",
    "config = neumf_config\n",
    "engine = NeuMFEngine(config)\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    precision, recall = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    engine.save(config['alias'], epoch, precision, recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
