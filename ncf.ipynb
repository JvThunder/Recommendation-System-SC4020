{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models_ncf.neumf import NeuMFEngine\n",
    "from models_ncf.data import SampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_config = {'alias': 'neumf_first_try',\n",
    "                'num_epoch': 10,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 6040,\n",
    "                'num_items': 3706,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16, 64, 32, 16, 8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "                'l2_regularization': 0.0000001,\n",
    "                'weight_init_gaussian': True,\n",
    "                'use_cuda': True,\n",
    "                'use_bachify_eval': True,\n",
    "                'device_id': 0,\n",
    "                'pretrain': False,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_precision0.6391_recall0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_precision0.5606_recall0.2463.model'),\n",
    "                'model_dir': 'checkpoints/{}_Epoch{}_precision{:.4f}_recall{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>mid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid   mid  rating  timestamp\n",
       "0           1  1193       5  978300760\n",
       "1           1   661       3  978302109\n",
       "2           1   914       3  978301968\n",
       "3           1  3408       4  978300275\n",
       "4           1  2355       5  978824291\n",
       "...       ...   ...     ...        ...\n",
       "1000204  6040  1091       1  956716541\n",
       "1000205  6040  1094       5  956704887\n",
       "1000206  6040   562       5  956704746\n",
       "1000207  6040  1096       4  956715648\n",
       "1000208  6040  1097       4  956715569\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "ml1m_dir = 'ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "ml1m_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n",
      "Index(['userId', 'itemId', 'rating', 'real_score', 'negative_samples'], dtype='object')\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6262699365615845\n",
      "[Training Epoch 0] Batch 1, Loss 0.6340720653533936\n",
      "[Training Epoch 0] Batch 2, Loss 0.6321974396705627\n",
      "[Training Epoch 0] Batch 3, Loss 0.6328192949295044\n",
      "[Training Epoch 0] Batch 4, Loss 0.6281704902648926\n",
      "[Training Epoch 0] Batch 5, Loss 0.6260139346122742\n",
      "[Training Epoch 0] Batch 6, Loss 0.6263896226882935\n",
      "[Training Epoch 0] Batch 7, Loss 0.6270053386688232\n",
      "[Training Epoch 0] Batch 8, Loss 0.6248157024383545\n",
      "[Training Epoch 0] Batch 9, Loss 0.6308578252792358\n",
      "[Training Epoch 0] Batch 10, Loss 0.6244058609008789\n",
      "[Training Epoch 0] Batch 11, Loss 0.6255131959915161\n",
      "[Training Epoch 0] Batch 12, Loss 0.6309454441070557\n",
      "[Training Epoch 0] Batch 13, Loss 0.6272881627082825\n",
      "[Training Epoch 0] Batch 14, Loss 0.6257727146148682\n",
      "[Training Epoch 0] Batch 15, Loss 0.6288657784461975\n",
      "[Training Epoch 0] Batch 16, Loss 0.6317489743232727\n",
      "[Training Epoch 0] Batch 17, Loss 0.6258141398429871\n",
      "[Training Epoch 0] Batch 18, Loss 0.6227836608886719\n",
      "[Training Epoch 0] Batch 19, Loss 0.622951328754425\n",
      "[Training Epoch 0] Batch 20, Loss 0.6271172761917114\n",
      "[Training Epoch 0] Batch 21, Loss 0.6222894787788391\n",
      "[Training Epoch 0] Batch 22, Loss 0.6247308850288391\n",
      "[Training Epoch 0] Batch 23, Loss 0.623138427734375\n",
      "[Training Epoch 0] Batch 24, Loss 0.6207631826400757\n",
      "[Training Epoch 0] Batch 25, Loss 0.6219534873962402\n",
      "[Training Epoch 0] Batch 26, Loss 0.6254787445068359\n",
      "[Training Epoch 0] Batch 27, Loss 0.6261902451515198\n",
      "[Training Epoch 0] Batch 28, Loss 0.6248236894607544\n",
      "[Training Epoch 0] Batch 29, Loss 0.6242289543151855\n",
      "[Training Epoch 0] Batch 30, Loss 0.6191427707672119\n",
      "[Training Epoch 0] Batch 31, Loss 0.6171797513961792\n",
      "[Training Epoch 0] Batch 32, Loss 0.6263986825942993\n",
      "[Training Epoch 0] Batch 33, Loss 0.6131792068481445\n",
      "[Training Epoch 0] Batch 34, Loss 0.6138381958007812\n",
      "[Training Epoch 0] Batch 35, Loss 0.6183034181594849\n",
      "[Training Epoch 0] Batch 36, Loss 0.6159873008728027\n",
      "[Training Epoch 0] Batch 37, Loss 0.6180389523506165\n",
      "[Training Epoch 0] Batch 38, Loss 0.6165149211883545\n",
      "[Training Epoch 0] Batch 39, Loss 0.6177531480789185\n",
      "[Training Epoch 0] Batch 40, Loss 0.6201255917549133\n",
      "[Training Epoch 0] Batch 41, Loss 0.6216796636581421\n",
      "[Training Epoch 0] Batch 42, Loss 0.6172827482223511\n",
      "[Training Epoch 0] Batch 43, Loss 0.6131052374839783\n",
      "[Training Epoch 0] Batch 44, Loss 0.6137614846229553\n",
      "[Training Epoch 0] Batch 45, Loss 0.6129674315452576\n",
      "[Training Epoch 0] Batch 46, Loss 0.6144934892654419\n",
      "[Training Epoch 0] Batch 47, Loss 0.6083679795265198\n",
      "[Training Epoch 0] Batch 48, Loss 0.6143207550048828\n",
      "[Training Epoch 0] Batch 49, Loss 0.6146676540374756\n",
      "[Training Epoch 0] Batch 50, Loss 0.6131985187530518\n",
      "[Training Epoch 0] Batch 51, Loss 0.6068168878555298\n",
      "[Training Epoch 0] Batch 52, Loss 0.6150692701339722\n",
      "[Training Epoch 0] Batch 53, Loss 0.6088758111000061\n",
      "[Training Epoch 0] Batch 54, Loss 0.606311559677124\n",
      "[Training Epoch 0] Batch 55, Loss 0.6141231060028076\n",
      "[Training Epoch 0] Batch 56, Loss 0.5971487760543823\n",
      "[Training Epoch 0] Batch 57, Loss 0.6069315075874329\n",
      "[Training Epoch 0] Batch 58, Loss 0.6064574122428894\n",
      "[Training Epoch 0] Batch 59, Loss 0.6112425327301025\n",
      "[Training Epoch 0] Batch 60, Loss 0.6034244298934937\n",
      "[Training Epoch 0] Batch 61, Loss 0.597100019454956\n",
      "[Training Epoch 0] Batch 62, Loss 0.6032570004463196\n",
      "[Training Epoch 0] Batch 63, Loss 0.600536048412323\n",
      "[Training Epoch 0] Batch 64, Loss 0.5970132350921631\n",
      "[Training Epoch 0] Batch 65, Loss 0.5987221002578735\n",
      "[Training Epoch 0] Batch 66, Loss 0.6066058874130249\n",
      "[Training Epoch 0] Batch 67, Loss 0.6011342406272888\n",
      "[Training Epoch 0] Batch 68, Loss 0.5883930325508118\n",
      "[Training Epoch 0] Batch 69, Loss 0.5930032730102539\n",
      "[Training Epoch 0] Batch 70, Loss 0.5950815677642822\n",
      "[Training Epoch 0] Batch 71, Loss 0.5944739580154419\n",
      "[Training Epoch 0] Batch 72, Loss 0.5945978164672852\n",
      "[Training Epoch 0] Batch 73, Loss 0.586169958114624\n",
      "[Training Epoch 0] Batch 74, Loss 0.5948227643966675\n",
      "[Training Epoch 0] Batch 75, Loss 0.5856513977050781\n",
      "[Training Epoch 0] Batch 76, Loss 0.5929214358329773\n",
      "[Training Epoch 0] Batch 77, Loss 0.578445553779602\n",
      "[Training Epoch 0] Batch 78, Loss 0.5750027298927307\n",
      "[Training Epoch 0] Batch 79, Loss 0.5829297304153442\n",
      "[Training Epoch 0] Batch 80, Loss 0.5776582956314087\n",
      "[Training Epoch 0] Batch 81, Loss 0.5632656812667847\n",
      "[Training Epoch 0] Batch 82, Loss 0.5761371850967407\n",
      "[Training Epoch 0] Batch 83, Loss 0.5747525691986084\n",
      "[Training Epoch 0] Batch 84, Loss 0.5682846903800964\n",
      "[Training Epoch 0] Batch 85, Loss 0.5646716356277466\n",
      "[Training Epoch 0] Batch 86, Loss 0.5664017200469971\n",
      "[Training Epoch 0] Batch 87, Loss 0.5528922080993652\n",
      "[Training Epoch 0] Batch 88, Loss 0.5461769104003906\n",
      "[Training Epoch 0] Batch 89, Loss 0.5504583716392517\n",
      "[Training Epoch 0] Batch 90, Loss 0.5434787273406982\n",
      "[Training Epoch 0] Batch 91, Loss 0.5527117848396301\n",
      "[Training Epoch 0] Batch 92, Loss 0.5427583456039429\n",
      "[Training Epoch 0] Batch 93, Loss 0.5454713106155396\n",
      "[Training Epoch 0] Batch 94, Loss 0.5443427562713623\n",
      "[Training Epoch 0] Batch 95, Loss 0.5362991690635681\n",
      "[Training Epoch 0] Batch 96, Loss 0.5303589701652527\n",
      "[Training Epoch 0] Batch 97, Loss 0.515536904335022\n",
      "[Training Epoch 0] Batch 98, Loss 0.5361867547035217\n",
      "[Training Epoch 0] Batch 99, Loss 0.5184910893440247\n",
      "[Training Epoch 0] Batch 100, Loss 0.5055150985717773\n",
      "[Training Epoch 0] Batch 101, Loss 0.5219988822937012\n",
      "[Training Epoch 0] Batch 102, Loss 0.5207080841064453\n",
      "[Training Epoch 0] Batch 103, Loss 0.5079996585845947\n",
      "[Training Epoch 0] Batch 104, Loss 0.5101653337478638\n",
      "[Training Epoch 0] Batch 105, Loss 0.5158997178077698\n",
      "[Training Epoch 0] Batch 106, Loss 0.5083730220794678\n",
      "[Training Epoch 0] Batch 107, Loss 0.4912688136100769\n",
      "[Training Epoch 0] Batch 108, Loss 0.4957650899887085\n",
      "[Training Epoch 0] Batch 109, Loss 0.4927138090133667\n",
      "[Training Epoch 0] Batch 110, Loss 0.48722654581069946\n",
      "[Training Epoch 0] Batch 111, Loss 0.5161356925964355\n",
      "[Training Epoch 0] Batch 112, Loss 0.4485122561454773\n",
      "[Training Epoch 0] Batch 113, Loss 0.4623727798461914\n",
      "[Training Epoch 0] Batch 114, Loss 0.4947919249534607\n",
      "[Training Epoch 0] Batch 115, Loss 0.45253920555114746\n",
      "[Training Epoch 0] Batch 116, Loss 0.46510231494903564\n",
      "[Training Epoch 0] Batch 117, Loss 0.4517226815223694\n",
      "[Training Epoch 0] Batch 118, Loss 0.4742738902568817\n",
      "[Training Epoch 0] Batch 119, Loss 0.4887169897556305\n",
      "[Training Epoch 0] Batch 120, Loss 0.45398056507110596\n",
      "[Training Epoch 0] Batch 121, Loss 0.466894268989563\n",
      "[Training Epoch 0] Batch 122, Loss 0.4714698791503906\n",
      "[Training Epoch 0] Batch 123, Loss 0.46734753251075745\n",
      "[Training Epoch 0] Batch 124, Loss 0.4817206561565399\n",
      "[Training Epoch 0] Batch 125, Loss 0.46410566568374634\n",
      "[Training Epoch 0] Batch 126, Loss 0.4858536720275879\n",
      "[Training Epoch 0] Batch 127, Loss 0.4941413104534149\n",
      "[Training Epoch 0] Batch 128, Loss 0.48209938406944275\n",
      "[Training Epoch 0] Batch 129, Loss 0.4613155424594879\n",
      "[Training Epoch 0] Batch 130, Loss 0.476353257894516\n",
      "[Training Epoch 0] Batch 131, Loss 0.4632791578769684\n",
      "[Training Epoch 0] Batch 132, Loss 0.5449936389923096\n",
      "[Training Epoch 0] Batch 133, Loss 0.43288081884384155\n",
      "[Training Epoch 0] Batch 134, Loss 0.4973997175693512\n",
      "[Training Epoch 0] Batch 135, Loss 0.49358803033828735\n",
      "[Training Epoch 0] Batch 136, Loss 0.43477869033813477\n",
      "[Training Epoch 0] Batch 137, Loss 0.4436315894126892\n",
      "[Training Epoch 0] Batch 138, Loss 0.4491177201271057\n",
      "[Training Epoch 0] Batch 139, Loss 0.4472697973251343\n",
      "[Training Epoch 0] Batch 140, Loss 0.4999648928642273\n",
      "[Training Epoch 0] Batch 141, Loss 0.4467701315879822\n",
      "[Training Epoch 0] Batch 142, Loss 0.5014086961746216\n",
      "[Training Epoch 0] Batch 143, Loss 0.4734024703502655\n",
      "[Training Epoch 0] Batch 144, Loss 0.4504408836364746\n",
      "[Training Epoch 0] Batch 145, Loss 0.45354244112968445\n",
      "[Training Epoch 0] Batch 146, Loss 0.4394032955169678\n",
      "[Training Epoch 0] Batch 147, Loss 0.4735051989555359\n",
      "[Training Epoch 0] Batch 148, Loss 0.45848846435546875\n",
      "[Training Epoch 0] Batch 149, Loss 0.4679577052593231\n",
      "[Training Epoch 0] Batch 150, Loss 0.42942073941230774\n",
      "[Training Epoch 0] Batch 151, Loss 0.4489196538925171\n",
      "[Training Epoch 0] Batch 152, Loss 0.4417797923088074\n",
      "[Training Epoch 0] Batch 153, Loss 0.43743908405303955\n",
      "[Training Epoch 0] Batch 154, Loss 0.4605414867401123\n",
      "[Training Epoch 0] Batch 155, Loss 0.45791834592819214\n",
      "[Training Epoch 0] Batch 156, Loss 0.45158594846725464\n",
      "[Training Epoch 0] Batch 157, Loss 0.43733206391334534\n",
      "[Training Epoch 0] Batch 158, Loss 0.45763635635375977\n",
      "[Training Epoch 0] Batch 159, Loss 0.47737765312194824\n",
      "[Training Epoch 0] Batch 160, Loss 0.42319780588150024\n",
      "[Training Epoch 0] Batch 161, Loss 0.49841582775115967\n",
      "[Training Epoch 0] Batch 162, Loss 0.43636807799339294\n",
      "[Training Epoch 0] Batch 163, Loss 0.446012020111084\n",
      "[Training Epoch 0] Batch 164, Loss 0.4513663649559021\n",
      "[Training Epoch 0] Batch 165, Loss 0.4186484217643738\n",
      "[Training Epoch 0] Batch 166, Loss 0.4539785385131836\n",
      "[Training Epoch 0] Batch 167, Loss 0.4212402105331421\n",
      "[Training Epoch 0] Batch 168, Loss 0.48056885600090027\n",
      "[Training Epoch 0] Batch 169, Loss 0.4526492953300476\n",
      "[Training Epoch 0] Batch 170, Loss 0.46314966678619385\n",
      "[Training Epoch 0] Batch 171, Loss 0.43231695890426636\n",
      "[Training Epoch 0] Batch 172, Loss 0.4469544291496277\n",
      "[Training Epoch 0] Batch 173, Loss 0.43297016620635986\n",
      "[Training Epoch 0] Batch 174, Loss 0.4458167552947998\n",
      "[Training Epoch 0] Batch 175, Loss 0.41117215156555176\n",
      "[Training Epoch 0] Batch 176, Loss 0.3920590281486511\n",
      "[Training Epoch 0] Batch 177, Loss 0.4381852149963379\n",
      "[Training Epoch 0] Batch 178, Loss 0.44672173261642456\n",
      "[Training Epoch 0] Batch 179, Loss 0.4145767390727997\n",
      "[Training Epoch 0] Batch 180, Loss 0.43935710191726685\n",
      "[Training Epoch 0] Batch 181, Loss 0.4137042760848999\n",
      "[Training Epoch 0] Batch 182, Loss 0.40831902623176575\n",
      "[Training Epoch 0] Batch 183, Loss 0.4434989094734192\n",
      "[Training Epoch 0] Batch 184, Loss 0.43146026134490967\n",
      "[Training Epoch 0] Batch 185, Loss 0.46577319502830505\n",
      "[Training Epoch 0] Batch 186, Loss 0.368500679731369\n",
      "[Training Epoch 0] Batch 187, Loss 0.43069833517074585\n",
      "[Training Epoch 0] Batch 188, Loss 0.44661933183670044\n",
      "[Training Epoch 0] Batch 189, Loss 0.4368348717689514\n",
      "[Training Epoch 0] Batch 190, Loss 0.42103642225265503\n",
      "[Training Epoch 0] Batch 191, Loss 0.4020520746707916\n",
      "[Training Epoch 0] Batch 192, Loss 0.42323869466781616\n",
      "[Training Epoch 0] Batch 193, Loss 0.40190619230270386\n",
      "[Training Epoch 0] Batch 194, Loss 0.40344613790512085\n",
      "[Training Epoch 0] Batch 195, Loss 0.4241214990615845\n",
      "[Training Epoch 0] Batch 196, Loss 0.42061924934387207\n",
      "[Training Epoch 0] Batch 197, Loss 0.41085222363471985\n",
      "[Training Epoch 0] Batch 198, Loss 0.4273906350135803\n",
      "[Training Epoch 0] Batch 199, Loss 0.395398885011673\n",
      "[Training Epoch 0] Batch 200, Loss 0.4124147891998291\n",
      "[Training Epoch 0] Batch 201, Loss 0.4301207661628723\n",
      "[Training Epoch 0] Batch 202, Loss 0.44618451595306396\n",
      "[Training Epoch 0] Batch 203, Loss 0.4200545847415924\n",
      "[Training Epoch 0] Batch 204, Loss 0.4264009892940521\n",
      "[Training Epoch 0] Batch 205, Loss 0.41857343912124634\n",
      "[Training Epoch 0] Batch 206, Loss 0.41064876317977905\n",
      "[Training Epoch 0] Batch 207, Loss 0.35678380727767944\n",
      "[Training Epoch 0] Batch 208, Loss 0.41959303617477417\n",
      "[Training Epoch 0] Batch 209, Loss 0.420918345451355\n",
      "[Training Epoch 0] Batch 210, Loss 0.415872186422348\n",
      "[Training Epoch 0] Batch 211, Loss 0.42207059264183044\n",
      "[Training Epoch 0] Batch 212, Loss 0.41475361585617065\n",
      "[Training Epoch 0] Batch 213, Loss 0.3975811004638672\n",
      "[Training Epoch 0] Batch 214, Loss 0.42411404848098755\n",
      "[Training Epoch 0] Batch 215, Loss 0.38293972611427307\n",
      "[Training Epoch 0] Batch 216, Loss 0.4002302289009094\n",
      "[Training Epoch 0] Batch 217, Loss 0.37740960717201233\n",
      "[Training Epoch 0] Batch 218, Loss 0.40317103266716003\n",
      "[Training Epoch 0] Batch 219, Loss 0.39549171924591064\n",
      "[Training Epoch 0] Batch 220, Loss 0.38799887895584106\n",
      "[Training Epoch 0] Batch 221, Loss 0.4216356873512268\n",
      "[Training Epoch 0] Batch 222, Loss 0.3898079991340637\n",
      "[Training Epoch 0] Batch 223, Loss 0.39630112051963806\n",
      "[Training Epoch 0] Batch 224, Loss 0.4082142114639282\n",
      "[Training Epoch 0] Batch 225, Loss 0.39297765493392944\n",
      "[Training Epoch 0] Batch 226, Loss 0.4083796739578247\n",
      "[Training Epoch 0] Batch 227, Loss 0.38005709648132324\n",
      "[Training Epoch 0] Batch 228, Loss 0.38370025157928467\n",
      "[Training Epoch 0] Batch 229, Loss 0.3744887113571167\n",
      "[Training Epoch 0] Batch 230, Loss 0.3928990960121155\n",
      "[Training Epoch 0] Batch 231, Loss 0.3492991328239441\n",
      "[Training Epoch 0] Batch 232, Loss 0.38527441024780273\n",
      "[Training Epoch 0] Batch 233, Loss 0.40504229068756104\n",
      "[Training Epoch 0] Batch 234, Loss 0.36884066462516785\n",
      "[Training Epoch 0] Batch 235, Loss 0.38518011569976807\n",
      "[Training Epoch 0] Batch 236, Loss 0.37920060753822327\n",
      "[Training Epoch 0] Batch 237, Loss 0.39288151264190674\n",
      "[Training Epoch 0] Batch 238, Loss 0.36088934540748596\n",
      "[Training Epoch 0] Batch 239, Loss 0.385440856218338\n",
      "[Training Epoch 0] Batch 240, Loss 0.4222424626350403\n",
      "[Training Epoch 0] Batch 241, Loss 0.36300039291381836\n",
      "[Training Epoch 0] Batch 242, Loss 0.4159071743488312\n",
      "[Training Epoch 0] Batch 243, Loss 0.4205285906791687\n",
      "[Training Epoch 0] Batch 244, Loss 0.41053393483161926\n",
      "[Training Epoch 0] Batch 245, Loss 0.3707277178764343\n",
      "[Training Epoch 0] Batch 246, Loss 0.3846431374549866\n",
      "[Training Epoch 0] Batch 247, Loss 0.39582568407058716\n",
      "[Training Epoch 0] Batch 248, Loss 0.3758051097393036\n",
      "[Training Epoch 0] Batch 249, Loss 0.3665854036808014\n",
      "[Training Epoch 0] Batch 250, Loss 0.38265109062194824\n",
      "[Training Epoch 0] Batch 251, Loss 0.36955660581588745\n",
      "[Training Epoch 0] Batch 252, Loss 0.42290160059928894\n",
      "[Training Epoch 0] Batch 253, Loss 0.3784831166267395\n",
      "[Training Epoch 0] Batch 254, Loss 0.38406944274902344\n",
      "[Training Epoch 0] Batch 255, Loss 0.39336922764778137\n",
      "[Training Epoch 0] Batch 256, Loss 0.39659583568573\n",
      "[Training Epoch 0] Batch 257, Loss 0.3887365460395813\n",
      "[Training Epoch 0] Batch 258, Loss 0.39256006479263306\n",
      "[Training Epoch 0] Batch 259, Loss 0.4154803454875946\n",
      "[Training Epoch 0] Batch 260, Loss 0.37631434202194214\n",
      "[Training Epoch 0] Batch 261, Loss 0.42701777815818787\n",
      "[Training Epoch 0] Batch 262, Loss 0.39781656861305237\n",
      "[Training Epoch 0] Batch 263, Loss 0.39228585362434387\n",
      "[Training Epoch 0] Batch 264, Loss 0.3514370322227478\n",
      "[Training Epoch 0] Batch 265, Loss 0.3863435685634613\n",
      "[Training Epoch 0] Batch 266, Loss 0.4103378653526306\n",
      "[Training Epoch 0] Batch 267, Loss 0.4017367959022522\n",
      "[Training Epoch 0] Batch 268, Loss 0.39550894498825073\n",
      "[Training Epoch 0] Batch 269, Loss 0.3937138319015503\n",
      "[Training Epoch 0] Batch 270, Loss 0.37710845470428467\n",
      "[Training Epoch 0] Batch 271, Loss 0.3872174024581909\n",
      "[Training Epoch 0] Batch 272, Loss 0.4101386070251465\n",
      "[Training Epoch 0] Batch 273, Loss 0.40315675735473633\n",
      "[Training Epoch 0] Batch 274, Loss 0.3669389486312866\n",
      "[Training Epoch 0] Batch 275, Loss 0.3880741596221924\n",
      "[Training Epoch 0] Batch 276, Loss 0.40093451738357544\n",
      "[Training Epoch 0] Batch 277, Loss 0.35755184292793274\n",
      "[Training Epoch 0] Batch 278, Loss 0.4040318727493286\n",
      "[Training Epoch 0] Batch 279, Loss 0.3659353256225586\n",
      "[Training Epoch 0] Batch 280, Loss 0.38762226700782776\n",
      "[Training Epoch 0] Batch 281, Loss 0.36670488119125366\n",
      "[Training Epoch 0] Batch 282, Loss 0.37083154916763306\n",
      "[Training Epoch 0] Batch 283, Loss 0.3748810887336731\n",
      "[Training Epoch 0] Batch 284, Loss 0.4116934537887573\n",
      "[Training Epoch 0] Batch 285, Loss 0.39774420857429504\n",
      "[Training Epoch 0] Batch 286, Loss 0.39919692277908325\n",
      "[Training Epoch 0] Batch 287, Loss 0.36843281984329224\n",
      "[Training Epoch 0] Batch 288, Loss 0.37319082021713257\n",
      "[Training Epoch 0] Batch 289, Loss 0.38285648822784424\n",
      "[Training Epoch 0] Batch 290, Loss 0.3763279914855957\n",
      "[Training Epoch 0] Batch 291, Loss 0.3647903800010681\n",
      "[Training Epoch 0] Batch 292, Loss 0.3743613362312317\n",
      "[Training Epoch 0] Batch 293, Loss 0.392295241355896\n",
      "[Training Epoch 0] Batch 294, Loss 0.3701169490814209\n",
      "[Training Epoch 0] Batch 295, Loss 0.4055168330669403\n",
      "[Training Epoch 0] Batch 296, Loss 0.39030587673187256\n",
      "[Training Epoch 0] Batch 297, Loss 0.3655545115470886\n",
      "[Training Epoch 0] Batch 298, Loss 0.3871951401233673\n",
      "[Training Epoch 0] Batch 299, Loss 0.3470420241355896\n",
      "[Training Epoch 0] Batch 300, Loss 0.37225472927093506\n",
      "[Training Epoch 0] Batch 301, Loss 0.3521362543106079\n",
      "[Training Epoch 0] Batch 302, Loss 0.3956705927848816\n",
      "[Training Epoch 0] Batch 303, Loss 0.37282589077949524\n",
      "[Training Epoch 0] Batch 304, Loss 0.3812218904495239\n",
      "[Training Epoch 0] Batch 305, Loss 0.3535844385623932\n",
      "[Training Epoch 0] Batch 306, Loss 0.3944636583328247\n",
      "[Training Epoch 0] Batch 307, Loss 0.35372433066368103\n",
      "[Training Epoch 0] Batch 308, Loss 0.3353310227394104\n",
      "[Training Epoch 0] Batch 309, Loss 0.3794948160648346\n",
      "[Training Epoch 0] Batch 310, Loss 0.422194242477417\n",
      "[Training Epoch 0] Batch 311, Loss 0.38495421409606934\n",
      "[Training Epoch 0] Batch 312, Loss 0.3797416687011719\n",
      "[Training Epoch 0] Batch 313, Loss 0.3852711021900177\n",
      "[Training Epoch 0] Batch 314, Loss 0.38477540016174316\n",
      "[Training Epoch 0] Batch 315, Loss 0.38045060634613037\n",
      "[Training Epoch 0] Batch 316, Loss 0.3744942545890808\n",
      "[Training Epoch 0] Batch 317, Loss 0.40087270736694336\n",
      "[Training Epoch 0] Batch 318, Loss 0.3558388352394104\n",
      "[Training Epoch 0] Batch 319, Loss 0.4027615189552307\n",
      "[Training Epoch 0] Batch 320, Loss 0.40535223484039307\n",
      "[Training Epoch 0] Batch 321, Loss 0.3309766948223114\n",
      "[Training Epoch 0] Batch 322, Loss 0.3706292510032654\n",
      "[Training Epoch 0] Batch 323, Loss 0.3890373110771179\n",
      "[Training Epoch 0] Batch 324, Loss 0.3728353977203369\n",
      "[Training Epoch 0] Batch 325, Loss 0.39008134603500366\n",
      "[Training Epoch 0] Batch 326, Loss 0.3943009376525879\n",
      "[Training Epoch 0] Batch 327, Loss 0.3961732089519501\n",
      "[Training Epoch 0] Batch 328, Loss 0.36473777890205383\n",
      "[Training Epoch 0] Batch 329, Loss 0.4208695888519287\n",
      "[Training Epoch 0] Batch 330, Loss 0.3447265625\n",
      "[Training Epoch 0] Batch 331, Loss 0.36891448497772217\n",
      "[Training Epoch 0] Batch 332, Loss 0.3992535471916199\n",
      "[Training Epoch 0] Batch 333, Loss 0.40236949920654297\n",
      "[Training Epoch 0] Batch 334, Loss 0.3329886198043823\n",
      "[Training Epoch 0] Batch 335, Loss 0.3677023947238922\n",
      "[Training Epoch 0] Batch 336, Loss 0.3773047924041748\n",
      "[Training Epoch 0] Batch 337, Loss 0.3875838816165924\n",
      "[Training Epoch 0] Batch 338, Loss 0.374443382024765\n",
      "[Training Epoch 0] Batch 339, Loss 0.3423515260219574\n",
      "[Training Epoch 0] Batch 340, Loss 0.37152355909347534\n",
      "[Training Epoch 0] Batch 341, Loss 0.38148942589759827\n",
      "[Training Epoch 0] Batch 342, Loss 0.3662460446357727\n",
      "[Training Epoch 0] Batch 343, Loss 0.3853662610054016\n",
      "[Training Epoch 0] Batch 344, Loss 0.39389556646347046\n",
      "[Training Epoch 0] Batch 345, Loss 0.401082307100296\n",
      "[Training Epoch 0] Batch 346, Loss 0.38320887088775635\n",
      "[Training Epoch 0] Batch 347, Loss 0.38023003935813904\n",
      "[Training Epoch 0] Batch 348, Loss 0.33626872301101685\n",
      "[Training Epoch 0] Batch 349, Loss 0.3996148109436035\n",
      "[Training Epoch 0] Batch 350, Loss 0.365644246339798\n",
      "[Training Epoch 0] Batch 351, Loss 0.3906942903995514\n",
      "[Training Epoch 0] Batch 352, Loss 0.3821665644645691\n",
      "[Training Epoch 0] Batch 353, Loss 0.37049567699432373\n",
      "[Training Epoch 0] Batch 354, Loss 0.37251120805740356\n",
      "[Training Epoch 0] Batch 355, Loss 0.3719547986984253\n",
      "[Training Epoch 0] Batch 356, Loss 0.35836514830589294\n",
      "[Training Epoch 0] Batch 357, Loss 0.3579214811325073\n",
      "[Training Epoch 0] Batch 358, Loss 0.39031898975372314\n",
      "[Training Epoch 0] Batch 359, Loss 0.3885626196861267\n",
      "[Training Epoch 0] Batch 360, Loss 0.38523510098457336\n",
      "[Training Epoch 0] Batch 361, Loss 0.36725130677223206\n",
      "[Training Epoch 0] Batch 362, Loss 0.3502236008644104\n",
      "[Training Epoch 0] Batch 363, Loss 0.37223345041275024\n",
      "[Training Epoch 0] Batch 364, Loss 0.34951967000961304\n",
      "[Training Epoch 0] Batch 365, Loss 0.34249216318130493\n",
      "[Training Epoch 0] Batch 366, Loss 0.377789705991745\n",
      "[Training Epoch 0] Batch 367, Loss 0.3629069924354553\n",
      "[Training Epoch 0] Batch 368, Loss 0.38477328419685364\n",
      "[Training Epoch 0] Batch 369, Loss 0.3869222104549408\n",
      "[Training Epoch 0] Batch 370, Loss 0.37940365076065063\n",
      "[Training Epoch 0] Batch 371, Loss 0.3520301282405853\n",
      "[Training Epoch 0] Batch 372, Loss 0.37647563219070435\n",
      "[Training Epoch 0] Batch 373, Loss 0.3863523304462433\n",
      "[Training Epoch 0] Batch 374, Loss 0.3502335548400879\n",
      "[Training Epoch 0] Batch 375, Loss 0.36494341492652893\n",
      "[Training Epoch 0] Batch 376, Loss 0.3625766634941101\n",
      "[Training Epoch 0] Batch 377, Loss 0.3960166573524475\n",
      "[Training Epoch 0] Batch 378, Loss 0.37535810470581055\n",
      "[Training Epoch 0] Batch 379, Loss 0.388239324092865\n",
      "[Training Epoch 0] Batch 380, Loss 0.37929314374923706\n",
      "[Training Epoch 0] Batch 381, Loss 0.36156371235847473\n",
      "[Training Epoch 0] Batch 382, Loss 0.4056769013404846\n",
      "[Training Epoch 0] Batch 383, Loss 0.3814994692802429\n",
      "[Training Epoch 0] Batch 384, Loss 0.3758598864078522\n",
      "[Training Epoch 0] Batch 385, Loss 0.3753219544887543\n",
      "[Training Epoch 0] Batch 386, Loss 0.3621681332588196\n",
      "[Training Epoch 0] Batch 387, Loss 0.36738619208335876\n",
      "[Training Epoch 0] Batch 388, Loss 0.38466817140579224\n",
      "[Training Epoch 0] Batch 389, Loss 0.38152074813842773\n",
      "[Training Epoch 0] Batch 390, Loss 0.37853100895881653\n",
      "[Training Epoch 0] Batch 391, Loss 0.3599829077720642\n",
      "[Training Epoch 0] Batch 392, Loss 0.38700899481773376\n",
      "[Training Epoch 0] Batch 393, Loss 0.3886910378932953\n",
      "[Training Epoch 0] Batch 394, Loss 0.39243072271347046\n",
      "[Training Epoch 0] Batch 395, Loss 0.35831135511398315\n",
      "[Training Epoch 0] Batch 396, Loss 0.3601936399936676\n",
      "[Training Epoch 0] Batch 397, Loss 0.37836819887161255\n",
      "[Training Epoch 0] Batch 398, Loss 0.3828163743019104\n",
      "[Training Epoch 0] Batch 399, Loss 0.3806835114955902\n",
      "[Training Epoch 0] Batch 400, Loss 0.3850727081298828\n",
      "[Training Epoch 0] Batch 401, Loss 0.3839174509048462\n",
      "[Training Epoch 0] Batch 402, Loss 0.36493051052093506\n",
      "[Training Epoch 0] Batch 403, Loss 0.36740946769714355\n",
      "[Training Epoch 0] Batch 404, Loss 0.3824276924133301\n",
      "[Training Epoch 0] Batch 405, Loss 0.3545636832714081\n",
      "[Training Epoch 0] Batch 406, Loss 0.3638817369937897\n",
      "[Training Epoch 0] Batch 407, Loss 0.3908700942993164\n",
      "[Training Epoch 0] Batch 408, Loss 0.3818037509918213\n",
      "[Training Epoch 0] Batch 409, Loss 0.3959414064884186\n",
      "[Training Epoch 0] Batch 410, Loss 0.41495391726493835\n",
      "[Training Epoch 0] Batch 411, Loss 0.38731321692466736\n",
      "[Training Epoch 0] Batch 412, Loss 0.38279256224632263\n",
      "[Training Epoch 0] Batch 413, Loss 0.37571319937705994\n",
      "[Training Epoch 0] Batch 414, Loss 0.38161513209342957\n",
      "[Training Epoch 0] Batch 415, Loss 0.3729727566242218\n",
      "[Training Epoch 0] Batch 416, Loss 0.37256038188934326\n",
      "[Training Epoch 0] Batch 417, Loss 0.3909582495689392\n",
      "[Training Epoch 0] Batch 418, Loss 0.4045323133468628\n",
      "[Training Epoch 0] Batch 419, Loss 0.377976655960083\n",
      "[Training Epoch 0] Batch 420, Loss 0.401633083820343\n",
      "[Training Epoch 0] Batch 421, Loss 0.3538956940174103\n",
      "[Training Epoch 0] Batch 422, Loss 0.36508670449256897\n",
      "[Training Epoch 0] Batch 423, Loss 0.365071177482605\n",
      "[Training Epoch 0] Batch 424, Loss 0.3822985887527466\n",
      "[Training Epoch 0] Batch 425, Loss 0.369553804397583\n",
      "[Training Epoch 0] Batch 426, Loss 0.3696296811103821\n",
      "[Training Epoch 0] Batch 427, Loss 0.3645421862602234\n",
      "[Training Epoch 0] Batch 428, Loss 0.36771735548973083\n",
      "[Training Epoch 0] Batch 429, Loss 0.36120742559432983\n",
      "[Training Epoch 0] Batch 430, Loss 0.34596407413482666\n",
      "[Training Epoch 0] Batch 431, Loss 0.38318169116973877\n",
      "[Training Epoch 0] Batch 432, Loss 0.3790610432624817\n",
      "[Training Epoch 0] Batch 433, Loss 0.38431286811828613\n",
      "[Training Epoch 0] Batch 434, Loss 0.3594980239868164\n",
      "[Training Epoch 0] Batch 435, Loss 0.3873888850212097\n",
      "[Training Epoch 0] Batch 436, Loss 0.3676219582557678\n",
      "[Training Epoch 0] Batch 437, Loss 0.3606131076812744\n",
      "[Training Epoch 0] Batch 438, Loss 0.3867133557796478\n",
      "[Training Epoch 0] Batch 439, Loss 0.3596198558807373\n",
      "[Training Epoch 0] Batch 440, Loss 0.379300057888031\n",
      "[Training Epoch 0] Batch 441, Loss 0.3626728653907776\n",
      "[Training Epoch 0] Batch 442, Loss 0.4011212885379791\n",
      "[Training Epoch 0] Batch 443, Loss 0.3931162357330322\n",
      "[Training Epoch 0] Batch 444, Loss 0.38168931007385254\n",
      "[Training Epoch 0] Batch 445, Loss 0.39172279834747314\n",
      "[Training Epoch 0] Batch 446, Loss 0.38174968957901\n",
      "[Training Epoch 0] Batch 447, Loss 0.35443806648254395\n",
      "[Training Epoch 0] Batch 448, Loss 0.3773752450942993\n",
      "[Training Epoch 0] Batch 449, Loss 0.3781077265739441\n",
      "[Training Epoch 0] Batch 450, Loss 0.37091395258903503\n",
      "[Training Epoch 0] Batch 451, Loss 0.3831903338432312\n",
      "[Training Epoch 0] Batch 452, Loss 0.36954283714294434\n",
      "[Training Epoch 0] Batch 453, Loss 0.33695513010025024\n",
      "[Training Epoch 0] Batch 454, Loss 0.3595893085002899\n",
      "[Training Epoch 0] Batch 455, Loss 0.3605392575263977\n",
      "[Training Epoch 0] Batch 456, Loss 0.36956965923309326\n",
      "[Training Epoch 0] Batch 457, Loss 0.36596524715423584\n",
      "[Training Epoch 0] Batch 458, Loss 0.37385404109954834\n",
      "[Training Epoch 0] Batch 459, Loss 0.3952943682670593\n",
      "[Training Epoch 0] Batch 460, Loss 0.3339375853538513\n",
      "[Training Epoch 0] Batch 461, Loss 0.36821258068084717\n",
      "[Training Epoch 0] Batch 462, Loss 0.38446059823036194\n",
      "[Training Epoch 0] Batch 463, Loss 0.41593310236930847\n",
      "[Training Epoch 0] Batch 464, Loss 0.3950945734977722\n",
      "[Training Epoch 0] Batch 465, Loss 0.383081316947937\n",
      "[Training Epoch 0] Batch 466, Loss 0.3778420090675354\n",
      "[Training Epoch 0] Batch 467, Loss 0.3254910111427307\n",
      "[Training Epoch 0] Batch 468, Loss 0.3846697211265564\n",
      "[Training Epoch 0] Batch 469, Loss 0.36949849128723145\n",
      "[Training Epoch 0] Batch 470, Loss 0.4163491725921631\n",
      "[Training Epoch 0] Batch 471, Loss 0.3882608115673065\n",
      "[Training Epoch 0] Batch 472, Loss 0.3875701129436493\n",
      "[Training Epoch 0] Batch 473, Loss 0.31821444630622864\n",
      "[Training Epoch 0] Batch 474, Loss 0.37566739320755005\n",
      "[Training Epoch 0] Batch 475, Loss 0.36303457617759705\n",
      "[Training Epoch 0] Batch 476, Loss 0.38798683881759644\n",
      "[Training Epoch 0] Batch 477, Loss 0.34723079204559326\n",
      "[Training Epoch 0] Batch 478, Loss 0.3856430947780609\n",
      "[Training Epoch 0] Batch 479, Loss 0.3697830140590668\n",
      "[Training Epoch 0] Batch 480, Loss 0.38840657472610474\n",
      "[Training Epoch 0] Batch 481, Loss 0.36596882343292236\n",
      "[Training Epoch 0] Batch 482, Loss 0.37734413146972656\n",
      "[Training Epoch 0] Batch 483, Loss 0.3975142240524292\n",
      "[Training Epoch 0] Batch 484, Loss 0.3509197235107422\n",
      "[Training Epoch 0] Batch 485, Loss 0.3546963930130005\n",
      "[Training Epoch 0] Batch 486, Loss 0.37523436546325684\n",
      "[Training Epoch 0] Batch 487, Loss 0.37773245573043823\n",
      "[Training Epoch 0] Batch 488, Loss 0.3766289949417114\n",
      "[Training Epoch 0] Batch 489, Loss 0.36433008313179016\n",
      "[Training Epoch 0] Batch 490, Loss 0.3948099613189697\n",
      "[Training Epoch 0] Batch 491, Loss 0.39600977301597595\n",
      "[Training Epoch 0] Batch 492, Loss 0.3808871805667877\n",
      "[Training Epoch 0] Batch 493, Loss 0.36222538352012634\n",
      "[Training Epoch 0] Batch 494, Loss 0.36953312158584595\n",
      "[Training Epoch 0] Batch 495, Loss 0.38299593329429626\n",
      "[Training Epoch 0] Batch 496, Loss 0.38206878304481506\n",
      "[Training Epoch 0] Batch 497, Loss 0.37774813175201416\n",
      "[Training Epoch 0] Batch 498, Loss 0.3803901672363281\n",
      "[Training Epoch 0] Batch 499, Loss 0.3600674271583557\n",
      "[Training Epoch 0] Batch 500, Loss 0.3756646513938904\n",
      "[Training Epoch 0] Batch 501, Loss 0.37059861421585083\n",
      "[Training Epoch 0] Batch 502, Loss 0.35626769065856934\n",
      "[Training Epoch 0] Batch 503, Loss 0.38680750131607056\n",
      "[Training Epoch 0] Batch 504, Loss 0.380772203207016\n",
      "[Training Epoch 0] Batch 505, Loss 0.38370585441589355\n",
      "[Training Epoch 0] Batch 506, Loss 0.3783608376979828\n",
      "[Training Epoch 0] Batch 507, Loss 0.35544121265411377\n",
      "[Training Epoch 0] Batch 508, Loss 0.3520161807537079\n",
      "[Training Epoch 0] Batch 509, Loss 0.41707462072372437\n",
      "[Training Epoch 0] Batch 510, Loss 0.3653799891471863\n",
      "[Training Epoch 0] Batch 511, Loss 0.35153716802597046\n",
      "[Training Epoch 0] Batch 512, Loss 0.3679390549659729\n",
      "[Training Epoch 0] Batch 513, Loss 0.4192291796207428\n",
      "[Training Epoch 0] Batch 514, Loss 0.3653693199157715\n",
      "[Training Epoch 0] Batch 515, Loss 0.34636497497558594\n",
      "[Training Epoch 0] Batch 516, Loss 0.3657499849796295\n",
      "[Training Epoch 0] Batch 517, Loss 0.36019599437713623\n",
      "[Training Epoch 0] Batch 518, Loss 0.39962899684906006\n",
      "[Training Epoch 0] Batch 519, Loss 0.36506330966949463\n",
      "[Training Epoch 0] Batch 520, Loss 0.33821946382522583\n",
      "[Training Epoch 0] Batch 521, Loss 0.35605287551879883\n",
      "[Training Epoch 0] Batch 522, Loss 0.3746629059314728\n",
      "[Training Epoch 0] Batch 523, Loss 0.393882691860199\n",
      "[Training Epoch 0] Batch 524, Loss 0.3473907709121704\n",
      "[Training Epoch 0] Batch 525, Loss 0.35722479224205017\n",
      "[Training Epoch 0] Batch 526, Loss 0.3901109993457794\n",
      "[Training Epoch 0] Batch 527, Loss 0.3538304567337036\n",
      "[Training Epoch 0] Batch 528, Loss 0.384816437959671\n",
      "[Training Epoch 0] Batch 529, Loss 0.37964561581611633\n",
      "[Training Epoch 0] Batch 530, Loss 0.3647739887237549\n",
      "[Training Epoch 0] Batch 531, Loss 0.3548118472099304\n",
      "[Training Epoch 0] Batch 532, Loss 0.379555881023407\n",
      "[Training Epoch 0] Batch 533, Loss 0.37955498695373535\n",
      "[Training Epoch 0] Batch 534, Loss 0.36992472410202026\n",
      "[Training Epoch 0] Batch 535, Loss 0.3742503523826599\n",
      "[Training Epoch 0] Batch 536, Loss 0.37543386220932007\n",
      "[Training Epoch 0] Batch 537, Loss 0.35135161876678467\n",
      "[Training Epoch 0] Batch 538, Loss 0.3958958089351654\n",
      "[Training Epoch 0] Batch 539, Loss 0.34753310680389404\n",
      "[Training Epoch 0] Batch 540, Loss 0.3658367991447449\n",
      "[Training Epoch 0] Batch 541, Loss 0.36174190044403076\n",
      "[Training Epoch 0] Batch 542, Loss 0.3594757914543152\n",
      "[Training Epoch 0] Batch 543, Loss 0.37958207726478577\n",
      "[Training Epoch 0] Batch 544, Loss 0.4029134511947632\n",
      "[Training Epoch 0] Batch 545, Loss 0.3750341534614563\n",
      "[Training Epoch 0] Batch 546, Loss 0.35064804553985596\n",
      "[Training Epoch 0] Batch 547, Loss 0.349666953086853\n",
      "[Training Epoch 0] Batch 548, Loss 0.3744676113128662\n",
      "[Training Epoch 0] Batch 549, Loss 0.37912482023239136\n",
      "[Training Epoch 0] Batch 550, Loss 0.348934531211853\n",
      "[Training Epoch 0] Batch 551, Loss 0.39743438363075256\n",
      "[Training Epoch 0] Batch 552, Loss 0.3403194844722748\n",
      "[Training Epoch 0] Batch 553, Loss 0.3499048948287964\n",
      "[Training Epoch 0] Batch 554, Loss 0.3727591633796692\n",
      "[Training Epoch 0] Batch 555, Loss 0.34932857751846313\n",
      "[Training Epoch 0] Batch 556, Loss 0.3848152160644531\n",
      "[Training Epoch 0] Batch 557, Loss 0.4076228737831116\n",
      "[Training Epoch 0] Batch 558, Loss 0.3939753770828247\n",
      "[Training Epoch 0] Batch 559, Loss 0.40411120653152466\n",
      "[Training Epoch 0] Batch 560, Loss 0.38025254011154175\n",
      "[Training Epoch 0] Batch 561, Loss 0.36296403408050537\n",
      "[Training Epoch 0] Batch 562, Loss 0.3690061867237091\n",
      "[Training Epoch 0] Batch 563, Loss 0.3815244138240814\n",
      "[Training Epoch 0] Batch 564, Loss 0.3712211847305298\n",
      "[Training Epoch 0] Batch 565, Loss 0.38339394330978394\n",
      "[Training Epoch 0] Batch 566, Loss 0.3775900602340698\n",
      "[Training Epoch 0] Batch 567, Loss 0.4125179052352905\n",
      "[Training Epoch 0] Batch 568, Loss 0.3543921709060669\n",
      "[Training Epoch 0] Batch 569, Loss 0.3708081841468811\n",
      "[Training Epoch 0] Batch 570, Loss 0.3637118637561798\n",
      "[Training Epoch 0] Batch 571, Loss 0.38493168354034424\n",
      "[Training Epoch 0] Batch 572, Loss 0.3772069811820984\n",
      "[Training Epoch 0] Batch 573, Loss 0.37130898237228394\n",
      "[Training Epoch 0] Batch 574, Loss 0.35990196466445923\n",
      "[Training Epoch 0] Batch 575, Loss 0.3737632930278778\n",
      "[Training Epoch 0] Batch 576, Loss 0.3437637388706207\n",
      "[Training Epoch 0] Batch 577, Loss 0.3616006374359131\n",
      "[Training Epoch 0] Batch 578, Loss 0.3674970865249634\n",
      "[Training Epoch 0] Batch 579, Loss 0.3874506950378418\n",
      "[Training Epoch 0] Batch 580, Loss 0.4094122648239136\n",
      "[Training Epoch 0] Batch 581, Loss 0.35564059019088745\n",
      "[Training Epoch 0] Batch 582, Loss 0.36252790689468384\n",
      "[Training Epoch 0] Batch 583, Loss 0.37049025297164917\n",
      "[Training Epoch 0] Batch 584, Loss 0.3692174553871155\n",
      "[Training Epoch 0] Batch 585, Loss 0.3737063705921173\n",
      "[Training Epoch 0] Batch 586, Loss 0.3787807524204254\n",
      "[Training Epoch 0] Batch 587, Loss 0.3412693738937378\n",
      "[Training Epoch 0] Batch 588, Loss 0.3761575222015381\n",
      "[Training Epoch 0] Batch 589, Loss 0.3732880651950836\n",
      "[Training Epoch 0] Batch 590, Loss 0.38240528106689453\n",
      "[Training Epoch 0] Batch 591, Loss 0.3462345004081726\n",
      "[Training Epoch 0] Batch 592, Loss 0.377759724855423\n",
      "[Training Epoch 0] Batch 593, Loss 0.3649258017539978\n",
      "[Training Epoch 0] Batch 594, Loss 0.359372079372406\n",
      "[Training Epoch 0] Batch 595, Loss 0.3686504662036896\n",
      "[Training Epoch 0] Batch 596, Loss 0.393883615732193\n",
      "[Training Epoch 0] Batch 597, Loss 0.3792128562927246\n",
      "[Training Epoch 0] Batch 598, Loss 0.37823641300201416\n",
      "[Training Epoch 0] Batch 599, Loss 0.3859111964702606\n",
      "[Training Epoch 0] Batch 600, Loss 0.3859248161315918\n",
      "[Training Epoch 0] Batch 601, Loss 0.35863199830055237\n",
      "[Training Epoch 0] Batch 602, Loss 0.35534554719924927\n",
      "[Training Epoch 0] Batch 603, Loss 0.38447532057762146\n",
      "[Training Epoch 0] Batch 604, Loss 0.3876524567604065\n",
      "[Training Epoch 0] Batch 605, Loss 0.3751656115055084\n",
      "[Training Epoch 0] Batch 606, Loss 0.3540956974029541\n",
      "[Training Epoch 0] Batch 607, Loss 0.3395538628101349\n",
      "[Training Epoch 0] Batch 608, Loss 0.3509249985218048\n",
      "[Training Epoch 0] Batch 609, Loss 0.3655630350112915\n",
      "[Training Epoch 0] Batch 610, Loss 0.36393725872039795\n",
      "[Training Epoch 0] Batch 611, Loss 0.34831055998802185\n",
      "[Training Epoch 0] Batch 612, Loss 0.3917221128940582\n",
      "[Training Epoch 0] Batch 613, Loss 0.37364673614501953\n",
      "[Training Epoch 0] Batch 614, Loss 0.34674689173698425\n",
      "[Training Epoch 0] Batch 615, Loss 0.35668233036994934\n",
      "[Training Epoch 0] Batch 616, Loss 0.37848466634750366\n",
      "[Training Epoch 0] Batch 617, Loss 0.3875703513622284\n",
      "[Training Epoch 0] Batch 618, Loss 0.37151557207107544\n",
      "[Training Epoch 0] Batch 619, Loss 0.3562130331993103\n",
      "[Training Epoch 0] Batch 620, Loss 0.3757229745388031\n",
      "[Training Epoch 0] Batch 621, Loss 0.33674877882003784\n",
      "[Training Epoch 0] Batch 622, Loss 0.37238264083862305\n",
      "[Training Epoch 0] Batch 623, Loss 0.36597442626953125\n",
      "[Training Epoch 0] Batch 624, Loss 0.4078384041786194\n",
      "[Training Epoch 0] Batch 625, Loss 0.35043472051620483\n",
      "[Training Epoch 0] Batch 626, Loss 0.3708920478820801\n",
      "[Training Epoch 0] Batch 627, Loss 0.38938796520233154\n",
      "[Training Epoch 0] Batch 628, Loss 0.35432571172714233\n",
      "[Training Epoch 0] Batch 629, Loss 0.3448638319969177\n",
      "[Training Epoch 0] Batch 630, Loss 0.3460189402103424\n",
      "[Training Epoch 0] Batch 631, Loss 0.38546109199523926\n",
      "[Training Epoch 0] Batch 632, Loss 0.3723064959049225\n",
      "[Training Epoch 0] Batch 633, Loss 0.35019582509994507\n",
      "[Training Epoch 0] Batch 634, Loss 0.3659795820713043\n",
      "[Training Epoch 0] Batch 635, Loss 0.3631463348865509\n",
      "[Training Epoch 0] Batch 636, Loss 0.3409595191478729\n",
      "[Training Epoch 0] Batch 637, Loss 0.3494189381599426\n",
      "[Training Epoch 0] Batch 638, Loss 0.3553687334060669\n",
      "[Training Epoch 0] Batch 639, Loss 0.35759609937667847\n",
      "[Training Epoch 0] Batch 640, Loss 0.383831262588501\n",
      "[Training Epoch 0] Batch 641, Loss 0.37758535146713257\n",
      "[Training Epoch 0] Batch 642, Loss 0.39343369007110596\n",
      "[Training Epoch 0] Batch 643, Loss 0.3722883462905884\n",
      "[Training Epoch 0] Batch 644, Loss 0.3309986889362335\n",
      "[Training Epoch 0] Batch 645, Loss 0.37589728832244873\n",
      "[Training Epoch 0] Batch 646, Loss 0.35560789704322815\n",
      "[Training Epoch 0] Batch 647, Loss 0.37723249197006226\n",
      "[Training Epoch 0] Batch 648, Loss 0.3771994709968567\n",
      "[Training Epoch 0] Batch 649, Loss 0.34915149211883545\n",
      "[Training Epoch 0] Batch 650, Loss 0.35972005128860474\n",
      "[Training Epoch 0] Batch 651, Loss 0.33422720432281494\n",
      "[Training Epoch 0] Batch 652, Loss 0.35700762271881104\n",
      "[Training Epoch 0] Batch 653, Loss 0.34759974479675293\n",
      "[Training Epoch 0] Batch 654, Loss 0.3761860132217407\n",
      "[Training Epoch 0] Batch 655, Loss 0.39960867166519165\n",
      "[Training Epoch 0] Batch 656, Loss 0.35213011503219604\n",
      "[Training Epoch 0] Batch 657, Loss 0.34745681285858154\n",
      "[Training Epoch 0] Batch 658, Loss 0.3693650960922241\n",
      "[Training Epoch 0] Batch 659, Loss 0.38750746846199036\n",
      "[Training Epoch 0] Batch 660, Loss 0.35619983077049255\n",
      "[Training Epoch 0] Batch 661, Loss 0.35220927000045776\n",
      "[Training Epoch 0] Batch 662, Loss 0.3421141803264618\n",
      "[Training Epoch 0] Batch 663, Loss 0.3522665798664093\n",
      "[Training Epoch 0] Batch 664, Loss 0.3543132543563843\n",
      "[Training Epoch 0] Batch 665, Loss 0.36857086420059204\n",
      "[Training Epoch 0] Batch 666, Loss 0.35163772106170654\n",
      "[Training Epoch 0] Batch 667, Loss 0.377561092376709\n",
      "[Training Epoch 0] Batch 668, Loss 0.374101459980011\n",
      "[Training Epoch 0] Batch 669, Loss 0.34385889768600464\n",
      "[Training Epoch 0] Batch 670, Loss 0.37743425369262695\n",
      "[Training Epoch 0] Batch 671, Loss 0.3680073916912079\n",
      "[Training Epoch 0] Batch 672, Loss 0.3497350811958313\n",
      "[Training Epoch 0] Batch 673, Loss 0.35244280099868774\n",
      "[Training Epoch 0] Batch 674, Loss 0.37159138917922974\n",
      "[Training Epoch 0] Batch 675, Loss 0.38876205682754517\n",
      "[Training Epoch 0] Batch 676, Loss 0.3416803181171417\n",
      "[Training Epoch 0] Batch 677, Loss 0.40331554412841797\n",
      "[Training Epoch 0] Batch 678, Loss 0.3484869599342346\n",
      "[Training Epoch 0] Batch 679, Loss 0.3414827883243561\n",
      "[Training Epoch 0] Batch 680, Loss 0.3483143746852875\n",
      "[Training Epoch 0] Batch 681, Loss 0.3540589511394501\n",
      "[Training Epoch 0] Batch 682, Loss 0.3545980453491211\n",
      "[Training Epoch 0] Batch 683, Loss 0.36974239349365234\n",
      "[Training Epoch 0] Batch 684, Loss 0.36461132764816284\n",
      "[Training Epoch 0] Batch 685, Loss 0.34149253368377686\n",
      "[Training Epoch 0] Batch 686, Loss 0.3340702950954437\n",
      "[Training Epoch 0] Batch 687, Loss 0.3626203238964081\n",
      "[Training Epoch 0] Batch 688, Loss 0.37511396408081055\n",
      "[Training Epoch 0] Batch 689, Loss 0.37535953521728516\n",
      "[Training Epoch 0] Batch 690, Loss 0.37953558564186096\n",
      "[Training Epoch 0] Batch 691, Loss 0.3701764941215515\n",
      "[Training Epoch 0] Batch 692, Loss 0.35914045572280884\n",
      "[Training Epoch 0] Batch 693, Loss 0.36360061168670654\n",
      "[Training Epoch 0] Batch 694, Loss 0.33894914388656616\n",
      "[Training Epoch 0] Batch 695, Loss 0.3936380445957184\n",
      "[Training Epoch 0] Batch 696, Loss 0.38268768787384033\n",
      "[Training Epoch 0] Batch 697, Loss 0.36239176988601685\n",
      "[Training Epoch 0] Batch 698, Loss 0.3723008930683136\n",
      "[Training Epoch 0] Batch 699, Loss 0.3372933268547058\n",
      "[Training Epoch 0] Batch 700, Loss 0.3484121561050415\n",
      "[Training Epoch 0] Batch 701, Loss 0.3753628432750702\n",
      "[Training Epoch 0] Batch 702, Loss 0.3459893763065338\n",
      "[Training Epoch 0] Batch 703, Loss 0.3753526508808136\n",
      "[Training Epoch 0] Batch 704, Loss 0.3624557852745056\n",
      "[Training Epoch 0] Batch 705, Loss 0.3685331344604492\n",
      "[Training Epoch 0] Batch 706, Loss 0.3664337396621704\n",
      "[Training Epoch 0] Batch 707, Loss 0.35519570112228394\n",
      "[Training Epoch 0] Batch 708, Loss 0.3765701651573181\n",
      "[Training Epoch 0] Batch 709, Loss 0.331650972366333\n",
      "[Training Epoch 0] Batch 710, Loss 0.3674871623516083\n",
      "[Training Epoch 0] Batch 711, Loss 0.3460030257701874\n",
      "[Training Epoch 0] Batch 712, Loss 0.35989975929260254\n",
      "[Training Epoch 0] Batch 713, Loss 0.3789154291152954\n",
      "[Training Epoch 0] Batch 714, Loss 0.3777637779712677\n",
      "[Training Epoch 0] Batch 715, Loss 0.355255663394928\n",
      "[Training Epoch 0] Batch 716, Loss 0.38731586933135986\n",
      "[Training Epoch 0] Batch 717, Loss 0.3873910903930664\n",
      "[Training Epoch 0] Batch 718, Loss 0.35525548458099365\n",
      "[Training Epoch 0] Batch 719, Loss 0.38703688979148865\n",
      "[Training Epoch 0] Batch 720, Loss 0.35576915740966797\n",
      "[Training Epoch 0] Batch 721, Loss 0.3787698745727539\n",
      "[Training Epoch 0] Batch 722, Loss 0.3795301914215088\n",
      "[Training Epoch 0] Batch 723, Loss 0.3647933006286621\n",
      "[Training Epoch 0] Batch 724, Loss 0.3560032844543457\n",
      "[Training Epoch 0] Batch 725, Loss 0.34965741634368896\n",
      "[Training Epoch 0] Batch 726, Loss 0.3403814435005188\n",
      "[Training Epoch 0] Batch 727, Loss 0.39342451095581055\n",
      "[Training Epoch 0] Batch 728, Loss 0.3916637897491455\n",
      "[Training Epoch 0] Batch 729, Loss 0.3670418858528137\n",
      "[Training Epoch 0] Batch 730, Loss 0.3902394771575928\n",
      "[Training Epoch 0] Batch 731, Loss 0.3510098457336426\n",
      "[Training Epoch 0] Batch 732, Loss 0.36537283658981323\n",
      "[Training Epoch 0] Batch 733, Loss 0.3578321039676666\n",
      "[Training Epoch 0] Batch 734, Loss 0.3745235502719879\n",
      "[Training Epoch 0] Batch 735, Loss 0.3424118161201477\n",
      "[Training Epoch 0] Batch 736, Loss 0.35506099462509155\n",
      "[Training Epoch 0] Batch 737, Loss 0.3438219428062439\n",
      "[Training Epoch 0] Batch 738, Loss 0.3805757761001587\n",
      "[Training Epoch 0] Batch 739, Loss 0.34443506598472595\n",
      "[Training Epoch 0] Batch 740, Loss 0.3523406386375427\n",
      "[Training Epoch 0] Batch 741, Loss 0.3754749298095703\n",
      "[Training Epoch 0] Batch 742, Loss 0.34727609157562256\n",
      "[Training Epoch 0] Batch 743, Loss 0.36887839436531067\n",
      "[Training Epoch 0] Batch 744, Loss 0.3800398111343384\n",
      "[Training Epoch 0] Batch 745, Loss 0.39372023940086365\n",
      "[Training Epoch 0] Batch 746, Loss 0.3772006332874298\n",
      "[Training Epoch 0] Batch 747, Loss 0.37383097410202026\n",
      "[Training Epoch 0] Batch 748, Loss 0.3582565486431122\n",
      "[Training Epoch 0] Batch 749, Loss 0.321673721075058\n",
      "[Training Epoch 0] Batch 750, Loss 0.3663141131401062\n",
      "[Training Epoch 0] Batch 751, Loss 0.3679133653640747\n",
      "[Training Epoch 0] Batch 752, Loss 0.35716378688812256\n",
      "[Training Epoch 0] Batch 753, Loss 0.3702433705329895\n",
      "[Training Epoch 0] Batch 754, Loss 0.38309723138809204\n",
      "[Training Epoch 0] Batch 755, Loss 0.3413093090057373\n",
      "[Training Epoch 0] Batch 756, Loss 0.38082653284072876\n",
      "[Training Epoch 0] Batch 757, Loss 0.3807560205459595\n",
      "[Training Epoch 0] Batch 758, Loss 0.36193424463272095\n",
      "[Training Epoch 0] Batch 759, Loss 0.3325999975204468\n",
      "[Training Epoch 0] Batch 760, Loss 0.3522176444530487\n",
      "[Training Epoch 0] Batch 761, Loss 0.38409215211868286\n",
      "[Training Epoch 0] Batch 762, Loss 0.3429950773715973\n",
      "[Training Epoch 0] Batch 763, Loss 0.3426351547241211\n",
      "[Training Epoch 0] Batch 764, Loss 0.37580394744873047\n",
      "[Training Epoch 0] Batch 765, Loss 0.37187057733535767\n",
      "[Training Epoch 0] Batch 766, Loss 0.33382928371429443\n",
      "[Training Epoch 0] Batch 767, Loss 0.3920477628707886\n",
      "[Training Epoch 0] Batch 768, Loss 0.3690955638885498\n",
      "[Training Epoch 0] Batch 769, Loss 0.3726862668991089\n",
      "[Training Epoch 0] Batch 770, Loss 0.36662954092025757\n",
      "[Training Epoch 0] Batch 771, Loss 0.3411899209022522\n",
      "[Training Epoch 0] Batch 772, Loss 0.3786037564277649\n",
      "[Training Epoch 0] Batch 773, Loss 0.3599228262901306\n",
      "[Training Epoch 0] Batch 774, Loss 0.3422032296657562\n",
      "[Training Epoch 0] Batch 775, Loss 0.3466576039791107\n",
      "[Training Epoch 0] Batch 776, Loss 0.3568221926689148\n",
      "[Training Epoch 0] Batch 777, Loss 0.3371128439903259\n",
      "[Training Epoch 0] Batch 778, Loss 0.37806236743927\n",
      "[Training Epoch 0] Batch 779, Loss 0.36678680777549744\n",
      "[Training Epoch 0] Batch 780, Loss 0.3650391697883606\n",
      "[Training Epoch 0] Batch 781, Loss 0.37940648198127747\n",
      "[Training Epoch 0] Batch 782, Loss 0.36124756932258606\n",
      "[Training Epoch 0] Batch 783, Loss 0.37054306268692017\n",
      "[Training Epoch 0] Batch 784, Loss 0.35252460837364197\n",
      "[Training Epoch 0] Batch 785, Loss 0.36059659719467163\n",
      "[Training Epoch 0] Batch 786, Loss 0.35852915048599243\n",
      "[Training Epoch 0] Batch 787, Loss 0.36203110218048096\n",
      "[Training Epoch 0] Batch 788, Loss 0.36408042907714844\n",
      "[Training Epoch 0] Batch 789, Loss 0.3508680462837219\n",
      "[Training Epoch 0] Batch 790, Loss 0.3685453534126282\n",
      "[Training Epoch 0] Batch 791, Loss 0.3526129722595215\n",
      "[Training Epoch 0] Batch 792, Loss 0.3787915110588074\n",
      "[Training Epoch 0] Batch 793, Loss 0.35890305042266846\n",
      "[Training Epoch 0] Batch 794, Loss 0.37543749809265137\n",
      "[Training Epoch 0] Batch 795, Loss 0.3598012626171112\n",
      "[Training Epoch 0] Batch 796, Loss 0.3911252021789551\n",
      "[Training Epoch 0] Batch 797, Loss 0.3418419361114502\n",
      "[Training Epoch 0] Batch 798, Loss 0.3758776783943176\n",
      "[Training Epoch 0] Batch 799, Loss 0.36884361505508423\n",
      "[Training Epoch 0] Batch 800, Loss 0.37150394916534424\n",
      "[Training Epoch 0] Batch 801, Loss 0.3205813765525818\n",
      "[Training Epoch 0] Batch 802, Loss 0.3566153347492218\n",
      "[Training Epoch 0] Batch 803, Loss 0.36918389797210693\n",
      "[Training Epoch 0] Batch 804, Loss 0.34938058257102966\n",
      "[Training Epoch 0] Batch 805, Loss 0.352688729763031\n",
      "[Training Epoch 0] Batch 806, Loss 0.37094002962112427\n",
      "[Training Epoch 0] Batch 807, Loss 0.3858047127723694\n",
      "[Training Epoch 0] Batch 808, Loss 0.37422165274620056\n",
      "[Training Epoch 0] Batch 809, Loss 0.359842985868454\n",
      "[Training Epoch 0] Batch 810, Loss 0.34844064712524414\n",
      "[Training Epoch 0] Batch 811, Loss 0.3498029112815857\n",
      "[Training Epoch 0] Batch 812, Loss 0.37806129455566406\n",
      "[Training Epoch 0] Batch 813, Loss 0.36679786443710327\n",
      "[Training Epoch 0] Batch 814, Loss 0.3524850606918335\n",
      "[Training Epoch 0] Batch 815, Loss 0.3292813301086426\n",
      "[Training Epoch 0] Batch 816, Loss 0.3551434874534607\n",
      "[Training Epoch 0] Batch 817, Loss 0.35621529817581177\n",
      "[Training Epoch 0] Batch 818, Loss 0.3440631031990051\n",
      "[Training Epoch 0] Batch 819, Loss 0.3541865348815918\n",
      "[Training Epoch 0] Batch 820, Loss 0.35105741024017334\n",
      "[Training Epoch 0] Batch 821, Loss 0.3605676591396332\n",
      "[Training Epoch 0] Batch 822, Loss 0.3697143793106079\n",
      "[Training Epoch 0] Batch 823, Loss 0.35613682866096497\n",
      "[Training Epoch 0] Batch 824, Loss 0.3173675537109375\n",
      "[Training Epoch 0] Batch 825, Loss 0.3648196756839752\n",
      "[Training Epoch 0] Batch 826, Loss 0.3755224943161011\n",
      "[Training Epoch 0] Batch 827, Loss 0.36193591356277466\n",
      "[Training Epoch 0] Batch 828, Loss 0.3509520888328552\n",
      "[Training Epoch 0] Batch 829, Loss 0.33796489238739014\n",
      "[Training Epoch 0] Batch 830, Loss 0.341152548789978\n",
      "[Training Epoch 0] Batch 831, Loss 0.3593955636024475\n",
      "[Training Epoch 0] Batch 832, Loss 0.3760504126548767\n",
      "[Training Epoch 0] Batch 833, Loss 0.35721275210380554\n",
      "[Training Epoch 0] Batch 834, Loss 0.35675546526908875\n",
      "[Training Epoch 0] Batch 835, Loss 0.35176464915275574\n",
      "[Training Epoch 0] Batch 836, Loss 0.3532227873802185\n",
      "[Training Epoch 0] Batch 837, Loss 0.35497137904167175\n",
      "[Training Epoch 0] Batch 838, Loss 0.3636610507965088\n",
      "[Training Epoch 0] Batch 839, Loss 0.36191579699516296\n",
      "[Training Epoch 0] Batch 840, Loss 0.37515634298324585\n",
      "[Training Epoch 0] Batch 841, Loss 0.3451080024242401\n",
      "[Training Epoch 0] Batch 842, Loss 0.3504824638366699\n",
      "[Training Epoch 0] Batch 843, Loss 0.3841215968132019\n",
      "[Training Epoch 0] Batch 844, Loss 0.3567409813404083\n",
      "[Training Epoch 0] Batch 845, Loss 0.379561185836792\n",
      "[Training Epoch 0] Batch 846, Loss 0.3714261054992676\n",
      "[Training Epoch 0] Batch 847, Loss 0.3511466979980469\n",
      "[Training Epoch 0] Batch 848, Loss 0.3432372212409973\n",
      "[Training Epoch 0] Batch 849, Loss 0.3495063781738281\n",
      "[Training Epoch 0] Batch 850, Loss 0.3488251268863678\n",
      "[Training Epoch 0] Batch 851, Loss 0.35181567072868347\n",
      "[Training Epoch 0] Batch 852, Loss 0.3704764246940613\n",
      "[Training Epoch 0] Batch 853, Loss 0.3681207299232483\n",
      "[Training Epoch 0] Batch 854, Loss 0.32858937978744507\n",
      "[Training Epoch 0] Batch 855, Loss 0.3297570049762726\n",
      "[Training Epoch 0] Batch 856, Loss 0.35998156666755676\n",
      "[Training Epoch 0] Batch 857, Loss 0.3481345474720001\n",
      "[Training Epoch 0] Batch 858, Loss 0.36607593297958374\n",
      "[Training Epoch 0] Batch 859, Loss 0.34986716508865356\n",
      "[Training Epoch 0] Batch 860, Loss 0.38325488567352295\n",
      "[Training Epoch 0] Batch 861, Loss 0.375616192817688\n",
      "[Training Epoch 0] Batch 862, Loss 0.3686189651489258\n",
      "[Training Epoch 0] Batch 863, Loss 0.33650702238082886\n",
      "[Training Epoch 0] Batch 864, Loss 0.340042382478714\n",
      "[Training Epoch 0] Batch 865, Loss 0.3910766839981079\n",
      "[Training Epoch 0] Batch 866, Loss 0.3900434374809265\n",
      "[Training Epoch 0] Batch 867, Loss 0.35429155826568604\n",
      "[Training Epoch 0] Batch 868, Loss 0.316478967666626\n",
      "[Training Epoch 0] Batch 869, Loss 0.3573487401008606\n",
      "[Training Epoch 0] Batch 870, Loss 0.3576648235321045\n",
      "[Training Epoch 0] Batch 871, Loss 0.3458736538887024\n",
      "[Training Epoch 0] Batch 872, Loss 0.3857988715171814\n",
      "[Training Epoch 0] Batch 873, Loss 0.36027976870536804\n",
      "[Training Epoch 0] Batch 874, Loss 0.3589770793914795\n",
      "[Training Epoch 0] Batch 875, Loss 0.37309908866882324\n",
      "[Training Epoch 0] Batch 876, Loss 0.3435859978199005\n",
      "[Training Epoch 0] Batch 877, Loss 0.34651675820350647\n",
      "[Training Epoch 0] Batch 878, Loss 0.3455854654312134\n",
      "[Training Epoch 0] Batch 879, Loss 0.35203972458839417\n",
      "[Training Epoch 0] Batch 880, Loss 0.344068706035614\n",
      "[Training Epoch 0] Batch 881, Loss 0.3518260717391968\n",
      "[Training Epoch 0] Batch 882, Loss 0.35631775856018066\n",
      "[Training Epoch 0] Batch 883, Loss 0.36068078875541687\n",
      "[Training Epoch 0] Batch 884, Loss 0.38672196865081787\n",
      "[Training Epoch 0] Batch 885, Loss 0.3447760045528412\n",
      "[Training Epoch 0] Batch 886, Loss 0.3451906442642212\n",
      "[Training Epoch 0] Batch 887, Loss 0.3400212526321411\n",
      "[Training Epoch 0] Batch 888, Loss 0.3643355965614319\n",
      "[Training Epoch 0] Batch 889, Loss 0.34459948539733887\n",
      "[Training Epoch 0] Batch 890, Loss 0.34485936164855957\n",
      "[Training Epoch 0] Batch 891, Loss 0.34895509481430054\n",
      "[Training Epoch 0] Batch 892, Loss 0.4212009906768799\n",
      "[Training Epoch 0] Batch 893, Loss 0.3434436619281769\n",
      "[Training Epoch 0] Batch 894, Loss 0.3584133982658386\n",
      "[Training Epoch 0] Batch 895, Loss 0.35096773505210876\n",
      "[Training Epoch 0] Batch 896, Loss 0.37339720129966736\n",
      "[Training Epoch 0] Batch 897, Loss 0.4073546230792999\n",
      "[Training Epoch 0] Batch 898, Loss 0.3587741553783417\n",
      "[Training Epoch 0] Batch 899, Loss 0.35882502794265747\n",
      "[Training Epoch 0] Batch 900, Loss 0.3549630045890808\n",
      "[Training Epoch 0] Batch 901, Loss 0.37659990787506104\n",
      "[Training Epoch 0] Batch 902, Loss 0.33099648356437683\n",
      "[Training Epoch 0] Batch 903, Loss 0.3464459776878357\n",
      "[Training Epoch 0] Batch 904, Loss 0.34132516384124756\n",
      "[Training Epoch 0] Batch 905, Loss 0.3664235472679138\n",
      "[Training Epoch 0] Batch 906, Loss 0.3502836227416992\n",
      "[Training Epoch 0] Batch 907, Loss 0.3668432831764221\n",
      "[Training Epoch 0] Batch 908, Loss 0.36103248596191406\n",
      "[Training Epoch 0] Batch 909, Loss 0.38700222969055176\n",
      "[Training Epoch 0] Batch 910, Loss 0.33540987968444824\n",
      "[Training Epoch 0] Batch 911, Loss 0.35801488161087036\n",
      "[Training Epoch 0] Batch 912, Loss 0.36452144384384155\n",
      "[Training Epoch 0] Batch 913, Loss 0.3598286509513855\n",
      "[Training Epoch 0] Batch 914, Loss 0.34843623638153076\n",
      "[Training Epoch 0] Batch 915, Loss 0.3534110188484192\n",
      "[Training Epoch 0] Batch 916, Loss 0.35145705938339233\n",
      "[Training Epoch 0] Batch 917, Loss 0.3474964499473572\n",
      "[Training Epoch 0] Batch 918, Loss 0.35735023021698\n",
      "[Training Epoch 0] Batch 919, Loss 0.33930474519729614\n",
      "[Training Epoch 0] Batch 920, Loss 0.36496102809906006\n",
      "[Training Epoch 0] Batch 921, Loss 0.32519352436065674\n",
      "[Training Epoch 0] Batch 922, Loss 0.3324499726295471\n",
      "[Training Epoch 0] Batch 923, Loss 0.35865968465805054\n",
      "[Training Epoch 0] Batch 924, Loss 0.34948763251304626\n",
      "[Training Epoch 0] Batch 925, Loss 0.3395382761955261\n",
      "[Training Epoch 0] Batch 926, Loss 0.3368285596370697\n",
      "[Training Epoch 0] Batch 927, Loss 0.34582406282424927\n",
      "[Training Epoch 0] Batch 928, Loss 0.390021413564682\n",
      "[Training Epoch 0] Batch 929, Loss 0.39394113421440125\n",
      "[Training Epoch 0] Batch 930, Loss 0.3759685158729553\n",
      "[Training Epoch 0] Batch 931, Loss 0.3532143831253052\n",
      "[Training Epoch 0] Batch 932, Loss 0.3409537672996521\n",
      "[Training Epoch 0] Batch 933, Loss 0.3436366319656372\n",
      "[Training Epoch 0] Batch 934, Loss 0.37566903233528137\n",
      "[Training Epoch 0] Batch 935, Loss 0.385963499546051\n",
      "[Training Epoch 0] Batch 936, Loss 0.37854063510894775\n",
      "[Training Epoch 0] Batch 937, Loss 0.35450124740600586\n",
      "[Training Epoch 0] Batch 938, Loss 0.35431545972824097\n",
      "[Training Epoch 0] Batch 939, Loss 0.3731573820114136\n",
      "[Training Epoch 0] Batch 940, Loss 0.3564254641532898\n",
      "[Training Epoch 0] Batch 941, Loss 0.36311137676239014\n",
      "[Training Epoch 0] Batch 942, Loss 0.34023717045783997\n",
      "[Training Epoch 0] Batch 943, Loss 0.3341217637062073\n",
      "[Training Epoch 0] Batch 944, Loss 0.3758406937122345\n",
      "[Training Epoch 0] Batch 945, Loss 0.3841347098350525\n",
      "[Training Epoch 0] Batch 946, Loss 0.3601266145706177\n",
      "[Training Epoch 0] Batch 947, Loss 0.3417041301727295\n",
      "[Training Epoch 0] Batch 948, Loss 0.35057368874549866\n",
      "[Training Epoch 0] Batch 949, Loss 0.3491854965686798\n",
      "[Training Epoch 0] Batch 950, Loss 0.35191673040390015\n",
      "[Training Epoch 0] Batch 951, Loss 0.36600857973098755\n",
      "[Training Epoch 0] Batch 952, Loss 0.37851682305336\n",
      "[Training Epoch 0] Batch 953, Loss 0.36511704325675964\n",
      "[Training Epoch 0] Batch 954, Loss 0.36493217945098877\n",
      "[Training Epoch 0] Batch 955, Loss 0.3419671654701233\n",
      "[Training Epoch 0] Batch 956, Loss 0.35762619972229004\n",
      "[Training Epoch 0] Batch 957, Loss 0.3705099821090698\n",
      "[Training Epoch 0] Batch 958, Loss 0.34547334909439087\n",
      "[Training Epoch 0] Batch 959, Loss 0.3583514094352722\n",
      "[Training Epoch 0] Batch 960, Loss 0.36244550347328186\n",
      "[Training Epoch 0] Batch 961, Loss 0.332479864358902\n",
      "[Training Epoch 0] Batch 962, Loss 0.3437955975532532\n",
      "[Training Epoch 0] Batch 963, Loss 0.397236704826355\n",
      "[Training Epoch 0] Batch 964, Loss 0.3711635172367096\n",
      "[Training Epoch 0] Batch 965, Loss 0.37039557099342346\n",
      "[Training Epoch 0] Batch 966, Loss 0.3553010821342468\n",
      "[Training Epoch 0] Batch 967, Loss 0.363112211227417\n",
      "[Training Epoch 0] Batch 968, Loss 0.38254255056381226\n",
      "[Training Epoch 0] Batch 969, Loss 0.3493858873844147\n",
      "[Training Epoch 0] Batch 970, Loss 0.3739624321460724\n",
      "[Training Epoch 0] Batch 971, Loss 0.3579685091972351\n",
      "[Training Epoch 0] Batch 972, Loss 0.346092164516449\n",
      "[Training Epoch 0] Batch 973, Loss 0.3545927107334137\n",
      "[Training Epoch 0] Batch 974, Loss 0.34789353609085083\n",
      "[Training Epoch 0] Batch 975, Loss 0.3744699954986572\n",
      "[Training Epoch 0] Batch 976, Loss 0.31421446800231934\n",
      "[Training Epoch 0] Batch 977, Loss 0.35512423515319824\n",
      "[Training Epoch 0] Batch 978, Loss 0.3980942964553833\n",
      "[Training Epoch 0] Batch 979, Loss 0.3648623526096344\n",
      "[Training Epoch 0] Batch 980, Loss 0.3586808741092682\n",
      "[Training Epoch 0] Batch 981, Loss 0.37662041187286377\n",
      "[Training Epoch 0] Batch 982, Loss 0.36167991161346436\n",
      "[Training Epoch 0] Batch 983, Loss 0.3573606610298157\n",
      "[Training Epoch 0] Batch 984, Loss 0.3720893859863281\n",
      "[Training Epoch 0] Batch 985, Loss 0.3563671410083771\n",
      "[Training Epoch 0] Batch 986, Loss 0.3834579586982727\n",
      "[Training Epoch 0] Batch 987, Loss 0.33953380584716797\n",
      "[Training Epoch 0] Batch 988, Loss 0.34882858395576477\n",
      "[Training Epoch 0] Batch 989, Loss 0.3358178734779358\n",
      "[Training Epoch 0] Batch 990, Loss 0.3898172080516815\n",
      "[Training Epoch 0] Batch 991, Loss 0.35366663336753845\n",
      "[Training Epoch 0] Batch 992, Loss 0.34212446212768555\n",
      "[Training Epoch 0] Batch 993, Loss 0.37289857864379883\n",
      "[Training Epoch 0] Batch 994, Loss 0.3522016406059265\n",
      "[Training Epoch 0] Batch 995, Loss 0.36261969804763794\n",
      "[Training Epoch 0] Batch 996, Loss 0.36739206314086914\n",
      "[Training Epoch 0] Batch 997, Loss 0.37161487340927124\n",
      "[Training Epoch 0] Batch 998, Loss 0.37889641523361206\n",
      "[Training Epoch 0] Batch 999, Loss 0.3430746793746948\n",
      "[Training Epoch 0] Batch 1000, Loss 0.3303396701812744\n",
      "[Training Epoch 0] Batch 1001, Loss 0.3647523522377014\n",
      "[Training Epoch 0] Batch 1002, Loss 0.36648261547088623\n",
      "[Training Epoch 0] Batch 1003, Loss 0.3592652380466461\n",
      "[Training Epoch 0] Batch 1004, Loss 0.34799495339393616\n",
      "[Training Epoch 0] Batch 1005, Loss 0.3529723584651947\n",
      "[Training Epoch 0] Batch 1006, Loss 0.3464008867740631\n",
      "[Training Epoch 0] Batch 1007, Loss 0.35634034872055054\n",
      "[Training Epoch 0] Batch 1008, Loss 0.3700544536113739\n",
      "[Training Epoch 0] Batch 1009, Loss 0.37698644399642944\n",
      "[Training Epoch 0] Batch 1010, Loss 0.366270512342453\n",
      "[Training Epoch 0] Batch 1011, Loss 0.34201693534851074\n",
      "[Training Epoch 0] Batch 1012, Loss 0.35987502336502075\n",
      "[Training Epoch 0] Batch 1013, Loss 0.34135547280311584\n",
      "[Training Epoch 0] Batch 1014, Loss 0.34899401664733887\n",
      "[Training Epoch 0] Batch 1015, Loss 0.32596200704574585\n",
      "[Training Epoch 0] Batch 1016, Loss 0.37464791536331177\n",
      "[Training Epoch 0] Batch 1017, Loss 0.36087948083877563\n",
      "[Training Epoch 0] Batch 1018, Loss 0.3286028802394867\n",
      "[Training Epoch 0] Batch 1019, Loss 0.33513569831848145\n",
      "[Training Epoch 0] Batch 1020, Loss 0.3496796488761902\n",
      "[Training Epoch 0] Batch 1021, Loss 0.3492873013019562\n",
      "[Training Epoch 0] Batch 1022, Loss 0.3475959897041321\n",
      "[Training Epoch 0] Batch 1023, Loss 0.30694490671157837\n",
      "[Training Epoch 0] Batch 1024, Loss 0.3665899932384491\n",
      "[Training Epoch 0] Batch 1025, Loss 0.34693443775177\n",
      "[Training Epoch 0] Batch 1026, Loss 0.3558597266674042\n",
      "[Training Epoch 0] Batch 1027, Loss 0.3715093731880188\n",
      "[Training Epoch 0] Batch 1028, Loss 0.3204500079154968\n",
      "[Training Epoch 0] Batch 1029, Loss 0.3417954444885254\n",
      "[Training Epoch 0] Batch 1030, Loss 0.3581130802631378\n",
      "[Training Epoch 0] Batch 1031, Loss 0.3763292729854584\n",
      "[Training Epoch 0] Batch 1032, Loss 0.350538969039917\n",
      "[Training Epoch 0] Batch 1033, Loss 0.39013081789016724\n",
      "[Training Epoch 0] Batch 1034, Loss 0.3707822859287262\n",
      "[Training Epoch 0] Batch 1035, Loss 0.3307521343231201\n",
      "[Training Epoch 0] Batch 1036, Loss 0.3462187647819519\n",
      "[Training Epoch 0] Batch 1037, Loss 0.35545796155929565\n",
      "[Training Epoch 0] Batch 1038, Loss 0.3662712872028351\n",
      "[Training Epoch 0] Batch 1039, Loss 0.3720282316207886\n",
      "[Training Epoch 0] Batch 1040, Loss 0.364876389503479\n",
      "[Training Epoch 0] Batch 1041, Loss 0.3912582993507385\n",
      "[Training Epoch 0] Batch 1042, Loss 0.35257387161254883\n",
      "[Training Epoch 0] Batch 1043, Loss 0.35774144530296326\n",
      "[Training Epoch 0] Batch 1044, Loss 0.36211317777633667\n",
      "[Training Epoch 0] Batch 1045, Loss 0.3630927801132202\n",
      "[Training Epoch 0] Batch 1046, Loss 0.38250279426574707\n",
      "[Training Epoch 0] Batch 1047, Loss 0.3747197985649109\n",
      "[Training Epoch 0] Batch 1048, Loss 0.37426501512527466\n",
      "[Training Epoch 0] Batch 1049, Loss 0.36190053820610046\n",
      "[Training Epoch 0] Batch 1050, Loss 0.34795960783958435\n",
      "[Training Epoch 0] Batch 1051, Loss 0.3664509057998657\n",
      "[Training Epoch 0] Batch 1052, Loss 0.3814564645290375\n",
      "[Training Epoch 0] Batch 1053, Loss 0.3728587031364441\n",
      "[Training Epoch 0] Batch 1054, Loss 0.3602695167064667\n",
      "[Training Epoch 0] Batch 1055, Loss 0.37449413537979126\n",
      "[Training Epoch 0] Batch 1056, Loss 0.34902113676071167\n",
      "[Training Epoch 0] Batch 1057, Loss 0.35719162225723267\n",
      "[Training Epoch 0] Batch 1058, Loss 0.38696378469467163\n",
      "[Training Epoch 0] Batch 1059, Loss 0.35111916065216064\n",
      "[Training Epoch 0] Batch 1060, Loss 0.3724491596221924\n",
      "[Training Epoch 0] Batch 1061, Loss 0.35145503282546997\n",
      "[Training Epoch 0] Batch 1062, Loss 0.3502925634384155\n",
      "[Training Epoch 0] Batch 1063, Loss 0.3514217138290405\n",
      "[Training Epoch 0] Batch 1064, Loss 0.3299493193626404\n",
      "[Training Epoch 0] Batch 1065, Loss 0.3497318625450134\n",
      "[Training Epoch 0] Batch 1066, Loss 0.3525959253311157\n",
      "[Training Epoch 0] Batch 1067, Loss 0.36618441343307495\n",
      "[Training Epoch 0] Batch 1068, Loss 0.3397003412246704\n",
      "[Training Epoch 0] Batch 1069, Loss 0.3670693039894104\n",
      "[Training Epoch 0] Batch 1070, Loss 0.3483785390853882\n",
      "[Training Epoch 0] Batch 1071, Loss 0.35448455810546875\n",
      "[Training Epoch 0] Batch 1072, Loss 0.3956741392612457\n",
      "[Training Epoch 0] Batch 1073, Loss 0.36646902561187744\n",
      "[Training Epoch 0] Batch 1074, Loss 0.3552457094192505\n",
      "[Training Epoch 0] Batch 1075, Loss 0.3324359655380249\n",
      "[Training Epoch 0] Batch 1076, Loss 0.35814082622528076\n",
      "[Training Epoch 0] Batch 1077, Loss 0.33652734756469727\n",
      "[Training Epoch 0] Batch 1078, Loss 0.34203672409057617\n",
      "[Training Epoch 0] Batch 1079, Loss 0.36111682653427124\n",
      "[Training Epoch 0] Batch 1080, Loss 0.3441147804260254\n",
      "[Training Epoch 0] Batch 1081, Loss 0.3664630651473999\n",
      "[Training Epoch 0] Batch 1082, Loss 0.343254029750824\n",
      "[Training Epoch 0] Batch 1083, Loss 0.32363227009773254\n",
      "[Training Epoch 0] Batch 1084, Loss 0.35880276560783386\n",
      "[Training Epoch 0] Batch 1085, Loss 0.34742414951324463\n",
      "[Training Epoch 0] Batch 1086, Loss 0.34858810901641846\n",
      "[Training Epoch 0] Batch 1087, Loss 0.35814225673675537\n",
      "[Training Epoch 0] Batch 1088, Loss 0.31285783648490906\n",
      "[Training Epoch 0] Batch 1089, Loss 0.3391721546649933\n",
      "[Training Epoch 0] Batch 1090, Loss 0.34437859058380127\n",
      "[Training Epoch 0] Batch 1091, Loss 0.37952321767807007\n",
      "[Training Epoch 0] Batch 1092, Loss 0.3533574044704437\n",
      "[Training Epoch 0] Batch 1093, Loss 0.3399133086204529\n",
      "[Training Epoch 0] Batch 1094, Loss 0.38238367438316345\n",
      "[Training Epoch 0] Batch 1095, Loss 0.3536560833454132\n",
      "[Training Epoch 0] Batch 1096, Loss 0.3333049714565277\n",
      "[Training Epoch 0] Batch 1097, Loss 0.35334277153015137\n",
      "[Training Epoch 0] Batch 1098, Loss 0.3594539761543274\n",
      "[Training Epoch 0] Batch 1099, Loss 0.36435467004776\n",
      "[Training Epoch 0] Batch 1100, Loss 0.34726184606552124\n",
      "[Training Epoch 0] Batch 1101, Loss 0.3429454267024994\n",
      "[Training Epoch 0] Batch 1102, Loss 0.36378419399261475\n",
      "[Training Epoch 0] Batch 1103, Loss 0.38886573910713196\n",
      "[Training Epoch 0] Batch 1104, Loss 0.35505908727645874\n",
      "[Training Epoch 0] Batch 1105, Loss 0.3187156319618225\n",
      "[Training Epoch 0] Batch 1106, Loss 0.34129124879837036\n",
      "[Training Epoch 0] Batch 1107, Loss 0.3808225393295288\n",
      "[Training Epoch 0] Batch 1108, Loss 0.3568156361579895\n",
      "[Training Epoch 0] Batch 1109, Loss 0.34713220596313477\n",
      "[Training Epoch 0] Batch 1110, Loss 0.3432621359825134\n",
      "[Training Epoch 0] Batch 1111, Loss 0.3413456678390503\n",
      "[Training Epoch 0] Batch 1112, Loss 0.3672463893890381\n",
      "[Training Epoch 0] Batch 1113, Loss 0.34669044613838196\n",
      "[Training Epoch 0] Batch 1114, Loss 0.3524577021598816\n",
      "[Training Epoch 0] Batch 1115, Loss 0.33581238985061646\n",
      "[Training Epoch 0] Batch 1116, Loss 0.338758647441864\n",
      "[Training Epoch 0] Batch 1117, Loss 0.324872761964798\n",
      "[Training Epoch 0] Batch 1118, Loss 0.3358708620071411\n",
      "[Training Epoch 0] Batch 1119, Loss 0.32965609431266785\n",
      "[Training Epoch 0] Batch 1120, Loss 0.34708547592163086\n",
      "[Training Epoch 0] Batch 1121, Loss 0.32500404119491577\n",
      "[Training Epoch 0] Batch 1122, Loss 0.3554466962814331\n",
      "[Training Epoch 0] Batch 1123, Loss 0.33532243967056274\n",
      "[Training Epoch 0] Batch 1124, Loss 0.36475205421447754\n",
      "[Training Epoch 0] Batch 1125, Loss 0.35186100006103516\n",
      "[Training Epoch 0] Batch 1126, Loss 0.35085418820381165\n",
      "[Training Epoch 0] Batch 1127, Loss 0.33382734656333923\n",
      "[Training Epoch 0] Batch 1128, Loss 0.3414084315299988\n",
      "[Training Epoch 0] Batch 1129, Loss 0.3549295663833618\n",
      "[Training Epoch 0] Batch 1130, Loss 0.36336231231689453\n",
      "[Training Epoch 0] Batch 1131, Loss 0.35697293281555176\n",
      "[Training Epoch 0] Batch 1132, Loss 0.3203209340572357\n",
      "[Training Epoch 0] Batch 1133, Loss 0.3508906066417694\n",
      "[Training Epoch 0] Batch 1134, Loss 0.3378264009952545\n",
      "[Training Epoch 0] Batch 1135, Loss 0.3387117087841034\n",
      "[Training Epoch 0] Batch 1136, Loss 0.3642720580101013\n",
      "[Training Epoch 0] Batch 1137, Loss 0.3501055836677551\n",
      "[Training Epoch 0] Batch 1138, Loss 0.3846193552017212\n",
      "[Training Epoch 0] Batch 1139, Loss 0.37787821888923645\n",
      "[Training Epoch 0] Batch 1140, Loss 0.3407732844352722\n",
      "[Training Epoch 0] Batch 1141, Loss 0.35744303464889526\n",
      "[Training Epoch 0] Batch 1142, Loss 0.3143909275531769\n",
      "[Training Epoch 0] Batch 1143, Loss 0.349697470664978\n",
      "[Training Epoch 0] Batch 1144, Loss 0.36026158928871155\n",
      "[Training Epoch 0] Batch 1145, Loss 0.34597092866897583\n",
      "[Training Epoch 0] Batch 1146, Loss 0.3406059443950653\n",
      "[Training Epoch 0] Batch 1147, Loss 0.3632066547870636\n",
      "[Training Epoch 0] Batch 1148, Loss 0.3529905378818512\n",
      "[Training Epoch 0] Batch 1149, Loss 0.35878708958625793\n",
      "[Training Epoch 0] Batch 1150, Loss 0.36215776205062866\n",
      "[Training Epoch 0] Batch 1151, Loss 0.3273598551750183\n",
      "[Training Epoch 0] Batch 1152, Loss 0.3472326397895813\n",
      "[Training Epoch 0] Batch 1153, Loss 0.33921390771865845\n",
      "[Training Epoch 0] Batch 1154, Loss 0.35228514671325684\n",
      "[Training Epoch 0] Batch 1155, Loss 0.3791806101799011\n",
      "[Training Epoch 0] Batch 1156, Loss 0.3654325008392334\n",
      "[Training Epoch 0] Batch 1157, Loss 0.32011502981185913\n",
      "[Training Epoch 0] Batch 1158, Loss 0.32656964659690857\n",
      "[Training Epoch 0] Batch 1159, Loss 0.30774986743927\n",
      "[Training Epoch 0] Batch 1160, Loss 0.3508255183696747\n",
      "[Training Epoch 0] Batch 1161, Loss 0.3701360523700714\n",
      "[Training Epoch 0] Batch 1162, Loss 0.3578760325908661\n",
      "[Training Epoch 0] Batch 1163, Loss 0.3266477882862091\n",
      "[Training Epoch 0] Batch 1164, Loss 0.3508418798446655\n",
      "[Training Epoch 0] Batch 1165, Loss 0.35281652212142944\n",
      "[Training Epoch 0] Batch 1166, Loss 0.3364235758781433\n",
      "[Training Epoch 0] Batch 1167, Loss 0.37019428610801697\n",
      "[Training Epoch 0] Batch 1168, Loss 0.30628296732902527\n",
      "[Training Epoch 0] Batch 1169, Loss 0.34763145446777344\n",
      "[Training Epoch 0] Batch 1170, Loss 0.35111308097839355\n",
      "[Training Epoch 0] Batch 1171, Loss 0.37646761536598206\n",
      "[Training Epoch 0] Batch 1172, Loss 0.3339764475822449\n",
      "[Training Epoch 0] Batch 1173, Loss 0.36536186933517456\n",
      "[Training Epoch 0] Batch 1174, Loss 0.3581106662750244\n",
      "[Training Epoch 0] Batch 1175, Loss 0.3604637384414673\n",
      "[Training Epoch 0] Batch 1176, Loss 0.3425687551498413\n",
      "[Training Epoch 0] Batch 1177, Loss 0.3593308925628662\n",
      "[Training Epoch 0] Batch 1178, Loss 0.37589797377586365\n",
      "[Training Epoch 0] Batch 1179, Loss 0.3494099974632263\n",
      "[Training Epoch 0] Batch 1180, Loss 0.364431232213974\n",
      "[Training Epoch 0] Batch 1181, Loss 0.3557695150375366\n",
      "[Training Epoch 0] Batch 1182, Loss 0.37317517399787903\n",
      "[Training Epoch 0] Batch 1183, Loss 0.32445716857910156\n",
      "[Training Epoch 0] Batch 1184, Loss 0.38232165575027466\n",
      "[Training Epoch 0] Batch 1185, Loss 0.33729785680770874\n",
      "[Training Epoch 0] Batch 1186, Loss 0.3474111557006836\n",
      "[Training Epoch 0] Batch 1187, Loss 0.32830679416656494\n",
      "[Training Epoch 0] Batch 1188, Loss 0.3506489098072052\n",
      "[Training Epoch 0] Batch 1189, Loss 0.34634318947792053\n",
      "[Training Epoch 0] Batch 1190, Loss 0.3212921619415283\n",
      "[Training Epoch 0] Batch 1191, Loss 0.36357223987579346\n",
      "[Training Epoch 0] Batch 1192, Loss 0.34168127179145813\n",
      "[Training Epoch 0] Batch 1193, Loss 0.37061238288879395\n",
      "[Training Epoch 0] Batch 1194, Loss 0.3774094581604004\n",
      "[Training Epoch 0] Batch 1195, Loss 0.3576555848121643\n",
      "[Training Epoch 0] Batch 1196, Loss 0.34493768215179443\n",
      "[Training Epoch 0] Batch 1197, Loss 0.36929595470428467\n",
      "[Training Epoch 0] Batch 1198, Loss 0.3264457881450653\n",
      "[Training Epoch 0] Batch 1199, Loss 0.36498019099235535\n",
      "[Training Epoch 0] Batch 1200, Loss 0.3421482443809509\n",
      "[Training Epoch 0] Batch 1201, Loss 0.3522893786430359\n",
      "[Training Epoch 0] Batch 1202, Loss 0.361136794090271\n",
      "[Training Epoch 0] Batch 1203, Loss 0.3654453158378601\n",
      "[Training Epoch 0] Batch 1204, Loss 0.36671680212020874\n",
      "[Training Epoch 0] Batch 1205, Loss 0.34354016184806824\n",
      "[Training Epoch 0] Batch 1206, Loss 0.3366037607192993\n",
      "[Training Epoch 0] Batch 1207, Loss 0.3440733253955841\n",
      "[Training Epoch 0] Batch 1208, Loss 0.36328762769699097\n",
      "[Training Epoch 0] Batch 1209, Loss 0.32302650809288025\n",
      "[Training Epoch 0] Batch 1210, Loss 0.3370405435562134\n",
      "[Training Epoch 0] Batch 1211, Loss 0.37316828966140747\n",
      "[Training Epoch 0] Batch 1212, Loss 0.34890979528427124\n",
      "[Training Epoch 0] Batch 1213, Loss 0.33324307203292847\n",
      "[Training Epoch 0] Batch 1214, Loss 0.34574589133262634\n",
      "[Training Epoch 0] Batch 1215, Loss 0.3147781789302826\n",
      "[Training Epoch 0] Batch 1216, Loss 0.3346055746078491\n",
      "[Training Epoch 0] Batch 1217, Loss 0.34304386377334595\n",
      "[Training Epoch 0] Batch 1218, Loss 0.37517285346984863\n",
      "[Training Epoch 0] Batch 1219, Loss 0.3477572798728943\n",
      "[Training Epoch 0] Batch 1220, Loss 0.38587114214897156\n",
      "[Training Epoch 0] Batch 1221, Loss 0.35485172271728516\n",
      "[Training Epoch 0] Batch 1222, Loss 0.38403916358947754\n",
      "[Training Epoch 0] Batch 1223, Loss 0.37903136014938354\n",
      "[Training Epoch 0] Batch 1224, Loss 0.3549981117248535\n",
      "[Training Epoch 0] Batch 1225, Loss 0.3287903070449829\n",
      "[Training Epoch 0] Batch 1226, Loss 0.3504798710346222\n",
      "[Training Epoch 0] Batch 1227, Loss 0.33566707372665405\n",
      "[Training Epoch 0] Batch 1228, Loss 0.36641639471054077\n",
      "[Training Epoch 0] Batch 1229, Loss 0.33621448278427124\n",
      "[Training Epoch 0] Batch 1230, Loss 0.3181836009025574\n",
      "[Training Epoch 0] Batch 1231, Loss 0.33216920495033264\n",
      "[Training Epoch 0] Batch 1232, Loss 0.33521658182144165\n",
      "[Training Epoch 0] Batch 1233, Loss 0.3592973053455353\n",
      "[Training Epoch 0] Batch 1234, Loss 0.37974101305007935\n",
      "[Training Epoch 0] Batch 1235, Loss 0.3671965003013611\n",
      "[Training Epoch 0] Batch 1236, Loss 0.36630353331565857\n",
      "[Training Epoch 0] Batch 1237, Loss 0.3542463183403015\n",
      "[Training Epoch 0] Batch 1238, Loss 0.35903406143188477\n",
      "[Training Epoch 0] Batch 1239, Loss 0.37170276045799255\n",
      "[Training Epoch 0] Batch 1240, Loss 0.38054513931274414\n",
      "[Training Epoch 0] Batch 1241, Loss 0.35025444626808167\n",
      "[Training Epoch 0] Batch 1242, Loss 0.37001240253448486\n",
      "[Training Epoch 0] Batch 1243, Loss 0.35503819584846497\n",
      "[Training Epoch 0] Batch 1244, Loss 0.34949737787246704\n",
      "[Training Epoch 0] Batch 1245, Loss 0.34780430793762207\n",
      "[Training Epoch 0] Batch 1246, Loss 0.3382233679294586\n",
      "[Training Epoch 0] Batch 1247, Loss 0.376539945602417\n",
      "[Training Epoch 0] Batch 1248, Loss 0.32778698205947876\n",
      "[Training Epoch 0] Batch 1249, Loss 0.3625454306602478\n",
      "[Training Epoch 0] Batch 1250, Loss 0.32789888978004456\n",
      "[Training Epoch 0] Batch 1251, Loss 0.3378358781337738\n",
      "[Training Epoch 0] Batch 1252, Loss 0.35538315773010254\n",
      "[Training Epoch 0] Batch 1253, Loss 0.36148470640182495\n",
      "[Training Epoch 0] Batch 1254, Loss 0.3415720462799072\n",
      "[Training Epoch 0] Batch 1255, Loss 0.32622870802879333\n",
      "[Training Epoch 0] Batch 1256, Loss 0.3440188765525818\n",
      "[Training Epoch 0] Batch 1257, Loss 0.34894096851348877\n",
      "[Training Epoch 0] Batch 1258, Loss 0.3628023862838745\n",
      "[Training Epoch 0] Batch 1259, Loss 0.3348753750324249\n",
      "[Training Epoch 0] Batch 1260, Loss 0.3265107274055481\n",
      "[Training Epoch 0] Batch 1261, Loss 0.39728862047195435\n",
      "[Training Epoch 0] Batch 1262, Loss 0.36135685443878174\n",
      "[Training Epoch 0] Batch 1263, Loss 0.33218955993652344\n",
      "[Training Epoch 0] Batch 1264, Loss 0.37297776341438293\n",
      "[Training Epoch 0] Batch 1265, Loss 0.36051833629608154\n",
      "[Training Epoch 0] Batch 1266, Loss 0.3390359580516815\n",
      "[Training Epoch 0] Batch 1267, Loss 0.34504973888397217\n",
      "[Training Epoch 0] Batch 1268, Loss 0.37188541889190674\n",
      "[Training Epoch 0] Batch 1269, Loss 0.37203580141067505\n",
      "[Training Epoch 0] Batch 1270, Loss 0.3395620286464691\n",
      "[Training Epoch 0] Batch 1271, Loss 0.3509278893470764\n",
      "[Training Epoch 0] Batch 1272, Loss 0.34930968284606934\n",
      "[Training Epoch 0] Batch 1273, Loss 0.3362889587879181\n",
      "[Training Epoch 0] Batch 1274, Loss 0.36861786246299744\n",
      "[Training Epoch 0] Batch 1275, Loss 0.3660334348678589\n",
      "[Training Epoch 0] Batch 1276, Loss 0.3302125930786133\n",
      "[Training Epoch 0] Batch 1277, Loss 0.35335981845855713\n",
      "[Training Epoch 0] Batch 1278, Loss 0.3437168300151825\n",
      "[Training Epoch 0] Batch 1279, Loss 0.3341693878173828\n",
      "[Training Epoch 0] Batch 1280, Loss 0.32106101512908936\n",
      "[Training Epoch 0] Batch 1281, Loss 0.3592264652252197\n",
      "[Training Epoch 0] Batch 1282, Loss 0.33466044068336487\n",
      "[Training Epoch 0] Batch 1283, Loss 0.3437020778656006\n",
      "[Training Epoch 0] Batch 1284, Loss 0.3540189266204834\n",
      "[Training Epoch 0] Batch 1285, Loss 0.3451005220413208\n",
      "[Training Epoch 0] Batch 1286, Loss 0.3651430606842041\n",
      "[Training Epoch 0] Batch 1287, Loss 0.3490769565105438\n",
      "[Training Epoch 0] Batch 1288, Loss 0.32267338037490845\n",
      "[Training Epoch 0] Batch 1289, Loss 0.32937943935394287\n",
      "[Training Epoch 0] Batch 1290, Loss 0.35216060280799866\n",
      "[Training Epoch 0] Batch 1291, Loss 0.3396059274673462\n",
      "[Training Epoch 0] Batch 1292, Loss 0.34874874353408813\n",
      "[Training Epoch 0] Batch 1293, Loss 0.3727858066558838\n",
      "[Training Epoch 0] Batch 1294, Loss 0.3288651704788208\n",
      "[Training Epoch 0] Batch 1295, Loss 0.36005687713623047\n",
      "[Training Epoch 0] Batch 1296, Loss 0.33791661262512207\n",
      "[Training Epoch 0] Batch 1297, Loss 0.34484827518463135\n",
      "[Training Epoch 0] Batch 1298, Loss 0.3482128381729126\n",
      "[Training Epoch 0] Batch 1299, Loss 0.32955610752105713\n",
      "[Training Epoch 0] Batch 1300, Loss 0.3464162349700928\n",
      "[Training Epoch 0] Batch 1301, Loss 0.3611333966255188\n",
      "[Training Epoch 0] Batch 1302, Loss 0.3738816976547241\n",
      "[Training Epoch 0] Batch 1303, Loss 0.35123732686042786\n",
      "[Training Epoch 0] Batch 1304, Loss 0.3749959468841553\n",
      "[Training Epoch 0] Batch 1305, Loss 0.35267379879951477\n",
      "[Training Epoch 0] Batch 1306, Loss 0.3434181809425354\n",
      "[Training Epoch 0] Batch 1307, Loss 0.3413694500923157\n",
      "[Training Epoch 0] Batch 1308, Loss 0.35298585891723633\n",
      "[Training Epoch 0] Batch 1309, Loss 0.3304649889469147\n",
      "[Training Epoch 0] Batch 1310, Loss 0.3549954295158386\n",
      "[Training Epoch 0] Batch 1311, Loss 0.34397774934768677\n",
      "[Training Epoch 0] Batch 1312, Loss 0.3383439779281616\n",
      "[Training Epoch 0] Batch 1313, Loss 0.35718968510627747\n",
      "[Training Epoch 0] Batch 1314, Loss 0.38115599751472473\n",
      "[Training Epoch 0] Batch 1315, Loss 0.3319770097732544\n",
      "[Training Epoch 0] Batch 1316, Loss 0.3285297453403473\n",
      "[Training Epoch 0] Batch 1317, Loss 0.33829399943351746\n",
      "[Training Epoch 0] Batch 1318, Loss 0.3531457781791687\n",
      "[Training Epoch 0] Batch 1319, Loss 0.37379515171051025\n",
      "[Training Epoch 0] Batch 1320, Loss 0.34193164110183716\n",
      "[Training Epoch 0] Batch 1321, Loss 0.32462456822395325\n",
      "[Training Epoch 0] Batch 1322, Loss 0.335480272769928\n",
      "[Training Epoch 0] Batch 1323, Loss 0.33629918098449707\n",
      "[Training Epoch 0] Batch 1324, Loss 0.37417641282081604\n",
      "[Training Epoch 0] Batch 1325, Loss 0.3897640109062195\n",
      "[Training Epoch 0] Batch 1326, Loss 0.3383345603942871\n",
      "[Training Epoch 0] Batch 1327, Loss 0.3622215986251831\n",
      "[Training Epoch 0] Batch 1328, Loss 0.32731181383132935\n",
      "[Training Epoch 0] Batch 1329, Loss 0.3502040505409241\n",
      "[Training Epoch 0] Batch 1330, Loss 0.36913251876831055\n",
      "[Training Epoch 0] Batch 1331, Loss 0.3739379048347473\n",
      "[Training Epoch 0] Batch 1332, Loss 0.3287942409515381\n",
      "[Training Epoch 0] Batch 1333, Loss 0.3398900628089905\n",
      "[Training Epoch 0] Batch 1334, Loss 0.3592671751976013\n",
      "[Training Epoch 0] Batch 1335, Loss 0.35393083095550537\n",
      "[Training Epoch 0] Batch 1336, Loss 0.3454367220401764\n",
      "[Training Epoch 0] Batch 1337, Loss 0.3203957676887512\n",
      "[Training Epoch 0] Batch 1338, Loss 0.3379727900028229\n",
      "[Training Epoch 0] Batch 1339, Loss 0.34349602460861206\n",
      "[Training Epoch 0] Batch 1340, Loss 0.36716046929359436\n",
      "[Training Epoch 0] Batch 1341, Loss 0.34378111362457275\n",
      "[Training Epoch 0] Batch 1342, Loss 0.3590807020664215\n",
      "[Training Epoch 0] Batch 1343, Loss 0.34989801049232483\n",
      "[Training Epoch 0] Batch 1344, Loss 0.3574829697608948\n",
      "[Training Epoch 0] Batch 1345, Loss 0.334896981716156\n",
      "[Training Epoch 0] Batch 1346, Loss 0.3322293162345886\n",
      "[Training Epoch 0] Batch 1347, Loss 0.35165050625801086\n",
      "[Training Epoch 0] Batch 1348, Loss 0.3314135670661926\n",
      "[Training Epoch 0] Batch 1349, Loss 0.3523971438407898\n",
      "[Training Epoch 0] Batch 1350, Loss 0.35317516326904297\n",
      "[Training Epoch 0] Batch 1351, Loss 0.3506184220314026\n",
      "[Training Epoch 0] Batch 1352, Loss 0.35863786935806274\n",
      "[Training Epoch 0] Batch 1353, Loss 0.3531022071838379\n",
      "[Training Epoch 0] Batch 1354, Loss 0.33933573961257935\n",
      "[Training Epoch 0] Batch 1355, Loss 0.339141309261322\n",
      "[Training Epoch 0] Batch 1356, Loss 0.3306978940963745\n",
      "[Training Epoch 0] Batch 1357, Loss 0.3655887842178345\n",
      "[Training Epoch 0] Batch 1358, Loss 0.33402371406555176\n",
      "[Training Epoch 0] Batch 1359, Loss 0.32956963777542114\n",
      "[Training Epoch 0] Batch 1360, Loss 0.3259276747703552\n",
      "[Training Epoch 0] Batch 1361, Loss 0.32923540472984314\n",
      "[Training Epoch 0] Batch 1362, Loss 0.3489248752593994\n",
      "[Training Epoch 0] Batch 1363, Loss 0.3694342374801636\n",
      "[Training Epoch 0] Batch 1364, Loss 0.3747674226760864\n",
      "[Training Epoch 0] Batch 1365, Loss 0.3381310701370239\n",
      "[Training Epoch 0] Batch 1366, Loss 0.3443201184272766\n",
      "[Training Epoch 0] Batch 1367, Loss 0.3564549386501312\n",
      "[Training Epoch 0] Batch 1368, Loss 0.37479788064956665\n",
      "[Training Epoch 0] Batch 1369, Loss 0.3501376509666443\n",
      "[Training Epoch 0] Batch 1370, Loss 0.35545438528060913\n",
      "[Training Epoch 0] Batch 1371, Loss 0.38130176067352295\n",
      "[Training Epoch 0] Batch 1372, Loss 0.3488115668296814\n",
      "[Training Epoch 0] Batch 1373, Loss 0.32642030715942383\n",
      "[Training Epoch 0] Batch 1374, Loss 0.3440650999546051\n",
      "[Training Epoch 0] Batch 1375, Loss 0.36361759901046753\n",
      "[Training Epoch 0] Batch 1376, Loss 0.34166139364242554\n",
      "[Training Epoch 0] Batch 1377, Loss 0.34304866194725037\n",
      "[Training Epoch 0] Batch 1378, Loss 0.34832823276519775\n",
      "[Training Epoch 0] Batch 1379, Loss 0.3460231125354767\n",
      "[Training Epoch 0] Batch 1380, Loss 0.33787110447883606\n",
      "[Training Epoch 0] Batch 1381, Loss 0.35957199335098267\n",
      "[Training Epoch 0] Batch 1382, Loss 0.3522564768791199\n",
      "[Training Epoch 0] Batch 1383, Loss 0.34181368350982666\n",
      "[Training Epoch 0] Batch 1384, Loss 0.34751853346824646\n",
      "[Training Epoch 0] Batch 1385, Loss 0.3428005576133728\n",
      "[Training Epoch 0] Batch 1386, Loss 0.3497285842895508\n",
      "[Training Epoch 0] Batch 1387, Loss 0.34472766518592834\n",
      "[Training Epoch 0] Batch 1388, Loss 0.35651957988739014\n",
      "[Training Epoch 0] Batch 1389, Loss 0.3326503038406372\n",
      "[Training Epoch 0] Batch 1390, Loss 0.35779666900634766\n",
      "[Training Epoch 0] Batch 1391, Loss 0.3318488597869873\n",
      "[Training Epoch 0] Batch 1392, Loss 0.3387001156806946\n",
      "[Training Epoch 0] Batch 1393, Loss 0.3397065997123718\n",
      "[Training Epoch 0] Batch 1394, Loss 0.32535624504089355\n",
      "[Training Epoch 0] Batch 1395, Loss 0.35602322220802307\n",
      "[Training Epoch 0] Batch 1396, Loss 0.35586899518966675\n",
      "[Training Epoch 0] Batch 1397, Loss 0.3731827437877655\n",
      "[Training Epoch 0] Batch 1398, Loss 0.3386918902397156\n",
      "[Training Epoch 0] Batch 1399, Loss 0.34435027837753296\n",
      "[Training Epoch 0] Batch 1400, Loss 0.3262372612953186\n",
      "[Training Epoch 0] Batch 1401, Loss 0.3601749837398529\n",
      "[Training Epoch 0] Batch 1402, Loss 0.33274802565574646\n",
      "[Training Epoch 0] Batch 1403, Loss 0.3586622476577759\n",
      "[Training Epoch 0] Batch 1404, Loss 0.35633939504623413\n",
      "[Training Epoch 0] Batch 1405, Loss 0.34000784158706665\n",
      "[Training Epoch 0] Batch 1406, Loss 0.33784276247024536\n",
      "[Training Epoch 0] Batch 1407, Loss 0.32991743087768555\n",
      "[Training Epoch 0] Batch 1408, Loss 0.34342697262763977\n",
      "[Training Epoch 0] Batch 1409, Loss 0.3314543068408966\n",
      "[Training Epoch 0] Batch 1410, Loss 0.390561044216156\n",
      "[Training Epoch 0] Batch 1411, Loss 0.3481668531894684\n",
      "[Training Epoch 0] Batch 1412, Loss 0.3241877555847168\n",
      "[Training Epoch 0] Batch 1413, Loss 0.35104748606681824\n",
      "[Training Epoch 0] Batch 1414, Loss 0.37906190752983093\n",
      "[Training Epoch 0] Batch 1415, Loss 0.3742433190345764\n",
      "[Training Epoch 0] Batch 1416, Loss 0.3291228711605072\n",
      "[Training Epoch 0] Batch 1417, Loss 0.37036532163619995\n",
      "[Training Epoch 0] Batch 1418, Loss 0.344801127910614\n",
      "[Training Epoch 0] Batch 1419, Loss 0.32777342200279236\n",
      "[Training Epoch 0] Batch 1420, Loss 0.3457393944263458\n",
      "[Training Epoch 0] Batch 1421, Loss 0.35263872146606445\n",
      "[Training Epoch 0] Batch 1422, Loss 0.31952711939811707\n",
      "[Training Epoch 0] Batch 1423, Loss 0.35177725553512573\n",
      "[Training Epoch 0] Batch 1424, Loss 0.3405577540397644\n",
      "[Training Epoch 0] Batch 1425, Loss 0.36561232805252075\n",
      "[Training Epoch 0] Batch 1426, Loss 0.3176979720592499\n",
      "[Training Epoch 0] Batch 1427, Loss 0.32633015513420105\n",
      "[Training Epoch 0] Batch 1428, Loss 0.35305851697921753\n",
      "[Training Epoch 0] Batch 1429, Loss 0.35981908440589905\n",
      "[Training Epoch 0] Batch 1430, Loss 0.3615206778049469\n",
      "[Training Epoch 0] Batch 1431, Loss 0.3715245723724365\n",
      "[Training Epoch 0] Batch 1432, Loss 0.35994142293930054\n",
      "[Training Epoch 0] Batch 1433, Loss 0.34431853890419006\n",
      "[Training Epoch 0] Batch 1434, Loss 0.3336758613586426\n",
      "[Training Epoch 0] Batch 1435, Loss 0.37158259749412537\n",
      "[Training Epoch 0] Batch 1436, Loss 0.3598885238170624\n",
      "[Training Epoch 0] Batch 1437, Loss 0.3537558317184448\n",
      "[Training Epoch 0] Batch 1438, Loss 0.32995495200157166\n",
      "[Training Epoch 0] Batch 1439, Loss 0.36796724796295166\n",
      "[Training Epoch 0] Batch 1440, Loss 0.34797945618629456\n",
      "[Training Epoch 0] Batch 1441, Loss 0.3482549488544464\n",
      "[Training Epoch 0] Batch 1442, Loss 0.3467501997947693\n",
      "[Training Epoch 0] Batch 1443, Loss 0.36328738927841187\n",
      "[Training Epoch 0] Batch 1444, Loss 0.3264108896255493\n",
      "[Training Epoch 0] Batch 1445, Loss 0.36602118611335754\n",
      "[Training Epoch 0] Batch 1446, Loss 0.3359623849391937\n",
      "[Training Epoch 0] Batch 1447, Loss 0.3709133267402649\n",
      "[Training Epoch 0] Batch 1448, Loss 0.33837008476257324\n",
      "[Training Epoch 0] Batch 1449, Loss 0.3488556444644928\n",
      "[Training Epoch 0] Batch 1450, Loss 0.36094531416893005\n",
      "[Training Epoch 0] Batch 1451, Loss 0.3046517074108124\n",
      "[Training Epoch 0] Batch 1452, Loss 0.3567301034927368\n",
      "[Training Epoch 0] Batch 1453, Loss 0.3280554413795471\n",
      "[Training Epoch 0] Batch 1454, Loss 0.3847045302391052\n",
      "[Training Epoch 0] Batch 1455, Loss 0.35405433177948\n",
      "[Training Epoch 0] Batch 1456, Loss 0.3474595546722412\n",
      "[Training Epoch 0] Batch 1457, Loss 0.3677668869495392\n",
      "[Training Epoch 0] Batch 1458, Loss 0.35302671790122986\n",
      "[Training Epoch 0] Batch 1459, Loss 0.3242412209510803\n",
      "[Training Epoch 0] Batch 1460, Loss 0.35993796586990356\n",
      "[Training Epoch 0] Batch 1461, Loss 0.3704938292503357\n",
      "[Training Epoch 0] Batch 1462, Loss 0.33222025632858276\n",
      "[Training Epoch 0] Batch 1463, Loss 0.35660499334335327\n",
      "[Training Epoch 0] Batch 1464, Loss 0.3560526967048645\n",
      "[Training Epoch 0] Batch 1465, Loss 0.3315350115299225\n",
      "[Training Epoch 0] Batch 1466, Loss 0.3521007299423218\n",
      "[Training Epoch 0] Batch 1467, Loss 0.34799444675445557\n",
      "[Training Epoch 0] Batch 1468, Loss 0.35627204179763794\n",
      "[Training Epoch 0] Batch 1469, Loss 0.34151673316955566\n",
      "[Training Epoch 0] Batch 1470, Loss 0.3174028992652893\n",
      "[Training Epoch 0] Batch 1471, Loss 0.32924604415893555\n",
      "[Training Epoch 0] Batch 1472, Loss 0.39171919226646423\n",
      "[Training Epoch 0] Batch 1473, Loss 0.3425177335739136\n",
      "[Training Epoch 0] Batch 1474, Loss 0.36130356788635254\n",
      "[Training Epoch 0] Batch 1475, Loss 0.37992775440216064\n",
      "[Training Epoch 0] Batch 1476, Loss 0.3706672489643097\n",
      "[Training Epoch 0] Batch 1477, Loss 0.3408040404319763\n",
      "[Training Epoch 0] Batch 1478, Loss 0.34281808137893677\n",
      "[Training Epoch 0] Batch 1479, Loss 0.35934799909591675\n",
      "[Training Epoch 0] Batch 1480, Loss 0.32684019207954407\n",
      "[Training Epoch 0] Batch 1481, Loss 0.30728286504745483\n",
      "[Training Epoch 0] Batch 1482, Loss 0.32773780822753906\n",
      "[Training Epoch 0] Batch 1483, Loss 0.33683067560195923\n",
      "[Training Epoch 0] Batch 1484, Loss 0.33658331632614136\n",
      "[Training Epoch 0] Batch 1485, Loss 0.31617581844329834\n",
      "[Training Epoch 0] Batch 1486, Loss 0.34331172704696655\n",
      "[Training Epoch 0] Batch 1487, Loss 0.3435207009315491\n",
      "[Training Epoch 0] Batch 1488, Loss 0.37038981914520264\n",
      "[Training Epoch 0] Batch 1489, Loss 0.31343376636505127\n",
      "[Training Epoch 0] Batch 1490, Loss 0.3310199975967407\n",
      "[Training Epoch 0] Batch 1491, Loss 0.33089911937713623\n",
      "[Training Epoch 0] Batch 1492, Loss 0.34998852014541626\n",
      "[Training Epoch 0] Batch 1493, Loss 0.3219340443611145\n",
      "[Training Epoch 0] Batch 1494, Loss 0.3261513113975525\n",
      "[Training Epoch 0] Batch 1495, Loss 0.3541709780693054\n",
      "[Training Epoch 0] Batch 1496, Loss 0.36866191029548645\n",
      "[Training Epoch 0] Batch 1497, Loss 0.3419243097305298\n",
      "[Training Epoch 0] Batch 1498, Loss 0.34758350253105164\n",
      "[Training Epoch 0] Batch 1499, Loss 0.3226226568222046\n",
      "[Training Epoch 0] Batch 1500, Loss 0.3509300649166107\n",
      "[Training Epoch 0] Batch 1501, Loss 0.3498997688293457\n",
      "[Training Epoch 0] Batch 1502, Loss 0.3746109902858734\n",
      "[Training Epoch 0] Batch 1503, Loss 0.36073338985443115\n",
      "[Training Epoch 0] Batch 1504, Loss 0.33867722749710083\n",
      "[Training Epoch 0] Batch 1505, Loss 0.36776819825172424\n",
      "[Training Epoch 0] Batch 1506, Loss 0.3444923162460327\n",
      "[Training Epoch 0] Batch 1507, Loss 0.3566737174987793\n",
      "[Training Epoch 0] Batch 1508, Loss 0.3705448508262634\n",
      "[Training Epoch 0] Batch 1509, Loss 0.3865344822406769\n",
      "[Training Epoch 0] Batch 1510, Loss 0.34615230560302734\n",
      "[Training Epoch 0] Batch 1511, Loss 0.366491436958313\n",
      "[Training Epoch 0] Batch 1512, Loss 0.34204670786857605\n",
      "[Training Epoch 0] Batch 1513, Loss 0.3260064423084259\n",
      "[Training Epoch 0] Batch 1514, Loss 0.3396419286727905\n",
      "[Training Epoch 0] Batch 1515, Loss 0.3537982106208801\n",
      "[Training Epoch 0] Batch 1516, Loss 0.34297290444374084\n",
      "[Training Epoch 0] Batch 1517, Loss 0.35124942660331726\n",
      "[Training Epoch 0] Batch 1518, Loss 0.36715954542160034\n",
      "[Training Epoch 0] Batch 1519, Loss 0.333876371383667\n",
      "[Training Epoch 0] Batch 1520, Loss 0.35488665103912354\n",
      "[Training Epoch 0] Batch 1521, Loss 0.36047470569610596\n",
      "[Training Epoch 0] Batch 1522, Loss 0.35617268085479736\n",
      "[Training Epoch 0] Batch 1523, Loss 0.3734847903251648\n",
      "[Training Epoch 0] Batch 1524, Loss 0.3552703261375427\n",
      "[Training Epoch 0] Batch 1525, Loss 0.3410317897796631\n",
      "[Training Epoch 0] Batch 1526, Loss 0.3620344400405884\n",
      "[Training Epoch 0] Batch 1527, Loss 0.35272958874702454\n",
      "[Training Epoch 0] Batch 1528, Loss 0.34341442584991455\n",
      "[Training Epoch 0] Batch 1529, Loss 0.3685096800327301\n",
      "[Training Epoch 0] Batch 1530, Loss 0.3366304636001587\n",
      "[Training Epoch 0] Batch 1531, Loss 0.37658530473709106\n",
      "[Training Epoch 0] Batch 1532, Loss 0.3489646911621094\n",
      "[Training Epoch 0] Batch 1533, Loss 0.3804464340209961\n",
      "[Training Epoch 0] Batch 1534, Loss 0.3443548083305359\n",
      "[Training Epoch 0] Batch 1535, Loss 0.339811772108078\n",
      "[Training Epoch 0] Batch 1536, Loss 0.36827343702316284\n",
      "[Training Epoch 0] Batch 1537, Loss 0.3375100791454315\n",
      "[Training Epoch 0] Batch 1538, Loss 0.31197768449783325\n",
      "[Training Epoch 0] Batch 1539, Loss 0.336351215839386\n",
      "[Training Epoch 0] Batch 1540, Loss 0.3483629822731018\n",
      "[Training Epoch 0] Batch 1541, Loss 0.34292757511138916\n",
      "[Training Epoch 0] Batch 1542, Loss 0.35535019636154175\n",
      "[Training Epoch 0] Batch 1543, Loss 0.32515931129455566\n",
      "[Training Epoch 0] Batch 1544, Loss 0.3419283330440521\n",
      "[Training Epoch 0] Batch 1545, Loss 0.3510395884513855\n",
      "[Training Epoch 0] Batch 1546, Loss 0.3529794216156006\n",
      "[Training Epoch 0] Batch 1547, Loss 0.3528742492198944\n",
      "[Training Epoch 0] Batch 1548, Loss 0.365042120218277\n",
      "[Training Epoch 0] Batch 1549, Loss 0.3686809837818146\n",
      "[Training Epoch 0] Batch 1550, Loss 0.39038270711898804\n",
      "[Training Epoch 0] Batch 1551, Loss 0.33208173513412476\n",
      "[Training Epoch 0] Batch 1552, Loss 0.3644527792930603\n",
      "[Training Epoch 0] Batch 1553, Loss 0.3476976156234741\n",
      "[Training Epoch 0] Batch 1554, Loss 0.3442095220088959\n",
      "[Training Epoch 0] Batch 1555, Loss 0.34725654125213623\n",
      "[Training Epoch 0] Batch 1556, Loss 0.36457931995391846\n",
      "[Training Epoch 0] Batch 1557, Loss 0.3090953230857849\n",
      "[Training Epoch 0] Batch 1558, Loss 0.3342042565345764\n",
      "[Training Epoch 0] Batch 1559, Loss 0.3189462423324585\n",
      "[Training Epoch 0] Batch 1560, Loss 0.3452228605747223\n",
      "[Training Epoch 0] Batch 1561, Loss 0.3231808543205261\n",
      "[Training Epoch 0] Batch 1562, Loss 0.3494333028793335\n",
      "[Training Epoch 0] Batch 1563, Loss 0.3468940258026123\n",
      "[Training Epoch 0] Batch 1564, Loss 0.3660251796245575\n",
      "[Training Epoch 0] Batch 1565, Loss 0.3558560013771057\n",
      "[Training Epoch 0] Batch 1566, Loss 0.3316762447357178\n",
      "[Training Epoch 0] Batch 1567, Loss 0.329216867685318\n",
      "[Training Epoch 0] Batch 1568, Loss 0.3663150370121002\n",
      "[Training Epoch 0] Batch 1569, Loss 0.3684999346733093\n",
      "[Training Epoch 0] Batch 1570, Loss 0.331140398979187\n",
      "[Training Epoch 0] Batch 1571, Loss 0.35557425022125244\n",
      "[Training Epoch 0] Batch 1572, Loss 0.3367876708507538\n",
      "[Training Epoch 0] Batch 1573, Loss 0.32277345657348633\n",
      "[Training Epoch 0] Batch 1574, Loss 0.3890247046947479\n",
      "[Training Epoch 0] Batch 1575, Loss 0.35845234990119934\n",
      "[Training Epoch 0] Batch 1576, Loss 0.33995798230171204\n",
      "[Training Epoch 0] Batch 1577, Loss 0.33273041248321533\n",
      "[Training Epoch 0] Batch 1578, Loss 0.3192442059516907\n",
      "[Training Epoch 0] Batch 1579, Loss 0.3283306658267975\n",
      "[Training Epoch 0] Batch 1580, Loss 0.35073786973953247\n",
      "[Training Epoch 0] Batch 1581, Loss 0.3562185764312744\n",
      "[Training Epoch 0] Batch 1582, Loss 0.3585231304168701\n",
      "[Training Epoch 0] Batch 1583, Loss 0.3300406336784363\n",
      "[Training Epoch 0] Batch 1584, Loss 0.36578524112701416\n",
      "[Training Epoch 0] Batch 1585, Loss 0.3139021396636963\n",
      "[Training Epoch 0] Batch 1586, Loss 0.34178900718688965\n",
      "[Training Epoch 0] Batch 1587, Loss 0.3491942882537842\n",
      "[Training Epoch 0] Batch 1588, Loss 0.36378511786460876\n",
      "[Training Epoch 0] Batch 1589, Loss 0.34002232551574707\n",
      "[Training Epoch 0] Batch 1590, Loss 0.36874517798423767\n",
      "[Training Epoch 0] Batch 1591, Loss 0.3307720720767975\n",
      "[Training Epoch 0] Batch 1592, Loss 0.37609928846359253\n",
      "[Training Epoch 0] Batch 1593, Loss 0.33966925740242004\n",
      "[Training Epoch 0] Batch 1594, Loss 0.36555016040802\n",
      "[Training Epoch 0] Batch 1595, Loss 0.34348487854003906\n",
      "[Training Epoch 0] Batch 1596, Loss 0.3388168215751648\n",
      "[Training Epoch 0] Batch 1597, Loss 0.38052570819854736\n",
      "[Training Epoch 0] Batch 1598, Loss 0.3453531265258789\n",
      "[Training Epoch 0] Batch 1599, Loss 0.34414345026016235\n",
      "[Training Epoch 0] Batch 1600, Loss 0.3522575795650482\n",
      "[Training Epoch 0] Batch 1601, Loss 0.361404150724411\n",
      "[Training Epoch 0] Batch 1602, Loss 0.35400766134262085\n",
      "[Training Epoch 0] Batch 1603, Loss 0.33333343267440796\n",
      "[Training Epoch 0] Batch 1604, Loss 0.36063382029533386\n",
      "[Training Epoch 0] Batch 1605, Loss 0.34780240058898926\n",
      "[Training Epoch 0] Batch 1606, Loss 0.33826690912246704\n",
      "[Training Epoch 0] Batch 1607, Loss 0.3375707268714905\n",
      "[Training Epoch 0] Batch 1608, Loss 0.3551521301269531\n",
      "[Training Epoch 0] Batch 1609, Loss 0.348804771900177\n",
      "[Training Epoch 0] Batch 1610, Loss 0.36824148893356323\n",
      "[Training Epoch 0] Batch 1611, Loss 0.3561476469039917\n",
      "[Training Epoch 0] Batch 1612, Loss 0.3945983052253723\n",
      "[Training Epoch 0] Batch 1613, Loss 0.3597407937049866\n",
      "[Training Epoch 0] Batch 1614, Loss 0.3752062916755676\n",
      "[Training Epoch 0] Batch 1615, Loss 0.3628464341163635\n",
      "[Training Epoch 0] Batch 1616, Loss 0.34245654940605164\n",
      "[Training Epoch 0] Batch 1617, Loss 0.3638863265514374\n",
      "[Training Epoch 0] Batch 1618, Loss 0.3739200532436371\n",
      "[Training Epoch 0] Batch 1619, Loss 0.3496066927909851\n",
      "[Training Epoch 0] Batch 1620, Loss 0.3517792224884033\n",
      "[Training Epoch 0] Batch 1621, Loss 0.3475256562232971\n",
      "[Training Epoch 0] Batch 1622, Loss 0.33973681926727295\n",
      "[Training Epoch 0] Batch 1623, Loss 0.3510271906852722\n",
      "[Training Epoch 0] Batch 1624, Loss 0.33773237466812134\n",
      "[Training Epoch 0] Batch 1625, Loss 0.34056758880615234\n",
      "[Training Epoch 0] Batch 1626, Loss 0.3516938090324402\n",
      "[Training Epoch 0] Batch 1627, Loss 0.32909151911735535\n",
      "[Training Epoch 0] Batch 1628, Loss 0.3423519730567932\n",
      "[Training Epoch 0] Batch 1629, Loss 0.3288227915763855\n",
      "[Training Epoch 0] Batch 1630, Loss 0.3385131061077118\n",
      "[Training Epoch 0] Batch 1631, Loss 0.3374403119087219\n",
      "[Training Epoch 0] Batch 1632, Loss 0.35186266899108887\n",
      "[Training Epoch 0] Batch 1633, Loss 0.3633027672767639\n",
      "[Training Epoch 0] Batch 1634, Loss 0.37938392162323\n",
      "[Training Epoch 0] Batch 1635, Loss 0.3437294661998749\n",
      "[Training Epoch 0] Batch 1636, Loss 0.3709925413131714\n",
      "[Training Epoch 0] Batch 1637, Loss 0.3402649760246277\n",
      "[Training Epoch 0] Batch 1638, Loss 0.34077686071395874\n",
      "[Training Epoch 0] Batch 1639, Loss 0.358330100774765\n",
      "[Training Epoch 0] Batch 1640, Loss 0.3387901186943054\n",
      "[Training Epoch 0] Batch 1641, Loss 0.3087533414363861\n",
      "[Training Epoch 0] Batch 1642, Loss 0.36220309138298035\n",
      "[Training Epoch 0] Batch 1643, Loss 0.3301679193973541\n",
      "[Training Epoch 0] Batch 1644, Loss 0.3392074704170227\n",
      "[Training Epoch 0] Batch 1645, Loss 0.3232152462005615\n",
      "[Training Epoch 0] Batch 1646, Loss 0.36171555519104004\n",
      "[Training Epoch 0] Batch 1647, Loss 0.33874768018722534\n",
      "[Training Epoch 0] Batch 1648, Loss 0.3422482907772064\n",
      "[Training Epoch 0] Batch 1649, Loss 0.3538159728050232\n",
      "[Training Epoch 0] Batch 1650, Loss 0.34449273347854614\n",
      "[Training Epoch 0] Batch 1651, Loss 0.3491082787513733\n",
      "[Training Epoch 0] Batch 1652, Loss 0.3177633285522461\n",
      "[Training Epoch 0] Batch 1653, Loss 0.31652435660362244\n",
      "[Training Epoch 0] Batch 1654, Loss 0.319458544254303\n",
      "[Training Epoch 0] Batch 1655, Loss 0.35471534729003906\n",
      "[Training Epoch 0] Batch 1656, Loss 0.3503623604774475\n",
      "[Training Epoch 0] Batch 1657, Loss 0.3556179702281952\n",
      "[Training Epoch 0] Batch 1658, Loss 0.31924450397491455\n",
      "[Training Epoch 0] Batch 1659, Loss 0.34655484557151794\n",
      "[Training Epoch 0] Batch 1660, Loss 0.35889700055122375\n",
      "[Training Epoch 0] Batch 1661, Loss 0.3351382613182068\n",
      "[Training Epoch 0] Batch 1662, Loss 0.3296925723552704\n",
      "[Training Epoch 0] Batch 1663, Loss 0.31068193912506104\n",
      "[Training Epoch 0] Batch 1664, Loss 0.3430251479148865\n",
      "[Training Epoch 0] Batch 1665, Loss 0.3513450026512146\n",
      "[Training Epoch 0] Batch 1666, Loss 0.35550951957702637\n",
      "[Training Epoch 0] Batch 1667, Loss 0.37073367834091187\n",
      "[Training Epoch 0] Batch 1668, Loss 0.35842031240463257\n",
      "[Training Epoch 0] Batch 1669, Loss 0.3576064705848694\n",
      "[Training Epoch 0] Batch 1670, Loss 0.30027657747268677\n",
      "[Training Epoch 0] Batch 1671, Loss 0.36593738198280334\n",
      "[Training Epoch 0] Batch 1672, Loss 0.3428422212600708\n",
      "[Training Epoch 0] Batch 1673, Loss 0.35728001594543457\n",
      "[Training Epoch 0] Batch 1674, Loss 0.37088412046432495\n",
      "[Training Epoch 0] Batch 1675, Loss 0.335286021232605\n",
      "[Training Epoch 0] Batch 1676, Loss 0.34511280059814453\n",
      "[Training Epoch 0] Batch 1677, Loss 0.3539833426475525\n",
      "[Training Epoch 0] Batch 1678, Loss 0.34959590435028076\n",
      "[Training Epoch 0] Batch 1679, Loss 0.36911672353744507\n",
      "[Training Epoch 0] Batch 1680, Loss 0.3407341241836548\n",
      "[Training Epoch 0] Batch 1681, Loss 0.3287547826766968\n",
      "[Training Epoch 0] Batch 1682, Loss 0.3422093391418457\n",
      "[Training Epoch 0] Batch 1683, Loss 0.34043166041374207\n",
      "[Training Epoch 0] Batch 1684, Loss 0.37693697214126587\n",
      "[Training Epoch 0] Batch 1685, Loss 0.3437137007713318\n",
      "[Training Epoch 0] Batch 1686, Loss 0.3475914001464844\n",
      "[Training Epoch 0] Batch 1687, Loss 0.34800106287002563\n",
      "[Training Epoch 0] Batch 1688, Loss 0.35392338037490845\n",
      "[Training Epoch 0] Batch 1689, Loss 0.3392826318740845\n",
      "[Training Epoch 0] Batch 1690, Loss 0.3327021896839142\n",
      "[Training Epoch 0] Batch 1691, Loss 0.34721511602401733\n",
      "[Training Epoch 0] Batch 1692, Loss 0.34167638421058655\n",
      "[Training Epoch 0] Batch 1693, Loss 0.36784273386001587\n",
      "[Training Epoch 0] Batch 1694, Loss 0.3521789014339447\n",
      "[Training Epoch 0] Batch 1695, Loss 0.34580329060554504\n",
      "[Training Epoch 0] Batch 1696, Loss 0.3379078507423401\n",
      "[Training Epoch 0] Batch 1697, Loss 0.3797832727432251\n",
      "[Training Epoch 0] Batch 1698, Loss 0.31814008951187134\n",
      "[Training Epoch 0] Batch 1699, Loss 0.3592795729637146\n",
      "[Training Epoch 0] Batch 1700, Loss 0.38926035165786743\n",
      "[Training Epoch 0] Batch 1701, Loss 0.33573776483535767\n",
      "[Training Epoch 0] Batch 1702, Loss 0.33954334259033203\n",
      "[Training Epoch 0] Batch 1703, Loss 0.3622583746910095\n",
      "[Training Epoch 0] Batch 1704, Loss 0.34709811210632324\n",
      "[Training Epoch 0] Batch 1705, Loss 0.3447868227958679\n",
      "[Training Epoch 0] Batch 1706, Loss 0.32740122079849243\n",
      "[Training Epoch 0] Batch 1707, Loss 0.3500902056694031\n",
      "[Training Epoch 0] Batch 1708, Loss 0.3706982731819153\n",
      "[Training Epoch 0] Batch 1709, Loss 0.34830528497695923\n",
      "[Training Epoch 0] Batch 1710, Loss 0.32597649097442627\n",
      "[Training Epoch 0] Batch 1711, Loss 0.37385112047195435\n",
      "[Training Epoch 0] Batch 1712, Loss 0.3340788185596466\n",
      "[Training Epoch 0] Batch 1713, Loss 0.3306581974029541\n",
      "[Training Epoch 0] Batch 1714, Loss 0.3781907260417938\n",
      "[Training Epoch 0] Batch 1715, Loss 0.3340473473072052\n",
      "[Training Epoch 0] Batch 1716, Loss 0.341765820980072\n",
      "[Training Epoch 0] Batch 1717, Loss 0.3517681658267975\n",
      "[Training Epoch 0] Batch 1718, Loss 0.33045893907546997\n",
      "[Training Epoch 0] Batch 1719, Loss 0.3407072424888611\n",
      "[Training Epoch 0] Batch 1720, Loss 0.3360231816768646\n",
      "[Training Epoch 0] Batch 1721, Loss 0.35667556524276733\n",
      "[Training Epoch 0] Batch 1722, Loss 0.3644932806491852\n",
      "[Training Epoch 0] Batch 1723, Loss 0.36460596323013306\n",
      "[Training Epoch 0] Batch 1724, Loss 0.3461782932281494\n",
      "[Training Epoch 0] Batch 1725, Loss 0.3536089062690735\n",
      "[Training Epoch 0] Batch 1726, Loss 0.33995065093040466\n",
      "[Training Epoch 0] Batch 1727, Loss 0.37290486693382263\n",
      "[Training Epoch 0] Batch 1728, Loss 0.34775304794311523\n",
      "[Training Epoch 0] Batch 1729, Loss 0.3544517159461975\n",
      "[Training Epoch 0] Batch 1730, Loss 0.3270525634288788\n",
      "[Training Epoch 0] Batch 1731, Loss 0.3399811387062073\n",
      "[Training Epoch 0] Batch 1732, Loss 0.35794198513031006\n",
      "[Training Epoch 0] Batch 1733, Loss 0.3096342086791992\n",
      "[Training Epoch 0] Batch 1734, Loss 0.3275253176689148\n",
      "[Training Epoch 0] Batch 1735, Loss 0.35143813490867615\n",
      "[Training Epoch 0] Batch 1736, Loss 0.34727394580841064\n",
      "[Training Epoch 0] Batch 1737, Loss 0.34616512060165405\n",
      "[Training Epoch 0] Batch 1738, Loss 0.364452987909317\n",
      "[Training Epoch 0] Batch 1739, Loss 0.3261764943599701\n",
      "[Training Epoch 0] Batch 1740, Loss 0.3466559648513794\n",
      "[Training Epoch 0] Batch 1741, Loss 0.30675697326660156\n",
      "[Training Epoch 0] Batch 1742, Loss 0.3600756525993347\n",
      "[Training Epoch 0] Batch 1743, Loss 0.38969290256500244\n",
      "[Training Epoch 0] Batch 1744, Loss 0.3473926782608032\n",
      "[Training Epoch 0] Batch 1745, Loss 0.34231942892074585\n",
      "[Training Epoch 0] Batch 1746, Loss 0.3393216133117676\n",
      "[Training Epoch 0] Batch 1747, Loss 0.32769566774368286\n",
      "[Training Epoch 0] Batch 1748, Loss 0.3453584313392639\n",
      "[Training Epoch 0] Batch 1749, Loss 0.3464670479297638\n",
      "[Training Epoch 0] Batch 1750, Loss 0.3415842652320862\n",
      "[Training Epoch 0] Batch 1751, Loss 0.3448224365711212\n",
      "[Training Epoch 0] Batch 1752, Loss 0.3701094686985016\n",
      "[Training Epoch 0] Batch 1753, Loss 0.3490764796733856\n",
      "[Training Epoch 0] Batch 1754, Loss 0.31711864471435547\n",
      "[Training Epoch 0] Batch 1755, Loss 0.3528743386268616\n",
      "[Training Epoch 0] Batch 1756, Loss 0.33849871158599854\n",
      "[Training Epoch 0] Batch 1757, Loss 0.3101385533809662\n",
      "[Training Epoch 0] Batch 1758, Loss 0.3632669448852539\n",
      "[Training Epoch 0] Batch 1759, Loss 0.3434686064720154\n",
      "[Training Epoch 0] Batch 1760, Loss 0.348952978849411\n",
      "[Training Epoch 0] Batch 1761, Loss 0.33542150259017944\n",
      "[Training Epoch 0] Batch 1762, Loss 0.35755711793899536\n",
      "[Training Epoch 0] Batch 1763, Loss 0.3100006580352783\n",
      "[Training Epoch 0] Batch 1764, Loss 0.3318105638027191\n",
      "[Training Epoch 0] Batch 1765, Loss 0.3353769779205322\n",
      "[Training Epoch 0] Batch 1766, Loss 0.3476812541484833\n",
      "[Training Epoch 0] Batch 1767, Loss 0.3122119903564453\n",
      "[Training Epoch 0] Batch 1768, Loss 0.3710969090461731\n",
      "[Training Epoch 0] Batch 1769, Loss 0.3390795588493347\n",
      "[Training Epoch 0] Batch 1770, Loss 0.35352087020874023\n",
      "[Training Epoch 0] Batch 1771, Loss 0.32755276560783386\n",
      "[Training Epoch 0] Batch 1772, Loss 0.35352039337158203\n",
      "[Training Epoch 0] Batch 1773, Loss 0.3189619183540344\n",
      "[Training Epoch 0] Batch 1774, Loss 0.29391685128211975\n",
      "[Training Epoch 0] Batch 1775, Loss 0.35195350646972656\n",
      "[Training Epoch 0] Batch 1776, Loss 0.3597850203514099\n",
      "[Training Epoch 0] Batch 1777, Loss 0.34831756353378296\n",
      "[Training Epoch 0] Batch 1778, Loss 0.34421950578689575\n",
      "[Training Epoch 0] Batch 1779, Loss 0.38430315256118774\n",
      "[Training Epoch 0] Batch 1780, Loss 0.344563752412796\n",
      "[Training Epoch 0] Batch 1781, Loss 0.3318265676498413\n",
      "[Training Epoch 0] Batch 1782, Loss 0.3308113217353821\n",
      "[Training Epoch 0] Batch 1783, Loss 0.33609139919281006\n",
      "[Training Epoch 0] Batch 1784, Loss 0.36559998989105225\n",
      "[Training Epoch 0] Batch 1785, Loss 0.32331421971321106\n",
      "[Training Epoch 0] Batch 1786, Loss 0.355885773897171\n",
      "[Training Epoch 0] Batch 1787, Loss 0.3397219181060791\n",
      "[Training Epoch 0] Batch 1788, Loss 0.34349268674850464\n",
      "[Training Epoch 0] Batch 1789, Loss 0.35487741231918335\n",
      "[Training Epoch 0] Batch 1790, Loss 0.33787330985069275\n",
      "[Training Epoch 0] Batch 1791, Loss 0.34002241492271423\n",
      "[Training Epoch 0] Batch 1792, Loss 0.35918939113616943\n",
      "[Training Epoch 0] Batch 1793, Loss 0.3623776435852051\n",
      "[Training Epoch 0] Batch 1794, Loss 0.36570867896080017\n",
      "[Training Epoch 0] Batch 1795, Loss 0.33130621910095215\n",
      "[Training Epoch 0] Batch 1796, Loss 0.3567683696746826\n",
      "[Training Epoch 0] Batch 1797, Loss 0.3277100920677185\n",
      "[Training Epoch 0] Batch 1798, Loss 0.31263887882232666\n",
      "[Training Epoch 0] Batch 1799, Loss 0.3576185703277588\n",
      "[Training Epoch 0] Batch 1800, Loss 0.35890132188796997\n",
      "[Training Epoch 0] Batch 1801, Loss 0.3151704668998718\n",
      "[Training Epoch 0] Batch 1802, Loss 0.30933189392089844\n",
      "[Training Epoch 0] Batch 1803, Loss 0.306304931640625\n",
      "[Training Epoch 0] Batch 1804, Loss 0.35339272022247314\n",
      "[Training Epoch 0] Batch 1805, Loss 0.32682591676712036\n",
      "[Training Epoch 0] Batch 1806, Loss 0.34125787019729614\n",
      "[Training Epoch 0] Batch 1807, Loss 0.3383938670158386\n",
      "[Training Epoch 0] Batch 1808, Loss 0.3211810886859894\n",
      "[Training Epoch 0] Batch 1809, Loss 0.3055236339569092\n",
      "[Training Epoch 0] Batch 1810, Loss 0.3383820354938507\n",
      "[Training Epoch 0] Batch 1811, Loss 0.3234579265117645\n",
      "[Training Epoch 0] Batch 1812, Loss 0.3597143888473511\n",
      "[Training Epoch 0] Batch 1813, Loss 0.33831527829170227\n",
      "[Training Epoch 0] Batch 1814, Loss 0.32780057191848755\n",
      "[Training Epoch 0] Batch 1815, Loss 0.3672347366809845\n",
      "[Training Epoch 0] Batch 1816, Loss 0.3284745514392853\n",
      "[Training Epoch 0] Batch 1817, Loss 0.3376769423484802\n",
      "[Training Epoch 0] Batch 1818, Loss 0.31997132301330566\n",
      "[Training Epoch 0] Batch 1819, Loss 0.35784876346588135\n",
      "[Training Epoch 0] Batch 1820, Loss 0.3290848135948181\n",
      "[Training Epoch 0] Batch 1821, Loss 0.32124418020248413\n",
      "[Training Epoch 0] Batch 1822, Loss 0.3446664810180664\n",
      "[Training Epoch 0] Batch 1823, Loss 0.32753145694732666\n",
      "[Training Epoch 0] Batch 1824, Loss 0.35919609665870667\n",
      "[Training Epoch 0] Batch 1825, Loss 0.3288196921348572\n",
      "[Training Epoch 0] Batch 1826, Loss 0.35540223121643066\n",
      "[Training Epoch 0] Batch 1827, Loss 0.34573692083358765\n",
      "[Training Epoch 0] Batch 1828, Loss 0.32263797521591187\n",
      "[Training Epoch 0] Batch 1829, Loss 0.34300267696380615\n",
      "[Training Epoch 0] Batch 1830, Loss 0.35602065920829773\n",
      "[Training Epoch 0] Batch 1831, Loss 0.33028531074523926\n",
      "[Training Epoch 0] Batch 1832, Loss 0.3461775779724121\n",
      "[Training Epoch 0] Batch 1833, Loss 0.3431323170661926\n",
      "[Training Epoch 0] Batch 1834, Loss 0.3528006672859192\n",
      "[Training Epoch 0] Batch 1835, Loss 0.3482823371887207\n",
      "[Training Epoch 0] Batch 1836, Loss 0.35283032059669495\n",
      "[Training Epoch 0] Batch 1837, Loss 0.3629928231239319\n",
      "[Training Epoch 0] Batch 1838, Loss 0.3255000412464142\n",
      "[Training Epoch 0] Batch 1839, Loss 0.3483022451400757\n",
      "[Training Epoch 0] Batch 1840, Loss 0.35946691036224365\n",
      "[Training Epoch 0] Batch 1841, Loss 0.34253644943237305\n",
      "[Training Epoch 0] Batch 1842, Loss 0.38185083866119385\n",
      "[Training Epoch 0] Batch 1843, Loss 0.342609703540802\n",
      "[Training Epoch 0] Batch 1844, Loss 0.34226882457733154\n",
      "[Training Epoch 0] Batch 1845, Loss 0.35958385467529297\n",
      "[Training Epoch 0] Batch 1846, Loss 0.36946535110473633\n",
      "[Training Epoch 0] Batch 1847, Loss 0.35718321800231934\n",
      "[Training Epoch 0] Batch 1848, Loss 0.33898037672042847\n",
      "[Training Epoch 0] Batch 1849, Loss 0.32581019401550293\n",
      "[Training Epoch 0] Batch 1850, Loss 0.33656376600265503\n",
      "[Training Epoch 0] Batch 1851, Loss 0.3351411819458008\n",
      "[Training Epoch 0] Batch 1852, Loss 0.3594458997249603\n",
      "[Training Epoch 0] Batch 1853, Loss 0.376705527305603\n",
      "[Training Epoch 0] Batch 1854, Loss 0.3321605324745178\n",
      "[Training Epoch 0] Batch 1855, Loss 0.3411661684513092\n",
      "[Training Epoch 0] Batch 1856, Loss 0.32899776101112366\n",
      "[Training Epoch 0] Batch 1857, Loss 0.3218044638633728\n",
      "[Training Epoch 0] Batch 1858, Loss 0.32514211535453796\n",
      "[Training Epoch 0] Batch 1859, Loss 0.34851646423339844\n",
      "[Training Epoch 0] Batch 1860, Loss 0.32280677556991577\n",
      "[Training Epoch 0] Batch 1861, Loss 0.3421551585197449\n",
      "[Training Epoch 0] Batch 1862, Loss 0.34608912467956543\n",
      "[Training Epoch 0] Batch 1863, Loss 0.3096890151500702\n",
      "[Training Epoch 0] Batch 1864, Loss 0.34642159938812256\n",
      "[Training Epoch 0] Batch 1865, Loss 0.35976332426071167\n",
      "[Training Epoch 0] Batch 1866, Loss 0.35390353202819824\n",
      "[Training Epoch 0] Batch 1867, Loss 0.3525107502937317\n",
      "[Training Epoch 0] Batch 1868, Loss 0.3342827558517456\n",
      "[Training Epoch 0] Batch 1869, Loss 0.34141039848327637\n",
      "[Training Epoch 0] Batch 1870, Loss 0.3368123769760132\n",
      "[Training Epoch 0] Batch 1871, Loss 0.35139185190200806\n",
      "[Training Epoch 0] Batch 1872, Loss 0.34344643354415894\n",
      "[Training Epoch 0] Batch 1873, Loss 0.3342905342578888\n",
      "[Training Epoch 0] Batch 1874, Loss 0.35907411575317383\n",
      "[Training Epoch 0] Batch 1875, Loss 0.342164009809494\n",
      "[Training Epoch 0] Batch 1876, Loss 0.34340226650238037\n",
      "[Training Epoch 0] Batch 1877, Loss 0.36073794960975647\n",
      "[Training Epoch 0] Batch 1878, Loss 0.3551351726055145\n",
      "[Training Epoch 0] Batch 1879, Loss 0.3398866355419159\n",
      "[Training Epoch 0] Batch 1880, Loss 0.3333932161331177\n",
      "[Training Epoch 0] Batch 1881, Loss 0.372734010219574\n",
      "[Training Epoch 0] Batch 1882, Loss 0.33440375328063965\n",
      "[Training Epoch 0] Batch 1883, Loss 0.34929823875427246\n",
      "[Training Epoch 0] Batch 1884, Loss 0.3570088744163513\n",
      "[Training Epoch 0] Batch 1885, Loss 0.3331088423728943\n",
      "[Training Epoch 0] Batch 1886, Loss 0.34634149074554443\n",
      "[Training Epoch 0] Batch 1887, Loss 0.33218005299568176\n",
      "[Training Epoch 0] Batch 1888, Loss 0.34892526268959045\n",
      "[Training Epoch 0] Batch 1889, Loss 0.3489525020122528\n",
      "[Training Epoch 0] Batch 1890, Loss 0.3310415744781494\n",
      "[Training Epoch 0] Batch 1891, Loss 0.3236740231513977\n",
      "[Training Epoch 0] Batch 1892, Loss 0.3480457067489624\n",
      "[Training Epoch 0] Batch 1893, Loss 0.3368905186653137\n",
      "[Training Epoch 0] Batch 1894, Loss 0.3402876853942871\n",
      "[Training Epoch 0] Batch 1895, Loss 0.3305441737174988\n",
      "[Training Epoch 0] Batch 1896, Loss 0.3308982849121094\n",
      "[Training Epoch 0] Batch 1897, Loss 0.3348250985145569\n",
      "[Training Epoch 0] Batch 1898, Loss 0.3410848379135132\n",
      "[Training Epoch 0] Batch 1899, Loss 0.34376251697540283\n",
      "[Training Epoch 0] Batch 1900, Loss 0.3429386019706726\n",
      "[Training Epoch 0] Batch 1901, Loss 0.3757445216178894\n",
      "[Training Epoch 0] Batch 1902, Loss 0.35387057065963745\n",
      "[Training Epoch 0] Batch 1903, Loss 0.3548644781112671\n",
      "[Training Epoch 0] Batch 1904, Loss 0.3307799696922302\n",
      "[Training Epoch 0] Batch 1905, Loss 0.3521800637245178\n",
      "[Training Epoch 0] Batch 1906, Loss 0.3160422742366791\n",
      "[Training Epoch 0] Batch 1907, Loss 0.3542560636997223\n",
      "[Training Epoch 0] Batch 1908, Loss 0.33164650201797485\n",
      "[Training Epoch 0] Batch 1909, Loss 0.3425191640853882\n",
      "[Training Epoch 0] Batch 1910, Loss 0.33933025598526\n",
      "[Training Epoch 0] Batch 1911, Loss 0.3188408613204956\n",
      "[Training Epoch 0] Batch 1912, Loss 0.32583361864089966\n",
      "[Training Epoch 0] Batch 1913, Loss 0.3593941926956177\n",
      "[Training Epoch 0] Batch 1914, Loss 0.340613454580307\n",
      "[Training Epoch 0] Batch 1915, Loss 0.3352275490760803\n",
      "[Training Epoch 0] Batch 1916, Loss 0.34543558955192566\n",
      "[Training Epoch 0] Batch 1917, Loss 0.3366560637950897\n",
      "[Training Epoch 0] Batch 1918, Loss 0.3188149333000183\n",
      "[Training Epoch 0] Batch 1919, Loss 0.35045331716537476\n",
      "[Training Epoch 0] Batch 1920, Loss 0.3257225751876831\n",
      "[Training Epoch 0] Batch 1921, Loss 0.31889522075653076\n",
      "[Training Epoch 0] Batch 1922, Loss 0.3238043487071991\n",
      "[Training Epoch 0] Batch 1923, Loss 0.30869060754776\n",
      "[Training Epoch 0] Batch 1924, Loss 0.3236810863018036\n",
      "[Training Epoch 0] Batch 1925, Loss 0.34564411640167236\n",
      "[Training Epoch 0] Batch 1926, Loss 0.32469385862350464\n",
      "[Training Epoch 0] Batch 1927, Loss 0.3515058755874634\n",
      "[Training Epoch 0] Batch 1928, Loss 0.3327952027320862\n",
      "[Training Epoch 0] Batch 1929, Loss 0.3390432596206665\n",
      "[Training Epoch 0] Batch 1930, Loss 0.35109609365463257\n",
      "[Training Epoch 0] Batch 1931, Loss 0.33651793003082275\n",
      "[Training Epoch 0] Batch 1932, Loss 0.3427978754043579\n",
      "[Training Epoch 0] Batch 1933, Loss 0.327470600605011\n",
      "[Training Epoch 0] Batch 1934, Loss 0.34553849697113037\n",
      "[Training Epoch 0] Batch 1935, Loss 0.31532377004623413\n",
      "[Training Epoch 0] Batch 1936, Loss 0.31511273980140686\n",
      "[Training Epoch 0] Batch 1937, Loss 0.3590705990791321\n",
      "[Training Epoch 0] Batch 1938, Loss 0.3186332881450653\n",
      "[Training Epoch 0] Batch 1939, Loss 0.3395628333091736\n",
      "[Training Epoch 0] Batch 1940, Loss 0.35226285457611084\n",
      "[Training Epoch 0] Batch 1941, Loss 0.37862429022789\n",
      "[Training Epoch 0] Batch 1942, Loss 0.32720720767974854\n",
      "[Training Epoch 0] Batch 1943, Loss 0.33948850631713867\n",
      "[Training Epoch 0] Batch 1944, Loss 0.33230119943618774\n",
      "[Training Epoch 0] Batch 1945, Loss 0.3316807746887207\n",
      "[Training Epoch 0] Batch 1946, Loss 0.3359149992465973\n",
      "[Training Epoch 0] Batch 1947, Loss 0.334642231464386\n",
      "[Training Epoch 0] Batch 1948, Loss 0.35729366540908813\n",
      "[Training Epoch 0] Batch 1949, Loss 0.3501899838447571\n",
      "[Training Epoch 0] Batch 1950, Loss 0.3430790901184082\n",
      "[Training Epoch 0] Batch 1951, Loss 0.3207963705062866\n",
      "[Training Epoch 0] Batch 1952, Loss 0.3373068869113922\n",
      "[Training Epoch 0] Batch 1953, Loss 0.3515310287475586\n",
      "[Training Epoch 0] Batch 1954, Loss 0.3181174397468567\n",
      "[Training Epoch 0] Batch 1955, Loss 0.3322964906692505\n",
      "[Training Epoch 0] Batch 1956, Loss 0.3598518967628479\n",
      "[Training Epoch 0] Batch 1957, Loss 0.32836660742759705\n",
      "[Training Epoch 0] Batch 1958, Loss 0.3416767120361328\n",
      "[Training Epoch 0] Batch 1959, Loss 0.3630244731903076\n",
      "[Training Epoch 0] Batch 1960, Loss 0.32660722732543945\n",
      "[Training Epoch 0] Batch 1961, Loss 0.3089349567890167\n",
      "[Training Epoch 0] Batch 1962, Loss 0.3673076927661896\n",
      "[Training Epoch 0] Batch 1963, Loss 0.34940290451049805\n",
      "[Training Epoch 0] Batch 1964, Loss 0.34704944491386414\n",
      "[Training Epoch 0] Batch 1965, Loss 0.3183610439300537\n",
      "[Training Epoch 0] Batch 1966, Loss 0.349475622177124\n",
      "[Training Epoch 0] Batch 1967, Loss 0.3230084478855133\n",
      "[Training Epoch 0] Batch 1968, Loss 0.3470792770385742\n",
      "[Training Epoch 0] Batch 1969, Loss 0.3256797790527344\n",
      "[Training Epoch 0] Batch 1970, Loss 0.33090195059776306\n",
      "[Training Epoch 0] Batch 1971, Loss 0.33829930424690247\n",
      "[Training Epoch 0] Batch 1972, Loss 0.34990817308425903\n",
      "[Training Epoch 0] Batch 1973, Loss 0.3332780599594116\n",
      "[Training Epoch 0] Batch 1974, Loss 0.34612423181533813\n",
      "[Training Epoch 0] Batch 1975, Loss 0.34130924940109253\n",
      "[Training Epoch 0] Batch 1976, Loss 0.3216220438480377\n",
      "[Training Epoch 0] Batch 1977, Loss 0.3045409619808197\n",
      "[Training Epoch 0] Batch 1978, Loss 0.346937894821167\n",
      "[Training Epoch 0] Batch 1979, Loss 0.31074923276901245\n",
      "[Training Epoch 0] Batch 1980, Loss 0.3193899989128113\n",
      "[Training Epoch 0] Batch 1981, Loss 0.34746599197387695\n",
      "[Training Epoch 0] Batch 1982, Loss 0.34591931104660034\n",
      "[Training Epoch 0] Batch 1983, Loss 0.314454585313797\n",
      "[Training Epoch 0] Batch 1984, Loss 0.35082679986953735\n",
      "[Training Epoch 0] Batch 1985, Loss 0.30574923753738403\n",
      "[Training Epoch 0] Batch 1986, Loss 0.3649517893791199\n",
      "[Training Epoch 0] Batch 1987, Loss 0.34366732835769653\n",
      "[Training Epoch 0] Batch 1988, Loss 0.35182708501815796\n",
      "[Training Epoch 0] Batch 1989, Loss 0.33920717239379883\n",
      "[Training Epoch 0] Batch 1990, Loss 0.34420719742774963\n",
      "[Training Epoch 0] Batch 1991, Loss 0.3637053966522217\n",
      "[Training Epoch 0] Batch 1992, Loss 0.36649975180625916\n",
      "[Training Epoch 0] Batch 1993, Loss 0.3287719488143921\n",
      "[Training Epoch 0] Batch 1994, Loss 0.29511144757270813\n",
      "[Training Epoch 0] Batch 1995, Loss 0.35395532846450806\n",
      "[Training Epoch 0] Batch 1996, Loss 0.3438485264778137\n",
      "[Training Epoch 0] Batch 1997, Loss 0.3399014174938202\n",
      "[Training Epoch 0] Batch 1998, Loss 0.358482301235199\n",
      "[Training Epoch 0] Batch 1999, Loss 0.3262527883052826\n",
      "[Training Epoch 0] Batch 2000, Loss 0.3335757553577423\n",
      "[Training Epoch 0] Batch 2001, Loss 0.3480697274208069\n",
      "[Training Epoch 0] Batch 2002, Loss 0.3560827970504761\n",
      "[Training Epoch 0] Batch 2003, Loss 0.328106552362442\n",
      "[Training Epoch 0] Batch 2004, Loss 0.362116277217865\n",
      "[Training Epoch 0] Batch 2005, Loss 0.3592945337295532\n",
      "[Training Epoch 0] Batch 2006, Loss 0.3615058660507202\n",
      "[Training Epoch 0] Batch 2007, Loss 0.33430954813957214\n",
      "[Training Epoch 0] Batch 2008, Loss 0.3359568119049072\n",
      "[Training Epoch 0] Batch 2009, Loss 0.3374117612838745\n",
      "[Training Epoch 0] Batch 2010, Loss 0.35069596767425537\n",
      "[Training Epoch 0] Batch 2011, Loss 0.3540777564048767\n",
      "[Training Epoch 0] Batch 2012, Loss 0.34706562757492065\n",
      "[Training Epoch 0] Batch 2013, Loss 0.3201831877231598\n",
      "[Training Epoch 0] Batch 2014, Loss 0.330966591835022\n",
      "[Training Epoch 0] Batch 2015, Loss 0.31438374519348145\n",
      "[Training Epoch 0] Batch 2016, Loss 0.3200112581253052\n",
      "[Training Epoch 0] Batch 2017, Loss 0.3337225914001465\n",
      "[Training Epoch 0] Batch 2018, Loss 0.3458895683288574\n",
      "[Training Epoch 0] Batch 2019, Loss 0.34409859776496887\n",
      "[Training Epoch 0] Batch 2020, Loss 0.3423813283443451\n",
      "[Training Epoch 0] Batch 2021, Loss 0.3318224251270294\n",
      "[Training Epoch 0] Batch 2022, Loss 0.3246425986289978\n",
      "[Training Epoch 0] Batch 2023, Loss 0.32670003175735474\n",
      "[Training Epoch 0] Batch 2024, Loss 0.3203742802143097\n",
      "[Training Epoch 0] Batch 2025, Loss 0.3211158812046051\n",
      "[Training Epoch 0] Batch 2026, Loss 0.33542588353157043\n",
      "[Training Epoch 0] Batch 2027, Loss 0.31604787707328796\n",
      "[Training Epoch 0] Batch 2028, Loss 0.3306175470352173\n",
      "[Training Epoch 0] Batch 2029, Loss 0.28401637077331543\n",
      "[Training Epoch 0] Batch 2030, Loss 0.3355645537376404\n",
      "[Training Epoch 0] Batch 2031, Loss 0.328281044960022\n",
      "[Training Epoch 0] Batch 2032, Loss 0.32931074500083923\n",
      "[Training Epoch 0] Batch 2033, Loss 0.35426679253578186\n",
      "[Training Epoch 0] Batch 2034, Loss 0.3534538745880127\n",
      "[Training Epoch 0] Batch 2035, Loss 0.3038674592971802\n",
      "[Training Epoch 0] Batch 2036, Loss 0.3249603807926178\n",
      "[Training Epoch 0] Batch 2037, Loss 0.351230263710022\n",
      "[Training Epoch 0] Batch 2038, Loss 0.33896002173423767\n",
      "[Training Epoch 0] Batch 2039, Loss 0.34784001111984253\n",
      "[Training Epoch 0] Batch 2040, Loss 0.33386144042015076\n",
      "[Training Epoch 0] Batch 2041, Loss 0.3715746998786926\n",
      "[Training Epoch 0] Batch 2042, Loss 0.33923423290252686\n",
      "[Training Epoch 0] Batch 2043, Loss 0.3422173261642456\n",
      "[Training Epoch 0] Batch 2044, Loss 0.3428608477115631\n",
      "[Training Epoch 0] Batch 2045, Loss 0.3396240770816803\n",
      "[Training Epoch 0] Batch 2046, Loss 0.34674084186553955\n",
      "[Training Epoch 0] Batch 2047, Loss 0.34653985500335693\n",
      "[Training Epoch 0] Batch 2048, Loss 0.3576241135597229\n",
      "[Training Epoch 0] Batch 2049, Loss 0.3093912601470947\n",
      "[Training Epoch 0] Batch 2050, Loss 0.36482176184654236\n",
      "[Training Epoch 0] Batch 2051, Loss 0.3659995198249817\n",
      "[Training Epoch 0] Batch 2052, Loss 0.34895551204681396\n",
      "[Training Epoch 0] Batch 2053, Loss 0.33543330430984497\n",
      "[Training Epoch 0] Batch 2054, Loss 0.3548940420150757\n",
      "[Training Epoch 0] Batch 2055, Loss 0.3398137092590332\n",
      "[Training Epoch 0] Batch 2056, Loss 0.3525209426879883\n",
      "[Training Epoch 0] Batch 2057, Loss 0.3283880352973938\n",
      "[Training Epoch 0] Batch 2058, Loss 0.33193355798721313\n",
      "[Training Epoch 0] Batch 2059, Loss 0.35953664779663086\n",
      "[Training Epoch 0] Batch 2060, Loss 0.33457326889038086\n",
      "[Training Epoch 0] Batch 2061, Loss 0.35634881258010864\n",
      "[Training Epoch 0] Batch 2062, Loss 0.3242172300815582\n",
      "[Training Epoch 0] Batch 2063, Loss 0.3391619920730591\n",
      "[Training Epoch 0] Batch 2064, Loss 0.34808462858200073\n",
      "[Training Epoch 0] Batch 2065, Loss 0.34587305784225464\n",
      "[Training Epoch 0] Batch 2066, Loss 0.34826287627220154\n",
      "[Training Epoch 0] Batch 2067, Loss 0.3383297920227051\n",
      "[Training Epoch 0] Batch 2068, Loss 0.35863518714904785\n",
      "[Training Epoch 0] Batch 2069, Loss 0.31693586707115173\n",
      "[Training Epoch 0] Batch 2070, Loss 0.33087849617004395\n",
      "[Training Epoch 0] Batch 2071, Loss 0.3437129557132721\n",
      "[Training Epoch 0] Batch 2072, Loss 0.36530160903930664\n",
      "[Training Epoch 0] Batch 2073, Loss 0.3392297625541687\n",
      "[Training Epoch 0] Batch 2074, Loss 0.3027169108390808\n",
      "[Training Epoch 0] Batch 2075, Loss 0.32577091455459595\n",
      "[Training Epoch 0] Batch 2076, Loss 0.31576356291770935\n",
      "[Training Epoch 0] Batch 2077, Loss 0.34618836641311646\n",
      "[Training Epoch 0] Batch 2078, Loss 0.3415065407752991\n",
      "[Training Epoch 0] Batch 2079, Loss 0.3090800642967224\n",
      "[Training Epoch 0] Batch 2080, Loss 0.3538612127304077\n",
      "[Training Epoch 0] Batch 2081, Loss 0.3216971755027771\n",
      "[Training Epoch 0] Batch 2082, Loss 0.3529951572418213\n",
      "[Training Epoch 0] Batch 2083, Loss 0.35968342423439026\n",
      "[Training Epoch 0] Batch 2084, Loss 0.33566680550575256\n",
      "[Training Epoch 0] Batch 2085, Loss 0.34992921352386475\n",
      "[Training Epoch 0] Batch 2086, Loss 0.39808011054992676\n",
      "[Training Epoch 0] Batch 2087, Loss 0.3423440754413605\n",
      "[Training Epoch 0] Batch 2088, Loss 0.3526322841644287\n",
      "[Training Epoch 0] Batch 2089, Loss 0.3607614040374756\n",
      "[Training Epoch 0] Batch 2090, Loss 0.33606797456741333\n",
      "[Training Epoch 0] Batch 2091, Loss 0.32458463311195374\n",
      "[Training Epoch 0] Batch 2092, Loss 0.35577428340911865\n",
      "[Training Epoch 0] Batch 2093, Loss 0.3473568558692932\n",
      "[Training Epoch 0] Batch 2094, Loss 0.33339789509773254\n",
      "[Training Epoch 0] Batch 2095, Loss 0.33864378929138184\n",
      "[Training Epoch 0] Batch 2096, Loss 0.3350648581981659\n",
      "[Training Epoch 0] Batch 2097, Loss 0.3098921775817871\n",
      "[Training Epoch 0] Batch 2098, Loss 0.35484790802001953\n",
      "[Training Epoch 0] Batch 2099, Loss 0.33449679613113403\n",
      "[Training Epoch 0] Batch 2100, Loss 0.31875520944595337\n",
      "[Training Epoch 0] Batch 2101, Loss 0.36892688274383545\n",
      "[Training Epoch 0] Batch 2102, Loss 0.34674692153930664\n",
      "[Training Epoch 0] Batch 2103, Loss 0.35470956563949585\n",
      "[Training Epoch 0] Batch 2104, Loss 0.3474065959453583\n",
      "[Training Epoch 0] Batch 2105, Loss 0.3292867839336395\n",
      "[Training Epoch 0] Batch 2106, Loss 0.3650897145271301\n",
      "[Training Epoch 0] Batch 2107, Loss 0.3529752790927887\n",
      "[Training Epoch 0] Batch 2108, Loss 0.3933612108230591\n",
      "[Training Epoch 0] Batch 2109, Loss 0.349567711353302\n",
      "[Training Epoch 0] Batch 2110, Loss 0.33056044578552246\n",
      "[Training Epoch 0] Batch 2111, Loss 0.3279546797275543\n",
      "[Training Epoch 0] Batch 2112, Loss 0.3552234172821045\n",
      "[Training Epoch 0] Batch 2113, Loss 0.3262941539287567\n",
      "[Training Epoch 0] Batch 2114, Loss 0.3448230028152466\n",
      "[Training Epoch 0] Batch 2115, Loss 0.32606321573257446\n",
      "[Training Epoch 0] Batch 2116, Loss 0.31882983446121216\n",
      "[Training Epoch 0] Batch 2117, Loss 0.36970433592796326\n",
      "[Training Epoch 0] Batch 2118, Loss 0.31891489028930664\n",
      "[Training Epoch 0] Batch 2119, Loss 0.35591644048690796\n",
      "[Training Epoch 0] Batch 2120, Loss 0.36535853147506714\n",
      "[Training Epoch 0] Batch 2121, Loss 0.3195074200630188\n",
      "[Training Epoch 0] Batch 2122, Loss 0.35116657614707947\n",
      "[Training Epoch 0] Batch 2123, Loss 0.3205583989620209\n",
      "[Training Epoch 0] Batch 2124, Loss 0.3112702965736389\n",
      "[Training Epoch 0] Batch 2125, Loss 0.3280264139175415\n",
      "[Training Epoch 0] Batch 2126, Loss 0.3307846188545227\n",
      "[Training Epoch 0] Batch 2127, Loss 0.3058527708053589\n",
      "[Training Epoch 0] Batch 2128, Loss 0.3219342529773712\n",
      "[Training Epoch 0] Batch 2129, Loss 0.33683276176452637\n",
      "[Training Epoch 0] Batch 2130, Loss 0.34189480543136597\n",
      "[Training Epoch 0] Batch 2131, Loss 0.36127957701683044\n",
      "[Training Epoch 0] Batch 2132, Loss 0.3282719552516937\n",
      "[Training Epoch 0] Batch 2133, Loss 0.35910817980766296\n",
      "[Training Epoch 0] Batch 2134, Loss 0.3490584194660187\n",
      "[Training Epoch 0] Batch 2135, Loss 0.309719443321228\n",
      "[Training Epoch 0] Batch 2136, Loss 0.31652626395225525\n",
      "[Training Epoch 0] Batch 2137, Loss 0.31028467416763306\n",
      "[Training Epoch 0] Batch 2138, Loss 0.36148741841316223\n",
      "[Training Epoch 0] Batch 2139, Loss 0.3397673964500427\n",
      "[Training Epoch 0] Batch 2140, Loss 0.34256240725517273\n",
      "[Training Epoch 0] Batch 2141, Loss 0.36206549406051636\n",
      "[Training Epoch 0] Batch 2142, Loss 0.34036868810653687\n",
      "[Training Epoch 0] Batch 2143, Loss 0.3621578812599182\n",
      "[Training Epoch 0] Batch 2144, Loss 0.3489101529121399\n",
      "[Training Epoch 0] Batch 2145, Loss 0.3453008234500885\n",
      "[Training Epoch 0] Batch 2146, Loss 0.3553459644317627\n",
      "[Training Epoch 0] Batch 2147, Loss 0.3408060669898987\n",
      "[Training Epoch 0] Batch 2148, Loss 0.33526623249053955\n",
      "[Training Epoch 0] Batch 2149, Loss 0.31865233182907104\n",
      "[Training Epoch 0] Batch 2150, Loss 0.36128392815589905\n",
      "[Training Epoch 0] Batch 2151, Loss 0.33561834692955017\n",
      "[Training Epoch 0] Batch 2152, Loss 0.3284875154495239\n",
      "[Training Epoch 0] Batch 2153, Loss 0.350208044052124\n",
      "[Training Epoch 0] Batch 2154, Loss 0.3303905129432678\n",
      "[Training Epoch 0] Batch 2155, Loss 0.33945316076278687\n",
      "[Training Epoch 0] Batch 2156, Loss 0.32781732082366943\n",
      "[Training Epoch 0] Batch 2157, Loss 0.35299140214920044\n",
      "[Training Epoch 0] Batch 2158, Loss 0.3687761425971985\n",
      "[Training Epoch 0] Batch 2159, Loss 0.3336746096611023\n",
      "[Training Epoch 0] Batch 2160, Loss 0.33083200454711914\n",
      "[Training Epoch 0] Batch 2161, Loss 0.34903544187545776\n",
      "[Training Epoch 0] Batch 2162, Loss 0.33591240644454956\n",
      "[Training Epoch 0] Batch 2163, Loss 0.3034133315086365\n",
      "[Training Epoch 0] Batch 2164, Loss 0.2973990738391876\n",
      "[Training Epoch 0] Batch 2165, Loss 0.33328789472579956\n",
      "[Training Epoch 0] Batch 2166, Loss 0.351223886013031\n",
      "[Training Epoch 0] Batch 2167, Loss 0.3335363268852234\n",
      "[Training Epoch 0] Batch 2168, Loss 0.3236045241355896\n",
      "[Training Epoch 0] Batch 2169, Loss 0.3485868573188782\n",
      "[Training Epoch 0] Batch 2170, Loss 0.319000244140625\n",
      "[Training Epoch 0] Batch 2171, Loss 0.34171944856643677\n",
      "[Training Epoch 0] Batch 2172, Loss 0.3343322277069092\n",
      "[Training Epoch 0] Batch 2173, Loss 0.3245716989040375\n",
      "[Training Epoch 0] Batch 2174, Loss 0.3205530345439911\n",
      "[Training Epoch 0] Batch 2175, Loss 0.3200192153453827\n",
      "[Training Epoch 0] Batch 2176, Loss 0.3352386951446533\n",
      "[Training Epoch 0] Batch 2177, Loss 0.31792157888412476\n",
      "[Training Epoch 0] Batch 2178, Loss 0.35977035760879517\n",
      "[Training Epoch 0] Batch 2179, Loss 0.33531251549720764\n",
      "[Training Epoch 0] Batch 2180, Loss 0.3298564553260803\n",
      "[Training Epoch 0] Batch 2181, Loss 0.34505075216293335\n",
      "[Training Epoch 0] Batch 2182, Loss 0.3262576162815094\n",
      "[Training Epoch 0] Batch 2183, Loss 0.34796786308288574\n",
      "[Training Epoch 0] Batch 2184, Loss 0.3021547794342041\n",
      "[Training Epoch 0] Batch 2185, Loss 0.3364003598690033\n",
      "[Training Epoch 0] Batch 2186, Loss 0.3614635467529297\n",
      "[Training Epoch 0] Batch 2187, Loss 0.34410780668258667\n",
      "[Training Epoch 0] Batch 2188, Loss 0.3520779609680176\n",
      "[Training Epoch 0] Batch 2189, Loss 0.35175126791000366\n",
      "[Training Epoch 0] Batch 2190, Loss 0.35846081376075745\n",
      "[Training Epoch 0] Batch 2191, Loss 0.34127277135849\n",
      "[Training Epoch 0] Batch 2192, Loss 0.3694624900817871\n",
      "[Training Epoch 0] Batch 2193, Loss 0.34340956807136536\n",
      "[Training Epoch 0] Batch 2194, Loss 0.3278296887874603\n",
      "[Training Epoch 0] Batch 2195, Loss 0.35253190994262695\n",
      "[Training Epoch 0] Batch 2196, Loss 0.2993753254413605\n",
      "[Training Epoch 0] Batch 2197, Loss 0.337357759475708\n",
      "[Training Epoch 0] Batch 2198, Loss 0.3515472412109375\n",
      "[Training Epoch 0] Batch 2199, Loss 0.3487989902496338\n",
      "[Training Epoch 0] Batch 2200, Loss 0.3042793869972229\n",
      "[Training Epoch 0] Batch 2201, Loss 0.3314157724380493\n",
      "[Training Epoch 0] Batch 2202, Loss 0.32932397723197937\n",
      "[Training Epoch 0] Batch 2203, Loss 0.33450454473495483\n",
      "[Training Epoch 0] Batch 2204, Loss 0.32435494661331177\n",
      "[Training Epoch 0] Batch 2205, Loss 0.3319779336452484\n",
      "[Training Epoch 0] Batch 2206, Loss 0.3303263783454895\n",
      "[Training Epoch 0] Batch 2207, Loss 0.35469600558280945\n",
      "[Training Epoch 0] Batch 2208, Loss 0.3563421964645386\n",
      "[Training Epoch 0] Batch 2209, Loss 0.3308480381965637\n",
      "[Training Epoch 0] Batch 2210, Loss 0.31837230920791626\n",
      "[Training Epoch 0] Batch 2211, Loss 0.3444559574127197\n",
      "[Training Epoch 0] Batch 2212, Loss 0.3522081971168518\n",
      "[Training Epoch 0] Batch 2213, Loss 0.3290824592113495\n",
      "[Training Epoch 0] Batch 2214, Loss 0.32523781061172485\n",
      "[Training Epoch 0] Batch 2215, Loss 0.3318372368812561\n",
      "[Training Epoch 0] Batch 2216, Loss 0.31005650758743286\n",
      "[Training Epoch 0] Batch 2217, Loss 0.34900107979774475\n",
      "[Training Epoch 0] Batch 2218, Loss 0.33409661054611206\n",
      "[Training Epoch 0] Batch 2219, Loss 0.33966153860092163\n",
      "[Training Epoch 0] Batch 2220, Loss 0.3555399179458618\n",
      "[Training Epoch 0] Batch 2221, Loss 0.3481469452381134\n",
      "[Training Epoch 0] Batch 2222, Loss 0.35656702518463135\n",
      "[Training Epoch 0] Batch 2223, Loss 0.3162713050842285\n",
      "[Training Epoch 0] Batch 2224, Loss 0.3050166964530945\n",
      "[Training Epoch 0] Batch 2225, Loss 0.315584272146225\n",
      "[Training Epoch 0] Batch 2226, Loss 0.33277565240859985\n",
      "[Training Epoch 0] Batch 2227, Loss 0.31911802291870117\n",
      "[Training Epoch 0] Batch 2228, Loss 0.3356413245201111\n",
      "[Training Epoch 0] Batch 2229, Loss 0.33226239681243896\n",
      "[Training Epoch 0] Batch 2230, Loss 0.35030409693717957\n",
      "[Training Epoch 0] Batch 2231, Loss 0.3596493899822235\n",
      "[Training Epoch 0] Batch 2232, Loss 0.3574810326099396\n",
      "[Training Epoch 0] Batch 2233, Loss 0.31108400225639343\n",
      "[Training Epoch 0] Batch 2234, Loss 0.3396962881088257\n",
      "[Training Epoch 0] Batch 2235, Loss 0.33221206068992615\n",
      "[Training Epoch 0] Batch 2236, Loss 0.39695051312446594\n",
      "[Training Epoch 0] Batch 2237, Loss 0.3485666513442993\n",
      "[Training Epoch 0] Batch 2238, Loss 0.32467514276504517\n",
      "[Training Epoch 0] Batch 2239, Loss 0.3104757070541382\n",
      "[Training Epoch 0] Batch 2240, Loss 0.3554917275905609\n",
      "[Training Epoch 0] Batch 2241, Loss 0.368314266204834\n",
      "[Training Epoch 0] Batch 2242, Loss 0.33501553535461426\n",
      "[Training Epoch 0] Batch 2243, Loss 0.35165613889694214\n",
      "[Training Epoch 0] Batch 2244, Loss 0.3196501135826111\n",
      "[Training Epoch 0] Batch 2245, Loss 0.33761000633239746\n",
      "[Training Epoch 0] Batch 2246, Loss 0.340182900428772\n",
      "[Training Epoch 0] Batch 2247, Loss 0.3438394367694855\n",
      "[Training Epoch 0] Batch 2248, Loss 0.3155462145805359\n",
      "[Training Epoch 0] Batch 2249, Loss 0.3132327198982239\n",
      "[Training Epoch 0] Batch 2250, Loss 0.30326563119888306\n",
      "[Training Epoch 0] Batch 2251, Loss 0.34219488501548767\n",
      "[Training Epoch 0] Batch 2252, Loss 0.3290795087814331\n",
      "[Training Epoch 0] Batch 2253, Loss 0.3558127284049988\n",
      "[Training Epoch 0] Batch 2254, Loss 0.31147679686546326\n",
      "[Training Epoch 0] Batch 2255, Loss 0.322322279214859\n",
      "[Training Epoch 0] Batch 2256, Loss 0.3500211238861084\n",
      "[Training Epoch 0] Batch 2257, Loss 0.33405986428260803\n",
      "[Training Epoch 0] Batch 2258, Loss 0.3596208691596985\n",
      "[Training Epoch 0] Batch 2259, Loss 0.30603528022766113\n",
      "[Training Epoch 0] Batch 2260, Loss 0.3038107752799988\n",
      "[Training Epoch 0] Batch 2261, Loss 0.3422926664352417\n",
      "[Training Epoch 0] Batch 2262, Loss 0.35887545347213745\n",
      "[Training Epoch 0] Batch 2263, Loss 0.3509185016155243\n",
      "[Training Epoch 0] Batch 2264, Loss 0.32975828647613525\n",
      "[Training Epoch 0] Batch 2265, Loss 0.3268323242664337\n",
      "[Training Epoch 0] Batch 2266, Loss 0.3605976998806\n",
      "[Training Epoch 0] Batch 2267, Loss 0.35711508989334106\n",
      "[Training Epoch 0] Batch 2268, Loss 0.3300427198410034\n",
      "[Training Epoch 0] Batch 2269, Loss 0.3116949200630188\n",
      "[Training Epoch 0] Batch 2270, Loss 0.34200209379196167\n",
      "[Training Epoch 0] Batch 2271, Loss 0.32892897725105286\n",
      "[Training Epoch 0] Batch 2272, Loss 0.31793826818466187\n",
      "[Training Epoch 0] Batch 2273, Loss 0.3195723295211792\n",
      "[Training Epoch 0] Batch 2274, Loss 0.3571707010269165\n",
      "[Training Epoch 0] Batch 2275, Loss 0.3286326229572296\n",
      "[Training Epoch 0] Batch 2276, Loss 0.3766575753688812\n",
      "[Training Epoch 0] Batch 2277, Loss 0.34043455123901367\n",
      "[Training Epoch 0] Batch 2278, Loss 0.32660147547721863\n",
      "[Training Epoch 0] Batch 2279, Loss 0.3305549621582031\n",
      "[Training Epoch 0] Batch 2280, Loss 0.316115140914917\n",
      "[Training Epoch 0] Batch 2281, Loss 0.3375771641731262\n",
      "[Training Epoch 0] Batch 2282, Loss 0.3384215533733368\n",
      "[Training Epoch 0] Batch 2283, Loss 0.3188353180885315\n",
      "[Training Epoch 0] Batch 2284, Loss 0.3496668338775635\n",
      "[Training Epoch 0] Batch 2285, Loss 0.357774555683136\n",
      "[Training Epoch 0] Batch 2286, Loss 0.31725743412971497\n",
      "[Training Epoch 0] Batch 2287, Loss 0.3217933475971222\n",
      "[Training Epoch 0] Batch 2288, Loss 0.3320484161376953\n",
      "[Training Epoch 0] Batch 2289, Loss 0.3397643566131592\n",
      "[Training Epoch 0] Batch 2290, Loss 0.33551284670829773\n",
      "[Training Epoch 0] Batch 2291, Loss 0.3535322844982147\n",
      "[Training Epoch 0] Batch 2292, Loss 0.34427815675735474\n",
      "[Training Epoch 0] Batch 2293, Loss 0.3098437190055847\n",
      "[Training Epoch 0] Batch 2294, Loss 0.3436132073402405\n",
      "[Training Epoch 0] Batch 2295, Loss 0.36606675386428833\n",
      "[Training Epoch 0] Batch 2296, Loss 0.36096903681755066\n",
      "[Training Epoch 0] Batch 2297, Loss 0.32945847511291504\n",
      "[Training Epoch 0] Batch 2298, Loss 0.329953134059906\n",
      "[Training Epoch 0] Batch 2299, Loss 0.3256935477256775\n",
      "[Training Epoch 0] Batch 2300, Loss 0.3459022045135498\n",
      "[Training Epoch 0] Batch 2301, Loss 0.33577507734298706\n",
      "[Training Epoch 0] Batch 2302, Loss 0.32629942893981934\n",
      "[Training Epoch 0] Batch 2303, Loss 0.3355104923248291\n",
      "[Training Epoch 0] Batch 2304, Loss 0.34436434507369995\n",
      "[Training Epoch 0] Batch 2305, Loss 0.34461337327957153\n",
      "[Training Epoch 0] Batch 2306, Loss 0.3443325161933899\n",
      "[Training Epoch 0] Batch 2307, Loss 0.35833102464675903\n",
      "[Training Epoch 0] Batch 2308, Loss 0.33450615406036377\n",
      "[Training Epoch 0] Batch 2309, Loss 0.3323822021484375\n",
      "[Training Epoch 0] Batch 2310, Loss 0.3399798274040222\n",
      "[Training Epoch 0] Batch 2311, Loss 0.31489241123199463\n",
      "[Training Epoch 0] Batch 2312, Loss 0.3229868710041046\n",
      "[Training Epoch 0] Batch 2313, Loss 0.36541566252708435\n",
      "[Training Epoch 0] Batch 2314, Loss 0.3568558096885681\n",
      "[Training Epoch 0] Batch 2315, Loss 0.3513230085372925\n",
      "[Training Epoch 0] Batch 2316, Loss 0.352283239364624\n",
      "[Training Epoch 0] Batch 2317, Loss 0.3616023659706116\n",
      "[Training Epoch 0] Batch 2318, Loss 0.34860891103744507\n",
      "[Training Epoch 0] Batch 2319, Loss 0.33037999272346497\n",
      "[Training Epoch 0] Batch 2320, Loss 0.3334105312824249\n",
      "[Training Epoch 0] Batch 2321, Loss 0.3039299249649048\n",
      "[Training Epoch 0] Batch 2322, Loss 0.32712873816490173\n",
      "[Training Epoch 0] Batch 2323, Loss 0.3433167338371277\n",
      "[Training Epoch 0] Batch 2324, Loss 0.34262174367904663\n",
      "[Training Epoch 0] Batch 2325, Loss 0.33239710330963135\n",
      "[Training Epoch 0] Batch 2326, Loss 0.3041543960571289\n",
      "[Training Epoch 0] Batch 2327, Loss 0.32617896795272827\n",
      "[Training Epoch 0] Batch 2328, Loss 0.34409165382385254\n",
      "[Training Epoch 0] Batch 2329, Loss 0.3506345748901367\n",
      "[Training Epoch 0] Batch 2330, Loss 0.35384759306907654\n",
      "[Training Epoch 0] Batch 2331, Loss 0.3576809763908386\n",
      "[Training Epoch 0] Batch 2332, Loss 0.31948405504226685\n",
      "[Training Epoch 0] Batch 2333, Loss 0.338962584733963\n",
      "[Training Epoch 0] Batch 2334, Loss 0.32339856028556824\n",
      "[Training Epoch 0] Batch 2335, Loss 0.32787469029426575\n",
      "[Training Epoch 0] Batch 2336, Loss 0.3406142294406891\n",
      "[Training Epoch 0] Batch 2337, Loss 0.3526660203933716\n",
      "[Training Epoch 0] Batch 2338, Loss 0.3157271146774292\n",
      "[Training Epoch 0] Batch 2339, Loss 0.30061575770378113\n",
      "[Training Epoch 0] Batch 2340, Loss 0.3424120545387268\n",
      "[Training Epoch 0] Batch 2341, Loss 0.3077322244644165\n",
      "[Training Epoch 0] Batch 2342, Loss 0.3205650746822357\n",
      "[Training Epoch 0] Batch 2343, Loss 0.3199099898338318\n",
      "[Training Epoch 0] Batch 2344, Loss 0.3521415591239929\n",
      "[Training Epoch 0] Batch 2345, Loss 0.34546512365341187\n",
      "[Training Epoch 0] Batch 2346, Loss 0.3306502103805542\n",
      "[Training Epoch 0] Batch 2347, Loss 0.33235830068588257\n",
      "[Training Epoch 0] Batch 2348, Loss 0.3296785354614258\n",
      "[Training Epoch 0] Batch 2349, Loss 0.3352339267730713\n",
      "[Training Epoch 0] Batch 2350, Loss 0.3314424753189087\n",
      "[Training Epoch 0] Batch 2351, Loss 0.3427295684814453\n",
      "[Training Epoch 0] Batch 2352, Loss 0.3505558371543884\n",
      "[Training Epoch 0] Batch 2353, Loss 0.3115047216415405\n",
      "[Training Epoch 0] Batch 2354, Loss 0.3361812233924866\n",
      "[Training Epoch 0] Batch 2355, Loss 0.32273581624031067\n",
      "[Training Epoch 0] Batch 2356, Loss 0.34247809648513794\n",
      "[Training Epoch 0] Batch 2357, Loss 0.3397565484046936\n",
      "[Training Epoch 0] Batch 2358, Loss 0.31638747453689575\n",
      "[Training Epoch 0] Batch 2359, Loss 0.31275853514671326\n",
      "[Training Epoch 0] Batch 2360, Loss 0.3359714150428772\n",
      "[Training Epoch 0] Batch 2361, Loss 0.31950151920318604\n",
      "[Training Epoch 0] Batch 2362, Loss 0.34160375595092773\n",
      "[Training Epoch 0] Batch 2363, Loss 0.3293954133987427\n",
      "[Training Epoch 0] Batch 2364, Loss 0.3394921123981476\n",
      "[Training Epoch 0] Batch 2365, Loss 0.3439409136772156\n",
      "[Training Epoch 0] Batch 2366, Loss 0.32308441400527954\n",
      "[Training Epoch 0] Batch 2367, Loss 0.34140676259994507\n",
      "[Training Epoch 0] Batch 2368, Loss 0.32210516929626465\n",
      "[Training Epoch 0] Batch 2369, Loss 0.3662668466567993\n",
      "[Training Epoch 0] Batch 2370, Loss 0.3339843451976776\n",
      "[Training Epoch 0] Batch 2371, Loss 0.32856959104537964\n",
      "[Training Epoch 0] Batch 2372, Loss 0.3486272692680359\n",
      "[Training Epoch 0] Batch 2373, Loss 0.35703563690185547\n",
      "[Training Epoch 0] Batch 2374, Loss 0.33800408244132996\n",
      "[Training Epoch 0] Batch 2375, Loss 0.34225112199783325\n",
      "[Training Epoch 0] Batch 2376, Loss 0.3196655511856079\n",
      "[Training Epoch 0] Batch 2377, Loss 0.34495410323143005\n",
      "[Training Epoch 0] Batch 2378, Loss 0.3152673840522766\n",
      "[Training Epoch 0] Batch 2379, Loss 0.3385525941848755\n",
      "[Training Epoch 0] Batch 2380, Loss 0.36250966787338257\n",
      "[Training Epoch 0] Batch 2381, Loss 0.34069401025772095\n",
      "[Training Epoch 0] Batch 2382, Loss 0.3242742717266083\n",
      "[Training Epoch 0] Batch 2383, Loss 0.32562756538391113\n",
      "[Training Epoch 0] Batch 2384, Loss 0.3380091190338135\n",
      "[Training Epoch 0] Batch 2385, Loss 0.3135908842086792\n",
      "[Training Epoch 0] Batch 2386, Loss 0.3437765836715698\n",
      "[Training Epoch 0] Batch 2387, Loss 0.3504401445388794\n",
      "[Training Epoch 0] Batch 2388, Loss 0.33421826362609863\n",
      "[Training Epoch 0] Batch 2389, Loss 0.3471302092075348\n",
      "[Training Epoch 0] Batch 2390, Loss 0.3500668406486511\n",
      "[Training Epoch 0] Batch 2391, Loss 0.31433969736099243\n",
      "[Training Epoch 0] Batch 2392, Loss 0.3533591032028198\n",
      "[Training Epoch 0] Batch 2393, Loss 0.3411802649497986\n",
      "[Training Epoch 0] Batch 2394, Loss 0.3604567050933838\n",
      "[Training Epoch 0] Batch 2395, Loss 0.35700976848602295\n",
      "[Training Epoch 0] Batch 2396, Loss 0.31722643971443176\n",
      "[Training Epoch 0] Batch 2397, Loss 0.30659598112106323\n",
      "[Training Epoch 0] Batch 2398, Loss 0.334004670381546\n",
      "[Training Epoch 0] Batch 2399, Loss 0.3058643043041229\n",
      "[Training Epoch 0] Batch 2400, Loss 0.3241313695907593\n",
      "[Training Epoch 0] Batch 2401, Loss 0.35429248213768005\n",
      "[Training Epoch 0] Batch 2402, Loss 0.3251221776008606\n",
      "[Training Epoch 0] Batch 2403, Loss 0.33499932289123535\n",
      "[Training Epoch 0] Batch 2404, Loss 0.3193420469760895\n",
      "[Training Epoch 0] Batch 2405, Loss 0.31914740800857544\n",
      "[Training Epoch 0] Batch 2406, Loss 0.351024866104126\n",
      "[Training Epoch 0] Batch 2407, Loss 0.35829704999923706\n",
      "[Training Epoch 0] Batch 2408, Loss 0.33335867524147034\n",
      "[Training Epoch 0] Batch 2409, Loss 0.3189067244529724\n",
      "[Training Epoch 0] Batch 2410, Loss 0.35445547103881836\n",
      "[Training Epoch 0] Batch 2411, Loss 0.3384644389152527\n",
      "[Training Epoch 0] Batch 2412, Loss 0.3192237913608551\n",
      "[Training Epoch 0] Batch 2413, Loss 0.34626877307891846\n",
      "[Training Epoch 0] Batch 2414, Loss 0.3219984173774719\n",
      "[Training Epoch 0] Batch 2415, Loss 0.33867642283439636\n",
      "[Training Epoch 0] Batch 2416, Loss 0.32277220487594604\n",
      "[Training Epoch 0] Batch 2417, Loss 0.347858190536499\n",
      "[Training Epoch 0] Batch 2418, Loss 0.31806135177612305\n",
      "[Training Epoch 0] Batch 2419, Loss 0.3395352363586426\n",
      "[Training Epoch 0] Batch 2420, Loss 0.3393348455429077\n",
      "[Training Epoch 0] Batch 2421, Loss 0.3306078314781189\n",
      "[Training Epoch 0] Batch 2422, Loss 0.33064842224121094\n",
      "[Training Epoch 0] Batch 2423, Loss 0.33841651678085327\n",
      "[Training Epoch 0] Batch 2424, Loss 0.352205365896225\n",
      "[Training Epoch 0] Batch 2425, Loss 0.33713793754577637\n",
      "[Training Epoch 0] Batch 2426, Loss 0.3245430588722229\n",
      "[Training Epoch 0] Batch 2427, Loss 0.32111531496047974\n",
      "[Training Epoch 0] Batch 2428, Loss 0.31540733575820923\n",
      "[Training Epoch 0] Batch 2429, Loss 0.33969318866729736\n",
      "[Training Epoch 0] Batch 2430, Loss 0.33191582560539246\n",
      "[Training Epoch 0] Batch 2431, Loss 0.32052892446517944\n",
      "[Training Epoch 0] Batch 2432, Loss 0.323835164308548\n",
      "[Training Epoch 0] Batch 2433, Loss 0.3744243383407593\n",
      "[Training Epoch 0] Batch 2434, Loss 0.3298351764678955\n",
      "[Training Epoch 0] Batch 2435, Loss 0.33745265007019043\n",
      "[Training Epoch 0] Batch 2436, Loss 0.323125422000885\n",
      "[Training Epoch 0] Batch 2437, Loss 0.35647884011268616\n",
      "[Training Epoch 0] Batch 2438, Loss 0.3224930167198181\n",
      "[Training Epoch 0] Batch 2439, Loss 0.34222304821014404\n",
      "[Training Epoch 0] Batch 2440, Loss 0.33523350954055786\n",
      "[Training Epoch 0] Batch 2441, Loss 0.33030107617378235\n",
      "[Training Epoch 0] Batch 2442, Loss 0.33728569746017456\n",
      "[Training Epoch 0] Batch 2443, Loss 0.34849822521209717\n",
      "[Training Epoch 0] Batch 2444, Loss 0.33732831478118896\n",
      "[Training Epoch 0] Batch 2445, Loss 0.3198850154876709\n",
      "[Training Epoch 0] Batch 2446, Loss 0.3242608904838562\n",
      "[Training Epoch 0] Batch 2447, Loss 0.344178169965744\n",
      "[Training Epoch 0] Batch 2448, Loss 0.3347611725330353\n",
      "[Training Epoch 0] Batch 2449, Loss 0.3195280432701111\n",
      "[Training Epoch 0] Batch 2450, Loss 0.32430222630500793\n",
      "[Training Epoch 0] Batch 2451, Loss 0.3520369827747345\n",
      "[Training Epoch 0] Batch 2452, Loss 0.2974218726158142\n",
      "[Training Epoch 0] Batch 2453, Loss 0.32210394740104675\n",
      "[Training Epoch 0] Batch 2454, Loss 0.3383370041847229\n",
      "[Training Epoch 0] Batch 2455, Loss 0.3358076512813568\n",
      "[Training Epoch 0] Batch 2456, Loss 0.3191557228565216\n",
      "[Training Epoch 0] Batch 2457, Loss 0.33995622396469116\n",
      "[Training Epoch 0] Batch 2458, Loss 0.32333871722221375\n",
      "[Training Epoch 0] Batch 2459, Loss 0.3539835214614868\n",
      "[Training Epoch 0] Batch 2460, Loss 0.35406965017318726\n",
      "[Training Epoch 0] Batch 2461, Loss 0.33306387066841125\n",
      "[Training Epoch 0] Batch 2462, Loss 0.33029887080192566\n",
      "[Training Epoch 0] Batch 2463, Loss 0.32201048731803894\n",
      "[Training Epoch 0] Batch 2464, Loss 0.34114059805870056\n",
      "[Training Epoch 0] Batch 2465, Loss 0.3352462649345398\n",
      "[Training Epoch 0] Batch 2466, Loss 0.3413546681404114\n",
      "[Training Epoch 0] Batch 2467, Loss 0.33828237652778625\n",
      "[Training Epoch 0] Batch 2468, Loss 0.33889639377593994\n",
      "[Training Epoch 0] Batch 2469, Loss 0.32057690620422363\n",
      "[Training Epoch 0] Batch 2470, Loss 0.32926470041275024\n",
      "[Training Epoch 0] Batch 2471, Loss 0.38519173860549927\n",
      "[Training Epoch 0] Batch 2472, Loss 0.35389211773872375\n",
      "[Training Epoch 0] Batch 2473, Loss 0.34177762269973755\n",
      "[Training Epoch 0] Batch 2474, Loss 0.33684444427490234\n",
      "[Training Epoch 0] Batch 2475, Loss 0.326498419046402\n",
      "[Training Epoch 0] Batch 2476, Loss 0.3248034715652466\n",
      "[Training Epoch 0] Batch 2477, Loss 0.33763226866722107\n",
      "[Training Epoch 0] Batch 2478, Loss 0.317487895488739\n",
      "[Training Epoch 0] Batch 2479, Loss 0.313831627368927\n",
      "[Training Epoch 0] Batch 2480, Loss 0.34127527475357056\n",
      "[Training Epoch 0] Batch 2481, Loss 0.35876402258872986\n",
      "[Training Epoch 0] Batch 2482, Loss 0.3373726010322571\n",
      "[Training Epoch 0] Batch 2483, Loss 0.3153277039527893\n",
      "[Training Epoch 0] Batch 2484, Loss 0.3317480683326721\n",
      "[Training Epoch 0] Batch 2485, Loss 0.35447224974632263\n",
      "[Training Epoch 0] Batch 2486, Loss 0.32225465774536133\n",
      "[Training Epoch 0] Batch 2487, Loss 0.3299829363822937\n",
      "[Training Epoch 0] Batch 2488, Loss 0.33278343081474304\n",
      "[Training Epoch 0] Batch 2489, Loss 0.32798707485198975\n",
      "[Training Epoch 0] Batch 2490, Loss 0.3442789316177368\n",
      "[Training Epoch 0] Batch 2491, Loss 0.34061679244041443\n",
      "[Training Epoch 0] Batch 2492, Loss 0.3498172163963318\n",
      "[Training Epoch 0] Batch 2493, Loss 0.3086875379085541\n",
      "[Training Epoch 0] Batch 2494, Loss 0.3161768913269043\n",
      "[Training Epoch 0] Batch 2495, Loss 0.3459847867488861\n",
      "[Training Epoch 0] Batch 2496, Loss 0.32121577858924866\n",
      "[Training Epoch 0] Batch 2497, Loss 0.3449305295944214\n",
      "[Training Epoch 0] Batch 2498, Loss 0.34065937995910645\n",
      "[Training Epoch 0] Batch 2499, Loss 0.3568772077560425\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3674486577510834\n",
      "[Training Epoch 0] Batch 2501, Loss 0.3329310715198517\n",
      "[Training Epoch 0] Batch 2502, Loss 0.3397451639175415\n",
      "[Training Epoch 0] Batch 2503, Loss 0.3280766010284424\n",
      "[Training Epoch 0] Batch 2504, Loss 0.32939815521240234\n",
      "[Training Epoch 0] Batch 2505, Loss 0.3407796621322632\n",
      "[Training Epoch 0] Batch 2506, Loss 0.339908242225647\n",
      "[Training Epoch 0] Batch 2507, Loss 0.3405596613883972\n",
      "[Training Epoch 0] Batch 2508, Loss 0.3177288770675659\n",
      "[Training Epoch 0] Batch 2509, Loss 0.33497774600982666\n",
      "[Training Epoch 0] Batch 2510, Loss 0.3396303057670593\n",
      "[Training Epoch 0] Batch 2511, Loss 0.3310134708881378\n",
      "[Training Epoch 0] Batch 2512, Loss 0.31176239252090454\n",
      "[Training Epoch 0] Batch 2513, Loss 0.34543371200561523\n",
      "[Training Epoch 0] Batch 2514, Loss 0.3146418333053589\n",
      "[Training Epoch 0] Batch 2515, Loss 0.32906192541122437\n",
      "[Training Epoch 0] Batch 2516, Loss 0.3220958113670349\n",
      "[Training Epoch 0] Batch 2517, Loss 0.34998250007629395\n",
      "[Training Epoch 0] Batch 2518, Loss 0.3210880160331726\n",
      "[Training Epoch 0] Batch 2519, Loss 0.3240073323249817\n",
      "[Training Epoch 0] Batch 2520, Loss 0.3252817690372467\n",
      "[Training Epoch 0] Batch 2521, Loss 0.3145672678947449\n",
      "[Training Epoch 0] Batch 2522, Loss 0.34666895866394043\n",
      "[Training Epoch 0] Batch 2523, Loss 0.37351328134536743\n",
      "[Training Epoch 0] Batch 2524, Loss 0.32874614000320435\n",
      "[Training Epoch 0] Batch 2525, Loss 0.2986067235469818\n",
      "[Training Epoch 0] Batch 2526, Loss 0.357899934053421\n",
      "[Training Epoch 0] Batch 2527, Loss 0.3010180592536926\n",
      "[Training Epoch 0] Batch 2528, Loss 0.3273705542087555\n",
      "[Training Epoch 0] Batch 2529, Loss 0.3518531918525696\n",
      "[Training Epoch 0] Batch 2530, Loss 0.3281867504119873\n",
      "[Training Epoch 0] Batch 2531, Loss 0.3196295201778412\n",
      "[Training Epoch 0] Batch 2532, Loss 0.35228097438812256\n",
      "[Training Epoch 0] Batch 2533, Loss 0.32931846380233765\n",
      "[Training Epoch 0] Batch 2534, Loss 0.32927146553993225\n",
      "[Training Epoch 0] Batch 2535, Loss 0.35723888874053955\n",
      "[Training Epoch 0] Batch 2536, Loss 0.31537413597106934\n",
      "[Training Epoch 0] Batch 2537, Loss 0.3057975172996521\n",
      "[Training Epoch 0] Batch 2538, Loss 0.3531612753868103\n",
      "[Training Epoch 0] Batch 2539, Loss 0.31476688385009766\n",
      "[Training Epoch 0] Batch 2540, Loss 0.32890135049819946\n",
      "[Training Epoch 0] Batch 2541, Loss 0.3438493609428406\n",
      "[Training Epoch 0] Batch 2542, Loss 0.30473408102989197\n",
      "[Training Epoch 0] Batch 2543, Loss 0.33423078060150146\n",
      "[Training Epoch 0] Batch 2544, Loss 0.3446783423423767\n",
      "[Training Epoch 0] Batch 2545, Loss 0.3281135559082031\n",
      "[Training Epoch 0] Batch 2546, Loss 0.3067476749420166\n",
      "[Training Epoch 0] Batch 2547, Loss 0.3533805310726166\n",
      "[Training Epoch 0] Batch 2548, Loss 0.3391987681388855\n",
      "[Training Epoch 0] Batch 2549, Loss 0.3120400309562683\n",
      "[Training Epoch 0] Batch 2550, Loss 0.34858348965644836\n",
      "[Training Epoch 0] Batch 2551, Loss 0.33281266689300537\n",
      "[Training Epoch 0] Batch 2552, Loss 0.3024125099182129\n",
      "[Training Epoch 0] Batch 2553, Loss 0.31384700536727905\n",
      "[Training Epoch 0] Batch 2554, Loss 0.33831512928009033\n",
      "[Training Epoch 0] Batch 2555, Loss 0.34604930877685547\n",
      "[Training Epoch 0] Batch 2556, Loss 0.3199818730354309\n",
      "[Training Epoch 0] Batch 2557, Loss 0.3400660753250122\n",
      "[Training Epoch 0] Batch 2558, Loss 0.3256725072860718\n",
      "[Training Epoch 0] Batch 2559, Loss 0.3036844730377197\n",
      "[Training Epoch 0] Batch 2560, Loss 0.3491007685661316\n",
      "[Training Epoch 0] Batch 2561, Loss 0.3381772041320801\n",
      "[Training Epoch 0] Batch 2562, Loss 0.34423333406448364\n",
      "[Training Epoch 0] Batch 2563, Loss 0.3198901414871216\n",
      "[Training Epoch 0] Batch 2564, Loss 0.35071054100990295\n",
      "[Training Epoch 0] Batch 2565, Loss 0.3486939072608948\n",
      "[Training Epoch 0] Batch 2566, Loss 0.33506304025650024\n",
      "[Training Epoch 0] Batch 2567, Loss 0.32950860261917114\n",
      "[Training Epoch 0] Batch 2568, Loss 0.35082656145095825\n",
      "[Training Epoch 0] Batch 2569, Loss 0.3311039209365845\n",
      "[Training Epoch 0] Batch 2570, Loss 0.34688520431518555\n",
      "[Training Epoch 0] Batch 2571, Loss 0.3250213861465454\n",
      "[Training Epoch 0] Batch 2572, Loss 0.33102136850357056\n",
      "[Training Epoch 0] Batch 2573, Loss 0.3359947204589844\n",
      "[Training Epoch 0] Batch 2574, Loss 0.32544469833374023\n",
      "[Training Epoch 0] Batch 2575, Loss 0.34006500244140625\n",
      "[Training Epoch 0] Batch 2576, Loss 0.35146579146385193\n",
      "[Training Epoch 0] Batch 2577, Loss 0.34779876470565796\n",
      "[Training Epoch 0] Batch 2578, Loss 0.364743709564209\n",
      "[Training Epoch 0] Batch 2579, Loss 0.3480730652809143\n",
      "[Training Epoch 0] Batch 2580, Loss 0.3331112265586853\n",
      "[Training Epoch 0] Batch 2581, Loss 0.32437753677368164\n",
      "[Training Epoch 0] Batch 2582, Loss 0.35838958621025085\n",
      "[Training Epoch 0] Batch 2583, Loss 0.3572513461112976\n",
      "[Training Epoch 0] Batch 2584, Loss 0.3145335912704468\n",
      "[Training Epoch 0] Batch 2585, Loss 0.3203772306442261\n",
      "[Training Epoch 0] Batch 2586, Loss 0.33093106746673584\n",
      "[Training Epoch 0] Batch 2587, Loss 0.29663416743278503\n",
      "[Training Epoch 0] Batch 2588, Loss 0.3206431269645691\n",
      "[Training Epoch 0] Batch 2589, Loss 0.3586024045944214\n",
      "[Training Epoch 0] Batch 2590, Loss 0.33087974786758423\n",
      "[Training Epoch 0] Batch 2591, Loss 0.336170494556427\n",
      "[Training Epoch 0] Batch 2592, Loss 0.3516819477081299\n",
      "[Training Epoch 0] Batch 2593, Loss 0.32135483622550964\n",
      "[Training Epoch 0] Batch 2594, Loss 0.31868016719818115\n",
      "[Training Epoch 0] Batch 2595, Loss 0.30835413932800293\n",
      "[Training Epoch 0] Batch 2596, Loss 0.3333057463169098\n",
      "[Training Epoch 0] Batch 2597, Loss 0.3329615592956543\n",
      "[Training Epoch 0] Batch 2598, Loss 0.3316882252693176\n",
      "[Training Epoch 0] Batch 2599, Loss 0.32846301794052124\n",
      "[Training Epoch 0] Batch 2600, Loss 0.3487399220466614\n",
      "[Training Epoch 0] Batch 2601, Loss 0.34736400842666626\n",
      "[Training Epoch 0] Batch 2602, Loss 0.3386930227279663\n",
      "[Training Epoch 0] Batch 2603, Loss 0.3327367603778839\n",
      "[Training Epoch 0] Batch 2604, Loss 0.3388587236404419\n",
      "[Training Epoch 0] Batch 2605, Loss 0.3356463313102722\n",
      "[Training Epoch 0] Batch 2606, Loss 0.2897835671901703\n",
      "[Training Epoch 0] Batch 2607, Loss 0.332665354013443\n",
      "[Training Epoch 0] Batch 2608, Loss 0.32365837693214417\n",
      "[Training Epoch 0] Batch 2609, Loss 0.3555837571620941\n",
      "[Training Epoch 0] Batch 2610, Loss 0.3198208808898926\n",
      "[Training Epoch 0] Batch 2611, Loss 0.3265696167945862\n",
      "[Training Epoch 0] Batch 2612, Loss 0.33595365285873413\n",
      "[Training Epoch 0] Batch 2613, Loss 0.3619319200515747\n",
      "[Training Epoch 0] Batch 2614, Loss 0.346746027469635\n",
      "[Training Epoch 0] Batch 2615, Loss 0.3330419361591339\n",
      "[Training Epoch 0] Batch 2616, Loss 0.29630768299102783\n",
      "[Training Epoch 0] Batch 2617, Loss 0.3541538417339325\n",
      "[Training Epoch 0] Batch 2618, Loss 0.3084035813808441\n",
      "[Training Epoch 0] Batch 2619, Loss 0.33438602089881897\n",
      "[Training Epoch 0] Batch 2620, Loss 0.3502688407897949\n",
      "[Training Epoch 0] Batch 2621, Loss 0.33925050497055054\n",
      "[Training Epoch 0] Batch 2622, Loss 0.31229472160339355\n",
      "[Training Epoch 0] Batch 2623, Loss 0.35038232803344727\n",
      "[Training Epoch 0] Batch 2624, Loss 0.34619447588920593\n",
      "[Training Epoch 0] Batch 2625, Loss 0.3501042425632477\n",
      "[Training Epoch 0] Batch 2626, Loss 0.3486362099647522\n",
      "[Training Epoch 0] Batch 2627, Loss 0.34713035821914673\n",
      "[Training Epoch 0] Batch 2628, Loss 0.32087817788124084\n",
      "[Training Epoch 0] Batch 2629, Loss 0.32579028606414795\n",
      "[Training Epoch 0] Batch 2630, Loss 0.35698843002319336\n",
      "[Training Epoch 0] Batch 2631, Loss 0.309684157371521\n",
      "[Training Epoch 0] Batch 2632, Loss 0.32695725560188293\n",
      "[Training Epoch 0] Batch 2633, Loss 0.31020987033843994\n",
      "[Training Epoch 0] Batch 2634, Loss 0.33985763788223267\n",
      "[Training Epoch 0] Batch 2635, Loss 0.33776092529296875\n",
      "[Training Epoch 0] Batch 2636, Loss 0.3143403232097626\n",
      "[Training Epoch 0] Batch 2637, Loss 0.36574140191078186\n",
      "[Training Epoch 0] Batch 2638, Loss 0.3610701262950897\n",
      "[Training Epoch 0] Batch 2639, Loss 0.3490632474422455\n",
      "[Training Epoch 0] Batch 2640, Loss 0.342640221118927\n",
      "[Training Epoch 0] Batch 2641, Loss 0.3668036460876465\n",
      "[Training Epoch 0] Batch 2642, Loss 0.3253211975097656\n",
      "[Training Epoch 0] Batch 2643, Loss 0.33717450499534607\n",
      "[Training Epoch 0] Batch 2644, Loss 0.3182934820652008\n",
      "[Training Epoch 0] Batch 2645, Loss 0.3187020421028137\n",
      "[Training Epoch 0] Batch 2646, Loss 0.3139074444770813\n",
      "[Training Epoch 0] Batch 2647, Loss 0.352575421333313\n",
      "[Training Epoch 0] Batch 2648, Loss 0.35180431604385376\n",
      "[Training Epoch 0] Batch 2649, Loss 0.31884533166885376\n",
      "[Training Epoch 0] Batch 2650, Loss 0.3356528878211975\n",
      "[Training Epoch 0] Batch 2651, Loss 0.33025485277175903\n",
      "[Training Epoch 0] Batch 2652, Loss 0.33224064111709595\n",
      "[Training Epoch 0] Batch 2653, Loss 0.3162684142589569\n",
      "[Training Epoch 0] Batch 2654, Loss 0.34889209270477295\n",
      "[Training Epoch 0] Batch 2655, Loss 0.31206220388412476\n",
      "[Training Epoch 0] Batch 2656, Loss 0.32374870777130127\n",
      "[Training Epoch 0] Batch 2657, Loss 0.3329501152038574\n",
      "[Training Epoch 0] Batch 2658, Loss 0.3637324571609497\n",
      "[Training Epoch 0] Batch 2659, Loss 0.36487117409706116\n",
      "[Training Epoch 0] Batch 2660, Loss 0.3097912669181824\n",
      "[Training Epoch 0] Batch 2661, Loss 0.303189754486084\n",
      "[Training Epoch 0] Batch 2662, Loss 0.33788394927978516\n",
      "[Training Epoch 0] Batch 2663, Loss 0.3193661868572235\n",
      "[Training Epoch 0] Batch 2664, Loss 0.35489994287490845\n",
      "[Training Epoch 0] Batch 2665, Loss 0.3435441851615906\n",
      "[Training Epoch 0] Batch 2666, Loss 0.35105717182159424\n",
      "[Training Epoch 0] Batch 2667, Loss 0.3302409052848816\n",
      "[Training Epoch 0] Batch 2668, Loss 0.3371945023536682\n",
      "[Training Epoch 0] Batch 2669, Loss 0.3462885618209839\n",
      "[Training Epoch 0] Batch 2670, Loss 0.342512309551239\n",
      "[Training Epoch 0] Batch 2671, Loss 0.31184327602386475\n",
      "[Training Epoch 0] Batch 2672, Loss 0.33297133445739746\n",
      "[Training Epoch 0] Batch 2673, Loss 0.334018349647522\n",
      "[Training Epoch 0] Batch 2674, Loss 0.3705977201461792\n",
      "[Training Epoch 0] Batch 2675, Loss 0.3294283151626587\n",
      "[Training Epoch 0] Batch 2676, Loss 0.32033273577690125\n",
      "[Training Epoch 0] Batch 2677, Loss 0.31597256660461426\n",
      "[Training Epoch 0] Batch 2678, Loss 0.3486138880252838\n",
      "[Training Epoch 0] Batch 2679, Loss 0.3423326313495636\n",
      "[Training Epoch 0] Batch 2680, Loss 0.3410817086696625\n",
      "[Training Epoch 0] Batch 2681, Loss 0.33269059658050537\n",
      "[Training Epoch 0] Batch 2682, Loss 0.3441306948661804\n",
      "[Training Epoch 0] Batch 2683, Loss 0.34046560525894165\n",
      "[Training Epoch 0] Batch 2684, Loss 0.3089703619480133\n",
      "[Training Epoch 0] Batch 2685, Loss 0.32578980922698975\n",
      "[Training Epoch 0] Batch 2686, Loss 0.31113094091415405\n",
      "[Training Epoch 0] Batch 2687, Loss 0.34275877475738525\n",
      "[Training Epoch 0] Batch 2688, Loss 0.3741491436958313\n",
      "[Training Epoch 0] Batch 2689, Loss 0.32506439089775085\n",
      "[Training Epoch 0] Batch 2690, Loss 0.3218945562839508\n",
      "[Training Epoch 0] Batch 2691, Loss 0.34419482946395874\n",
      "[Training Epoch 0] Batch 2692, Loss 0.32610341906547546\n",
      "[Training Epoch 0] Batch 2693, Loss 0.31409814953804016\n",
      "[Training Epoch 0] Batch 2694, Loss 0.32076919078826904\n",
      "[Training Epoch 0] Batch 2695, Loss 0.3109128177165985\n",
      "[Training Epoch 0] Batch 2696, Loss 0.32773303985595703\n",
      "[Training Epoch 0] Batch 2697, Loss 0.3350299596786499\n",
      "[Training Epoch 0] Batch 2698, Loss 0.35751593112945557\n",
      "[Training Epoch 0] Batch 2699, Loss 0.3481278717517853\n",
      "[Training Epoch 0] Batch 2700, Loss 0.3157525360584259\n",
      "[Training Epoch 0] Batch 2701, Loss 0.3009706139564514\n",
      "[Training Epoch 0] Batch 2702, Loss 0.31695112586021423\n",
      "[Training Epoch 0] Batch 2703, Loss 0.34111905097961426\n",
      "[Training Epoch 0] Batch 2704, Loss 0.30835968255996704\n",
      "[Training Epoch 0] Batch 2705, Loss 0.3290412127971649\n",
      "[Training Epoch 0] Batch 2706, Loss 0.32827338576316833\n",
      "[Training Epoch 0] Batch 2707, Loss 0.32724595069885254\n",
      "[Training Epoch 0] Batch 2708, Loss 0.3148687183856964\n",
      "[Training Epoch 0] Batch 2709, Loss 0.3156821131706238\n",
      "[Training Epoch 0] Batch 2710, Loss 0.3470097780227661\n",
      "[Training Epoch 0] Batch 2711, Loss 0.3173164129257202\n",
      "[Training Epoch 0] Batch 2712, Loss 0.32964587211608887\n",
      "[Training Epoch 0] Batch 2713, Loss 0.3517453670501709\n",
      "[Training Epoch 0] Batch 2714, Loss 0.33940067887306213\n",
      "[Training Epoch 0] Batch 2715, Loss 0.3272606134414673\n",
      "[Training Epoch 0] Batch 2716, Loss 0.3363327980041504\n",
      "[Training Epoch 0] Batch 2717, Loss 0.3162936866283417\n",
      "[Training Epoch 0] Batch 2718, Loss 0.32094645500183105\n",
      "[Training Epoch 0] Batch 2719, Loss 0.31101369857788086\n",
      "[Training Epoch 0] Batch 2720, Loss 0.32067546248435974\n",
      "[Training Epoch 0] Batch 2721, Loss 0.331377774477005\n",
      "[Training Epoch 0] Batch 2722, Loss 0.35466769337654114\n",
      "[Training Epoch 0] Batch 2723, Loss 0.3312479853630066\n",
      "[Training Epoch 0] Batch 2724, Loss 0.3272066116333008\n",
      "[Training Epoch 0] Batch 2725, Loss 0.3284284770488739\n",
      "[Training Epoch 0] Batch 2726, Loss 0.3320789635181427\n",
      "[Training Epoch 0] Batch 2727, Loss 0.31594914197921753\n",
      "[Training Epoch 0] Batch 2728, Loss 0.34724855422973633\n",
      "[Training Epoch 0] Batch 2729, Loss 0.31595003604888916\n",
      "[Training Epoch 0] Batch 2730, Loss 0.307935506105423\n",
      "[Training Epoch 0] Batch 2731, Loss 0.3345794677734375\n",
      "[Training Epoch 0] Batch 2732, Loss 0.3509453535079956\n",
      "[Training Epoch 0] Batch 2733, Loss 0.3551657199859619\n",
      "[Training Epoch 0] Batch 2734, Loss 0.3294071555137634\n",
      "[Training Epoch 0] Batch 2735, Loss 0.3407394289970398\n",
      "[Training Epoch 0] Batch 2736, Loss 0.33899563550949097\n",
      "[Training Epoch 0] Batch 2737, Loss 0.32820355892181396\n",
      "[Training Epoch 0] Batch 2738, Loss 0.3383563756942749\n",
      "[Training Epoch 0] Batch 2739, Loss 0.33110928535461426\n",
      "[Training Epoch 0] Batch 2740, Loss 0.31978169083595276\n",
      "[Training Epoch 0] Batch 2741, Loss 0.3254014849662781\n",
      "[Training Epoch 0] Batch 2742, Loss 0.33634334802627563\n",
      "[Training Epoch 0] Batch 2743, Loss 0.331417441368103\n",
      "[Training Epoch 0] Batch 2744, Loss 0.3597857654094696\n",
      "[Training Epoch 0] Batch 2745, Loss 0.35390716791152954\n",
      "[Training Epoch 0] Batch 2746, Loss 0.3329927325248718\n",
      "[Training Epoch 0] Batch 2747, Loss 0.32037630677223206\n",
      "[Training Epoch 0] Batch 2748, Loss 0.31793904304504395\n",
      "[Training Epoch 0] Batch 2749, Loss 0.32576531171798706\n",
      "[Training Epoch 0] Batch 2750, Loss 0.3680766820907593\n",
      "[Training Epoch 0] Batch 2751, Loss 0.31884804368019104\n",
      "[Training Epoch 0] Batch 2752, Loss 0.3542219400405884\n",
      "[Training Epoch 0] Batch 2753, Loss 0.3341579735279083\n",
      "[Training Epoch 0] Batch 2754, Loss 0.32081741094589233\n",
      "[Training Epoch 0] Batch 2755, Loss 0.31263574957847595\n",
      "[Training Epoch 0] Batch 2756, Loss 0.3373279571533203\n",
      "[Training Epoch 0] Batch 2757, Loss 0.313726544380188\n",
      "[Training Epoch 0] Batch 2758, Loss 0.3290368616580963\n",
      "[Training Epoch 0] Batch 2759, Loss 0.33021900057792664\n",
      "[Training Epoch 0] Batch 2760, Loss 0.3173366189002991\n",
      "[Training Epoch 0] Batch 2761, Loss 0.3253273665904999\n",
      "[Training Epoch 0] Batch 2762, Loss 0.34585729241371155\n",
      "[Training Epoch 0] Batch 2763, Loss 0.3465835154056549\n",
      "[Training Epoch 0] Batch 2764, Loss 0.3212175965309143\n",
      "[Training Epoch 0] Batch 2765, Loss 0.3402552008628845\n",
      "[Training Epoch 0] Batch 2766, Loss 0.29238319396972656\n",
      "[Training Epoch 0] Batch 2767, Loss 0.3241388499736786\n",
      "[Training Epoch 0] Batch 2768, Loss 0.30598926544189453\n",
      "[Training Epoch 0] Batch 2769, Loss 0.35364067554473877\n",
      "[Training Epoch 0] Batch 2770, Loss 0.3274246156215668\n",
      "[Training Epoch 0] Batch 2771, Loss 0.3048878014087677\n",
      "[Training Epoch 0] Batch 2772, Loss 0.30360859632492065\n",
      "[Training Epoch 0] Batch 2773, Loss 0.3110978603363037\n",
      "[Training Epoch 0] Batch 2774, Loss 0.3417232632637024\n",
      "[Training Epoch 0] Batch 2775, Loss 0.3682177662849426\n",
      "[Training Epoch 0] Batch 2776, Loss 0.3162935972213745\n",
      "[Training Epoch 0] Batch 2777, Loss 0.34300366044044495\n",
      "[Training Epoch 0] Batch 2778, Loss 0.32252809405326843\n",
      "[Training Epoch 0] Batch 2779, Loss 0.370493620634079\n",
      "[Training Epoch 0] Batch 2780, Loss 0.32774484157562256\n",
      "[Training Epoch 0] Batch 2781, Loss 0.29722192883491516\n",
      "[Training Epoch 0] Batch 2782, Loss 0.33744674921035767\n",
      "[Training Epoch 0] Batch 2783, Loss 0.3152787387371063\n",
      "[Training Epoch 0] Batch 2784, Loss 0.3461381196975708\n",
      "[Training Epoch 0] Batch 2785, Loss 0.32847797870635986\n",
      "[Training Epoch 0] Batch 2786, Loss 0.3384598195552826\n",
      "[Training Epoch 0] Batch 2787, Loss 0.345742791891098\n",
      "[Training Epoch 0] Batch 2788, Loss 0.350766658782959\n",
      "[Training Epoch 0] Batch 2789, Loss 0.34905940294265747\n",
      "[Training Epoch 0] Batch 2790, Loss 0.36341390013694763\n",
      "[Training Epoch 0] Batch 2791, Loss 0.3148641288280487\n",
      "[Training Epoch 0] Batch 2792, Loss 0.3186585307121277\n",
      "[Training Epoch 0] Batch 2793, Loss 0.3083244562149048\n",
      "[Training Epoch 0] Batch 2794, Loss 0.33960461616516113\n",
      "[Training Epoch 0] Batch 2795, Loss 0.343239963054657\n",
      "[Training Epoch 0] Batch 2796, Loss 0.37060096859931946\n",
      "[Training Epoch 0] Batch 2797, Loss 0.31016358733177185\n",
      "[Training Epoch 0] Batch 2798, Loss 0.33026841282844543\n",
      "[Training Epoch 0] Batch 2799, Loss 0.31553804874420166\n",
      "[Training Epoch 0] Batch 2800, Loss 0.3280099630355835\n",
      "[Training Epoch 0] Batch 2801, Loss 0.3264591097831726\n",
      "[Training Epoch 0] Batch 2802, Loss 0.3235623240470886\n",
      "[Training Epoch 0] Batch 2803, Loss 0.3028736412525177\n",
      "[Training Epoch 0] Batch 2804, Loss 0.3263002932071686\n",
      "[Training Epoch 0] Batch 2805, Loss 0.3503243923187256\n",
      "[Training Epoch 0] Batch 2806, Loss 0.35951149463653564\n",
      "[Training Epoch 0] Batch 2807, Loss 0.34640222787857056\n",
      "[Training Epoch 0] Batch 2808, Loss 0.32915860414505005\n",
      "[Training Epoch 0] Batch 2809, Loss 0.3394313454627991\n",
      "[Training Epoch 0] Batch 2810, Loss 0.325955331325531\n",
      "[Training Epoch 0] Batch 2811, Loss 0.32279515266418457\n",
      "[Training Epoch 0] Batch 2812, Loss 0.345409095287323\n",
      "[Training Epoch 0] Batch 2813, Loss 0.33697783946990967\n",
      "[Training Epoch 0] Batch 2814, Loss 0.3133887052536011\n",
      "[Training Epoch 0] Batch 2815, Loss 0.3425554037094116\n",
      "[Training Epoch 0] Batch 2816, Loss 0.31306442618370056\n",
      "[Training Epoch 0] Batch 2817, Loss 0.3510429859161377\n",
      "[Training Epoch 0] Batch 2818, Loss 0.31976959109306335\n",
      "[Training Epoch 0] Batch 2819, Loss 0.34473899006843567\n",
      "[Training Epoch 0] Batch 2820, Loss 0.3428462743759155\n",
      "[Training Epoch 0] Batch 2821, Loss 0.3502460718154907\n",
      "[Training Epoch 0] Batch 2822, Loss 0.3226012587547302\n",
      "[Training Epoch 0] Batch 2823, Loss 0.32089436054229736\n",
      "[Training Epoch 0] Batch 2824, Loss 0.3492858409881592\n",
      "[Training Epoch 0] Batch 2825, Loss 0.34186840057373047\n",
      "[Training Epoch 0] Batch 2826, Loss 0.3195427656173706\n",
      "[Training Epoch 0] Batch 2827, Loss 0.32714635133743286\n",
      "[Training Epoch 0] Batch 2828, Loss 0.3228364586830139\n",
      "[Training Epoch 0] Batch 2829, Loss 0.33982351422309875\n",
      "[Training Epoch 0] Batch 2830, Loss 0.3322206735610962\n",
      "[Training Epoch 0] Batch 2831, Loss 0.32944777607917786\n",
      "[Training Epoch 0] Batch 2832, Loss 0.3226514160633087\n",
      "[Training Epoch 0] Batch 2833, Loss 0.34529009461402893\n",
      "[Training Epoch 0] Batch 2834, Loss 0.3163191080093384\n",
      "[Training Epoch 0] Batch 2835, Loss 0.35891956090927124\n",
      "[Training Epoch 0] Batch 2836, Loss 0.32516568899154663\n",
      "[Training Epoch 0] Batch 2837, Loss 0.3510206937789917\n",
      "[Training Epoch 0] Batch 2838, Loss 0.33765506744384766\n",
      "[Training Epoch 0] Batch 2839, Loss 0.3279682993888855\n",
      "[Training Epoch 0] Batch 2840, Loss 0.3268894851207733\n",
      "[Training Epoch 0] Batch 2841, Loss 0.30960625410079956\n",
      "[Training Epoch 0] Batch 2842, Loss 0.3437649607658386\n",
      "[Training Epoch 0] Batch 2843, Loss 0.3111877739429474\n",
      "[Training Epoch 0] Batch 2844, Loss 0.3353792130947113\n",
      "[Training Epoch 0] Batch 2845, Loss 0.33868542313575745\n",
      "[Training Epoch 0] Batch 2846, Loss 0.3265237808227539\n",
      "[Training Epoch 0] Batch 2847, Loss 0.33423739671707153\n",
      "[Training Epoch 0] Batch 2848, Loss 0.3340417146682739\n",
      "[Training Epoch 0] Batch 2849, Loss 0.331318199634552\n",
      "[Training Epoch 0] Batch 2850, Loss 0.3399774134159088\n",
      "[Training Epoch 0] Batch 2851, Loss 0.3170975148677826\n",
      "[Training Epoch 0] Batch 2852, Loss 0.34092819690704346\n",
      "[Training Epoch 0] Batch 2853, Loss 0.32515692710876465\n",
      "[Training Epoch 0] Batch 2854, Loss 0.3152226209640503\n",
      "[Training Epoch 0] Batch 2855, Loss 0.32277894020080566\n",
      "[Training Epoch 0] Batch 2856, Loss 0.3144491910934448\n",
      "[Training Epoch 0] Batch 2857, Loss 0.30426836013793945\n",
      "[Training Epoch 0] Batch 2858, Loss 0.3531133532524109\n",
      "[Training Epoch 0] Batch 2859, Loss 0.3137918710708618\n",
      "[Training Epoch 0] Batch 2860, Loss 0.332770437002182\n",
      "[Training Epoch 0] Batch 2861, Loss 0.3548191487789154\n",
      "[Training Epoch 0] Batch 2862, Loss 0.3317795991897583\n",
      "[Training Epoch 0] Batch 2863, Loss 0.3161444365978241\n",
      "[Training Epoch 0] Batch 2864, Loss 0.29900190234184265\n",
      "[Training Epoch 0] Batch 2865, Loss 0.30662405490875244\n",
      "[Training Epoch 0] Batch 2866, Loss 0.31368696689605713\n",
      "[Training Epoch 0] Batch 2867, Loss 0.3265056014060974\n",
      "[Training Epoch 0] Batch 2868, Loss 0.30967801809310913\n",
      "[Training Epoch 0] Batch 2869, Loss 0.3127029538154602\n",
      "[Training Epoch 0] Batch 2870, Loss 0.3253616690635681\n",
      "[Training Epoch 0] Batch 2871, Loss 0.3455948829650879\n",
      "[Training Epoch 0] Batch 2872, Loss 0.31798309087753296\n",
      "[Training Epoch 0] Batch 2873, Loss 0.33115333318710327\n",
      "[Training Epoch 0] Batch 2874, Loss 0.32611939311027527\n",
      "[Training Epoch 0] Batch 2875, Loss 0.33100372552871704\n",
      "[Training Epoch 0] Batch 2876, Loss 0.3135139048099518\n",
      "[Training Epoch 0] Batch 2877, Loss 0.32196277379989624\n",
      "[Training Epoch 0] Batch 2878, Loss 0.3170536756515503\n",
      "[Training Epoch 0] Batch 2879, Loss 0.3156784772872925\n",
      "[Training Epoch 0] Batch 2880, Loss 0.3122750222682953\n",
      "[Training Epoch 0] Batch 2881, Loss 0.3443605303764343\n",
      "[Training Epoch 0] Batch 2882, Loss 0.3685263395309448\n",
      "[Training Epoch 0] Batch 2883, Loss 0.3657127022743225\n",
      "[Training Epoch 0] Batch 2884, Loss 0.3207535743713379\n",
      "[Training Epoch 0] Batch 2885, Loss 0.3448740243911743\n",
      "[Training Epoch 0] Batch 2886, Loss 0.3176027536392212\n",
      "[Training Epoch 0] Batch 2887, Loss 0.3306616544723511\n",
      "[Training Epoch 0] Batch 2888, Loss 0.2990054488182068\n",
      "[Training Epoch 0] Batch 2889, Loss 0.34680354595184326\n",
      "[Training Epoch 0] Batch 2890, Loss 0.30863410234451294\n",
      "[Training Epoch 0] Batch 2891, Loss 0.3371580243110657\n",
      "[Training Epoch 0] Batch 2892, Loss 0.3711346685886383\n",
      "[Training Epoch 0] Batch 2893, Loss 0.32422587275505066\n",
      "[Training Epoch 0] Batch 2894, Loss 0.3096427023410797\n",
      "[Training Epoch 0] Batch 2895, Loss 0.32548582553863525\n",
      "[Training Epoch 0] Batch 2896, Loss 0.3365115821361542\n",
      "[Training Epoch 0] Batch 2897, Loss 0.34059393405914307\n",
      "[Training Epoch 0] Batch 2898, Loss 0.33206549286842346\n",
      "[Training Epoch 0] Batch 2899, Loss 0.3546091914176941\n",
      "[Training Epoch 0] Batch 2900, Loss 0.34027642011642456\n",
      "[Training Epoch 0] Batch 2901, Loss 0.333528995513916\n",
      "[Training Epoch 0] Batch 2902, Loss 0.3167577385902405\n",
      "[Training Epoch 0] Batch 2903, Loss 0.34264272451400757\n",
      "[Training Epoch 0] Batch 2904, Loss 0.3580830693244934\n",
      "[Training Epoch 0] Batch 2905, Loss 0.33963102102279663\n",
      "[Training Epoch 0] Batch 2906, Loss 0.3119361102581024\n",
      "[Training Epoch 0] Batch 2907, Loss 0.3244597315788269\n",
      "[Training Epoch 0] Batch 2908, Loss 0.32524409890174866\n",
      "[Training Epoch 0] Batch 2909, Loss 0.3280982971191406\n",
      "[Training Epoch 0] Batch 2910, Loss 0.34607499837875366\n",
      "[Training Epoch 0] Batch 2911, Loss 0.3406965732574463\n",
      "[Training Epoch 0] Batch 2912, Loss 0.33563682436943054\n",
      "[Training Epoch 0] Batch 2913, Loss 0.30741822719573975\n",
      "[Training Epoch 0] Batch 2914, Loss 0.3046634793281555\n",
      "[Training Epoch 0] Batch 2915, Loss 0.31695127487182617\n",
      "[Training Epoch 0] Batch 2916, Loss 0.33121681213378906\n",
      "[Training Epoch 0] Batch 2917, Loss 0.36712735891342163\n",
      "[Training Epoch 0] Batch 2918, Loss 0.32511186599731445\n",
      "[Training Epoch 0] Batch 2919, Loss 0.29186832904815674\n",
      "[Training Epoch 0] Batch 2920, Loss 0.3184548020362854\n",
      "[Training Epoch 0] Batch 2921, Loss 0.3321398198604584\n",
      "[Training Epoch 0] Batch 2922, Loss 0.30953580141067505\n",
      "[Training Epoch 0] Batch 2923, Loss 0.32080549001693726\n",
      "[Training Epoch 0] Batch 2924, Loss 0.30706286430358887\n",
      "[Training Epoch 0] Batch 2925, Loss 0.3314744234085083\n",
      "[Training Epoch 0] Batch 2926, Loss 0.32469719648361206\n",
      "[Training Epoch 0] Batch 2927, Loss 0.3373038172721863\n",
      "[Training Epoch 0] Batch 2928, Loss 0.3257256746292114\n",
      "[Training Epoch 0] Batch 2929, Loss 0.33820676803588867\n",
      "[Training Epoch 0] Batch 2930, Loss 0.32225802540779114\n",
      "[Training Epoch 0] Batch 2931, Loss 0.33863458037376404\n",
      "[Training Epoch 0] Batch 2932, Loss 0.3259585499763489\n",
      "[Training Epoch 0] Batch 2933, Loss 0.32272660732269287\n",
      "[Training Epoch 0] Batch 2934, Loss 0.34270256757736206\n",
      "[Training Epoch 0] Batch 2935, Loss 0.3351686894893646\n",
      "[Training Epoch 0] Batch 2936, Loss 0.3118295967578888\n",
      "[Training Epoch 0] Batch 2937, Loss 0.3481907248497009\n",
      "[Training Epoch 0] Batch 2938, Loss 0.30933961272239685\n",
      "[Training Epoch 0] Batch 2939, Loss 0.3180503845214844\n",
      "[Training Epoch 0] Batch 2940, Loss 0.32912561297416687\n",
      "[Training Epoch 0] Batch 2941, Loss 0.31315159797668457\n",
      "[Training Epoch 0] Batch 2942, Loss 0.32515084743499756\n",
      "[Training Epoch 0] Batch 2943, Loss 0.3440855145454407\n",
      "[Training Epoch 0] Batch 2944, Loss 0.3413640260696411\n",
      "[Training Epoch 0] Batch 2945, Loss 0.344241201877594\n",
      "[Training Epoch 0] Batch 2946, Loss 0.33886241912841797\n",
      "[Training Epoch 0] Batch 2947, Loss 0.3362897038459778\n",
      "[Training Epoch 0] Batch 2948, Loss 0.3476678729057312\n",
      "[Training Epoch 0] Batch 2949, Loss 0.33668744564056396\n",
      "[Training Epoch 0] Batch 2950, Loss 0.3493373990058899\n",
      "[Training Epoch 0] Batch 2951, Loss 0.33182770013809204\n",
      "[Training Epoch 0] Batch 2952, Loss 0.35520219802856445\n",
      "[Training Epoch 0] Batch 2953, Loss 0.35343828797340393\n",
      "[Training Epoch 0] Batch 2954, Loss 0.35110607743263245\n",
      "[Training Epoch 0] Batch 2955, Loss 0.33580321073532104\n",
      "[Training Epoch 0] Batch 2956, Loss 0.35341230034828186\n",
      "[Training Epoch 0] Batch 2957, Loss 0.3274284303188324\n",
      "[Training Epoch 0] Batch 2958, Loss 0.3175726532936096\n",
      "[Training Epoch 0] Batch 2959, Loss 0.322182297706604\n",
      "[Training Epoch 0] Batch 2960, Loss 0.30034857988357544\n",
      "[Training Epoch 0] Batch 2961, Loss 0.32439500093460083\n",
      "[Training Epoch 0] Batch 2962, Loss 0.3127996325492859\n",
      "[Training Epoch 0] Batch 2963, Loss 0.3408331871032715\n",
      "[Training Epoch 0] Batch 2964, Loss 0.29855871200561523\n",
      "[Training Epoch 0] Batch 2965, Loss 0.3189089298248291\n",
      "[Training Epoch 0] Batch 2966, Loss 0.3169970214366913\n",
      "[Training Epoch 0] Batch 2967, Loss 0.32586994767189026\n",
      "[Training Epoch 0] Batch 2968, Loss 0.31367892026901245\n",
      "[Training Epoch 0] Batch 2969, Loss 0.3303701877593994\n",
      "[Training Epoch 0] Batch 2970, Loss 0.3342694044113159\n",
      "[Training Epoch 0] Batch 2971, Loss 0.3408696949481964\n",
      "[Training Epoch 0] Batch 2972, Loss 0.3370267450809479\n",
      "[Training Epoch 0] Batch 2973, Loss 0.32026800513267517\n",
      "[Training Epoch 0] Batch 2974, Loss 0.33778995275497437\n",
      "[Training Epoch 0] Batch 2975, Loss 0.32025542855262756\n",
      "[Training Epoch 0] Batch 2976, Loss 0.30619126558303833\n",
      "[Training Epoch 0] Batch 2977, Loss 0.3144215941429138\n",
      "[Training Epoch 0] Batch 2978, Loss 0.35452863574028015\n",
      "[Training Epoch 0] Batch 2979, Loss 0.3412022590637207\n",
      "[Training Epoch 0] Batch 2980, Loss 0.31567099690437317\n",
      "[Training Epoch 0] Batch 2981, Loss 0.31829190254211426\n",
      "[Training Epoch 0] Batch 2982, Loss 0.28451281785964966\n",
      "[Training Epoch 0] Batch 2983, Loss 0.310452401638031\n",
      "[Training Epoch 0] Batch 2984, Loss 0.3101574182510376\n",
      "[Training Epoch 0] Batch 2985, Loss 0.3183823227882385\n",
      "[Training Epoch 0] Batch 2986, Loss 0.29836809635162354\n",
      "[Training Epoch 0] Batch 2987, Loss 0.339036226272583\n",
      "[Training Epoch 0] Batch 2988, Loss 0.3387719690799713\n",
      "[Training Epoch 0] Batch 2989, Loss 0.31402498483657837\n",
      "[Training Epoch 0] Batch 2990, Loss 0.33502745628356934\n",
      "[Training Epoch 0] Batch 2991, Loss 0.32564401626586914\n",
      "[Training Epoch 0] Batch 2992, Loss 0.32199496030807495\n",
      "[Training Epoch 0] Batch 2993, Loss 0.3292457163333893\n",
      "[Training Epoch 0] Batch 2994, Loss 0.31947603821754456\n",
      "[Training Epoch 0] Batch 2995, Loss 0.3240830898284912\n",
      "[Training Epoch 0] Batch 2996, Loss 0.35593438148498535\n",
      "[Training Epoch 0] Batch 2997, Loss 0.3455931544303894\n",
      "[Training Epoch 0] Batch 2998, Loss 0.3388184905052185\n",
      "[Training Epoch 0] Batch 2999, Loss 0.31875914335250854\n",
      "[Training Epoch 0] Batch 3000, Loss 0.3429957628250122\n",
      "[Training Epoch 0] Batch 3001, Loss 0.3089200556278229\n",
      "[Training Epoch 0] Batch 3002, Loss 0.3205985426902771\n",
      "[Training Epoch 0] Batch 3003, Loss 0.31627094745635986\n",
      "[Training Epoch 0] Batch 3004, Loss 0.29668062925338745\n",
      "[Training Epoch 0] Batch 3005, Loss 0.3448230028152466\n",
      "[Training Epoch 0] Batch 3006, Loss 0.3162994980812073\n",
      "[Training Epoch 0] Batch 3007, Loss 0.33406391739845276\n",
      "[Training Epoch 0] Batch 3008, Loss 0.3263288140296936\n",
      "[Training Epoch 0] Batch 3009, Loss 0.3065110445022583\n",
      "[Training Epoch 0] Batch 3010, Loss 0.2987075448036194\n",
      "[Training Epoch 0] Batch 3011, Loss 0.32539620995521545\n",
      "[Training Epoch 0] Batch 3012, Loss 0.33282148838043213\n",
      "[Training Epoch 0] Batch 3013, Loss 0.3164263665676117\n",
      "[Training Epoch 0] Batch 3014, Loss 0.36522176861763\n",
      "[Training Epoch 0] Batch 3015, Loss 0.3232698142528534\n",
      "[Training Epoch 0] Batch 3016, Loss 0.3592936396598816\n",
      "[Training Epoch 0] Batch 3017, Loss 0.3579135239124298\n",
      "[Training Epoch 0] Batch 3018, Loss 0.3205834627151489\n",
      "[Training Epoch 0] Batch 3019, Loss 0.3148113489151001\n",
      "[Training Epoch 0] Batch 3020, Loss 0.31152409315109253\n",
      "[Training Epoch 0] Batch 3021, Loss 0.31650030612945557\n",
      "[Training Epoch 0] Batch 3022, Loss 0.35436856746673584\n",
      "[Training Epoch 0] Batch 3023, Loss 0.3238357901573181\n",
      "[Training Epoch 0] Batch 3024, Loss 0.30031242966651917\n",
      "[Training Epoch 0] Batch 3025, Loss 0.3228275179862976\n",
      "[Training Epoch 0] Batch 3026, Loss 0.3413568139076233\n",
      "[Training Epoch 0] Batch 3027, Loss 0.3295755088329315\n",
      "[Training Epoch 0] Batch 3028, Loss 0.3432775139808655\n",
      "[Training Epoch 0] Batch 3029, Loss 0.34183210134506226\n",
      "[Training Epoch 0] Batch 3030, Loss 0.3384608030319214\n",
      "[Training Epoch 0] Batch 3031, Loss 0.3088628649711609\n",
      "[Training Epoch 0] Batch 3032, Loss 0.3199036121368408\n",
      "[Training Epoch 0] Batch 3033, Loss 0.33923646807670593\n",
      "[Training Epoch 0] Batch 3034, Loss 0.3454308807849884\n",
      "[Training Epoch 0] Batch 3035, Loss 0.3319063186645508\n",
      "[Training Epoch 0] Batch 3036, Loss 0.29881027340888977\n",
      "[Training Epoch 0] Batch 3037, Loss 0.33687475323677063\n",
      "[Training Epoch 0] Batch 3038, Loss 0.3408236503601074\n",
      "[Training Epoch 0] Batch 3039, Loss 0.3562890291213989\n",
      "[Training Epoch 0] Batch 3040, Loss 0.3091317415237427\n",
      "[Training Epoch 0] Batch 3041, Loss 0.32594090700149536\n",
      "[Training Epoch 0] Batch 3042, Loss 0.292955219745636\n",
      "[Training Epoch 0] Batch 3043, Loss 0.3563080430030823\n",
      "[Training Epoch 0] Batch 3044, Loss 0.32689645886421204\n",
      "[Training Epoch 0] Batch 3045, Loss 0.32684722542762756\n",
      "[Training Epoch 0] Batch 3046, Loss 0.3072359561920166\n",
      "[Training Epoch 0] Batch 3047, Loss 0.3379676342010498\n",
      "[Training Epoch 0] Batch 3048, Loss 0.3427770733833313\n",
      "[Training Epoch 0] Batch 3049, Loss 0.27978843450546265\n",
      "[Training Epoch 0] Batch 3050, Loss 0.3177199363708496\n",
      "[Training Epoch 0] Batch 3051, Loss 0.3262327313423157\n",
      "[Training Epoch 0] Batch 3052, Loss 0.31232744455337524\n",
      "[Training Epoch 0] Batch 3053, Loss 0.30141115188598633\n",
      "[Training Epoch 0] Batch 3054, Loss 0.3425343334674835\n",
      "[Training Epoch 0] Batch 3055, Loss 0.32310062646865845\n",
      "[Training Epoch 0] Batch 3056, Loss 0.3692449927330017\n",
      "[Training Epoch 0] Batch 3057, Loss 0.3583378493785858\n",
      "[Training Epoch 0] Batch 3058, Loss 0.32598280906677246\n",
      "[Training Epoch 0] Batch 3059, Loss 0.3388574719429016\n",
      "[Training Epoch 0] Batch 3060, Loss 0.34265556931495667\n",
      "[Training Epoch 0] Batch 3061, Loss 0.31346797943115234\n",
      "[Training Epoch 0] Batch 3062, Loss 0.3321155905723572\n",
      "[Training Epoch 0] Batch 3063, Loss 0.3351304829120636\n",
      "[Training Epoch 0] Batch 3064, Loss 0.343744158744812\n",
      "[Training Epoch 0] Batch 3065, Loss 0.3462614417076111\n",
      "[Training Epoch 0] Batch 3066, Loss 0.30663618445396423\n",
      "[Training Epoch 0] Batch 3067, Loss 0.3198496103286743\n",
      "[Training Epoch 0] Batch 3068, Loss 0.35008668899536133\n",
      "[Training Epoch 0] Batch 3069, Loss 0.3379106819629669\n",
      "[Training Epoch 0] Batch 3070, Loss 0.3384268581867218\n",
      "[Training Epoch 0] Batch 3071, Loss 0.3376414477825165\n",
      "[Training Epoch 0] Batch 3072, Loss 0.31689441204071045\n",
      "[Training Epoch 0] Batch 3073, Loss 0.3187749683856964\n",
      "[Training Epoch 0] Batch 3074, Loss 0.3214801549911499\n",
      "[Training Epoch 0] Batch 3075, Loss 0.34067678451538086\n",
      "[Training Epoch 0] Batch 3076, Loss 0.33032363653182983\n",
      "[Training Epoch 0] Batch 3077, Loss 0.33686715364456177\n",
      "[Training Epoch 0] Batch 3078, Loss 0.32556962966918945\n",
      "[Training Epoch 0] Batch 3079, Loss 0.3127060830593109\n",
      "[Training Epoch 0] Batch 3080, Loss 0.3411089777946472\n",
      "[Training Epoch 0] Batch 3081, Loss 0.342072457075119\n",
      "[Training Epoch 0] Batch 3082, Loss 0.3570195734500885\n",
      "[Training Epoch 0] Batch 3083, Loss 0.33404505252838135\n",
      "[Training Epoch 0] Batch 3084, Loss 0.32782599329948425\n",
      "[Training Epoch 0] Batch 3085, Loss 0.3107348084449768\n",
      "[Training Epoch 0] Batch 3086, Loss 0.3442005515098572\n",
      "[Training Epoch 0] Batch 3087, Loss 0.30841872096061707\n",
      "[Training Epoch 0] Batch 3088, Loss 0.3678790032863617\n",
      "[Training Epoch 0] Batch 3089, Loss 0.32963672280311584\n",
      "[Training Epoch 0] Batch 3090, Loss 0.3470252454280853\n",
      "[Training Epoch 0] Batch 3091, Loss 0.31123197078704834\n",
      "[Training Epoch 0] Batch 3092, Loss 0.32848840951919556\n",
      "[Training Epoch 0] Batch 3093, Loss 0.3334515392780304\n",
      "[Training Epoch 0] Batch 3094, Loss 0.2977573871612549\n",
      "[Training Epoch 0] Batch 3095, Loss 0.30719810724258423\n",
      "[Training Epoch 0] Batch 3096, Loss 0.33683156967163086\n",
      "[Training Epoch 0] Batch 3097, Loss 0.31424081325531006\n",
      "[Training Epoch 0] Batch 3098, Loss 0.32260075211524963\n",
      "[Training Epoch 0] Batch 3099, Loss 0.32504802942276\n",
      "[Training Epoch 0] Batch 3100, Loss 0.3344240188598633\n",
      "[Training Epoch 0] Batch 3101, Loss 0.32404178380966187\n",
      "[Training Epoch 0] Batch 3102, Loss 0.36110419034957886\n",
      "[Training Epoch 0] Batch 3103, Loss 0.3267071843147278\n",
      "[Training Epoch 0] Batch 3104, Loss 0.30630365014076233\n",
      "[Training Epoch 0] Batch 3105, Loss 0.32522377371788025\n",
      "[Training Epoch 0] Batch 3106, Loss 0.3366367220878601\n",
      "[Training Epoch 0] Batch 3107, Loss 0.3389771580696106\n",
      "[Training Epoch 0] Batch 3108, Loss 0.3229403793811798\n",
      "[Training Epoch 0] Batch 3109, Loss 0.2898021936416626\n",
      "[Training Epoch 0] Batch 3110, Loss 0.3483796715736389\n",
      "[Training Epoch 0] Batch 3111, Loss 0.3379886746406555\n",
      "[Training Epoch 0] Batch 3112, Loss 0.35379600524902344\n",
      "[Training Epoch 0] Batch 3113, Loss 0.35239842534065247\n",
      "[Training Epoch 0] Batch 3114, Loss 0.323544979095459\n",
      "[Training Epoch 0] Batch 3115, Loss 0.32590365409851074\n",
      "[Training Epoch 0] Batch 3116, Loss 0.31483274698257446\n",
      "[Training Epoch 0] Batch 3117, Loss 0.3568345904350281\n",
      "[Training Epoch 0] Batch 3118, Loss 0.31198737025260925\n",
      "[Training Epoch 0] Batch 3119, Loss 0.3177090287208557\n",
      "[Training Epoch 0] Batch 3120, Loss 0.32234612107276917\n",
      "[Training Epoch 0] Batch 3121, Loss 0.32708507776260376\n",
      "[Training Epoch 0] Batch 3122, Loss 0.3304230570793152\n",
      "[Training Epoch 0] Batch 3123, Loss 0.31053975224494934\n",
      "[Training Epoch 0] Batch 3124, Loss 0.3423534631729126\n",
      "[Training Epoch 0] Batch 3125, Loss 0.3402408957481384\n",
      "[Training Epoch 0] Batch 3126, Loss 0.35421234369277954\n",
      "[Training Epoch 0] Batch 3127, Loss 0.33565762639045715\n",
      "[Training Epoch 0] Batch 3128, Loss 0.3121486306190491\n",
      "[Training Epoch 0] Batch 3129, Loss 0.3436168432235718\n",
      "[Training Epoch 0] Batch 3130, Loss 0.32401108741760254\n",
      "[Training Epoch 0] Batch 3131, Loss 0.31834620237350464\n",
      "[Training Epoch 0] Batch 3132, Loss 0.32816773653030396\n",
      "[Training Epoch 0] Batch 3133, Loss 0.3283042907714844\n",
      "[Training Epoch 0] Batch 3134, Loss 0.322794109582901\n",
      "[Training Epoch 0] Batch 3135, Loss 0.33171001076698303\n",
      "[Training Epoch 0] Batch 3136, Loss 0.29263031482696533\n",
      "[Training Epoch 0] Batch 3137, Loss 0.33063387870788574\n",
      "[Training Epoch 0] Batch 3138, Loss 0.3395901620388031\n",
      "[Training Epoch 0] Batch 3139, Loss 0.3275841772556305\n",
      "[Training Epoch 0] Batch 3140, Loss 0.3199032247066498\n",
      "[Training Epoch 0] Batch 3141, Loss 0.30712175369262695\n",
      "[Training Epoch 0] Batch 3142, Loss 0.3131284713745117\n",
      "[Training Epoch 0] Batch 3143, Loss 0.3119007647037506\n",
      "[Training Epoch 0] Batch 3144, Loss 0.32719892263412476\n",
      "[Training Epoch 0] Batch 3145, Loss 0.3159330487251282\n",
      "[Training Epoch 0] Batch 3146, Loss 0.33653318881988525\n",
      "[Training Epoch 0] Batch 3147, Loss 0.3065378963947296\n",
      "[Training Epoch 0] Batch 3148, Loss 0.3157771825790405\n",
      "[Training Epoch 0] Batch 3149, Loss 0.3502541184425354\n",
      "[Training Epoch 0] Batch 3150, Loss 0.3008185625076294\n",
      "[Training Epoch 0] Batch 3151, Loss 0.3158402442932129\n",
      "[Training Epoch 0] Batch 3152, Loss 0.32701820135116577\n",
      "[Training Epoch 0] Batch 3153, Loss 0.3264485001564026\n",
      "[Training Epoch 0] Batch 3154, Loss 0.3335714042186737\n",
      "[Training Epoch 0] Batch 3155, Loss 0.32366591691970825\n",
      "[Training Epoch 0] Batch 3156, Loss 0.34992295503616333\n",
      "[Training Epoch 0] Batch 3157, Loss 0.34870007634162903\n",
      "[Training Epoch 0] Batch 3158, Loss 0.288370281457901\n",
      "[Training Epoch 0] Batch 3159, Loss 0.33027592301368713\n",
      "[Training Epoch 0] Batch 3160, Loss 0.35097596049308777\n",
      "[Training Epoch 0] Batch 3161, Loss 0.32315731048583984\n",
      "[Training Epoch 0] Batch 3162, Loss 0.3315349817276001\n",
      "[Training Epoch 0] Batch 3163, Loss 0.34464237093925476\n",
      "[Training Epoch 0] Batch 3164, Loss 0.3370417356491089\n",
      "[Training Epoch 0] Batch 3165, Loss 0.31097471714019775\n",
      "[Training Epoch 0] Batch 3166, Loss 0.3222186267375946\n",
      "[Training Epoch 0] Batch 3167, Loss 0.3439165949821472\n",
      "[Training Epoch 0] Batch 3168, Loss 0.3494478166103363\n",
      "[Training Epoch 0] Batch 3169, Loss 0.3218579590320587\n",
      "[Training Epoch 0] Batch 3170, Loss 0.28352007269859314\n",
      "[Training Epoch 0] Batch 3171, Loss 0.3130381107330322\n",
      "[Training Epoch 0] Batch 3172, Loss 0.29401057958602905\n",
      "[Training Epoch 0] Batch 3173, Loss 0.31224969029426575\n",
      "[Training Epoch 0] Batch 3174, Loss 0.3082464933395386\n",
      "[Training Epoch 0] Batch 3175, Loss 0.31297793984413147\n",
      "[Training Epoch 0] Batch 3176, Loss 0.33190733194351196\n",
      "[Training Epoch 0] Batch 3177, Loss 0.30883073806762695\n",
      "[Training Epoch 0] Batch 3178, Loss 0.3216182589530945\n",
      "[Training Epoch 0] Batch 3179, Loss 0.3560525178909302\n",
      "[Training Epoch 0] Batch 3180, Loss 0.35680168867111206\n",
      "[Training Epoch 0] Batch 3181, Loss 0.29710835218429565\n",
      "[Training Epoch 0] Batch 3182, Loss 0.34791213274002075\n",
      "[Training Epoch 0] Batch 3183, Loss 0.31824976205825806\n",
      "[Training Epoch 0] Batch 3184, Loss 0.3379054665565491\n",
      "[Training Epoch 0] Batch 3185, Loss 0.32861584424972534\n",
      "[Training Epoch 0] Batch 3186, Loss 0.31690797209739685\n",
      "[Training Epoch 0] Batch 3187, Loss 0.31095337867736816\n",
      "[Training Epoch 0] Batch 3188, Loss 0.3625006079673767\n",
      "[Training Epoch 0] Batch 3189, Loss 0.3011152446269989\n",
      "[Training Epoch 0] Batch 3190, Loss 0.337653785943985\n",
      "[Training Epoch 0] Batch 3191, Loss 0.29692962765693665\n",
      "[Training Epoch 0] Batch 3192, Loss 0.3026784062385559\n",
      "[Training Epoch 0] Batch 3193, Loss 0.31673330068588257\n",
      "[Training Epoch 0] Batch 3194, Loss 0.32632434368133545\n",
      "[Training Epoch 0] Batch 3195, Loss 0.34086233377456665\n",
      "[Training Epoch 0] Batch 3196, Loss 0.35471951961517334\n",
      "[Training Epoch 0] Batch 3197, Loss 0.31965047121047974\n",
      "[Training Epoch 0] Batch 3198, Loss 0.33554649353027344\n",
      "[Training Epoch 0] Batch 3199, Loss 0.3205586075782776\n",
      "[Training Epoch 0] Batch 3200, Loss 0.33476442098617554\n",
      "[Training Epoch 0] Batch 3201, Loss 0.29595887660980225\n",
      "[Training Epoch 0] Batch 3202, Loss 0.3163309097290039\n",
      "[Training Epoch 0] Batch 3203, Loss 0.30855339765548706\n",
      "[Training Epoch 0] Batch 3204, Loss 0.3333565592765808\n",
      "[Training Epoch 0] Batch 3205, Loss 0.3181552290916443\n",
      "[Training Epoch 0] Batch 3206, Loss 0.31860262155532837\n",
      "[Training Epoch 0] Batch 3207, Loss 0.3186666965484619\n",
      "[Training Epoch 0] Batch 3208, Loss 0.3197450041770935\n",
      "[Training Epoch 0] Batch 3209, Loss 0.3322101831436157\n",
      "[Training Epoch 0] Batch 3210, Loss 0.33969685435295105\n",
      "[Training Epoch 0] Batch 3211, Loss 0.3456895649433136\n",
      "[Training Epoch 0] Batch 3212, Loss 0.35630422830581665\n",
      "[Training Epoch 0] Batch 3213, Loss 0.28952503204345703\n",
      "[Training Epoch 0] Batch 3214, Loss 0.32922661304473877\n",
      "[Training Epoch 0] Batch 3215, Loss 0.3444700241088867\n",
      "[Training Epoch 0] Batch 3216, Loss 0.30071574449539185\n",
      "[Training Epoch 0] Batch 3217, Loss 0.3274909555912018\n",
      "[Training Epoch 0] Batch 3218, Loss 0.305473268032074\n",
      "[Training Epoch 0] Batch 3219, Loss 0.3103998303413391\n",
      "[Training Epoch 0] Batch 3220, Loss 0.34197676181793213\n",
      "[Training Epoch 0] Batch 3221, Loss 0.3415988087654114\n",
      "[Training Epoch 0] Batch 3222, Loss 0.29457342624664307\n",
      "[Training Epoch 0] Batch 3223, Loss 0.3128102421760559\n",
      "[Training Epoch 0] Batch 3224, Loss 0.3216918110847473\n",
      "[Training Epoch 0] Batch 3225, Loss 0.3415237069129944\n",
      "[Training Epoch 0] Batch 3226, Loss 0.341805100440979\n",
      "[Training Epoch 0] Batch 3227, Loss 0.3121740221977234\n",
      "[Training Epoch 0] Batch 3228, Loss 0.32469481229782104\n",
      "[Training Epoch 0] Batch 3229, Loss 0.3327719569206238\n",
      "[Training Epoch 0] Batch 3230, Loss 0.31117093563079834\n",
      "[Training Epoch 0] Batch 3231, Loss 0.32165002822875977\n",
      "[Training Epoch 0] Batch 3232, Loss 0.3512892425060272\n",
      "[Training Epoch 0] Batch 3233, Loss 0.33250170946121216\n",
      "[Training Epoch 0] Batch 3234, Loss 0.3117252588272095\n",
      "[Training Epoch 0] Batch 3235, Loss 0.3171204924583435\n",
      "[Training Epoch 0] Batch 3236, Loss 0.3407992124557495\n",
      "[Training Epoch 0] Batch 3237, Loss 0.32101204991340637\n",
      "[Training Epoch 0] Batch 3238, Loss 0.33744823932647705\n",
      "[Training Epoch 0] Batch 3239, Loss 0.29351285099983215\n",
      "[Training Epoch 0] Batch 3240, Loss 0.3358689546585083\n",
      "[Training Epoch 0] Batch 3241, Loss 0.3414444029331207\n",
      "[Training Epoch 0] Batch 3242, Loss 0.3406277894973755\n",
      "[Training Epoch 0] Batch 3243, Loss 0.3223995566368103\n",
      "[Training Epoch 0] Batch 3244, Loss 0.32135552167892456\n",
      "[Training Epoch 0] Batch 3245, Loss 0.35043424367904663\n",
      "[Training Epoch 0] Batch 3246, Loss 0.3573355972766876\n",
      "[Training Epoch 0] Batch 3247, Loss 0.34504663944244385\n",
      "[Training Epoch 0] Batch 3248, Loss 0.32393375039100647\n",
      "[Training Epoch 0] Batch 3249, Loss 0.35074761509895325\n",
      "[Training Epoch 0] Batch 3250, Loss 0.3037286400794983\n",
      "[Training Epoch 0] Batch 3251, Loss 0.3345947861671448\n",
      "[Training Epoch 0] Batch 3252, Loss 0.3507961928844452\n",
      "[Training Epoch 0] Batch 3253, Loss 0.3239259123802185\n",
      "[Training Epoch 0] Batch 3254, Loss 0.323769211769104\n",
      "[Training Epoch 0] Batch 3255, Loss 0.32539939880371094\n",
      "[Training Epoch 0] Batch 3256, Loss 0.3377639353275299\n",
      "[Training Epoch 0] Batch 3257, Loss 0.31348147988319397\n",
      "[Training Epoch 0] Batch 3258, Loss 0.3204039931297302\n",
      "[Training Epoch 0] Batch 3259, Loss 0.3273571729660034\n",
      "[Training Epoch 0] Batch 3260, Loss 0.3048897087574005\n",
      "[Training Epoch 0] Batch 3261, Loss 0.3219914436340332\n",
      "[Training Epoch 0] Batch 3262, Loss 0.32291650772094727\n",
      "[Training Epoch 0] Batch 3263, Loss 0.3300428092479706\n",
      "[Training Epoch 0] Batch 3264, Loss 0.3392501473426819\n",
      "[Training Epoch 0] Batch 3265, Loss 0.3147013783454895\n",
      "[Training Epoch 0] Batch 3266, Loss 0.3514488637447357\n",
      "[Training Epoch 0] Batch 3267, Loss 0.3054274320602417\n",
      "[Training Epoch 0] Batch 3268, Loss 0.2911646366119385\n",
      "[Training Epoch 0] Batch 3269, Loss 0.3027189373970032\n",
      "[Training Epoch 0] Batch 3270, Loss 0.3313372731208801\n",
      "[Training Epoch 0] Batch 3271, Loss 0.3038090467453003\n",
      "[Training Epoch 0] Batch 3272, Loss 0.3226240277290344\n",
      "[Training Epoch 0] Batch 3273, Loss 0.3198539614677429\n",
      "[Training Epoch 0] Batch 3274, Loss 0.33636781573295593\n",
      "[Training Epoch 0] Batch 3275, Loss 0.33343106508255005\n",
      "[Training Epoch 0] Batch 3276, Loss 0.347603976726532\n",
      "[Training Epoch 0] Batch 3277, Loss 0.3186338245868683\n",
      "[Training Epoch 0] Batch 3278, Loss 0.3450120687484741\n",
      "[Training Epoch 0] Batch 3279, Loss 0.3395320475101471\n",
      "[Training Epoch 0] Batch 3280, Loss 0.3278271555900574\n",
      "[Training Epoch 0] Batch 3281, Loss 0.3167177736759186\n",
      "[Training Epoch 0] Batch 3282, Loss 0.33043354749679565\n",
      "[Training Epoch 0] Batch 3283, Loss 0.30866533517837524\n",
      "[Training Epoch 0] Batch 3284, Loss 0.3417815566062927\n",
      "[Training Epoch 0] Batch 3285, Loss 0.3533332347869873\n",
      "[Training Epoch 0] Batch 3286, Loss 0.33815625309944153\n",
      "[Training Epoch 0] Batch 3287, Loss 0.35772454738616943\n",
      "[Training Epoch 0] Batch 3288, Loss 0.32285067439079285\n",
      "[Training Epoch 0] Batch 3289, Loss 0.31323590874671936\n",
      "[Training Epoch 0] Batch 3290, Loss 0.34726595878601074\n",
      "[Training Epoch 0] Batch 3291, Loss 0.3038294017314911\n",
      "[Training Epoch 0] Batch 3292, Loss 0.3048109710216522\n",
      "[Training Epoch 0] Batch 3293, Loss 0.3507637083530426\n",
      "[Training Epoch 0] Batch 3294, Loss 0.3372706472873688\n",
      "[Training Epoch 0] Batch 3295, Loss 0.3340657949447632\n",
      "[Training Epoch 0] Batch 3296, Loss 0.315513014793396\n",
      "[Training Epoch 0] Batch 3297, Loss 0.3350968360900879\n",
      "[Training Epoch 0] Batch 3298, Loss 0.3156318664550781\n",
      "[Training Epoch 0] Batch 3299, Loss 0.33442845940589905\n",
      "[Training Epoch 0] Batch 3300, Loss 0.3620961308479309\n",
      "[Training Epoch 0] Batch 3301, Loss 0.340756893157959\n",
      "[Training Epoch 0] Batch 3302, Loss 0.3188950717449188\n",
      "[Training Epoch 0] Batch 3303, Loss 0.3492215573787689\n",
      "[Training Epoch 0] Batch 3304, Loss 0.31559303402900696\n",
      "[Training Epoch 0] Batch 3305, Loss 0.3138291835784912\n",
      "[Training Epoch 0] Batch 3306, Loss 0.333194375038147\n",
      "[Training Epoch 0] Batch 3307, Loss 0.32434821128845215\n",
      "[Training Epoch 0] Batch 3308, Loss 0.3631168603897095\n",
      "[Training Epoch 0] Batch 3309, Loss 0.31994980573654175\n",
      "[Training Epoch 0] Batch 3310, Loss 0.3397928476333618\n",
      "[Training Epoch 0] Batch 3311, Loss 0.31383082270622253\n",
      "[Training Epoch 0] Batch 3312, Loss 0.2991420030593872\n",
      "[Training Epoch 0] Batch 3313, Loss 0.34282585978507996\n",
      "[Training Epoch 0] Batch 3314, Loss 0.3101468086242676\n",
      "[Training Epoch 0] Batch 3315, Loss 0.3279472589492798\n",
      "[Training Epoch 0] Batch 3316, Loss 0.31717777252197266\n",
      "[Training Epoch 0] Batch 3317, Loss 0.3053942620754242\n",
      "[Training Epoch 0] Batch 3318, Loss 0.3164200186729431\n",
      "[Training Epoch 0] Batch 3319, Loss 0.31577956676483154\n",
      "[Training Epoch 0] Batch 3320, Loss 0.32573461532592773\n",
      "[Training Epoch 0] Batch 3321, Loss 0.34689977765083313\n",
      "[Training Epoch 0] Batch 3322, Loss 0.3135663866996765\n",
      "[Training Epoch 0] Batch 3323, Loss 0.2935240864753723\n",
      "[Training Epoch 0] Batch 3324, Loss 0.31684234738349915\n",
      "[Training Epoch 0] Batch 3325, Loss 0.3232188820838928\n",
      "[Training Epoch 0] Batch 3326, Loss 0.3201892375946045\n",
      "[Training Epoch 0] Batch 3327, Loss 0.3235865831375122\n",
      "[Training Epoch 0] Batch 3328, Loss 0.34887179732322693\n",
      "[Training Epoch 0] Batch 3329, Loss 0.335183709859848\n",
      "[Training Epoch 0] Batch 3330, Loss 0.3061935007572174\n",
      "[Training Epoch 0] Batch 3331, Loss 0.31174829602241516\n",
      "[Training Epoch 0] Batch 3332, Loss 0.33294928073883057\n",
      "[Training Epoch 0] Batch 3333, Loss 0.3177306652069092\n",
      "[Training Epoch 0] Batch 3334, Loss 0.33725476264953613\n",
      "[Training Epoch 0] Batch 3335, Loss 0.33506831526756287\n",
      "[Training Epoch 0] Batch 3336, Loss 0.31628966331481934\n",
      "[Training Epoch 0] Batch 3337, Loss 0.3357134759426117\n",
      "[Training Epoch 0] Batch 3338, Loss 0.30456840991973877\n",
      "[Training Epoch 0] Batch 3339, Loss 0.30564212799072266\n",
      "[Training Epoch 0] Batch 3340, Loss 0.36183321475982666\n",
      "[Training Epoch 0] Batch 3341, Loss 0.3200315237045288\n",
      "[Training Epoch 0] Batch 3342, Loss 0.3179784417152405\n",
      "[Training Epoch 0] Batch 3343, Loss 0.3329218924045563\n",
      "[Training Epoch 0] Batch 3344, Loss 0.292533814907074\n",
      "[Training Epoch 0] Batch 3345, Loss 0.3167575001716614\n",
      "[Training Epoch 0] Batch 3346, Loss 0.3140466809272766\n",
      "[Training Epoch 0] Batch 3347, Loss 0.30460280179977417\n",
      "[Training Epoch 0] Batch 3348, Loss 0.33982616662979126\n",
      "[Training Epoch 0] Batch 3349, Loss 0.3239642381668091\n",
      "[Training Epoch 0] Batch 3350, Loss 0.3353673219680786\n",
      "[Training Epoch 0] Batch 3351, Loss 0.29949280619621277\n",
      "[Training Epoch 0] Batch 3352, Loss 0.3304736614227295\n",
      "[Training Epoch 0] Batch 3353, Loss 0.30634891986846924\n",
      "[Training Epoch 0] Batch 3354, Loss 0.34592580795288086\n",
      "[Training Epoch 0] Batch 3355, Loss 0.3550844192504883\n",
      "[Training Epoch 0] Batch 3356, Loss 0.31279847025871277\n",
      "[Training Epoch 0] Batch 3357, Loss 0.3157980740070343\n",
      "[Training Epoch 0] Batch 3358, Loss 0.3396086096763611\n",
      "[Training Epoch 0] Batch 3359, Loss 0.3200252950191498\n",
      "[Training Epoch 0] Batch 3360, Loss 0.31116700172424316\n",
      "[Training Epoch 0] Batch 3361, Loss 0.30872488021850586\n",
      "[Training Epoch 0] Batch 3362, Loss 0.3146846294403076\n",
      "[Training Epoch 0] Batch 3363, Loss 0.3621973395347595\n",
      "[Training Epoch 0] Batch 3364, Loss 0.29848650097846985\n",
      "[Training Epoch 0] Batch 3365, Loss 0.3194887638092041\n",
      "[Training Epoch 0] Batch 3366, Loss 0.3393716812133789\n",
      "[Training Epoch 0] Batch 3367, Loss 0.3203483521938324\n",
      "[Training Epoch 0] Batch 3368, Loss 0.3042953610420227\n",
      "[Training Epoch 0] Batch 3369, Loss 0.3113832473754883\n",
      "[Training Epoch 0] Batch 3370, Loss 0.3208344578742981\n",
      "[Training Epoch 0] Batch 3371, Loss 0.326484352350235\n",
      "[Training Epoch 0] Batch 3372, Loss 0.309099018573761\n",
      "[Training Epoch 0] Batch 3373, Loss 0.29310399293899536\n",
      "[Training Epoch 0] Batch 3374, Loss 0.31457406282424927\n",
      "[Training Epoch 0] Batch 3375, Loss 0.3395751714706421\n",
      "[Training Epoch 0] Batch 3376, Loss 0.3250357508659363\n",
      "[Training Epoch 0] Batch 3377, Loss 0.29476577043533325\n",
      "[Training Epoch 0] Batch 3378, Loss 0.3296440839767456\n",
      "[Training Epoch 0] Batch 3379, Loss 0.31494462490081787\n",
      "[Training Epoch 0] Batch 3380, Loss 0.32584723830223083\n",
      "[Training Epoch 0] Batch 3381, Loss 0.3221791386604309\n",
      "[Training Epoch 0] Batch 3382, Loss 0.3213516175746918\n",
      "[Training Epoch 0] Batch 3383, Loss 0.3404209613800049\n",
      "[Training Epoch 0] Batch 3384, Loss 0.334694504737854\n",
      "[Training Epoch 0] Batch 3385, Loss 0.31511372327804565\n",
      "[Training Epoch 0] Batch 3386, Loss 0.3093857169151306\n",
      "[Training Epoch 0] Batch 3387, Loss 0.3406144976615906\n",
      "[Training Epoch 0] Batch 3388, Loss 0.3252703845500946\n",
      "[Training Epoch 0] Batch 3389, Loss 0.30302757024765015\n",
      "[Training Epoch 0] Batch 3390, Loss 0.315623015165329\n",
      "[Training Epoch 0] Batch 3391, Loss 0.3346983790397644\n",
      "[Training Epoch 0] Batch 3392, Loss 0.33578944206237793\n",
      "[Training Epoch 0] Batch 3393, Loss 0.31807541847229004\n",
      "[Training Epoch 0] Batch 3394, Loss 0.31089460849761963\n",
      "[Training Epoch 0] Batch 3395, Loss 0.34358370304107666\n",
      "[Training Epoch 0] Batch 3396, Loss 0.31672483682632446\n",
      "[Training Epoch 0] Batch 3397, Loss 0.3057377338409424\n",
      "[Training Epoch 0] Batch 3398, Loss 0.32876765727996826\n",
      "[Training Epoch 0] Batch 3399, Loss 0.31719470024108887\n",
      "[Training Epoch 0] Batch 3400, Loss 0.3228200376033783\n",
      "[Training Epoch 0] Batch 3401, Loss 0.3076934218406677\n",
      "[Training Epoch 0] Batch 3402, Loss 0.3287990093231201\n",
      "[Training Epoch 0] Batch 3403, Loss 0.31359565258026123\n",
      "[Training Epoch 0] Batch 3404, Loss 0.3372635245323181\n",
      "[Training Epoch 0] Batch 3405, Loss 0.35710835456848145\n",
      "[Training Epoch 0] Batch 3406, Loss 0.3420363664627075\n",
      "[Training Epoch 0] Batch 3407, Loss 0.31407231092453003\n",
      "[Training Epoch 0] Batch 3408, Loss 0.2947235107421875\n",
      "[Training Epoch 0] Batch 3409, Loss 0.31668969988822937\n",
      "[Training Epoch 0] Batch 3410, Loss 0.3473860025405884\n",
      "[Training Epoch 0] Batch 3411, Loss 0.33055049180984497\n",
      "[Training Epoch 0] Batch 3412, Loss 0.32566773891448975\n",
      "[Training Epoch 0] Batch 3413, Loss 0.33525973558425903\n",
      "[Training Epoch 0] Batch 3414, Loss 0.3294081687927246\n",
      "[Training Epoch 0] Batch 3415, Loss 0.3380821645259857\n",
      "[Training Epoch 0] Batch 3416, Loss 0.32007157802581787\n",
      "[Training Epoch 0] Batch 3417, Loss 0.32072335481643677\n",
      "[Training Epoch 0] Batch 3418, Loss 0.3438355028629303\n",
      "[Training Epoch 0] Batch 3419, Loss 0.37192776799201965\n",
      "[Training Epoch 0] Batch 3420, Loss 0.3044764995574951\n",
      "[Training Epoch 0] Batch 3421, Loss 0.2967386543750763\n",
      "[Training Epoch 0] Batch 3422, Loss 0.3361475467681885\n",
      "[Training Epoch 0] Batch 3423, Loss 0.3240167200565338\n",
      "[Training Epoch 0] Batch 3424, Loss 0.31970709562301636\n",
      "[Training Epoch 0] Batch 3425, Loss 0.3049039840698242\n",
      "[Training Epoch 0] Batch 3426, Loss 0.34073150157928467\n",
      "[Training Epoch 0] Batch 3427, Loss 0.3086405098438263\n",
      "[Training Epoch 0] Batch 3428, Loss 0.3065769672393799\n",
      "[Training Epoch 0] Batch 3429, Loss 0.3267512917518616\n",
      "[Training Epoch 0] Batch 3430, Loss 0.32713544368743896\n",
      "[Training Epoch 0] Batch 3431, Loss 0.35545283555984497\n",
      "[Training Epoch 0] Batch 3432, Loss 0.30722320079803467\n",
      "[Training Epoch 0] Batch 3433, Loss 0.33212536573410034\n",
      "[Training Epoch 0] Batch 3434, Loss 0.33868351578712463\n",
      "[Training Epoch 0] Batch 3435, Loss 0.33905109763145447\n",
      "[Training Epoch 0] Batch 3436, Loss 0.3364420533180237\n",
      "[Training Epoch 0] Batch 3437, Loss 0.3393588066101074\n",
      "[Training Epoch 0] Batch 3438, Loss 0.33812007308006287\n",
      "[Training Epoch 0] Batch 3439, Loss 0.3260428309440613\n",
      "[Training Epoch 0] Batch 3440, Loss 0.2978641390800476\n",
      "[Training Epoch 0] Batch 3441, Loss 0.33032676577568054\n",
      "[Training Epoch 0] Batch 3442, Loss 0.33796465396881104\n",
      "[Training Epoch 0] Batch 3443, Loss 0.311784565448761\n",
      "[Training Epoch 0] Batch 3444, Loss 0.34001827239990234\n",
      "[Training Epoch 0] Batch 3445, Loss 0.33051079511642456\n",
      "[Training Epoch 0] Batch 3446, Loss 0.35375380516052246\n",
      "[Training Epoch 0] Batch 3447, Loss 0.3174053430557251\n",
      "[Training Epoch 0] Batch 3448, Loss 0.3183286786079407\n",
      "[Training Epoch 0] Batch 3449, Loss 0.32859936356544495\n",
      "[Training Epoch 0] Batch 3450, Loss 0.34593918919563293\n",
      "[Training Epoch 0] Batch 3451, Loss 0.3277137875556946\n",
      "[Training Epoch 0] Batch 3452, Loss 0.3029549717903137\n",
      "[Training Epoch 0] Batch 3453, Loss 0.314963698387146\n",
      "[Training Epoch 0] Batch 3454, Loss 0.2954120337963104\n",
      "[Training Epoch 0] Batch 3455, Loss 0.31584107875823975\n",
      "[Training Epoch 0] Batch 3456, Loss 0.34744060039520264\n",
      "[Training Epoch 0] Batch 3457, Loss 0.33281761407852173\n",
      "[Training Epoch 0] Batch 3458, Loss 0.31184595823287964\n",
      "[Training Epoch 0] Batch 3459, Loss 0.3340340852737427\n",
      "[Training Epoch 0] Batch 3460, Loss 0.32801389694213867\n",
      "[Training Epoch 0] Batch 3461, Loss 0.314028799533844\n",
      "[Training Epoch 0] Batch 3462, Loss 0.323103129863739\n",
      "[Training Epoch 0] Batch 3463, Loss 0.3194634020328522\n",
      "[Training Epoch 0] Batch 3464, Loss 0.3459862172603607\n",
      "[Training Epoch 0] Batch 3465, Loss 0.3110332190990448\n",
      "[Training Epoch 0] Batch 3466, Loss 0.30344077944755554\n",
      "[Training Epoch 0] Batch 3467, Loss 0.3277224898338318\n",
      "[Training Epoch 0] Batch 3468, Loss 0.3138502836227417\n",
      "[Training Epoch 0] Batch 3469, Loss 0.30275213718414307\n",
      "[Training Epoch 0] Batch 3470, Loss 0.32352492213249207\n",
      "[Training Epoch 0] Batch 3471, Loss 0.3157418966293335\n",
      "[Training Epoch 0] Batch 3472, Loss 0.33431196212768555\n",
      "[Training Epoch 0] Batch 3473, Loss 0.31832486391067505\n",
      "[Training Epoch 0] Batch 3474, Loss 0.35678598284721375\n",
      "[Training Epoch 0] Batch 3475, Loss 0.319391667842865\n",
      "[Training Epoch 0] Batch 3476, Loss 0.307496577501297\n",
      "[Training Epoch 0] Batch 3477, Loss 0.2968919575214386\n",
      "[Training Epoch 0] Batch 3478, Loss 0.2904972732067108\n",
      "[Training Epoch 0] Batch 3479, Loss 0.33082035183906555\n",
      "[Training Epoch 0] Batch 3480, Loss 0.3195751905441284\n",
      "[Training Epoch 0] Batch 3481, Loss 0.33851659297943115\n",
      "[Training Epoch 0] Batch 3482, Loss 0.34514743089675903\n",
      "[Training Epoch 0] Batch 3483, Loss 0.31211405992507935\n",
      "[Training Epoch 0] Batch 3484, Loss 0.3128715753555298\n",
      "[Training Epoch 0] Batch 3485, Loss 0.31221428513526917\n",
      "[Training Epoch 0] Batch 3486, Loss 0.3314666748046875\n",
      "[Training Epoch 0] Batch 3487, Loss 0.32757532596588135\n",
      "[Training Epoch 0] Batch 3488, Loss 0.3253445029258728\n",
      "[Training Epoch 0] Batch 3489, Loss 0.33126282691955566\n",
      "[Training Epoch 0] Batch 3490, Loss 0.36284738779067993\n",
      "[Training Epoch 0] Batch 3491, Loss 0.32601869106292725\n",
      "[Training Epoch 0] Batch 3492, Loss 0.30684030055999756\n",
      "[Training Epoch 0] Batch 3493, Loss 0.3319304585456848\n",
      "[Training Epoch 0] Batch 3494, Loss 0.3129943609237671\n",
      "[Training Epoch 0] Batch 3495, Loss 0.33270949125289917\n",
      "[Training Epoch 0] Batch 3496, Loss 0.3170236349105835\n",
      "[Training Epoch 0] Batch 3497, Loss 0.3306603729724884\n",
      "[Training Epoch 0] Batch 3498, Loss 0.32343560457229614\n",
      "[Training Epoch 0] Batch 3499, Loss 0.34142011404037476\n",
      "[Training Epoch 0] Batch 3500, Loss 0.29025211930274963\n",
      "[Training Epoch 0] Batch 3501, Loss 0.3191418945789337\n",
      "[Training Epoch 0] Batch 3502, Loss 0.31780779361724854\n",
      "[Training Epoch 0] Batch 3503, Loss 0.3362233638763428\n",
      "[Training Epoch 0] Batch 3504, Loss 0.3445447087287903\n",
      "[Training Epoch 0] Batch 3505, Loss 0.33337002992630005\n",
      "[Training Epoch 0] Batch 3506, Loss 0.3255128264427185\n",
      "[Training Epoch 0] Batch 3507, Loss 0.33695700764656067\n",
      "[Training Epoch 0] Batch 3508, Loss 0.3282415270805359\n",
      "[Training Epoch 0] Batch 3509, Loss 0.33670440316200256\n",
      "[Training Epoch 0] Batch 3510, Loss 0.31709349155426025\n",
      "[Training Epoch 0] Batch 3511, Loss 0.3293928802013397\n",
      "[Training Epoch 0] Batch 3512, Loss 0.33830785751342773\n",
      "[Training Epoch 0] Batch 3513, Loss 0.3489207625389099\n",
      "[Training Epoch 0] Batch 3514, Loss 0.302005410194397\n",
      "[Training Epoch 0] Batch 3515, Loss 0.35445335507392883\n",
      "[Training Epoch 0] Batch 3516, Loss 0.32412517070770264\n",
      "[Training Epoch 0] Batch 3517, Loss 0.34381359815597534\n",
      "[Training Epoch 0] Batch 3518, Loss 0.3130863308906555\n",
      "[Training Epoch 0] Batch 3519, Loss 0.36317628622055054\n",
      "[Training Epoch 0] Batch 3520, Loss 0.3144373595714569\n",
      "[Training Epoch 0] Batch 3521, Loss 0.3228306174278259\n",
      "[Training Epoch 0] Batch 3522, Loss 0.325658917427063\n",
      "[Training Epoch 0] Batch 3523, Loss 0.3196898400783539\n",
      "[Training Epoch 0] Batch 3524, Loss 0.3491842448711395\n",
      "[Training Epoch 0] Batch 3525, Loss 0.34056001901626587\n",
      "[Training Epoch 0] Batch 3526, Loss 0.3083239793777466\n",
      "[Training Epoch 0] Batch 3527, Loss 0.35891789197921753\n",
      "[Training Epoch 0] Batch 3528, Loss 0.2960798442363739\n",
      "[Training Epoch 0] Batch 3529, Loss 0.34073173999786377\n",
      "[Training Epoch 0] Batch 3530, Loss 0.32511118054389954\n",
      "[Training Epoch 0] Batch 3531, Loss 0.3487561047077179\n",
      "[Training Epoch 0] Batch 3532, Loss 0.3338181674480438\n",
      "[Training Epoch 0] Batch 3533, Loss 0.333002507686615\n",
      "[Training Epoch 0] Batch 3534, Loss 0.3316117227077484\n",
      "[Training Epoch 0] Batch 3535, Loss 0.3255472183227539\n",
      "[Training Epoch 0] Batch 3536, Loss 0.2944072484970093\n",
      "[Training Epoch 0] Batch 3537, Loss 0.2988242506980896\n",
      "[Training Epoch 0] Batch 3538, Loss 0.31641483306884766\n",
      "[Training Epoch 0] Batch 3539, Loss 0.31201744079589844\n",
      "[Training Epoch 0] Batch 3540, Loss 0.34280362725257874\n",
      "[Training Epoch 0] Batch 3541, Loss 0.3656408190727234\n",
      "[Training Epoch 0] Batch 3542, Loss 0.3471965789794922\n",
      "[Training Epoch 0] Batch 3543, Loss 0.31594085693359375\n",
      "[Training Epoch 0] Batch 3544, Loss 0.33022943139076233\n",
      "[Training Epoch 0] Batch 3545, Loss 0.295916885137558\n",
      "[Training Epoch 0] Batch 3546, Loss 0.3274441957473755\n",
      "[Training Epoch 0] Batch 3547, Loss 0.32275083661079407\n",
      "[Training Epoch 0] Batch 3548, Loss 0.3287808299064636\n",
      "[Training Epoch 0] Batch 3549, Loss 0.3373739421367645\n",
      "[Training Epoch 0] Batch 3550, Loss 0.3155096173286438\n",
      "[Training Epoch 0] Batch 3551, Loss 0.29605716466903687\n",
      "[Training Epoch 0] Batch 3552, Loss 0.3468366861343384\n",
      "[Training Epoch 0] Batch 3553, Loss 0.3344041407108307\n",
      "[Training Epoch 0] Batch 3554, Loss 0.3079383969306946\n",
      "[Training Epoch 0] Batch 3555, Loss 0.3288905918598175\n",
      "[Training Epoch 0] Batch 3556, Loss 0.3501707911491394\n",
      "[Training Epoch 0] Batch 3557, Loss 0.34045737981796265\n",
      "[Training Epoch 0] Batch 3558, Loss 0.30577632784843445\n",
      "[Training Epoch 0] Batch 3559, Loss 0.3390555679798126\n",
      "[Training Epoch 0] Batch 3560, Loss 0.3520200550556183\n",
      "[Training Epoch 0] Batch 3561, Loss 0.3238745331764221\n",
      "[Training Epoch 0] Batch 3562, Loss 0.32427120208740234\n",
      "[Training Epoch 0] Batch 3563, Loss 0.33354252576828003\n",
      "[Training Epoch 0] Batch 3564, Loss 0.34702610969543457\n",
      "[Training Epoch 0] Batch 3565, Loss 0.3374394178390503\n",
      "[Training Epoch 0] Batch 3566, Loss 0.3005415201187134\n",
      "[Training Epoch 0] Batch 3567, Loss 0.30119776725769043\n",
      "[Training Epoch 0] Batch 3568, Loss 0.3173007369041443\n",
      "[Training Epoch 0] Batch 3569, Loss 0.3244938254356384\n",
      "[Training Epoch 0] Batch 3570, Loss 0.3117866516113281\n",
      "[Training Epoch 0] Batch 3571, Loss 0.32570457458496094\n",
      "[Training Epoch 0] Batch 3572, Loss 0.3000238537788391\n",
      "[Training Epoch 0] Batch 3573, Loss 0.3008424937725067\n",
      "[Training Epoch 0] Batch 3574, Loss 0.2985777258872986\n",
      "[Training Epoch 0] Batch 3575, Loss 0.32052603363990784\n",
      "[Training Epoch 0] Batch 3576, Loss 0.33479273319244385\n",
      "[Training Epoch 0] Batch 3577, Loss 0.3344842195510864\n",
      "[Training Epoch 0] Batch 3578, Loss 0.31302183866500854\n",
      "[Training Epoch 0] Batch 3579, Loss 0.30549025535583496\n",
      "[Training Epoch 0] Batch 3580, Loss 0.3317594528198242\n",
      "[Training Epoch 0] Batch 3581, Loss 0.2834475636482239\n",
      "[Training Epoch 0] Batch 3582, Loss 0.33873066306114197\n",
      "[Training Epoch 0] Batch 3583, Loss 0.3206249475479126\n",
      "[Training Epoch 0] Batch 3584, Loss 0.3346572816371918\n",
      "[Training Epoch 0] Batch 3585, Loss 0.2936011850833893\n",
      "[Training Epoch 0] Batch 3586, Loss 0.3022932708263397\n",
      "[Training Epoch 0] Batch 3587, Loss 0.3197069764137268\n",
      "[Training Epoch 0] Batch 3588, Loss 0.30216914415359497\n",
      "[Training Epoch 0] Batch 3589, Loss 0.31125980615615845\n",
      "[Training Epoch 0] Batch 3590, Loss 0.3107167184352875\n",
      "[Training Epoch 0] Batch 3591, Loss 0.3203241527080536\n",
      "[Training Epoch 0] Batch 3592, Loss 0.3351562023162842\n",
      "[Training Epoch 0] Batch 3593, Loss 0.3497958779335022\n",
      "[Training Epoch 0] Batch 3594, Loss 0.3126837909221649\n",
      "[Training Epoch 0] Batch 3595, Loss 0.3077371120452881\n",
      "[Training Epoch 0] Batch 3596, Loss 0.32493317127227783\n",
      "[Training Epoch 0] Batch 3597, Loss 0.3628493845462799\n",
      "[Training Epoch 0] Batch 3598, Loss 0.33884871006011963\n",
      "[Training Epoch 0] Batch 3599, Loss 0.3168942928314209\n",
      "[Training Epoch 0] Batch 3600, Loss 0.30149179697036743\n",
      "[Training Epoch 0] Batch 3601, Loss 0.3161287009716034\n",
      "[Training Epoch 0] Batch 3602, Loss 0.3200441598892212\n",
      "[Training Epoch 0] Batch 3603, Loss 0.3533995747566223\n",
      "[Training Epoch 0] Batch 3604, Loss 0.32638460397720337\n",
      "[Training Epoch 0] Batch 3605, Loss 0.3279959559440613\n",
      "[Training Epoch 0] Batch 3606, Loss 0.34975990653038025\n",
      "[Training Epoch 0] Batch 3607, Loss 0.32138288021087646\n",
      "[Training Epoch 0] Batch 3608, Loss 0.30679288506507874\n",
      "[Training Epoch 0] Batch 3609, Loss 0.280864953994751\n",
      "[Training Epoch 0] Batch 3610, Loss 0.32579243183135986\n",
      "[Training Epoch 0] Batch 3611, Loss 0.34136563539505005\n",
      "[Training Epoch 0] Batch 3612, Loss 0.3151194453239441\n",
      "[Training Epoch 0] Batch 3613, Loss 0.3192228674888611\n",
      "[Training Epoch 0] Batch 3614, Loss 0.32039475440979004\n",
      "[Training Epoch 0] Batch 3615, Loss 0.31380078196525574\n",
      "[Training Epoch 0] Batch 3616, Loss 0.3461819291114807\n",
      "[Training Epoch 0] Batch 3617, Loss 0.31824398040771484\n",
      "[Training Epoch 0] Batch 3618, Loss 0.3288654685020447\n",
      "[Training Epoch 0] Batch 3619, Loss 0.35178905725479126\n",
      "[Training Epoch 0] Batch 3620, Loss 0.3268139958381653\n",
      "[Training Epoch 0] Batch 3621, Loss 0.32232466340065\n",
      "[Training Epoch 0] Batch 3622, Loss 0.32531628012657166\n",
      "[Training Epoch 0] Batch 3623, Loss 0.32508018612861633\n",
      "[Training Epoch 0] Batch 3624, Loss 0.3245353102684021\n",
      "[Training Epoch 0] Batch 3625, Loss 0.31154197454452515\n",
      "[Training Epoch 0] Batch 3626, Loss 0.33320677280426025\n",
      "[Training Epoch 0] Batch 3627, Loss 0.3198060393333435\n",
      "[Training Epoch 0] Batch 3628, Loss 0.3305412232875824\n",
      "[Training Epoch 0] Batch 3629, Loss 0.31491097807884216\n",
      "[Training Epoch 0] Batch 3630, Loss 0.3470129370689392\n",
      "[Training Epoch 0] Batch 3631, Loss 0.29643088579177856\n",
      "[Training Epoch 0] Batch 3632, Loss 0.29300981760025024\n",
      "[Training Epoch 0] Batch 3633, Loss 0.3211100697517395\n",
      "[Training Epoch 0] Batch 3634, Loss 0.2947373390197754\n",
      "[Training Epoch 0] Batch 3635, Loss 0.3040879964828491\n",
      "[Training Epoch 0] Batch 3636, Loss 0.3148949146270752\n",
      "[Training Epoch 0] Batch 3637, Loss 0.31802427768707275\n",
      "[Training Epoch 0] Batch 3638, Loss 0.3376893103122711\n",
      "[Training Epoch 0] Batch 3639, Loss 0.3150043487548828\n",
      "[Training Epoch 0] Batch 3640, Loss 0.3275631070137024\n",
      "[Training Epoch 0] Batch 3641, Loss 0.3427383005619049\n",
      "[Training Epoch 0] Batch 3642, Loss 0.3485766351222992\n",
      "[Training Epoch 0] Batch 3643, Loss 0.3439787030220032\n",
      "[Training Epoch 0] Batch 3644, Loss 0.3137452006340027\n",
      "[Training Epoch 0] Batch 3645, Loss 0.3226109743118286\n",
      "[Training Epoch 0] Batch 3646, Loss 0.32709652185440063\n",
      "[Training Epoch 0] Batch 3647, Loss 0.330788791179657\n",
      "[Training Epoch 0] Batch 3648, Loss 0.33678174018859863\n",
      "[Training Epoch 0] Batch 3649, Loss 0.337400883436203\n",
      "[Training Epoch 0] Batch 3650, Loss 0.3306206464767456\n",
      "[Training Epoch 0] Batch 3651, Loss 0.35292983055114746\n",
      "[Training Epoch 0] Batch 3652, Loss 0.32131847739219666\n",
      "[Training Epoch 0] Batch 3653, Loss 0.3231354355812073\n",
      "[Training Epoch 0] Batch 3654, Loss 0.31713372468948364\n",
      "[Training Epoch 0] Batch 3655, Loss 0.35839805006980896\n",
      "[Training Epoch 0] Batch 3656, Loss 0.32066890597343445\n",
      "[Training Epoch 0] Batch 3657, Loss 0.32046017050743103\n",
      "[Training Epoch 0] Batch 3658, Loss 0.3330106735229492\n",
      "[Training Epoch 0] Batch 3659, Loss 0.3152294158935547\n",
      "[Training Epoch 0] Batch 3660, Loss 0.30239927768707275\n",
      "[Training Epoch 0] Batch 3661, Loss 0.3231023848056793\n",
      "[Training Epoch 0] Batch 3662, Loss 0.34330764412879944\n",
      "[Training Epoch 0] Batch 3663, Loss 0.3669734299182892\n",
      "[Training Epoch 0] Batch 3664, Loss 0.32072827219963074\n",
      "[Training Epoch 0] Batch 3665, Loss 0.3547798991203308\n",
      "[Training Epoch 0] Batch 3666, Loss 0.33239296078681946\n",
      "[Training Epoch 0] Batch 3667, Loss 0.3598591089248657\n",
      "[Training Epoch 0] Batch 3668, Loss 0.3439798057079315\n",
      "[Training Epoch 0] Batch 3669, Loss 0.32133108377456665\n",
      "[Training Epoch 0] Batch 3670, Loss 0.3144790530204773\n",
      "[Training Epoch 0] Batch 3671, Loss 0.32571709156036377\n",
      "[Training Epoch 0] Batch 3672, Loss 0.3044188320636749\n",
      "[Training Epoch 0] Batch 3673, Loss 0.320427268743515\n",
      "[Training Epoch 0] Batch 3674, Loss 0.32390475273132324\n",
      "[Training Epoch 0] Batch 3675, Loss 0.3283398151397705\n",
      "[Training Epoch 0] Batch 3676, Loss 0.311732679605484\n",
      "[Training Epoch 0] Batch 3677, Loss 0.34070685505867004\n",
      "[Training Epoch 0] Batch 3678, Loss 0.34323206543922424\n",
      "[Training Epoch 0] Batch 3679, Loss 0.3324630558490753\n",
      "[Training Epoch 0] Batch 3680, Loss 0.3392871022224426\n",
      "[Training Epoch 0] Batch 3681, Loss 0.3412485718727112\n",
      "[Training Epoch 0] Batch 3682, Loss 0.32599306106567383\n",
      "[Training Epoch 0] Batch 3683, Loss 0.331800639629364\n",
      "[Training Epoch 0] Batch 3684, Loss 0.29646843671798706\n",
      "[Training Epoch 0] Batch 3685, Loss 0.36637911200523376\n",
      "[Training Epoch 0] Batch 3686, Loss 0.30465131998062134\n",
      "[Training Epoch 0] Batch 3687, Loss 0.32364046573638916\n",
      "[Training Epoch 0] Batch 3688, Loss 0.2798600196838379\n",
      "[Training Epoch 0] Batch 3689, Loss 0.2950947880744934\n",
      "[Training Epoch 0] Batch 3690, Loss 0.2963051199913025\n",
      "[Training Epoch 0] Batch 3691, Loss 0.3333284258842468\n",
      "[Training Epoch 0] Batch 3692, Loss 0.33406761288642883\n",
      "[Training Epoch 0] Batch 3693, Loss 0.32255876064300537\n",
      "[Training Epoch 0] Batch 3694, Loss 0.337953120470047\n",
      "[Training Epoch 0] Batch 3695, Loss 0.30448803305625916\n",
      "[Training Epoch 0] Batch 3696, Loss 0.32046884298324585\n",
      "[Training Epoch 0] Batch 3697, Loss 0.3340919315814972\n",
      "[Training Epoch 0] Batch 3698, Loss 0.31186771392822266\n",
      "[Training Epoch 0] Batch 3699, Loss 0.32409900426864624\n",
      "[Training Epoch 0] Batch 3700, Loss 0.332338809967041\n",
      "[Training Epoch 0] Batch 3701, Loss 0.3329501748085022\n",
      "[Training Epoch 0] Batch 3702, Loss 0.34620681405067444\n",
      "[Training Epoch 0] Batch 3703, Loss 0.33578845858573914\n",
      "[Training Epoch 0] Batch 3704, Loss 0.3509157598018646\n",
      "[Training Epoch 0] Batch 3705, Loss 0.3235704302787781\n",
      "[Training Epoch 0] Batch 3706, Loss 0.32656875252723694\n",
      "[Training Epoch 0] Batch 3707, Loss 0.31085848808288574\n",
      "[Training Epoch 0] Batch 3708, Loss 0.34309473633766174\n",
      "[Training Epoch 0] Batch 3709, Loss 0.351796418428421\n",
      "[Training Epoch 0] Batch 3710, Loss 0.32662874460220337\n",
      "[Training Epoch 0] Batch 3711, Loss 0.31242862343788147\n",
      "[Training Epoch 0] Batch 3712, Loss 0.31587907671928406\n",
      "[Training Epoch 0] Batch 3713, Loss 0.31356263160705566\n",
      "[Training Epoch 0] Batch 3714, Loss 0.3034079968929291\n",
      "[Training Epoch 0] Batch 3715, Loss 0.319124698638916\n",
      "[Training Epoch 0] Batch 3716, Loss 0.32925939559936523\n",
      "[Training Epoch 0] Batch 3717, Loss 0.32001620531082153\n",
      "[Training Epoch 0] Batch 3718, Loss 0.333064466714859\n",
      "[Training Epoch 0] Batch 3719, Loss 0.32061028480529785\n",
      "[Training Epoch 0] Batch 3720, Loss 0.3072208762168884\n",
      "[Training Epoch 0] Batch 3721, Loss 0.3165968358516693\n",
      "[Training Epoch 0] Batch 3722, Loss 0.3238057792186737\n",
      "[Training Epoch 0] Batch 3723, Loss 0.31754666566848755\n",
      "[Training Epoch 0] Batch 3724, Loss 0.3127637505531311\n",
      "[Training Epoch 0] Batch 3725, Loss 0.3175414204597473\n",
      "[Training Epoch 0] Batch 3726, Loss 0.3137005567550659\n",
      "[Training Epoch 0] Batch 3727, Loss 0.30410075187683105\n",
      "[Training Epoch 0] Batch 3728, Loss 0.3345786929130554\n",
      "[Training Epoch 0] Batch 3729, Loss 0.30491524934768677\n",
      "[Training Epoch 0] Batch 3730, Loss 0.3281005024909973\n",
      "[Training Epoch 0] Batch 3731, Loss 0.3396732211112976\n",
      "[Training Epoch 0] Batch 3732, Loss 0.3520747125148773\n",
      "[Training Epoch 0] Batch 3733, Loss 0.32829251885414124\n",
      "[Training Epoch 0] Batch 3734, Loss 0.3508314788341522\n",
      "[Training Epoch 0] Batch 3735, Loss 0.3139438331127167\n",
      "[Training Epoch 0] Batch 3736, Loss 0.34654656052589417\n",
      "[Training Epoch 0] Batch 3737, Loss 0.30860066413879395\n",
      "[Training Epoch 0] Batch 3738, Loss 0.3418433666229248\n",
      "[Training Epoch 0] Batch 3739, Loss 0.31699228286743164\n",
      "[Training Epoch 0] Batch 3740, Loss 0.31136980652809143\n",
      "[Training Epoch 0] Batch 3741, Loss 0.30169951915740967\n",
      "[Training Epoch 0] Batch 3742, Loss 0.30636662244796753\n",
      "[Training Epoch 0] Batch 3743, Loss 0.311030775308609\n",
      "[Training Epoch 0] Batch 3744, Loss 0.33044737577438354\n",
      "[Training Epoch 0] Batch 3745, Loss 0.3068252503871918\n",
      "[Training Epoch 0] Batch 3746, Loss 0.3007684350013733\n",
      "[Training Epoch 0] Batch 3747, Loss 0.3454777002334595\n",
      "[Training Epoch 0] Batch 3748, Loss 0.34620869159698486\n",
      "[Training Epoch 0] Batch 3749, Loss 0.3044649660587311\n",
      "[Training Epoch 0] Batch 3750, Loss 0.3138774037361145\n",
      "[Training Epoch 0] Batch 3751, Loss 0.30262959003448486\n",
      "[Training Epoch 0] Batch 3752, Loss 0.31309038400650024\n",
      "[Training Epoch 0] Batch 3753, Loss 0.3329468071460724\n",
      "[Training Epoch 0] Batch 3754, Loss 0.32641613483428955\n",
      "[Training Epoch 0] Batch 3755, Loss 0.335263729095459\n",
      "[Training Epoch 0] Batch 3756, Loss 0.3275330066680908\n",
      "[Training Epoch 0] Batch 3757, Loss 0.3372875154018402\n",
      "[Training Epoch 0] Batch 3758, Loss 0.3247820734977722\n",
      "[Training Epoch 0] Batch 3759, Loss 0.35014718770980835\n",
      "[Training Epoch 0] Batch 3760, Loss 0.3241010308265686\n",
      "[Training Epoch 0] Batch 3761, Loss 0.29951706528663635\n",
      "[Training Epoch 0] Batch 3762, Loss 0.2977555990219116\n",
      "[Training Epoch 0] Batch 3763, Loss 0.3399564325809479\n",
      "[Training Epoch 0] Batch 3764, Loss 0.31544190645217896\n",
      "[Training Epoch 0] Batch 3765, Loss 0.3160548508167267\n",
      "[Training Epoch 0] Batch 3766, Loss 0.33329761028289795\n",
      "[Training Epoch 0] Batch 3767, Loss 0.32812875509262085\n",
      "[Training Epoch 0] Batch 3768, Loss 0.3080686032772064\n",
      "[Training Epoch 0] Batch 3769, Loss 0.3223716914653778\n",
      "[Training Epoch 0] Batch 3770, Loss 0.3332318067550659\n",
      "[Training Epoch 0] Batch 3771, Loss 0.33663904666900635\n",
      "[Training Epoch 0] Batch 3772, Loss 0.30859652161598206\n",
      "[Training Epoch 0] Batch 3773, Loss 0.3264883756637573\n",
      "[Training Epoch 0] Batch 3774, Loss 0.32914531230926514\n",
      "[Training Epoch 0] Batch 3775, Loss 0.3119969069957733\n",
      "[Training Epoch 0] Batch 3776, Loss 0.328494668006897\n",
      "[Training Epoch 0] Batch 3777, Loss 0.3222099542617798\n",
      "[Training Epoch 0] Batch 3778, Loss 0.3204554319381714\n",
      "[Training Epoch 0] Batch 3779, Loss 0.3168870806694031\n",
      "[Training Epoch 0] Batch 3780, Loss 0.28404727578163147\n",
      "[Training Epoch 0] Batch 3781, Loss 0.30701154470443726\n",
      "[Training Epoch 0] Batch 3782, Loss 0.33099043369293213\n",
      "[Training Epoch 0] Batch 3783, Loss 0.3332183361053467\n",
      "[Training Epoch 0] Batch 3784, Loss 0.3255384564399719\n",
      "[Training Epoch 0] Batch 3785, Loss 0.3352997303009033\n",
      "[Training Epoch 0] Batch 3786, Loss 0.30572783946990967\n",
      "[Training Epoch 0] Batch 3787, Loss 0.3151327073574066\n",
      "[Training Epoch 0] Batch 3788, Loss 0.3474731147289276\n",
      "[Training Epoch 0] Batch 3789, Loss 0.3175908029079437\n",
      "[Training Epoch 0] Batch 3790, Loss 0.32827627658843994\n",
      "[Training Epoch 0] Batch 3791, Loss 0.32323968410491943\n",
      "[Training Epoch 0] Batch 3792, Loss 0.366455078125\n",
      "[Training Epoch 0] Batch 3793, Loss 0.29931432008743286\n",
      "[Training Epoch 0] Batch 3794, Loss 0.32588353753089905\n",
      "[Training Epoch 0] Batch 3795, Loss 0.31396687030792236\n",
      "[Training Epoch 0] Batch 3796, Loss 0.334911584854126\n",
      "[Training Epoch 0] Batch 3797, Loss 0.29607877135276794\n",
      "[Training Epoch 0] Batch 3798, Loss 0.31483548879623413\n",
      "[Training Epoch 0] Batch 3799, Loss 0.3350580930709839\n",
      "[Training Epoch 0] Batch 3800, Loss 0.2850518226623535\n",
      "[Training Epoch 0] Batch 3801, Loss 0.31609046459198\n",
      "[Training Epoch 0] Batch 3802, Loss 0.3208587169647217\n",
      "[Training Epoch 0] Batch 3803, Loss 0.35623443126678467\n",
      "[Training Epoch 0] Batch 3804, Loss 0.3046409785747528\n",
      "[Training Epoch 0] Batch 3805, Loss 0.3298739492893219\n",
      "[Training Epoch 0] Batch 3806, Loss 0.3261529803276062\n",
      "[Training Epoch 0] Batch 3807, Loss 0.292751282453537\n",
      "[Training Epoch 0] Batch 3808, Loss 0.33284929394721985\n",
      "[Training Epoch 0] Batch 3809, Loss 0.31458207964897156\n",
      "[Training Epoch 0] Batch 3810, Loss 0.3247109353542328\n",
      "[Training Epoch 0] Batch 3811, Loss 0.33384811878204346\n",
      "[Training Epoch 0] Batch 3812, Loss 0.29785752296447754\n",
      "[Training Epoch 0] Batch 3813, Loss 0.3316400647163391\n",
      "[Training Epoch 0] Batch 3814, Loss 0.3110996186733246\n",
      "[Training Epoch 0] Batch 3815, Loss 0.328451931476593\n",
      "[Training Epoch 0] Batch 3816, Loss 0.30896615982055664\n",
      "[Training Epoch 0] Batch 3817, Loss 0.32731756567955017\n",
      "[Training Epoch 0] Batch 3818, Loss 0.33182597160339355\n",
      "[Training Epoch 0] Batch 3819, Loss 0.3088502883911133\n",
      "[Training Epoch 0] Batch 3820, Loss 0.2958829998970032\n",
      "[Training Epoch 0] Batch 3821, Loss 0.3208039402961731\n",
      "[Training Epoch 0] Batch 3822, Loss 0.34898239374160767\n",
      "[Training Epoch 0] Batch 3823, Loss 0.29477596282958984\n",
      "[Training Epoch 0] Batch 3824, Loss 0.33653444051742554\n",
      "[Training Epoch 0] Batch 3825, Loss 0.3136249780654907\n",
      "[Training Epoch 0] Batch 3826, Loss 0.3225522041320801\n",
      "[Training Epoch 0] Batch 3827, Loss 0.3110211491584778\n",
      "[Training Epoch 0] Batch 3828, Loss 0.31161439418792725\n",
      "[Training Epoch 0] Batch 3829, Loss 0.28599774837493896\n",
      "[Training Epoch 0] Batch 3830, Loss 0.31604060530662537\n",
      "[Training Epoch 0] Batch 3831, Loss 0.3365662395954132\n",
      "[Training Epoch 0] Batch 3832, Loss 0.294022798538208\n",
      "[Training Epoch 0] Batch 3833, Loss 0.2996075749397278\n",
      "[Training Epoch 0] Batch 3834, Loss 0.34066832065582275\n",
      "[Training Epoch 0] Batch 3835, Loss 0.3538992404937744\n",
      "[Training Epoch 0] Batch 3836, Loss 0.3330473303794861\n",
      "[Training Epoch 0] Batch 3837, Loss 0.30977004766464233\n",
      "[Training Epoch 0] Batch 3838, Loss 0.31125783920288086\n",
      "[Training Epoch 0] Batch 3839, Loss 0.31600821018218994\n",
      "[Training Epoch 0] Batch 3840, Loss 0.323513925075531\n",
      "[Training Epoch 0] Batch 3841, Loss 0.30631542205810547\n",
      "[Training Epoch 0] Batch 3842, Loss 0.33549362421035767\n",
      "[Training Epoch 0] Batch 3843, Loss 0.33734625577926636\n",
      "[Training Epoch 0] Batch 3844, Loss 0.3187206983566284\n",
      "[Training Epoch 0] Batch 3845, Loss 0.3067127764225006\n",
      "[Training Epoch 0] Batch 3846, Loss 0.3457987606525421\n",
      "[Training Epoch 0] Batch 3847, Loss 0.3210517168045044\n",
      "[Training Epoch 0] Batch 3848, Loss 0.34008848667144775\n",
      "[Training Epoch 0] Batch 3849, Loss 0.33521685004234314\n",
      "[Training Epoch 0] Batch 3850, Loss 0.33437952399253845\n",
      "[Training Epoch 0] Batch 3851, Loss 0.3098447322845459\n",
      "[Training Epoch 0] Batch 3852, Loss 0.34410470724105835\n",
      "[Training Epoch 0] Batch 3853, Loss 0.33210504055023193\n",
      "[Training Epoch 0] Batch 3854, Loss 0.3409072756767273\n",
      "[Training Epoch 0] Batch 3855, Loss 0.32209649682044983\n",
      "[Training Epoch 0] Batch 3856, Loss 0.30264371633529663\n",
      "[Training Epoch 0] Batch 3857, Loss 0.34365254640579224\n",
      "[Training Epoch 0] Batch 3858, Loss 0.29384952783584595\n",
      "[Training Epoch 0] Batch 3859, Loss 0.2979857921600342\n",
      "[Training Epoch 0] Batch 3860, Loss 0.32240867614746094\n",
      "[Training Epoch 0] Batch 3861, Loss 0.3271516263484955\n",
      "[Training Epoch 0] Batch 3862, Loss 0.32125064730644226\n",
      "[Training Epoch 0] Batch 3863, Loss 0.3097645938396454\n",
      "[Training Epoch 0] Batch 3864, Loss 0.35362058877944946\n",
      "[Training Epoch 0] Batch 3865, Loss 0.326898455619812\n",
      "[Training Epoch 0] Batch 3866, Loss 0.3418715298175812\n",
      "[Training Epoch 0] Batch 3867, Loss 0.3218826949596405\n",
      "[Training Epoch 0] Batch 3868, Loss 0.32297447323799133\n",
      "[Training Epoch 0] Batch 3869, Loss 0.3202283978462219\n",
      "[Training Epoch 0] Batch 3870, Loss 0.3230922222137451\n",
      "[Training Epoch 0] Batch 3871, Loss 0.3035600781440735\n",
      "[Training Epoch 0] Batch 3872, Loss 0.3299297094345093\n",
      "[Training Epoch 0] Batch 3873, Loss 0.3009205460548401\n",
      "[Training Epoch 0] Batch 3874, Loss 0.2929442822933197\n",
      "[Training Epoch 0] Batch 3875, Loss 0.35936063528060913\n",
      "[Training Epoch 0] Batch 3876, Loss 0.33678215742111206\n",
      "[Training Epoch 0] Batch 3877, Loss 0.3369775414466858\n",
      "[Training Epoch 0] Batch 3878, Loss 0.345969021320343\n",
      "[Training Epoch 0] Batch 3879, Loss 0.35248863697052\n",
      "[Training Epoch 0] Batch 3880, Loss 0.3578369915485382\n",
      "[Training Epoch 0] Batch 3881, Loss 0.32754775881767273\n",
      "[Training Epoch 0] Batch 3882, Loss 0.29401713609695435\n",
      "[Training Epoch 0] Batch 3883, Loss 0.33755743503570557\n",
      "[Training Epoch 0] Batch 3884, Loss 0.3451697826385498\n",
      "[Training Epoch 0] Batch 3885, Loss 0.3268173933029175\n",
      "[Training Epoch 0] Batch 3886, Loss 0.34773489832878113\n",
      "[Training Epoch 0] Batch 3887, Loss 0.32419610023498535\n",
      "[Training Epoch 0] Batch 3888, Loss 0.3402174115180969\n",
      "[Training Epoch 0] Batch 3889, Loss 0.28270387649536133\n",
      "[Training Epoch 0] Batch 3890, Loss 0.31435543298721313\n",
      "[Training Epoch 0] Batch 3891, Loss 0.33329343795776367\n",
      "[Training Epoch 0] Batch 3892, Loss 0.3096235394477844\n",
      "[Training Epoch 0] Batch 3893, Loss 0.3472291827201843\n",
      "[Training Epoch 0] Batch 3894, Loss 0.311745285987854\n",
      "[Training Epoch 0] Batch 3895, Loss 0.3531612753868103\n",
      "[Training Epoch 0] Batch 3896, Loss 0.3032657206058502\n",
      "[Training Epoch 0] Batch 3897, Loss 0.3104429244995117\n",
      "[Training Epoch 0] Batch 3898, Loss 0.33789363503456116\n",
      "[Training Epoch 0] Batch 3899, Loss 0.35089635848999023\n",
      "[Training Epoch 0] Batch 3900, Loss 0.33147600293159485\n",
      "[Training Epoch 0] Batch 3901, Loss 0.3003418445587158\n",
      "[Training Epoch 0] Batch 3902, Loss 0.3388969302177429\n",
      "[Training Epoch 0] Batch 3903, Loss 0.29189491271972656\n",
      "[Training Epoch 0] Batch 3904, Loss 0.3353242874145508\n",
      "[Training Epoch 0] Batch 3905, Loss 0.3050331175327301\n",
      "[Training Epoch 0] Batch 3906, Loss 0.3498409390449524\n",
      "[Training Epoch 0] Batch 3907, Loss 0.3316381573677063\n",
      "[Training Epoch 0] Batch 3908, Loss 0.3088683784008026\n",
      "[Training Epoch 0] Batch 3909, Loss 0.28838610649108887\n",
      "[Training Epoch 0] Batch 3910, Loss 0.33076125383377075\n",
      "[Training Epoch 0] Batch 3911, Loss 0.3086421489715576\n",
      "[Training Epoch 0] Batch 3912, Loss 0.31374603509902954\n",
      "[Training Epoch 0] Batch 3913, Loss 0.351176381111145\n",
      "[Training Epoch 0] Batch 3914, Loss 0.3354003429412842\n",
      "[Training Epoch 0] Batch 3915, Loss 0.32345062494277954\n",
      "[Training Epoch 0] Batch 3916, Loss 0.3132411241531372\n",
      "[Training Epoch 0] Batch 3917, Loss 0.31167638301849365\n",
      "[Training Epoch 0] Batch 3918, Loss 0.32542839646339417\n",
      "[Training Epoch 0] Batch 3919, Loss 0.32086700201034546\n",
      "[Training Epoch 0] Batch 3920, Loss 0.32984593510627747\n",
      "[Training Epoch 0] Batch 3921, Loss 0.3296118378639221\n",
      "[Training Epoch 0] Batch 3922, Loss 0.3356301188468933\n",
      "[Training Epoch 0] Batch 3923, Loss 0.32118916511535645\n",
      "[Training Epoch 0] Batch 3924, Loss 0.3209991455078125\n",
      "[Training Epoch 0] Batch 3925, Loss 0.3211789131164551\n",
      "[Training Epoch 0] Batch 3926, Loss 0.31769877672195435\n",
      "[Training Epoch 0] Batch 3927, Loss 0.3306141495704651\n",
      "[Training Epoch 0] Batch 3928, Loss 0.3482128977775574\n",
      "[Training Epoch 0] Batch 3929, Loss 0.3262442946434021\n",
      "[Training Epoch 0] Batch 3930, Loss 0.32028257846832275\n",
      "[Training Epoch 0] Batch 3931, Loss 0.29844844341278076\n",
      "[Training Epoch 0] Batch 3932, Loss 0.3073989152908325\n",
      "[Training Epoch 0] Batch 3933, Loss 0.3064173460006714\n",
      "[Training Epoch 0] Batch 3934, Loss 0.3271452784538269\n",
      "[Training Epoch 0] Batch 3935, Loss 0.31376737356185913\n",
      "[Training Epoch 0] Batch 3936, Loss 0.3084277808666229\n",
      "[Training Epoch 0] Batch 3937, Loss 0.2968428432941437\n",
      "[Training Epoch 0] Batch 3938, Loss 0.33367016911506653\n",
      "[Training Epoch 0] Batch 3939, Loss 0.2834106385707855\n",
      "[Training Epoch 0] Batch 3940, Loss 0.3138931691646576\n",
      "[Training Epoch 0] Batch 3941, Loss 0.30680137872695923\n",
      "[Training Epoch 0] Batch 3942, Loss 0.31576478481292725\n",
      "[Training Epoch 0] Batch 3943, Loss 0.2997581362724304\n",
      "[Training Epoch 0] Batch 3944, Loss 0.3389434516429901\n",
      "[Training Epoch 0] Batch 3945, Loss 0.3046916723251343\n",
      "[Training Epoch 0] Batch 3946, Loss 0.2983376681804657\n",
      "[Training Epoch 0] Batch 3947, Loss 0.33986812829971313\n",
      "[Training Epoch 0] Batch 3948, Loss 0.2788681387901306\n",
      "[Training Epoch 0] Batch 3949, Loss 0.31117281317710876\n",
      "[Training Epoch 0] Batch 3950, Loss 0.31540435552597046\n",
      "[Training Epoch 0] Batch 3951, Loss 0.322303831577301\n",
      "[Training Epoch 0] Batch 3952, Loss 0.3098781704902649\n",
      "[Training Epoch 0] Batch 3953, Loss 0.3258999288082123\n",
      "[Training Epoch 0] Batch 3954, Loss 0.29913175106048584\n",
      "[Training Epoch 0] Batch 3955, Loss 0.30927062034606934\n",
      "[Training Epoch 0] Batch 3956, Loss 0.2912638485431671\n",
      "[Training Epoch 0] Batch 3957, Loss 0.3431117534637451\n",
      "[Training Epoch 0] Batch 3958, Loss 0.3241421580314636\n",
      "[Training Epoch 0] Batch 3959, Loss 0.3147909641265869\n",
      "[Training Epoch 0] Batch 3960, Loss 0.3302004635334015\n",
      "[Training Epoch 0] Batch 3961, Loss 0.32028669118881226\n",
      "[Training Epoch 0] Batch 3962, Loss 0.32618555426597595\n",
      "[Training Epoch 0] Batch 3963, Loss 0.31940311193466187\n",
      "[Training Epoch 0] Batch 3964, Loss 0.34052205085754395\n",
      "[Training Epoch 0] Batch 3965, Loss 0.3286381661891937\n",
      "[Training Epoch 0] Batch 3966, Loss 0.3357279896736145\n",
      "[Training Epoch 0] Batch 3967, Loss 0.3283972442150116\n",
      "[Training Epoch 0] Batch 3968, Loss 0.3249583840370178\n",
      "[Training Epoch 0] Batch 3969, Loss 0.3063308894634247\n",
      "[Training Epoch 0] Batch 3970, Loss 0.3111376166343689\n",
      "[Training Epoch 0] Batch 3971, Loss 0.33188596367836\n",
      "[Training Epoch 0] Batch 3972, Loss 0.3253244161605835\n",
      "[Training Epoch 0] Batch 3973, Loss 0.3058203458786011\n",
      "[Training Epoch 0] Batch 3974, Loss 0.33508914709091187\n",
      "[Training Epoch 0] Batch 3975, Loss 0.3228664994239807\n",
      "[Training Epoch 0] Batch 3976, Loss 0.3033526837825775\n",
      "[Training Epoch 0] Batch 3977, Loss 0.3204152584075928\n",
      "[Training Epoch 0] Batch 3978, Loss 0.30842703580856323\n",
      "[Training Epoch 0] Batch 3979, Loss 0.32676342129707336\n",
      "[Training Epoch 0] Batch 3980, Loss 0.304352343082428\n",
      "[Training Epoch 0] Batch 3981, Loss 0.3268393874168396\n",
      "[Training Epoch 0] Batch 3982, Loss 0.31385284662246704\n",
      "[Training Epoch 0] Batch 3983, Loss 0.31218820810317993\n",
      "[Training Epoch 0] Batch 3984, Loss 0.3388764262199402\n",
      "[Training Epoch 0] Batch 3985, Loss 0.3328253924846649\n",
      "[Training Epoch 0] Batch 3986, Loss 0.33069562911987305\n",
      "[Training Epoch 0] Batch 3987, Loss 0.3161317706108093\n",
      "[Training Epoch 0] Batch 3988, Loss 0.2947925329208374\n",
      "[Training Epoch 0] Batch 3989, Loss 0.31141331791877747\n",
      "[Training Epoch 0] Batch 3990, Loss 0.2949875593185425\n",
      "[Training Epoch 0] Batch 3991, Loss 0.30160778760910034\n",
      "[Training Epoch 0] Batch 3992, Loss 0.3188146948814392\n",
      "[Training Epoch 0] Batch 3993, Loss 0.3338311016559601\n",
      "[Training Epoch 0] Batch 3994, Loss 0.29389315843582153\n",
      "[Training Epoch 0] Batch 3995, Loss 0.30753153562545776\n",
      "[Training Epoch 0] Batch 3996, Loss 0.31209567189216614\n",
      "[Training Epoch 0] Batch 3997, Loss 0.30959051847457886\n",
      "[Training Epoch 0] Batch 3998, Loss 0.3300720453262329\n",
      "[Training Epoch 0] Batch 3999, Loss 0.3015611171722412\n",
      "[Training Epoch 0] Batch 4000, Loss 0.3179453909397125\n",
      "[Training Epoch 0] Batch 4001, Loss 0.3140797019004822\n",
      "[Training Epoch 0] Batch 4002, Loss 0.3075193166732788\n",
      "[Training Epoch 0] Batch 4003, Loss 0.33654049038887024\n",
      "[Training Epoch 0] Batch 4004, Loss 0.30617111921310425\n",
      "[Training Epoch 0] Batch 4005, Loss 0.3254033923149109\n",
      "[Training Epoch 0] Batch 4006, Loss 0.33488985896110535\n",
      "[Training Epoch 0] Batch 4007, Loss 0.295989990234375\n",
      "[Training Epoch 0] Batch 4008, Loss 0.31593507528305054\n",
      "[Training Epoch 0] Batch 4009, Loss 0.3186449706554413\n",
      "[Training Epoch 0] Batch 4010, Loss 0.3526051342487335\n",
      "[Training Epoch 0] Batch 4011, Loss 0.29902175068855286\n",
      "[Training Epoch 0] Batch 4012, Loss 0.31760895252227783\n",
      "[Training Epoch 0] Batch 4013, Loss 0.31791451573371887\n",
      "[Training Epoch 0] Batch 4014, Loss 0.33953240513801575\n",
      "[Training Epoch 0] Batch 4015, Loss 0.33660265803337097\n",
      "[Training Epoch 0] Batch 4016, Loss 0.3222937285900116\n",
      "[Training Epoch 0] Batch 4017, Loss 0.3354133069515228\n",
      "[Training Epoch 0] Batch 4018, Loss 0.3261943459510803\n",
      "[Training Epoch 0] Batch 4019, Loss 0.31161728501319885\n",
      "[Training Epoch 0] Batch 4020, Loss 0.3309383988380432\n",
      "[Training Epoch 0] Batch 4021, Loss 0.2913658916950226\n",
      "[Training Epoch 0] Batch 4022, Loss 0.32201138138771057\n",
      "[Training Epoch 0] Batch 4023, Loss 0.3083956837654114\n",
      "[Training Epoch 0] Batch 4024, Loss 0.3280579149723053\n",
      "[Training Epoch 0] Batch 4025, Loss 0.29855814576148987\n",
      "[Training Epoch 0] Batch 4026, Loss 0.28085604310035706\n",
      "[Training Epoch 0] Batch 4027, Loss 0.33175262808799744\n",
      "[Training Epoch 0] Batch 4028, Loss 0.32649266719818115\n",
      "[Training Epoch 0] Batch 4029, Loss 0.32834315299987793\n",
      "[Training Epoch 0] Batch 4030, Loss 0.32300448417663574\n",
      "[Training Epoch 0] Batch 4031, Loss 0.34052401781082153\n",
      "[Training Epoch 0] Batch 4032, Loss 0.3019559979438782\n",
      "[Training Epoch 0] Batch 4033, Loss 0.3133785128593445\n",
      "[Training Epoch 0] Batch 4034, Loss 0.28480350971221924\n",
      "[Training Epoch 0] Batch 4035, Loss 0.34499871730804443\n",
      "[Training Epoch 0] Batch 4036, Loss 0.31884127855300903\n",
      "[Training Epoch 0] Batch 4037, Loss 0.30382972955703735\n",
      "[Training Epoch 0] Batch 4038, Loss 0.32382041215896606\n",
      "[Training Epoch 0] Batch 4039, Loss 0.33312198519706726\n",
      "[Training Epoch 0] Batch 4040, Loss 0.32644718885421753\n",
      "[Training Epoch 0] Batch 4041, Loss 0.3236731290817261\n",
      "[Training Epoch 0] Batch 4042, Loss 0.3237738609313965\n",
      "[Training Epoch 0] Batch 4043, Loss 0.32123002409935\n",
      "[Training Epoch 0] Batch 4044, Loss 0.30120277404785156\n",
      "[Training Epoch 0] Batch 4045, Loss 0.3562442362308502\n",
      "[Training Epoch 0] Batch 4046, Loss 0.3026876449584961\n",
      "[Training Epoch 0] Batch 4047, Loss 0.3221612572669983\n",
      "[Training Epoch 0] Batch 4048, Loss 0.31665241718292236\n",
      "[Training Epoch 0] Batch 4049, Loss 0.33898669481277466\n",
      "[Training Epoch 0] Batch 4050, Loss 0.32693159580230713\n",
      "[Training Epoch 0] Batch 4051, Loss 0.28664857149124146\n",
      "[Training Epoch 0] Batch 4052, Loss 0.31804507970809937\n",
      "[Training Epoch 0] Batch 4053, Loss 0.3416280448436737\n",
      "[Training Epoch 0] Batch 4054, Loss 0.29639214277267456\n",
      "[Training Epoch 0] Batch 4055, Loss 0.2985057830810547\n",
      "[Training Epoch 0] Batch 4056, Loss 0.27622300386428833\n",
      "[Training Epoch 0] Batch 4057, Loss 0.32221055030822754\n",
      "[Training Epoch 0] Batch 4058, Loss 0.3230651021003723\n",
      "[Training Epoch 0] Batch 4059, Loss 0.33485105633735657\n",
      "[Training Epoch 0] Batch 4060, Loss 0.3125980496406555\n",
      "[Training Epoch 0] Batch 4061, Loss 0.31012219190597534\n",
      "[Training Epoch 0] Batch 4062, Loss 0.30925941467285156\n",
      "[Training Epoch 0] Batch 4063, Loss 0.32964015007019043\n",
      "[Training Epoch 0] Batch 4064, Loss 0.2988225221633911\n",
      "[Training Epoch 0] Batch 4065, Loss 0.3420376181602478\n",
      "[Training Epoch 0] Batch 4066, Loss 0.31806477904319763\n",
      "[Training Epoch 0] Batch 4067, Loss 0.2957853674888611\n",
      "[Training Epoch 0] Batch 4068, Loss 0.32106173038482666\n",
      "[Training Epoch 0] Batch 4069, Loss 0.3106846809387207\n",
      "[Training Epoch 0] Batch 4070, Loss 0.29896387457847595\n",
      "[Training Epoch 0] Batch 4071, Loss 0.2993251383304596\n",
      "[Training Epoch 0] Batch 4072, Loss 0.34553325176239014\n",
      "[Training Epoch 0] Batch 4073, Loss 0.32546401023864746\n",
      "[Training Epoch 0] Batch 4074, Loss 0.34955716133117676\n",
      "[Training Epoch 0] Batch 4075, Loss 0.31432920694351196\n",
      "[Training Epoch 0] Batch 4076, Loss 0.31793877482414246\n",
      "[Training Epoch 0] Batch 4077, Loss 0.3005954623222351\n",
      "[Training Epoch 0] Batch 4078, Loss 0.3347786068916321\n",
      "[Training Epoch 0] Batch 4079, Loss 0.2945464253425598\n",
      "[Training Epoch 0] Batch 4080, Loss 0.2960747182369232\n",
      "[Training Epoch 0] Batch 4081, Loss 0.3059007525444031\n",
      "[Training Epoch 0] Batch 4082, Loss 0.3196905255317688\n",
      "[Training Epoch 0] Batch 4083, Loss 0.33670079708099365\n",
      "[Training Epoch 0] Batch 4084, Loss 0.3187177777290344\n",
      "[Training Epoch 0] Batch 4085, Loss 0.3403913378715515\n",
      "[Training Epoch 0] Batch 4086, Loss 0.2898479104042053\n",
      "[Training Epoch 0] Batch 4087, Loss 0.3178018629550934\n",
      "[Training Epoch 0] Batch 4088, Loss 0.3093351125717163\n",
      "[Training Epoch 0] Batch 4089, Loss 0.2946743369102478\n",
      "[Training Epoch 0] Batch 4090, Loss 0.3340657353401184\n",
      "[Training Epoch 0] Batch 4091, Loss 0.31069785356521606\n",
      "[Training Epoch 0] Batch 4092, Loss 0.32774683833122253\n",
      "[Training Epoch 0] Batch 4093, Loss 0.2884364426136017\n",
      "[Training Epoch 0] Batch 4094, Loss 0.28854674100875854\n",
      "[Training Epoch 0] Batch 4095, Loss 0.32020604610443115\n",
      "[Training Epoch 0] Batch 4096, Loss 0.3188686668872833\n",
      "[Training Epoch 0] Batch 4097, Loss 0.34000706672668457\n",
      "[Training Epoch 0] Batch 4098, Loss 0.29797637462615967\n",
      "[Training Epoch 0] Batch 4099, Loss 0.30808424949645996\n",
      "[Training Epoch 0] Batch 4100, Loss 0.3279784917831421\n",
      "[Training Epoch 0] Batch 4101, Loss 0.332681268453598\n",
      "[Training Epoch 0] Batch 4102, Loss 0.33025234937667847\n",
      "[Training Epoch 0] Batch 4103, Loss 0.32741519808769226\n",
      "[Training Epoch 0] Batch 4104, Loss 0.2996891736984253\n",
      "[Training Epoch 0] Batch 4105, Loss 0.2857338488101959\n",
      "[Training Epoch 0] Batch 4106, Loss 0.3000182509422302\n",
      "[Training Epoch 0] Batch 4107, Loss 0.3093302845954895\n",
      "[Training Epoch 0] Batch 4108, Loss 0.3259875178337097\n",
      "[Training Epoch 0] Batch 4109, Loss 0.3158149719238281\n",
      "[Training Epoch 0] Batch 4110, Loss 0.3172164559364319\n",
      "[Training Epoch 0] Batch 4111, Loss 0.33030030131340027\n",
      "[Training Epoch 0] Batch 4112, Loss 0.3466484546661377\n",
      "[Training Epoch 0] Batch 4113, Loss 0.3022359013557434\n",
      "[Training Epoch 0] Batch 4114, Loss 0.3059043884277344\n",
      "[Training Epoch 0] Batch 4115, Loss 0.3100232481956482\n",
      "[Training Epoch 0] Batch 4116, Loss 0.3259669840335846\n",
      "[Training Epoch 0] Batch 4117, Loss 0.2993585765361786\n",
      "[Training Epoch 0] Batch 4118, Loss 0.3139020800590515\n",
      "[Training Epoch 0] Batch 4119, Loss 0.3026488125324249\n",
      "[Training Epoch 0] Batch 4120, Loss 0.31477969884872437\n",
      "[Training Epoch 0] Batch 4121, Loss 0.3187180161476135\n",
      "[Training Epoch 0] Batch 4122, Loss 0.2979021966457367\n",
      "[Training Epoch 0] Batch 4123, Loss 0.3286336064338684\n",
      "[Training Epoch 0] Batch 4124, Loss 0.3446537256240845\n",
      "[Training Epoch 0] Batch 4125, Loss 0.3190072178840637\n",
      "[Training Epoch 0] Batch 4126, Loss 0.351743221282959\n",
      "[Training Epoch 0] Batch 4127, Loss 0.31219953298568726\n",
      "[Training Epoch 0] Batch 4128, Loss 0.3247125744819641\n",
      "[Training Epoch 0] Batch 4129, Loss 0.32715147733688354\n",
      "[Training Epoch 0] Batch 4130, Loss 0.32003703713417053\n",
      "[Training Epoch 0] Batch 4131, Loss 0.318575918674469\n",
      "[Training Epoch 0] Batch 4132, Loss 0.296366810798645\n",
      "[Training Epoch 0] Batch 4133, Loss 0.31139788031578064\n",
      "[Training Epoch 0] Batch 4134, Loss 0.31463682651519775\n",
      "[Training Epoch 0] Batch 4135, Loss 0.31694886088371277\n",
      "[Training Epoch 0] Batch 4136, Loss 0.3306038975715637\n",
      "[Training Epoch 0] Batch 4137, Loss 0.3215109407901764\n",
      "[Training Epoch 0] Batch 4138, Loss 0.32531577348709106\n",
      "[Training Epoch 0] Batch 4139, Loss 0.2932128310203552\n",
      "[Training Epoch 0] Batch 4140, Loss 0.3031212091445923\n",
      "[Training Epoch 0] Batch 4141, Loss 0.35932159423828125\n",
      "[Training Epoch 0] Batch 4142, Loss 0.32744789123535156\n",
      "[Training Epoch 0] Batch 4143, Loss 0.3271317183971405\n",
      "[Training Epoch 0] Batch 4144, Loss 0.33726757764816284\n",
      "[Training Epoch 0] Batch 4145, Loss 0.32605376839637756\n",
      "[Training Epoch 0] Batch 4146, Loss 0.3310582935810089\n",
      "[Training Epoch 0] Batch 4147, Loss 0.3349207043647766\n",
      "[Training Epoch 0] Batch 4148, Loss 0.3380376696586609\n",
      "[Training Epoch 0] Batch 4149, Loss 0.3013283312320709\n",
      "[Training Epoch 0] Batch 4150, Loss 0.3010358214378357\n",
      "[Training Epoch 0] Batch 4151, Loss 0.2966112494468689\n",
      "[Training Epoch 0] Batch 4152, Loss 0.3193317651748657\n",
      "[Training Epoch 0] Batch 4153, Loss 0.348531574010849\n",
      "[Training Epoch 0] Batch 4154, Loss 0.27755749225616455\n",
      "[Training Epoch 0] Batch 4155, Loss 0.3226378560066223\n",
      "[Training Epoch 0] Batch 4156, Loss 0.303930401802063\n",
      "[Training Epoch 0] Batch 4157, Loss 0.2858985662460327\n",
      "[Training Epoch 0] Batch 4158, Loss 0.33161187171936035\n",
      "[Training Epoch 0] Batch 4159, Loss 0.31578654050827026\n",
      "[Training Epoch 0] Batch 4160, Loss 0.3155108690261841\n",
      "[Training Epoch 0] Batch 4161, Loss 0.31162697076797485\n",
      "[Training Epoch 0] Batch 4162, Loss 0.31766027212142944\n",
      "[Training Epoch 0] Batch 4163, Loss 0.343106746673584\n",
      "[Training Epoch 0] Batch 4164, Loss 0.31089353561401367\n",
      "[Training Epoch 0] Batch 4165, Loss 0.3400192856788635\n",
      "[Training Epoch 0] Batch 4166, Loss 0.31598395109176636\n",
      "[Training Epoch 0] Batch 4167, Loss 0.3249657154083252\n",
      "[Training Epoch 0] Batch 4168, Loss 0.3106650412082672\n",
      "[Training Epoch 0] Batch 4169, Loss 0.32031023502349854\n",
      "[Training Epoch 0] Batch 4170, Loss 0.3189445734024048\n",
      "[Training Epoch 0] Batch 4171, Loss 0.3381778597831726\n",
      "[Training Epoch 0] Batch 4172, Loss 0.32393285632133484\n",
      "[Training Epoch 0] Batch 4173, Loss 0.29068106412887573\n",
      "[Training Epoch 0] Batch 4174, Loss 0.3106203079223633\n",
      "[Training Epoch 0] Batch 4175, Loss 0.3205869793891907\n",
      "[Training Epoch 0] Batch 4176, Loss 0.34876978397369385\n",
      "[Training Epoch 0] Batch 4177, Loss 0.3351937532424927\n",
      "[Training Epoch 0] Batch 4178, Loss 0.31682252883911133\n",
      "[Training Epoch 0] Batch 4179, Loss 0.32438287138938904\n",
      "[Training Epoch 0] Batch 4180, Loss 0.31814703345298767\n",
      "[Training Epoch 0] Batch 4181, Loss 0.2829558551311493\n",
      "[Training Epoch 0] Batch 4182, Loss 0.337094783782959\n",
      "[Training Epoch 0] Batch 4183, Loss 0.28852081298828125\n",
      "[Training Epoch 0] Batch 4184, Loss 0.34785357117652893\n",
      "[Training Epoch 0] Batch 4185, Loss 0.32367828488349915\n",
      "[Training Epoch 0] Batch 4186, Loss 0.3338491916656494\n",
      "[Training Epoch 0] Batch 4187, Loss 0.3154555559158325\n",
      "[Training Epoch 0] Batch 4188, Loss 0.3148982524871826\n",
      "[Training Epoch 0] Batch 4189, Loss 0.3248409628868103\n",
      "[Training Epoch 0] Batch 4190, Loss 0.29482096433639526\n",
      "[Training Epoch 0] Batch 4191, Loss 0.31033754348754883\n",
      "[Training Epoch 0] Batch 4192, Loss 0.324607253074646\n",
      "[Training Epoch 0] Batch 4193, Loss 0.31980472803115845\n",
      "[Training Epoch 0] Batch 4194, Loss 0.3243449032306671\n",
      "[Training Epoch 0] Batch 4195, Loss 0.28229546546936035\n",
      "[Training Epoch 0] Batch 4196, Loss 0.31394511461257935\n",
      "[Training Epoch 0] Batch 4197, Loss 0.3111501932144165\n",
      "[Training Epoch 0] Batch 4198, Loss 0.3337254524230957\n",
      "[Training Epoch 0] Batch 4199, Loss 0.3330000638961792\n",
      "[Training Epoch 0] Batch 4200, Loss 0.3392285704612732\n",
      "[Training Epoch 0] Batch 4201, Loss 0.31695958971977234\n",
      "[Training Epoch 0] Batch 4202, Loss 0.30789387226104736\n",
      "[Training Epoch 0] Batch 4203, Loss 0.3150651156902313\n",
      "[Training Epoch 0] Batch 4204, Loss 0.2906643748283386\n",
      "[Training Epoch 0] Batch 4205, Loss 0.34195369482040405\n",
      "[Training Epoch 0] Batch 4206, Loss 0.34581518173217773\n",
      "[Training Epoch 0] Batch 4207, Loss 0.3074262738227844\n",
      "[Training Epoch 0] Batch 4208, Loss 0.34110960364341736\n",
      "[Training Epoch 0] Batch 4209, Loss 0.31927913427352905\n",
      "[Training Epoch 0] Batch 4210, Loss 0.32253217697143555\n",
      "[Training Epoch 0] Batch 4211, Loss 0.3243945240974426\n",
      "[Training Epoch 0] Batch 4212, Loss 0.3100522756576538\n",
      "[Training Epoch 0] Batch 4213, Loss 0.32156044244766235\n",
      "[Training Epoch 0] Batch 4214, Loss 0.334933340549469\n",
      "[Training Epoch 0] Batch 4215, Loss 0.32956835627555847\n",
      "[Training Epoch 0] Batch 4216, Loss 0.3199167251586914\n",
      "[Training Epoch 0] Batch 4217, Loss 0.3382927179336548\n",
      "[Training Epoch 0] Batch 4218, Loss 0.32583045959472656\n",
      "[Training Epoch 0] Batch 4219, Loss 0.33003777265548706\n",
      "[Training Epoch 0] Batch 4220, Loss 0.3175559937953949\n",
      "[Training Epoch 0] Batch 4221, Loss 0.3219306170940399\n",
      "[Training Epoch 0] Batch 4222, Loss 0.3326544463634491\n",
      "[Training Epoch 0] Batch 4223, Loss 0.33865123987197876\n",
      "[Training Epoch 0] Batch 4224, Loss 0.29874688386917114\n",
      "[Training Epoch 0] Batch 4225, Loss 0.3154829740524292\n",
      "[Training Epoch 0] Batch 4226, Loss 0.3219956159591675\n",
      "[Training Epoch 0] Batch 4227, Loss 0.3311580717563629\n",
      "[Training Epoch 0] Batch 4228, Loss 0.3093962073326111\n",
      "[Training Epoch 0] Batch 4229, Loss 0.34532231092453003\n",
      "[Training Epoch 0] Batch 4230, Loss 0.3274233341217041\n",
      "[Training Epoch 0] Batch 4231, Loss 0.32164767384529114\n",
      "[Training Epoch 0] Batch 4232, Loss 0.3253755569458008\n",
      "[Training Epoch 0] Batch 4233, Loss 0.3230507969856262\n",
      "[Training Epoch 0] Batch 4234, Loss 0.32383835315704346\n",
      "[Training Epoch 0] Batch 4235, Loss 0.36282283067703247\n",
      "[Training Epoch 0] Batch 4236, Loss 0.3147227466106415\n",
      "[Training Epoch 0] Batch 4237, Loss 0.30251431465148926\n",
      "[Training Epoch 0] Batch 4238, Loss 0.31672924757003784\n",
      "[Training Epoch 0] Batch 4239, Loss 0.3109293580055237\n",
      "[Training Epoch 0] Batch 4240, Loss 0.3276433050632477\n",
      "[Training Epoch 0] Batch 4241, Loss 0.30754125118255615\n",
      "[Training Epoch 0] Batch 4242, Loss 0.3031686842441559\n",
      "[Training Epoch 0] Batch 4243, Loss 0.33599555492401123\n",
      "[Training Epoch 0] Batch 4244, Loss 0.3057529628276825\n",
      "[Training Epoch 0] Batch 4245, Loss 0.31839194893836975\n",
      "[Training Epoch 0] Batch 4246, Loss 0.3151267468929291\n",
      "[Training Epoch 0] Batch 4247, Loss 0.3235560953617096\n",
      "[Training Epoch 0] Batch 4248, Loss 0.3094805181026459\n",
      "[Training Epoch 0] Batch 4249, Loss 0.3034469485282898\n",
      "[Training Epoch 0] Batch 4250, Loss 0.32353904843330383\n",
      "[Training Epoch 0] Batch 4251, Loss 0.33732712268829346\n",
      "[Training Epoch 0] Batch 4252, Loss 0.3184410035610199\n",
      "[Training Epoch 0] Batch 4253, Loss 0.3115406632423401\n",
      "[Training Epoch 0] Batch 4254, Loss 0.32152658700942993\n",
      "[Training Epoch 0] Batch 4255, Loss 0.3380112648010254\n",
      "[Training Epoch 0] Batch 4256, Loss 0.3155940771102905\n",
      "[Training Epoch 0] Batch 4257, Loss 0.30416661500930786\n",
      "[Training Epoch 0] Batch 4258, Loss 0.3437824249267578\n",
      "[Training Epoch 0] Batch 4259, Loss 0.2946246564388275\n",
      "[Training Epoch 0] Batch 4260, Loss 0.3279164433479309\n",
      "[Training Epoch 0] Batch 4261, Loss 0.28003454208374023\n",
      "[Training Epoch 0] Batch 4262, Loss 0.3319723606109619\n",
      "[Training Epoch 0] Batch 4263, Loss 0.33094990253448486\n",
      "[Training Epoch 0] Batch 4264, Loss 0.3352786600589752\n",
      "[Training Epoch 0] Batch 4265, Loss 0.3396875262260437\n",
      "[Training Epoch 0] Batch 4266, Loss 0.3343416750431061\n",
      "[Training Epoch 0] Batch 4267, Loss 0.3228299021720886\n",
      "[Training Epoch 0] Batch 4268, Loss 0.32568442821502686\n",
      "[Training Epoch 0] Batch 4269, Loss 0.32828956842422485\n",
      "[Training Epoch 0] Batch 4270, Loss 0.3252740502357483\n",
      "[Training Epoch 0] Batch 4271, Loss 0.3037988543510437\n",
      "[Training Epoch 0] Batch 4272, Loss 0.30764269828796387\n",
      "[Training Epoch 0] Batch 4273, Loss 0.3156297206878662\n",
      "[Training Epoch 0] Batch 4274, Loss 0.3188928961753845\n",
      "[Training Epoch 0] Batch 4275, Loss 0.3239580988883972\n",
      "[Training Epoch 0] Batch 4276, Loss 0.2986494302749634\n",
      "[Training Epoch 0] Batch 4277, Loss 0.3418215811252594\n",
      "[Training Epoch 0] Batch 4278, Loss 0.3143025040626526\n",
      "[Training Epoch 0] Batch 4279, Loss 0.3424626290798187\n",
      "[Training Epoch 0] Batch 4280, Loss 0.3148287534713745\n",
      "[Training Epoch 0] Batch 4281, Loss 0.33286669850349426\n",
      "[Training Epoch 0] Batch 4282, Loss 0.31604719161987305\n",
      "[Training Epoch 0] Batch 4283, Loss 0.3246144652366638\n",
      "[Training Epoch 0] Batch 4284, Loss 0.3024587631225586\n",
      "[Training Epoch 0] Batch 4285, Loss 0.3546600341796875\n",
      "[Training Epoch 0] Batch 4286, Loss 0.30733996629714966\n",
      "[Training Epoch 0] Batch 4287, Loss 0.3128534257411957\n",
      "[Training Epoch 0] Batch 4288, Loss 0.33064988255500793\n",
      "[Training Epoch 0] Batch 4289, Loss 0.3141721189022064\n",
      "[Training Epoch 0] Batch 4290, Loss 0.27792924642562866\n",
      "[Training Epoch 0] Batch 4291, Loss 0.31305280327796936\n",
      "[Training Epoch 0] Batch 4292, Loss 0.3092449903488159\n",
      "[Training Epoch 0] Batch 4293, Loss 0.3394508957862854\n",
      "[Training Epoch 0] Batch 4294, Loss 0.3054545521736145\n",
      "[Training Epoch 0] Batch 4295, Loss 0.3361799120903015\n",
      "[Training Epoch 0] Batch 4296, Loss 0.32672610878944397\n",
      "[Training Epoch 0] Batch 4297, Loss 0.3524351418018341\n",
      "[Training Epoch 0] Batch 4298, Loss 0.32314765453338623\n",
      "[Training Epoch 0] Batch 4299, Loss 0.3334079384803772\n",
      "[Training Epoch 0] Batch 4300, Loss 0.3127822279930115\n",
      "[Training Epoch 0] Batch 4301, Loss 0.3406696617603302\n",
      "[Training Epoch 0] Batch 4302, Loss 0.29767462611198425\n",
      "[Training Epoch 0] Batch 4303, Loss 0.28312256932258606\n",
      "[Training Epoch 0] Batch 4304, Loss 0.32588499784469604\n",
      "[Training Epoch 0] Batch 4305, Loss 0.3199270963668823\n",
      "[Training Epoch 0] Batch 4306, Loss 0.32499998807907104\n",
      "[Training Epoch 0] Batch 4307, Loss 0.32212281227111816\n",
      "[Training Epoch 0] Batch 4308, Loss 0.32239142060279846\n",
      "[Training Epoch 0] Batch 4309, Loss 0.32454419136047363\n",
      "[Training Epoch 0] Batch 4310, Loss 0.347371369600296\n",
      "[Training Epoch 0] Batch 4311, Loss 0.34297558665275574\n",
      "[Training Epoch 0] Batch 4312, Loss 0.3179281949996948\n",
      "[Training Epoch 0] Batch 4313, Loss 0.289683997631073\n",
      "[Training Epoch 0] Batch 4314, Loss 0.31219714879989624\n",
      "[Training Epoch 0] Batch 4315, Loss 0.2985982894897461\n",
      "[Training Epoch 0] Batch 4316, Loss 0.3019430637359619\n",
      "[Training Epoch 0] Batch 4317, Loss 0.34596550464630127\n",
      "[Training Epoch 0] Batch 4318, Loss 0.342565655708313\n",
      "[Training Epoch 0] Batch 4319, Loss 0.3095296025276184\n",
      "[Training Epoch 0] Batch 4320, Loss 0.3189169764518738\n",
      "[Training Epoch 0] Batch 4321, Loss 0.30133458971977234\n",
      "[Training Epoch 0] Batch 4322, Loss 0.3339768052101135\n",
      "[Training Epoch 0] Batch 4323, Loss 0.3283431828022003\n",
      "[Training Epoch 0] Batch 4324, Loss 0.3224113881587982\n",
      "[Training Epoch 0] Batch 4325, Loss 0.2963084578514099\n",
      "[Training Epoch 0] Batch 4326, Loss 0.28564029932022095\n",
      "[Training Epoch 0] Batch 4327, Loss 0.34062474966049194\n",
      "[Training Epoch 0] Batch 4328, Loss 0.3421062231063843\n",
      "[Training Epoch 0] Batch 4329, Loss 0.32112300395965576\n",
      "[Training Epoch 0] Batch 4330, Loss 0.3089972734451294\n",
      "[Training Epoch 0] Batch 4331, Loss 0.3144068419933319\n",
      "[Training Epoch 0] Batch 4332, Loss 0.34359776973724365\n",
      "[Training Epoch 0] Batch 4333, Loss 0.2979568541049957\n",
      "[Training Epoch 0] Batch 4334, Loss 0.3276006579399109\n",
      "[Training Epoch 0] Batch 4335, Loss 0.3540329933166504\n",
      "[Training Epoch 0] Batch 4336, Loss 0.30051475763320923\n",
      "[Training Epoch 0] Batch 4337, Loss 0.3372501730918884\n",
      "[Training Epoch 0] Batch 4338, Loss 0.2956392168998718\n",
      "[Training Epoch 0] Batch 4339, Loss 0.3061070144176483\n",
      "[Training Epoch 0] Batch 4340, Loss 0.32818275690078735\n",
      "[Training Epoch 0] Batch 4341, Loss 0.3007001578807831\n",
      "[Training Epoch 0] Batch 4342, Loss 0.3471272587776184\n",
      "[Training Epoch 0] Batch 4343, Loss 0.3338767886161804\n",
      "[Training Epoch 0] Batch 4344, Loss 0.3307427763938904\n",
      "[Training Epoch 0] Batch 4345, Loss 0.2951970398426056\n",
      "[Training Epoch 0] Batch 4346, Loss 0.33731454610824585\n",
      "[Training Epoch 0] Batch 4347, Loss 0.33121174573898315\n",
      "[Training Epoch 0] Batch 4348, Loss 0.31864112615585327\n",
      "[Training Epoch 0] Batch 4349, Loss 0.31316378712654114\n",
      "[Training Epoch 0] Batch 4350, Loss 0.32872921228408813\n",
      "[Training Epoch 0] Batch 4351, Loss 0.305804580450058\n",
      "[Training Epoch 0] Batch 4352, Loss 0.3010995090007782\n",
      "[Training Epoch 0] Batch 4353, Loss 0.3076900243759155\n",
      "[Training Epoch 0] Batch 4354, Loss 0.34313589334487915\n",
      "[Training Epoch 0] Batch 4355, Loss 0.32928287982940674\n",
      "[Training Epoch 0] Batch 4356, Loss 0.2871849238872528\n",
      "[Training Epoch 0] Batch 4357, Loss 0.29741090536117554\n",
      "[Training Epoch 0] Batch 4358, Loss 0.3252880573272705\n",
      "[Training Epoch 0] Batch 4359, Loss 0.29594284296035767\n",
      "[Training Epoch 0] Batch 4360, Loss 0.29796963930130005\n",
      "[Training Epoch 0] Batch 4361, Loss 0.30095550417900085\n",
      "[Training Epoch 0] Batch 4362, Loss 0.2938527762889862\n",
      "[Training Epoch 0] Batch 4363, Loss 0.28594112396240234\n",
      "[Training Epoch 0] Batch 4364, Loss 0.32615014910697937\n",
      "[Training Epoch 0] Batch 4365, Loss 0.312940776348114\n",
      "[Training Epoch 0] Batch 4366, Loss 0.2929040193557739\n",
      "[Training Epoch 0] Batch 4367, Loss 0.3406409025192261\n",
      "[Training Epoch 0] Batch 4368, Loss 0.33893898129463196\n",
      "[Training Epoch 0] Batch 4369, Loss 0.31529319286346436\n",
      "[Training Epoch 0] Batch 4370, Loss 0.32846927642822266\n",
      "[Training Epoch 0] Batch 4371, Loss 0.280610054731369\n",
      "[Training Epoch 0] Batch 4372, Loss 0.2939736843109131\n",
      "[Training Epoch 0] Batch 4373, Loss 0.30503013730049133\n",
      "[Training Epoch 0] Batch 4374, Loss 0.3153074085712433\n",
      "[Training Epoch 0] Batch 4375, Loss 0.2970394790172577\n",
      "[Training Epoch 0] Batch 4376, Loss 0.32164937257766724\n",
      "[Training Epoch 0] Batch 4377, Loss 0.32344433665275574\n",
      "[Training Epoch 0] Batch 4378, Loss 0.3161945641040802\n",
      "[Training Epoch 0] Batch 4379, Loss 0.332313597202301\n",
      "[Training Epoch 0] Batch 4380, Loss 0.2906606197357178\n",
      "[Training Epoch 0] Batch 4381, Loss 0.34708550572395325\n",
      "[Training Epoch 0] Batch 4382, Loss 0.38828298449516296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2019.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 0] Precision = 0.2623, Recall = 0.7758\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3102486729621887\n",
      "[Training Epoch 1] Batch 1, Loss 0.310570627450943\n",
      "[Training Epoch 1] Batch 2, Loss 0.31516993045806885\n",
      "[Training Epoch 1] Batch 3, Loss 0.3371031582355499\n",
      "[Training Epoch 1] Batch 4, Loss 0.31891119480133057\n",
      "[Training Epoch 1] Batch 5, Loss 0.2950422763824463\n",
      "[Training Epoch 1] Batch 6, Loss 0.29444846510887146\n",
      "[Training Epoch 1] Batch 7, Loss 0.30788883566856384\n",
      "[Training Epoch 1] Batch 8, Loss 0.3101208806037903\n",
      "[Training Epoch 1] Batch 9, Loss 0.3221988081932068\n",
      "[Training Epoch 1] Batch 10, Loss 0.29978638887405396\n",
      "[Training Epoch 1] Batch 11, Loss 0.316616952419281\n",
      "[Training Epoch 1] Batch 12, Loss 0.30091363191604614\n",
      "[Training Epoch 1] Batch 13, Loss 0.33852726221084595\n",
      "[Training Epoch 1] Batch 14, Loss 0.3355109691619873\n",
      "[Training Epoch 1] Batch 15, Loss 0.29385462403297424\n",
      "[Training Epoch 1] Batch 16, Loss 0.29137855768203735\n",
      "[Training Epoch 1] Batch 17, Loss 0.3014465868473053\n",
      "[Training Epoch 1] Batch 18, Loss 0.3107343912124634\n",
      "[Training Epoch 1] Batch 19, Loss 0.31183966994285583\n",
      "[Training Epoch 1] Batch 20, Loss 0.3047754764556885\n",
      "[Training Epoch 1] Batch 21, Loss 0.28088533878326416\n",
      "[Training Epoch 1] Batch 22, Loss 0.2910804748535156\n",
      "[Training Epoch 1] Batch 23, Loss 0.3430781364440918\n",
      "[Training Epoch 1] Batch 24, Loss 0.3182528614997864\n",
      "[Training Epoch 1] Batch 25, Loss 0.3097655177116394\n",
      "[Training Epoch 1] Batch 26, Loss 0.3219975531101227\n",
      "[Training Epoch 1] Batch 27, Loss 0.3204146921634674\n",
      "[Training Epoch 1] Batch 28, Loss 0.28635913133621216\n",
      "[Training Epoch 1] Batch 29, Loss 0.28907692432403564\n",
      "[Training Epoch 1] Batch 30, Loss 0.3278490900993347\n",
      "[Training Epoch 1] Batch 31, Loss 0.36371633410453796\n",
      "[Training Epoch 1] Batch 32, Loss 0.3114916682243347\n",
      "[Training Epoch 1] Batch 33, Loss 0.31840652227401733\n",
      "[Training Epoch 1] Batch 34, Loss 0.31506234407424927\n",
      "[Training Epoch 1] Batch 35, Loss 0.32534995675086975\n",
      "[Training Epoch 1] Batch 36, Loss 0.3096259832382202\n",
      "[Training Epoch 1] Batch 37, Loss 0.2829177975654602\n",
      "[Training Epoch 1] Batch 38, Loss 0.29919326305389404\n",
      "[Training Epoch 1] Batch 39, Loss 0.3064340651035309\n",
      "[Training Epoch 1] Batch 40, Loss 0.27434587478637695\n",
      "[Training Epoch 1] Batch 41, Loss 0.3036668300628662\n",
      "[Training Epoch 1] Batch 42, Loss 0.29727092385292053\n",
      "[Training Epoch 1] Batch 43, Loss 0.30732297897338867\n",
      "[Training Epoch 1] Batch 44, Loss 0.325324147939682\n",
      "[Training Epoch 1] Batch 45, Loss 0.31435924768447876\n",
      "[Training Epoch 1] Batch 46, Loss 0.2982552647590637\n",
      "[Training Epoch 1] Batch 47, Loss 0.3052467107772827\n",
      "[Training Epoch 1] Batch 48, Loss 0.3070521354675293\n",
      "[Training Epoch 1] Batch 49, Loss 0.323935866355896\n",
      "[Training Epoch 1] Batch 50, Loss 0.30763518810272217\n",
      "[Training Epoch 1] Batch 51, Loss 0.30992400646209717\n",
      "[Training Epoch 1] Batch 52, Loss 0.3132275640964508\n",
      "[Training Epoch 1] Batch 53, Loss 0.3331298828125\n",
      "[Training Epoch 1] Batch 54, Loss 0.3139849305152893\n",
      "[Training Epoch 1] Batch 55, Loss 0.3112999200820923\n",
      "[Training Epoch 1] Batch 56, Loss 0.3208286464214325\n",
      "[Training Epoch 1] Batch 57, Loss 0.36268824338912964\n",
      "[Training Epoch 1] Batch 58, Loss 0.3244745135307312\n",
      "[Training Epoch 1] Batch 59, Loss 0.31672126054763794\n",
      "[Training Epoch 1] Batch 60, Loss 0.3154605031013489\n",
      "[Training Epoch 1] Batch 61, Loss 0.3186696469783783\n",
      "[Training Epoch 1] Batch 62, Loss 0.27607443928718567\n",
      "[Training Epoch 1] Batch 63, Loss 0.31973206996917725\n",
      "[Training Epoch 1] Batch 64, Loss 0.3187149167060852\n",
      "[Training Epoch 1] Batch 65, Loss 0.3017944097518921\n",
      "[Training Epoch 1] Batch 66, Loss 0.3082593083381653\n",
      "[Training Epoch 1] Batch 67, Loss 0.28273266553878784\n",
      "[Training Epoch 1] Batch 68, Loss 0.3231142461299896\n",
      "[Training Epoch 1] Batch 69, Loss 0.3078509569168091\n",
      "[Training Epoch 1] Batch 70, Loss 0.2804934084415436\n",
      "[Training Epoch 1] Batch 71, Loss 0.30447298288345337\n",
      "[Training Epoch 1] Batch 72, Loss 0.31444022059440613\n",
      "[Training Epoch 1] Batch 73, Loss 0.2916969060897827\n",
      "[Training Epoch 1] Batch 74, Loss 0.3091723322868347\n",
      "[Training Epoch 1] Batch 75, Loss 0.34244269132614136\n",
      "[Training Epoch 1] Batch 76, Loss 0.3304847180843353\n",
      "[Training Epoch 1] Batch 77, Loss 0.3238207697868347\n",
      "[Training Epoch 1] Batch 78, Loss 0.2918166518211365\n",
      "[Training Epoch 1] Batch 79, Loss 0.3120478689670563\n",
      "[Training Epoch 1] Batch 80, Loss 0.30061763525009155\n",
      "[Training Epoch 1] Batch 81, Loss 0.30263960361480713\n",
      "[Training Epoch 1] Batch 82, Loss 0.31256723403930664\n",
      "[Training Epoch 1] Batch 83, Loss 0.3175637125968933\n",
      "[Training Epoch 1] Batch 84, Loss 0.3000199794769287\n",
      "[Training Epoch 1] Batch 85, Loss 0.31428438425064087\n",
      "[Training Epoch 1] Batch 86, Loss 0.3051050901412964\n",
      "[Training Epoch 1] Batch 87, Loss 0.3149697184562683\n",
      "[Training Epoch 1] Batch 88, Loss 0.32377976179122925\n",
      "[Training Epoch 1] Batch 89, Loss 0.30888503789901733\n",
      "[Training Epoch 1] Batch 90, Loss 0.33129236102104187\n",
      "[Training Epoch 1] Batch 91, Loss 0.2998991012573242\n",
      "[Training Epoch 1] Batch 92, Loss 0.31214916706085205\n",
      "[Training Epoch 1] Batch 93, Loss 0.3165885806083679\n",
      "[Training Epoch 1] Batch 94, Loss 0.33663758635520935\n",
      "[Training Epoch 1] Batch 95, Loss 0.3245553970336914\n",
      "[Training Epoch 1] Batch 96, Loss 0.30447930097579956\n",
      "[Training Epoch 1] Batch 97, Loss 0.31547364592552185\n",
      "[Training Epoch 1] Batch 98, Loss 0.3154744505882263\n",
      "[Training Epoch 1] Batch 99, Loss 0.3187140226364136\n",
      "[Training Epoch 1] Batch 100, Loss 0.31148794293403625\n",
      "[Training Epoch 1] Batch 101, Loss 0.2933151125907898\n",
      "[Training Epoch 1] Batch 102, Loss 0.3140990138053894\n",
      "[Training Epoch 1] Batch 103, Loss 0.30593040585517883\n",
      "[Training Epoch 1] Batch 104, Loss 0.33044153451919556\n",
      "[Training Epoch 1] Batch 105, Loss 0.29347291588783264\n",
      "[Training Epoch 1] Batch 106, Loss 0.28843826055526733\n",
      "[Training Epoch 1] Batch 107, Loss 0.3199315667152405\n",
      "[Training Epoch 1] Batch 108, Loss 0.2991369962692261\n",
      "[Training Epoch 1] Batch 109, Loss 0.3329535722732544\n",
      "[Training Epoch 1] Batch 110, Loss 0.31690239906311035\n",
      "[Training Epoch 1] Batch 111, Loss 0.33260583877563477\n",
      "[Training Epoch 1] Batch 112, Loss 0.3015570342540741\n",
      "[Training Epoch 1] Batch 113, Loss 0.3088647723197937\n",
      "[Training Epoch 1] Batch 114, Loss 0.3293720483779907\n",
      "[Training Epoch 1] Batch 115, Loss 0.3406992554664612\n",
      "[Training Epoch 1] Batch 116, Loss 0.3105062246322632\n",
      "[Training Epoch 1] Batch 117, Loss 0.29759353399276733\n",
      "[Training Epoch 1] Batch 118, Loss 0.3100963234901428\n",
      "[Training Epoch 1] Batch 119, Loss 0.3069847524166107\n",
      "[Training Epoch 1] Batch 120, Loss 0.3213007152080536\n",
      "[Training Epoch 1] Batch 121, Loss 0.2830943465232849\n",
      "[Training Epoch 1] Batch 122, Loss 0.28618887066841125\n",
      "[Training Epoch 1] Batch 123, Loss 0.3307170867919922\n",
      "[Training Epoch 1] Batch 124, Loss 0.30605533719062805\n",
      "[Training Epoch 1] Batch 125, Loss 0.3293887972831726\n",
      "[Training Epoch 1] Batch 126, Loss 0.31501269340515137\n",
      "[Training Epoch 1] Batch 127, Loss 0.2898615002632141\n",
      "[Training Epoch 1] Batch 128, Loss 0.30509233474731445\n",
      "[Training Epoch 1] Batch 129, Loss 0.3075717091560364\n",
      "[Training Epoch 1] Batch 130, Loss 0.29801446199417114\n",
      "[Training Epoch 1] Batch 131, Loss 0.33687326312065125\n",
      "[Training Epoch 1] Batch 132, Loss 0.29443973302841187\n",
      "[Training Epoch 1] Batch 133, Loss 0.29700714349746704\n",
      "[Training Epoch 1] Batch 134, Loss 0.3173379898071289\n",
      "[Training Epoch 1] Batch 135, Loss 0.3269592523574829\n",
      "[Training Epoch 1] Batch 136, Loss 0.31369075179100037\n",
      "[Training Epoch 1] Batch 137, Loss 0.30728060007095337\n",
      "[Training Epoch 1] Batch 138, Loss 0.32734715938568115\n",
      "[Training Epoch 1] Batch 139, Loss 0.29450035095214844\n",
      "[Training Epoch 1] Batch 140, Loss 0.3213527798652649\n",
      "[Training Epoch 1] Batch 141, Loss 0.29977965354919434\n",
      "[Training Epoch 1] Batch 142, Loss 0.3012361526489258\n",
      "[Training Epoch 1] Batch 143, Loss 0.3343604505062103\n",
      "[Training Epoch 1] Batch 144, Loss 0.28902095556259155\n",
      "[Training Epoch 1] Batch 145, Loss 0.32939451932907104\n",
      "[Training Epoch 1] Batch 146, Loss 0.33329129219055176\n",
      "[Training Epoch 1] Batch 147, Loss 0.3071916103363037\n",
      "[Training Epoch 1] Batch 148, Loss 0.34012216329574585\n",
      "[Training Epoch 1] Batch 149, Loss 0.31124305725097656\n",
      "[Training Epoch 1] Batch 150, Loss 0.29656752943992615\n",
      "[Training Epoch 1] Batch 151, Loss 0.33669817447662354\n",
      "[Training Epoch 1] Batch 152, Loss 0.3097725212574005\n",
      "[Training Epoch 1] Batch 153, Loss 0.3238823115825653\n",
      "[Training Epoch 1] Batch 154, Loss 0.31725987792015076\n",
      "[Training Epoch 1] Batch 155, Loss 0.31107211112976074\n",
      "[Training Epoch 1] Batch 156, Loss 0.3035076856613159\n",
      "[Training Epoch 1] Batch 157, Loss 0.29019421339035034\n",
      "[Training Epoch 1] Batch 158, Loss 0.3015404939651489\n",
      "[Training Epoch 1] Batch 159, Loss 0.31149667501449585\n",
      "[Training Epoch 1] Batch 160, Loss 0.30033743381500244\n",
      "[Training Epoch 1] Batch 161, Loss 0.3135974109172821\n",
      "[Training Epoch 1] Batch 162, Loss 0.3278462886810303\n",
      "[Training Epoch 1] Batch 163, Loss 0.33123570680618286\n",
      "[Training Epoch 1] Batch 164, Loss 0.28259244561195374\n",
      "[Training Epoch 1] Batch 165, Loss 0.3260665237903595\n",
      "[Training Epoch 1] Batch 166, Loss 0.304385244846344\n",
      "[Training Epoch 1] Batch 167, Loss 0.32516714930534363\n",
      "[Training Epoch 1] Batch 168, Loss 0.31017109751701355\n",
      "[Training Epoch 1] Batch 169, Loss 0.30628323554992676\n",
      "[Training Epoch 1] Batch 170, Loss 0.3098753094673157\n",
      "[Training Epoch 1] Batch 171, Loss 0.3167392313480377\n",
      "[Training Epoch 1] Batch 172, Loss 0.3060416281223297\n",
      "[Training Epoch 1] Batch 173, Loss 0.3236377239227295\n",
      "[Training Epoch 1] Batch 174, Loss 0.31670287251472473\n",
      "[Training Epoch 1] Batch 175, Loss 0.30765432119369507\n",
      "[Training Epoch 1] Batch 176, Loss 0.2970413267612457\n",
      "[Training Epoch 1] Batch 177, Loss 0.32633957266807556\n",
      "[Training Epoch 1] Batch 178, Loss 0.3196471631526947\n",
      "[Training Epoch 1] Batch 179, Loss 0.3043789267539978\n",
      "[Training Epoch 1] Batch 180, Loss 0.3023912310600281\n",
      "[Training Epoch 1] Batch 181, Loss 0.3115532696247101\n",
      "[Training Epoch 1] Batch 182, Loss 0.2954877018928528\n",
      "[Training Epoch 1] Batch 183, Loss 0.32288315892219543\n",
      "[Training Epoch 1] Batch 184, Loss 0.35712000727653503\n",
      "[Training Epoch 1] Batch 185, Loss 0.32537132501602173\n",
      "[Training Epoch 1] Batch 186, Loss 0.31167709827423096\n",
      "[Training Epoch 1] Batch 187, Loss 0.2894659638404846\n",
      "[Training Epoch 1] Batch 188, Loss 0.30769532918930054\n",
      "[Training Epoch 1] Batch 189, Loss 0.31179073452949524\n",
      "[Training Epoch 1] Batch 190, Loss 0.3001351058483124\n",
      "[Training Epoch 1] Batch 191, Loss 0.3097535967826843\n",
      "[Training Epoch 1] Batch 192, Loss 0.3201484978199005\n",
      "[Training Epoch 1] Batch 193, Loss 0.33816277980804443\n",
      "[Training Epoch 1] Batch 194, Loss 0.2997848391532898\n",
      "[Training Epoch 1] Batch 195, Loss 0.3291079103946686\n",
      "[Training Epoch 1] Batch 196, Loss 0.3090117871761322\n",
      "[Training Epoch 1] Batch 197, Loss 0.33162564039230347\n",
      "[Training Epoch 1] Batch 198, Loss 0.32608091831207275\n",
      "[Training Epoch 1] Batch 199, Loss 0.3080003261566162\n",
      "[Training Epoch 1] Batch 200, Loss 0.31195497512817383\n",
      "[Training Epoch 1] Batch 201, Loss 0.35726451873779297\n",
      "[Training Epoch 1] Batch 202, Loss 0.3429116904735565\n",
      "[Training Epoch 1] Batch 203, Loss 0.30824556946754456\n",
      "[Training Epoch 1] Batch 204, Loss 0.3186019957065582\n",
      "[Training Epoch 1] Batch 205, Loss 0.30905991792678833\n",
      "[Training Epoch 1] Batch 206, Loss 0.3245781660079956\n",
      "[Training Epoch 1] Batch 207, Loss 0.32777807116508484\n",
      "[Training Epoch 1] Batch 208, Loss 0.3402484655380249\n",
      "[Training Epoch 1] Batch 209, Loss 0.2870844602584839\n",
      "[Training Epoch 1] Batch 210, Loss 0.30755794048309326\n",
      "[Training Epoch 1] Batch 211, Loss 0.2952396869659424\n",
      "[Training Epoch 1] Batch 212, Loss 0.34679603576660156\n",
      "[Training Epoch 1] Batch 213, Loss 0.3202412724494934\n",
      "[Training Epoch 1] Batch 214, Loss 0.33446764945983887\n",
      "[Training Epoch 1] Batch 215, Loss 0.28510814905166626\n",
      "[Training Epoch 1] Batch 216, Loss 0.3031539022922516\n",
      "[Training Epoch 1] Batch 217, Loss 0.3250257968902588\n",
      "[Training Epoch 1] Batch 218, Loss 0.3042867183685303\n",
      "[Training Epoch 1] Batch 219, Loss 0.31148016452789307\n",
      "[Training Epoch 1] Batch 220, Loss 0.34662166237831116\n",
      "[Training Epoch 1] Batch 221, Loss 0.31961339712142944\n",
      "[Training Epoch 1] Batch 222, Loss 0.33560097217559814\n",
      "[Training Epoch 1] Batch 223, Loss 0.3258497714996338\n",
      "[Training Epoch 1] Batch 224, Loss 0.32172954082489014\n",
      "[Training Epoch 1] Batch 225, Loss 0.281186580657959\n",
      "[Training Epoch 1] Batch 226, Loss 0.32330265641212463\n",
      "[Training Epoch 1] Batch 227, Loss 0.3254021108150482\n",
      "[Training Epoch 1] Batch 228, Loss 0.30956533551216125\n",
      "[Training Epoch 1] Batch 229, Loss 0.2940785586833954\n",
      "[Training Epoch 1] Batch 230, Loss 0.3161464333534241\n",
      "[Training Epoch 1] Batch 231, Loss 0.3214149475097656\n",
      "[Training Epoch 1] Batch 232, Loss 0.32359403371810913\n",
      "[Training Epoch 1] Batch 233, Loss 0.36346006393432617\n",
      "[Training Epoch 1] Batch 234, Loss 0.2896226942539215\n",
      "[Training Epoch 1] Batch 235, Loss 0.30194568634033203\n",
      "[Training Epoch 1] Batch 236, Loss 0.31487876176834106\n",
      "[Training Epoch 1] Batch 237, Loss 0.31205618381500244\n",
      "[Training Epoch 1] Batch 238, Loss 0.2943410277366638\n",
      "[Training Epoch 1] Batch 239, Loss 0.30353203415870667\n",
      "[Training Epoch 1] Batch 240, Loss 0.31840455532073975\n",
      "[Training Epoch 1] Batch 241, Loss 0.29362422227859497\n",
      "[Training Epoch 1] Batch 242, Loss 0.29829347133636475\n",
      "[Training Epoch 1] Batch 243, Loss 0.30520135164260864\n",
      "[Training Epoch 1] Batch 244, Loss 0.32345274090766907\n",
      "[Training Epoch 1] Batch 245, Loss 0.328036367893219\n",
      "[Training Epoch 1] Batch 246, Loss 0.33312851190567017\n",
      "[Training Epoch 1] Batch 247, Loss 0.31043946743011475\n",
      "[Training Epoch 1] Batch 248, Loss 0.2956375181674957\n",
      "[Training Epoch 1] Batch 249, Loss 0.2896886467933655\n",
      "[Training Epoch 1] Batch 250, Loss 0.3054819703102112\n",
      "[Training Epoch 1] Batch 251, Loss 0.2988055348396301\n",
      "[Training Epoch 1] Batch 252, Loss 0.30398643016815186\n",
      "[Training Epoch 1] Batch 253, Loss 0.3061681389808655\n",
      "[Training Epoch 1] Batch 254, Loss 0.29521244764328003\n",
      "[Training Epoch 1] Batch 255, Loss 0.2824993133544922\n",
      "[Training Epoch 1] Batch 256, Loss 0.30440765619277954\n",
      "[Training Epoch 1] Batch 257, Loss 0.3419076204299927\n",
      "[Training Epoch 1] Batch 258, Loss 0.31696683168411255\n",
      "[Training Epoch 1] Batch 259, Loss 0.3126794397830963\n",
      "[Training Epoch 1] Batch 260, Loss 0.2980937957763672\n",
      "[Training Epoch 1] Batch 261, Loss 0.2993761897087097\n",
      "[Training Epoch 1] Batch 262, Loss 0.3201529383659363\n",
      "[Training Epoch 1] Batch 263, Loss 0.30388376116752625\n",
      "[Training Epoch 1] Batch 264, Loss 0.2981262505054474\n",
      "[Training Epoch 1] Batch 265, Loss 0.2881600856781006\n",
      "[Training Epoch 1] Batch 266, Loss 0.3082194924354553\n",
      "[Training Epoch 1] Batch 267, Loss 0.3149544596672058\n",
      "[Training Epoch 1] Batch 268, Loss 0.3521895110607147\n",
      "[Training Epoch 1] Batch 269, Loss 0.30498307943344116\n",
      "[Training Epoch 1] Batch 270, Loss 0.30968111753463745\n",
      "[Training Epoch 1] Batch 271, Loss 0.3093468248844147\n",
      "[Training Epoch 1] Batch 272, Loss 0.28531160950660706\n",
      "[Training Epoch 1] Batch 273, Loss 0.31183740496635437\n",
      "[Training Epoch 1] Batch 274, Loss 0.3203388452529907\n",
      "[Training Epoch 1] Batch 275, Loss 0.33085376024246216\n",
      "[Training Epoch 1] Batch 276, Loss 0.33188295364379883\n",
      "[Training Epoch 1] Batch 277, Loss 0.3126276731491089\n",
      "[Training Epoch 1] Batch 278, Loss 0.2942649722099304\n",
      "[Training Epoch 1] Batch 279, Loss 0.30941879749298096\n",
      "[Training Epoch 1] Batch 280, Loss 0.3453533351421356\n",
      "[Training Epoch 1] Batch 281, Loss 0.3622492551803589\n",
      "[Training Epoch 1] Batch 282, Loss 0.33826035261154175\n",
      "[Training Epoch 1] Batch 283, Loss 0.3398180902004242\n",
      "[Training Epoch 1] Batch 284, Loss 0.31089162826538086\n",
      "[Training Epoch 1] Batch 285, Loss 0.3207402229309082\n",
      "[Training Epoch 1] Batch 286, Loss 0.3488127589225769\n",
      "[Training Epoch 1] Batch 287, Loss 0.2957005798816681\n",
      "[Training Epoch 1] Batch 288, Loss 0.30503636598587036\n",
      "[Training Epoch 1] Batch 289, Loss 0.30571794509887695\n",
      "[Training Epoch 1] Batch 290, Loss 0.33302950859069824\n",
      "[Training Epoch 1] Batch 291, Loss 0.2996255159378052\n",
      "[Training Epoch 1] Batch 292, Loss 0.3193293511867523\n",
      "[Training Epoch 1] Batch 293, Loss 0.2977198362350464\n",
      "[Training Epoch 1] Batch 294, Loss 0.3028833866119385\n",
      "[Training Epoch 1] Batch 295, Loss 0.32070815563201904\n",
      "[Training Epoch 1] Batch 296, Loss 0.31256669759750366\n",
      "[Training Epoch 1] Batch 297, Loss 0.3252168297767639\n",
      "[Training Epoch 1] Batch 298, Loss 0.3244149684906006\n",
      "[Training Epoch 1] Batch 299, Loss 0.32438385486602783\n",
      "[Training Epoch 1] Batch 300, Loss 0.3191520869731903\n",
      "[Training Epoch 1] Batch 301, Loss 0.3048015832901001\n",
      "[Training Epoch 1] Batch 302, Loss 0.2991883158683777\n",
      "[Training Epoch 1] Batch 303, Loss 0.3434271514415741\n",
      "[Training Epoch 1] Batch 304, Loss 0.3118542730808258\n",
      "[Training Epoch 1] Batch 305, Loss 0.3419480323791504\n",
      "[Training Epoch 1] Batch 306, Loss 0.32461971044540405\n",
      "[Training Epoch 1] Batch 307, Loss 0.3108412027359009\n",
      "[Training Epoch 1] Batch 308, Loss 0.30003565549850464\n",
      "[Training Epoch 1] Batch 309, Loss 0.27861467003822327\n",
      "[Training Epoch 1] Batch 310, Loss 0.2948501706123352\n",
      "[Training Epoch 1] Batch 311, Loss 0.327078253030777\n",
      "[Training Epoch 1] Batch 312, Loss 0.33714181184768677\n",
      "[Training Epoch 1] Batch 313, Loss 0.29855817556381226\n",
      "[Training Epoch 1] Batch 314, Loss 0.30690181255340576\n",
      "[Training Epoch 1] Batch 315, Loss 0.32010021805763245\n",
      "[Training Epoch 1] Batch 316, Loss 0.3026926517486572\n",
      "[Training Epoch 1] Batch 317, Loss 0.3120368719100952\n",
      "[Training Epoch 1] Batch 318, Loss 0.30263859033584595\n",
      "[Training Epoch 1] Batch 319, Loss 0.3084648847579956\n",
      "[Training Epoch 1] Batch 320, Loss 0.33928900957107544\n",
      "[Training Epoch 1] Batch 321, Loss 0.3150596022605896\n",
      "[Training Epoch 1] Batch 322, Loss 0.3291253447532654\n",
      "[Training Epoch 1] Batch 323, Loss 0.28443920612335205\n",
      "[Training Epoch 1] Batch 324, Loss 0.31895455718040466\n",
      "[Training Epoch 1] Batch 325, Loss 0.33894455432891846\n",
      "[Training Epoch 1] Batch 326, Loss 0.31771546602249146\n",
      "[Training Epoch 1] Batch 327, Loss 0.29979294538497925\n",
      "[Training Epoch 1] Batch 328, Loss 0.31022030115127563\n",
      "[Training Epoch 1] Batch 329, Loss 0.27117300033569336\n",
      "[Training Epoch 1] Batch 330, Loss 0.2875785827636719\n",
      "[Training Epoch 1] Batch 331, Loss 0.3006768226623535\n",
      "[Training Epoch 1] Batch 332, Loss 0.30675777792930603\n",
      "[Training Epoch 1] Batch 333, Loss 0.32242244482040405\n",
      "[Training Epoch 1] Batch 334, Loss 0.32434597611427307\n",
      "[Training Epoch 1] Batch 335, Loss 0.3043819069862366\n",
      "[Training Epoch 1] Batch 336, Loss 0.285882830619812\n",
      "[Training Epoch 1] Batch 337, Loss 0.3477179706096649\n",
      "[Training Epoch 1] Batch 338, Loss 0.33726269006729126\n",
      "[Training Epoch 1] Batch 339, Loss 0.29606300592422485\n",
      "[Training Epoch 1] Batch 340, Loss 0.31319886445999146\n",
      "[Training Epoch 1] Batch 341, Loss 0.3128877580165863\n",
      "[Training Epoch 1] Batch 342, Loss 0.3153568506240845\n",
      "[Training Epoch 1] Batch 343, Loss 0.30434268712997437\n",
      "[Training Epoch 1] Batch 344, Loss 0.29985737800598145\n",
      "[Training Epoch 1] Batch 345, Loss 0.29158279299736023\n",
      "[Training Epoch 1] Batch 346, Loss 0.3099275529384613\n",
      "[Training Epoch 1] Batch 347, Loss 0.29879137873649597\n",
      "[Training Epoch 1] Batch 348, Loss 0.30919915437698364\n",
      "[Training Epoch 1] Batch 349, Loss 0.29983097314834595\n",
      "[Training Epoch 1] Batch 350, Loss 0.3398068845272064\n",
      "[Training Epoch 1] Batch 351, Loss 0.34806206822395325\n",
      "[Training Epoch 1] Batch 352, Loss 0.3237003982067108\n",
      "[Training Epoch 1] Batch 353, Loss 0.2808823585510254\n",
      "[Training Epoch 1] Batch 354, Loss 0.3031459450721741\n",
      "[Training Epoch 1] Batch 355, Loss 0.3076484799385071\n",
      "[Training Epoch 1] Batch 356, Loss 0.3186259865760803\n",
      "[Training Epoch 1] Batch 357, Loss 0.30450600385665894\n",
      "[Training Epoch 1] Batch 358, Loss 0.3111809492111206\n",
      "[Training Epoch 1] Batch 359, Loss 0.3145563006401062\n",
      "[Training Epoch 1] Batch 360, Loss 0.31890684366226196\n",
      "[Training Epoch 1] Batch 361, Loss 0.29847222566604614\n",
      "[Training Epoch 1] Batch 362, Loss 0.3093719482421875\n",
      "[Training Epoch 1] Batch 363, Loss 0.29173606634140015\n",
      "[Training Epoch 1] Batch 364, Loss 0.30797144770622253\n",
      "[Training Epoch 1] Batch 365, Loss 0.3181156516075134\n",
      "[Training Epoch 1] Batch 366, Loss 0.3036452531814575\n",
      "[Training Epoch 1] Batch 367, Loss 0.33205536007881165\n",
      "[Training Epoch 1] Batch 368, Loss 0.31372734904289246\n",
      "[Training Epoch 1] Batch 369, Loss 0.3110073208808899\n",
      "[Training Epoch 1] Batch 370, Loss 0.2941312789916992\n",
      "[Training Epoch 1] Batch 371, Loss 0.30460137128829956\n",
      "[Training Epoch 1] Batch 372, Loss 0.2965927720069885\n",
      "[Training Epoch 1] Batch 373, Loss 0.3034658432006836\n",
      "[Training Epoch 1] Batch 374, Loss 0.3478652834892273\n",
      "[Training Epoch 1] Batch 375, Loss 0.3255695402622223\n",
      "[Training Epoch 1] Batch 376, Loss 0.2988234758377075\n",
      "[Training Epoch 1] Batch 377, Loss 0.33131468296051025\n",
      "[Training Epoch 1] Batch 378, Loss 0.2907901406288147\n",
      "[Training Epoch 1] Batch 379, Loss 0.31339311599731445\n",
      "[Training Epoch 1] Batch 380, Loss 0.3180302381515503\n",
      "[Training Epoch 1] Batch 381, Loss 0.31834033131599426\n",
      "[Training Epoch 1] Batch 382, Loss 0.31548166275024414\n",
      "[Training Epoch 1] Batch 383, Loss 0.298955500125885\n",
      "[Training Epoch 1] Batch 384, Loss 0.32049378752708435\n",
      "[Training Epoch 1] Batch 385, Loss 0.3358851969242096\n",
      "[Training Epoch 1] Batch 386, Loss 0.3015499711036682\n",
      "[Training Epoch 1] Batch 387, Loss 0.29058295488357544\n",
      "[Training Epoch 1] Batch 388, Loss 0.302785724401474\n",
      "[Training Epoch 1] Batch 389, Loss 0.32753986120224\n",
      "[Training Epoch 1] Batch 390, Loss 0.3167288899421692\n",
      "[Training Epoch 1] Batch 391, Loss 0.30768656730651855\n",
      "[Training Epoch 1] Batch 392, Loss 0.31896767020225525\n",
      "[Training Epoch 1] Batch 393, Loss 0.31533950567245483\n",
      "[Training Epoch 1] Batch 394, Loss 0.2993772327899933\n",
      "[Training Epoch 1] Batch 395, Loss 0.30855971574783325\n",
      "[Training Epoch 1] Batch 396, Loss 0.33623600006103516\n",
      "[Training Epoch 1] Batch 397, Loss 0.2926598787307739\n",
      "[Training Epoch 1] Batch 398, Loss 0.3129993677139282\n",
      "[Training Epoch 1] Batch 399, Loss 0.31526899337768555\n",
      "[Training Epoch 1] Batch 400, Loss 0.32092559337615967\n",
      "[Training Epoch 1] Batch 401, Loss 0.30439144372940063\n",
      "[Training Epoch 1] Batch 402, Loss 0.3146589696407318\n",
      "[Training Epoch 1] Batch 403, Loss 0.3037945628166199\n",
      "[Training Epoch 1] Batch 404, Loss 0.32423728704452515\n",
      "[Training Epoch 1] Batch 405, Loss 0.33748847246170044\n",
      "[Training Epoch 1] Batch 406, Loss 0.30442970991134644\n",
      "[Training Epoch 1] Batch 407, Loss 0.3246668577194214\n",
      "[Training Epoch 1] Batch 408, Loss 0.2880473732948303\n",
      "[Training Epoch 1] Batch 409, Loss 0.32430773973464966\n",
      "[Training Epoch 1] Batch 410, Loss 0.3192746341228485\n",
      "[Training Epoch 1] Batch 411, Loss 0.31200799345970154\n",
      "[Training Epoch 1] Batch 412, Loss 0.2996768057346344\n",
      "[Training Epoch 1] Batch 413, Loss 0.29187124967575073\n",
      "[Training Epoch 1] Batch 414, Loss 0.33153635263442993\n",
      "[Training Epoch 1] Batch 415, Loss 0.31905385851860046\n",
      "[Training Epoch 1] Batch 416, Loss 0.30116963386535645\n",
      "[Training Epoch 1] Batch 417, Loss 0.3278235197067261\n",
      "[Training Epoch 1] Batch 418, Loss 0.33694398403167725\n",
      "[Training Epoch 1] Batch 419, Loss 0.31496286392211914\n",
      "[Training Epoch 1] Batch 420, Loss 0.30688726902008057\n",
      "[Training Epoch 1] Batch 421, Loss 0.2904967665672302\n",
      "[Training Epoch 1] Batch 422, Loss 0.33072441816329956\n",
      "[Training Epoch 1] Batch 423, Loss 0.3130238354206085\n",
      "[Training Epoch 1] Batch 424, Loss 0.3021984398365021\n",
      "[Training Epoch 1] Batch 425, Loss 0.2966762185096741\n",
      "[Training Epoch 1] Batch 426, Loss 0.3116537630558014\n",
      "[Training Epoch 1] Batch 427, Loss 0.3103349208831787\n",
      "[Training Epoch 1] Batch 428, Loss 0.3172740936279297\n",
      "[Training Epoch 1] Batch 429, Loss 0.32270699739456177\n",
      "[Training Epoch 1] Batch 430, Loss 0.29431378841400146\n",
      "[Training Epoch 1] Batch 431, Loss 0.3191453516483307\n",
      "[Training Epoch 1] Batch 432, Loss 0.293048620223999\n",
      "[Training Epoch 1] Batch 433, Loss 0.30380553007125854\n",
      "[Training Epoch 1] Batch 434, Loss 0.31215834617614746\n",
      "[Training Epoch 1] Batch 435, Loss 0.2963988184928894\n",
      "[Training Epoch 1] Batch 436, Loss 0.2982757091522217\n",
      "[Training Epoch 1] Batch 437, Loss 0.3040565848350525\n",
      "[Training Epoch 1] Batch 438, Loss 0.2938416004180908\n",
      "[Training Epoch 1] Batch 439, Loss 0.3139781653881073\n",
      "[Training Epoch 1] Batch 440, Loss 0.3309018313884735\n",
      "[Training Epoch 1] Batch 441, Loss 0.3154757618904114\n",
      "[Training Epoch 1] Batch 442, Loss 0.3022304177284241\n",
      "[Training Epoch 1] Batch 443, Loss 0.320576548576355\n",
      "[Training Epoch 1] Batch 444, Loss 0.30250728130340576\n",
      "[Training Epoch 1] Batch 445, Loss 0.31304028630256653\n",
      "[Training Epoch 1] Batch 446, Loss 0.2876104414463043\n",
      "[Training Epoch 1] Batch 447, Loss 0.3290787935256958\n",
      "[Training Epoch 1] Batch 448, Loss 0.3054683804512024\n",
      "[Training Epoch 1] Batch 449, Loss 0.3438025712966919\n",
      "[Training Epoch 1] Batch 450, Loss 0.32228946685791016\n",
      "[Training Epoch 1] Batch 451, Loss 0.30975502729415894\n",
      "[Training Epoch 1] Batch 452, Loss 0.3334939479827881\n",
      "[Training Epoch 1] Batch 453, Loss 0.30451205372810364\n",
      "[Training Epoch 1] Batch 454, Loss 0.2893783450126648\n",
      "[Training Epoch 1] Batch 455, Loss 0.27250975370407104\n",
      "[Training Epoch 1] Batch 456, Loss 0.3188324570655823\n",
      "[Training Epoch 1] Batch 457, Loss 0.28658831119537354\n",
      "[Training Epoch 1] Batch 458, Loss 0.31117573380470276\n",
      "[Training Epoch 1] Batch 459, Loss 0.3190082907676697\n",
      "[Training Epoch 1] Batch 460, Loss 0.2928006052970886\n",
      "[Training Epoch 1] Batch 461, Loss 0.31074026226997375\n",
      "[Training Epoch 1] Batch 462, Loss 0.29728469252586365\n",
      "[Training Epoch 1] Batch 463, Loss 0.32037776708602905\n",
      "[Training Epoch 1] Batch 464, Loss 0.34388554096221924\n",
      "[Training Epoch 1] Batch 465, Loss 0.3124105930328369\n",
      "[Training Epoch 1] Batch 466, Loss 0.32636725902557373\n",
      "[Training Epoch 1] Batch 467, Loss 0.31961050629615784\n",
      "[Training Epoch 1] Batch 468, Loss 0.2995271682739258\n",
      "[Training Epoch 1] Batch 469, Loss 0.31826362013816833\n",
      "[Training Epoch 1] Batch 470, Loss 0.297059565782547\n",
      "[Training Epoch 1] Batch 471, Loss 0.30191731452941895\n",
      "[Training Epoch 1] Batch 472, Loss 0.3228093981742859\n",
      "[Training Epoch 1] Batch 473, Loss 0.29421114921569824\n",
      "[Training Epoch 1] Batch 474, Loss 0.3088122606277466\n",
      "[Training Epoch 1] Batch 475, Loss 0.3043556809425354\n",
      "[Training Epoch 1] Batch 476, Loss 0.32307112216949463\n",
      "[Training Epoch 1] Batch 477, Loss 0.30804407596588135\n",
      "[Training Epoch 1] Batch 478, Loss 0.3201867938041687\n",
      "[Training Epoch 1] Batch 479, Loss 0.295041561126709\n",
      "[Training Epoch 1] Batch 480, Loss 0.3112356662750244\n",
      "[Training Epoch 1] Batch 481, Loss 0.3205451965332031\n",
      "[Training Epoch 1] Batch 482, Loss 0.28879886865615845\n",
      "[Training Epoch 1] Batch 483, Loss 0.29751700162887573\n",
      "[Training Epoch 1] Batch 484, Loss 0.31219929456710815\n",
      "[Training Epoch 1] Batch 485, Loss 0.3583523631095886\n",
      "[Training Epoch 1] Batch 486, Loss 0.3126469850540161\n",
      "[Training Epoch 1] Batch 487, Loss 0.31003403663635254\n",
      "[Training Epoch 1] Batch 488, Loss 0.33033329248428345\n",
      "[Training Epoch 1] Batch 489, Loss 0.3160472810268402\n",
      "[Training Epoch 1] Batch 490, Loss 0.31668999791145325\n",
      "[Training Epoch 1] Batch 491, Loss 0.3268277049064636\n",
      "[Training Epoch 1] Batch 492, Loss 0.3252600431442261\n",
      "[Training Epoch 1] Batch 493, Loss 0.33255714178085327\n",
      "[Training Epoch 1] Batch 494, Loss 0.2933109402656555\n",
      "[Training Epoch 1] Batch 495, Loss 0.33093464374542236\n",
      "[Training Epoch 1] Batch 496, Loss 0.3172231912612915\n",
      "[Training Epoch 1] Batch 497, Loss 0.30134183168411255\n",
      "[Training Epoch 1] Batch 498, Loss 0.32982927560806274\n",
      "[Training Epoch 1] Batch 499, Loss 0.30291298031806946\n",
      "[Training Epoch 1] Batch 500, Loss 0.31494665145874023\n",
      "[Training Epoch 1] Batch 501, Loss 0.3165961503982544\n",
      "[Training Epoch 1] Batch 502, Loss 0.32887357473373413\n",
      "[Training Epoch 1] Batch 503, Loss 0.28192585706710815\n",
      "[Training Epoch 1] Batch 504, Loss 0.340742826461792\n",
      "[Training Epoch 1] Batch 505, Loss 0.31302276253700256\n",
      "[Training Epoch 1] Batch 506, Loss 0.2944810390472412\n",
      "[Training Epoch 1] Batch 507, Loss 0.27287447452545166\n",
      "[Training Epoch 1] Batch 508, Loss 0.2887305021286011\n",
      "[Training Epoch 1] Batch 509, Loss 0.2926448583602905\n",
      "[Training Epoch 1] Batch 510, Loss 0.28941333293914795\n",
      "[Training Epoch 1] Batch 511, Loss 0.29717451333999634\n",
      "[Training Epoch 1] Batch 512, Loss 0.3017381429672241\n",
      "[Training Epoch 1] Batch 513, Loss 0.28559958934783936\n",
      "[Training Epoch 1] Batch 514, Loss 0.2961804270744324\n",
      "[Training Epoch 1] Batch 515, Loss 0.3207267224788666\n",
      "[Training Epoch 1] Batch 516, Loss 0.2957823872566223\n",
      "[Training Epoch 1] Batch 517, Loss 0.33194035291671753\n",
      "[Training Epoch 1] Batch 518, Loss 0.2997024655342102\n",
      "[Training Epoch 1] Batch 519, Loss 0.26268622279167175\n",
      "[Training Epoch 1] Batch 520, Loss 0.3151748478412628\n",
      "[Training Epoch 1] Batch 521, Loss 0.3136119246482849\n",
      "[Training Epoch 1] Batch 522, Loss 0.3054177761077881\n",
      "[Training Epoch 1] Batch 523, Loss 0.3203832507133484\n",
      "[Training Epoch 1] Batch 524, Loss 0.27196598052978516\n",
      "[Training Epoch 1] Batch 525, Loss 0.3126887083053589\n",
      "[Training Epoch 1] Batch 526, Loss 0.3009982109069824\n",
      "[Training Epoch 1] Batch 527, Loss 0.3083479106426239\n",
      "[Training Epoch 1] Batch 528, Loss 0.3120431900024414\n",
      "[Training Epoch 1] Batch 529, Loss 0.32331007719039917\n",
      "[Training Epoch 1] Batch 530, Loss 0.312335729598999\n",
      "[Training Epoch 1] Batch 531, Loss 0.29595762491226196\n",
      "[Training Epoch 1] Batch 532, Loss 0.29144978523254395\n",
      "[Training Epoch 1] Batch 533, Loss 0.3148229420185089\n",
      "[Training Epoch 1] Batch 534, Loss 0.3090684115886688\n",
      "[Training Epoch 1] Batch 535, Loss 0.3312314748764038\n",
      "[Training Epoch 1] Batch 536, Loss 0.30766281485557556\n",
      "[Training Epoch 1] Batch 537, Loss 0.31836214661598206\n",
      "[Training Epoch 1] Batch 538, Loss 0.2923773229122162\n",
      "[Training Epoch 1] Batch 539, Loss 0.3209538161754608\n",
      "[Training Epoch 1] Batch 540, Loss 0.33509910106658936\n",
      "[Training Epoch 1] Batch 541, Loss 0.3177635669708252\n",
      "[Training Epoch 1] Batch 542, Loss 0.31222259998321533\n",
      "[Training Epoch 1] Batch 543, Loss 0.3127390444278717\n",
      "[Training Epoch 1] Batch 544, Loss 0.30898481607437134\n",
      "[Training Epoch 1] Batch 545, Loss 0.31881165504455566\n",
      "[Training Epoch 1] Batch 546, Loss 0.35144945979118347\n",
      "[Training Epoch 1] Batch 547, Loss 0.3074604868888855\n",
      "[Training Epoch 1] Batch 548, Loss 0.3265782296657562\n",
      "[Training Epoch 1] Batch 549, Loss 0.3325321078300476\n",
      "[Training Epoch 1] Batch 550, Loss 0.3146328330039978\n",
      "[Training Epoch 1] Batch 551, Loss 0.29361334443092346\n",
      "[Training Epoch 1] Batch 552, Loss 0.2800271511077881\n",
      "[Training Epoch 1] Batch 553, Loss 0.2874628007411957\n",
      "[Training Epoch 1] Batch 554, Loss 0.29322391748428345\n",
      "[Training Epoch 1] Batch 555, Loss 0.32054492831230164\n",
      "[Training Epoch 1] Batch 556, Loss 0.31039178371429443\n",
      "[Training Epoch 1] Batch 557, Loss 0.31012728810310364\n",
      "[Training Epoch 1] Batch 558, Loss 0.2971186637878418\n",
      "[Training Epoch 1] Batch 559, Loss 0.30859464406967163\n",
      "[Training Epoch 1] Batch 560, Loss 0.3177087903022766\n",
      "[Training Epoch 1] Batch 561, Loss 0.30240657925605774\n",
      "[Training Epoch 1] Batch 562, Loss 0.3115277886390686\n",
      "[Training Epoch 1] Batch 563, Loss 0.3409828543663025\n",
      "[Training Epoch 1] Batch 564, Loss 0.3204460144042969\n",
      "[Training Epoch 1] Batch 565, Loss 0.35449492931365967\n",
      "[Training Epoch 1] Batch 566, Loss 0.33524683117866516\n",
      "[Training Epoch 1] Batch 567, Loss 0.28408104181289673\n",
      "[Training Epoch 1] Batch 568, Loss 0.3058699369430542\n",
      "[Training Epoch 1] Batch 569, Loss 0.3100622892379761\n",
      "[Training Epoch 1] Batch 570, Loss 0.31897518038749695\n",
      "[Training Epoch 1] Batch 571, Loss 0.3207302689552307\n",
      "[Training Epoch 1] Batch 572, Loss 0.31644007563591003\n",
      "[Training Epoch 1] Batch 573, Loss 0.3197110891342163\n",
      "[Training Epoch 1] Batch 574, Loss 0.3496937155723572\n",
      "[Training Epoch 1] Batch 575, Loss 0.3169054388999939\n",
      "[Training Epoch 1] Batch 576, Loss 0.316078245639801\n",
      "[Training Epoch 1] Batch 577, Loss 0.27093419432640076\n",
      "[Training Epoch 1] Batch 578, Loss 0.3118060231208801\n",
      "[Training Epoch 1] Batch 579, Loss 0.27024710178375244\n",
      "[Training Epoch 1] Batch 580, Loss 0.3238200843334198\n",
      "[Training Epoch 1] Batch 581, Loss 0.28802722692489624\n",
      "[Training Epoch 1] Batch 582, Loss 0.3046072721481323\n",
      "[Training Epoch 1] Batch 583, Loss 0.33294373750686646\n",
      "[Training Epoch 1] Batch 584, Loss 0.31252825260162354\n",
      "[Training Epoch 1] Batch 585, Loss 0.3237941563129425\n",
      "[Training Epoch 1] Batch 586, Loss 0.30797523260116577\n",
      "[Training Epoch 1] Batch 587, Loss 0.3144364655017853\n",
      "[Training Epoch 1] Batch 588, Loss 0.3002832531929016\n",
      "[Training Epoch 1] Batch 589, Loss 0.3014049530029297\n",
      "[Training Epoch 1] Batch 590, Loss 0.3207892179489136\n",
      "[Training Epoch 1] Batch 591, Loss 0.30639660358428955\n",
      "[Training Epoch 1] Batch 592, Loss 0.30725473165512085\n",
      "[Training Epoch 1] Batch 593, Loss 0.293728232383728\n",
      "[Training Epoch 1] Batch 594, Loss 0.30835145711898804\n",
      "[Training Epoch 1] Batch 595, Loss 0.34394705295562744\n",
      "[Training Epoch 1] Batch 596, Loss 0.31982773542404175\n",
      "[Training Epoch 1] Batch 597, Loss 0.3009691834449768\n",
      "[Training Epoch 1] Batch 598, Loss 0.3436342477798462\n",
      "[Training Epoch 1] Batch 599, Loss 0.31107139587402344\n",
      "[Training Epoch 1] Batch 600, Loss 0.32389557361602783\n",
      "[Training Epoch 1] Batch 601, Loss 0.3247646689414978\n",
      "[Training Epoch 1] Batch 602, Loss 0.33244240283966064\n",
      "[Training Epoch 1] Batch 603, Loss 0.27930396795272827\n",
      "[Training Epoch 1] Batch 604, Loss 0.3135492205619812\n",
      "[Training Epoch 1] Batch 605, Loss 0.3035251498222351\n",
      "[Training Epoch 1] Batch 606, Loss 0.301902711391449\n",
      "[Training Epoch 1] Batch 607, Loss 0.2933570146560669\n",
      "[Training Epoch 1] Batch 608, Loss 0.3107258677482605\n",
      "[Training Epoch 1] Batch 609, Loss 0.31642037630081177\n",
      "[Training Epoch 1] Batch 610, Loss 0.3058941662311554\n",
      "[Training Epoch 1] Batch 611, Loss 0.31488898396492004\n",
      "[Training Epoch 1] Batch 612, Loss 0.30236178636550903\n",
      "[Training Epoch 1] Batch 613, Loss 0.3241305947303772\n",
      "[Training Epoch 1] Batch 614, Loss 0.3259497284889221\n",
      "[Training Epoch 1] Batch 615, Loss 0.3123784065246582\n",
      "[Training Epoch 1] Batch 616, Loss 0.2975379228591919\n",
      "[Training Epoch 1] Batch 617, Loss 0.30723339319229126\n",
      "[Training Epoch 1] Batch 618, Loss 0.2795475721359253\n",
      "[Training Epoch 1] Batch 619, Loss 0.28960132598876953\n",
      "[Training Epoch 1] Batch 620, Loss 0.2742745578289032\n",
      "[Training Epoch 1] Batch 621, Loss 0.2980046272277832\n",
      "[Training Epoch 1] Batch 622, Loss 0.309440016746521\n",
      "[Training Epoch 1] Batch 623, Loss 0.335621178150177\n",
      "[Training Epoch 1] Batch 624, Loss 0.3091573119163513\n",
      "[Training Epoch 1] Batch 625, Loss 0.3237673342227936\n",
      "[Training Epoch 1] Batch 626, Loss 0.27625054121017456\n",
      "[Training Epoch 1] Batch 627, Loss 0.3227602243423462\n",
      "[Training Epoch 1] Batch 628, Loss 0.3344687521457672\n",
      "[Training Epoch 1] Batch 629, Loss 0.2817328870296478\n",
      "[Training Epoch 1] Batch 630, Loss 0.29051315784454346\n",
      "[Training Epoch 1] Batch 631, Loss 0.3209521174430847\n",
      "[Training Epoch 1] Batch 632, Loss 0.31034135818481445\n",
      "[Training Epoch 1] Batch 633, Loss 0.29408133029937744\n",
      "[Training Epoch 1] Batch 634, Loss 0.3155190646648407\n",
      "[Training Epoch 1] Batch 635, Loss 0.28527700901031494\n",
      "[Training Epoch 1] Batch 636, Loss 0.31827157735824585\n",
      "[Training Epoch 1] Batch 637, Loss 0.3132435381412506\n",
      "[Training Epoch 1] Batch 638, Loss 0.27915167808532715\n",
      "[Training Epoch 1] Batch 639, Loss 0.301241010427475\n",
      "[Training Epoch 1] Batch 640, Loss 0.32433125376701355\n",
      "[Training Epoch 1] Batch 641, Loss 0.3029884099960327\n",
      "[Training Epoch 1] Batch 642, Loss 0.31523221731185913\n",
      "[Training Epoch 1] Batch 643, Loss 0.3092561960220337\n",
      "[Training Epoch 1] Batch 644, Loss 0.30196845531463623\n",
      "[Training Epoch 1] Batch 645, Loss 0.310718297958374\n",
      "[Training Epoch 1] Batch 646, Loss 0.30899134278297424\n",
      "[Training Epoch 1] Batch 647, Loss 0.350017249584198\n",
      "[Training Epoch 1] Batch 648, Loss 0.3189755082130432\n",
      "[Training Epoch 1] Batch 649, Loss 0.3021250069141388\n",
      "[Training Epoch 1] Batch 650, Loss 0.30432918667793274\n",
      "[Training Epoch 1] Batch 651, Loss 0.3122621178627014\n",
      "[Training Epoch 1] Batch 652, Loss 0.32740017771720886\n",
      "[Training Epoch 1] Batch 653, Loss 0.3095515966415405\n",
      "[Training Epoch 1] Batch 654, Loss 0.28493863344192505\n",
      "[Training Epoch 1] Batch 655, Loss 0.32879745960235596\n",
      "[Training Epoch 1] Batch 656, Loss 0.27967143058776855\n",
      "[Training Epoch 1] Batch 657, Loss 0.29467207193374634\n",
      "[Training Epoch 1] Batch 658, Loss 0.34083276987075806\n",
      "[Training Epoch 1] Batch 659, Loss 0.26877403259277344\n",
      "[Training Epoch 1] Batch 660, Loss 0.2849023938179016\n",
      "[Training Epoch 1] Batch 661, Loss 0.31223946809768677\n",
      "[Training Epoch 1] Batch 662, Loss 0.3075696527957916\n",
      "[Training Epoch 1] Batch 663, Loss 0.2833627462387085\n",
      "[Training Epoch 1] Batch 664, Loss 0.31829971075057983\n",
      "[Training Epoch 1] Batch 665, Loss 0.31624096632003784\n",
      "[Training Epoch 1] Batch 666, Loss 0.3022508919239044\n",
      "[Training Epoch 1] Batch 667, Loss 0.33251214027404785\n",
      "[Training Epoch 1] Batch 668, Loss 0.2912009358406067\n",
      "[Training Epoch 1] Batch 669, Loss 0.3042214512825012\n",
      "[Training Epoch 1] Batch 670, Loss 0.305217444896698\n",
      "[Training Epoch 1] Batch 671, Loss 0.30651211738586426\n",
      "[Training Epoch 1] Batch 672, Loss 0.3141933083534241\n",
      "[Training Epoch 1] Batch 673, Loss 0.2957148849964142\n",
      "[Training Epoch 1] Batch 674, Loss 0.3139747679233551\n",
      "[Training Epoch 1] Batch 675, Loss 0.3080856204032898\n",
      "[Training Epoch 1] Batch 676, Loss 0.3369597792625427\n",
      "[Training Epoch 1] Batch 677, Loss 0.26461082696914673\n",
      "[Training Epoch 1] Batch 678, Loss 0.3093380033969879\n",
      "[Training Epoch 1] Batch 679, Loss 0.29934558272361755\n",
      "[Training Epoch 1] Batch 680, Loss 0.31358182430267334\n",
      "[Training Epoch 1] Batch 681, Loss 0.29013943672180176\n",
      "[Training Epoch 1] Batch 682, Loss 0.2978519797325134\n",
      "[Training Epoch 1] Batch 683, Loss 0.30028218030929565\n",
      "[Training Epoch 1] Batch 684, Loss 0.28983622789382935\n",
      "[Training Epoch 1] Batch 685, Loss 0.32408297061920166\n",
      "[Training Epoch 1] Batch 686, Loss 0.29960498213768005\n",
      "[Training Epoch 1] Batch 687, Loss 0.323854923248291\n",
      "[Training Epoch 1] Batch 688, Loss 0.30702269077301025\n",
      "[Training Epoch 1] Batch 689, Loss 0.33806660771369934\n",
      "[Training Epoch 1] Batch 690, Loss 0.29471081495285034\n",
      "[Training Epoch 1] Batch 691, Loss 0.3340829014778137\n",
      "[Training Epoch 1] Batch 692, Loss 0.3187766373157501\n",
      "[Training Epoch 1] Batch 693, Loss 0.3135295510292053\n",
      "[Training Epoch 1] Batch 694, Loss 0.27771735191345215\n",
      "[Training Epoch 1] Batch 695, Loss 0.2935771942138672\n",
      "[Training Epoch 1] Batch 696, Loss 0.2962222695350647\n",
      "[Training Epoch 1] Batch 697, Loss 0.34176722168922424\n",
      "[Training Epoch 1] Batch 698, Loss 0.31110680103302\n",
      "[Training Epoch 1] Batch 699, Loss 0.30314725637435913\n",
      "[Training Epoch 1] Batch 700, Loss 0.29019787907600403\n",
      "[Training Epoch 1] Batch 701, Loss 0.3113354444503784\n",
      "[Training Epoch 1] Batch 702, Loss 0.2804468870162964\n",
      "[Training Epoch 1] Batch 703, Loss 0.319835901260376\n",
      "[Training Epoch 1] Batch 704, Loss 0.2937679886817932\n",
      "[Training Epoch 1] Batch 705, Loss 0.3251694440841675\n",
      "[Training Epoch 1] Batch 706, Loss 0.324753999710083\n",
      "[Training Epoch 1] Batch 707, Loss 0.29888105392456055\n",
      "[Training Epoch 1] Batch 708, Loss 0.3147534728050232\n",
      "[Training Epoch 1] Batch 709, Loss 0.3301472067832947\n",
      "[Training Epoch 1] Batch 710, Loss 0.297153115272522\n",
      "[Training Epoch 1] Batch 711, Loss 0.31925541162490845\n",
      "[Training Epoch 1] Batch 712, Loss 0.2849277853965759\n",
      "[Training Epoch 1] Batch 713, Loss 0.3114670217037201\n",
      "[Training Epoch 1] Batch 714, Loss 0.303312748670578\n",
      "[Training Epoch 1] Batch 715, Loss 0.29067593812942505\n",
      "[Training Epoch 1] Batch 716, Loss 0.31468236446380615\n",
      "[Training Epoch 1] Batch 717, Loss 0.3195587992668152\n",
      "[Training Epoch 1] Batch 718, Loss 0.32732850313186646\n",
      "[Training Epoch 1] Batch 719, Loss 0.3143637478351593\n",
      "[Training Epoch 1] Batch 720, Loss 0.2962663173675537\n",
      "[Training Epoch 1] Batch 721, Loss 0.2965269684791565\n",
      "[Training Epoch 1] Batch 722, Loss 0.29668599367141724\n",
      "[Training Epoch 1] Batch 723, Loss 0.28124481439590454\n",
      "[Training Epoch 1] Batch 724, Loss 0.32825613021850586\n",
      "[Training Epoch 1] Batch 725, Loss 0.32015830278396606\n",
      "[Training Epoch 1] Batch 726, Loss 0.28503015637397766\n",
      "[Training Epoch 1] Batch 727, Loss 0.2991451919078827\n",
      "[Training Epoch 1] Batch 728, Loss 0.2912149429321289\n",
      "[Training Epoch 1] Batch 729, Loss 0.29231688380241394\n",
      "[Training Epoch 1] Batch 730, Loss 0.31588685512542725\n",
      "[Training Epoch 1] Batch 731, Loss 0.2973821759223938\n",
      "[Training Epoch 1] Batch 732, Loss 0.3108171820640564\n",
      "[Training Epoch 1] Batch 733, Loss 0.28866833448410034\n",
      "[Training Epoch 1] Batch 734, Loss 0.3306509256362915\n",
      "[Training Epoch 1] Batch 735, Loss 0.3033324182033539\n",
      "[Training Epoch 1] Batch 736, Loss 0.28502243757247925\n",
      "[Training Epoch 1] Batch 737, Loss 0.3030087351799011\n",
      "[Training Epoch 1] Batch 738, Loss 0.30666080117225647\n",
      "[Training Epoch 1] Batch 739, Loss 0.29371798038482666\n",
      "[Training Epoch 1] Batch 740, Loss 0.3192562460899353\n",
      "[Training Epoch 1] Batch 741, Loss 0.299685001373291\n",
      "[Training Epoch 1] Batch 742, Loss 0.307134211063385\n",
      "[Training Epoch 1] Batch 743, Loss 0.3002709746360779\n",
      "[Training Epoch 1] Batch 744, Loss 0.31681597232818604\n",
      "[Training Epoch 1] Batch 745, Loss 0.323901504278183\n",
      "[Training Epoch 1] Batch 746, Loss 0.3107989430427551\n",
      "[Training Epoch 1] Batch 747, Loss 0.32595324516296387\n",
      "[Training Epoch 1] Batch 748, Loss 0.27227747440338135\n",
      "[Training Epoch 1] Batch 749, Loss 0.3050222098827362\n",
      "[Training Epoch 1] Batch 750, Loss 0.3010636866092682\n",
      "[Training Epoch 1] Batch 751, Loss 0.2726210653781891\n",
      "[Training Epoch 1] Batch 752, Loss 0.30778807401657104\n",
      "[Training Epoch 1] Batch 753, Loss 0.3053334355354309\n",
      "[Training Epoch 1] Batch 754, Loss 0.2973478436470032\n",
      "[Training Epoch 1] Batch 755, Loss 0.285264253616333\n",
      "[Training Epoch 1] Batch 756, Loss 0.3092098832130432\n",
      "[Training Epoch 1] Batch 757, Loss 0.32165080308914185\n",
      "[Training Epoch 1] Batch 758, Loss 0.3127671480178833\n",
      "[Training Epoch 1] Batch 759, Loss 0.2911455035209656\n",
      "[Training Epoch 1] Batch 760, Loss 0.33041149377822876\n",
      "[Training Epoch 1] Batch 761, Loss 0.3035740554332733\n",
      "[Training Epoch 1] Batch 762, Loss 0.32244551181793213\n",
      "[Training Epoch 1] Batch 763, Loss 0.29812178015708923\n",
      "[Training Epoch 1] Batch 764, Loss 0.3202725648880005\n",
      "[Training Epoch 1] Batch 765, Loss 0.2863250970840454\n",
      "[Training Epoch 1] Batch 766, Loss 0.29920482635498047\n",
      "[Training Epoch 1] Batch 767, Loss 0.3150729835033417\n",
      "[Training Epoch 1] Batch 768, Loss 0.28845512866973877\n",
      "[Training Epoch 1] Batch 769, Loss 0.32222461700439453\n",
      "[Training Epoch 1] Batch 770, Loss 0.30207663774490356\n",
      "[Training Epoch 1] Batch 771, Loss 0.29699647426605225\n",
      "[Training Epoch 1] Batch 772, Loss 0.3096315860748291\n",
      "[Training Epoch 1] Batch 773, Loss 0.29115405678749084\n",
      "[Training Epoch 1] Batch 774, Loss 0.2920101583003998\n",
      "[Training Epoch 1] Batch 775, Loss 0.31061336398124695\n",
      "[Training Epoch 1] Batch 776, Loss 0.3304537534713745\n",
      "[Training Epoch 1] Batch 777, Loss 0.27456098794937134\n",
      "[Training Epoch 1] Batch 778, Loss 0.31091558933258057\n",
      "[Training Epoch 1] Batch 779, Loss 0.30317047238349915\n",
      "[Training Epoch 1] Batch 780, Loss 0.3142404556274414\n",
      "[Training Epoch 1] Batch 781, Loss 0.28872209787368774\n",
      "[Training Epoch 1] Batch 782, Loss 0.3390893042087555\n",
      "[Training Epoch 1] Batch 783, Loss 0.31189242005348206\n",
      "[Training Epoch 1] Batch 784, Loss 0.2837632894515991\n",
      "[Training Epoch 1] Batch 785, Loss 0.28189170360565186\n",
      "[Training Epoch 1] Batch 786, Loss 0.31108957529067993\n",
      "[Training Epoch 1] Batch 787, Loss 0.32790637016296387\n",
      "[Training Epoch 1] Batch 788, Loss 0.2829885482788086\n",
      "[Training Epoch 1] Batch 789, Loss 0.2900049686431885\n",
      "[Training Epoch 1] Batch 790, Loss 0.30926382541656494\n",
      "[Training Epoch 1] Batch 791, Loss 0.32418394088745117\n",
      "[Training Epoch 1] Batch 792, Loss 0.2967108488082886\n",
      "[Training Epoch 1] Batch 793, Loss 0.29976940155029297\n",
      "[Training Epoch 1] Batch 794, Loss 0.3207438290119171\n",
      "[Training Epoch 1] Batch 795, Loss 0.32900673151016235\n",
      "[Training Epoch 1] Batch 796, Loss 0.2878420948982239\n",
      "[Training Epoch 1] Batch 797, Loss 0.26447784900665283\n",
      "[Training Epoch 1] Batch 798, Loss 0.3163456320762634\n",
      "[Training Epoch 1] Batch 799, Loss 0.350810170173645\n",
      "[Training Epoch 1] Batch 800, Loss 0.29423797130584717\n",
      "[Training Epoch 1] Batch 801, Loss 0.3229660987854004\n",
      "[Training Epoch 1] Batch 802, Loss 0.3197174072265625\n",
      "[Training Epoch 1] Batch 803, Loss 0.3096086084842682\n",
      "[Training Epoch 1] Batch 804, Loss 0.32367703318595886\n",
      "[Training Epoch 1] Batch 805, Loss 0.3149133324623108\n",
      "[Training Epoch 1] Batch 806, Loss 0.3037991523742676\n",
      "[Training Epoch 1] Batch 807, Loss 0.3133334517478943\n",
      "[Training Epoch 1] Batch 808, Loss 0.2982219457626343\n",
      "[Training Epoch 1] Batch 809, Loss 0.3265429437160492\n",
      "[Training Epoch 1] Batch 810, Loss 0.3209525942802429\n",
      "[Training Epoch 1] Batch 811, Loss 0.29135632514953613\n",
      "[Training Epoch 1] Batch 812, Loss 0.2871904969215393\n",
      "[Training Epoch 1] Batch 813, Loss 0.32872581481933594\n",
      "[Training Epoch 1] Batch 814, Loss 0.3018958866596222\n",
      "[Training Epoch 1] Batch 815, Loss 0.30872541666030884\n",
      "[Training Epoch 1] Batch 816, Loss 0.2916238605976105\n",
      "[Training Epoch 1] Batch 817, Loss 0.32861456274986267\n",
      "[Training Epoch 1] Batch 818, Loss 0.3142896890640259\n",
      "[Training Epoch 1] Batch 819, Loss 0.3241400420665741\n",
      "[Training Epoch 1] Batch 820, Loss 0.32520291209220886\n",
      "[Training Epoch 1] Batch 821, Loss 0.29442155361175537\n",
      "[Training Epoch 1] Batch 822, Loss 0.3145208954811096\n",
      "[Training Epoch 1] Batch 823, Loss 0.29368847608566284\n",
      "[Training Epoch 1] Batch 824, Loss 0.31630492210388184\n",
      "[Training Epoch 1] Batch 825, Loss 0.32214194536209106\n",
      "[Training Epoch 1] Batch 826, Loss 0.300689697265625\n",
      "[Training Epoch 1] Batch 827, Loss 0.308475136756897\n",
      "[Training Epoch 1] Batch 828, Loss 0.29208996891975403\n",
      "[Training Epoch 1] Batch 829, Loss 0.3145274519920349\n",
      "[Training Epoch 1] Batch 830, Loss 0.2864891290664673\n",
      "[Training Epoch 1] Batch 831, Loss 0.3339961767196655\n",
      "[Training Epoch 1] Batch 832, Loss 0.2874191105365753\n",
      "[Training Epoch 1] Batch 833, Loss 0.321130633354187\n",
      "[Training Epoch 1] Batch 834, Loss 0.3337424397468567\n",
      "[Training Epoch 1] Batch 835, Loss 0.33224809169769287\n",
      "[Training Epoch 1] Batch 836, Loss 0.2996917963027954\n",
      "[Training Epoch 1] Batch 837, Loss 0.3046504259109497\n",
      "[Training Epoch 1] Batch 838, Loss 0.33554255962371826\n",
      "[Training Epoch 1] Batch 839, Loss 0.3307259678840637\n",
      "[Training Epoch 1] Batch 840, Loss 0.3189561069011688\n",
      "[Training Epoch 1] Batch 841, Loss 0.3148000240325928\n",
      "[Training Epoch 1] Batch 842, Loss 0.30719757080078125\n",
      "[Training Epoch 1] Batch 843, Loss 0.34253358840942383\n",
      "[Training Epoch 1] Batch 844, Loss 0.3316859006881714\n",
      "[Training Epoch 1] Batch 845, Loss 0.31591081619262695\n",
      "[Training Epoch 1] Batch 846, Loss 0.3072196841239929\n",
      "[Training Epoch 1] Batch 847, Loss 0.2963929772377014\n",
      "[Training Epoch 1] Batch 848, Loss 0.3017289340496063\n",
      "[Training Epoch 1] Batch 849, Loss 0.3317227363586426\n",
      "[Training Epoch 1] Batch 850, Loss 0.3105272352695465\n",
      "[Training Epoch 1] Batch 851, Loss 0.3405791223049164\n",
      "[Training Epoch 1] Batch 852, Loss 0.34114623069763184\n",
      "[Training Epoch 1] Batch 853, Loss 0.29379379749298096\n",
      "[Training Epoch 1] Batch 854, Loss 0.28025108575820923\n",
      "[Training Epoch 1] Batch 855, Loss 0.2960083484649658\n",
      "[Training Epoch 1] Batch 856, Loss 0.3120613098144531\n",
      "[Training Epoch 1] Batch 857, Loss 0.3198823630809784\n",
      "[Training Epoch 1] Batch 858, Loss 0.28528720140457153\n",
      "[Training Epoch 1] Batch 859, Loss 0.3393127918243408\n",
      "[Training Epoch 1] Batch 860, Loss 0.31336450576782227\n",
      "[Training Epoch 1] Batch 861, Loss 0.32425233721733093\n",
      "[Training Epoch 1] Batch 862, Loss 0.2909359335899353\n",
      "[Training Epoch 1] Batch 863, Loss 0.30406516790390015\n",
      "[Training Epoch 1] Batch 864, Loss 0.31223177909851074\n",
      "[Training Epoch 1] Batch 865, Loss 0.31791985034942627\n",
      "[Training Epoch 1] Batch 866, Loss 0.3310312032699585\n",
      "[Training Epoch 1] Batch 867, Loss 0.32431524991989136\n",
      "[Training Epoch 1] Batch 868, Loss 0.3014029562473297\n",
      "[Training Epoch 1] Batch 869, Loss 0.30472880601882935\n",
      "[Training Epoch 1] Batch 870, Loss 0.30125129222869873\n",
      "[Training Epoch 1] Batch 871, Loss 0.3204745054244995\n",
      "[Training Epoch 1] Batch 872, Loss 0.3221897482872009\n",
      "[Training Epoch 1] Batch 873, Loss 0.2892921566963196\n",
      "[Training Epoch 1] Batch 874, Loss 0.29216477274894714\n",
      "[Training Epoch 1] Batch 875, Loss 0.3122445344924927\n",
      "[Training Epoch 1] Batch 876, Loss 0.30215832591056824\n",
      "[Training Epoch 1] Batch 877, Loss 0.28364771604537964\n",
      "[Training Epoch 1] Batch 878, Loss 0.3060993552207947\n",
      "[Training Epoch 1] Batch 879, Loss 0.3237757682800293\n",
      "[Training Epoch 1] Batch 880, Loss 0.31808000802993774\n",
      "[Training Epoch 1] Batch 881, Loss 0.292961061000824\n",
      "[Training Epoch 1] Batch 882, Loss 0.3295102119445801\n",
      "[Training Epoch 1] Batch 883, Loss 0.31101781129837036\n",
      "[Training Epoch 1] Batch 884, Loss 0.3299383521080017\n",
      "[Training Epoch 1] Batch 885, Loss 0.31145182251930237\n",
      "[Training Epoch 1] Batch 886, Loss 0.3281267285346985\n",
      "[Training Epoch 1] Batch 887, Loss 0.30878889560699463\n",
      "[Training Epoch 1] Batch 888, Loss 0.3083627223968506\n",
      "[Training Epoch 1] Batch 889, Loss 0.29724204540252686\n",
      "[Training Epoch 1] Batch 890, Loss 0.32132089138031006\n",
      "[Training Epoch 1] Batch 891, Loss 0.28970420360565186\n",
      "[Training Epoch 1] Batch 892, Loss 0.33463042974472046\n",
      "[Training Epoch 1] Batch 893, Loss 0.2829124927520752\n",
      "[Training Epoch 1] Batch 894, Loss 0.3243491053581238\n",
      "[Training Epoch 1] Batch 895, Loss 0.30414947867393494\n",
      "[Training Epoch 1] Batch 896, Loss 0.3195316791534424\n",
      "[Training Epoch 1] Batch 897, Loss 0.288226842880249\n",
      "[Training Epoch 1] Batch 898, Loss 0.29912805557250977\n",
      "[Training Epoch 1] Batch 899, Loss 0.31761035323143005\n",
      "[Training Epoch 1] Batch 900, Loss 0.27702680230140686\n",
      "[Training Epoch 1] Batch 901, Loss 0.3116644024848938\n",
      "[Training Epoch 1] Batch 902, Loss 0.30987489223480225\n",
      "[Training Epoch 1] Batch 903, Loss 0.2976900339126587\n",
      "[Training Epoch 1] Batch 904, Loss 0.32395434379577637\n",
      "[Training Epoch 1] Batch 905, Loss 0.303583562374115\n",
      "[Training Epoch 1] Batch 906, Loss 0.3620092272758484\n",
      "[Training Epoch 1] Batch 907, Loss 0.30660712718963623\n",
      "[Training Epoch 1] Batch 908, Loss 0.3194432854652405\n",
      "[Training Epoch 1] Batch 909, Loss 0.3306597173213959\n",
      "[Training Epoch 1] Batch 910, Loss 0.348305881023407\n",
      "[Training Epoch 1] Batch 911, Loss 0.32460731267929077\n",
      "[Training Epoch 1] Batch 912, Loss 0.2814532518386841\n",
      "[Training Epoch 1] Batch 913, Loss 0.3146705627441406\n",
      "[Training Epoch 1] Batch 914, Loss 0.3033541440963745\n",
      "[Training Epoch 1] Batch 915, Loss 0.317572683095932\n",
      "[Training Epoch 1] Batch 916, Loss 0.31202149391174316\n",
      "[Training Epoch 1] Batch 917, Loss 0.2980688810348511\n",
      "[Training Epoch 1] Batch 918, Loss 0.3158971667289734\n",
      "[Training Epoch 1] Batch 919, Loss 0.29006046056747437\n",
      "[Training Epoch 1] Batch 920, Loss 0.3002752661705017\n",
      "[Training Epoch 1] Batch 921, Loss 0.3051052689552307\n",
      "[Training Epoch 1] Batch 922, Loss 0.3349496126174927\n",
      "[Training Epoch 1] Batch 923, Loss 0.3166503310203552\n",
      "[Training Epoch 1] Batch 924, Loss 0.2957967519760132\n",
      "[Training Epoch 1] Batch 925, Loss 0.29948484897613525\n",
      "[Training Epoch 1] Batch 926, Loss 0.3001294732093811\n",
      "[Training Epoch 1] Batch 927, Loss 0.3041080832481384\n",
      "[Training Epoch 1] Batch 928, Loss 0.2908676266670227\n",
      "[Training Epoch 1] Batch 929, Loss 0.2908536195755005\n",
      "[Training Epoch 1] Batch 930, Loss 0.2843649685382843\n",
      "[Training Epoch 1] Batch 931, Loss 0.28682124614715576\n",
      "[Training Epoch 1] Batch 932, Loss 0.3193822205066681\n",
      "[Training Epoch 1] Batch 933, Loss 0.34497031569480896\n",
      "[Training Epoch 1] Batch 934, Loss 0.3118903636932373\n",
      "[Training Epoch 1] Batch 935, Loss 0.3602966368198395\n",
      "[Training Epoch 1] Batch 936, Loss 0.28887939453125\n",
      "[Training Epoch 1] Batch 937, Loss 0.32552143931388855\n",
      "[Training Epoch 1] Batch 938, Loss 0.29890623688697815\n",
      "[Training Epoch 1] Batch 939, Loss 0.29703783988952637\n",
      "[Training Epoch 1] Batch 940, Loss 0.3370997905731201\n",
      "[Training Epoch 1] Batch 941, Loss 0.3217087984085083\n",
      "[Training Epoch 1] Batch 942, Loss 0.32516300678253174\n",
      "[Training Epoch 1] Batch 943, Loss 0.3001292049884796\n",
      "[Training Epoch 1] Batch 944, Loss 0.3091961741447449\n",
      "[Training Epoch 1] Batch 945, Loss 0.314708411693573\n",
      "[Training Epoch 1] Batch 946, Loss 0.3070484697818756\n",
      "[Training Epoch 1] Batch 947, Loss 0.3015385866165161\n",
      "[Training Epoch 1] Batch 948, Loss 0.30042311549186707\n",
      "[Training Epoch 1] Batch 949, Loss 0.29886192083358765\n",
      "[Training Epoch 1] Batch 950, Loss 0.2940073311328888\n",
      "[Training Epoch 1] Batch 951, Loss 0.29527080059051514\n",
      "[Training Epoch 1] Batch 952, Loss 0.3060225546360016\n",
      "[Training Epoch 1] Batch 953, Loss 0.30888453125953674\n",
      "[Training Epoch 1] Batch 954, Loss 0.2715306580066681\n",
      "[Training Epoch 1] Batch 955, Loss 0.2896276116371155\n",
      "[Training Epoch 1] Batch 956, Loss 0.27684256434440613\n",
      "[Training Epoch 1] Batch 957, Loss 0.3052966892719269\n",
      "[Training Epoch 1] Batch 958, Loss 0.32304784655570984\n",
      "[Training Epoch 1] Batch 959, Loss 0.3296549916267395\n",
      "[Training Epoch 1] Batch 960, Loss 0.29105281829833984\n",
      "[Training Epoch 1] Batch 961, Loss 0.3347061276435852\n",
      "[Training Epoch 1] Batch 962, Loss 0.3098371624946594\n",
      "[Training Epoch 1] Batch 963, Loss 0.30348169803619385\n",
      "[Training Epoch 1] Batch 964, Loss 0.32451876997947693\n",
      "[Training Epoch 1] Batch 965, Loss 0.27341219782829285\n",
      "[Training Epoch 1] Batch 966, Loss 0.30548039078712463\n",
      "[Training Epoch 1] Batch 967, Loss 0.293931782245636\n",
      "[Training Epoch 1] Batch 968, Loss 0.2898138165473938\n",
      "[Training Epoch 1] Batch 969, Loss 0.32794588804244995\n",
      "[Training Epoch 1] Batch 970, Loss 0.28801846504211426\n",
      "[Training Epoch 1] Batch 971, Loss 0.30252498388290405\n",
      "[Training Epoch 1] Batch 972, Loss 0.2996177077293396\n",
      "[Training Epoch 1] Batch 973, Loss 0.3313179016113281\n",
      "[Training Epoch 1] Batch 974, Loss 0.30623239278793335\n",
      "[Training Epoch 1] Batch 975, Loss 0.3258230686187744\n",
      "[Training Epoch 1] Batch 976, Loss 0.3048100471496582\n",
      "[Training Epoch 1] Batch 977, Loss 0.3155352473258972\n",
      "[Training Epoch 1] Batch 978, Loss 0.32055792212486267\n",
      "[Training Epoch 1] Batch 979, Loss 0.31474173069000244\n",
      "[Training Epoch 1] Batch 980, Loss 0.2956225275993347\n",
      "[Training Epoch 1] Batch 981, Loss 0.2731938362121582\n",
      "[Training Epoch 1] Batch 982, Loss 0.31587159633636475\n",
      "[Training Epoch 1] Batch 983, Loss 0.3093969225883484\n",
      "[Training Epoch 1] Batch 984, Loss 0.32693466544151306\n",
      "[Training Epoch 1] Batch 985, Loss 0.31641650199890137\n",
      "[Training Epoch 1] Batch 986, Loss 0.3122526705265045\n",
      "[Training Epoch 1] Batch 987, Loss 0.2955571115016937\n",
      "[Training Epoch 1] Batch 988, Loss 0.2972587049007416\n",
      "[Training Epoch 1] Batch 989, Loss 0.3064880073070526\n",
      "[Training Epoch 1] Batch 990, Loss 0.33289051055908203\n",
      "[Training Epoch 1] Batch 991, Loss 0.3104068338871002\n",
      "[Training Epoch 1] Batch 992, Loss 0.29924413561820984\n",
      "[Training Epoch 1] Batch 993, Loss 0.29964756965637207\n",
      "[Training Epoch 1] Batch 994, Loss 0.3652978539466858\n",
      "[Training Epoch 1] Batch 995, Loss 0.33150655031204224\n",
      "[Training Epoch 1] Batch 996, Loss 0.321916788816452\n",
      "[Training Epoch 1] Batch 997, Loss 0.3185994625091553\n",
      "[Training Epoch 1] Batch 998, Loss 0.3065398335456848\n",
      "[Training Epoch 1] Batch 999, Loss 0.3073757290840149\n",
      "[Training Epoch 1] Batch 1000, Loss 0.2989889979362488\n",
      "[Training Epoch 1] Batch 1001, Loss 0.3022065758705139\n",
      "[Training Epoch 1] Batch 1002, Loss 0.2937104105949402\n",
      "[Training Epoch 1] Batch 1003, Loss 0.3002745509147644\n",
      "[Training Epoch 1] Batch 1004, Loss 0.33612608909606934\n",
      "[Training Epoch 1] Batch 1005, Loss 0.32626181840896606\n",
      "[Training Epoch 1] Batch 1006, Loss 0.312539279460907\n",
      "[Training Epoch 1] Batch 1007, Loss 0.32659202814102173\n",
      "[Training Epoch 1] Batch 1008, Loss 0.3275502324104309\n",
      "[Training Epoch 1] Batch 1009, Loss 0.32652872800827026\n",
      "[Training Epoch 1] Batch 1010, Loss 0.3213289976119995\n",
      "[Training Epoch 1] Batch 1011, Loss 0.27805066108703613\n",
      "[Training Epoch 1] Batch 1012, Loss 0.3080565929412842\n",
      "[Training Epoch 1] Batch 1013, Loss 0.3094196319580078\n",
      "[Training Epoch 1] Batch 1014, Loss 0.296430766582489\n",
      "[Training Epoch 1] Batch 1015, Loss 0.3040100932121277\n",
      "[Training Epoch 1] Batch 1016, Loss 0.3105406165122986\n",
      "[Training Epoch 1] Batch 1017, Loss 0.3002323806285858\n",
      "[Training Epoch 1] Batch 1018, Loss 0.3192945122718811\n",
      "[Training Epoch 1] Batch 1019, Loss 0.31422606110572815\n",
      "[Training Epoch 1] Batch 1020, Loss 0.301817923784256\n",
      "[Training Epoch 1] Batch 1021, Loss 0.2948988676071167\n",
      "[Training Epoch 1] Batch 1022, Loss 0.31804782152175903\n",
      "[Training Epoch 1] Batch 1023, Loss 0.3245827853679657\n",
      "[Training Epoch 1] Batch 1024, Loss 0.3146279454231262\n",
      "[Training Epoch 1] Batch 1025, Loss 0.29237955808639526\n",
      "[Training Epoch 1] Batch 1026, Loss 0.3351908326148987\n",
      "[Training Epoch 1] Batch 1027, Loss 0.29847556352615356\n",
      "[Training Epoch 1] Batch 1028, Loss 0.3146989047527313\n",
      "[Training Epoch 1] Batch 1029, Loss 0.31476449966430664\n",
      "[Training Epoch 1] Batch 1030, Loss 0.34979933500289917\n",
      "[Training Epoch 1] Batch 1031, Loss 0.3180794417858124\n",
      "[Training Epoch 1] Batch 1032, Loss 0.3302696645259857\n",
      "[Training Epoch 1] Batch 1033, Loss 0.3197060227394104\n",
      "[Training Epoch 1] Batch 1034, Loss 0.33812305331230164\n",
      "[Training Epoch 1] Batch 1035, Loss 0.3044980764389038\n",
      "[Training Epoch 1] Batch 1036, Loss 0.2946828007698059\n",
      "[Training Epoch 1] Batch 1037, Loss 0.31984394788742065\n",
      "[Training Epoch 1] Batch 1038, Loss 0.27865099906921387\n",
      "[Training Epoch 1] Batch 1039, Loss 0.2916022837162018\n",
      "[Training Epoch 1] Batch 1040, Loss 0.3012279272079468\n",
      "[Training Epoch 1] Batch 1041, Loss 0.28589630126953125\n",
      "[Training Epoch 1] Batch 1042, Loss 0.3166985809803009\n",
      "[Training Epoch 1] Batch 1043, Loss 0.30509433150291443\n",
      "[Training Epoch 1] Batch 1044, Loss 0.28674888610839844\n",
      "[Training Epoch 1] Batch 1045, Loss 0.31819188594818115\n",
      "[Training Epoch 1] Batch 1046, Loss 0.3217194974422455\n",
      "[Training Epoch 1] Batch 1047, Loss 0.2913719415664673\n",
      "[Training Epoch 1] Batch 1048, Loss 0.34690940380096436\n",
      "[Training Epoch 1] Batch 1049, Loss 0.3012874722480774\n",
      "[Training Epoch 1] Batch 1050, Loss 0.33067673444747925\n",
      "[Training Epoch 1] Batch 1051, Loss 0.3030213713645935\n",
      "[Training Epoch 1] Batch 1052, Loss 0.29612746834754944\n",
      "[Training Epoch 1] Batch 1053, Loss 0.3252663016319275\n",
      "[Training Epoch 1] Batch 1054, Loss 0.30401188135147095\n",
      "[Training Epoch 1] Batch 1055, Loss 0.2779463529586792\n",
      "[Training Epoch 1] Batch 1056, Loss 0.3068367838859558\n",
      "[Training Epoch 1] Batch 1057, Loss 0.3030383586883545\n",
      "[Training Epoch 1] Batch 1058, Loss 0.25710391998291016\n",
      "[Training Epoch 1] Batch 1059, Loss 0.2943091094493866\n",
      "[Training Epoch 1] Batch 1060, Loss 0.2953048348426819\n",
      "[Training Epoch 1] Batch 1061, Loss 0.31816190481185913\n",
      "[Training Epoch 1] Batch 1062, Loss 0.29409611225128174\n",
      "[Training Epoch 1] Batch 1063, Loss 0.3026300072669983\n",
      "[Training Epoch 1] Batch 1064, Loss 0.3251742422580719\n",
      "[Training Epoch 1] Batch 1065, Loss 0.2683618664741516\n",
      "[Training Epoch 1] Batch 1066, Loss 0.2949564456939697\n",
      "[Training Epoch 1] Batch 1067, Loss 0.30786818265914917\n",
      "[Training Epoch 1] Batch 1068, Loss 0.29484620690345764\n",
      "[Training Epoch 1] Batch 1069, Loss 0.30944329500198364\n",
      "[Training Epoch 1] Batch 1070, Loss 0.3083037734031677\n",
      "[Training Epoch 1] Batch 1071, Loss 0.31178903579711914\n",
      "[Training Epoch 1] Batch 1072, Loss 0.2921074628829956\n",
      "[Training Epoch 1] Batch 1073, Loss 0.3231799900531769\n",
      "[Training Epoch 1] Batch 1074, Loss 0.3172370195388794\n",
      "[Training Epoch 1] Batch 1075, Loss 0.29361242055892944\n",
      "[Training Epoch 1] Batch 1076, Loss 0.2927210032939911\n",
      "[Training Epoch 1] Batch 1077, Loss 0.2914181053638458\n",
      "[Training Epoch 1] Batch 1078, Loss 0.33949795365333557\n",
      "[Training Epoch 1] Batch 1079, Loss 0.30821889638900757\n",
      "[Training Epoch 1] Batch 1080, Loss 0.32092881202697754\n",
      "[Training Epoch 1] Batch 1081, Loss 0.30289649963378906\n",
      "[Training Epoch 1] Batch 1082, Loss 0.29541194438934326\n",
      "[Training Epoch 1] Batch 1083, Loss 0.29812225699424744\n",
      "[Training Epoch 1] Batch 1084, Loss 0.32112395763397217\n",
      "[Training Epoch 1] Batch 1085, Loss 0.2951430082321167\n",
      "[Training Epoch 1] Batch 1086, Loss 0.30646032094955444\n",
      "[Training Epoch 1] Batch 1087, Loss 0.3215639889240265\n",
      "[Training Epoch 1] Batch 1088, Loss 0.28118205070495605\n",
      "[Training Epoch 1] Batch 1089, Loss 0.30083948373794556\n",
      "[Training Epoch 1] Batch 1090, Loss 0.32884079217910767\n",
      "[Training Epoch 1] Batch 1091, Loss 0.3029696047306061\n",
      "[Training Epoch 1] Batch 1092, Loss 0.30429548025131226\n",
      "[Training Epoch 1] Batch 1093, Loss 0.3336564004421234\n",
      "[Training Epoch 1] Batch 1094, Loss 0.2942025363445282\n",
      "[Training Epoch 1] Batch 1095, Loss 0.2966674566268921\n",
      "[Training Epoch 1] Batch 1096, Loss 0.3332632780075073\n",
      "[Training Epoch 1] Batch 1097, Loss 0.28560125827789307\n",
      "[Training Epoch 1] Batch 1098, Loss 0.32641103863716125\n",
      "[Training Epoch 1] Batch 1099, Loss 0.3490886092185974\n",
      "[Training Epoch 1] Batch 1100, Loss 0.29897698760032654\n",
      "[Training Epoch 1] Batch 1101, Loss 0.30087196826934814\n",
      "[Training Epoch 1] Batch 1102, Loss 0.2689378261566162\n",
      "[Training Epoch 1] Batch 1103, Loss 0.31771138310432434\n",
      "[Training Epoch 1] Batch 1104, Loss 0.2963509261608124\n",
      "[Training Epoch 1] Batch 1105, Loss 0.29158663749694824\n",
      "[Training Epoch 1] Batch 1106, Loss 0.2836795449256897\n",
      "[Training Epoch 1] Batch 1107, Loss 0.29302409291267395\n",
      "[Training Epoch 1] Batch 1108, Loss 0.2775183320045471\n",
      "[Training Epoch 1] Batch 1109, Loss 0.30012643337249756\n",
      "[Training Epoch 1] Batch 1110, Loss 0.31070324778556824\n",
      "[Training Epoch 1] Batch 1111, Loss 0.30711549520492554\n",
      "[Training Epoch 1] Batch 1112, Loss 0.3176378607749939\n",
      "[Training Epoch 1] Batch 1113, Loss 0.31101250648498535\n",
      "[Training Epoch 1] Batch 1114, Loss 0.29999154806137085\n",
      "[Training Epoch 1] Batch 1115, Loss 0.280213326215744\n",
      "[Training Epoch 1] Batch 1116, Loss 0.29964566230773926\n",
      "[Training Epoch 1] Batch 1117, Loss 0.30520808696746826\n",
      "[Training Epoch 1] Batch 1118, Loss 0.29939785599708557\n",
      "[Training Epoch 1] Batch 1119, Loss 0.3219754695892334\n",
      "[Training Epoch 1] Batch 1120, Loss 0.3065613806247711\n",
      "[Training Epoch 1] Batch 1121, Loss 0.2998308539390564\n",
      "[Training Epoch 1] Batch 1122, Loss 0.3161608576774597\n",
      "[Training Epoch 1] Batch 1123, Loss 0.2987176775932312\n",
      "[Training Epoch 1] Batch 1124, Loss 0.3048161268234253\n",
      "[Training Epoch 1] Batch 1125, Loss 0.3078470826148987\n",
      "[Training Epoch 1] Batch 1126, Loss 0.30594903230667114\n",
      "[Training Epoch 1] Batch 1127, Loss 0.30380767583847046\n",
      "[Training Epoch 1] Batch 1128, Loss 0.28955793380737305\n",
      "[Training Epoch 1] Batch 1129, Loss 0.3181084096431732\n",
      "[Training Epoch 1] Batch 1130, Loss 0.3272865414619446\n",
      "[Training Epoch 1] Batch 1131, Loss 0.2804754972457886\n",
      "[Training Epoch 1] Batch 1132, Loss 0.2880192995071411\n",
      "[Training Epoch 1] Batch 1133, Loss 0.32043522596359253\n",
      "[Training Epoch 1] Batch 1134, Loss 0.3035796880722046\n",
      "[Training Epoch 1] Batch 1135, Loss 0.2826656699180603\n",
      "[Training Epoch 1] Batch 1136, Loss 0.2854419946670532\n",
      "[Training Epoch 1] Batch 1137, Loss 0.33371156454086304\n",
      "[Training Epoch 1] Batch 1138, Loss 0.31526148319244385\n",
      "[Training Epoch 1] Batch 1139, Loss 0.3129069209098816\n",
      "[Training Epoch 1] Batch 1140, Loss 0.317839115858078\n",
      "[Training Epoch 1] Batch 1141, Loss 0.3233594298362732\n",
      "[Training Epoch 1] Batch 1142, Loss 0.3113435208797455\n",
      "[Training Epoch 1] Batch 1143, Loss 0.30368945002555847\n",
      "[Training Epoch 1] Batch 1144, Loss 0.2812073826789856\n",
      "[Training Epoch 1] Batch 1145, Loss 0.29123467206954956\n",
      "[Training Epoch 1] Batch 1146, Loss 0.3091961443424225\n",
      "[Training Epoch 1] Batch 1147, Loss 0.3066549301147461\n",
      "[Training Epoch 1] Batch 1148, Loss 0.2860181927680969\n",
      "[Training Epoch 1] Batch 1149, Loss 0.29778915643692017\n",
      "[Training Epoch 1] Batch 1150, Loss 0.30359935760498047\n",
      "[Training Epoch 1] Batch 1151, Loss 0.27186983823776245\n",
      "[Training Epoch 1] Batch 1152, Loss 0.30160921812057495\n",
      "[Training Epoch 1] Batch 1153, Loss 0.2924402952194214\n",
      "[Training Epoch 1] Batch 1154, Loss 0.2977898120880127\n",
      "[Training Epoch 1] Batch 1155, Loss 0.2751501798629761\n",
      "[Training Epoch 1] Batch 1156, Loss 0.27752402424812317\n",
      "[Training Epoch 1] Batch 1157, Loss 0.3007625937461853\n",
      "[Training Epoch 1] Batch 1158, Loss 0.3002525269985199\n",
      "[Training Epoch 1] Batch 1159, Loss 0.3072200119495392\n",
      "[Training Epoch 1] Batch 1160, Loss 0.3058243989944458\n",
      "[Training Epoch 1] Batch 1161, Loss 0.31177106499671936\n",
      "[Training Epoch 1] Batch 1162, Loss 0.30610978603363037\n",
      "[Training Epoch 1] Batch 1163, Loss 0.28438013792037964\n",
      "[Training Epoch 1] Batch 1164, Loss 0.2891162037849426\n",
      "[Training Epoch 1] Batch 1165, Loss 0.32345837354660034\n",
      "[Training Epoch 1] Batch 1166, Loss 0.300518274307251\n",
      "[Training Epoch 1] Batch 1167, Loss 0.29028016328811646\n",
      "[Training Epoch 1] Batch 1168, Loss 0.32456761598587036\n",
      "[Training Epoch 1] Batch 1169, Loss 0.3331504464149475\n",
      "[Training Epoch 1] Batch 1170, Loss 0.29419371485710144\n",
      "[Training Epoch 1] Batch 1171, Loss 0.3087747395038605\n",
      "[Training Epoch 1] Batch 1172, Loss 0.3252648711204529\n",
      "[Training Epoch 1] Batch 1173, Loss 0.28858596086502075\n",
      "[Training Epoch 1] Batch 1174, Loss 0.30025672912597656\n",
      "[Training Epoch 1] Batch 1175, Loss 0.2929159998893738\n",
      "[Training Epoch 1] Batch 1176, Loss 0.30441218614578247\n",
      "[Training Epoch 1] Batch 1177, Loss 0.2705864906311035\n",
      "[Training Epoch 1] Batch 1178, Loss 0.28806084394454956\n",
      "[Training Epoch 1] Batch 1179, Loss 0.2990431785583496\n",
      "[Training Epoch 1] Batch 1180, Loss 0.28698673844337463\n",
      "[Training Epoch 1] Batch 1181, Loss 0.31950366497039795\n",
      "[Training Epoch 1] Batch 1182, Loss 0.2846132516860962\n",
      "[Training Epoch 1] Batch 1183, Loss 0.32950350642204285\n",
      "[Training Epoch 1] Batch 1184, Loss 0.28635597229003906\n",
      "[Training Epoch 1] Batch 1185, Loss 0.3026440143585205\n",
      "[Training Epoch 1] Batch 1186, Loss 0.3073345720767975\n",
      "[Training Epoch 1] Batch 1187, Loss 0.3233577013015747\n",
      "[Training Epoch 1] Batch 1188, Loss 0.3117446303367615\n",
      "[Training Epoch 1] Batch 1189, Loss 0.30515602231025696\n",
      "[Training Epoch 1] Batch 1190, Loss 0.31708192825317383\n",
      "[Training Epoch 1] Batch 1191, Loss 0.30794358253479004\n",
      "[Training Epoch 1] Batch 1192, Loss 0.31646087765693665\n",
      "[Training Epoch 1] Batch 1193, Loss 0.29097023606300354\n",
      "[Training Epoch 1] Batch 1194, Loss 0.3271346986293793\n",
      "[Training Epoch 1] Batch 1195, Loss 0.2917349338531494\n",
      "[Training Epoch 1] Batch 1196, Loss 0.3175557553768158\n",
      "[Training Epoch 1] Batch 1197, Loss 0.2956187427043915\n",
      "[Training Epoch 1] Batch 1198, Loss 0.33941182494163513\n",
      "[Training Epoch 1] Batch 1199, Loss 0.311864972114563\n",
      "[Training Epoch 1] Batch 1200, Loss 0.3258204758167267\n",
      "[Training Epoch 1] Batch 1201, Loss 0.34737831354141235\n",
      "[Training Epoch 1] Batch 1202, Loss 0.3330451250076294\n",
      "[Training Epoch 1] Batch 1203, Loss 0.28564631938934326\n",
      "[Training Epoch 1] Batch 1204, Loss 0.29529985785484314\n",
      "[Training Epoch 1] Batch 1205, Loss 0.2877163589000702\n",
      "[Training Epoch 1] Batch 1206, Loss 0.29529187083244324\n",
      "[Training Epoch 1] Batch 1207, Loss 0.29515185952186584\n",
      "[Training Epoch 1] Batch 1208, Loss 0.32076525688171387\n",
      "[Training Epoch 1] Batch 1209, Loss 0.3221778869628906\n",
      "[Training Epoch 1] Batch 1210, Loss 0.3075217008590698\n",
      "[Training Epoch 1] Batch 1211, Loss 0.310342013835907\n",
      "[Training Epoch 1] Batch 1212, Loss 0.27055129408836365\n",
      "[Training Epoch 1] Batch 1213, Loss 0.32895591855049133\n",
      "[Training Epoch 1] Batch 1214, Loss 0.28801095485687256\n",
      "[Training Epoch 1] Batch 1215, Loss 0.30264630913734436\n",
      "[Training Epoch 1] Batch 1216, Loss 0.2991769313812256\n",
      "[Training Epoch 1] Batch 1217, Loss 0.33181142807006836\n",
      "[Training Epoch 1] Batch 1218, Loss 0.3085756301879883\n",
      "[Training Epoch 1] Batch 1219, Loss 0.2822464406490326\n",
      "[Training Epoch 1] Batch 1220, Loss 0.2798755168914795\n",
      "[Training Epoch 1] Batch 1221, Loss 0.30914634466171265\n",
      "[Training Epoch 1] Batch 1222, Loss 0.30765217542648315\n",
      "[Training Epoch 1] Batch 1223, Loss 0.3190574645996094\n",
      "[Training Epoch 1] Batch 1224, Loss 0.31172868609428406\n",
      "[Training Epoch 1] Batch 1225, Loss 0.29538553953170776\n",
      "[Training Epoch 1] Batch 1226, Loss 0.30710870027542114\n",
      "[Training Epoch 1] Batch 1227, Loss 0.3055334687232971\n",
      "[Training Epoch 1] Batch 1228, Loss 0.3115119934082031\n",
      "[Training Epoch 1] Batch 1229, Loss 0.31501200795173645\n",
      "[Training Epoch 1] Batch 1230, Loss 0.3116057813167572\n",
      "[Training Epoch 1] Batch 1231, Loss 0.2700168490409851\n",
      "[Training Epoch 1] Batch 1232, Loss 0.297882080078125\n",
      "[Training Epoch 1] Batch 1233, Loss 0.3008078336715698\n",
      "[Training Epoch 1] Batch 1234, Loss 0.30672821402549744\n",
      "[Training Epoch 1] Batch 1235, Loss 0.29944124817848206\n",
      "[Training Epoch 1] Batch 1236, Loss 0.29289793968200684\n",
      "[Training Epoch 1] Batch 1237, Loss 0.29244130849838257\n",
      "[Training Epoch 1] Batch 1238, Loss 0.30724263191223145\n",
      "[Training Epoch 1] Batch 1239, Loss 0.3268970549106598\n",
      "[Training Epoch 1] Batch 1240, Loss 0.291525661945343\n",
      "[Training Epoch 1] Batch 1241, Loss 0.2862703502178192\n",
      "[Training Epoch 1] Batch 1242, Loss 0.2997794449329376\n",
      "[Training Epoch 1] Batch 1243, Loss 0.33783861994743347\n",
      "[Training Epoch 1] Batch 1244, Loss 0.3024206757545471\n",
      "[Training Epoch 1] Batch 1245, Loss 0.30718275904655457\n",
      "[Training Epoch 1] Batch 1246, Loss 0.3179489076137543\n",
      "[Training Epoch 1] Batch 1247, Loss 0.32301878929138184\n",
      "[Training Epoch 1] Batch 1248, Loss 0.29460054636001587\n",
      "[Training Epoch 1] Batch 1249, Loss 0.2950774133205414\n",
      "[Training Epoch 1] Batch 1250, Loss 0.2930704951286316\n",
      "[Training Epoch 1] Batch 1251, Loss 0.3139019012451172\n",
      "[Training Epoch 1] Batch 1252, Loss 0.3275151252746582\n",
      "[Training Epoch 1] Batch 1253, Loss 0.2854900062084198\n",
      "[Training Epoch 1] Batch 1254, Loss 0.34866034984588623\n",
      "[Training Epoch 1] Batch 1255, Loss 0.3200029134750366\n",
      "[Training Epoch 1] Batch 1256, Loss 0.3031664490699768\n",
      "[Training Epoch 1] Batch 1257, Loss 0.308846652507782\n",
      "[Training Epoch 1] Batch 1258, Loss 0.30129867792129517\n",
      "[Training Epoch 1] Batch 1259, Loss 0.2974872291088104\n",
      "[Training Epoch 1] Batch 1260, Loss 0.30426374077796936\n",
      "[Training Epoch 1] Batch 1261, Loss 0.3050234317779541\n",
      "[Training Epoch 1] Batch 1262, Loss 0.29208123683929443\n",
      "[Training Epoch 1] Batch 1263, Loss 0.28397977352142334\n",
      "[Training Epoch 1] Batch 1264, Loss 0.29516375064849854\n",
      "[Training Epoch 1] Batch 1265, Loss 0.3240121006965637\n",
      "[Training Epoch 1] Batch 1266, Loss 0.2842511236667633\n",
      "[Training Epoch 1] Batch 1267, Loss 0.3271903395652771\n",
      "[Training Epoch 1] Batch 1268, Loss 0.2779095768928528\n",
      "[Training Epoch 1] Batch 1269, Loss 0.33432888984680176\n",
      "[Training Epoch 1] Batch 1270, Loss 0.33167892694473267\n",
      "[Training Epoch 1] Batch 1271, Loss 0.30563563108444214\n",
      "[Training Epoch 1] Batch 1272, Loss 0.27941417694091797\n",
      "[Training Epoch 1] Batch 1273, Loss 0.31279200315475464\n",
      "[Training Epoch 1] Batch 1274, Loss 0.31463706493377686\n",
      "[Training Epoch 1] Batch 1275, Loss 0.30044567584991455\n",
      "[Training Epoch 1] Batch 1276, Loss 0.3023456335067749\n",
      "[Training Epoch 1] Batch 1277, Loss 0.3128659427165985\n",
      "[Training Epoch 1] Batch 1278, Loss 0.31901776790618896\n",
      "[Training Epoch 1] Batch 1279, Loss 0.2977018356323242\n",
      "[Training Epoch 1] Batch 1280, Loss 0.3072630763053894\n",
      "[Training Epoch 1] Batch 1281, Loss 0.3209303021430969\n",
      "[Training Epoch 1] Batch 1282, Loss 0.3079097270965576\n",
      "[Training Epoch 1] Batch 1283, Loss 0.3006710112094879\n",
      "[Training Epoch 1] Batch 1284, Loss 0.27235567569732666\n",
      "[Training Epoch 1] Batch 1285, Loss 0.3047131896018982\n",
      "[Training Epoch 1] Batch 1286, Loss 0.31924372911453247\n",
      "[Training Epoch 1] Batch 1287, Loss 0.2724967300891876\n",
      "[Training Epoch 1] Batch 1288, Loss 0.29554128646850586\n",
      "[Training Epoch 1] Batch 1289, Loss 0.33888745307922363\n",
      "[Training Epoch 1] Batch 1290, Loss 0.3022879362106323\n",
      "[Training Epoch 1] Batch 1291, Loss 0.33130165934562683\n",
      "[Training Epoch 1] Batch 1292, Loss 0.30124908685684204\n",
      "[Training Epoch 1] Batch 1293, Loss 0.2940690517425537\n",
      "[Training Epoch 1] Batch 1294, Loss 0.3131733238697052\n",
      "[Training Epoch 1] Batch 1295, Loss 0.27540504932403564\n",
      "[Training Epoch 1] Batch 1296, Loss 0.289154589176178\n",
      "[Training Epoch 1] Batch 1297, Loss 0.2932315468788147\n",
      "[Training Epoch 1] Batch 1298, Loss 0.30377036333084106\n",
      "[Training Epoch 1] Batch 1299, Loss 0.29459744691848755\n",
      "[Training Epoch 1] Batch 1300, Loss 0.29315483570098877\n",
      "[Training Epoch 1] Batch 1301, Loss 0.3095473647117615\n",
      "[Training Epoch 1] Batch 1302, Loss 0.32581451535224915\n",
      "[Training Epoch 1] Batch 1303, Loss 0.29570266604423523\n",
      "[Training Epoch 1] Batch 1304, Loss 0.3354862928390503\n",
      "[Training Epoch 1] Batch 1305, Loss 0.3090522885322571\n",
      "[Training Epoch 1] Batch 1306, Loss 0.2838338017463684\n",
      "[Training Epoch 1] Batch 1307, Loss 0.3177511692047119\n",
      "[Training Epoch 1] Batch 1308, Loss 0.28368550539016724\n",
      "[Training Epoch 1] Batch 1309, Loss 0.2805037796497345\n",
      "[Training Epoch 1] Batch 1310, Loss 0.3016808032989502\n",
      "[Training Epoch 1] Batch 1311, Loss 0.3004959523677826\n",
      "[Training Epoch 1] Batch 1312, Loss 0.30130088329315186\n",
      "[Training Epoch 1] Batch 1313, Loss 0.338112473487854\n",
      "[Training Epoch 1] Batch 1314, Loss 0.3385402262210846\n",
      "[Training Epoch 1] Batch 1315, Loss 0.29525572061538696\n",
      "[Training Epoch 1] Batch 1316, Loss 0.29018181562423706\n",
      "[Training Epoch 1] Batch 1317, Loss 0.301460862159729\n",
      "[Training Epoch 1] Batch 1318, Loss 0.29922381043434143\n",
      "[Training Epoch 1] Batch 1319, Loss 0.2777215242385864\n",
      "[Training Epoch 1] Batch 1320, Loss 0.2842549681663513\n",
      "[Training Epoch 1] Batch 1321, Loss 0.32036811113357544\n",
      "[Training Epoch 1] Batch 1322, Loss 0.32918432354927063\n",
      "[Training Epoch 1] Batch 1323, Loss 0.3167465031147003\n",
      "[Training Epoch 1] Batch 1324, Loss 0.3089829087257385\n",
      "[Training Epoch 1] Batch 1325, Loss 0.283589243888855\n",
      "[Training Epoch 1] Batch 1326, Loss 0.32343319058418274\n",
      "[Training Epoch 1] Batch 1327, Loss 0.3036946654319763\n",
      "[Training Epoch 1] Batch 1328, Loss 0.2957383990287781\n",
      "[Training Epoch 1] Batch 1329, Loss 0.28375959396362305\n",
      "[Training Epoch 1] Batch 1330, Loss 0.3033399283885956\n",
      "[Training Epoch 1] Batch 1331, Loss 0.3075035810470581\n",
      "[Training Epoch 1] Batch 1332, Loss 0.3188135623931885\n",
      "[Training Epoch 1] Batch 1333, Loss 0.2955626845359802\n",
      "[Training Epoch 1] Batch 1334, Loss 0.2805631160736084\n",
      "[Training Epoch 1] Batch 1335, Loss 0.3065704107284546\n",
      "[Training Epoch 1] Batch 1336, Loss 0.3002324402332306\n",
      "[Training Epoch 1] Batch 1337, Loss 0.2763344645500183\n",
      "[Training Epoch 1] Batch 1338, Loss 0.29366329312324524\n",
      "[Training Epoch 1] Batch 1339, Loss 0.30808359384536743\n",
      "[Training Epoch 1] Batch 1340, Loss 0.3165854215621948\n",
      "[Training Epoch 1] Batch 1341, Loss 0.29046550393104553\n",
      "[Training Epoch 1] Batch 1342, Loss 0.30323076248168945\n",
      "[Training Epoch 1] Batch 1343, Loss 0.2999644875526428\n",
      "[Training Epoch 1] Batch 1344, Loss 0.2990843653678894\n",
      "[Training Epoch 1] Batch 1345, Loss 0.33574068546295166\n",
      "[Training Epoch 1] Batch 1346, Loss 0.3032556474208832\n",
      "[Training Epoch 1] Batch 1347, Loss 0.27035757899284363\n",
      "[Training Epoch 1] Batch 1348, Loss 0.29028958082199097\n",
      "[Training Epoch 1] Batch 1349, Loss 0.31723466515541077\n",
      "[Training Epoch 1] Batch 1350, Loss 0.262345552444458\n",
      "[Training Epoch 1] Batch 1351, Loss 0.3164845108985901\n",
      "[Training Epoch 1] Batch 1352, Loss 0.3060535192489624\n",
      "[Training Epoch 1] Batch 1353, Loss 0.30201977491378784\n",
      "[Training Epoch 1] Batch 1354, Loss 0.3090602159500122\n",
      "[Training Epoch 1] Batch 1355, Loss 0.29945746064186096\n",
      "[Training Epoch 1] Batch 1356, Loss 0.3222791254520416\n",
      "[Training Epoch 1] Batch 1357, Loss 0.3060559034347534\n",
      "[Training Epoch 1] Batch 1358, Loss 0.3041410446166992\n",
      "[Training Epoch 1] Batch 1359, Loss 0.30780094861984253\n",
      "[Training Epoch 1] Batch 1360, Loss 0.322617769241333\n",
      "[Training Epoch 1] Batch 1361, Loss 0.30282533168792725\n",
      "[Training Epoch 1] Batch 1362, Loss 0.3120861053466797\n",
      "[Training Epoch 1] Batch 1363, Loss 0.31002140045166016\n",
      "[Training Epoch 1] Batch 1364, Loss 0.2999606728553772\n",
      "[Training Epoch 1] Batch 1365, Loss 0.3173519968986511\n",
      "[Training Epoch 1] Batch 1366, Loss 0.29018712043762207\n",
      "[Training Epoch 1] Batch 1367, Loss 0.3019399642944336\n",
      "[Training Epoch 1] Batch 1368, Loss 0.28965532779693604\n",
      "[Training Epoch 1] Batch 1369, Loss 0.3055633306503296\n",
      "[Training Epoch 1] Batch 1370, Loss 0.3169177174568176\n",
      "[Training Epoch 1] Batch 1371, Loss 0.30341196060180664\n",
      "[Training Epoch 1] Batch 1372, Loss 0.29549041390419006\n",
      "[Training Epoch 1] Batch 1373, Loss 0.3072221279144287\n",
      "[Training Epoch 1] Batch 1374, Loss 0.3026806116104126\n",
      "[Training Epoch 1] Batch 1375, Loss 0.33762404322624207\n",
      "[Training Epoch 1] Batch 1376, Loss 0.30423521995544434\n",
      "[Training Epoch 1] Batch 1377, Loss 0.33020836114883423\n",
      "[Training Epoch 1] Batch 1378, Loss 0.29838934540748596\n",
      "[Training Epoch 1] Batch 1379, Loss 0.30351561307907104\n",
      "[Training Epoch 1] Batch 1380, Loss 0.3345622420310974\n",
      "[Training Epoch 1] Batch 1381, Loss 0.3184536099433899\n",
      "[Training Epoch 1] Batch 1382, Loss 0.30541524291038513\n",
      "[Training Epoch 1] Batch 1383, Loss 0.296822190284729\n",
      "[Training Epoch 1] Batch 1384, Loss 0.3011104166507721\n",
      "[Training Epoch 1] Batch 1385, Loss 0.2969142496585846\n",
      "[Training Epoch 1] Batch 1386, Loss 0.31863483786582947\n",
      "[Training Epoch 1] Batch 1387, Loss 0.32084619998931885\n",
      "[Training Epoch 1] Batch 1388, Loss 0.2818060517311096\n",
      "[Training Epoch 1] Batch 1389, Loss 0.3183259963989258\n",
      "[Training Epoch 1] Batch 1390, Loss 0.28765594959259033\n",
      "[Training Epoch 1] Batch 1391, Loss 0.29582253098487854\n",
      "[Training Epoch 1] Batch 1392, Loss 0.30271947383880615\n",
      "[Training Epoch 1] Batch 1393, Loss 0.3004229664802551\n",
      "[Training Epoch 1] Batch 1394, Loss 0.30010730028152466\n",
      "[Training Epoch 1] Batch 1395, Loss 0.2634974420070648\n",
      "[Training Epoch 1] Batch 1396, Loss 0.3270636200904846\n",
      "[Training Epoch 1] Batch 1397, Loss 0.322022408246994\n",
      "[Training Epoch 1] Batch 1398, Loss 0.3199300765991211\n",
      "[Training Epoch 1] Batch 1399, Loss 0.28838205337524414\n",
      "[Training Epoch 1] Batch 1400, Loss 0.3067700266838074\n",
      "[Training Epoch 1] Batch 1401, Loss 0.2920655608177185\n",
      "[Training Epoch 1] Batch 1402, Loss 0.3179374933242798\n",
      "[Training Epoch 1] Batch 1403, Loss 0.2936789393424988\n",
      "[Training Epoch 1] Batch 1404, Loss 0.3158254623413086\n",
      "[Training Epoch 1] Batch 1405, Loss 0.3074083924293518\n",
      "[Training Epoch 1] Batch 1406, Loss 0.30948829650878906\n",
      "[Training Epoch 1] Batch 1407, Loss 0.3137579560279846\n",
      "[Training Epoch 1] Batch 1408, Loss 0.30215486884117126\n",
      "[Training Epoch 1] Batch 1409, Loss 0.2859206795692444\n",
      "[Training Epoch 1] Batch 1410, Loss 0.3223332166671753\n",
      "[Training Epoch 1] Batch 1411, Loss 0.3323117792606354\n",
      "[Training Epoch 1] Batch 1412, Loss 0.3169814944267273\n",
      "[Training Epoch 1] Batch 1413, Loss 0.308626651763916\n",
      "[Training Epoch 1] Batch 1414, Loss 0.2861519753932953\n",
      "[Training Epoch 1] Batch 1415, Loss 0.30847474932670593\n",
      "[Training Epoch 1] Batch 1416, Loss 0.3011716604232788\n",
      "[Training Epoch 1] Batch 1417, Loss 0.30165132880210876\n",
      "[Training Epoch 1] Batch 1418, Loss 0.32391420006752014\n",
      "[Training Epoch 1] Batch 1419, Loss 0.36075150966644287\n",
      "[Training Epoch 1] Batch 1420, Loss 0.30082473158836365\n",
      "[Training Epoch 1] Batch 1421, Loss 0.30703797936439514\n",
      "[Training Epoch 1] Batch 1422, Loss 0.30597126483917236\n",
      "[Training Epoch 1] Batch 1423, Loss 0.2929762601852417\n",
      "[Training Epoch 1] Batch 1424, Loss 0.33857789635658264\n",
      "[Training Epoch 1] Batch 1425, Loss 0.31297731399536133\n",
      "[Training Epoch 1] Batch 1426, Loss 0.3266044855117798\n",
      "[Training Epoch 1] Batch 1427, Loss 0.28488385677337646\n",
      "[Training Epoch 1] Batch 1428, Loss 0.31573575735092163\n",
      "[Training Epoch 1] Batch 1429, Loss 0.29735249280929565\n",
      "[Training Epoch 1] Batch 1430, Loss 0.29823726415634155\n",
      "[Training Epoch 1] Batch 1431, Loss 0.34181004762649536\n",
      "[Training Epoch 1] Batch 1432, Loss 0.3154992461204529\n",
      "[Training Epoch 1] Batch 1433, Loss 0.2903714179992676\n",
      "[Training Epoch 1] Batch 1434, Loss 0.30211812257766724\n",
      "[Training Epoch 1] Batch 1435, Loss 0.30751508474349976\n",
      "[Training Epoch 1] Batch 1436, Loss 0.30659621953964233\n",
      "[Training Epoch 1] Batch 1437, Loss 0.31563350558280945\n",
      "[Training Epoch 1] Batch 1438, Loss 0.2978423237800598\n",
      "[Training Epoch 1] Batch 1439, Loss 0.2936946749687195\n",
      "[Training Epoch 1] Batch 1440, Loss 0.2650641202926636\n",
      "[Training Epoch 1] Batch 1441, Loss 0.2877391576766968\n",
      "[Training Epoch 1] Batch 1442, Loss 0.3008512258529663\n",
      "[Training Epoch 1] Batch 1443, Loss 0.31152886152267456\n",
      "[Training Epoch 1] Batch 1444, Loss 0.32370883226394653\n",
      "[Training Epoch 1] Batch 1445, Loss 0.2869798243045807\n",
      "[Training Epoch 1] Batch 1446, Loss 0.3088409900665283\n",
      "[Training Epoch 1] Batch 1447, Loss 0.27987027168273926\n",
      "[Training Epoch 1] Batch 1448, Loss 0.28369322419166565\n",
      "[Training Epoch 1] Batch 1449, Loss 0.31673502922058105\n",
      "[Training Epoch 1] Batch 1450, Loss 0.280185341835022\n",
      "[Training Epoch 1] Batch 1451, Loss 0.2898344397544861\n",
      "[Training Epoch 1] Batch 1452, Loss 0.30908483266830444\n",
      "[Training Epoch 1] Batch 1453, Loss 0.31370899081230164\n",
      "[Training Epoch 1] Batch 1454, Loss 0.29092293977737427\n",
      "[Training Epoch 1] Batch 1455, Loss 0.3075597584247589\n",
      "[Training Epoch 1] Batch 1456, Loss 0.29563480615615845\n",
      "[Training Epoch 1] Batch 1457, Loss 0.3065696060657501\n",
      "[Training Epoch 1] Batch 1458, Loss 0.271213561296463\n",
      "[Training Epoch 1] Batch 1459, Loss 0.2837194800376892\n",
      "[Training Epoch 1] Batch 1460, Loss 0.3092946410179138\n",
      "[Training Epoch 1] Batch 1461, Loss 0.32293373346328735\n",
      "[Training Epoch 1] Batch 1462, Loss 0.30165302753448486\n",
      "[Training Epoch 1] Batch 1463, Loss 0.3020094037055969\n",
      "[Training Epoch 1] Batch 1464, Loss 0.3104294538497925\n",
      "[Training Epoch 1] Batch 1465, Loss 0.2986530363559723\n",
      "[Training Epoch 1] Batch 1466, Loss 0.3143329918384552\n",
      "[Training Epoch 1] Batch 1467, Loss 0.2747117280960083\n",
      "[Training Epoch 1] Batch 1468, Loss 0.31796520948410034\n",
      "[Training Epoch 1] Batch 1469, Loss 0.3022330403327942\n",
      "[Training Epoch 1] Batch 1470, Loss 0.3065579831600189\n",
      "[Training Epoch 1] Batch 1471, Loss 0.29855650663375854\n",
      "[Training Epoch 1] Batch 1472, Loss 0.30689865350723267\n",
      "[Training Epoch 1] Batch 1473, Loss 0.3247069716453552\n",
      "[Training Epoch 1] Batch 1474, Loss 0.3058924376964569\n",
      "[Training Epoch 1] Batch 1475, Loss 0.270916223526001\n",
      "[Training Epoch 1] Batch 1476, Loss 0.3297548294067383\n",
      "[Training Epoch 1] Batch 1477, Loss 0.26406222581863403\n",
      "[Training Epoch 1] Batch 1478, Loss 0.3321123719215393\n",
      "[Training Epoch 1] Batch 1479, Loss 0.2987017333507538\n",
      "[Training Epoch 1] Batch 1480, Loss 0.3258400559425354\n",
      "[Training Epoch 1] Batch 1481, Loss 0.31802016496658325\n",
      "[Training Epoch 1] Batch 1482, Loss 0.29774510860443115\n",
      "[Training Epoch 1] Batch 1483, Loss 0.3219539523124695\n",
      "[Training Epoch 1] Batch 1484, Loss 0.2961234748363495\n",
      "[Training Epoch 1] Batch 1485, Loss 0.309423565864563\n",
      "[Training Epoch 1] Batch 1486, Loss 0.3177810311317444\n",
      "[Training Epoch 1] Batch 1487, Loss 0.31886541843414307\n",
      "[Training Epoch 1] Batch 1488, Loss 0.3042660355567932\n",
      "[Training Epoch 1] Batch 1489, Loss 0.29442644119262695\n",
      "[Training Epoch 1] Batch 1490, Loss 0.3100658059120178\n",
      "[Training Epoch 1] Batch 1491, Loss 0.338708758354187\n",
      "[Training Epoch 1] Batch 1492, Loss 0.31632623076438904\n",
      "[Training Epoch 1] Batch 1493, Loss 0.29231709241867065\n",
      "[Training Epoch 1] Batch 1494, Loss 0.290661484003067\n",
      "[Training Epoch 1] Batch 1495, Loss 0.2888951599597931\n",
      "[Training Epoch 1] Batch 1496, Loss 0.3113742768764496\n",
      "[Training Epoch 1] Batch 1497, Loss 0.3179532289505005\n",
      "[Training Epoch 1] Batch 1498, Loss 0.3152095079421997\n",
      "[Training Epoch 1] Batch 1499, Loss 0.2822433412075043\n",
      "[Training Epoch 1] Batch 1500, Loss 0.3270048201084137\n",
      "[Training Epoch 1] Batch 1501, Loss 0.3074442148208618\n",
      "[Training Epoch 1] Batch 1502, Loss 0.2965526580810547\n",
      "[Training Epoch 1] Batch 1503, Loss 0.3131694495677948\n",
      "[Training Epoch 1] Batch 1504, Loss 0.3307023346424103\n",
      "[Training Epoch 1] Batch 1505, Loss 0.28546416759490967\n",
      "[Training Epoch 1] Batch 1506, Loss 0.2881987392902374\n",
      "[Training Epoch 1] Batch 1507, Loss 0.31110578775405884\n",
      "[Training Epoch 1] Batch 1508, Loss 0.28252413868904114\n",
      "[Training Epoch 1] Batch 1509, Loss 0.31149983406066895\n",
      "[Training Epoch 1] Batch 1510, Loss 0.2979811131954193\n",
      "[Training Epoch 1] Batch 1511, Loss 0.2700796127319336\n",
      "[Training Epoch 1] Batch 1512, Loss 0.3249271512031555\n",
      "[Training Epoch 1] Batch 1513, Loss 0.3011822998523712\n",
      "[Training Epoch 1] Batch 1514, Loss 0.29680731892585754\n",
      "[Training Epoch 1] Batch 1515, Loss 0.292409211397171\n",
      "[Training Epoch 1] Batch 1516, Loss 0.31933891773223877\n",
      "[Training Epoch 1] Batch 1517, Loss 0.28752148151397705\n",
      "[Training Epoch 1] Batch 1518, Loss 0.3184584379196167\n",
      "[Training Epoch 1] Batch 1519, Loss 0.2868431508541107\n",
      "[Training Epoch 1] Batch 1520, Loss 0.29702943563461304\n",
      "[Training Epoch 1] Batch 1521, Loss 0.31453895568847656\n",
      "[Training Epoch 1] Batch 1522, Loss 0.30465635657310486\n",
      "[Training Epoch 1] Batch 1523, Loss 0.27571749687194824\n",
      "[Training Epoch 1] Batch 1524, Loss 0.3239538073539734\n",
      "[Training Epoch 1] Batch 1525, Loss 0.3202807307243347\n",
      "[Training Epoch 1] Batch 1526, Loss 0.2872624397277832\n",
      "[Training Epoch 1] Batch 1527, Loss 0.2962910532951355\n",
      "[Training Epoch 1] Batch 1528, Loss 0.31800326704978943\n",
      "[Training Epoch 1] Batch 1529, Loss 0.3016183078289032\n",
      "[Training Epoch 1] Batch 1530, Loss 0.3047794699668884\n",
      "[Training Epoch 1] Batch 1531, Loss 0.3248039484024048\n",
      "[Training Epoch 1] Batch 1532, Loss 0.2933216691017151\n",
      "[Training Epoch 1] Batch 1533, Loss 0.2996385395526886\n",
      "[Training Epoch 1] Batch 1534, Loss 0.3077620267868042\n",
      "[Training Epoch 1] Batch 1535, Loss 0.27204322814941406\n",
      "[Training Epoch 1] Batch 1536, Loss 0.2847825288772583\n",
      "[Training Epoch 1] Batch 1537, Loss 0.2978377342224121\n",
      "[Training Epoch 1] Batch 1538, Loss 0.29481929540634155\n",
      "[Training Epoch 1] Batch 1539, Loss 0.31866565346717834\n",
      "[Training Epoch 1] Batch 1540, Loss 0.28075164556503296\n",
      "[Training Epoch 1] Batch 1541, Loss 0.2782258987426758\n",
      "[Training Epoch 1] Batch 1542, Loss 0.3233146667480469\n",
      "[Training Epoch 1] Batch 1543, Loss 0.30976802110671997\n",
      "[Training Epoch 1] Batch 1544, Loss 0.3167824149131775\n",
      "[Training Epoch 1] Batch 1545, Loss 0.30806899070739746\n",
      "[Training Epoch 1] Batch 1546, Loss 0.32586127519607544\n",
      "[Training Epoch 1] Batch 1547, Loss 0.28903961181640625\n",
      "[Training Epoch 1] Batch 1548, Loss 0.3352382779121399\n",
      "[Training Epoch 1] Batch 1549, Loss 0.3330641984939575\n",
      "[Training Epoch 1] Batch 1550, Loss 0.31950560212135315\n",
      "[Training Epoch 1] Batch 1551, Loss 0.27673566341400146\n",
      "[Training Epoch 1] Batch 1552, Loss 0.30557751655578613\n",
      "[Training Epoch 1] Batch 1553, Loss 0.28435948491096497\n",
      "[Training Epoch 1] Batch 1554, Loss 0.3028898537158966\n",
      "[Training Epoch 1] Batch 1555, Loss 0.2937084436416626\n",
      "[Training Epoch 1] Batch 1556, Loss 0.3047678768634796\n",
      "[Training Epoch 1] Batch 1557, Loss 0.32615920901298523\n",
      "[Training Epoch 1] Batch 1558, Loss 0.3118314743041992\n",
      "[Training Epoch 1] Batch 1559, Loss 0.32627376914024353\n",
      "[Training Epoch 1] Batch 1560, Loss 0.30613863468170166\n",
      "[Training Epoch 1] Batch 1561, Loss 0.321185827255249\n",
      "[Training Epoch 1] Batch 1562, Loss 0.2851105332374573\n",
      "[Training Epoch 1] Batch 1563, Loss 0.29767078161239624\n",
      "[Training Epoch 1] Batch 1564, Loss 0.31534814834594727\n",
      "[Training Epoch 1] Batch 1565, Loss 0.3250141143798828\n",
      "[Training Epoch 1] Batch 1566, Loss 0.32784926891326904\n",
      "[Training Epoch 1] Batch 1567, Loss 0.3288027048110962\n",
      "[Training Epoch 1] Batch 1568, Loss 0.31688129901885986\n",
      "[Training Epoch 1] Batch 1569, Loss 0.281531423330307\n",
      "[Training Epoch 1] Batch 1570, Loss 0.328058123588562\n",
      "[Training Epoch 1] Batch 1571, Loss 0.3100704550743103\n",
      "[Training Epoch 1] Batch 1572, Loss 0.2761589288711548\n",
      "[Training Epoch 1] Batch 1573, Loss 0.31285691261291504\n",
      "[Training Epoch 1] Batch 1574, Loss 0.32358235120773315\n",
      "[Training Epoch 1] Batch 1575, Loss 0.3114122450351715\n",
      "[Training Epoch 1] Batch 1576, Loss 0.303819477558136\n",
      "[Training Epoch 1] Batch 1577, Loss 0.3036658763885498\n",
      "[Training Epoch 1] Batch 1578, Loss 0.29051849246025085\n",
      "[Training Epoch 1] Batch 1579, Loss 0.291472852230072\n",
      "[Training Epoch 1] Batch 1580, Loss 0.3120744526386261\n",
      "[Training Epoch 1] Batch 1581, Loss 0.3233022689819336\n",
      "[Training Epoch 1] Batch 1582, Loss 0.32486510276794434\n",
      "[Training Epoch 1] Batch 1583, Loss 0.2999153137207031\n",
      "[Training Epoch 1] Batch 1584, Loss 0.2912783622741699\n",
      "[Training Epoch 1] Batch 1585, Loss 0.30903705954551697\n",
      "[Training Epoch 1] Batch 1586, Loss 0.3214234411716461\n",
      "[Training Epoch 1] Batch 1587, Loss 0.2973287105560303\n",
      "[Training Epoch 1] Batch 1588, Loss 0.28635162115097046\n",
      "[Training Epoch 1] Batch 1589, Loss 0.29393845796585083\n",
      "[Training Epoch 1] Batch 1590, Loss 0.31568312644958496\n",
      "[Training Epoch 1] Batch 1591, Loss 0.31131511926651\n",
      "[Training Epoch 1] Batch 1592, Loss 0.3140004277229309\n",
      "[Training Epoch 1] Batch 1593, Loss 0.2894713878631592\n",
      "[Training Epoch 1] Batch 1594, Loss 0.31203293800354004\n",
      "[Training Epoch 1] Batch 1595, Loss 0.3134861886501312\n",
      "[Training Epoch 1] Batch 1596, Loss 0.2911684215068817\n",
      "[Training Epoch 1] Batch 1597, Loss 0.30132168531417847\n",
      "[Training Epoch 1] Batch 1598, Loss 0.31516993045806885\n",
      "[Training Epoch 1] Batch 1599, Loss 0.334524929523468\n",
      "[Training Epoch 1] Batch 1600, Loss 0.30781012773513794\n",
      "[Training Epoch 1] Batch 1601, Loss 0.30132579803466797\n",
      "[Training Epoch 1] Batch 1602, Loss 0.3035370111465454\n",
      "[Training Epoch 1] Batch 1603, Loss 0.31482911109924316\n",
      "[Training Epoch 1] Batch 1604, Loss 0.2723807692527771\n",
      "[Training Epoch 1] Batch 1605, Loss 0.28798767924308777\n",
      "[Training Epoch 1] Batch 1606, Loss 0.2893712520599365\n",
      "[Training Epoch 1] Batch 1607, Loss 0.296103298664093\n",
      "[Training Epoch 1] Batch 1608, Loss 0.33540451526641846\n",
      "[Training Epoch 1] Batch 1609, Loss 0.30778807401657104\n",
      "[Training Epoch 1] Batch 1610, Loss 0.2992545962333679\n",
      "[Training Epoch 1] Batch 1611, Loss 0.28722983598709106\n",
      "[Training Epoch 1] Batch 1612, Loss 0.286507248878479\n",
      "[Training Epoch 1] Batch 1613, Loss 0.3162991404533386\n",
      "[Training Epoch 1] Batch 1614, Loss 0.3164510726928711\n",
      "[Training Epoch 1] Batch 1615, Loss 0.2978366017341614\n",
      "[Training Epoch 1] Batch 1616, Loss 0.32355189323425293\n",
      "[Training Epoch 1] Batch 1617, Loss 0.29445648193359375\n",
      "[Training Epoch 1] Batch 1618, Loss 0.314138799905777\n",
      "[Training Epoch 1] Batch 1619, Loss 0.30366039276123047\n",
      "[Training Epoch 1] Batch 1620, Loss 0.3047363758087158\n",
      "[Training Epoch 1] Batch 1621, Loss 0.28825345635414124\n",
      "[Training Epoch 1] Batch 1622, Loss 0.3200174868106842\n",
      "[Training Epoch 1] Batch 1623, Loss 0.30921486020088196\n",
      "[Training Epoch 1] Batch 1624, Loss 0.32089686393737793\n",
      "[Training Epoch 1] Batch 1625, Loss 0.3117774724960327\n",
      "[Training Epoch 1] Batch 1626, Loss 0.28651291131973267\n",
      "[Training Epoch 1] Batch 1627, Loss 0.2935653328895569\n",
      "[Training Epoch 1] Batch 1628, Loss 0.27693259716033936\n",
      "[Training Epoch 1] Batch 1629, Loss 0.3185465633869171\n",
      "[Training Epoch 1] Batch 1630, Loss 0.32300955057144165\n",
      "[Training Epoch 1] Batch 1631, Loss 0.3131925165653229\n",
      "[Training Epoch 1] Batch 1632, Loss 0.3083687424659729\n",
      "[Training Epoch 1] Batch 1633, Loss 0.31499773263931274\n",
      "[Training Epoch 1] Batch 1634, Loss 0.2905884385108948\n",
      "[Training Epoch 1] Batch 1635, Loss 0.2793620228767395\n",
      "[Training Epoch 1] Batch 1636, Loss 0.314287006855011\n",
      "[Training Epoch 1] Batch 1637, Loss 0.3177487254142761\n",
      "[Training Epoch 1] Batch 1638, Loss 0.30384474992752075\n",
      "[Training Epoch 1] Batch 1639, Loss 0.28902703523635864\n",
      "[Training Epoch 1] Batch 1640, Loss 0.31362295150756836\n",
      "[Training Epoch 1] Batch 1641, Loss 0.3172508478164673\n",
      "[Training Epoch 1] Batch 1642, Loss 0.3040753901004791\n",
      "[Training Epoch 1] Batch 1643, Loss 0.31615614891052246\n",
      "[Training Epoch 1] Batch 1644, Loss 0.28927966952323914\n",
      "[Training Epoch 1] Batch 1645, Loss 0.2846110463142395\n",
      "[Training Epoch 1] Batch 1646, Loss 0.3175496459007263\n",
      "[Training Epoch 1] Batch 1647, Loss 0.27874112129211426\n",
      "[Training Epoch 1] Batch 1648, Loss 0.2871693968772888\n",
      "[Training Epoch 1] Batch 1649, Loss 0.29804638028144836\n",
      "[Training Epoch 1] Batch 1650, Loss 0.2964768707752228\n",
      "[Training Epoch 1] Batch 1651, Loss 0.2985689640045166\n",
      "[Training Epoch 1] Batch 1652, Loss 0.27986806631088257\n",
      "[Training Epoch 1] Batch 1653, Loss 0.28420567512512207\n",
      "[Training Epoch 1] Batch 1654, Loss 0.2800096273422241\n",
      "[Training Epoch 1] Batch 1655, Loss 0.30410081148147583\n",
      "[Training Epoch 1] Batch 1656, Loss 0.31087011098861694\n",
      "[Training Epoch 1] Batch 1657, Loss 0.30787259340286255\n",
      "[Training Epoch 1] Batch 1658, Loss 0.2969312071800232\n",
      "[Training Epoch 1] Batch 1659, Loss 0.3025113344192505\n",
      "[Training Epoch 1] Batch 1660, Loss 0.3254338204860687\n",
      "[Training Epoch 1] Batch 1661, Loss 0.288918137550354\n",
      "[Training Epoch 1] Batch 1662, Loss 0.2934423089027405\n",
      "[Training Epoch 1] Batch 1663, Loss 0.30086344480514526\n",
      "[Training Epoch 1] Batch 1664, Loss 0.3002467751502991\n",
      "[Training Epoch 1] Batch 1665, Loss 0.31827449798583984\n",
      "[Training Epoch 1] Batch 1666, Loss 0.2842203676700592\n",
      "[Training Epoch 1] Batch 1667, Loss 0.32894253730773926\n",
      "[Training Epoch 1] Batch 1668, Loss 0.3057095408439636\n",
      "[Training Epoch 1] Batch 1669, Loss 0.3235711455345154\n",
      "[Training Epoch 1] Batch 1670, Loss 0.28766393661499023\n",
      "[Training Epoch 1] Batch 1671, Loss 0.28959447145462036\n",
      "[Training Epoch 1] Batch 1672, Loss 0.31023406982421875\n",
      "[Training Epoch 1] Batch 1673, Loss 0.31455230712890625\n",
      "[Training Epoch 1] Batch 1674, Loss 0.31474655866622925\n",
      "[Training Epoch 1] Batch 1675, Loss 0.2952931523323059\n",
      "[Training Epoch 1] Batch 1676, Loss 0.2818549573421478\n",
      "[Training Epoch 1] Batch 1677, Loss 0.27548807859420776\n",
      "[Training Epoch 1] Batch 1678, Loss 0.2750186622142792\n",
      "[Training Epoch 1] Batch 1679, Loss 0.30597811937332153\n",
      "[Training Epoch 1] Batch 1680, Loss 0.33061307668685913\n",
      "[Training Epoch 1] Batch 1681, Loss 0.29399368166923523\n",
      "[Training Epoch 1] Batch 1682, Loss 0.2929830551147461\n",
      "[Training Epoch 1] Batch 1683, Loss 0.2919338047504425\n",
      "[Training Epoch 1] Batch 1684, Loss 0.3002394735813141\n",
      "[Training Epoch 1] Batch 1685, Loss 0.2980310022830963\n",
      "[Training Epoch 1] Batch 1686, Loss 0.3168807029724121\n",
      "[Training Epoch 1] Batch 1687, Loss 0.32067739963531494\n",
      "[Training Epoch 1] Batch 1688, Loss 0.3006148934364319\n",
      "[Training Epoch 1] Batch 1689, Loss 0.32184404134750366\n",
      "[Training Epoch 1] Batch 1690, Loss 0.3006574213504791\n",
      "[Training Epoch 1] Batch 1691, Loss 0.3040817975997925\n",
      "[Training Epoch 1] Batch 1692, Loss 0.300613135099411\n",
      "[Training Epoch 1] Batch 1693, Loss 0.3154344856739044\n",
      "[Training Epoch 1] Batch 1694, Loss 0.27496472001075745\n",
      "[Training Epoch 1] Batch 1695, Loss 0.2974826693534851\n",
      "[Training Epoch 1] Batch 1696, Loss 0.31825751066207886\n",
      "[Training Epoch 1] Batch 1697, Loss 0.3009517788887024\n",
      "[Training Epoch 1] Batch 1698, Loss 0.30327320098876953\n",
      "[Training Epoch 1] Batch 1699, Loss 0.31951433420181274\n",
      "[Training Epoch 1] Batch 1700, Loss 0.3188157379627228\n",
      "[Training Epoch 1] Batch 1701, Loss 0.3191131353378296\n",
      "[Training Epoch 1] Batch 1702, Loss 0.2914639711380005\n",
      "[Training Epoch 1] Batch 1703, Loss 0.30038946866989136\n",
      "[Training Epoch 1] Batch 1704, Loss 0.29497817158699036\n",
      "[Training Epoch 1] Batch 1705, Loss 0.30237680673599243\n",
      "[Training Epoch 1] Batch 1706, Loss 0.28501778841018677\n",
      "[Training Epoch 1] Batch 1707, Loss 0.279846727848053\n",
      "[Training Epoch 1] Batch 1708, Loss 0.3097032904624939\n",
      "[Training Epoch 1] Batch 1709, Loss 0.3012721538543701\n",
      "[Training Epoch 1] Batch 1710, Loss 0.2779353857040405\n",
      "[Training Epoch 1] Batch 1711, Loss 0.2916502058506012\n",
      "[Training Epoch 1] Batch 1712, Loss 0.33074063062667847\n",
      "[Training Epoch 1] Batch 1713, Loss 0.3008890151977539\n",
      "[Training Epoch 1] Batch 1714, Loss 0.32646530866622925\n",
      "[Training Epoch 1] Batch 1715, Loss 0.3049081563949585\n",
      "[Training Epoch 1] Batch 1716, Loss 0.30417680740356445\n",
      "[Training Epoch 1] Batch 1717, Loss 0.2938385009765625\n",
      "[Training Epoch 1] Batch 1718, Loss 0.3040757179260254\n",
      "[Training Epoch 1] Batch 1719, Loss 0.30466654896736145\n",
      "[Training Epoch 1] Batch 1720, Loss 0.3216586709022522\n",
      "[Training Epoch 1] Batch 1721, Loss 0.28417813777923584\n",
      "[Training Epoch 1] Batch 1722, Loss 0.32558611035346985\n",
      "[Training Epoch 1] Batch 1723, Loss 0.30879074335098267\n",
      "[Training Epoch 1] Batch 1724, Loss 0.29883211851119995\n",
      "[Training Epoch 1] Batch 1725, Loss 0.30794912576675415\n",
      "[Training Epoch 1] Batch 1726, Loss 0.3137740194797516\n",
      "[Training Epoch 1] Batch 1727, Loss 0.2892545163631439\n",
      "[Training Epoch 1] Batch 1728, Loss 0.31533610820770264\n",
      "[Training Epoch 1] Batch 1729, Loss 0.3067183792591095\n",
      "[Training Epoch 1] Batch 1730, Loss 0.2940710186958313\n",
      "[Training Epoch 1] Batch 1731, Loss 0.30624595284461975\n",
      "[Training Epoch 1] Batch 1732, Loss 0.28126636147499084\n",
      "[Training Epoch 1] Batch 1733, Loss 0.30422717332839966\n",
      "[Training Epoch 1] Batch 1734, Loss 0.31237971782684326\n",
      "[Training Epoch 1] Batch 1735, Loss 0.31395578384399414\n",
      "[Training Epoch 1] Batch 1736, Loss 0.2934953570365906\n",
      "[Training Epoch 1] Batch 1737, Loss 0.31605589389801025\n",
      "[Training Epoch 1] Batch 1738, Loss 0.2778058648109436\n",
      "[Training Epoch 1] Batch 1739, Loss 0.32105427980422974\n",
      "[Training Epoch 1] Batch 1740, Loss 0.3193184733390808\n",
      "[Training Epoch 1] Batch 1741, Loss 0.31835663318634033\n",
      "[Training Epoch 1] Batch 1742, Loss 0.28428277373313904\n",
      "[Training Epoch 1] Batch 1743, Loss 0.28896456956863403\n",
      "[Training Epoch 1] Batch 1744, Loss 0.28468751907348633\n",
      "[Training Epoch 1] Batch 1745, Loss 0.32914188504219055\n",
      "[Training Epoch 1] Batch 1746, Loss 0.27801990509033203\n",
      "[Training Epoch 1] Batch 1747, Loss 0.31498008966445923\n",
      "[Training Epoch 1] Batch 1748, Loss 0.2762214243412018\n",
      "[Training Epoch 1] Batch 1749, Loss 0.33973339200019836\n",
      "[Training Epoch 1] Batch 1750, Loss 0.31003379821777344\n",
      "[Training Epoch 1] Batch 1751, Loss 0.33058303594589233\n",
      "[Training Epoch 1] Batch 1752, Loss 0.2972289025783539\n",
      "[Training Epoch 1] Batch 1753, Loss 0.344818651676178\n",
      "[Training Epoch 1] Batch 1754, Loss 0.27359235286712646\n",
      "[Training Epoch 1] Batch 1755, Loss 0.29289931058883667\n",
      "[Training Epoch 1] Batch 1756, Loss 0.27529630064964294\n",
      "[Training Epoch 1] Batch 1757, Loss 0.30158519744873047\n",
      "[Training Epoch 1] Batch 1758, Loss 0.33230507373809814\n",
      "[Training Epoch 1] Batch 1759, Loss 0.30183863639831543\n",
      "[Training Epoch 1] Batch 1760, Loss 0.30761730670928955\n",
      "[Training Epoch 1] Batch 1761, Loss 0.3049687445163727\n",
      "[Training Epoch 1] Batch 1762, Loss 0.3123958110809326\n",
      "[Training Epoch 1] Batch 1763, Loss 0.29561740159988403\n",
      "[Training Epoch 1] Batch 1764, Loss 0.2920021712779999\n",
      "[Training Epoch 1] Batch 1765, Loss 0.326278954744339\n",
      "[Training Epoch 1] Batch 1766, Loss 0.3071598708629608\n",
      "[Training Epoch 1] Batch 1767, Loss 0.2835332453250885\n",
      "[Training Epoch 1] Batch 1768, Loss 0.30298689007759094\n",
      "[Training Epoch 1] Batch 1769, Loss 0.3021434545516968\n",
      "[Training Epoch 1] Batch 1770, Loss 0.2966553866863251\n",
      "[Training Epoch 1] Batch 1771, Loss 0.28998541831970215\n",
      "[Training Epoch 1] Batch 1772, Loss 0.30544692277908325\n",
      "[Training Epoch 1] Batch 1773, Loss 0.2996039390563965\n",
      "[Training Epoch 1] Batch 1774, Loss 0.2653133273124695\n",
      "[Training Epoch 1] Batch 1775, Loss 0.29169419407844543\n",
      "[Training Epoch 1] Batch 1776, Loss 0.2851732075214386\n",
      "[Training Epoch 1] Batch 1777, Loss 0.30867624282836914\n",
      "[Training Epoch 1] Batch 1778, Loss 0.3067557215690613\n",
      "[Training Epoch 1] Batch 1779, Loss 0.3220207095146179\n",
      "[Training Epoch 1] Batch 1780, Loss 0.3034067749977112\n",
      "[Training Epoch 1] Batch 1781, Loss 0.3019545078277588\n",
      "[Training Epoch 1] Batch 1782, Loss 0.3206599950790405\n",
      "[Training Epoch 1] Batch 1783, Loss 0.27822086215019226\n",
      "[Training Epoch 1] Batch 1784, Loss 0.29184672236442566\n",
      "[Training Epoch 1] Batch 1785, Loss 0.2885226607322693\n",
      "[Training Epoch 1] Batch 1786, Loss 0.32502466440200806\n",
      "[Training Epoch 1] Batch 1787, Loss 0.29067808389663696\n",
      "[Training Epoch 1] Batch 1788, Loss 0.29114556312561035\n",
      "[Training Epoch 1] Batch 1789, Loss 0.2895853817462921\n",
      "[Training Epoch 1] Batch 1790, Loss 0.31520092487335205\n",
      "[Training Epoch 1] Batch 1791, Loss 0.2850901484489441\n",
      "[Training Epoch 1] Batch 1792, Loss 0.32355833053588867\n",
      "[Training Epoch 1] Batch 1793, Loss 0.2961365282535553\n",
      "[Training Epoch 1] Batch 1794, Loss 0.3208715319633484\n",
      "[Training Epoch 1] Batch 1795, Loss 0.2935940623283386\n",
      "[Training Epoch 1] Batch 1796, Loss 0.2950010299682617\n",
      "[Training Epoch 1] Batch 1797, Loss 0.28709739446640015\n",
      "[Training Epoch 1] Batch 1798, Loss 0.33811330795288086\n",
      "[Training Epoch 1] Batch 1799, Loss 0.3111931383609772\n",
      "[Training Epoch 1] Batch 1800, Loss 0.2898232340812683\n",
      "[Training Epoch 1] Batch 1801, Loss 0.3107210099697113\n",
      "[Training Epoch 1] Batch 1802, Loss 0.30883222818374634\n",
      "[Training Epoch 1] Batch 1803, Loss 0.29631027579307556\n",
      "[Training Epoch 1] Batch 1804, Loss 0.30034303665161133\n",
      "[Training Epoch 1] Batch 1805, Loss 0.28872084617614746\n",
      "[Training Epoch 1] Batch 1806, Loss 0.29717105627059937\n",
      "[Training Epoch 1] Batch 1807, Loss 0.29242441058158875\n",
      "[Training Epoch 1] Batch 1808, Loss 0.2892717719078064\n",
      "[Training Epoch 1] Batch 1809, Loss 0.29912424087524414\n",
      "[Training Epoch 1] Batch 1810, Loss 0.2929309010505676\n",
      "[Training Epoch 1] Batch 1811, Loss 0.31712526082992554\n",
      "[Training Epoch 1] Batch 1812, Loss 0.2926580011844635\n",
      "[Training Epoch 1] Batch 1813, Loss 0.3188048005104065\n",
      "[Training Epoch 1] Batch 1814, Loss 0.3018922209739685\n",
      "[Training Epoch 1] Batch 1815, Loss 0.32278966903686523\n",
      "[Training Epoch 1] Batch 1816, Loss 0.3038657605648041\n",
      "[Training Epoch 1] Batch 1817, Loss 0.31644392013549805\n",
      "[Training Epoch 1] Batch 1818, Loss 0.32166045904159546\n",
      "[Training Epoch 1] Batch 1819, Loss 0.29011812806129456\n",
      "[Training Epoch 1] Batch 1820, Loss 0.29330986738204956\n",
      "[Training Epoch 1] Batch 1821, Loss 0.30196303129196167\n",
      "[Training Epoch 1] Batch 1822, Loss 0.2927182912826538\n",
      "[Training Epoch 1] Batch 1823, Loss 0.31617826223373413\n",
      "[Training Epoch 1] Batch 1824, Loss 0.29845505952835083\n",
      "[Training Epoch 1] Batch 1825, Loss 0.32244059443473816\n",
      "[Training Epoch 1] Batch 1826, Loss 0.2982214689254761\n",
      "[Training Epoch 1] Batch 1827, Loss 0.2914577126502991\n",
      "[Training Epoch 1] Batch 1828, Loss 0.32643038034439087\n",
      "[Training Epoch 1] Batch 1829, Loss 0.3098612427711487\n",
      "[Training Epoch 1] Batch 1830, Loss 0.308837354183197\n",
      "[Training Epoch 1] Batch 1831, Loss 0.3001020550727844\n",
      "[Training Epoch 1] Batch 1832, Loss 0.2967334985733032\n",
      "[Training Epoch 1] Batch 1833, Loss 0.3045580983161926\n",
      "[Training Epoch 1] Batch 1834, Loss 0.3214642405509949\n",
      "[Training Epoch 1] Batch 1835, Loss 0.3088035583496094\n",
      "[Training Epoch 1] Batch 1836, Loss 0.28380057215690613\n",
      "[Training Epoch 1] Batch 1837, Loss 0.30283820629119873\n",
      "[Training Epoch 1] Batch 1838, Loss 0.2964733839035034\n",
      "[Training Epoch 1] Batch 1839, Loss 0.30293795466423035\n",
      "[Training Epoch 1] Batch 1840, Loss 0.29904812574386597\n",
      "[Training Epoch 1] Batch 1841, Loss 0.30494725704193115\n",
      "[Training Epoch 1] Batch 1842, Loss 0.30986636877059937\n",
      "[Training Epoch 1] Batch 1843, Loss 0.29474014043807983\n",
      "[Training Epoch 1] Batch 1844, Loss 0.307515412569046\n",
      "[Training Epoch 1] Batch 1845, Loss 0.30861836671829224\n",
      "[Training Epoch 1] Batch 1846, Loss 0.2749790549278259\n",
      "[Training Epoch 1] Batch 1847, Loss 0.2713627219200134\n",
      "[Training Epoch 1] Batch 1848, Loss 0.3160351812839508\n",
      "[Training Epoch 1] Batch 1849, Loss 0.29928165674209595\n",
      "[Training Epoch 1] Batch 1850, Loss 0.2964727282524109\n",
      "[Training Epoch 1] Batch 1851, Loss 0.3154292106628418\n",
      "[Training Epoch 1] Batch 1852, Loss 0.3107810616493225\n",
      "[Training Epoch 1] Batch 1853, Loss 0.3127671182155609\n",
      "[Training Epoch 1] Batch 1854, Loss 0.2910683751106262\n",
      "[Training Epoch 1] Batch 1855, Loss 0.30537641048431396\n",
      "[Training Epoch 1] Batch 1856, Loss 0.31024423241615295\n",
      "[Training Epoch 1] Batch 1857, Loss 0.30158016085624695\n",
      "[Training Epoch 1] Batch 1858, Loss 0.26704156398773193\n",
      "[Training Epoch 1] Batch 1859, Loss 0.2994080185890198\n",
      "[Training Epoch 1] Batch 1860, Loss 0.26268449425697327\n",
      "[Training Epoch 1] Batch 1861, Loss 0.28307223320007324\n",
      "[Training Epoch 1] Batch 1862, Loss 0.2889282703399658\n",
      "[Training Epoch 1] Batch 1863, Loss 0.30262213945388794\n",
      "[Training Epoch 1] Batch 1864, Loss 0.32049471139907837\n",
      "[Training Epoch 1] Batch 1865, Loss 0.3182995915412903\n",
      "[Training Epoch 1] Batch 1866, Loss 0.29011186957359314\n",
      "[Training Epoch 1] Batch 1867, Loss 0.2936214208602905\n",
      "[Training Epoch 1] Batch 1868, Loss 0.2648187577724457\n",
      "[Training Epoch 1] Batch 1869, Loss 0.3061975836753845\n",
      "[Training Epoch 1] Batch 1870, Loss 0.28472304344177246\n",
      "[Training Epoch 1] Batch 1871, Loss 0.2962864339351654\n",
      "[Training Epoch 1] Batch 1872, Loss 0.30348944664001465\n",
      "[Training Epoch 1] Batch 1873, Loss 0.31996506452560425\n",
      "[Training Epoch 1] Batch 1874, Loss 0.3092828691005707\n",
      "[Training Epoch 1] Batch 1875, Loss 0.3178476095199585\n",
      "[Training Epoch 1] Batch 1876, Loss 0.28854507207870483\n",
      "[Training Epoch 1] Batch 1877, Loss 0.28404468297958374\n",
      "[Training Epoch 1] Batch 1878, Loss 0.280813068151474\n",
      "[Training Epoch 1] Batch 1879, Loss 0.32221877574920654\n",
      "[Training Epoch 1] Batch 1880, Loss 0.2965349853038788\n",
      "[Training Epoch 1] Batch 1881, Loss 0.2795218229293823\n",
      "[Training Epoch 1] Batch 1882, Loss 0.3145437240600586\n",
      "[Training Epoch 1] Batch 1883, Loss 0.30721166729927063\n",
      "[Training Epoch 1] Batch 1884, Loss 0.2810192108154297\n",
      "[Training Epoch 1] Batch 1885, Loss 0.3042570650577545\n",
      "[Training Epoch 1] Batch 1886, Loss 0.29857856035232544\n",
      "[Training Epoch 1] Batch 1887, Loss 0.2941014766693115\n",
      "[Training Epoch 1] Batch 1888, Loss 0.3060263991355896\n",
      "[Training Epoch 1] Batch 1889, Loss 0.2975229024887085\n",
      "[Training Epoch 1] Batch 1890, Loss 0.279840350151062\n",
      "[Training Epoch 1] Batch 1891, Loss 0.29799094796180725\n",
      "[Training Epoch 1] Batch 1892, Loss 0.2927020788192749\n",
      "[Training Epoch 1] Batch 1893, Loss 0.3127245604991913\n",
      "[Training Epoch 1] Batch 1894, Loss 0.2773647904396057\n",
      "[Training Epoch 1] Batch 1895, Loss 0.28389549255371094\n",
      "[Training Epoch 1] Batch 1896, Loss 0.3039245009422302\n",
      "[Training Epoch 1] Batch 1897, Loss 0.28523480892181396\n",
      "[Training Epoch 1] Batch 1898, Loss 0.2754709720611572\n",
      "[Training Epoch 1] Batch 1899, Loss 0.2809501588344574\n",
      "[Training Epoch 1] Batch 1900, Loss 0.299862265586853\n",
      "[Training Epoch 1] Batch 1901, Loss 0.2819616198539734\n",
      "[Training Epoch 1] Batch 1902, Loss 0.3157374858856201\n",
      "[Training Epoch 1] Batch 1903, Loss 0.2853890061378479\n",
      "[Training Epoch 1] Batch 1904, Loss 0.3055598735809326\n",
      "[Training Epoch 1] Batch 1905, Loss 0.3075152039527893\n",
      "[Training Epoch 1] Batch 1906, Loss 0.27913686633110046\n",
      "[Training Epoch 1] Batch 1907, Loss 0.32395753264427185\n",
      "[Training Epoch 1] Batch 1908, Loss 0.32749876379966736\n",
      "[Training Epoch 1] Batch 1909, Loss 0.2913735508918762\n",
      "[Training Epoch 1] Batch 1910, Loss 0.287165105342865\n",
      "[Training Epoch 1] Batch 1911, Loss 0.29536721110343933\n",
      "[Training Epoch 1] Batch 1912, Loss 0.319120317697525\n",
      "[Training Epoch 1] Batch 1913, Loss 0.2981080412864685\n",
      "[Training Epoch 1] Batch 1914, Loss 0.29493433237075806\n",
      "[Training Epoch 1] Batch 1915, Loss 0.28768670558929443\n",
      "[Training Epoch 1] Batch 1916, Loss 0.29621565341949463\n",
      "[Training Epoch 1] Batch 1917, Loss 0.30845004320144653\n",
      "[Training Epoch 1] Batch 1918, Loss 0.30773040652275085\n",
      "[Training Epoch 1] Batch 1919, Loss 0.28527048230171204\n",
      "[Training Epoch 1] Batch 1920, Loss 0.3136342763900757\n",
      "[Training Epoch 1] Batch 1921, Loss 0.29049551486968994\n",
      "[Training Epoch 1] Batch 1922, Loss 0.2923932671546936\n",
      "[Training Epoch 1] Batch 1923, Loss 0.32053321599960327\n",
      "[Training Epoch 1] Batch 1924, Loss 0.3230929970741272\n",
      "[Training Epoch 1] Batch 1925, Loss 0.30711430311203003\n",
      "[Training Epoch 1] Batch 1926, Loss 0.3281988799571991\n",
      "[Training Epoch 1] Batch 1927, Loss 0.2897946238517761\n",
      "[Training Epoch 1] Batch 1928, Loss 0.29943400621414185\n",
      "[Training Epoch 1] Batch 1929, Loss 0.32785603404045105\n",
      "[Training Epoch 1] Batch 1930, Loss 0.30926740169525146\n",
      "[Training Epoch 1] Batch 1931, Loss 0.3205977976322174\n",
      "[Training Epoch 1] Batch 1932, Loss 0.3059600591659546\n",
      "[Training Epoch 1] Batch 1933, Loss 0.29696452617645264\n",
      "[Training Epoch 1] Batch 1934, Loss 0.31522029638290405\n",
      "[Training Epoch 1] Batch 1935, Loss 0.29418495297431946\n",
      "[Training Epoch 1] Batch 1936, Loss 0.2949293851852417\n",
      "[Training Epoch 1] Batch 1937, Loss 0.3194897472858429\n",
      "[Training Epoch 1] Batch 1938, Loss 0.31470733880996704\n",
      "[Training Epoch 1] Batch 1939, Loss 0.3157636523246765\n",
      "[Training Epoch 1] Batch 1940, Loss 0.32165998220443726\n",
      "[Training Epoch 1] Batch 1941, Loss 0.30156946182250977\n",
      "[Training Epoch 1] Batch 1942, Loss 0.33794745802879333\n",
      "[Training Epoch 1] Batch 1943, Loss 0.30226853489875793\n",
      "[Training Epoch 1] Batch 1944, Loss 0.3000187873840332\n",
      "[Training Epoch 1] Batch 1945, Loss 0.304955393075943\n",
      "[Training Epoch 1] Batch 1946, Loss 0.30603158473968506\n",
      "[Training Epoch 1] Batch 1947, Loss 0.309805691242218\n",
      "[Training Epoch 1] Batch 1948, Loss 0.28394436836242676\n",
      "[Training Epoch 1] Batch 1949, Loss 0.31324854493141174\n",
      "[Training Epoch 1] Batch 1950, Loss 0.29686278104782104\n",
      "[Training Epoch 1] Batch 1951, Loss 0.2788804769515991\n",
      "[Training Epoch 1] Batch 1952, Loss 0.2752368748188019\n",
      "[Training Epoch 1] Batch 1953, Loss 0.2910394072532654\n",
      "[Training Epoch 1] Batch 1954, Loss 0.2637282609939575\n",
      "[Training Epoch 1] Batch 1955, Loss 0.2995535731315613\n",
      "[Training Epoch 1] Batch 1956, Loss 0.3051930367946625\n",
      "[Training Epoch 1] Batch 1957, Loss 0.32331669330596924\n",
      "[Training Epoch 1] Batch 1958, Loss 0.3091655969619751\n",
      "[Training Epoch 1] Batch 1959, Loss 0.336838960647583\n",
      "[Training Epoch 1] Batch 1960, Loss 0.3061492443084717\n",
      "[Training Epoch 1] Batch 1961, Loss 0.31954559683799744\n",
      "[Training Epoch 1] Batch 1962, Loss 0.2933768630027771\n",
      "[Training Epoch 1] Batch 1963, Loss 0.3005061149597168\n",
      "[Training Epoch 1] Batch 1964, Loss 0.30236679315567017\n",
      "[Training Epoch 1] Batch 1965, Loss 0.27872464060783386\n",
      "[Training Epoch 1] Batch 1966, Loss 0.28965136408805847\n",
      "[Training Epoch 1] Batch 1967, Loss 0.30566662549972534\n",
      "[Training Epoch 1] Batch 1968, Loss 0.2976195514202118\n",
      "[Training Epoch 1] Batch 1969, Loss 0.28496524691581726\n",
      "[Training Epoch 1] Batch 1970, Loss 0.3024393320083618\n",
      "[Training Epoch 1] Batch 1971, Loss 0.3217293620109558\n",
      "[Training Epoch 1] Batch 1972, Loss 0.2948756814002991\n",
      "[Training Epoch 1] Batch 1973, Loss 0.2911345362663269\n",
      "[Training Epoch 1] Batch 1974, Loss 0.29320499300956726\n",
      "[Training Epoch 1] Batch 1975, Loss 0.312200665473938\n",
      "[Training Epoch 1] Batch 1976, Loss 0.30845528841018677\n",
      "[Training Epoch 1] Batch 1977, Loss 0.3062131404876709\n",
      "[Training Epoch 1] Batch 1978, Loss 0.28433358669281006\n",
      "[Training Epoch 1] Batch 1979, Loss 0.2762683033943176\n",
      "[Training Epoch 1] Batch 1980, Loss 0.29042550921440125\n",
      "[Training Epoch 1] Batch 1981, Loss 0.29933375120162964\n",
      "[Training Epoch 1] Batch 1982, Loss 0.29066717624664307\n",
      "[Training Epoch 1] Batch 1983, Loss 0.313478946685791\n",
      "[Training Epoch 1] Batch 1984, Loss 0.2743435800075531\n",
      "[Training Epoch 1] Batch 1985, Loss 0.3083738088607788\n",
      "[Training Epoch 1] Batch 1986, Loss 0.3132156431674957\n",
      "[Training Epoch 1] Batch 1987, Loss 0.30370497703552246\n",
      "[Training Epoch 1] Batch 1988, Loss 0.29829198122024536\n",
      "[Training Epoch 1] Batch 1989, Loss 0.295646071434021\n",
      "[Training Epoch 1] Batch 1990, Loss 0.2811397612094879\n",
      "[Training Epoch 1] Batch 1991, Loss 0.3109821081161499\n",
      "[Training Epoch 1] Batch 1992, Loss 0.33623090386390686\n",
      "[Training Epoch 1] Batch 1993, Loss 0.2897055149078369\n",
      "[Training Epoch 1] Batch 1994, Loss 0.3059193193912506\n",
      "[Training Epoch 1] Batch 1995, Loss 0.2936975955963135\n",
      "[Training Epoch 1] Batch 1996, Loss 0.318234920501709\n",
      "[Training Epoch 1] Batch 1997, Loss 0.29070669412612915\n",
      "[Training Epoch 1] Batch 1998, Loss 0.28474506735801697\n",
      "[Training Epoch 1] Batch 1999, Loss 0.30210238695144653\n",
      "[Training Epoch 1] Batch 2000, Loss 0.264579713344574\n",
      "[Training Epoch 1] Batch 2001, Loss 0.2716940939426422\n",
      "[Training Epoch 1] Batch 2002, Loss 0.29958420991897583\n",
      "[Training Epoch 1] Batch 2003, Loss 0.29938119649887085\n",
      "[Training Epoch 1] Batch 2004, Loss 0.299355685710907\n",
      "[Training Epoch 1] Batch 2005, Loss 0.32678520679473877\n",
      "[Training Epoch 1] Batch 2006, Loss 0.2884606719017029\n",
      "[Training Epoch 1] Batch 2007, Loss 0.2998538613319397\n",
      "[Training Epoch 1] Batch 2008, Loss 0.28016024827957153\n",
      "[Training Epoch 1] Batch 2009, Loss 0.2915094792842865\n",
      "[Training Epoch 1] Batch 2010, Loss 0.3183404505252838\n",
      "[Training Epoch 1] Batch 2011, Loss 0.32027947902679443\n",
      "[Training Epoch 1] Batch 2012, Loss 0.28679394721984863\n",
      "[Training Epoch 1] Batch 2013, Loss 0.3043708801269531\n",
      "[Training Epoch 1] Batch 2014, Loss 0.2757049798965454\n",
      "[Training Epoch 1] Batch 2015, Loss 0.28310102224349976\n",
      "[Training Epoch 1] Batch 2016, Loss 0.31047797203063965\n",
      "[Training Epoch 1] Batch 2017, Loss 0.28382742404937744\n",
      "[Training Epoch 1] Batch 2018, Loss 0.3490481376647949\n",
      "[Training Epoch 1] Batch 2019, Loss 0.3082042634487152\n",
      "[Training Epoch 1] Batch 2020, Loss 0.3046596050262451\n",
      "[Training Epoch 1] Batch 2021, Loss 0.3426639437675476\n",
      "[Training Epoch 1] Batch 2022, Loss 0.2783137261867523\n",
      "[Training Epoch 1] Batch 2023, Loss 0.26448607444763184\n",
      "[Training Epoch 1] Batch 2024, Loss 0.3074636161327362\n",
      "[Training Epoch 1] Batch 2025, Loss 0.2855517268180847\n",
      "[Training Epoch 1] Batch 2026, Loss 0.3191278576850891\n",
      "[Training Epoch 1] Batch 2027, Loss 0.28563472628593445\n",
      "[Training Epoch 1] Batch 2028, Loss 0.333493173122406\n",
      "[Training Epoch 1] Batch 2029, Loss 0.295904278755188\n",
      "[Training Epoch 1] Batch 2030, Loss 0.282898485660553\n",
      "[Training Epoch 1] Batch 2031, Loss 0.28133195638656616\n",
      "[Training Epoch 1] Batch 2032, Loss 0.301933228969574\n",
      "[Training Epoch 1] Batch 2033, Loss 0.2871761918067932\n",
      "[Training Epoch 1] Batch 2034, Loss 0.3030844032764435\n",
      "[Training Epoch 1] Batch 2035, Loss 0.30487555265426636\n",
      "[Training Epoch 1] Batch 2036, Loss 0.34558165073394775\n",
      "[Training Epoch 1] Batch 2037, Loss 0.30828607082366943\n",
      "[Training Epoch 1] Batch 2038, Loss 0.31189578771591187\n",
      "[Training Epoch 1] Batch 2039, Loss 0.29766368865966797\n",
      "[Training Epoch 1] Batch 2040, Loss 0.307390421628952\n",
      "[Training Epoch 1] Batch 2041, Loss 0.2911996841430664\n",
      "[Training Epoch 1] Batch 2042, Loss 0.28919869661331177\n",
      "[Training Epoch 1] Batch 2043, Loss 0.3064129054546356\n",
      "[Training Epoch 1] Batch 2044, Loss 0.30069077014923096\n",
      "[Training Epoch 1] Batch 2045, Loss 0.3018638491630554\n",
      "[Training Epoch 1] Batch 2046, Loss 0.29024815559387207\n",
      "[Training Epoch 1] Batch 2047, Loss 0.2911246418952942\n",
      "[Training Epoch 1] Batch 2048, Loss 0.3008599877357483\n",
      "[Training Epoch 1] Batch 2049, Loss 0.31515300273895264\n",
      "[Training Epoch 1] Batch 2050, Loss 0.3116111159324646\n",
      "[Training Epoch 1] Batch 2051, Loss 0.3072695732116699\n",
      "[Training Epoch 1] Batch 2052, Loss 0.2900514006614685\n",
      "[Training Epoch 1] Batch 2053, Loss 0.30111008882522583\n",
      "[Training Epoch 1] Batch 2054, Loss 0.30445414781570435\n",
      "[Training Epoch 1] Batch 2055, Loss 0.2764650285243988\n",
      "[Training Epoch 1] Batch 2056, Loss 0.28967827558517456\n",
      "[Training Epoch 1] Batch 2057, Loss 0.3110637366771698\n",
      "[Training Epoch 1] Batch 2058, Loss 0.3233710527420044\n",
      "[Training Epoch 1] Batch 2059, Loss 0.28066426515579224\n",
      "[Training Epoch 1] Batch 2060, Loss 0.3184449076652527\n",
      "[Training Epoch 1] Batch 2061, Loss 0.3101431727409363\n",
      "[Training Epoch 1] Batch 2062, Loss 0.285550057888031\n",
      "[Training Epoch 1] Batch 2063, Loss 0.31930673122406006\n",
      "[Training Epoch 1] Batch 2064, Loss 0.2985462248325348\n",
      "[Training Epoch 1] Batch 2065, Loss 0.294150173664093\n",
      "[Training Epoch 1] Batch 2066, Loss 0.26690012216567993\n",
      "[Training Epoch 1] Batch 2067, Loss 0.29821479320526123\n",
      "[Training Epoch 1] Batch 2068, Loss 0.29591602087020874\n",
      "[Training Epoch 1] Batch 2069, Loss 0.300523042678833\n",
      "[Training Epoch 1] Batch 2070, Loss 0.2881234288215637\n",
      "[Training Epoch 1] Batch 2071, Loss 0.3007209300994873\n",
      "[Training Epoch 1] Batch 2072, Loss 0.294944703578949\n",
      "[Training Epoch 1] Batch 2073, Loss 0.30193257331848145\n",
      "[Training Epoch 1] Batch 2074, Loss 0.26424312591552734\n",
      "[Training Epoch 1] Batch 2075, Loss 0.3279419541358948\n",
      "[Training Epoch 1] Batch 2076, Loss 0.29024362564086914\n",
      "[Training Epoch 1] Batch 2077, Loss 0.31067466735839844\n",
      "[Training Epoch 1] Batch 2078, Loss 0.30731505155563354\n",
      "[Training Epoch 1] Batch 2079, Loss 0.2927125096321106\n",
      "[Training Epoch 1] Batch 2080, Loss 0.2835958003997803\n",
      "[Training Epoch 1] Batch 2081, Loss 0.3122760057449341\n",
      "[Training Epoch 1] Batch 2082, Loss 0.27208009362220764\n",
      "[Training Epoch 1] Batch 2083, Loss 0.3036508858203888\n",
      "[Training Epoch 1] Batch 2084, Loss 0.302653044462204\n",
      "[Training Epoch 1] Batch 2085, Loss 0.2997523546218872\n",
      "[Training Epoch 1] Batch 2086, Loss 0.31583157181739807\n",
      "[Training Epoch 1] Batch 2087, Loss 0.2814232110977173\n",
      "[Training Epoch 1] Batch 2088, Loss 0.2957756519317627\n",
      "[Training Epoch 1] Batch 2089, Loss 0.32383713126182556\n",
      "[Training Epoch 1] Batch 2090, Loss 0.2862699031829834\n",
      "[Training Epoch 1] Batch 2091, Loss 0.32212719321250916\n",
      "[Training Epoch 1] Batch 2092, Loss 0.3045226037502289\n",
      "[Training Epoch 1] Batch 2093, Loss 0.28000593185424805\n",
      "[Training Epoch 1] Batch 2094, Loss 0.29721853137016296\n",
      "[Training Epoch 1] Batch 2095, Loss 0.2890329360961914\n",
      "[Training Epoch 1] Batch 2096, Loss 0.3067365884780884\n",
      "[Training Epoch 1] Batch 2097, Loss 0.2796062231063843\n",
      "[Training Epoch 1] Batch 2098, Loss 0.27884119749069214\n",
      "[Training Epoch 1] Batch 2099, Loss 0.30118733644485474\n",
      "[Training Epoch 1] Batch 2100, Loss 0.29817768931388855\n",
      "[Training Epoch 1] Batch 2101, Loss 0.3024880886077881\n",
      "[Training Epoch 1] Batch 2102, Loss 0.2953640818595886\n",
      "[Training Epoch 1] Batch 2103, Loss 0.2727351784706116\n",
      "[Training Epoch 1] Batch 2104, Loss 0.2837497889995575\n",
      "[Training Epoch 1] Batch 2105, Loss 0.32462915778160095\n",
      "[Training Epoch 1] Batch 2106, Loss 0.2698598802089691\n",
      "[Training Epoch 1] Batch 2107, Loss 0.2640616297721863\n",
      "[Training Epoch 1] Batch 2108, Loss 0.29322686791419983\n",
      "[Training Epoch 1] Batch 2109, Loss 0.3051113486289978\n",
      "[Training Epoch 1] Batch 2110, Loss 0.3051646649837494\n",
      "[Training Epoch 1] Batch 2111, Loss 0.29677191376686096\n",
      "[Training Epoch 1] Batch 2112, Loss 0.28796809911727905\n",
      "[Training Epoch 1] Batch 2113, Loss 0.27598685026168823\n",
      "[Training Epoch 1] Batch 2114, Loss 0.31592583656311035\n",
      "[Training Epoch 1] Batch 2115, Loss 0.3089686930179596\n",
      "[Training Epoch 1] Batch 2116, Loss 0.31253480911254883\n",
      "[Training Epoch 1] Batch 2117, Loss 0.2802187204360962\n",
      "[Training Epoch 1] Batch 2118, Loss 0.32598793506622314\n",
      "[Training Epoch 1] Batch 2119, Loss 0.31588122248649597\n",
      "[Training Epoch 1] Batch 2120, Loss 0.2977766990661621\n",
      "[Training Epoch 1] Batch 2121, Loss 0.2903183102607727\n",
      "[Training Epoch 1] Batch 2122, Loss 0.2955699861049652\n",
      "[Training Epoch 1] Batch 2123, Loss 0.3128523826599121\n",
      "[Training Epoch 1] Batch 2124, Loss 0.2969534993171692\n",
      "[Training Epoch 1] Batch 2125, Loss 0.2779076397418976\n",
      "[Training Epoch 1] Batch 2126, Loss 0.30225223302841187\n",
      "[Training Epoch 1] Batch 2127, Loss 0.28846025466918945\n",
      "[Training Epoch 1] Batch 2128, Loss 0.29164421558380127\n",
      "[Training Epoch 1] Batch 2129, Loss 0.28619223833084106\n",
      "[Training Epoch 1] Batch 2130, Loss 0.3288452625274658\n",
      "[Training Epoch 1] Batch 2131, Loss 0.3066565990447998\n",
      "[Training Epoch 1] Batch 2132, Loss 0.3128657341003418\n",
      "[Training Epoch 1] Batch 2133, Loss 0.3071559965610504\n",
      "[Training Epoch 1] Batch 2134, Loss 0.30459272861480713\n",
      "[Training Epoch 1] Batch 2135, Loss 0.3325931131839752\n",
      "[Training Epoch 1] Batch 2136, Loss 0.2790853977203369\n",
      "[Training Epoch 1] Batch 2137, Loss 0.3194458782672882\n",
      "[Training Epoch 1] Batch 2138, Loss 0.30676496028900146\n",
      "[Training Epoch 1] Batch 2139, Loss 0.2781449556350708\n",
      "[Training Epoch 1] Batch 2140, Loss 0.3261669874191284\n",
      "[Training Epoch 1] Batch 2141, Loss 0.2853810787200928\n",
      "[Training Epoch 1] Batch 2142, Loss 0.3151741027832031\n",
      "[Training Epoch 1] Batch 2143, Loss 0.2906903028488159\n",
      "[Training Epoch 1] Batch 2144, Loss 0.29434844851493835\n",
      "[Training Epoch 1] Batch 2145, Loss 0.33536484837532043\n",
      "[Training Epoch 1] Batch 2146, Loss 0.30454909801483154\n",
      "[Training Epoch 1] Batch 2147, Loss 0.30725008249282837\n",
      "[Training Epoch 1] Batch 2148, Loss 0.2982368767261505\n",
      "[Training Epoch 1] Batch 2149, Loss 0.3231436610221863\n",
      "[Training Epoch 1] Batch 2150, Loss 0.300188273191452\n",
      "[Training Epoch 1] Batch 2151, Loss 0.3021293878555298\n",
      "[Training Epoch 1] Batch 2152, Loss 0.31460699439048767\n",
      "[Training Epoch 1] Batch 2153, Loss 0.32268649339675903\n",
      "[Training Epoch 1] Batch 2154, Loss 0.32175707817077637\n",
      "[Training Epoch 1] Batch 2155, Loss 0.28580987453460693\n",
      "[Training Epoch 1] Batch 2156, Loss 0.3089354634284973\n",
      "[Training Epoch 1] Batch 2157, Loss 0.29713907837867737\n",
      "[Training Epoch 1] Batch 2158, Loss 0.29167264699935913\n",
      "[Training Epoch 1] Batch 2159, Loss 0.3027668595314026\n",
      "[Training Epoch 1] Batch 2160, Loss 0.29306885600090027\n",
      "[Training Epoch 1] Batch 2161, Loss 0.27087849378585815\n",
      "[Training Epoch 1] Batch 2162, Loss 0.330349862575531\n",
      "[Training Epoch 1] Batch 2163, Loss 0.3174015283584595\n",
      "[Training Epoch 1] Batch 2164, Loss 0.3170894384384155\n",
      "[Training Epoch 1] Batch 2165, Loss 0.31480324268341064\n",
      "[Training Epoch 1] Batch 2166, Loss 0.2994520962238312\n",
      "[Training Epoch 1] Batch 2167, Loss 0.28541940450668335\n",
      "[Training Epoch 1] Batch 2168, Loss 0.3177628219127655\n",
      "[Training Epoch 1] Batch 2169, Loss 0.31699323654174805\n",
      "[Training Epoch 1] Batch 2170, Loss 0.30007636547088623\n",
      "[Training Epoch 1] Batch 2171, Loss 0.3187338709831238\n",
      "[Training Epoch 1] Batch 2172, Loss 0.29241037368774414\n",
      "[Training Epoch 1] Batch 2173, Loss 0.2778758108615875\n",
      "[Training Epoch 1] Batch 2174, Loss 0.29517635703086853\n",
      "[Training Epoch 1] Batch 2175, Loss 0.2953326106071472\n",
      "[Training Epoch 1] Batch 2176, Loss 0.30979272723197937\n",
      "[Training Epoch 1] Batch 2177, Loss 0.28151899576187134\n",
      "[Training Epoch 1] Batch 2178, Loss 0.2987607419490814\n",
      "[Training Epoch 1] Batch 2179, Loss 0.27134451270103455\n",
      "[Training Epoch 1] Batch 2180, Loss 0.2807926535606384\n",
      "[Training Epoch 1] Batch 2181, Loss 0.3222627341747284\n",
      "[Training Epoch 1] Batch 2182, Loss 0.3196210265159607\n",
      "[Training Epoch 1] Batch 2183, Loss 0.30932140350341797\n",
      "[Training Epoch 1] Batch 2184, Loss 0.30863216519355774\n",
      "[Training Epoch 1] Batch 2185, Loss 0.28919047117233276\n",
      "[Training Epoch 1] Batch 2186, Loss 0.34105342626571655\n",
      "[Training Epoch 1] Batch 2187, Loss 0.29641193151474\n",
      "[Training Epoch 1] Batch 2188, Loss 0.3112819492816925\n",
      "[Training Epoch 1] Batch 2189, Loss 0.29755306243896484\n",
      "[Training Epoch 1] Batch 2190, Loss 0.32288190722465515\n",
      "[Training Epoch 1] Batch 2191, Loss 0.2754194736480713\n",
      "[Training Epoch 1] Batch 2192, Loss 0.3015260398387909\n",
      "[Training Epoch 1] Batch 2193, Loss 0.3094751834869385\n",
      "[Training Epoch 1] Batch 2194, Loss 0.309540331363678\n",
      "[Training Epoch 1] Batch 2195, Loss 0.26547756791114807\n",
      "[Training Epoch 1] Batch 2196, Loss 0.29063355922698975\n",
      "[Training Epoch 1] Batch 2197, Loss 0.29736754298210144\n",
      "[Training Epoch 1] Batch 2198, Loss 0.29888206720352173\n",
      "[Training Epoch 1] Batch 2199, Loss 0.31812047958374023\n",
      "[Training Epoch 1] Batch 2200, Loss 0.28991246223449707\n",
      "[Training Epoch 1] Batch 2201, Loss 0.34276220202445984\n",
      "[Training Epoch 1] Batch 2202, Loss 0.32645735144615173\n",
      "[Training Epoch 1] Batch 2203, Loss 0.28447017073631287\n",
      "[Training Epoch 1] Batch 2204, Loss 0.28312188386917114\n",
      "[Training Epoch 1] Batch 2205, Loss 0.2850628197193146\n",
      "[Training Epoch 1] Batch 2206, Loss 0.2930510640144348\n",
      "[Training Epoch 1] Batch 2207, Loss 0.28015071153640747\n",
      "[Training Epoch 1] Batch 2208, Loss 0.2768525779247284\n",
      "[Training Epoch 1] Batch 2209, Loss 0.26335394382476807\n",
      "[Training Epoch 1] Batch 2210, Loss 0.2771722674369812\n",
      "[Training Epoch 1] Batch 2211, Loss 0.27982771396636963\n",
      "[Training Epoch 1] Batch 2212, Loss 0.3061234652996063\n",
      "[Training Epoch 1] Batch 2213, Loss 0.32660138607025146\n",
      "[Training Epoch 1] Batch 2214, Loss 0.29031410813331604\n",
      "[Training Epoch 1] Batch 2215, Loss 0.26915502548217773\n",
      "[Training Epoch 1] Batch 2216, Loss 0.3119834065437317\n",
      "[Training Epoch 1] Batch 2217, Loss 0.3082524240016937\n",
      "[Training Epoch 1] Batch 2218, Loss 0.30408963561058044\n",
      "[Training Epoch 1] Batch 2219, Loss 0.29068538546562195\n",
      "[Training Epoch 1] Batch 2220, Loss 0.32449620962142944\n",
      "[Training Epoch 1] Batch 2221, Loss 0.32186853885650635\n",
      "[Training Epoch 1] Batch 2222, Loss 0.283025324344635\n",
      "[Training Epoch 1] Batch 2223, Loss 0.30703264474868774\n",
      "[Training Epoch 1] Batch 2224, Loss 0.3083302974700928\n",
      "[Training Epoch 1] Batch 2225, Loss 0.27941545844078064\n",
      "[Training Epoch 1] Batch 2226, Loss 0.2872772216796875\n",
      "[Training Epoch 1] Batch 2227, Loss 0.3134387135505676\n",
      "[Training Epoch 1] Batch 2228, Loss 0.2931743860244751\n",
      "[Training Epoch 1] Batch 2229, Loss 0.3164967894554138\n",
      "[Training Epoch 1] Batch 2230, Loss 0.30419227480888367\n",
      "[Training Epoch 1] Batch 2231, Loss 0.29136893153190613\n",
      "[Training Epoch 1] Batch 2232, Loss 0.3025844693183899\n",
      "[Training Epoch 1] Batch 2233, Loss 0.27120375633239746\n",
      "[Training Epoch 1] Batch 2234, Loss 0.262463241815567\n",
      "[Training Epoch 1] Batch 2235, Loss 0.29383429884910583\n",
      "[Training Epoch 1] Batch 2236, Loss 0.2929598391056061\n",
      "[Training Epoch 1] Batch 2237, Loss 0.28332459926605225\n",
      "[Training Epoch 1] Batch 2238, Loss 0.28744935989379883\n",
      "[Training Epoch 1] Batch 2239, Loss 0.28184545040130615\n",
      "[Training Epoch 1] Batch 2240, Loss 0.30432653427124023\n",
      "[Training Epoch 1] Batch 2241, Loss 0.3028336763381958\n",
      "[Training Epoch 1] Batch 2242, Loss 0.3052523136138916\n",
      "[Training Epoch 1] Batch 2243, Loss 0.29715853929519653\n",
      "[Training Epoch 1] Batch 2244, Loss 0.3224129378795624\n",
      "[Training Epoch 1] Batch 2245, Loss 0.30157390236854553\n",
      "[Training Epoch 1] Batch 2246, Loss 0.30701911449432373\n",
      "[Training Epoch 1] Batch 2247, Loss 0.29209038615226746\n",
      "[Training Epoch 1] Batch 2248, Loss 0.31729745864868164\n",
      "[Training Epoch 1] Batch 2249, Loss 0.3383687138557434\n",
      "[Training Epoch 1] Batch 2250, Loss 0.2925935983657837\n",
      "[Training Epoch 1] Batch 2251, Loss 0.30876249074935913\n",
      "[Training Epoch 1] Batch 2252, Loss 0.28989002108573914\n",
      "[Training Epoch 1] Batch 2253, Loss 0.28576281666755676\n",
      "[Training Epoch 1] Batch 2254, Loss 0.2960372567176819\n",
      "[Training Epoch 1] Batch 2255, Loss 0.31569331884384155\n",
      "[Training Epoch 1] Batch 2256, Loss 0.26368582248687744\n",
      "[Training Epoch 1] Batch 2257, Loss 0.26975446939468384\n",
      "[Training Epoch 1] Batch 2258, Loss 0.29509401321411133\n",
      "[Training Epoch 1] Batch 2259, Loss 0.27251899242401123\n",
      "[Training Epoch 1] Batch 2260, Loss 0.27722984552383423\n",
      "[Training Epoch 1] Batch 2261, Loss 0.30780652165412903\n",
      "[Training Epoch 1] Batch 2262, Loss 0.32797351479530334\n",
      "[Training Epoch 1] Batch 2263, Loss 0.30041369795799255\n",
      "[Training Epoch 1] Batch 2264, Loss 0.31330469250679016\n",
      "[Training Epoch 1] Batch 2265, Loss 0.29340553283691406\n",
      "[Training Epoch 1] Batch 2266, Loss 0.28888753056526184\n",
      "[Training Epoch 1] Batch 2267, Loss 0.32288670539855957\n",
      "[Training Epoch 1] Batch 2268, Loss 0.3175296187400818\n",
      "[Training Epoch 1] Batch 2269, Loss 0.30598390102386475\n",
      "[Training Epoch 1] Batch 2270, Loss 0.30204299092292786\n",
      "[Training Epoch 1] Batch 2271, Loss 0.30487513542175293\n",
      "[Training Epoch 1] Batch 2272, Loss 0.29819464683532715\n",
      "[Training Epoch 1] Batch 2273, Loss 0.2887028455734253\n",
      "[Training Epoch 1] Batch 2274, Loss 0.2975025177001953\n",
      "[Training Epoch 1] Batch 2275, Loss 0.2998679280281067\n",
      "[Training Epoch 1] Batch 2276, Loss 0.31657326221466064\n",
      "[Training Epoch 1] Batch 2277, Loss 0.3171444535255432\n",
      "[Training Epoch 1] Batch 2278, Loss 0.30085575580596924\n",
      "[Training Epoch 1] Batch 2279, Loss 0.2907467186450958\n",
      "[Training Epoch 1] Batch 2280, Loss 0.28079482913017273\n",
      "[Training Epoch 1] Batch 2281, Loss 0.2984023690223694\n",
      "[Training Epoch 1] Batch 2282, Loss 0.2911459803581238\n",
      "[Training Epoch 1] Batch 2283, Loss 0.31263142824172974\n",
      "[Training Epoch 1] Batch 2284, Loss 0.3098699450492859\n",
      "[Training Epoch 1] Batch 2285, Loss 0.2839401662349701\n",
      "[Training Epoch 1] Batch 2286, Loss 0.303743451833725\n",
      "[Training Epoch 1] Batch 2287, Loss 0.31281712651252747\n",
      "[Training Epoch 1] Batch 2288, Loss 0.28185874223709106\n",
      "[Training Epoch 1] Batch 2289, Loss 0.29905053973197937\n",
      "[Training Epoch 1] Batch 2290, Loss 0.30683839321136475\n",
      "[Training Epoch 1] Batch 2291, Loss 0.3207523226737976\n",
      "[Training Epoch 1] Batch 2292, Loss 0.3053160011768341\n",
      "[Training Epoch 1] Batch 2293, Loss 0.3164490759372711\n",
      "[Training Epoch 1] Batch 2294, Loss 0.2694953680038452\n",
      "[Training Epoch 1] Batch 2295, Loss 0.2880370318889618\n",
      "[Training Epoch 1] Batch 2296, Loss 0.31333127617836\n",
      "[Training Epoch 1] Batch 2297, Loss 0.31304025650024414\n",
      "[Training Epoch 1] Batch 2298, Loss 0.28101104497909546\n",
      "[Training Epoch 1] Batch 2299, Loss 0.3249228596687317\n",
      "[Training Epoch 1] Batch 2300, Loss 0.296566367149353\n",
      "[Training Epoch 1] Batch 2301, Loss 0.28779691457748413\n",
      "[Training Epoch 1] Batch 2302, Loss 0.31938549876213074\n",
      "[Training Epoch 1] Batch 2303, Loss 0.3120558261871338\n",
      "[Training Epoch 1] Batch 2304, Loss 0.290314644575119\n",
      "[Training Epoch 1] Batch 2305, Loss 0.3038032054901123\n",
      "[Training Epoch 1] Batch 2306, Loss 0.27646905183792114\n",
      "[Training Epoch 1] Batch 2307, Loss 0.31595784425735474\n",
      "[Training Epoch 1] Batch 2308, Loss 0.27760636806488037\n",
      "[Training Epoch 1] Batch 2309, Loss 0.2874932885169983\n",
      "[Training Epoch 1] Batch 2310, Loss 0.308224081993103\n",
      "[Training Epoch 1] Batch 2311, Loss 0.27733203768730164\n",
      "[Training Epoch 1] Batch 2312, Loss 0.30997800827026367\n",
      "[Training Epoch 1] Batch 2313, Loss 0.2993573546409607\n",
      "[Training Epoch 1] Batch 2314, Loss 0.3179807662963867\n",
      "[Training Epoch 1] Batch 2315, Loss 0.29361408948898315\n",
      "[Training Epoch 1] Batch 2316, Loss 0.28128641843795776\n",
      "[Training Epoch 1] Batch 2317, Loss 0.2826203405857086\n",
      "[Training Epoch 1] Batch 2318, Loss 0.28324514627456665\n",
      "[Training Epoch 1] Batch 2319, Loss 0.29950690269470215\n",
      "[Training Epoch 1] Batch 2320, Loss 0.27058762311935425\n",
      "[Training Epoch 1] Batch 2321, Loss 0.25402769446372986\n",
      "[Training Epoch 1] Batch 2322, Loss 0.32132524251937866\n",
      "[Training Epoch 1] Batch 2323, Loss 0.29968130588531494\n",
      "[Training Epoch 1] Batch 2324, Loss 0.30058085918426514\n",
      "[Training Epoch 1] Batch 2325, Loss 0.32065317034721375\n",
      "[Training Epoch 1] Batch 2326, Loss 0.2959499955177307\n",
      "[Training Epoch 1] Batch 2327, Loss 0.31957805156707764\n",
      "[Training Epoch 1] Batch 2328, Loss 0.28695011138916016\n",
      "[Training Epoch 1] Batch 2329, Loss 0.29393500089645386\n",
      "[Training Epoch 1] Batch 2330, Loss 0.2963995039463043\n",
      "[Training Epoch 1] Batch 2331, Loss 0.286138117313385\n",
      "[Training Epoch 1] Batch 2332, Loss 0.28957420587539673\n",
      "[Training Epoch 1] Batch 2333, Loss 0.28212830424308777\n",
      "[Training Epoch 1] Batch 2334, Loss 0.30353420972824097\n",
      "[Training Epoch 1] Batch 2335, Loss 0.3181859850883484\n",
      "[Training Epoch 1] Batch 2336, Loss 0.29529130458831787\n",
      "[Training Epoch 1] Batch 2337, Loss 0.28918999433517456\n",
      "[Training Epoch 1] Batch 2338, Loss 0.2985890209674835\n",
      "[Training Epoch 1] Batch 2339, Loss 0.27235811948776245\n",
      "[Training Epoch 1] Batch 2340, Loss 0.3097536861896515\n",
      "[Training Epoch 1] Batch 2341, Loss 0.2806221842765808\n",
      "[Training Epoch 1] Batch 2342, Loss 0.2991545796394348\n",
      "[Training Epoch 1] Batch 2343, Loss 0.3133001923561096\n",
      "[Training Epoch 1] Batch 2344, Loss 0.29750221967697144\n",
      "[Training Epoch 1] Batch 2345, Loss 0.2840130925178528\n",
      "[Training Epoch 1] Batch 2346, Loss 0.32217589020729065\n",
      "[Training Epoch 1] Batch 2347, Loss 0.3042595386505127\n",
      "[Training Epoch 1] Batch 2348, Loss 0.32372984290122986\n",
      "[Training Epoch 1] Batch 2349, Loss 0.2910240590572357\n",
      "[Training Epoch 1] Batch 2350, Loss 0.31166529655456543\n",
      "[Training Epoch 1] Batch 2351, Loss 0.2922953963279724\n",
      "[Training Epoch 1] Batch 2352, Loss 0.27291339635849\n",
      "[Training Epoch 1] Batch 2353, Loss 0.3417367935180664\n",
      "[Training Epoch 1] Batch 2354, Loss 0.31668946146965027\n",
      "[Training Epoch 1] Batch 2355, Loss 0.31769540905952454\n",
      "[Training Epoch 1] Batch 2356, Loss 0.29706358909606934\n",
      "[Training Epoch 1] Batch 2357, Loss 0.2670193910598755\n",
      "[Training Epoch 1] Batch 2358, Loss 0.2933623790740967\n",
      "[Training Epoch 1] Batch 2359, Loss 0.294287770986557\n",
      "[Training Epoch 1] Batch 2360, Loss 0.2975143790245056\n",
      "[Training Epoch 1] Batch 2361, Loss 0.3302323818206787\n",
      "[Training Epoch 1] Batch 2362, Loss 0.2947341799736023\n",
      "[Training Epoch 1] Batch 2363, Loss 0.30667656660079956\n",
      "[Training Epoch 1] Batch 2364, Loss 0.3138642907142639\n",
      "[Training Epoch 1] Batch 2365, Loss 0.28776443004608154\n",
      "[Training Epoch 1] Batch 2366, Loss 0.28872740268707275\n",
      "[Training Epoch 1] Batch 2367, Loss 0.29921478033065796\n",
      "[Training Epoch 1] Batch 2368, Loss 0.2992030382156372\n",
      "[Training Epoch 1] Batch 2369, Loss 0.3108205795288086\n",
      "[Training Epoch 1] Batch 2370, Loss 0.28684741258621216\n",
      "[Training Epoch 1] Batch 2371, Loss 0.2901720106601715\n",
      "[Training Epoch 1] Batch 2372, Loss 0.293765664100647\n",
      "[Training Epoch 1] Batch 2373, Loss 0.3215183615684509\n",
      "[Training Epoch 1] Batch 2374, Loss 0.29945069551467896\n",
      "[Training Epoch 1] Batch 2375, Loss 0.303691029548645\n",
      "[Training Epoch 1] Batch 2376, Loss 0.3358886241912842\n",
      "[Training Epoch 1] Batch 2377, Loss 0.29467087984085083\n",
      "[Training Epoch 1] Batch 2378, Loss 0.2900509238243103\n",
      "[Training Epoch 1] Batch 2379, Loss 0.2692432403564453\n",
      "[Training Epoch 1] Batch 2380, Loss 0.3065391778945923\n",
      "[Training Epoch 1] Batch 2381, Loss 0.28505995869636536\n",
      "[Training Epoch 1] Batch 2382, Loss 0.2829272747039795\n",
      "[Training Epoch 1] Batch 2383, Loss 0.3122881054878235\n",
      "[Training Epoch 1] Batch 2384, Loss 0.2999100685119629\n",
      "[Training Epoch 1] Batch 2385, Loss 0.3276714086532593\n",
      "[Training Epoch 1] Batch 2386, Loss 0.29694992303848267\n",
      "[Training Epoch 1] Batch 2387, Loss 0.2994239628314972\n",
      "[Training Epoch 1] Batch 2388, Loss 0.312695175409317\n",
      "[Training Epoch 1] Batch 2389, Loss 0.3151751160621643\n",
      "[Training Epoch 1] Batch 2390, Loss 0.3216554522514343\n",
      "[Training Epoch 1] Batch 2391, Loss 0.32200801372528076\n",
      "[Training Epoch 1] Batch 2392, Loss 0.32497936487197876\n",
      "[Training Epoch 1] Batch 2393, Loss 0.27047955989837646\n",
      "[Training Epoch 1] Batch 2394, Loss 0.29193490743637085\n",
      "[Training Epoch 1] Batch 2395, Loss 0.2864041328430176\n",
      "[Training Epoch 1] Batch 2396, Loss 0.27947965264320374\n",
      "[Training Epoch 1] Batch 2397, Loss 0.27863603830337524\n",
      "[Training Epoch 1] Batch 2398, Loss 0.3005046248435974\n",
      "[Training Epoch 1] Batch 2399, Loss 0.2840334475040436\n",
      "[Training Epoch 1] Batch 2400, Loss 0.3083927035331726\n",
      "[Training Epoch 1] Batch 2401, Loss 0.28495123982429504\n",
      "[Training Epoch 1] Batch 2402, Loss 0.30597996711730957\n",
      "[Training Epoch 1] Batch 2403, Loss 0.32253897190093994\n",
      "[Training Epoch 1] Batch 2404, Loss 0.2843170464038849\n",
      "[Training Epoch 1] Batch 2405, Loss 0.30951282382011414\n",
      "[Training Epoch 1] Batch 2406, Loss 0.2667772173881531\n",
      "[Training Epoch 1] Batch 2407, Loss 0.2946125268936157\n",
      "[Training Epoch 1] Batch 2408, Loss 0.29653456807136536\n",
      "[Training Epoch 1] Batch 2409, Loss 0.32359635829925537\n",
      "[Training Epoch 1] Batch 2410, Loss 0.3170511722564697\n",
      "[Training Epoch 1] Batch 2411, Loss 0.3066158890724182\n",
      "[Training Epoch 1] Batch 2412, Loss 0.3037392497062683\n",
      "[Training Epoch 1] Batch 2413, Loss 0.3091687560081482\n",
      "[Training Epoch 1] Batch 2414, Loss 0.2768613398075104\n",
      "[Training Epoch 1] Batch 2415, Loss 0.3161885738372803\n",
      "[Training Epoch 1] Batch 2416, Loss 0.28033214807510376\n",
      "[Training Epoch 1] Batch 2417, Loss 0.2918461859226227\n",
      "[Training Epoch 1] Batch 2418, Loss 0.2819693088531494\n",
      "[Training Epoch 1] Batch 2419, Loss 0.30557531118392944\n",
      "[Training Epoch 1] Batch 2420, Loss 0.3239055871963501\n",
      "[Training Epoch 1] Batch 2421, Loss 0.29861870408058167\n",
      "[Training Epoch 1] Batch 2422, Loss 0.28149157762527466\n",
      "[Training Epoch 1] Batch 2423, Loss 0.27905020117759705\n",
      "[Training Epoch 1] Batch 2424, Loss 0.2694368362426758\n",
      "[Training Epoch 1] Batch 2425, Loss 0.2979621887207031\n",
      "[Training Epoch 1] Batch 2426, Loss 0.28587964177131653\n",
      "[Training Epoch 1] Batch 2427, Loss 0.2958909273147583\n",
      "[Training Epoch 1] Batch 2428, Loss 0.3064166009426117\n",
      "[Training Epoch 1] Batch 2429, Loss 0.313820481300354\n",
      "[Training Epoch 1] Batch 2430, Loss 0.2882424592971802\n",
      "[Training Epoch 1] Batch 2431, Loss 0.30901601910591125\n",
      "[Training Epoch 1] Batch 2432, Loss 0.2847958207130432\n",
      "[Training Epoch 1] Batch 2433, Loss 0.2632024586200714\n",
      "[Training Epoch 1] Batch 2434, Loss 0.2933811843395233\n",
      "[Training Epoch 1] Batch 2435, Loss 0.31334078311920166\n",
      "[Training Epoch 1] Batch 2436, Loss 0.319597989320755\n",
      "[Training Epoch 1] Batch 2437, Loss 0.2968231439590454\n",
      "[Training Epoch 1] Batch 2438, Loss 0.27602747082710266\n",
      "[Training Epoch 1] Batch 2439, Loss 0.27445918321609497\n",
      "[Training Epoch 1] Batch 2440, Loss 0.3242717385292053\n",
      "[Training Epoch 1] Batch 2441, Loss 0.2841053605079651\n",
      "[Training Epoch 1] Batch 2442, Loss 0.2839062213897705\n",
      "[Training Epoch 1] Batch 2443, Loss 0.27622175216674805\n",
      "[Training Epoch 1] Batch 2444, Loss 0.28415024280548096\n",
      "[Training Epoch 1] Batch 2445, Loss 0.2757827341556549\n",
      "[Training Epoch 1] Batch 2446, Loss 0.30439114570617676\n",
      "[Training Epoch 1] Batch 2447, Loss 0.289431095123291\n",
      "[Training Epoch 1] Batch 2448, Loss 0.2963092625141144\n",
      "[Training Epoch 1] Batch 2449, Loss 0.3016049861907959\n",
      "[Training Epoch 1] Batch 2450, Loss 0.30101633071899414\n",
      "[Training Epoch 1] Batch 2451, Loss 0.2930832505226135\n",
      "[Training Epoch 1] Batch 2452, Loss 0.28964880108833313\n",
      "[Training Epoch 1] Batch 2453, Loss 0.29453861713409424\n",
      "[Training Epoch 1] Batch 2454, Loss 0.28538790345191956\n",
      "[Training Epoch 1] Batch 2455, Loss 0.2723095417022705\n",
      "[Training Epoch 1] Batch 2456, Loss 0.32627174258232117\n",
      "[Training Epoch 1] Batch 2457, Loss 0.286317378282547\n",
      "[Training Epoch 1] Batch 2458, Loss 0.2956410050392151\n",
      "[Training Epoch 1] Batch 2459, Loss 0.2854130268096924\n",
      "[Training Epoch 1] Batch 2460, Loss 0.2998971939086914\n",
      "[Training Epoch 1] Batch 2461, Loss 0.283193439245224\n",
      "[Training Epoch 1] Batch 2462, Loss 0.28934016823768616\n",
      "[Training Epoch 1] Batch 2463, Loss 0.2947159707546234\n",
      "[Training Epoch 1] Batch 2464, Loss 0.2806665897369385\n",
      "[Training Epoch 1] Batch 2465, Loss 0.31009241938591003\n",
      "[Training Epoch 1] Batch 2466, Loss 0.29514867067337036\n",
      "[Training Epoch 1] Batch 2467, Loss 0.3348865211009979\n",
      "[Training Epoch 1] Batch 2468, Loss 0.29078221321105957\n",
      "[Training Epoch 1] Batch 2469, Loss 0.28279149532318115\n",
      "[Training Epoch 1] Batch 2470, Loss 0.3108201324939728\n",
      "[Training Epoch 1] Batch 2471, Loss 0.2905659079551697\n",
      "[Training Epoch 1] Batch 2472, Loss 0.32302114367485046\n",
      "[Training Epoch 1] Batch 2473, Loss 0.26793819665908813\n",
      "[Training Epoch 1] Batch 2474, Loss 0.2971054017543793\n",
      "[Training Epoch 1] Batch 2475, Loss 0.28782278299331665\n",
      "[Training Epoch 1] Batch 2476, Loss 0.3075561225414276\n",
      "[Training Epoch 1] Batch 2477, Loss 0.31227171421051025\n",
      "[Training Epoch 1] Batch 2478, Loss 0.28221774101257324\n",
      "[Training Epoch 1] Batch 2479, Loss 0.3061712384223938\n",
      "[Training Epoch 1] Batch 2480, Loss 0.2688165307044983\n",
      "[Training Epoch 1] Batch 2481, Loss 0.289491206407547\n",
      "[Training Epoch 1] Batch 2482, Loss 0.2919876277446747\n",
      "[Training Epoch 1] Batch 2483, Loss 0.2942378520965576\n",
      "[Training Epoch 1] Batch 2484, Loss 0.29428061842918396\n",
      "[Training Epoch 1] Batch 2485, Loss 0.3089558184146881\n",
      "[Training Epoch 1] Batch 2486, Loss 0.3110276460647583\n",
      "[Training Epoch 1] Batch 2487, Loss 0.2910279929637909\n",
      "[Training Epoch 1] Batch 2488, Loss 0.297995388507843\n",
      "[Training Epoch 1] Batch 2489, Loss 0.3011247217655182\n",
      "[Training Epoch 1] Batch 2490, Loss 0.30668243765830994\n",
      "[Training Epoch 1] Batch 2491, Loss 0.3086802661418915\n",
      "[Training Epoch 1] Batch 2492, Loss 0.32575932145118713\n",
      "[Training Epoch 1] Batch 2493, Loss 0.3105151653289795\n",
      "[Training Epoch 1] Batch 2494, Loss 0.25701647996902466\n",
      "[Training Epoch 1] Batch 2495, Loss 0.2939528524875641\n",
      "[Training Epoch 1] Batch 2496, Loss 0.3064364194869995\n",
      "[Training Epoch 1] Batch 2497, Loss 0.30593281984329224\n",
      "[Training Epoch 1] Batch 2498, Loss 0.2804621458053589\n",
      "[Training Epoch 1] Batch 2499, Loss 0.30712825059890747\n",
      "[Training Epoch 1] Batch 2500, Loss 0.2893677353858948\n",
      "[Training Epoch 1] Batch 2501, Loss 0.29297131299972534\n",
      "[Training Epoch 1] Batch 2502, Loss 0.2838361859321594\n",
      "[Training Epoch 1] Batch 2503, Loss 0.27069970965385437\n",
      "[Training Epoch 1] Batch 2504, Loss 0.2709605395793915\n",
      "[Training Epoch 1] Batch 2505, Loss 0.2688882350921631\n",
      "[Training Epoch 1] Batch 2506, Loss 0.29534971714019775\n",
      "[Training Epoch 1] Batch 2507, Loss 0.3050256073474884\n",
      "[Training Epoch 1] Batch 2508, Loss 0.3159830570220947\n",
      "[Training Epoch 1] Batch 2509, Loss 0.2963276505470276\n",
      "[Training Epoch 1] Batch 2510, Loss 0.3202773928642273\n",
      "[Training Epoch 1] Batch 2511, Loss 0.2902449369430542\n",
      "[Training Epoch 1] Batch 2512, Loss 0.30820518732070923\n",
      "[Training Epoch 1] Batch 2513, Loss 0.28482064604759216\n",
      "[Training Epoch 1] Batch 2514, Loss 0.2713887095451355\n",
      "[Training Epoch 1] Batch 2515, Loss 0.29306137561798096\n",
      "[Training Epoch 1] Batch 2516, Loss 0.28931963443756104\n",
      "[Training Epoch 1] Batch 2517, Loss 0.27747252583503723\n",
      "[Training Epoch 1] Batch 2518, Loss 0.29961490631103516\n",
      "[Training Epoch 1] Batch 2519, Loss 0.272734135389328\n",
      "[Training Epoch 1] Batch 2520, Loss 0.2999507784843445\n",
      "[Training Epoch 1] Batch 2521, Loss 0.2524445950984955\n",
      "[Training Epoch 1] Batch 2522, Loss 0.2898055613040924\n",
      "[Training Epoch 1] Batch 2523, Loss 0.31643790006637573\n",
      "[Training Epoch 1] Batch 2524, Loss 0.32557642459869385\n",
      "[Training Epoch 1] Batch 2525, Loss 0.30954524874687195\n",
      "[Training Epoch 1] Batch 2526, Loss 0.27221816778182983\n",
      "[Training Epoch 1] Batch 2527, Loss 0.2993502616882324\n",
      "[Training Epoch 1] Batch 2528, Loss 0.3219761848449707\n",
      "[Training Epoch 1] Batch 2529, Loss 0.2975909113883972\n",
      "[Training Epoch 1] Batch 2530, Loss 0.29151323437690735\n",
      "[Training Epoch 1] Batch 2531, Loss 0.3193598985671997\n",
      "[Training Epoch 1] Batch 2532, Loss 0.30198365449905396\n",
      "[Training Epoch 1] Batch 2533, Loss 0.29013144969940186\n",
      "[Training Epoch 1] Batch 2534, Loss 0.3065289855003357\n",
      "[Training Epoch 1] Batch 2535, Loss 0.3080839216709137\n",
      "[Training Epoch 1] Batch 2536, Loss 0.27474313974380493\n",
      "[Training Epoch 1] Batch 2537, Loss 0.2977203130722046\n",
      "[Training Epoch 1] Batch 2538, Loss 0.29148542881011963\n",
      "[Training Epoch 1] Batch 2539, Loss 0.31692785024642944\n",
      "[Training Epoch 1] Batch 2540, Loss 0.3024156093597412\n",
      "[Training Epoch 1] Batch 2541, Loss 0.2849750518798828\n",
      "[Training Epoch 1] Batch 2542, Loss 0.3187648057937622\n",
      "[Training Epoch 1] Batch 2543, Loss 0.25517958402633667\n",
      "[Training Epoch 1] Batch 2544, Loss 0.2855932116508484\n",
      "[Training Epoch 1] Batch 2545, Loss 0.3043903708457947\n",
      "[Training Epoch 1] Batch 2546, Loss 0.28908607363700867\n",
      "[Training Epoch 1] Batch 2547, Loss 0.27559423446655273\n",
      "[Training Epoch 1] Batch 2548, Loss 0.27423155307769775\n",
      "[Training Epoch 1] Batch 2549, Loss 0.2980039119720459\n",
      "[Training Epoch 1] Batch 2550, Loss 0.31200075149536133\n",
      "[Training Epoch 1] Batch 2551, Loss 0.2891852855682373\n",
      "[Training Epoch 1] Batch 2552, Loss 0.3016030788421631\n",
      "[Training Epoch 1] Batch 2553, Loss 0.28848618268966675\n",
      "[Training Epoch 1] Batch 2554, Loss 0.3248673677444458\n",
      "[Training Epoch 1] Batch 2555, Loss 0.30559080839157104\n",
      "[Training Epoch 1] Batch 2556, Loss 0.2896905243396759\n",
      "[Training Epoch 1] Batch 2557, Loss 0.313210129737854\n",
      "[Training Epoch 1] Batch 2558, Loss 0.2964739501476288\n",
      "[Training Epoch 1] Batch 2559, Loss 0.2790061831474304\n",
      "[Training Epoch 1] Batch 2560, Loss 0.2806239426136017\n",
      "[Training Epoch 1] Batch 2561, Loss 0.3204908072948456\n",
      "[Training Epoch 1] Batch 2562, Loss 0.2752576470375061\n",
      "[Training Epoch 1] Batch 2563, Loss 0.28652703762054443\n",
      "[Training Epoch 1] Batch 2564, Loss 0.29934805631637573\n",
      "[Training Epoch 1] Batch 2565, Loss 0.3282454013824463\n",
      "[Training Epoch 1] Batch 2566, Loss 0.28024381399154663\n",
      "[Training Epoch 1] Batch 2567, Loss 0.31920796632766724\n",
      "[Training Epoch 1] Batch 2568, Loss 0.2937159538269043\n",
      "[Training Epoch 1] Batch 2569, Loss 0.3145895004272461\n",
      "[Training Epoch 1] Batch 2570, Loss 0.3036082684993744\n",
      "[Training Epoch 1] Batch 2571, Loss 0.3210725784301758\n",
      "[Training Epoch 1] Batch 2572, Loss 0.2571733295917511\n",
      "[Training Epoch 1] Batch 2573, Loss 0.29519492387771606\n",
      "[Training Epoch 1] Batch 2574, Loss 0.31814447045326233\n",
      "[Training Epoch 1] Batch 2575, Loss 0.30629032850265503\n",
      "[Training Epoch 1] Batch 2576, Loss 0.3074601888656616\n",
      "[Training Epoch 1] Batch 2577, Loss 0.3079474866390228\n",
      "[Training Epoch 1] Batch 2578, Loss 0.2859739661216736\n",
      "[Training Epoch 1] Batch 2579, Loss 0.29166272282600403\n",
      "[Training Epoch 1] Batch 2580, Loss 0.3146587014198303\n",
      "[Training Epoch 1] Batch 2581, Loss 0.3033711612224579\n",
      "[Training Epoch 1] Batch 2582, Loss 0.33234265446662903\n",
      "[Training Epoch 1] Batch 2583, Loss 0.2698323428630829\n",
      "[Training Epoch 1] Batch 2584, Loss 0.31166762113571167\n",
      "[Training Epoch 1] Batch 2585, Loss 0.3071155548095703\n",
      "[Training Epoch 1] Batch 2586, Loss 0.32225221395492554\n",
      "[Training Epoch 1] Batch 2587, Loss 0.29579949378967285\n",
      "[Training Epoch 1] Batch 2588, Loss 0.31343963742256165\n",
      "[Training Epoch 1] Batch 2589, Loss 0.2724210023880005\n",
      "[Training Epoch 1] Batch 2590, Loss 0.2853792905807495\n",
      "[Training Epoch 1] Batch 2591, Loss 0.30419427156448364\n",
      "[Training Epoch 1] Batch 2592, Loss 0.31617143750190735\n",
      "[Training Epoch 1] Batch 2593, Loss 0.2890889644622803\n",
      "[Training Epoch 1] Batch 2594, Loss 0.2959749698638916\n",
      "[Training Epoch 1] Batch 2595, Loss 0.28487661480903625\n",
      "[Training Epoch 1] Batch 2596, Loss 0.26675570011138916\n",
      "[Training Epoch 1] Batch 2597, Loss 0.3141319751739502\n",
      "[Training Epoch 1] Batch 2598, Loss 0.2935921549797058\n",
      "[Training Epoch 1] Batch 2599, Loss 0.2756339907646179\n",
      "[Training Epoch 1] Batch 2600, Loss 0.28864720463752747\n",
      "[Training Epoch 1] Batch 2601, Loss 0.29629236459732056\n",
      "[Training Epoch 1] Batch 2602, Loss 0.29695284366607666\n",
      "[Training Epoch 1] Batch 2603, Loss 0.2962167263031006\n",
      "[Training Epoch 1] Batch 2604, Loss 0.28584885597229004\n",
      "[Training Epoch 1] Batch 2605, Loss 0.27513155341148376\n",
      "[Training Epoch 1] Batch 2606, Loss 0.3080137073993683\n",
      "[Training Epoch 1] Batch 2607, Loss 0.2835899591445923\n",
      "[Training Epoch 1] Batch 2608, Loss 0.2687743902206421\n",
      "[Training Epoch 1] Batch 2609, Loss 0.27464166283607483\n",
      "[Training Epoch 1] Batch 2610, Loss 0.3020485043525696\n",
      "[Training Epoch 1] Batch 2611, Loss 0.3036995530128479\n",
      "[Training Epoch 1] Batch 2612, Loss 0.29428553581237793\n",
      "[Training Epoch 1] Batch 2613, Loss 0.30253612995147705\n",
      "[Training Epoch 1] Batch 2614, Loss 0.32399335503578186\n",
      "[Training Epoch 1] Batch 2615, Loss 0.28717392683029175\n",
      "[Training Epoch 1] Batch 2616, Loss 0.3041607737541199\n",
      "[Training Epoch 1] Batch 2617, Loss 0.27291175723075867\n",
      "[Training Epoch 1] Batch 2618, Loss 0.3078137934207916\n",
      "[Training Epoch 1] Batch 2619, Loss 0.2787353992462158\n",
      "[Training Epoch 1] Batch 2620, Loss 0.2879091203212738\n",
      "[Training Epoch 1] Batch 2621, Loss 0.3134744167327881\n",
      "[Training Epoch 1] Batch 2622, Loss 0.2857471704483032\n",
      "[Training Epoch 1] Batch 2623, Loss 0.2853030264377594\n",
      "[Training Epoch 1] Batch 2624, Loss 0.2751539945602417\n",
      "[Training Epoch 1] Batch 2625, Loss 0.30129408836364746\n",
      "[Training Epoch 1] Batch 2626, Loss 0.3050316572189331\n",
      "[Training Epoch 1] Batch 2627, Loss 0.30515116453170776\n",
      "[Training Epoch 1] Batch 2628, Loss 0.2892877161502838\n",
      "[Training Epoch 1] Batch 2629, Loss 0.32339370250701904\n",
      "[Training Epoch 1] Batch 2630, Loss 0.2981908321380615\n",
      "[Training Epoch 1] Batch 2631, Loss 0.2959855794906616\n",
      "[Training Epoch 1] Batch 2632, Loss 0.2847059965133667\n",
      "[Training Epoch 1] Batch 2633, Loss 0.28970038890838623\n",
      "[Training Epoch 1] Batch 2634, Loss 0.2743118107318878\n",
      "[Training Epoch 1] Batch 2635, Loss 0.3122814893722534\n",
      "[Training Epoch 1] Batch 2636, Loss 0.30417388677597046\n",
      "[Training Epoch 1] Batch 2637, Loss 0.30866920948028564\n",
      "[Training Epoch 1] Batch 2638, Loss 0.34448307752609253\n",
      "[Training Epoch 1] Batch 2639, Loss 0.27869856357574463\n",
      "[Training Epoch 1] Batch 2640, Loss 0.2900107204914093\n",
      "[Training Epoch 1] Batch 2641, Loss 0.29401594400405884\n",
      "[Training Epoch 1] Batch 2642, Loss 0.31178638339042664\n",
      "[Training Epoch 1] Batch 2643, Loss 0.2770841121673584\n",
      "[Training Epoch 1] Batch 2644, Loss 0.3173484206199646\n",
      "[Training Epoch 1] Batch 2645, Loss 0.3126234710216522\n",
      "[Training Epoch 1] Batch 2646, Loss 0.27373549342155457\n",
      "[Training Epoch 1] Batch 2647, Loss 0.30300015211105347\n",
      "[Training Epoch 1] Batch 2648, Loss 0.28224998712539673\n",
      "[Training Epoch 1] Batch 2649, Loss 0.29466280341148376\n",
      "[Training Epoch 1] Batch 2650, Loss 0.3005005717277527\n",
      "[Training Epoch 1] Batch 2651, Loss 0.28022506833076477\n",
      "[Training Epoch 1] Batch 2652, Loss 0.29794222116470337\n",
      "[Training Epoch 1] Batch 2653, Loss 0.30233559012413025\n",
      "[Training Epoch 1] Batch 2654, Loss 0.31015855073928833\n",
      "[Training Epoch 1] Batch 2655, Loss 0.29693350195884705\n",
      "[Training Epoch 1] Batch 2656, Loss 0.28880518674850464\n",
      "[Training Epoch 1] Batch 2657, Loss 0.2879621088504791\n",
      "[Training Epoch 1] Batch 2658, Loss 0.31449073553085327\n",
      "[Training Epoch 1] Batch 2659, Loss 0.30630868673324585\n",
      "[Training Epoch 1] Batch 2660, Loss 0.28905290365219116\n",
      "[Training Epoch 1] Batch 2661, Loss 0.28760403394699097\n",
      "[Training Epoch 1] Batch 2662, Loss 0.3136051297187805\n",
      "[Training Epoch 1] Batch 2663, Loss 0.30278587341308594\n",
      "[Training Epoch 1] Batch 2664, Loss 0.31688374280929565\n",
      "[Training Epoch 1] Batch 2665, Loss 0.2885183095932007\n",
      "[Training Epoch 1] Batch 2666, Loss 0.3004119098186493\n",
      "[Training Epoch 1] Batch 2667, Loss 0.2776610851287842\n",
      "[Training Epoch 1] Batch 2668, Loss 0.297279953956604\n",
      "[Training Epoch 1] Batch 2669, Loss 0.30216580629348755\n",
      "[Training Epoch 1] Batch 2670, Loss 0.29060113430023193\n",
      "[Training Epoch 1] Batch 2671, Loss 0.2915792465209961\n",
      "[Training Epoch 1] Batch 2672, Loss 0.29957008361816406\n",
      "[Training Epoch 1] Batch 2673, Loss 0.3097658157348633\n",
      "[Training Epoch 1] Batch 2674, Loss 0.27452346682548523\n",
      "[Training Epoch 1] Batch 2675, Loss 0.28518345952033997\n",
      "[Training Epoch 1] Batch 2676, Loss 0.2857774794101715\n",
      "[Training Epoch 1] Batch 2677, Loss 0.3055286705493927\n",
      "[Training Epoch 1] Batch 2678, Loss 0.302986741065979\n",
      "[Training Epoch 1] Batch 2679, Loss 0.30697929859161377\n",
      "[Training Epoch 1] Batch 2680, Loss 0.28857535123825073\n",
      "[Training Epoch 1] Batch 2681, Loss 0.2997196912765503\n",
      "[Training Epoch 1] Batch 2682, Loss 0.34647464752197266\n",
      "[Training Epoch 1] Batch 2683, Loss 0.2975115180015564\n",
      "[Training Epoch 1] Batch 2684, Loss 0.28829142451286316\n",
      "[Training Epoch 1] Batch 2685, Loss 0.3011409640312195\n",
      "[Training Epoch 1] Batch 2686, Loss 0.29015040397644043\n",
      "[Training Epoch 1] Batch 2687, Loss 0.31486672163009644\n",
      "[Training Epoch 1] Batch 2688, Loss 0.27129197120666504\n",
      "[Training Epoch 1] Batch 2689, Loss 0.27083179354667664\n",
      "[Training Epoch 1] Batch 2690, Loss 0.2828647792339325\n",
      "[Training Epoch 1] Batch 2691, Loss 0.2921672463417053\n",
      "[Training Epoch 1] Batch 2692, Loss 0.30410683155059814\n",
      "[Training Epoch 1] Batch 2693, Loss 0.30031877756118774\n",
      "[Training Epoch 1] Batch 2694, Loss 0.3027719259262085\n",
      "[Training Epoch 1] Batch 2695, Loss 0.2715786099433899\n",
      "[Training Epoch 1] Batch 2696, Loss 0.3187596797943115\n",
      "[Training Epoch 1] Batch 2697, Loss 0.3193521499633789\n",
      "[Training Epoch 1] Batch 2698, Loss 0.29235416650772095\n",
      "[Training Epoch 1] Batch 2699, Loss 0.2765895426273346\n",
      "[Training Epoch 1] Batch 2700, Loss 0.27618739008903503\n",
      "[Training Epoch 1] Batch 2701, Loss 0.29140061140060425\n",
      "[Training Epoch 1] Batch 2702, Loss 0.295234739780426\n",
      "[Training Epoch 1] Batch 2703, Loss 0.25901421904563904\n",
      "[Training Epoch 1] Batch 2704, Loss 0.28351742029190063\n",
      "[Training Epoch 1] Batch 2705, Loss 0.2878696918487549\n",
      "[Training Epoch 1] Batch 2706, Loss 0.28448963165283203\n",
      "[Training Epoch 1] Batch 2707, Loss 0.31555110216140747\n",
      "[Training Epoch 1] Batch 2708, Loss 0.29063117504119873\n",
      "[Training Epoch 1] Batch 2709, Loss 0.28225165605545044\n",
      "[Training Epoch 1] Batch 2710, Loss 0.2977887988090515\n",
      "[Training Epoch 1] Batch 2711, Loss 0.3021511435508728\n",
      "[Training Epoch 1] Batch 2712, Loss 0.2782655954360962\n",
      "[Training Epoch 1] Batch 2713, Loss 0.2996140122413635\n",
      "[Training Epoch 1] Batch 2714, Loss 0.29040318727493286\n",
      "[Training Epoch 1] Batch 2715, Loss 0.2769761383533478\n",
      "[Training Epoch 1] Batch 2716, Loss 0.30172574520111084\n",
      "[Training Epoch 1] Batch 2717, Loss 0.30733567476272583\n",
      "[Training Epoch 1] Batch 2718, Loss 0.31454336643218994\n",
      "[Training Epoch 1] Batch 2719, Loss 0.299125075340271\n",
      "[Training Epoch 1] Batch 2720, Loss 0.24877957999706268\n",
      "[Training Epoch 1] Batch 2721, Loss 0.3054999113082886\n",
      "[Training Epoch 1] Batch 2722, Loss 0.2968222498893738\n",
      "[Training Epoch 1] Batch 2723, Loss 0.28536471724510193\n",
      "[Training Epoch 1] Batch 2724, Loss 0.3096614181995392\n",
      "[Training Epoch 1] Batch 2725, Loss 0.30300173163414\n",
      "[Training Epoch 1] Batch 2726, Loss 0.32610684633255005\n",
      "[Training Epoch 1] Batch 2727, Loss 0.3283625543117523\n",
      "[Training Epoch 1] Batch 2728, Loss 0.2987978458404541\n",
      "[Training Epoch 1] Batch 2729, Loss 0.3063751459121704\n",
      "[Training Epoch 1] Batch 2730, Loss 0.2675279974937439\n",
      "[Training Epoch 1] Batch 2731, Loss 0.29252201318740845\n",
      "[Training Epoch 1] Batch 2732, Loss 0.2931426465511322\n",
      "[Training Epoch 1] Batch 2733, Loss 0.2854757308959961\n",
      "[Training Epoch 1] Batch 2734, Loss 0.3191176950931549\n",
      "[Training Epoch 1] Batch 2735, Loss 0.30976402759552\n",
      "[Training Epoch 1] Batch 2736, Loss 0.3158320188522339\n",
      "[Training Epoch 1] Batch 2737, Loss 0.2738351821899414\n",
      "[Training Epoch 1] Batch 2738, Loss 0.2980167865753174\n",
      "[Training Epoch 1] Batch 2739, Loss 0.3073139786720276\n",
      "[Training Epoch 1] Batch 2740, Loss 0.3083374500274658\n",
      "[Training Epoch 1] Batch 2741, Loss 0.2828490436077118\n",
      "[Training Epoch 1] Batch 2742, Loss 0.27542930841445923\n",
      "[Training Epoch 1] Batch 2743, Loss 0.27500036358833313\n",
      "[Training Epoch 1] Batch 2744, Loss 0.2804083824157715\n",
      "[Training Epoch 1] Batch 2745, Loss 0.29765790700912476\n",
      "[Training Epoch 1] Batch 2746, Loss 0.3032371997833252\n",
      "[Training Epoch 1] Batch 2747, Loss 0.30560097098350525\n",
      "[Training Epoch 1] Batch 2748, Loss 0.29849377274513245\n",
      "[Training Epoch 1] Batch 2749, Loss 0.3288474678993225\n",
      "[Training Epoch 1] Batch 2750, Loss 0.26678889989852905\n",
      "[Training Epoch 1] Batch 2751, Loss 0.3255002498626709\n",
      "[Training Epoch 1] Batch 2752, Loss 0.2852829396724701\n",
      "[Training Epoch 1] Batch 2753, Loss 0.29267221689224243\n",
      "[Training Epoch 1] Batch 2754, Loss 0.28785479068756104\n",
      "[Training Epoch 1] Batch 2755, Loss 0.28586074709892273\n",
      "[Training Epoch 1] Batch 2756, Loss 0.2965816259384155\n",
      "[Training Epoch 1] Batch 2757, Loss 0.3012177348136902\n",
      "[Training Epoch 1] Batch 2758, Loss 0.29674530029296875\n",
      "[Training Epoch 1] Batch 2759, Loss 0.31502699851989746\n",
      "[Training Epoch 1] Batch 2760, Loss 0.2782038748264313\n",
      "[Training Epoch 1] Batch 2761, Loss 0.29031747579574585\n",
      "[Training Epoch 1] Batch 2762, Loss 0.28338342905044556\n",
      "[Training Epoch 1] Batch 2763, Loss 0.2814186215400696\n",
      "[Training Epoch 1] Batch 2764, Loss 0.29501038789749146\n",
      "[Training Epoch 1] Batch 2765, Loss 0.2931397557258606\n",
      "[Training Epoch 1] Batch 2766, Loss 0.2949199378490448\n",
      "[Training Epoch 1] Batch 2767, Loss 0.30136391520500183\n",
      "[Training Epoch 1] Batch 2768, Loss 0.29421266913414\n",
      "[Training Epoch 1] Batch 2769, Loss 0.2899119257926941\n",
      "[Training Epoch 1] Batch 2770, Loss 0.29963958263397217\n",
      "[Training Epoch 1] Batch 2771, Loss 0.31708747148513794\n",
      "[Training Epoch 1] Batch 2772, Loss 0.30296623706817627\n",
      "[Training Epoch 1] Batch 2773, Loss 0.3044092655181885\n",
      "[Training Epoch 1] Batch 2774, Loss 0.2908012270927429\n",
      "[Training Epoch 1] Batch 2775, Loss 0.2996251583099365\n",
      "[Training Epoch 1] Batch 2776, Loss 0.28799158334732056\n",
      "[Training Epoch 1] Batch 2777, Loss 0.2703889012336731\n",
      "[Training Epoch 1] Batch 2778, Loss 0.27786773443222046\n",
      "[Training Epoch 1] Batch 2779, Loss 0.2794567942619324\n",
      "[Training Epoch 1] Batch 2780, Loss 0.3008425831794739\n",
      "[Training Epoch 1] Batch 2781, Loss 0.288744181394577\n",
      "[Training Epoch 1] Batch 2782, Loss 0.2595374584197998\n",
      "[Training Epoch 1] Batch 2783, Loss 0.30382972955703735\n",
      "[Training Epoch 1] Batch 2784, Loss 0.3138926327228546\n",
      "[Training Epoch 1] Batch 2785, Loss 0.297519326210022\n",
      "[Training Epoch 1] Batch 2786, Loss 0.28904321789741516\n",
      "[Training Epoch 1] Batch 2787, Loss 0.29431813955307007\n",
      "[Training Epoch 1] Batch 2788, Loss 0.30270543694496155\n",
      "[Training Epoch 1] Batch 2789, Loss 0.2904728055000305\n",
      "[Training Epoch 1] Batch 2790, Loss 0.3151402771472931\n",
      "[Training Epoch 1] Batch 2791, Loss 0.2683095932006836\n",
      "[Training Epoch 1] Batch 2792, Loss 0.3142314553260803\n",
      "[Training Epoch 1] Batch 2793, Loss 0.3071333169937134\n",
      "[Training Epoch 1] Batch 2794, Loss 0.3248179256916046\n",
      "[Training Epoch 1] Batch 2795, Loss 0.30292773246765137\n",
      "[Training Epoch 1] Batch 2796, Loss 0.30222296714782715\n",
      "[Training Epoch 1] Batch 2797, Loss 0.2791733741760254\n",
      "[Training Epoch 1] Batch 2798, Loss 0.28897082805633545\n",
      "[Training Epoch 1] Batch 2799, Loss 0.2831594944000244\n",
      "[Training Epoch 1] Batch 2800, Loss 0.28447675704956055\n",
      "[Training Epoch 1] Batch 2801, Loss 0.2944018840789795\n",
      "[Training Epoch 1] Batch 2802, Loss 0.3284100592136383\n",
      "[Training Epoch 1] Batch 2803, Loss 0.2983834147453308\n",
      "[Training Epoch 1] Batch 2804, Loss 0.2953084111213684\n",
      "[Training Epoch 1] Batch 2805, Loss 0.3262062072753906\n",
      "[Training Epoch 1] Batch 2806, Loss 0.31046491861343384\n",
      "[Training Epoch 1] Batch 2807, Loss 0.286581814289093\n",
      "[Training Epoch 1] Batch 2808, Loss 0.31968021392822266\n",
      "[Training Epoch 1] Batch 2809, Loss 0.3073047995567322\n",
      "[Training Epoch 1] Batch 2810, Loss 0.2741020917892456\n",
      "[Training Epoch 1] Batch 2811, Loss 0.29021185636520386\n",
      "[Training Epoch 1] Batch 2812, Loss 0.297593891620636\n",
      "[Training Epoch 1] Batch 2813, Loss 0.2729758620262146\n",
      "[Training Epoch 1] Batch 2814, Loss 0.2841321527957916\n",
      "[Training Epoch 1] Batch 2815, Loss 0.2810863256454468\n",
      "[Training Epoch 1] Batch 2816, Loss 0.29875150322914124\n",
      "[Training Epoch 1] Batch 2817, Loss 0.2833193242549896\n",
      "[Training Epoch 1] Batch 2818, Loss 0.29974114894866943\n",
      "[Training Epoch 1] Batch 2819, Loss 0.2733107805252075\n",
      "[Training Epoch 1] Batch 2820, Loss 0.3025193214416504\n",
      "[Training Epoch 1] Batch 2821, Loss 0.31472450494766235\n",
      "[Training Epoch 1] Batch 2822, Loss 0.2920534908771515\n",
      "[Training Epoch 1] Batch 2823, Loss 0.3006598949432373\n",
      "[Training Epoch 1] Batch 2824, Loss 0.302764356136322\n",
      "[Training Epoch 1] Batch 2825, Loss 0.2884472608566284\n",
      "[Training Epoch 1] Batch 2826, Loss 0.2994641661643982\n",
      "[Training Epoch 1] Batch 2827, Loss 0.30194318294525146\n",
      "[Training Epoch 1] Batch 2828, Loss 0.27776437997817993\n",
      "[Training Epoch 1] Batch 2829, Loss 0.29432839155197144\n",
      "[Training Epoch 1] Batch 2830, Loss 0.2840502858161926\n",
      "[Training Epoch 1] Batch 2831, Loss 0.2998558282852173\n",
      "[Training Epoch 1] Batch 2832, Loss 0.31126949191093445\n",
      "[Training Epoch 1] Batch 2833, Loss 0.3047206997871399\n",
      "[Training Epoch 1] Batch 2834, Loss 0.2900828421115875\n",
      "[Training Epoch 1] Batch 2835, Loss 0.29361653327941895\n",
      "[Training Epoch 1] Batch 2836, Loss 0.2862677574157715\n",
      "[Training Epoch 1] Batch 2837, Loss 0.2853640913963318\n",
      "[Training Epoch 1] Batch 2838, Loss 0.3031390905380249\n",
      "[Training Epoch 1] Batch 2839, Loss 0.3053382635116577\n",
      "[Training Epoch 1] Batch 2840, Loss 0.2959560751914978\n",
      "[Training Epoch 1] Batch 2841, Loss 0.28001272678375244\n",
      "[Training Epoch 1] Batch 2842, Loss 0.2872898578643799\n",
      "[Training Epoch 1] Batch 2843, Loss 0.30526483058929443\n",
      "[Training Epoch 1] Batch 2844, Loss 0.30171746015548706\n",
      "[Training Epoch 1] Batch 2845, Loss 0.2922396659851074\n",
      "[Training Epoch 1] Batch 2846, Loss 0.29301685094833374\n",
      "[Training Epoch 1] Batch 2847, Loss 0.2697429060935974\n",
      "[Training Epoch 1] Batch 2848, Loss 0.2895204424858093\n",
      "[Training Epoch 1] Batch 2849, Loss 0.2808322012424469\n",
      "[Training Epoch 1] Batch 2850, Loss 0.2998582124710083\n",
      "[Training Epoch 1] Batch 2851, Loss 0.2844487130641937\n",
      "[Training Epoch 1] Batch 2852, Loss 0.2942699193954468\n",
      "[Training Epoch 1] Batch 2853, Loss 0.28409087657928467\n",
      "[Training Epoch 1] Batch 2854, Loss 0.3134159445762634\n",
      "[Training Epoch 1] Batch 2855, Loss 0.3143727779388428\n",
      "[Training Epoch 1] Batch 2856, Loss 0.3031672239303589\n",
      "[Training Epoch 1] Batch 2857, Loss 0.31337758898735046\n",
      "[Training Epoch 1] Batch 2858, Loss 0.2755533754825592\n",
      "[Training Epoch 1] Batch 2859, Loss 0.2817874550819397\n",
      "[Training Epoch 1] Batch 2860, Loss 0.26068466901779175\n",
      "[Training Epoch 1] Batch 2861, Loss 0.2844838500022888\n",
      "[Training Epoch 1] Batch 2862, Loss 0.316362589597702\n",
      "[Training Epoch 1] Batch 2863, Loss 0.2770310640335083\n",
      "[Training Epoch 1] Batch 2864, Loss 0.2997647523880005\n",
      "[Training Epoch 1] Batch 2865, Loss 0.3124886155128479\n",
      "[Training Epoch 1] Batch 2866, Loss 0.316219300031662\n",
      "[Training Epoch 1] Batch 2867, Loss 0.2772518992424011\n",
      "[Training Epoch 1] Batch 2868, Loss 0.32152196764945984\n",
      "[Training Epoch 1] Batch 2869, Loss 0.3059449791908264\n",
      "[Training Epoch 1] Batch 2870, Loss 0.28052234649658203\n",
      "[Training Epoch 1] Batch 2871, Loss 0.29438382387161255\n",
      "[Training Epoch 1] Batch 2872, Loss 0.263141006231308\n",
      "[Training Epoch 1] Batch 2873, Loss 0.29934757947921753\n",
      "[Training Epoch 1] Batch 2874, Loss 0.29395541548728943\n",
      "[Training Epoch 1] Batch 2875, Loss 0.30743950605392456\n",
      "[Training Epoch 1] Batch 2876, Loss 0.3153184652328491\n",
      "[Training Epoch 1] Batch 2877, Loss 0.28700900077819824\n",
      "[Training Epoch 1] Batch 2878, Loss 0.29739266633987427\n",
      "[Training Epoch 1] Batch 2879, Loss 0.2996925711631775\n",
      "[Training Epoch 1] Batch 2880, Loss 0.3068400025367737\n",
      "[Training Epoch 1] Batch 2881, Loss 0.3117418885231018\n",
      "[Training Epoch 1] Batch 2882, Loss 0.2909550070762634\n",
      "[Training Epoch 1] Batch 2883, Loss 0.2847498655319214\n",
      "[Training Epoch 1] Batch 2884, Loss 0.27107587456703186\n",
      "[Training Epoch 1] Batch 2885, Loss 0.29765015840530396\n",
      "[Training Epoch 1] Batch 2886, Loss 0.3090556561946869\n",
      "[Training Epoch 1] Batch 2887, Loss 0.3189496695995331\n",
      "[Training Epoch 1] Batch 2888, Loss 0.2978220283985138\n",
      "[Training Epoch 1] Batch 2889, Loss 0.3204493522644043\n",
      "[Training Epoch 1] Batch 2890, Loss 0.2763342261314392\n",
      "[Training Epoch 1] Batch 2891, Loss 0.2910086512565613\n",
      "[Training Epoch 1] Batch 2892, Loss 0.2559620440006256\n",
      "[Training Epoch 1] Batch 2893, Loss 0.27460694313049316\n",
      "[Training Epoch 1] Batch 2894, Loss 0.320733904838562\n",
      "[Training Epoch 1] Batch 2895, Loss 0.28426289558410645\n",
      "[Training Epoch 1] Batch 2896, Loss 0.2985892593860626\n",
      "[Training Epoch 1] Batch 2897, Loss 0.3006000518798828\n",
      "[Training Epoch 1] Batch 2898, Loss 0.2903904914855957\n",
      "[Training Epoch 1] Batch 2899, Loss 0.288385272026062\n",
      "[Training Epoch 1] Batch 2900, Loss 0.3058815002441406\n",
      "[Training Epoch 1] Batch 2901, Loss 0.2954217791557312\n",
      "[Training Epoch 1] Batch 2902, Loss 0.28723061084747314\n",
      "[Training Epoch 1] Batch 2903, Loss 0.32816964387893677\n",
      "[Training Epoch 1] Batch 2904, Loss 0.30138683319091797\n",
      "[Training Epoch 1] Batch 2905, Loss 0.2724401652812958\n",
      "[Training Epoch 1] Batch 2906, Loss 0.29581570625305176\n",
      "[Training Epoch 1] Batch 2907, Loss 0.2864817976951599\n",
      "[Training Epoch 1] Batch 2908, Loss 0.27214938402175903\n",
      "[Training Epoch 1] Batch 2909, Loss 0.28437304496765137\n",
      "[Training Epoch 1] Batch 2910, Loss 0.28671133518218994\n",
      "[Training Epoch 1] Batch 2911, Loss 0.2781578004360199\n",
      "[Training Epoch 1] Batch 2912, Loss 0.2695338726043701\n",
      "[Training Epoch 1] Batch 2913, Loss 0.2900153696537018\n",
      "[Training Epoch 1] Batch 2914, Loss 0.32666367292404175\n",
      "[Training Epoch 1] Batch 2915, Loss 0.2946507930755615\n",
      "[Training Epoch 1] Batch 2916, Loss 0.27954918146133423\n",
      "[Training Epoch 1] Batch 2917, Loss 0.29525846242904663\n",
      "[Training Epoch 1] Batch 2918, Loss 0.31000450253486633\n",
      "[Training Epoch 1] Batch 2919, Loss 0.2992241680622101\n",
      "[Training Epoch 1] Batch 2920, Loss 0.3195589482784271\n",
      "[Training Epoch 1] Batch 2921, Loss 0.2878509759902954\n",
      "[Training Epoch 1] Batch 2922, Loss 0.31199413537979126\n",
      "[Training Epoch 1] Batch 2923, Loss 0.2676873207092285\n",
      "[Training Epoch 1] Batch 2924, Loss 0.2992814779281616\n",
      "[Training Epoch 1] Batch 2925, Loss 0.30331623554229736\n",
      "[Training Epoch 1] Batch 2926, Loss 0.29831987619400024\n",
      "[Training Epoch 1] Batch 2927, Loss 0.31725284457206726\n",
      "[Training Epoch 1] Batch 2928, Loss 0.303644597530365\n",
      "[Training Epoch 1] Batch 2929, Loss 0.27945491671562195\n",
      "[Training Epoch 1] Batch 2930, Loss 0.2716408669948578\n",
      "[Training Epoch 1] Batch 2931, Loss 0.31657737493515015\n",
      "[Training Epoch 1] Batch 2932, Loss 0.30333003401756287\n",
      "[Training Epoch 1] Batch 2933, Loss 0.29673629999160767\n",
      "[Training Epoch 1] Batch 2934, Loss 0.28750503063201904\n",
      "[Training Epoch 1] Batch 2935, Loss 0.25832104682922363\n",
      "[Training Epoch 1] Batch 2936, Loss 0.2589881718158722\n",
      "[Training Epoch 1] Batch 2937, Loss 0.27653881907463074\n",
      "[Training Epoch 1] Batch 2938, Loss 0.2678122818470001\n",
      "[Training Epoch 1] Batch 2939, Loss 0.28653693199157715\n",
      "[Training Epoch 1] Batch 2940, Loss 0.3005565106868744\n",
      "[Training Epoch 1] Batch 2941, Loss 0.26628386974334717\n",
      "[Training Epoch 1] Batch 2942, Loss 0.2998206615447998\n",
      "[Training Epoch 1] Batch 2943, Loss 0.2724137306213379\n",
      "[Training Epoch 1] Batch 2944, Loss 0.31277722120285034\n",
      "[Training Epoch 1] Batch 2945, Loss 0.29461944103240967\n",
      "[Training Epoch 1] Batch 2946, Loss 0.3124273121356964\n",
      "[Training Epoch 1] Batch 2947, Loss 0.2874975800514221\n",
      "[Training Epoch 1] Batch 2948, Loss 0.26203885674476624\n",
      "[Training Epoch 1] Batch 2949, Loss 0.30735310912132263\n",
      "[Training Epoch 1] Batch 2950, Loss 0.2980523109436035\n",
      "[Training Epoch 1] Batch 2951, Loss 0.30107179284095764\n",
      "[Training Epoch 1] Batch 2952, Loss 0.28701919317245483\n",
      "[Training Epoch 1] Batch 2953, Loss 0.2996579706668854\n",
      "[Training Epoch 1] Batch 2954, Loss 0.2951428294181824\n",
      "[Training Epoch 1] Batch 2955, Loss 0.3010437488555908\n",
      "[Training Epoch 1] Batch 2956, Loss 0.30623504519462585\n",
      "[Training Epoch 1] Batch 2957, Loss 0.29889756441116333\n",
      "[Training Epoch 1] Batch 2958, Loss 0.29291296005249023\n",
      "[Training Epoch 1] Batch 2959, Loss 0.26795902848243713\n",
      "[Training Epoch 1] Batch 2960, Loss 0.3147597908973694\n",
      "[Training Epoch 1] Batch 2961, Loss 0.3204372525215149\n",
      "[Training Epoch 1] Batch 2962, Loss 0.33623698353767395\n",
      "[Training Epoch 1] Batch 2963, Loss 0.2962607443332672\n",
      "[Training Epoch 1] Batch 2964, Loss 0.2947204113006592\n",
      "[Training Epoch 1] Batch 2965, Loss 0.2993761897087097\n",
      "[Training Epoch 1] Batch 2966, Loss 0.3089873194694519\n",
      "[Training Epoch 1] Batch 2967, Loss 0.2897911071777344\n",
      "[Training Epoch 1] Batch 2968, Loss 0.2741144001483917\n",
      "[Training Epoch 1] Batch 2969, Loss 0.3180837035179138\n",
      "[Training Epoch 1] Batch 2970, Loss 0.2780204713344574\n",
      "[Training Epoch 1] Batch 2971, Loss 0.30132371187210083\n",
      "[Training Epoch 1] Batch 2972, Loss 0.31427815556526184\n",
      "[Training Epoch 1] Batch 2973, Loss 0.2912273108959198\n",
      "[Training Epoch 1] Batch 2974, Loss 0.26670852303504944\n",
      "[Training Epoch 1] Batch 2975, Loss 0.2896164059638977\n",
      "[Training Epoch 1] Batch 2976, Loss 0.2613871991634369\n",
      "[Training Epoch 1] Batch 2977, Loss 0.31955188512802124\n",
      "[Training Epoch 1] Batch 2978, Loss 0.27242985367774963\n",
      "[Training Epoch 1] Batch 2979, Loss 0.2703397274017334\n",
      "[Training Epoch 1] Batch 2980, Loss 0.3013962507247925\n",
      "[Training Epoch 1] Batch 2981, Loss 0.27625584602355957\n",
      "[Training Epoch 1] Batch 2982, Loss 0.2895275950431824\n",
      "[Training Epoch 1] Batch 2983, Loss 0.3180125951766968\n",
      "[Training Epoch 1] Batch 2984, Loss 0.2899063527584076\n",
      "[Training Epoch 1] Batch 2985, Loss 0.29100731015205383\n",
      "[Training Epoch 1] Batch 2986, Loss 0.29524773359298706\n",
      "[Training Epoch 1] Batch 2987, Loss 0.29167237877845764\n",
      "[Training Epoch 1] Batch 2988, Loss 0.3060944080352783\n",
      "[Training Epoch 1] Batch 2989, Loss 0.29514777660369873\n",
      "[Training Epoch 1] Batch 2990, Loss 0.2903951406478882\n",
      "[Training Epoch 1] Batch 2991, Loss 0.31512102484703064\n",
      "[Training Epoch 1] Batch 2992, Loss 0.30273962020874023\n",
      "[Training Epoch 1] Batch 2993, Loss 0.31364351511001587\n",
      "[Training Epoch 1] Batch 2994, Loss 0.2894607186317444\n",
      "[Training Epoch 1] Batch 2995, Loss 0.321685791015625\n",
      "[Training Epoch 1] Batch 2996, Loss 0.27733495831489563\n",
      "[Training Epoch 1] Batch 2997, Loss 0.2826787829399109\n",
      "[Training Epoch 1] Batch 2998, Loss 0.3061981797218323\n",
      "[Training Epoch 1] Batch 2999, Loss 0.30382978916168213\n",
      "[Training Epoch 1] Batch 3000, Loss 0.2793305516242981\n",
      "[Training Epoch 1] Batch 3001, Loss 0.3132323622703552\n",
      "[Training Epoch 1] Batch 3002, Loss 0.2915954887866974\n",
      "[Training Epoch 1] Batch 3003, Loss 0.3005201816558838\n",
      "[Training Epoch 1] Batch 3004, Loss 0.31362780928611755\n",
      "[Training Epoch 1] Batch 3005, Loss 0.30027925968170166\n",
      "[Training Epoch 1] Batch 3006, Loss 0.3250058591365814\n",
      "[Training Epoch 1] Batch 3007, Loss 0.30057674646377563\n",
      "[Training Epoch 1] Batch 3008, Loss 0.2821398973464966\n",
      "[Training Epoch 1] Batch 3009, Loss 0.28281038999557495\n",
      "[Training Epoch 1] Batch 3010, Loss 0.2995185852050781\n",
      "[Training Epoch 1] Batch 3011, Loss 0.2940315306186676\n",
      "[Training Epoch 1] Batch 3012, Loss 0.2767302393913269\n",
      "[Training Epoch 1] Batch 3013, Loss 0.346824586391449\n",
      "[Training Epoch 1] Batch 3014, Loss 0.30381056666374207\n",
      "[Training Epoch 1] Batch 3015, Loss 0.33364415168762207\n",
      "[Training Epoch 1] Batch 3016, Loss 0.30059006810188293\n",
      "[Training Epoch 1] Batch 3017, Loss 0.2713544964790344\n",
      "[Training Epoch 1] Batch 3018, Loss 0.2957382798194885\n",
      "[Training Epoch 1] Batch 3019, Loss 0.3084583580493927\n",
      "[Training Epoch 1] Batch 3020, Loss 0.28764232993125916\n",
      "[Training Epoch 1] Batch 3021, Loss 0.29545316100120544\n",
      "[Training Epoch 1] Batch 3022, Loss 0.2804008722305298\n",
      "[Training Epoch 1] Batch 3023, Loss 0.27564501762390137\n",
      "[Training Epoch 1] Batch 3024, Loss 0.2904672622680664\n",
      "[Training Epoch 1] Batch 3025, Loss 0.298994243144989\n",
      "[Training Epoch 1] Batch 3026, Loss 0.29210907220840454\n",
      "[Training Epoch 1] Batch 3027, Loss 0.2772136926651001\n",
      "[Training Epoch 1] Batch 3028, Loss 0.3023487627506256\n",
      "[Training Epoch 1] Batch 3029, Loss 0.29464882612228394\n",
      "[Training Epoch 1] Batch 3030, Loss 0.304391473531723\n",
      "[Training Epoch 1] Batch 3031, Loss 0.29198089241981506\n",
      "[Training Epoch 1] Batch 3032, Loss 0.3026959002017975\n",
      "[Training Epoch 1] Batch 3033, Loss 0.28483152389526367\n",
      "[Training Epoch 1] Batch 3034, Loss 0.3003414273262024\n",
      "[Training Epoch 1] Batch 3035, Loss 0.2927374243736267\n",
      "[Training Epoch 1] Batch 3036, Loss 0.29186898469924927\n",
      "[Training Epoch 1] Batch 3037, Loss 0.2611809968948364\n",
      "[Training Epoch 1] Batch 3038, Loss 0.28708529472351074\n",
      "[Training Epoch 1] Batch 3039, Loss 0.29398655891418457\n",
      "[Training Epoch 1] Batch 3040, Loss 0.2704533338546753\n",
      "[Training Epoch 1] Batch 3041, Loss 0.31687748432159424\n",
      "[Training Epoch 1] Batch 3042, Loss 0.2812862992286682\n",
      "[Training Epoch 1] Batch 3043, Loss 0.28319811820983887\n",
      "[Training Epoch 1] Batch 3044, Loss 0.2915043532848358\n",
      "[Training Epoch 1] Batch 3045, Loss 0.2842748761177063\n",
      "[Training Epoch 1] Batch 3046, Loss 0.28112226724624634\n",
      "[Training Epoch 1] Batch 3047, Loss 0.28427162766456604\n",
      "[Training Epoch 1] Batch 3048, Loss 0.2803395390510559\n",
      "[Training Epoch 1] Batch 3049, Loss 0.2786334753036499\n",
      "[Training Epoch 1] Batch 3050, Loss 0.2809487283229828\n",
      "[Training Epoch 1] Batch 3051, Loss 0.3056330382823944\n",
      "[Training Epoch 1] Batch 3052, Loss 0.2577775716781616\n",
      "[Training Epoch 1] Batch 3053, Loss 0.28912973403930664\n",
      "[Training Epoch 1] Batch 3054, Loss 0.3046420216560364\n",
      "[Training Epoch 1] Batch 3055, Loss 0.3036087155342102\n",
      "[Training Epoch 1] Batch 3056, Loss 0.2937108874320984\n",
      "[Training Epoch 1] Batch 3057, Loss 0.269012451171875\n",
      "[Training Epoch 1] Batch 3058, Loss 0.2736232876777649\n",
      "[Training Epoch 1] Batch 3059, Loss 0.3244372606277466\n",
      "[Training Epoch 1] Batch 3060, Loss 0.2847371995449066\n",
      "[Training Epoch 1] Batch 3061, Loss 0.30406704545021057\n",
      "[Training Epoch 1] Batch 3062, Loss 0.2725650668144226\n",
      "[Training Epoch 1] Batch 3063, Loss 0.305355966091156\n",
      "[Training Epoch 1] Batch 3064, Loss 0.26249247789382935\n",
      "[Training Epoch 1] Batch 3065, Loss 0.2934236526489258\n",
      "[Training Epoch 1] Batch 3066, Loss 0.2870882451534271\n",
      "[Training Epoch 1] Batch 3067, Loss 0.2987914979457855\n",
      "[Training Epoch 1] Batch 3068, Loss 0.28975895047187805\n",
      "[Training Epoch 1] Batch 3069, Loss 0.2860686182975769\n",
      "[Training Epoch 1] Batch 3070, Loss 0.2840086817741394\n",
      "[Training Epoch 1] Batch 3071, Loss 0.28247979283332825\n",
      "[Training Epoch 1] Batch 3072, Loss 0.28358983993530273\n",
      "[Training Epoch 1] Batch 3073, Loss 0.3070233166217804\n",
      "[Training Epoch 1] Batch 3074, Loss 0.3141903877258301\n",
      "[Training Epoch 1] Batch 3075, Loss 0.26542431116104126\n",
      "[Training Epoch 1] Batch 3076, Loss 0.3087013363838196\n",
      "[Training Epoch 1] Batch 3077, Loss 0.285369873046875\n",
      "[Training Epoch 1] Batch 3078, Loss 0.2895667254924774\n",
      "[Training Epoch 1] Batch 3079, Loss 0.32737821340560913\n",
      "[Training Epoch 1] Batch 3080, Loss 0.2818894684314728\n",
      "[Training Epoch 1] Batch 3081, Loss 0.2903327941894531\n",
      "[Training Epoch 1] Batch 3082, Loss 0.3143095374107361\n",
      "[Training Epoch 1] Batch 3083, Loss 0.2912842929363251\n",
      "[Training Epoch 1] Batch 3084, Loss 0.29291343688964844\n",
      "[Training Epoch 1] Batch 3085, Loss 0.27763161063194275\n",
      "[Training Epoch 1] Batch 3086, Loss 0.2896558344364166\n",
      "[Training Epoch 1] Batch 3087, Loss 0.27469220757484436\n",
      "[Training Epoch 1] Batch 3088, Loss 0.29538920521736145\n",
      "[Training Epoch 1] Batch 3089, Loss 0.29967185854911804\n",
      "[Training Epoch 1] Batch 3090, Loss 0.2897924780845642\n",
      "[Training Epoch 1] Batch 3091, Loss 0.28164446353912354\n",
      "[Training Epoch 1] Batch 3092, Loss 0.28508391976356506\n",
      "[Training Epoch 1] Batch 3093, Loss 0.2718731462955475\n",
      "[Training Epoch 1] Batch 3094, Loss 0.28220975399017334\n",
      "[Training Epoch 1] Batch 3095, Loss 0.3101135790348053\n",
      "[Training Epoch 1] Batch 3096, Loss 0.28730398416519165\n",
      "[Training Epoch 1] Batch 3097, Loss 0.2831707000732422\n",
      "[Training Epoch 1] Batch 3098, Loss 0.3004561960697174\n",
      "[Training Epoch 1] Batch 3099, Loss 0.28461164236068726\n",
      "[Training Epoch 1] Batch 3100, Loss 0.2851342558860779\n",
      "[Training Epoch 1] Batch 3101, Loss 0.2822679877281189\n",
      "[Training Epoch 1] Batch 3102, Loss 0.2910880148410797\n",
      "[Training Epoch 1] Batch 3103, Loss 0.27407655119895935\n",
      "[Training Epoch 1] Batch 3104, Loss 0.3141598403453827\n",
      "[Training Epoch 1] Batch 3105, Loss 0.289752721786499\n",
      "[Training Epoch 1] Batch 3106, Loss 0.2853853106498718\n",
      "[Training Epoch 1] Batch 3107, Loss 0.32650870084762573\n",
      "[Training Epoch 1] Batch 3108, Loss 0.3128550350666046\n",
      "[Training Epoch 1] Batch 3109, Loss 0.3049982488155365\n",
      "[Training Epoch 1] Batch 3110, Loss 0.2662702798843384\n",
      "[Training Epoch 1] Batch 3111, Loss 0.30449599027633667\n",
      "[Training Epoch 1] Batch 3112, Loss 0.27320581674575806\n",
      "[Training Epoch 1] Batch 3113, Loss 0.294264018535614\n",
      "[Training Epoch 1] Batch 3114, Loss 0.30250608921051025\n",
      "[Training Epoch 1] Batch 3115, Loss 0.3102591037750244\n",
      "[Training Epoch 1] Batch 3116, Loss 0.29382842779159546\n",
      "[Training Epoch 1] Batch 3117, Loss 0.2959020137786865\n",
      "[Training Epoch 1] Batch 3118, Loss 0.278205931186676\n",
      "[Training Epoch 1] Batch 3119, Loss 0.30404797196388245\n",
      "[Training Epoch 1] Batch 3120, Loss 0.2728999853134155\n",
      "[Training Epoch 1] Batch 3121, Loss 0.2861284613609314\n",
      "[Training Epoch 1] Batch 3122, Loss 0.31904375553131104\n",
      "[Training Epoch 1] Batch 3123, Loss 0.27328258752822876\n",
      "[Training Epoch 1] Batch 3124, Loss 0.28906065225601196\n",
      "[Training Epoch 1] Batch 3125, Loss 0.3057140111923218\n",
      "[Training Epoch 1] Batch 3126, Loss 0.2948378324508667\n",
      "[Training Epoch 1] Batch 3127, Loss 0.2776721715927124\n",
      "[Training Epoch 1] Batch 3128, Loss 0.3063816428184509\n",
      "[Training Epoch 1] Batch 3129, Loss 0.30037349462509155\n",
      "[Training Epoch 1] Batch 3130, Loss 0.26544225215911865\n",
      "[Training Epoch 1] Batch 3131, Loss 0.29194509983062744\n",
      "[Training Epoch 1] Batch 3132, Loss 0.2863386571407318\n",
      "[Training Epoch 1] Batch 3133, Loss 0.3162667751312256\n",
      "[Training Epoch 1] Batch 3134, Loss 0.29037797451019287\n",
      "[Training Epoch 1] Batch 3135, Loss 0.31324514746665955\n",
      "[Training Epoch 1] Batch 3136, Loss 0.292385995388031\n",
      "[Training Epoch 1] Batch 3137, Loss 0.2857532501220703\n",
      "[Training Epoch 1] Batch 3138, Loss 0.29821497201919556\n",
      "[Training Epoch 1] Batch 3139, Loss 0.2721932530403137\n",
      "[Training Epoch 1] Batch 3140, Loss 0.28213924169540405\n",
      "[Training Epoch 1] Batch 3141, Loss 0.2945610284805298\n",
      "[Training Epoch 1] Batch 3142, Loss 0.28697484731674194\n",
      "[Training Epoch 1] Batch 3143, Loss 0.28509265184402466\n",
      "[Training Epoch 1] Batch 3144, Loss 0.3248308002948761\n",
      "[Training Epoch 1] Batch 3145, Loss 0.2771303057670593\n",
      "[Training Epoch 1] Batch 3146, Loss 0.2970345616340637\n",
      "[Training Epoch 1] Batch 3147, Loss 0.2651398777961731\n",
      "[Training Epoch 1] Batch 3148, Loss 0.2961788475513458\n",
      "[Training Epoch 1] Batch 3149, Loss 0.28871089220046997\n",
      "[Training Epoch 1] Batch 3150, Loss 0.32720792293548584\n",
      "[Training Epoch 1] Batch 3151, Loss 0.29037290811538696\n",
      "[Training Epoch 1] Batch 3152, Loss 0.2644350230693817\n",
      "[Training Epoch 1] Batch 3153, Loss 0.32153892517089844\n",
      "[Training Epoch 1] Batch 3154, Loss 0.3063976466655731\n",
      "[Training Epoch 1] Batch 3155, Loss 0.3005795180797577\n",
      "[Training Epoch 1] Batch 3156, Loss 0.2596394717693329\n",
      "[Training Epoch 1] Batch 3157, Loss 0.2935890555381775\n",
      "[Training Epoch 1] Batch 3158, Loss 0.29551002383232117\n",
      "[Training Epoch 1] Batch 3159, Loss 0.28198498487472534\n",
      "[Training Epoch 1] Batch 3160, Loss 0.3128761053085327\n",
      "[Training Epoch 1] Batch 3161, Loss 0.32124245166778564\n",
      "[Training Epoch 1] Batch 3162, Loss 0.3063889443874359\n",
      "[Training Epoch 1] Batch 3163, Loss 0.30531811714172363\n",
      "[Training Epoch 1] Batch 3164, Loss 0.2760491371154785\n",
      "[Training Epoch 1] Batch 3165, Loss 0.32094424962997437\n",
      "[Training Epoch 1] Batch 3166, Loss 0.33379292488098145\n",
      "[Training Epoch 1] Batch 3167, Loss 0.3113352358341217\n",
      "[Training Epoch 1] Batch 3168, Loss 0.2930567264556885\n",
      "[Training Epoch 1] Batch 3169, Loss 0.3051481246948242\n",
      "[Training Epoch 1] Batch 3170, Loss 0.30574366450309753\n",
      "[Training Epoch 1] Batch 3171, Loss 0.29717132449150085\n",
      "[Training Epoch 1] Batch 3172, Loss 0.2832239866256714\n",
      "[Training Epoch 1] Batch 3173, Loss 0.30072346329689026\n",
      "[Training Epoch 1] Batch 3174, Loss 0.3009267747402191\n",
      "[Training Epoch 1] Batch 3175, Loss 0.302106112241745\n",
      "[Training Epoch 1] Batch 3176, Loss 0.29401925206184387\n",
      "[Training Epoch 1] Batch 3177, Loss 0.3154190480709076\n",
      "[Training Epoch 1] Batch 3178, Loss 0.30427902936935425\n",
      "[Training Epoch 1] Batch 3179, Loss 0.2882496416568756\n",
      "[Training Epoch 1] Batch 3180, Loss 0.28881633281707764\n",
      "[Training Epoch 1] Batch 3181, Loss 0.30560874938964844\n",
      "[Training Epoch 1] Batch 3182, Loss 0.2809501588344574\n",
      "[Training Epoch 1] Batch 3183, Loss 0.3019658327102661\n",
      "[Training Epoch 1] Batch 3184, Loss 0.27785512804985046\n",
      "[Training Epoch 1] Batch 3185, Loss 0.27686816453933716\n",
      "[Training Epoch 1] Batch 3186, Loss 0.2940850853919983\n",
      "[Training Epoch 1] Batch 3187, Loss 0.2813887596130371\n",
      "[Training Epoch 1] Batch 3188, Loss 0.28584712743759155\n",
      "[Training Epoch 1] Batch 3189, Loss 0.2992193102836609\n",
      "[Training Epoch 1] Batch 3190, Loss 0.2737825810909271\n",
      "[Training Epoch 1] Batch 3191, Loss 0.28528892993927\n",
      "[Training Epoch 1] Batch 3192, Loss 0.28812849521636963\n",
      "[Training Epoch 1] Batch 3193, Loss 0.2680525779724121\n",
      "[Training Epoch 1] Batch 3194, Loss 0.29552289843559265\n",
      "[Training Epoch 1] Batch 3195, Loss 0.28710001707077026\n",
      "[Training Epoch 1] Batch 3196, Loss 0.307894229888916\n",
      "[Training Epoch 1] Batch 3197, Loss 0.29303574562072754\n",
      "[Training Epoch 1] Batch 3198, Loss 0.2853624224662781\n",
      "[Training Epoch 1] Batch 3199, Loss 0.30579066276550293\n",
      "[Training Epoch 1] Batch 3200, Loss 0.2721441388130188\n",
      "[Training Epoch 1] Batch 3201, Loss 0.28065571188926697\n",
      "[Training Epoch 1] Batch 3202, Loss 0.2630648612976074\n",
      "[Training Epoch 1] Batch 3203, Loss 0.2655603289604187\n",
      "[Training Epoch 1] Batch 3204, Loss 0.28636908531188965\n",
      "[Training Epoch 1] Batch 3205, Loss 0.30292797088623047\n",
      "[Training Epoch 1] Batch 3206, Loss 0.2968083620071411\n",
      "[Training Epoch 1] Batch 3207, Loss 0.31030774116516113\n",
      "[Training Epoch 1] Batch 3208, Loss 0.3138364851474762\n",
      "[Training Epoch 1] Batch 3209, Loss 0.3228447437286377\n",
      "[Training Epoch 1] Batch 3210, Loss 0.33403539657592773\n",
      "[Training Epoch 1] Batch 3211, Loss 0.28941601514816284\n",
      "[Training Epoch 1] Batch 3212, Loss 0.28684693574905396\n",
      "[Training Epoch 1] Batch 3213, Loss 0.2965949773788452\n",
      "[Training Epoch 1] Batch 3214, Loss 0.2827760875225067\n",
      "[Training Epoch 1] Batch 3215, Loss 0.28608840703964233\n",
      "[Training Epoch 1] Batch 3216, Loss 0.29300498962402344\n",
      "[Training Epoch 1] Batch 3217, Loss 0.2994554340839386\n",
      "[Training Epoch 1] Batch 3218, Loss 0.3133328855037689\n",
      "[Training Epoch 1] Batch 3219, Loss 0.3134693503379822\n",
      "[Training Epoch 1] Batch 3220, Loss 0.32892775535583496\n",
      "[Training Epoch 1] Batch 3221, Loss 0.2791264057159424\n",
      "[Training Epoch 1] Batch 3222, Loss 0.30800166726112366\n",
      "[Training Epoch 1] Batch 3223, Loss 0.28833264112472534\n",
      "[Training Epoch 1] Batch 3224, Loss 0.28567975759506226\n",
      "[Training Epoch 1] Batch 3225, Loss 0.316670298576355\n",
      "[Training Epoch 1] Batch 3226, Loss 0.2745811939239502\n",
      "[Training Epoch 1] Batch 3227, Loss 0.292815238237381\n",
      "[Training Epoch 1] Batch 3228, Loss 0.27217161655426025\n",
      "[Training Epoch 1] Batch 3229, Loss 0.2797493636608124\n",
      "[Training Epoch 1] Batch 3230, Loss 0.27825435996055603\n",
      "[Training Epoch 1] Batch 3231, Loss 0.29167985916137695\n",
      "[Training Epoch 1] Batch 3232, Loss 0.3125534653663635\n",
      "[Training Epoch 1] Batch 3233, Loss 0.3002234697341919\n",
      "[Training Epoch 1] Batch 3234, Loss 0.2836596369743347\n",
      "[Training Epoch 1] Batch 3235, Loss 0.3049977421760559\n",
      "[Training Epoch 1] Batch 3236, Loss 0.27539414167404175\n",
      "[Training Epoch 1] Batch 3237, Loss 0.26630908250808716\n",
      "[Training Epoch 1] Batch 3238, Loss 0.3127864897251129\n",
      "[Training Epoch 1] Batch 3239, Loss 0.28727662563323975\n",
      "[Training Epoch 1] Batch 3240, Loss 0.25905388593673706\n",
      "[Training Epoch 1] Batch 3241, Loss 0.2884010374546051\n",
      "[Training Epoch 1] Batch 3242, Loss 0.28507548570632935\n",
      "[Training Epoch 1] Batch 3243, Loss 0.2761792540550232\n",
      "[Training Epoch 1] Batch 3244, Loss 0.29348573088645935\n",
      "[Training Epoch 1] Batch 3245, Loss 0.2780417501926422\n",
      "[Training Epoch 1] Batch 3246, Loss 0.30970025062561035\n",
      "[Training Epoch 1] Batch 3247, Loss 0.2702668309211731\n",
      "[Training Epoch 1] Batch 3248, Loss 0.29842230677604675\n",
      "[Training Epoch 1] Batch 3249, Loss 0.32856425642967224\n",
      "[Training Epoch 1] Batch 3250, Loss 0.3278822898864746\n",
      "[Training Epoch 1] Batch 3251, Loss 0.2954173982143402\n",
      "[Training Epoch 1] Batch 3252, Loss 0.2862011790275574\n",
      "[Training Epoch 1] Batch 3253, Loss 0.27866485714912415\n",
      "[Training Epoch 1] Batch 3254, Loss 0.27018043398857117\n",
      "[Training Epoch 1] Batch 3255, Loss 0.2623755931854248\n",
      "[Training Epoch 1] Batch 3256, Loss 0.33125007152557373\n",
      "[Training Epoch 1] Batch 3257, Loss 0.31878983974456787\n",
      "[Training Epoch 1] Batch 3258, Loss 0.2887520492076874\n",
      "[Training Epoch 1] Batch 3259, Loss 0.30155664682388306\n",
      "[Training Epoch 1] Batch 3260, Loss 0.29133960604667664\n",
      "[Training Epoch 1] Batch 3261, Loss 0.2881568670272827\n",
      "[Training Epoch 1] Batch 3262, Loss 0.31745806336402893\n",
      "[Training Epoch 1] Batch 3263, Loss 0.2774854600429535\n",
      "[Training Epoch 1] Batch 3264, Loss 0.3117954432964325\n",
      "[Training Epoch 1] Batch 3265, Loss 0.30418795347213745\n",
      "[Training Epoch 1] Batch 3266, Loss 0.2963840961456299\n",
      "[Training Epoch 1] Batch 3267, Loss 0.2900296151638031\n",
      "[Training Epoch 1] Batch 3268, Loss 0.2917709946632385\n",
      "[Training Epoch 1] Batch 3269, Loss 0.29674673080444336\n",
      "[Training Epoch 1] Batch 3270, Loss 0.3071756958961487\n",
      "[Training Epoch 1] Batch 3271, Loss 0.28244084119796753\n",
      "[Training Epoch 1] Batch 3272, Loss 0.2765851318836212\n",
      "[Training Epoch 1] Batch 3273, Loss 0.2928827106952667\n",
      "[Training Epoch 1] Batch 3274, Loss 0.2827211916446686\n",
      "[Training Epoch 1] Batch 3275, Loss 0.2867925465106964\n",
      "[Training Epoch 1] Batch 3276, Loss 0.26915138959884644\n",
      "[Training Epoch 1] Batch 3277, Loss 0.31192827224731445\n",
      "[Training Epoch 1] Batch 3278, Loss 0.29543980956077576\n",
      "[Training Epoch 1] Batch 3279, Loss 0.30381178855895996\n",
      "[Training Epoch 1] Batch 3280, Loss 0.2745285928249359\n",
      "[Training Epoch 1] Batch 3281, Loss 0.2771593928337097\n",
      "[Training Epoch 1] Batch 3282, Loss 0.29743432998657227\n",
      "[Training Epoch 1] Batch 3283, Loss 0.26278966665267944\n",
      "[Training Epoch 1] Batch 3284, Loss 0.27964091300964355\n",
      "[Training Epoch 1] Batch 3285, Loss 0.28128862380981445\n",
      "[Training Epoch 1] Batch 3286, Loss 0.3089427649974823\n",
      "[Training Epoch 1] Batch 3287, Loss 0.3224904537200928\n",
      "[Training Epoch 1] Batch 3288, Loss 0.29330241680145264\n",
      "[Training Epoch 1] Batch 3289, Loss 0.3164130449295044\n",
      "[Training Epoch 1] Batch 3290, Loss 0.30061304569244385\n",
      "[Training Epoch 1] Batch 3291, Loss 0.27718842029571533\n",
      "[Training Epoch 1] Batch 3292, Loss 0.3005349636077881\n",
      "[Training Epoch 1] Batch 3293, Loss 0.29639363288879395\n",
      "[Training Epoch 1] Batch 3294, Loss 0.29291778802871704\n",
      "[Training Epoch 1] Batch 3295, Loss 0.2953808605670929\n",
      "[Training Epoch 1] Batch 3296, Loss 0.28605562448501587\n",
      "[Training Epoch 1] Batch 3297, Loss 0.3147108554840088\n",
      "[Training Epoch 1] Batch 3298, Loss 0.2786024808883667\n",
      "[Training Epoch 1] Batch 3299, Loss 0.2853620946407318\n",
      "[Training Epoch 1] Batch 3300, Loss 0.2829616963863373\n",
      "[Training Epoch 1] Batch 3301, Loss 0.31980544328689575\n",
      "[Training Epoch 1] Batch 3302, Loss 0.2722187638282776\n",
      "[Training Epoch 1] Batch 3303, Loss 0.27147722244262695\n",
      "[Training Epoch 1] Batch 3304, Loss 0.27662932872772217\n",
      "[Training Epoch 1] Batch 3305, Loss 0.2895762324333191\n",
      "[Training Epoch 1] Batch 3306, Loss 0.2887343466281891\n",
      "[Training Epoch 1] Batch 3307, Loss 0.28473132848739624\n",
      "[Training Epoch 1] Batch 3308, Loss 0.30585744976997375\n",
      "[Training Epoch 1] Batch 3309, Loss 0.32396724820137024\n",
      "[Training Epoch 1] Batch 3310, Loss 0.2974002957344055\n",
      "[Training Epoch 1] Batch 3311, Loss 0.2820272743701935\n",
      "[Training Epoch 1] Batch 3312, Loss 0.27756768465042114\n",
      "[Training Epoch 1] Batch 3313, Loss 0.293490469455719\n",
      "[Training Epoch 1] Batch 3314, Loss 0.28484076261520386\n",
      "[Training Epoch 1] Batch 3315, Loss 0.34025904536247253\n",
      "[Training Epoch 1] Batch 3316, Loss 0.29306504130363464\n",
      "[Training Epoch 1] Batch 3317, Loss 0.2954806089401245\n",
      "[Training Epoch 1] Batch 3318, Loss 0.2843189537525177\n",
      "[Training Epoch 1] Batch 3319, Loss 0.2804083228111267\n",
      "[Training Epoch 1] Batch 3320, Loss 0.2856881618499756\n",
      "[Training Epoch 1] Batch 3321, Loss 0.30043116211891174\n",
      "[Training Epoch 1] Batch 3322, Loss 0.2955099940299988\n",
      "[Training Epoch 1] Batch 3323, Loss 0.27615368366241455\n",
      "[Training Epoch 1] Batch 3324, Loss 0.30252569913864136\n",
      "[Training Epoch 1] Batch 3325, Loss 0.2786526679992676\n",
      "[Training Epoch 1] Batch 3326, Loss 0.30977246165275574\n",
      "[Training Epoch 1] Batch 3327, Loss 0.29921212792396545\n",
      "[Training Epoch 1] Batch 3328, Loss 0.25929543375968933\n",
      "[Training Epoch 1] Batch 3329, Loss 0.27210211753845215\n",
      "[Training Epoch 1] Batch 3330, Loss 0.28535014390945435\n",
      "[Training Epoch 1] Batch 3331, Loss 0.30043619871139526\n",
      "[Training Epoch 1] Batch 3332, Loss 0.3089958727359772\n",
      "[Training Epoch 1] Batch 3333, Loss 0.28503262996673584\n",
      "[Training Epoch 1] Batch 3334, Loss 0.28314340114593506\n",
      "[Training Epoch 1] Batch 3335, Loss 0.2824457287788391\n",
      "[Training Epoch 1] Batch 3336, Loss 0.3097603917121887\n",
      "[Training Epoch 1] Batch 3337, Loss 0.32474255561828613\n",
      "[Training Epoch 1] Batch 3338, Loss 0.27681949734687805\n",
      "[Training Epoch 1] Batch 3339, Loss 0.28816163539886475\n",
      "[Training Epoch 1] Batch 3340, Loss 0.2812578082084656\n",
      "[Training Epoch 1] Batch 3341, Loss 0.30503588914871216\n",
      "[Training Epoch 1] Batch 3342, Loss 0.2941637933254242\n",
      "[Training Epoch 1] Batch 3343, Loss 0.3122907876968384\n",
      "[Training Epoch 1] Batch 3344, Loss 0.31244054436683655\n",
      "[Training Epoch 1] Batch 3345, Loss 0.2822880744934082\n",
      "[Training Epoch 1] Batch 3346, Loss 0.2905160188674927\n",
      "[Training Epoch 1] Batch 3347, Loss 0.29321402311325073\n",
      "[Training Epoch 1] Batch 3348, Loss 0.281402587890625\n",
      "[Training Epoch 1] Batch 3349, Loss 0.3228524327278137\n",
      "[Training Epoch 1] Batch 3350, Loss 0.2801271975040436\n",
      "[Training Epoch 1] Batch 3351, Loss 0.2856980562210083\n",
      "[Training Epoch 1] Batch 3352, Loss 0.30394503474235535\n",
      "[Training Epoch 1] Batch 3353, Loss 0.2698405981063843\n",
      "[Training Epoch 1] Batch 3354, Loss 0.27609533071517944\n",
      "[Training Epoch 1] Batch 3355, Loss 0.30376148223876953\n",
      "[Training Epoch 1] Batch 3356, Loss 0.30970412492752075\n",
      "[Training Epoch 1] Batch 3357, Loss 0.26577818393707275\n",
      "[Training Epoch 1] Batch 3358, Loss 0.26257315278053284\n",
      "[Training Epoch 1] Batch 3359, Loss 0.3074233829975128\n",
      "[Training Epoch 1] Batch 3360, Loss 0.2756279408931732\n",
      "[Training Epoch 1] Batch 3361, Loss 0.2981488108634949\n",
      "[Training Epoch 1] Batch 3362, Loss 0.2890564799308777\n",
      "[Training Epoch 1] Batch 3363, Loss 0.29760029911994934\n",
      "[Training Epoch 1] Batch 3364, Loss 0.27772513031959534\n",
      "[Training Epoch 1] Batch 3365, Loss 0.2770487666130066\n",
      "[Training Epoch 1] Batch 3366, Loss 0.28399914503097534\n",
      "[Training Epoch 1] Batch 3367, Loss 0.2899971604347229\n",
      "[Training Epoch 1] Batch 3368, Loss 0.28572940826416016\n",
      "[Training Epoch 1] Batch 3369, Loss 0.31563040614128113\n",
      "[Training Epoch 1] Batch 3370, Loss 0.3231981098651886\n",
      "[Training Epoch 1] Batch 3371, Loss 0.31390753388404846\n",
      "[Training Epoch 1] Batch 3372, Loss 0.2804397940635681\n",
      "[Training Epoch 1] Batch 3373, Loss 0.28528621792793274\n",
      "[Training Epoch 1] Batch 3374, Loss 0.2737206518650055\n",
      "[Training Epoch 1] Batch 3375, Loss 0.3063216507434845\n",
      "[Training Epoch 1] Batch 3376, Loss 0.30690521001815796\n",
      "[Training Epoch 1] Batch 3377, Loss 0.28406891226768494\n",
      "[Training Epoch 1] Batch 3378, Loss 0.30124330520629883\n",
      "[Training Epoch 1] Batch 3379, Loss 0.2850883901119232\n",
      "[Training Epoch 1] Batch 3380, Loss 0.27879446744918823\n",
      "[Training Epoch 1] Batch 3381, Loss 0.29896438121795654\n",
      "[Training Epoch 1] Batch 3382, Loss 0.2959015369415283\n",
      "[Training Epoch 1] Batch 3383, Loss 0.29898542165756226\n",
      "[Training Epoch 1] Batch 3384, Loss 0.28128886222839355\n",
      "[Training Epoch 1] Batch 3385, Loss 0.29946279525756836\n",
      "[Training Epoch 1] Batch 3386, Loss 0.2704658508300781\n",
      "[Training Epoch 1] Batch 3387, Loss 0.3111681044101715\n",
      "[Training Epoch 1] Batch 3388, Loss 0.2976265847682953\n",
      "[Training Epoch 1] Batch 3389, Loss 0.2853987216949463\n",
      "[Training Epoch 1] Batch 3390, Loss 0.29804712533950806\n",
      "[Training Epoch 1] Batch 3391, Loss 0.2812681496143341\n",
      "[Training Epoch 1] Batch 3392, Loss 0.2729169726371765\n",
      "[Training Epoch 1] Batch 3393, Loss 0.2700155973434448\n",
      "[Training Epoch 1] Batch 3394, Loss 0.2687032222747803\n",
      "[Training Epoch 1] Batch 3395, Loss 0.30538320541381836\n",
      "[Training Epoch 1] Batch 3396, Loss 0.28641900420188904\n",
      "[Training Epoch 1] Batch 3397, Loss 0.2711447477340698\n",
      "[Training Epoch 1] Batch 3398, Loss 0.31586140394210815\n",
      "[Training Epoch 1] Batch 3399, Loss 0.35084784030914307\n",
      "[Training Epoch 1] Batch 3400, Loss 0.27961283922195435\n",
      "[Training Epoch 1] Batch 3401, Loss 0.28134843707084656\n",
      "[Training Epoch 1] Batch 3402, Loss 0.2791849672794342\n",
      "[Training Epoch 1] Batch 3403, Loss 0.2701246738433838\n",
      "[Training Epoch 1] Batch 3404, Loss 0.302187442779541\n",
      "[Training Epoch 1] Batch 3405, Loss 0.2887211740016937\n",
      "[Training Epoch 1] Batch 3406, Loss 0.2994416654109955\n",
      "[Training Epoch 1] Batch 3407, Loss 0.2941545248031616\n",
      "[Training Epoch 1] Batch 3408, Loss 0.30636030435562134\n",
      "[Training Epoch 1] Batch 3409, Loss 0.30687886476516724\n",
      "[Training Epoch 1] Batch 3410, Loss 0.2941943407058716\n",
      "[Training Epoch 1] Batch 3411, Loss 0.2784512937068939\n",
      "[Training Epoch 1] Batch 3412, Loss 0.2785314917564392\n",
      "[Training Epoch 1] Batch 3413, Loss 0.2904454171657562\n",
      "[Training Epoch 1] Batch 3414, Loss 0.2908545136451721\n",
      "[Training Epoch 1] Batch 3415, Loss 0.302854061126709\n",
      "[Training Epoch 1] Batch 3416, Loss 0.2936026453971863\n",
      "[Training Epoch 1] Batch 3417, Loss 0.31247249245643616\n",
      "[Training Epoch 1] Batch 3418, Loss 0.3201315402984619\n",
      "[Training Epoch 1] Batch 3419, Loss 0.2773842215538025\n",
      "[Training Epoch 1] Batch 3420, Loss 0.27988201379776\n",
      "[Training Epoch 1] Batch 3421, Loss 0.2828686833381653\n",
      "[Training Epoch 1] Batch 3422, Loss 0.2881057858467102\n",
      "[Training Epoch 1] Batch 3423, Loss 0.2925565838813782\n",
      "[Training Epoch 1] Batch 3424, Loss 0.279593288898468\n",
      "[Training Epoch 1] Batch 3425, Loss 0.2956605553627014\n",
      "[Training Epoch 1] Batch 3426, Loss 0.28472813963890076\n",
      "[Training Epoch 1] Batch 3427, Loss 0.291782021522522\n",
      "[Training Epoch 1] Batch 3428, Loss 0.286942720413208\n",
      "[Training Epoch 1] Batch 3429, Loss 0.28909116983413696\n",
      "[Training Epoch 1] Batch 3430, Loss 0.30700981616973877\n",
      "[Training Epoch 1] Batch 3431, Loss 0.30928629636764526\n",
      "[Training Epoch 1] Batch 3432, Loss 0.28391656279563904\n",
      "[Training Epoch 1] Batch 3433, Loss 0.29832565784454346\n",
      "[Training Epoch 1] Batch 3434, Loss 0.29407548904418945\n",
      "[Training Epoch 1] Batch 3435, Loss 0.31924039125442505\n",
      "[Training Epoch 1] Batch 3436, Loss 0.2941902279853821\n",
      "[Training Epoch 1] Batch 3437, Loss 0.3121851682662964\n",
      "[Training Epoch 1] Batch 3438, Loss 0.3018015921115875\n",
      "[Training Epoch 1] Batch 3439, Loss 0.290619432926178\n",
      "[Training Epoch 1] Batch 3440, Loss 0.3342610001564026\n",
      "[Training Epoch 1] Batch 3441, Loss 0.3056986331939697\n",
      "[Training Epoch 1] Batch 3442, Loss 0.3032451868057251\n",
      "[Training Epoch 1] Batch 3443, Loss 0.2835959792137146\n",
      "[Training Epoch 1] Batch 3444, Loss 0.32245779037475586\n",
      "[Training Epoch 1] Batch 3445, Loss 0.2835070490837097\n",
      "[Training Epoch 1] Batch 3446, Loss 0.2872805893421173\n",
      "[Training Epoch 1] Batch 3447, Loss 0.29964250326156616\n",
      "[Training Epoch 1] Batch 3448, Loss 0.2993967831134796\n",
      "[Training Epoch 1] Batch 3449, Loss 0.31368139386177063\n",
      "[Training Epoch 1] Batch 3450, Loss 0.27386942505836487\n",
      "[Training Epoch 1] Batch 3451, Loss 0.2776299715042114\n",
      "[Training Epoch 1] Batch 3452, Loss 0.28569114208221436\n",
      "[Training Epoch 1] Batch 3453, Loss 0.2914624512195587\n",
      "[Training Epoch 1] Batch 3454, Loss 0.31720566749572754\n",
      "[Training Epoch 1] Batch 3455, Loss 0.3095833957195282\n",
      "[Training Epoch 1] Batch 3456, Loss 0.2920607328414917\n",
      "[Training Epoch 1] Batch 3457, Loss 0.26569879055023193\n",
      "[Training Epoch 1] Batch 3458, Loss 0.2972462773323059\n",
      "[Training Epoch 1] Batch 3459, Loss 0.29140424728393555\n",
      "[Training Epoch 1] Batch 3460, Loss 0.31611770391464233\n",
      "[Training Epoch 1] Batch 3461, Loss 0.2936881184577942\n",
      "[Training Epoch 1] Batch 3462, Loss 0.30605548620224\n",
      "[Training Epoch 1] Batch 3463, Loss 0.2900075316429138\n",
      "[Training Epoch 1] Batch 3464, Loss 0.30671823024749756\n",
      "[Training Epoch 1] Batch 3465, Loss 0.27233898639678955\n",
      "[Training Epoch 1] Batch 3466, Loss 0.3149278461933136\n",
      "[Training Epoch 1] Batch 3467, Loss 0.2966020405292511\n",
      "[Training Epoch 1] Batch 3468, Loss 0.3094494938850403\n",
      "[Training Epoch 1] Batch 3469, Loss 0.2804403603076935\n",
      "[Training Epoch 1] Batch 3470, Loss 0.28592753410339355\n",
      "[Training Epoch 1] Batch 3471, Loss 0.2694452404975891\n",
      "[Training Epoch 1] Batch 3472, Loss 0.2870502471923828\n",
      "[Training Epoch 1] Batch 3473, Loss 0.29284951090812683\n",
      "[Training Epoch 1] Batch 3474, Loss 0.289029598236084\n",
      "[Training Epoch 1] Batch 3475, Loss 0.3121088445186615\n",
      "[Training Epoch 1] Batch 3476, Loss 0.26694685220718384\n",
      "[Training Epoch 1] Batch 3477, Loss 0.2845311164855957\n",
      "[Training Epoch 1] Batch 3478, Loss 0.3311205506324768\n",
      "[Training Epoch 1] Batch 3479, Loss 0.290030837059021\n",
      "[Training Epoch 1] Batch 3480, Loss 0.3139338791370392\n",
      "[Training Epoch 1] Batch 3481, Loss 0.29142460227012634\n",
      "[Training Epoch 1] Batch 3482, Loss 0.3027254343032837\n",
      "[Training Epoch 1] Batch 3483, Loss 0.3136894702911377\n",
      "[Training Epoch 1] Batch 3484, Loss 0.2928428053855896\n",
      "[Training Epoch 1] Batch 3485, Loss 0.30902770161628723\n",
      "[Training Epoch 1] Batch 3486, Loss 0.313073992729187\n",
      "[Training Epoch 1] Batch 3487, Loss 0.2997981309890747\n",
      "[Training Epoch 1] Batch 3488, Loss 0.26745709776878357\n",
      "[Training Epoch 1] Batch 3489, Loss 0.2950572967529297\n",
      "[Training Epoch 1] Batch 3490, Loss 0.3022296726703644\n",
      "[Training Epoch 1] Batch 3491, Loss 0.3064655363559723\n",
      "[Training Epoch 1] Batch 3492, Loss 0.2859436869621277\n",
      "[Training Epoch 1] Batch 3493, Loss 0.28705933690071106\n",
      "[Training Epoch 1] Batch 3494, Loss 0.2803710103034973\n",
      "[Training Epoch 1] Batch 3495, Loss 0.27727580070495605\n",
      "[Training Epoch 1] Batch 3496, Loss 0.27737361192703247\n",
      "[Training Epoch 1] Batch 3497, Loss 0.31418612599372864\n",
      "[Training Epoch 1] Batch 3498, Loss 0.2891693115234375\n",
      "[Training Epoch 1] Batch 3499, Loss 0.3010196089744568\n",
      "[Training Epoch 1] Batch 3500, Loss 0.30155178904533386\n",
      "[Training Epoch 1] Batch 3501, Loss 0.30246037244796753\n",
      "[Training Epoch 1] Batch 3502, Loss 0.28872066736221313\n",
      "[Training Epoch 1] Batch 3503, Loss 0.2791772484779358\n",
      "[Training Epoch 1] Batch 3504, Loss 0.2819983959197998\n",
      "[Training Epoch 1] Batch 3505, Loss 0.29901301860809326\n",
      "[Training Epoch 1] Batch 3506, Loss 0.2681472599506378\n",
      "[Training Epoch 1] Batch 3507, Loss 0.27518826723098755\n",
      "[Training Epoch 1] Batch 3508, Loss 0.3232913315296173\n",
      "[Training Epoch 1] Batch 3509, Loss 0.29243040084838867\n",
      "[Training Epoch 1] Batch 3510, Loss 0.2886273264884949\n",
      "[Training Epoch 1] Batch 3511, Loss 0.275155246257782\n",
      "[Training Epoch 1] Batch 3512, Loss 0.2773154675960541\n",
      "[Training Epoch 1] Batch 3513, Loss 0.27875983715057373\n",
      "[Training Epoch 1] Batch 3514, Loss 0.30259573459625244\n",
      "[Training Epoch 1] Batch 3515, Loss 0.29544010758399963\n",
      "[Training Epoch 1] Batch 3516, Loss 0.33235493302345276\n",
      "[Training Epoch 1] Batch 3517, Loss 0.31694895029067993\n",
      "[Training Epoch 1] Batch 3518, Loss 0.31012648344039917\n",
      "[Training Epoch 1] Batch 3519, Loss 0.28253239393234253\n",
      "[Training Epoch 1] Batch 3520, Loss 0.28812354803085327\n",
      "[Training Epoch 1] Batch 3521, Loss 0.3016549348831177\n",
      "[Training Epoch 1] Batch 3522, Loss 0.32580435276031494\n",
      "[Training Epoch 1] Batch 3523, Loss 0.2803712487220764\n",
      "[Training Epoch 1] Batch 3524, Loss 0.3139917254447937\n",
      "[Training Epoch 1] Batch 3525, Loss 0.2956699728965759\n",
      "[Training Epoch 1] Batch 3526, Loss 0.2930862009525299\n",
      "[Training Epoch 1] Batch 3527, Loss 0.30683234333992004\n",
      "[Training Epoch 1] Batch 3528, Loss 0.2728566825389862\n",
      "[Training Epoch 1] Batch 3529, Loss 0.3028657138347626\n",
      "[Training Epoch 1] Batch 3530, Loss 0.30450576543807983\n",
      "[Training Epoch 1] Batch 3531, Loss 0.29921460151672363\n",
      "[Training Epoch 1] Batch 3532, Loss 0.290933758020401\n",
      "[Training Epoch 1] Batch 3533, Loss 0.29048144817352295\n",
      "[Training Epoch 1] Batch 3534, Loss 0.2983156442642212\n",
      "[Training Epoch 1] Batch 3535, Loss 0.2773129343986511\n",
      "[Training Epoch 1] Batch 3536, Loss 0.3226279616355896\n",
      "[Training Epoch 1] Batch 3537, Loss 0.2734020948410034\n",
      "[Training Epoch 1] Batch 3538, Loss 0.30329060554504395\n",
      "[Training Epoch 1] Batch 3539, Loss 0.31706804037094116\n",
      "[Training Epoch 1] Batch 3540, Loss 0.2944285273551941\n",
      "[Training Epoch 1] Batch 3541, Loss 0.3068030774593353\n",
      "[Training Epoch 1] Batch 3542, Loss 0.3150724172592163\n",
      "[Training Epoch 1] Batch 3543, Loss 0.29329872131347656\n",
      "[Training Epoch 1] Batch 3544, Loss 0.2811363935470581\n",
      "[Training Epoch 1] Batch 3545, Loss 0.2831760048866272\n",
      "[Training Epoch 1] Batch 3546, Loss 0.2944377064704895\n",
      "[Training Epoch 1] Batch 3547, Loss 0.30117830634117126\n",
      "[Training Epoch 1] Batch 3548, Loss 0.2764441967010498\n",
      "[Training Epoch 1] Batch 3549, Loss 0.28077855706214905\n",
      "[Training Epoch 1] Batch 3550, Loss 0.2772691249847412\n",
      "[Training Epoch 1] Batch 3551, Loss 0.2978915870189667\n",
      "[Training Epoch 1] Batch 3552, Loss 0.29052209854125977\n",
      "[Training Epoch 1] Batch 3553, Loss 0.2971116304397583\n",
      "[Training Epoch 1] Batch 3554, Loss 0.26797547936439514\n",
      "[Training Epoch 1] Batch 3555, Loss 0.28405773639678955\n",
      "[Training Epoch 1] Batch 3556, Loss 0.3041134476661682\n",
      "[Training Epoch 1] Batch 3557, Loss 0.3127210736274719\n",
      "[Training Epoch 1] Batch 3558, Loss 0.28709033131599426\n",
      "[Training Epoch 1] Batch 3559, Loss 0.2624976933002472\n",
      "[Training Epoch 1] Batch 3560, Loss 0.30086463689804077\n",
      "[Training Epoch 1] Batch 3561, Loss 0.3191912770271301\n",
      "[Training Epoch 1] Batch 3562, Loss 0.3011603355407715\n",
      "[Training Epoch 1] Batch 3563, Loss 0.27302122116088867\n",
      "[Training Epoch 1] Batch 3564, Loss 0.3055986166000366\n",
      "[Training Epoch 1] Batch 3565, Loss 0.3379437327384949\n",
      "[Training Epoch 1] Batch 3566, Loss 0.30629345774650574\n",
      "[Training Epoch 1] Batch 3567, Loss 0.2810656428337097\n",
      "[Training Epoch 1] Batch 3568, Loss 0.2742396295070648\n",
      "[Training Epoch 1] Batch 3569, Loss 0.284445583820343\n",
      "[Training Epoch 1] Batch 3570, Loss 0.2844815254211426\n",
      "[Training Epoch 1] Batch 3571, Loss 0.29896992444992065\n",
      "[Training Epoch 1] Batch 3572, Loss 0.29285556077957153\n",
      "[Training Epoch 1] Batch 3573, Loss 0.2910574674606323\n",
      "[Training Epoch 1] Batch 3574, Loss 0.30806636810302734\n",
      "[Training Epoch 1] Batch 3575, Loss 0.29285162687301636\n",
      "[Training Epoch 1] Batch 3576, Loss 0.3003082573413849\n",
      "[Training Epoch 1] Batch 3577, Loss 0.2940158247947693\n",
      "[Training Epoch 1] Batch 3578, Loss 0.28377094864845276\n",
      "[Training Epoch 1] Batch 3579, Loss 0.31254351139068604\n",
      "[Training Epoch 1] Batch 3580, Loss 0.28144770860671997\n",
      "[Training Epoch 1] Batch 3581, Loss 0.26750609278678894\n",
      "[Training Epoch 1] Batch 3582, Loss 0.28655654191970825\n",
      "[Training Epoch 1] Batch 3583, Loss 0.29672014713287354\n",
      "[Training Epoch 1] Batch 3584, Loss 0.30060428380966187\n",
      "[Training Epoch 1] Batch 3585, Loss 0.27858754992485046\n",
      "[Training Epoch 1] Batch 3586, Loss 0.2777000963687897\n",
      "[Training Epoch 1] Batch 3587, Loss 0.33131662011146545\n",
      "[Training Epoch 1] Batch 3588, Loss 0.29957205057144165\n",
      "[Training Epoch 1] Batch 3589, Loss 0.26087409257888794\n",
      "[Training Epoch 1] Batch 3590, Loss 0.30551621317863464\n",
      "[Training Epoch 1] Batch 3591, Loss 0.2937713861465454\n",
      "[Training Epoch 1] Batch 3592, Loss 0.3003787398338318\n",
      "[Training Epoch 1] Batch 3593, Loss 0.2771454453468323\n",
      "[Training Epoch 1] Batch 3594, Loss 0.301053524017334\n",
      "[Training Epoch 1] Batch 3595, Loss 0.30648261308670044\n",
      "[Training Epoch 1] Batch 3596, Loss 0.29171958565711975\n",
      "[Training Epoch 1] Batch 3597, Loss 0.29312509298324585\n",
      "[Training Epoch 1] Batch 3598, Loss 0.26720327138900757\n",
      "[Training Epoch 1] Batch 3599, Loss 0.2990717887878418\n",
      "[Training Epoch 1] Batch 3600, Loss 0.30583882331848145\n",
      "[Training Epoch 1] Batch 3601, Loss 0.2700485587120056\n",
      "[Training Epoch 1] Batch 3602, Loss 0.2807058095932007\n",
      "[Training Epoch 1] Batch 3603, Loss 0.29663750529289246\n",
      "[Training Epoch 1] Batch 3604, Loss 0.29989102482795715\n",
      "[Training Epoch 1] Batch 3605, Loss 0.28728145360946655\n",
      "[Training Epoch 1] Batch 3606, Loss 0.2699331045150757\n",
      "[Training Epoch 1] Batch 3607, Loss 0.28904518485069275\n",
      "[Training Epoch 1] Batch 3608, Loss 0.26967769861221313\n",
      "[Training Epoch 1] Batch 3609, Loss 0.27405065298080444\n",
      "[Training Epoch 1] Batch 3610, Loss 0.2980044186115265\n",
      "[Training Epoch 1] Batch 3611, Loss 0.31632643938064575\n",
      "[Training Epoch 1] Batch 3612, Loss 0.28404074907302856\n",
      "[Training Epoch 1] Batch 3613, Loss 0.27165019512176514\n",
      "[Training Epoch 1] Batch 3614, Loss 0.3009082078933716\n",
      "[Training Epoch 1] Batch 3615, Loss 0.29629653692245483\n",
      "[Training Epoch 1] Batch 3616, Loss 0.2727397084236145\n",
      "[Training Epoch 1] Batch 3617, Loss 0.31825071573257446\n",
      "[Training Epoch 1] Batch 3618, Loss 0.2992056608200073\n",
      "[Training Epoch 1] Batch 3619, Loss 0.2932857871055603\n",
      "[Training Epoch 1] Batch 3620, Loss 0.32138049602508545\n",
      "[Training Epoch 1] Batch 3621, Loss 0.2846866250038147\n",
      "[Training Epoch 1] Batch 3622, Loss 0.27519673109054565\n",
      "[Training Epoch 1] Batch 3623, Loss 0.301470011472702\n",
      "[Training Epoch 1] Batch 3624, Loss 0.3168165683746338\n",
      "[Training Epoch 1] Batch 3625, Loss 0.2717515528202057\n",
      "[Training Epoch 1] Batch 3626, Loss 0.3058420717716217\n",
      "[Training Epoch 1] Batch 3627, Loss 0.28471165895462036\n",
      "[Training Epoch 1] Batch 3628, Loss 0.2679542303085327\n",
      "[Training Epoch 1] Batch 3629, Loss 0.2815481424331665\n",
      "[Training Epoch 1] Batch 3630, Loss 0.2890605032444\n",
      "[Training Epoch 1] Batch 3631, Loss 0.3162108063697815\n",
      "[Training Epoch 1] Batch 3632, Loss 0.29414403438568115\n",
      "[Training Epoch 1] Batch 3633, Loss 0.2809818387031555\n",
      "[Training Epoch 1] Batch 3634, Loss 0.284216046333313\n",
      "[Training Epoch 1] Batch 3635, Loss 0.2727423906326294\n",
      "[Training Epoch 1] Batch 3636, Loss 0.286703884601593\n",
      "[Training Epoch 1] Batch 3637, Loss 0.2685658037662506\n",
      "[Training Epoch 1] Batch 3638, Loss 0.26735344529151917\n",
      "[Training Epoch 1] Batch 3639, Loss 0.3056204319000244\n",
      "[Training Epoch 1] Batch 3640, Loss 0.29011982679367065\n",
      "[Training Epoch 1] Batch 3641, Loss 0.2930435240268707\n",
      "[Training Epoch 1] Batch 3642, Loss 0.2710091471672058\n",
      "[Training Epoch 1] Batch 3643, Loss 0.2958850860595703\n",
      "[Training Epoch 1] Batch 3644, Loss 0.2962484657764435\n",
      "[Training Epoch 1] Batch 3645, Loss 0.301452100276947\n",
      "[Training Epoch 1] Batch 3646, Loss 0.31674763560295105\n",
      "[Training Epoch 1] Batch 3647, Loss 0.27530327439308167\n",
      "[Training Epoch 1] Batch 3648, Loss 0.2931440472602844\n",
      "[Training Epoch 1] Batch 3649, Loss 0.30579522252082825\n",
      "[Training Epoch 1] Batch 3650, Loss 0.283161997795105\n",
      "[Training Epoch 1] Batch 3651, Loss 0.2870227098464966\n",
      "[Training Epoch 1] Batch 3652, Loss 0.29598331451416016\n",
      "[Training Epoch 1] Batch 3653, Loss 0.2926734983921051\n",
      "[Training Epoch 1] Batch 3654, Loss 0.30694809556007385\n",
      "[Training Epoch 1] Batch 3655, Loss 0.30541718006134033\n",
      "[Training Epoch 1] Batch 3656, Loss 0.283682644367218\n",
      "[Training Epoch 1] Batch 3657, Loss 0.2726859450340271\n",
      "[Training Epoch 1] Batch 3658, Loss 0.3124355971813202\n",
      "[Training Epoch 1] Batch 3659, Loss 0.3184124529361725\n",
      "[Training Epoch 1] Batch 3660, Loss 0.29538097977638245\n",
      "[Training Epoch 1] Batch 3661, Loss 0.2676374316215515\n",
      "[Training Epoch 1] Batch 3662, Loss 0.29247918725013733\n",
      "[Training Epoch 1] Batch 3663, Loss 0.2970006465911865\n",
      "[Training Epoch 1] Batch 3664, Loss 0.27100956439971924\n",
      "[Training Epoch 1] Batch 3665, Loss 0.2764489948749542\n",
      "[Training Epoch 1] Batch 3666, Loss 0.27520081400871277\n",
      "[Training Epoch 1] Batch 3667, Loss 0.30081695318222046\n",
      "[Training Epoch 1] Batch 3668, Loss 0.30469247698783875\n",
      "[Training Epoch 1] Batch 3669, Loss 0.28608396649360657\n",
      "[Training Epoch 1] Batch 3670, Loss 0.2873958349227905\n",
      "[Training Epoch 1] Batch 3671, Loss 0.2815972864627838\n",
      "[Training Epoch 1] Batch 3672, Loss 0.2978014349937439\n",
      "[Training Epoch 1] Batch 3673, Loss 0.30061087012290955\n",
      "[Training Epoch 1] Batch 3674, Loss 0.2994152903556824\n",
      "[Training Epoch 1] Batch 3675, Loss 0.26519709825515747\n",
      "[Training Epoch 1] Batch 3676, Loss 0.28831425309181213\n",
      "[Training Epoch 1] Batch 3677, Loss 0.27318283915519714\n",
      "[Training Epoch 1] Batch 3678, Loss 0.30606701970100403\n",
      "[Training Epoch 1] Batch 3679, Loss 0.26578810811042786\n",
      "[Training Epoch 1] Batch 3680, Loss 0.3055512309074402\n",
      "[Training Epoch 1] Batch 3681, Loss 0.25524893403053284\n",
      "[Training Epoch 1] Batch 3682, Loss 0.2728320360183716\n",
      "[Training Epoch 1] Batch 3683, Loss 0.27071505784988403\n",
      "[Training Epoch 1] Batch 3684, Loss 0.2916014492511749\n",
      "[Training Epoch 1] Batch 3685, Loss 0.27093613147735596\n",
      "[Training Epoch 1] Batch 3686, Loss 0.30012285709381104\n",
      "[Training Epoch 1] Batch 3687, Loss 0.31719082593917847\n",
      "[Training Epoch 1] Batch 3688, Loss 0.3029109239578247\n",
      "[Training Epoch 1] Batch 3689, Loss 0.27635306119918823\n",
      "[Training Epoch 1] Batch 3690, Loss 0.29972416162490845\n",
      "[Training Epoch 1] Batch 3691, Loss 0.28680455684661865\n",
      "[Training Epoch 1] Batch 3692, Loss 0.3237054646015167\n",
      "[Training Epoch 1] Batch 3693, Loss 0.27340906858444214\n",
      "[Training Epoch 1] Batch 3694, Loss 0.31622666120529175\n",
      "[Training Epoch 1] Batch 3695, Loss 0.3348809480667114\n",
      "[Training Epoch 1] Batch 3696, Loss 0.29641205072402954\n",
      "[Training Epoch 1] Batch 3697, Loss 0.27168840169906616\n",
      "[Training Epoch 1] Batch 3698, Loss 0.2818875312805176\n",
      "[Training Epoch 1] Batch 3699, Loss 0.31825369596481323\n",
      "[Training Epoch 1] Batch 3700, Loss 0.28135713934898376\n",
      "[Training Epoch 1] Batch 3701, Loss 0.27213847637176514\n",
      "[Training Epoch 1] Batch 3702, Loss 0.28853484988212585\n",
      "[Training Epoch 1] Batch 3703, Loss 0.3137280344963074\n",
      "[Training Epoch 1] Batch 3704, Loss 0.27100467681884766\n",
      "[Training Epoch 1] Batch 3705, Loss 0.287567138671875\n",
      "[Training Epoch 1] Batch 3706, Loss 0.3072093725204468\n",
      "[Training Epoch 1] Batch 3707, Loss 0.2978627383708954\n",
      "[Training Epoch 1] Batch 3708, Loss 0.28898513317108154\n",
      "[Training Epoch 1] Batch 3709, Loss 0.28321409225463867\n",
      "[Training Epoch 1] Batch 3710, Loss 0.30156558752059937\n",
      "[Training Epoch 1] Batch 3711, Loss 0.28923869132995605\n",
      "[Training Epoch 1] Batch 3712, Loss 0.3090806305408478\n",
      "[Training Epoch 1] Batch 3713, Loss 0.28864967823028564\n",
      "[Training Epoch 1] Batch 3714, Loss 0.3012527823448181\n",
      "[Training Epoch 1] Batch 3715, Loss 0.31795597076416016\n",
      "[Training Epoch 1] Batch 3716, Loss 0.3013785183429718\n",
      "[Training Epoch 1] Batch 3717, Loss 0.2505528926849365\n",
      "[Training Epoch 1] Batch 3718, Loss 0.29523444175720215\n",
      "[Training Epoch 1] Batch 3719, Loss 0.2722899913787842\n",
      "[Training Epoch 1] Batch 3720, Loss 0.2705448567867279\n",
      "[Training Epoch 1] Batch 3721, Loss 0.31518226861953735\n",
      "[Training Epoch 1] Batch 3722, Loss 0.2595335841178894\n",
      "[Training Epoch 1] Batch 3723, Loss 0.2813989818096161\n",
      "[Training Epoch 1] Batch 3724, Loss 0.2854662835597992\n",
      "[Training Epoch 1] Batch 3725, Loss 0.3047674298286438\n",
      "[Training Epoch 1] Batch 3726, Loss 0.28259867429733276\n",
      "[Training Epoch 1] Batch 3727, Loss 0.3073903024196625\n",
      "[Training Epoch 1] Batch 3728, Loss 0.2780241370201111\n",
      "[Training Epoch 1] Batch 3729, Loss 0.30085301399230957\n",
      "[Training Epoch 1] Batch 3730, Loss 0.2675693929195404\n",
      "[Training Epoch 1] Batch 3731, Loss 0.29743218421936035\n",
      "[Training Epoch 1] Batch 3732, Loss 0.2787768542766571\n",
      "[Training Epoch 1] Batch 3733, Loss 0.27614885568618774\n",
      "[Training Epoch 1] Batch 3734, Loss 0.30490702390670776\n",
      "[Training Epoch 1] Batch 3735, Loss 0.31118953227996826\n",
      "[Training Epoch 1] Batch 3736, Loss 0.3133064806461334\n",
      "[Training Epoch 1] Batch 3737, Loss 0.2703837454319\n",
      "[Training Epoch 1] Batch 3738, Loss 0.2747529149055481\n",
      "[Training Epoch 1] Batch 3739, Loss 0.3060607314109802\n",
      "[Training Epoch 1] Batch 3740, Loss 0.28575992584228516\n",
      "[Training Epoch 1] Batch 3741, Loss 0.2704945206642151\n",
      "[Training Epoch 1] Batch 3742, Loss 0.3092913031578064\n",
      "[Training Epoch 1] Batch 3743, Loss 0.2881312370300293\n",
      "[Training Epoch 1] Batch 3744, Loss 0.2612368166446686\n",
      "[Training Epoch 1] Batch 3745, Loss 0.2907799482345581\n",
      "[Training Epoch 1] Batch 3746, Loss 0.28662729263305664\n",
      "[Training Epoch 1] Batch 3747, Loss 0.2878727614879608\n",
      "[Training Epoch 1] Batch 3748, Loss 0.26581329107284546\n",
      "[Training Epoch 1] Batch 3749, Loss 0.3284907042980194\n",
      "[Training Epoch 1] Batch 3750, Loss 0.30573439598083496\n",
      "[Training Epoch 1] Batch 3751, Loss 0.3190610408782959\n",
      "[Training Epoch 1] Batch 3752, Loss 0.3032982349395752\n",
      "[Training Epoch 1] Batch 3753, Loss 0.28321802616119385\n",
      "[Training Epoch 1] Batch 3754, Loss 0.286527156829834\n",
      "[Training Epoch 1] Batch 3755, Loss 0.2762373685836792\n",
      "[Training Epoch 1] Batch 3756, Loss 0.28957048058509827\n",
      "[Training Epoch 1] Batch 3757, Loss 0.27271926403045654\n",
      "[Training Epoch 1] Batch 3758, Loss 0.3054239749908447\n",
      "[Training Epoch 1] Batch 3759, Loss 0.29650112986564636\n",
      "[Training Epoch 1] Batch 3760, Loss 0.2899760603904724\n",
      "[Training Epoch 1] Batch 3761, Loss 0.28833359479904175\n",
      "[Training Epoch 1] Batch 3762, Loss 0.29740121960639954\n",
      "[Training Epoch 1] Batch 3763, Loss 0.2880241274833679\n",
      "[Training Epoch 1] Batch 3764, Loss 0.3056303858757019\n",
      "[Training Epoch 1] Batch 3765, Loss 0.3085460662841797\n",
      "[Training Epoch 1] Batch 3766, Loss 0.2762347459793091\n",
      "[Training Epoch 1] Batch 3767, Loss 0.2943519651889801\n",
      "[Training Epoch 1] Batch 3768, Loss 0.2806251645088196\n",
      "[Training Epoch 1] Batch 3769, Loss 0.2954811751842499\n",
      "[Training Epoch 1] Batch 3770, Loss 0.3075483441352844\n",
      "[Training Epoch 1] Batch 3771, Loss 0.2602860927581787\n",
      "[Training Epoch 1] Batch 3772, Loss 0.27958211302757263\n",
      "[Training Epoch 1] Batch 3773, Loss 0.30447596311569214\n",
      "[Training Epoch 1] Batch 3774, Loss 0.28672969341278076\n",
      "[Training Epoch 1] Batch 3775, Loss 0.2952389717102051\n",
      "[Training Epoch 1] Batch 3776, Loss 0.3123854100704193\n",
      "[Training Epoch 1] Batch 3777, Loss 0.2785443067550659\n",
      "[Training Epoch 1] Batch 3778, Loss 0.2753152847290039\n",
      "[Training Epoch 1] Batch 3779, Loss 0.3113550543785095\n",
      "[Training Epoch 1] Batch 3780, Loss 0.28877317905426025\n",
      "[Training Epoch 1] Batch 3781, Loss 0.30299806594848633\n",
      "[Training Epoch 1] Batch 3782, Loss 0.29865562915802\n",
      "[Training Epoch 1] Batch 3783, Loss 0.2940533757209778\n",
      "[Training Epoch 1] Batch 3784, Loss 0.28178128600120544\n",
      "[Training Epoch 1] Batch 3785, Loss 0.286175400018692\n",
      "[Training Epoch 1] Batch 3786, Loss 0.28119003772735596\n",
      "[Training Epoch 1] Batch 3787, Loss 0.2875177264213562\n",
      "[Training Epoch 1] Batch 3788, Loss 0.30434873700141907\n",
      "[Training Epoch 1] Batch 3789, Loss 0.3041810393333435\n",
      "[Training Epoch 1] Batch 3790, Loss 0.3007909059524536\n",
      "[Training Epoch 1] Batch 3791, Loss 0.2992640733718872\n",
      "[Training Epoch 1] Batch 3792, Loss 0.2858526110649109\n",
      "[Training Epoch 1] Batch 3793, Loss 0.2646854519844055\n",
      "[Training Epoch 1] Batch 3794, Loss 0.27238184213638306\n",
      "[Training Epoch 1] Batch 3795, Loss 0.29938268661499023\n",
      "[Training Epoch 1] Batch 3796, Loss 0.310420423746109\n",
      "[Training Epoch 1] Batch 3797, Loss 0.3024788498878479\n",
      "[Training Epoch 1] Batch 3798, Loss 0.2846357226371765\n",
      "[Training Epoch 1] Batch 3799, Loss 0.2867647409439087\n",
      "[Training Epoch 1] Batch 3800, Loss 0.2910507321357727\n",
      "[Training Epoch 1] Batch 3801, Loss 0.24797174334526062\n",
      "[Training Epoch 1] Batch 3802, Loss 0.2741064727306366\n",
      "[Training Epoch 1] Batch 3803, Loss 0.2618362307548523\n",
      "[Training Epoch 1] Batch 3804, Loss 0.30187752842903137\n",
      "[Training Epoch 1] Batch 3805, Loss 0.27397575974464417\n",
      "[Training Epoch 1] Batch 3806, Loss 0.3100038766860962\n",
      "[Training Epoch 1] Batch 3807, Loss 0.31518515944480896\n",
      "[Training Epoch 1] Batch 3808, Loss 0.27690762281417847\n",
      "[Training Epoch 1] Batch 3809, Loss 0.3288170099258423\n",
      "[Training Epoch 1] Batch 3810, Loss 0.2734373211860657\n",
      "[Training Epoch 1] Batch 3811, Loss 0.27554088830947876\n",
      "[Training Epoch 1] Batch 3812, Loss 0.28813445568084717\n",
      "[Training Epoch 1] Batch 3813, Loss 0.2727450132369995\n",
      "[Training Epoch 1] Batch 3814, Loss 0.28583183884620667\n",
      "[Training Epoch 1] Batch 3815, Loss 0.28003013134002686\n",
      "[Training Epoch 1] Batch 3816, Loss 0.25621965527534485\n",
      "[Training Epoch 1] Batch 3817, Loss 0.2890034317970276\n",
      "[Training Epoch 1] Batch 3818, Loss 0.2738482654094696\n",
      "[Training Epoch 1] Batch 3819, Loss 0.2825999855995178\n",
      "[Training Epoch 1] Batch 3820, Loss 0.2653336226940155\n",
      "[Training Epoch 1] Batch 3821, Loss 0.29777589440345764\n",
      "[Training Epoch 1] Batch 3822, Loss 0.2756830155849457\n",
      "[Training Epoch 1] Batch 3823, Loss 0.2823234796524048\n",
      "[Training Epoch 1] Batch 3824, Loss 0.2889249324798584\n",
      "[Training Epoch 1] Batch 3825, Loss 0.30181628465652466\n",
      "[Training Epoch 1] Batch 3826, Loss 0.30969250202178955\n",
      "[Training Epoch 1] Batch 3827, Loss 0.3186491131782532\n",
      "[Training Epoch 1] Batch 3828, Loss 0.3177490830421448\n",
      "[Training Epoch 1] Batch 3829, Loss 0.2803422510623932\n",
      "[Training Epoch 1] Batch 3830, Loss 0.30841749906539917\n",
      "[Training Epoch 1] Batch 3831, Loss 0.25927913188934326\n",
      "[Training Epoch 1] Batch 3832, Loss 0.28279292583465576\n",
      "[Training Epoch 1] Batch 3833, Loss 0.28951841592788696\n",
      "[Training Epoch 1] Batch 3834, Loss 0.27415764331817627\n",
      "[Training Epoch 1] Batch 3835, Loss 0.27737361192703247\n",
      "[Training Epoch 1] Batch 3836, Loss 0.28673112392425537\n",
      "[Training Epoch 1] Batch 3837, Loss 0.30039411783218384\n",
      "[Training Epoch 1] Batch 3838, Loss 0.296718955039978\n",
      "[Training Epoch 1] Batch 3839, Loss 0.32600897550582886\n",
      "[Training Epoch 1] Batch 3840, Loss 0.2836535573005676\n",
      "[Training Epoch 1] Batch 3841, Loss 0.31060105562210083\n",
      "[Training Epoch 1] Batch 3842, Loss 0.2801204323768616\n",
      "[Training Epoch 1] Batch 3843, Loss 0.28954145312309265\n",
      "[Training Epoch 1] Batch 3844, Loss 0.335540771484375\n",
      "[Training Epoch 1] Batch 3845, Loss 0.2929481267929077\n",
      "[Training Epoch 1] Batch 3846, Loss 0.303340882062912\n",
      "[Training Epoch 1] Batch 3847, Loss 0.2789105176925659\n",
      "[Training Epoch 1] Batch 3848, Loss 0.2986072897911072\n",
      "[Training Epoch 1] Batch 3849, Loss 0.32917892932891846\n",
      "[Training Epoch 1] Batch 3850, Loss 0.2975396513938904\n",
      "[Training Epoch 1] Batch 3851, Loss 0.29456329345703125\n",
      "[Training Epoch 1] Batch 3852, Loss 0.2605026960372925\n",
      "[Training Epoch 1] Batch 3853, Loss 0.2839415669441223\n",
      "[Training Epoch 1] Batch 3854, Loss 0.3144288659095764\n",
      "[Training Epoch 1] Batch 3855, Loss 0.31522393226623535\n",
      "[Training Epoch 1] Batch 3856, Loss 0.31120985746383667\n",
      "[Training Epoch 1] Batch 3857, Loss 0.300021231174469\n",
      "[Training Epoch 1] Batch 3858, Loss 0.30342912673950195\n",
      "[Training Epoch 1] Batch 3859, Loss 0.2972702980041504\n",
      "[Training Epoch 1] Batch 3860, Loss 0.28031688928604126\n",
      "[Training Epoch 1] Batch 3861, Loss 0.2774202823638916\n",
      "[Training Epoch 1] Batch 3862, Loss 0.3026496171951294\n",
      "[Training Epoch 1] Batch 3863, Loss 0.2781679034233093\n",
      "[Training Epoch 1] Batch 3864, Loss 0.2807324528694153\n",
      "[Training Epoch 1] Batch 3865, Loss 0.3185875415802002\n",
      "[Training Epoch 1] Batch 3866, Loss 0.29677364230155945\n",
      "[Training Epoch 1] Batch 3867, Loss 0.2875061631202698\n",
      "[Training Epoch 1] Batch 3868, Loss 0.28165528178215027\n",
      "[Training Epoch 1] Batch 3869, Loss 0.2937988042831421\n",
      "[Training Epoch 1] Batch 3870, Loss 0.3091004490852356\n",
      "[Training Epoch 1] Batch 3871, Loss 0.2944277822971344\n",
      "[Training Epoch 1] Batch 3872, Loss 0.27116161584854126\n",
      "[Training Epoch 1] Batch 3873, Loss 0.3016536831855774\n",
      "[Training Epoch 1] Batch 3874, Loss 0.27474910020828247\n",
      "[Training Epoch 1] Batch 3875, Loss 0.3005595803260803\n",
      "[Training Epoch 1] Batch 3876, Loss 0.2829967737197876\n",
      "[Training Epoch 1] Batch 3877, Loss 0.29085806012153625\n",
      "[Training Epoch 1] Batch 3878, Loss 0.29016923904418945\n",
      "[Training Epoch 1] Batch 3879, Loss 0.29684534668922424\n",
      "[Training Epoch 1] Batch 3880, Loss 0.24872158467769623\n",
      "[Training Epoch 1] Batch 3881, Loss 0.3048538267612457\n",
      "[Training Epoch 1] Batch 3882, Loss 0.28738629817962646\n",
      "[Training Epoch 1] Batch 3883, Loss 0.28238779306411743\n",
      "[Training Epoch 1] Batch 3884, Loss 0.26907867193222046\n",
      "[Training Epoch 1] Batch 3885, Loss 0.3117750585079193\n",
      "[Training Epoch 1] Batch 3886, Loss 0.3017922043800354\n",
      "[Training Epoch 1] Batch 3887, Loss 0.2826727330684662\n",
      "[Training Epoch 1] Batch 3888, Loss 0.30959033966064453\n",
      "[Training Epoch 1] Batch 3889, Loss 0.30192479491233826\n",
      "[Training Epoch 1] Batch 3890, Loss 0.2815327048301697\n",
      "[Training Epoch 1] Batch 3891, Loss 0.2586401700973511\n",
      "[Training Epoch 1] Batch 3892, Loss 0.2796785533428192\n",
      "[Training Epoch 1] Batch 3893, Loss 0.287889689207077\n",
      "[Training Epoch 1] Batch 3894, Loss 0.30427777767181396\n",
      "[Training Epoch 1] Batch 3895, Loss 0.27964454889297485\n",
      "[Training Epoch 1] Batch 3896, Loss 0.28740257024765015\n",
      "[Training Epoch 1] Batch 3897, Loss 0.31635501980781555\n",
      "[Training Epoch 1] Batch 3898, Loss 0.30622416734695435\n",
      "[Training Epoch 1] Batch 3899, Loss 0.3302021026611328\n",
      "[Training Epoch 1] Batch 3900, Loss 0.33123621344566345\n",
      "[Training Epoch 1] Batch 3901, Loss 0.29760050773620605\n",
      "[Training Epoch 1] Batch 3902, Loss 0.3015894889831543\n",
      "[Training Epoch 1] Batch 3903, Loss 0.28927189111709595\n",
      "[Training Epoch 1] Batch 3904, Loss 0.3154611587524414\n",
      "[Training Epoch 1] Batch 3905, Loss 0.27619439363479614\n",
      "[Training Epoch 1] Batch 3906, Loss 0.2713432312011719\n",
      "[Training Epoch 1] Batch 3907, Loss 0.2904374897480011\n",
      "[Training Epoch 1] Batch 3908, Loss 0.27812260389328003\n",
      "[Training Epoch 1] Batch 3909, Loss 0.2965853810310364\n",
      "[Training Epoch 1] Batch 3910, Loss 0.27407580614089966\n",
      "[Training Epoch 1] Batch 3911, Loss 0.3123626708984375\n",
      "[Training Epoch 1] Batch 3912, Loss 0.30452975630760193\n",
      "[Training Epoch 1] Batch 3913, Loss 0.26841872930526733\n",
      "[Training Epoch 1] Batch 3914, Loss 0.29747912287712097\n",
      "[Training Epoch 1] Batch 3915, Loss 0.2750850319862366\n",
      "[Training Epoch 1] Batch 3916, Loss 0.2944257855415344\n",
      "[Training Epoch 1] Batch 3917, Loss 0.28855031728744507\n",
      "[Training Epoch 1] Batch 3918, Loss 0.3002370595932007\n",
      "[Training Epoch 1] Batch 3919, Loss 0.29929983615875244\n",
      "[Training Epoch 1] Batch 3920, Loss 0.2786886990070343\n",
      "[Training Epoch 1] Batch 3921, Loss 0.2842855453491211\n",
      "[Training Epoch 1] Batch 3922, Loss 0.3139060437679291\n",
      "[Training Epoch 1] Batch 3923, Loss 0.29387998580932617\n",
      "[Training Epoch 1] Batch 3924, Loss 0.28921884298324585\n",
      "[Training Epoch 1] Batch 3925, Loss 0.3150978088378906\n",
      "[Training Epoch 1] Batch 3926, Loss 0.2727832496166229\n",
      "[Training Epoch 1] Batch 3927, Loss 0.3005167543888092\n",
      "[Training Epoch 1] Batch 3928, Loss 0.3011045455932617\n",
      "[Training Epoch 1] Batch 3929, Loss 0.2835325598716736\n",
      "[Training Epoch 1] Batch 3930, Loss 0.301258385181427\n",
      "[Training Epoch 1] Batch 3931, Loss 0.279674232006073\n",
      "[Training Epoch 1] Batch 3932, Loss 0.3070862591266632\n",
      "[Training Epoch 1] Batch 3933, Loss 0.28328612446784973\n",
      "[Training Epoch 1] Batch 3934, Loss 0.3040521442890167\n",
      "[Training Epoch 1] Batch 3935, Loss 0.26204049587249756\n",
      "[Training Epoch 1] Batch 3936, Loss 0.2972792387008667\n",
      "[Training Epoch 1] Batch 3937, Loss 0.2869240641593933\n",
      "[Training Epoch 1] Batch 3938, Loss 0.27541232109069824\n",
      "[Training Epoch 1] Batch 3939, Loss 0.30898165702819824\n",
      "[Training Epoch 1] Batch 3940, Loss 0.2798938751220703\n",
      "[Training Epoch 1] Batch 3941, Loss 0.28531917929649353\n",
      "[Training Epoch 1] Batch 3942, Loss 0.2999148368835449\n",
      "[Training Epoch 1] Batch 3943, Loss 0.28749799728393555\n",
      "[Training Epoch 1] Batch 3944, Loss 0.25521355867385864\n",
      "[Training Epoch 1] Batch 3945, Loss 0.3136407136917114\n",
      "[Training Epoch 1] Batch 3946, Loss 0.28051936626434326\n",
      "[Training Epoch 1] Batch 3947, Loss 0.2630428075790405\n",
      "[Training Epoch 1] Batch 3948, Loss 0.2911584675312042\n",
      "[Training Epoch 1] Batch 3949, Loss 0.3172001242637634\n",
      "[Training Epoch 1] Batch 3950, Loss 0.27994152903556824\n",
      "[Training Epoch 1] Batch 3951, Loss 0.2803632616996765\n",
      "[Training Epoch 1] Batch 3952, Loss 0.2919875979423523\n",
      "[Training Epoch 1] Batch 3953, Loss 0.2713496685028076\n",
      "[Training Epoch 1] Batch 3954, Loss 0.2814145088195801\n",
      "[Training Epoch 1] Batch 3955, Loss 0.32286393642425537\n",
      "[Training Epoch 1] Batch 3956, Loss 0.3152548372745514\n",
      "[Training Epoch 1] Batch 3957, Loss 0.2851839065551758\n",
      "[Training Epoch 1] Batch 3958, Loss 0.29726457595825195\n",
      "[Training Epoch 1] Batch 3959, Loss 0.284851610660553\n",
      "[Training Epoch 1] Batch 3960, Loss 0.2822062373161316\n",
      "[Training Epoch 1] Batch 3961, Loss 0.26590457558631897\n",
      "[Training Epoch 1] Batch 3962, Loss 0.2973285913467407\n",
      "[Training Epoch 1] Batch 3963, Loss 0.28211700916290283\n",
      "[Training Epoch 1] Batch 3964, Loss 0.320112019777298\n",
      "[Training Epoch 1] Batch 3965, Loss 0.3077228367328644\n",
      "[Training Epoch 1] Batch 3966, Loss 0.2848399877548218\n",
      "[Training Epoch 1] Batch 3967, Loss 0.2933606803417206\n",
      "[Training Epoch 1] Batch 3968, Loss 0.30521947145462036\n",
      "[Training Epoch 1] Batch 3969, Loss 0.2881295680999756\n",
      "[Training Epoch 1] Batch 3970, Loss 0.30142509937286377\n",
      "[Training Epoch 1] Batch 3971, Loss 0.2907109260559082\n",
      "[Training Epoch 1] Batch 3972, Loss 0.31472235918045044\n",
      "[Training Epoch 1] Batch 3973, Loss 0.25073838233947754\n",
      "[Training Epoch 1] Batch 3974, Loss 0.29136550426483154\n",
      "[Training Epoch 1] Batch 3975, Loss 0.28048962354660034\n",
      "[Training Epoch 1] Batch 3976, Loss 0.2883806526660919\n",
      "[Training Epoch 1] Batch 3977, Loss 0.2830280363559723\n",
      "[Training Epoch 1] Batch 3978, Loss 0.3066386580467224\n",
      "[Training Epoch 1] Batch 3979, Loss 0.3092228174209595\n",
      "[Training Epoch 1] Batch 3980, Loss 0.3066927492618561\n",
      "[Training Epoch 1] Batch 3981, Loss 0.2780395746231079\n",
      "[Training Epoch 1] Batch 3982, Loss 0.2855392098426819\n",
      "[Training Epoch 1] Batch 3983, Loss 0.2752612233161926\n",
      "[Training Epoch 1] Batch 3984, Loss 0.28371450304985046\n",
      "[Training Epoch 1] Batch 3985, Loss 0.28395217657089233\n",
      "[Training Epoch 1] Batch 3986, Loss 0.29330968856811523\n",
      "[Training Epoch 1] Batch 3987, Loss 0.2988066077232361\n",
      "[Training Epoch 1] Batch 3988, Loss 0.29621678590774536\n",
      "[Training Epoch 1] Batch 3989, Loss 0.30962681770324707\n",
      "[Training Epoch 1] Batch 3990, Loss 0.3050147294998169\n",
      "[Training Epoch 1] Batch 3991, Loss 0.27031007409095764\n",
      "[Training Epoch 1] Batch 3992, Loss 0.29749125242233276\n",
      "[Training Epoch 1] Batch 3993, Loss 0.2965581715106964\n",
      "[Training Epoch 1] Batch 3994, Loss 0.2818893492221832\n",
      "[Training Epoch 1] Batch 3995, Loss 0.29567950963974\n",
      "[Training Epoch 1] Batch 3996, Loss 0.2751257121562958\n",
      "[Training Epoch 1] Batch 3997, Loss 0.2982802987098694\n",
      "[Training Epoch 1] Batch 3998, Loss 0.2939645051956177\n",
      "[Training Epoch 1] Batch 3999, Loss 0.2961203455924988\n",
      "[Training Epoch 1] Batch 4000, Loss 0.3063034117221832\n",
      "[Training Epoch 1] Batch 4001, Loss 0.2884325385093689\n",
      "[Training Epoch 1] Batch 4002, Loss 0.29645615816116333\n",
      "[Training Epoch 1] Batch 4003, Loss 0.28126662969589233\n",
      "[Training Epoch 1] Batch 4004, Loss 0.2669385075569153\n",
      "[Training Epoch 1] Batch 4005, Loss 0.32273316383361816\n",
      "[Training Epoch 1] Batch 4006, Loss 0.3059467077255249\n",
      "[Training Epoch 1] Batch 4007, Loss 0.2762455940246582\n",
      "[Training Epoch 1] Batch 4008, Loss 0.28249800205230713\n",
      "[Training Epoch 1] Batch 4009, Loss 0.30722281336784363\n",
      "[Training Epoch 1] Batch 4010, Loss 0.3183887302875519\n",
      "[Training Epoch 1] Batch 4011, Loss 0.303846150636673\n",
      "[Training Epoch 1] Batch 4012, Loss 0.27184635400772095\n",
      "[Training Epoch 1] Batch 4013, Loss 0.2852969765663147\n",
      "[Training Epoch 1] Batch 4014, Loss 0.2865428626537323\n",
      "[Training Epoch 1] Batch 4015, Loss 0.27700749039649963\n",
      "[Training Epoch 1] Batch 4016, Loss 0.2666192352771759\n",
      "[Training Epoch 1] Batch 4017, Loss 0.281677782535553\n",
      "[Training Epoch 1] Batch 4018, Loss 0.26700806617736816\n",
      "[Training Epoch 1] Batch 4019, Loss 0.3012555241584778\n",
      "[Training Epoch 1] Batch 4020, Loss 0.2642795145511627\n",
      "[Training Epoch 1] Batch 4021, Loss 0.29277026653289795\n",
      "[Training Epoch 1] Batch 4022, Loss 0.26334822177886963\n",
      "[Training Epoch 1] Batch 4023, Loss 0.29398781061172485\n",
      "[Training Epoch 1] Batch 4024, Loss 0.290217787027359\n",
      "[Training Epoch 1] Batch 4025, Loss 0.25930243730545044\n",
      "[Training Epoch 1] Batch 4026, Loss 0.27798235416412354\n",
      "[Training Epoch 1] Batch 4027, Loss 0.26832115650177\n",
      "[Training Epoch 1] Batch 4028, Loss 0.294731080532074\n",
      "[Training Epoch 1] Batch 4029, Loss 0.2878868579864502\n",
      "[Training Epoch 1] Batch 4030, Loss 0.27011680603027344\n",
      "[Training Epoch 1] Batch 4031, Loss 0.27278923988342285\n",
      "[Training Epoch 1] Batch 4032, Loss 0.2941496968269348\n",
      "[Training Epoch 1] Batch 4033, Loss 0.3165604770183563\n",
      "[Training Epoch 1] Batch 4034, Loss 0.3094150424003601\n",
      "[Training Epoch 1] Batch 4035, Loss 0.29333382844924927\n",
      "[Training Epoch 1] Batch 4036, Loss 0.2830229103565216\n",
      "[Training Epoch 1] Batch 4037, Loss 0.31184089183807373\n",
      "[Training Epoch 1] Batch 4038, Loss 0.30362021923065186\n",
      "[Training Epoch 1] Batch 4039, Loss 0.29176247119903564\n",
      "[Training Epoch 1] Batch 4040, Loss 0.31259918212890625\n",
      "[Training Epoch 1] Batch 4041, Loss 0.29158711433410645\n",
      "[Training Epoch 1] Batch 4042, Loss 0.31581151485443115\n",
      "[Training Epoch 1] Batch 4043, Loss 0.3151199221611023\n",
      "[Training Epoch 1] Batch 4044, Loss 0.31637316942214966\n",
      "[Training Epoch 1] Batch 4045, Loss 0.29670488834381104\n",
      "[Training Epoch 1] Batch 4046, Loss 0.27346271276474\n",
      "[Training Epoch 1] Batch 4047, Loss 0.28635311126708984\n",
      "[Training Epoch 1] Batch 4048, Loss 0.27164316177368164\n",
      "[Training Epoch 1] Batch 4049, Loss 0.2706890106201172\n",
      "[Training Epoch 1] Batch 4050, Loss 0.2640131711959839\n",
      "[Training Epoch 1] Batch 4051, Loss 0.28013479709625244\n",
      "[Training Epoch 1] Batch 4052, Loss 0.2758067846298218\n",
      "[Training Epoch 1] Batch 4053, Loss 0.27938735485076904\n",
      "[Training Epoch 1] Batch 4054, Loss 0.31182342767715454\n",
      "[Training Epoch 1] Batch 4055, Loss 0.3144921064376831\n",
      "[Training Epoch 1] Batch 4056, Loss 0.2914038896560669\n",
      "[Training Epoch 1] Batch 4057, Loss 0.2923152446746826\n",
      "[Training Epoch 1] Batch 4058, Loss 0.3296661078929901\n",
      "[Training Epoch 1] Batch 4059, Loss 0.2682262063026428\n",
      "[Training Epoch 1] Batch 4060, Loss 0.27583855390548706\n",
      "[Training Epoch 1] Batch 4061, Loss 0.3050553798675537\n",
      "[Training Epoch 1] Batch 4062, Loss 0.27648913860321045\n",
      "[Training Epoch 1] Batch 4063, Loss 0.29208552837371826\n",
      "[Training Epoch 1] Batch 4064, Loss 0.29325205087661743\n",
      "[Training Epoch 1] Batch 4065, Loss 0.2987147271633148\n",
      "[Training Epoch 1] Batch 4066, Loss 0.2791702151298523\n",
      "[Training Epoch 1] Batch 4067, Loss 0.2802165150642395\n",
      "[Training Epoch 1] Batch 4068, Loss 0.28995364904403687\n",
      "[Training Epoch 1] Batch 4069, Loss 0.27372264862060547\n",
      "[Training Epoch 1] Batch 4070, Loss 0.3143552541732788\n",
      "[Training Epoch 1] Batch 4071, Loss 0.2833362817764282\n",
      "[Training Epoch 1] Batch 4072, Loss 0.29571348428726196\n",
      "[Training Epoch 1] Batch 4073, Loss 0.30172184109687805\n",
      "[Training Epoch 1] Batch 4074, Loss 0.3078349828720093\n",
      "[Training Epoch 1] Batch 4075, Loss 0.29499924182891846\n",
      "[Training Epoch 1] Batch 4076, Loss 0.31176507472991943\n",
      "[Training Epoch 1] Batch 4077, Loss 0.2959147095680237\n",
      "[Training Epoch 1] Batch 4078, Loss 0.25842392444610596\n",
      "[Training Epoch 1] Batch 4079, Loss 0.26263463497161865\n",
      "[Training Epoch 1] Batch 4080, Loss 0.295566201210022\n",
      "[Training Epoch 1] Batch 4081, Loss 0.29444780945777893\n",
      "[Training Epoch 1] Batch 4082, Loss 0.2711864411830902\n",
      "[Training Epoch 1] Batch 4083, Loss 0.2961612343788147\n",
      "[Training Epoch 1] Batch 4084, Loss 0.2948882579803467\n",
      "[Training Epoch 1] Batch 4085, Loss 0.2761826813220978\n",
      "[Training Epoch 1] Batch 4086, Loss 0.2906181514263153\n",
      "[Training Epoch 1] Batch 4087, Loss 0.2999360263347626\n",
      "[Training Epoch 1] Batch 4088, Loss 0.3000127971172333\n",
      "[Training Epoch 1] Batch 4089, Loss 0.2828417420387268\n",
      "[Training Epoch 1] Batch 4090, Loss 0.3068743050098419\n",
      "[Training Epoch 1] Batch 4091, Loss 0.2701001763343811\n",
      "[Training Epoch 1] Batch 4092, Loss 0.2560541033744812\n",
      "[Training Epoch 1] Batch 4093, Loss 0.28624606132507324\n",
      "[Training Epoch 1] Batch 4094, Loss 0.3041602373123169\n",
      "[Training Epoch 1] Batch 4095, Loss 0.2710292935371399\n",
      "[Training Epoch 1] Batch 4096, Loss 0.30279409885406494\n",
      "[Training Epoch 1] Batch 4097, Loss 0.28206866979599\n",
      "[Training Epoch 1] Batch 4098, Loss 0.26490816473960876\n",
      "[Training Epoch 1] Batch 4099, Loss 0.3031248450279236\n",
      "[Training Epoch 1] Batch 4100, Loss 0.2990207076072693\n",
      "[Training Epoch 1] Batch 4101, Loss 0.27009665966033936\n",
      "[Training Epoch 1] Batch 4102, Loss 0.2930734157562256\n",
      "[Training Epoch 1] Batch 4103, Loss 0.29635217785835266\n",
      "[Training Epoch 1] Batch 4104, Loss 0.33445101976394653\n",
      "[Training Epoch 1] Batch 4105, Loss 0.28748881816864014\n",
      "[Training Epoch 1] Batch 4106, Loss 0.2898629307746887\n",
      "[Training Epoch 1] Batch 4107, Loss 0.2893517315387726\n",
      "[Training Epoch 1] Batch 4108, Loss 0.3121333122253418\n",
      "[Training Epoch 1] Batch 4109, Loss 0.2685416340827942\n",
      "[Training Epoch 1] Batch 4110, Loss 0.29370608925819397\n",
      "[Training Epoch 1] Batch 4111, Loss 0.32061830163002014\n",
      "[Training Epoch 1] Batch 4112, Loss 0.28764867782592773\n",
      "[Training Epoch 1] Batch 4113, Loss 0.28062015771865845\n",
      "[Training Epoch 1] Batch 4114, Loss 0.2763153910636902\n",
      "[Training Epoch 1] Batch 4115, Loss 0.31026238203048706\n",
      "[Training Epoch 1] Batch 4116, Loss 0.2853369116783142\n",
      "[Training Epoch 1] Batch 4117, Loss 0.26125508546829224\n",
      "[Training Epoch 1] Batch 4118, Loss 0.30333366990089417\n",
      "[Training Epoch 1] Batch 4119, Loss 0.28532710671424866\n",
      "[Training Epoch 1] Batch 4120, Loss 0.282164603471756\n",
      "[Training Epoch 1] Batch 4121, Loss 0.28533801436424255\n",
      "[Training Epoch 1] Batch 4122, Loss 0.30163702368736267\n",
      "[Training Epoch 1] Batch 4123, Loss 0.2914266586303711\n",
      "[Training Epoch 1] Batch 4124, Loss 0.30375760793685913\n",
      "[Training Epoch 1] Batch 4125, Loss 0.29648837447166443\n",
      "[Training Epoch 1] Batch 4126, Loss 0.30629193782806396\n",
      "[Training Epoch 1] Batch 4127, Loss 0.28101226687431335\n",
      "[Training Epoch 1] Batch 4128, Loss 0.29521283507347107\n",
      "[Training Epoch 1] Batch 4129, Loss 0.30397483706474304\n",
      "[Training Epoch 1] Batch 4130, Loss 0.29918715357780457\n",
      "[Training Epoch 1] Batch 4131, Loss 0.3150942623615265\n",
      "[Training Epoch 1] Batch 4132, Loss 0.2914885878562927\n",
      "[Training Epoch 1] Batch 4133, Loss 0.2742422819137573\n",
      "[Training Epoch 1] Batch 4134, Loss 0.2691878080368042\n",
      "[Training Epoch 1] Batch 4135, Loss 0.283182293176651\n",
      "[Training Epoch 1] Batch 4136, Loss 0.29064804315567017\n",
      "[Training Epoch 1] Batch 4137, Loss 0.26776471734046936\n",
      "[Training Epoch 1] Batch 4138, Loss 0.2811028063297272\n",
      "[Training Epoch 1] Batch 4139, Loss 0.2911865711212158\n",
      "[Training Epoch 1] Batch 4140, Loss 0.275446355342865\n",
      "[Training Epoch 1] Batch 4141, Loss 0.30668771266937256\n",
      "[Training Epoch 1] Batch 4142, Loss 0.3116883635520935\n",
      "[Training Epoch 1] Batch 4143, Loss 0.2891644239425659\n",
      "[Training Epoch 1] Batch 4144, Loss 0.2844378352165222\n",
      "[Training Epoch 1] Batch 4145, Loss 0.2748107314109802\n",
      "[Training Epoch 1] Batch 4146, Loss 0.2690388560295105\n",
      "[Training Epoch 1] Batch 4147, Loss 0.2691705822944641\n",
      "[Training Epoch 1] Batch 4148, Loss 0.28669503331184387\n",
      "[Training Epoch 1] Batch 4149, Loss 0.2916051745414734\n",
      "[Training Epoch 1] Batch 4150, Loss 0.28087520599365234\n",
      "[Training Epoch 1] Batch 4151, Loss 0.2660077214241028\n",
      "[Training Epoch 1] Batch 4152, Loss 0.27289682626724243\n",
      "[Training Epoch 1] Batch 4153, Loss 0.28047120571136475\n",
      "[Training Epoch 1] Batch 4154, Loss 0.2813223600387573\n",
      "[Training Epoch 1] Batch 4155, Loss 0.2774096131324768\n",
      "[Training Epoch 1] Batch 4156, Loss 0.27639028429985046\n",
      "[Training Epoch 1] Batch 4157, Loss 0.29662269353866577\n",
      "[Training Epoch 1] Batch 4158, Loss 0.28294652700424194\n",
      "[Training Epoch 1] Batch 4159, Loss 0.2796422839164734\n",
      "[Training Epoch 1] Batch 4160, Loss 0.29429954290390015\n",
      "[Training Epoch 1] Batch 4161, Loss 0.26746559143066406\n",
      "[Training Epoch 1] Batch 4162, Loss 0.29504454135894775\n",
      "[Training Epoch 1] Batch 4163, Loss 0.27488279342651367\n",
      "[Training Epoch 1] Batch 4164, Loss 0.2686251997947693\n",
      "[Training Epoch 1] Batch 4165, Loss 0.3084573745727539\n",
      "[Training Epoch 1] Batch 4166, Loss 0.2874927818775177\n",
      "[Training Epoch 1] Batch 4167, Loss 0.30139559507369995\n",
      "[Training Epoch 1] Batch 4168, Loss 0.29076504707336426\n",
      "[Training Epoch 1] Batch 4169, Loss 0.302786648273468\n",
      "[Training Epoch 1] Batch 4170, Loss 0.31362026929855347\n",
      "[Training Epoch 1] Batch 4171, Loss 0.2879541218280792\n",
      "[Training Epoch 1] Batch 4172, Loss 0.31558430194854736\n",
      "[Training Epoch 1] Batch 4173, Loss 0.2838948369026184\n",
      "[Training Epoch 1] Batch 4174, Loss 0.27592962980270386\n",
      "[Training Epoch 1] Batch 4175, Loss 0.30666500329971313\n",
      "[Training Epoch 1] Batch 4176, Loss 0.27847856283187866\n",
      "[Training Epoch 1] Batch 4177, Loss 0.29041966795921326\n",
      "[Training Epoch 1] Batch 4178, Loss 0.28995126485824585\n",
      "[Training Epoch 1] Batch 4179, Loss 0.30360209941864014\n",
      "[Training Epoch 1] Batch 4180, Loss 0.2746307849884033\n",
      "[Training Epoch 1] Batch 4181, Loss 0.2747150659561157\n",
      "[Training Epoch 1] Batch 4182, Loss 0.2635493874549866\n",
      "[Training Epoch 1] Batch 4183, Loss 0.31249046325683594\n",
      "[Training Epoch 1] Batch 4184, Loss 0.24258017539978027\n",
      "[Training Epoch 1] Batch 4185, Loss 0.31005311012268066\n",
      "[Training Epoch 1] Batch 4186, Loss 0.28735673427581787\n",
      "[Training Epoch 1] Batch 4187, Loss 0.2899763584136963\n",
      "[Training Epoch 1] Batch 4188, Loss 0.26916277408599854\n",
      "[Training Epoch 1] Batch 4189, Loss 0.2892555296421051\n",
      "[Training Epoch 1] Batch 4190, Loss 0.2851047217845917\n",
      "[Training Epoch 1] Batch 4191, Loss 0.30160343647003174\n",
      "[Training Epoch 1] Batch 4192, Loss 0.2806605100631714\n",
      "[Training Epoch 1] Batch 4193, Loss 0.2811736464500427\n",
      "[Training Epoch 1] Batch 4194, Loss 0.26044803857803345\n",
      "[Training Epoch 1] Batch 4195, Loss 0.28100210428237915\n",
      "[Training Epoch 1] Batch 4196, Loss 0.26108452677726746\n",
      "[Training Epoch 1] Batch 4197, Loss 0.2727888524532318\n",
      "[Training Epoch 1] Batch 4198, Loss 0.28353914618492126\n",
      "[Training Epoch 1] Batch 4199, Loss 0.26684847474098206\n",
      "[Training Epoch 1] Batch 4200, Loss 0.2756049931049347\n",
      "[Training Epoch 1] Batch 4201, Loss 0.3047667443752289\n",
      "[Training Epoch 1] Batch 4202, Loss 0.31116965413093567\n",
      "[Training Epoch 1] Batch 4203, Loss 0.31752443313598633\n",
      "[Training Epoch 1] Batch 4204, Loss 0.271287739276886\n",
      "[Training Epoch 1] Batch 4205, Loss 0.23802584409713745\n",
      "[Training Epoch 1] Batch 4206, Loss 0.2800719439983368\n",
      "[Training Epoch 1] Batch 4207, Loss 0.29383909702301025\n",
      "[Training Epoch 1] Batch 4208, Loss 0.2801046073436737\n",
      "[Training Epoch 1] Batch 4209, Loss 0.28238004446029663\n",
      "[Training Epoch 1] Batch 4210, Loss 0.2936238646507263\n",
      "[Training Epoch 1] Batch 4211, Loss 0.258841872215271\n",
      "[Training Epoch 1] Batch 4212, Loss 0.3015390932559967\n",
      "[Training Epoch 1] Batch 4213, Loss 0.29892536997795105\n",
      "[Training Epoch 1] Batch 4214, Loss 0.3037765920162201\n",
      "[Training Epoch 1] Batch 4215, Loss 0.28539255261421204\n",
      "[Training Epoch 1] Batch 4216, Loss 0.290985643863678\n",
      "[Training Epoch 1] Batch 4217, Loss 0.27942731976509094\n",
      "[Training Epoch 1] Batch 4218, Loss 0.3027019798755646\n",
      "[Training Epoch 1] Batch 4219, Loss 0.317546546459198\n",
      "[Training Epoch 1] Batch 4220, Loss 0.27230241894721985\n",
      "[Training Epoch 1] Batch 4221, Loss 0.2754839062690735\n",
      "[Training Epoch 1] Batch 4222, Loss 0.29131829738616943\n",
      "[Training Epoch 1] Batch 4223, Loss 0.2841586172580719\n",
      "[Training Epoch 1] Batch 4224, Loss 0.28951311111450195\n",
      "[Training Epoch 1] Batch 4225, Loss 0.30356475710868835\n",
      "[Training Epoch 1] Batch 4226, Loss 0.30094194412231445\n",
      "[Training Epoch 1] Batch 4227, Loss 0.3170321583747864\n",
      "[Training Epoch 1] Batch 4228, Loss 0.3277824819087982\n",
      "[Training Epoch 1] Batch 4229, Loss 0.2939728796482086\n",
      "[Training Epoch 1] Batch 4230, Loss 0.2718811631202698\n",
      "[Training Epoch 1] Batch 4231, Loss 0.2868361473083496\n",
      "[Training Epoch 1] Batch 4232, Loss 0.265902042388916\n",
      "[Training Epoch 1] Batch 4233, Loss 0.27552857995033264\n",
      "[Training Epoch 1] Batch 4234, Loss 0.30663880705833435\n",
      "[Training Epoch 1] Batch 4235, Loss 0.2706703841686249\n",
      "[Training Epoch 1] Batch 4236, Loss 0.3030354082584381\n",
      "[Training Epoch 1] Batch 4237, Loss 0.29706841707229614\n",
      "[Training Epoch 1] Batch 4238, Loss 0.2811742424964905\n",
      "[Training Epoch 1] Batch 4239, Loss 0.29919376969337463\n",
      "[Training Epoch 1] Batch 4240, Loss 0.322320818901062\n",
      "[Training Epoch 1] Batch 4241, Loss 0.298647403717041\n",
      "[Training Epoch 1] Batch 4242, Loss 0.2950180172920227\n",
      "[Training Epoch 1] Batch 4243, Loss 0.2798113524913788\n",
      "[Training Epoch 1] Batch 4244, Loss 0.3006512224674225\n",
      "[Training Epoch 1] Batch 4245, Loss 0.274761825799942\n",
      "[Training Epoch 1] Batch 4246, Loss 0.2774624526500702\n",
      "[Training Epoch 1] Batch 4247, Loss 0.27426761388778687\n",
      "[Training Epoch 1] Batch 4248, Loss 0.2984069585800171\n",
      "[Training Epoch 1] Batch 4249, Loss 0.3090527057647705\n",
      "[Training Epoch 1] Batch 4250, Loss 0.2803893983364105\n",
      "[Training Epoch 1] Batch 4251, Loss 0.28456974029541016\n",
      "[Training Epoch 1] Batch 4252, Loss 0.2826049327850342\n",
      "[Training Epoch 1] Batch 4253, Loss 0.2718822658061981\n",
      "[Training Epoch 1] Batch 4254, Loss 0.29686886072158813\n",
      "[Training Epoch 1] Batch 4255, Loss 0.2540275454521179\n",
      "[Training Epoch 1] Batch 4256, Loss 0.2898860573768616\n",
      "[Training Epoch 1] Batch 4257, Loss 0.3103437125682831\n",
      "[Training Epoch 1] Batch 4258, Loss 0.26819461584091187\n",
      "[Training Epoch 1] Batch 4259, Loss 0.2823501229286194\n",
      "[Training Epoch 1] Batch 4260, Loss 0.29102474451065063\n",
      "[Training Epoch 1] Batch 4261, Loss 0.2752511501312256\n",
      "[Training Epoch 1] Batch 4262, Loss 0.28671130537986755\n",
      "[Training Epoch 1] Batch 4263, Loss 0.27955731749534607\n",
      "[Training Epoch 1] Batch 4264, Loss 0.3108486533164978\n",
      "[Training Epoch 1] Batch 4265, Loss 0.30769646167755127\n",
      "[Training Epoch 1] Batch 4266, Loss 0.28962308168411255\n",
      "[Training Epoch 1] Batch 4267, Loss 0.29124218225479126\n",
      "[Training Epoch 1] Batch 4268, Loss 0.2900027632713318\n",
      "[Training Epoch 1] Batch 4269, Loss 0.262323796749115\n",
      "[Training Epoch 1] Batch 4270, Loss 0.3183037340641022\n",
      "[Training Epoch 1] Batch 4271, Loss 0.2862871289253235\n",
      "[Training Epoch 1] Batch 4272, Loss 0.3074618875980377\n",
      "[Training Epoch 1] Batch 4273, Loss 0.25177597999572754\n",
      "[Training Epoch 1] Batch 4274, Loss 0.2906430959701538\n",
      "[Training Epoch 1] Batch 4275, Loss 0.3061438798904419\n",
      "[Training Epoch 1] Batch 4276, Loss 0.28311824798583984\n",
      "[Training Epoch 1] Batch 4277, Loss 0.2739427089691162\n",
      "[Training Epoch 1] Batch 4278, Loss 0.28481894731521606\n",
      "[Training Epoch 1] Batch 4279, Loss 0.29336538910865784\n",
      "[Training Epoch 1] Batch 4280, Loss 0.2864978313446045\n",
      "[Training Epoch 1] Batch 4281, Loss 0.2792850136756897\n",
      "[Training Epoch 1] Batch 4282, Loss 0.2888614535331726\n",
      "[Training Epoch 1] Batch 4283, Loss 0.30538132786750793\n",
      "[Training Epoch 1] Batch 4284, Loss 0.2859468460083008\n",
      "[Training Epoch 1] Batch 4285, Loss 0.2936694025993347\n",
      "[Training Epoch 1] Batch 4286, Loss 0.2906227707862854\n",
      "[Training Epoch 1] Batch 4287, Loss 0.2605448365211487\n",
      "[Training Epoch 1] Batch 4288, Loss 0.2724211812019348\n",
      "[Training Epoch 1] Batch 4289, Loss 0.2750062942504883\n",
      "[Training Epoch 1] Batch 4290, Loss 0.29258906841278076\n",
      "[Training Epoch 1] Batch 4291, Loss 0.3252367079257965\n",
      "[Training Epoch 1] Batch 4292, Loss 0.2804095149040222\n",
      "[Training Epoch 1] Batch 4293, Loss 0.28073325753211975\n",
      "[Training Epoch 1] Batch 4294, Loss 0.3111961781978607\n",
      "[Training Epoch 1] Batch 4295, Loss 0.29106664657592773\n",
      "[Training Epoch 1] Batch 4296, Loss 0.27503687143325806\n",
      "[Training Epoch 1] Batch 4297, Loss 0.2843456566333771\n",
      "[Training Epoch 1] Batch 4298, Loss 0.28224095702171326\n",
      "[Training Epoch 1] Batch 4299, Loss 0.27881380915641785\n",
      "[Training Epoch 1] Batch 4300, Loss 0.2766282558441162\n",
      "[Training Epoch 1] Batch 4301, Loss 0.28949111700057983\n",
      "[Training Epoch 1] Batch 4302, Loss 0.2680317759513855\n",
      "[Training Epoch 1] Batch 4303, Loss 0.30199259519577026\n",
      "[Training Epoch 1] Batch 4304, Loss 0.28568845987319946\n",
      "[Training Epoch 1] Batch 4305, Loss 0.28217700123786926\n",
      "[Training Epoch 1] Batch 4306, Loss 0.28188908100128174\n",
      "[Training Epoch 1] Batch 4307, Loss 0.2855237126350403\n",
      "[Training Epoch 1] Batch 4308, Loss 0.26898622512817383\n",
      "[Training Epoch 1] Batch 4309, Loss 0.2953769564628601\n",
      "[Training Epoch 1] Batch 4310, Loss 0.2863619327545166\n",
      "[Training Epoch 1] Batch 4311, Loss 0.2858484387397766\n",
      "[Training Epoch 1] Batch 4312, Loss 0.28011780977249146\n",
      "[Training Epoch 1] Batch 4313, Loss 0.29268166422843933\n",
      "[Training Epoch 1] Batch 4314, Loss 0.29820045828819275\n",
      "[Training Epoch 1] Batch 4315, Loss 0.30337226390838623\n",
      "[Training Epoch 1] Batch 4316, Loss 0.2669762969017029\n",
      "[Training Epoch 1] Batch 4317, Loss 0.28684842586517334\n",
      "[Training Epoch 1] Batch 4318, Loss 0.29060789942741394\n",
      "[Training Epoch 1] Batch 4319, Loss 0.3014647364616394\n",
      "[Training Epoch 1] Batch 4320, Loss 0.2839930057525635\n",
      "[Training Epoch 1] Batch 4321, Loss 0.29430338740348816\n",
      "[Training Epoch 1] Batch 4322, Loss 0.32018202543258667\n",
      "[Training Epoch 1] Batch 4323, Loss 0.26116377115249634\n",
      "[Training Epoch 1] Batch 4324, Loss 0.30100566148757935\n",
      "[Training Epoch 1] Batch 4325, Loss 0.28779900074005127\n",
      "[Training Epoch 1] Batch 4326, Loss 0.3026353716850281\n",
      "[Training Epoch 1] Batch 4327, Loss 0.27510252594947815\n",
      "[Training Epoch 1] Batch 4328, Loss 0.2913205027580261\n",
      "[Training Epoch 1] Batch 4329, Loss 0.2721271812915802\n",
      "[Training Epoch 1] Batch 4330, Loss 0.28623026609420776\n",
      "[Training Epoch 1] Batch 4331, Loss 0.2972860634326935\n",
      "[Training Epoch 1] Batch 4332, Loss 0.27583783864974976\n",
      "[Training Epoch 1] Batch 4333, Loss 0.2887873649597168\n",
      "[Training Epoch 1] Batch 4334, Loss 0.27581679821014404\n",
      "[Training Epoch 1] Batch 4335, Loss 0.3095257580280304\n",
      "[Training Epoch 1] Batch 4336, Loss 0.2780970335006714\n",
      "[Training Epoch 1] Batch 4337, Loss 0.3175753951072693\n",
      "[Training Epoch 1] Batch 4338, Loss 0.248744398355484\n",
      "[Training Epoch 1] Batch 4339, Loss 0.30136924982070923\n",
      "[Training Epoch 1] Batch 4340, Loss 0.31001052260398865\n",
      "[Training Epoch 1] Batch 4341, Loss 0.2689971923828125\n",
      "[Training Epoch 1] Batch 4342, Loss 0.2992934584617615\n",
      "[Training Epoch 1] Batch 4343, Loss 0.2850360870361328\n",
      "[Training Epoch 1] Batch 4344, Loss 0.3163420855998993\n",
      "[Training Epoch 1] Batch 4345, Loss 0.28261619806289673\n",
      "[Training Epoch 1] Batch 4346, Loss 0.3134307265281677\n",
      "[Training Epoch 1] Batch 4347, Loss 0.27684223651885986\n",
      "[Training Epoch 1] Batch 4348, Loss 0.28970766067504883\n",
      "[Training Epoch 1] Batch 4349, Loss 0.30198511481285095\n",
      "[Training Epoch 1] Batch 4350, Loss 0.27035415172576904\n",
      "[Training Epoch 1] Batch 4351, Loss 0.30010050535202026\n",
      "[Training Epoch 1] Batch 4352, Loss 0.28233805298805237\n",
      "[Training Epoch 1] Batch 4353, Loss 0.319193959236145\n",
      "[Training Epoch 1] Batch 4354, Loss 0.31054359674453735\n",
      "[Training Epoch 1] Batch 4355, Loss 0.28285032510757446\n",
      "[Training Epoch 1] Batch 4356, Loss 0.3134085536003113\n",
      "[Training Epoch 1] Batch 4357, Loss 0.31443238258361816\n",
      "[Training Epoch 1] Batch 4358, Loss 0.2986040711402893\n",
      "[Training Epoch 1] Batch 4359, Loss 0.2948065400123596\n",
      "[Training Epoch 1] Batch 4360, Loss 0.30584555864334106\n",
      "[Training Epoch 1] Batch 4361, Loss 0.263694703578949\n",
      "[Training Epoch 1] Batch 4362, Loss 0.2854158282279968\n",
      "[Training Epoch 1] Batch 4363, Loss 0.28928083181381226\n",
      "[Training Epoch 1] Batch 4364, Loss 0.30533409118652344\n",
      "[Training Epoch 1] Batch 4365, Loss 0.30889078974723816\n",
      "[Training Epoch 1] Batch 4366, Loss 0.2723836898803711\n",
      "[Training Epoch 1] Batch 4367, Loss 0.2851962745189667\n",
      "[Training Epoch 1] Batch 4368, Loss 0.30730199813842773\n",
      "[Training Epoch 1] Batch 4369, Loss 0.29600465297698975\n",
      "[Training Epoch 1] Batch 4370, Loss 0.2839512825012207\n",
      "[Training Epoch 1] Batch 4371, Loss 0.2810342609882355\n",
      "[Training Epoch 1] Batch 4372, Loss 0.25487077236175537\n",
      "[Training Epoch 1] Batch 4373, Loss 0.28933385014533997\n",
      "[Training Epoch 1] Batch 4374, Loss 0.2648065686225891\n",
      "[Training Epoch 1] Batch 4375, Loss 0.285476416349411\n",
      "[Training Epoch 1] Batch 4376, Loss 0.27514415979385376\n",
      "[Training Epoch 1] Batch 4377, Loss 0.3061254620552063\n",
      "[Training Epoch 1] Batch 4378, Loss 0.286946177482605\n",
      "[Training Epoch 1] Batch 4379, Loss 0.2763281762599945\n",
      "[Training Epoch 1] Batch 4380, Loss 0.2943335473537445\n",
      "[Training Epoch 1] Batch 4381, Loss 0.3035621643066406\n",
      "[Training Epoch 1] Batch 4382, Loss 0.2226588875055313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:05<00:00, 1984.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 1] Precision = 0.2616, Recall = 0.7744\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.26416850090026855\n",
      "[Training Epoch 2] Batch 1, Loss 0.2735558748245239\n",
      "[Training Epoch 2] Batch 2, Loss 0.31524890661239624\n",
      "[Training Epoch 2] Batch 3, Loss 0.25924068689346313\n",
      "[Training Epoch 2] Batch 4, Loss 0.2707514762878418\n",
      "[Training Epoch 2] Batch 5, Loss 0.2809693217277527\n",
      "[Training Epoch 2] Batch 6, Loss 0.2827235758304596\n",
      "[Training Epoch 2] Batch 7, Loss 0.2767148017883301\n",
      "[Training Epoch 2] Batch 8, Loss 0.279736191034317\n",
      "[Training Epoch 2] Batch 9, Loss 0.27633699774742126\n",
      "[Training Epoch 2] Batch 10, Loss 0.24467654526233673\n",
      "[Training Epoch 2] Batch 11, Loss 0.26477566361427307\n",
      "[Training Epoch 2] Batch 12, Loss 0.2683340311050415\n",
      "[Training Epoch 2] Batch 13, Loss 0.2709121108055115\n",
      "[Training Epoch 2] Batch 14, Loss 0.2918931245803833\n",
      "[Training Epoch 2] Batch 15, Loss 0.29036056995391846\n",
      "[Training Epoch 2] Batch 16, Loss 0.27210313081741333\n",
      "[Training Epoch 2] Batch 17, Loss 0.2712234556674957\n",
      "[Training Epoch 2] Batch 18, Loss 0.2979480028152466\n",
      "[Training Epoch 2] Batch 19, Loss 0.2664144039154053\n",
      "[Training Epoch 2] Batch 20, Loss 0.2934175729751587\n",
      "[Training Epoch 2] Batch 21, Loss 0.3098468482494354\n",
      "[Training Epoch 2] Batch 22, Loss 0.27053964138031006\n",
      "[Training Epoch 2] Batch 23, Loss 0.31126707792282104\n",
      "[Training Epoch 2] Batch 24, Loss 0.29212725162506104\n",
      "[Training Epoch 2] Batch 25, Loss 0.28186553716659546\n",
      "[Training Epoch 2] Batch 26, Loss 0.2886577844619751\n",
      "[Training Epoch 2] Batch 27, Loss 0.2925962805747986\n",
      "[Training Epoch 2] Batch 28, Loss 0.2818251848220825\n",
      "[Training Epoch 2] Batch 29, Loss 0.2712547183036804\n",
      "[Training Epoch 2] Batch 30, Loss 0.27745264768600464\n",
      "[Training Epoch 2] Batch 31, Loss 0.2650793194770813\n",
      "[Training Epoch 2] Batch 32, Loss 0.29701459407806396\n",
      "[Training Epoch 2] Batch 33, Loss 0.29640597105026245\n",
      "[Training Epoch 2] Batch 34, Loss 0.30226850509643555\n",
      "[Training Epoch 2] Batch 35, Loss 0.2892923653125763\n",
      "[Training Epoch 2] Batch 36, Loss 0.27503544092178345\n",
      "[Training Epoch 2] Batch 37, Loss 0.2950824499130249\n",
      "[Training Epoch 2] Batch 38, Loss 0.29519182443618774\n",
      "[Training Epoch 2] Batch 39, Loss 0.30113357305526733\n",
      "[Training Epoch 2] Batch 40, Loss 0.29432275891304016\n",
      "[Training Epoch 2] Batch 41, Loss 0.2860347330570221\n",
      "[Training Epoch 2] Batch 42, Loss 0.2676294445991516\n",
      "[Training Epoch 2] Batch 43, Loss 0.2795734107494354\n",
      "[Training Epoch 2] Batch 44, Loss 0.25675129890441895\n",
      "[Training Epoch 2] Batch 45, Loss 0.28265076875686646\n",
      "[Training Epoch 2] Batch 46, Loss 0.27807098627090454\n",
      "[Training Epoch 2] Batch 47, Loss 0.2891111969947815\n",
      "[Training Epoch 2] Batch 48, Loss 0.3037480413913727\n",
      "[Training Epoch 2] Batch 49, Loss 0.27812811732292175\n",
      "[Training Epoch 2] Batch 50, Loss 0.30723249912261963\n",
      "[Training Epoch 2] Batch 51, Loss 0.28330060839653015\n",
      "[Training Epoch 2] Batch 52, Loss 0.2996522784233093\n",
      "[Training Epoch 2] Batch 53, Loss 0.2835681438446045\n",
      "[Training Epoch 2] Batch 54, Loss 0.29643312096595764\n",
      "[Training Epoch 2] Batch 55, Loss 0.29921460151672363\n",
      "[Training Epoch 2] Batch 56, Loss 0.2846266031265259\n",
      "[Training Epoch 2] Batch 57, Loss 0.32295262813568115\n",
      "[Training Epoch 2] Batch 58, Loss 0.3042593002319336\n",
      "[Training Epoch 2] Batch 59, Loss 0.2676751911640167\n",
      "[Training Epoch 2] Batch 60, Loss 0.28727737069129944\n",
      "[Training Epoch 2] Batch 61, Loss 0.26827311515808105\n",
      "[Training Epoch 2] Batch 62, Loss 0.27752164006233215\n",
      "[Training Epoch 2] Batch 63, Loss 0.2739432156085968\n",
      "[Training Epoch 2] Batch 64, Loss 0.27243566513061523\n",
      "[Training Epoch 2] Batch 65, Loss 0.27924901247024536\n",
      "[Training Epoch 2] Batch 66, Loss 0.29216068983078003\n",
      "[Training Epoch 2] Batch 67, Loss 0.3050383925437927\n",
      "[Training Epoch 2] Batch 68, Loss 0.29791682958602905\n",
      "[Training Epoch 2] Batch 69, Loss 0.3008077144622803\n",
      "[Training Epoch 2] Batch 70, Loss 0.2952556014060974\n",
      "[Training Epoch 2] Batch 71, Loss 0.27124762535095215\n",
      "[Training Epoch 2] Batch 72, Loss 0.3000714182853699\n",
      "[Training Epoch 2] Batch 73, Loss 0.2927325367927551\n",
      "[Training Epoch 2] Batch 74, Loss 0.309670627117157\n",
      "[Training Epoch 2] Batch 75, Loss 0.2743085026741028\n",
      "[Training Epoch 2] Batch 76, Loss 0.26529228687286377\n",
      "[Training Epoch 2] Batch 77, Loss 0.2901172339916229\n",
      "[Training Epoch 2] Batch 78, Loss 0.2665391266345978\n",
      "[Training Epoch 2] Batch 79, Loss 0.2632507383823395\n",
      "[Training Epoch 2] Batch 80, Loss 0.3007112145423889\n",
      "[Training Epoch 2] Batch 81, Loss 0.2673461437225342\n",
      "[Training Epoch 2] Batch 82, Loss 0.28802743554115295\n",
      "[Training Epoch 2] Batch 83, Loss 0.27736058831214905\n",
      "[Training Epoch 2] Batch 84, Loss 0.3076917827129364\n",
      "[Training Epoch 2] Batch 85, Loss 0.31202492117881775\n",
      "[Training Epoch 2] Batch 86, Loss 0.27378445863723755\n",
      "[Training Epoch 2] Batch 87, Loss 0.2864488363265991\n",
      "[Training Epoch 2] Batch 88, Loss 0.29794907569885254\n",
      "[Training Epoch 2] Batch 89, Loss 0.3063732385635376\n",
      "[Training Epoch 2] Batch 90, Loss 0.2513670027256012\n",
      "[Training Epoch 2] Batch 91, Loss 0.29677408933639526\n",
      "[Training Epoch 2] Batch 92, Loss 0.29745936393737793\n",
      "[Training Epoch 2] Batch 93, Loss 0.29808634519577026\n",
      "[Training Epoch 2] Batch 94, Loss 0.26831963658332825\n",
      "[Training Epoch 2] Batch 95, Loss 0.2890172004699707\n",
      "[Training Epoch 2] Batch 96, Loss 0.2957073450088501\n",
      "[Training Epoch 2] Batch 97, Loss 0.2921510636806488\n",
      "[Training Epoch 2] Batch 98, Loss 0.2775571346282959\n",
      "[Training Epoch 2] Batch 99, Loss 0.28226783871650696\n",
      "[Training Epoch 2] Batch 100, Loss 0.26265865564346313\n",
      "[Training Epoch 2] Batch 101, Loss 0.2701070308685303\n",
      "[Training Epoch 2] Batch 102, Loss 0.25603634119033813\n",
      "[Training Epoch 2] Batch 103, Loss 0.2784927487373352\n",
      "[Training Epoch 2] Batch 104, Loss 0.2754442095756531\n",
      "[Training Epoch 2] Batch 105, Loss 0.2761330306529999\n",
      "[Training Epoch 2] Batch 106, Loss 0.27411139011383057\n",
      "[Training Epoch 2] Batch 107, Loss 0.27774330973625183\n",
      "[Training Epoch 2] Batch 108, Loss 0.29112058877944946\n",
      "[Training Epoch 2] Batch 109, Loss 0.27212369441986084\n",
      "[Training Epoch 2] Batch 110, Loss 0.2649277448654175\n",
      "[Training Epoch 2] Batch 111, Loss 0.2725575566291809\n",
      "[Training Epoch 2] Batch 112, Loss 0.2690729796886444\n",
      "[Training Epoch 2] Batch 113, Loss 0.30719423294067383\n",
      "[Training Epoch 2] Batch 114, Loss 0.29343682527542114\n",
      "[Training Epoch 2] Batch 115, Loss 0.2874201536178589\n",
      "[Training Epoch 2] Batch 116, Loss 0.2999022603034973\n",
      "[Training Epoch 2] Batch 117, Loss 0.2748810648918152\n",
      "[Training Epoch 2] Batch 118, Loss 0.27855491638183594\n",
      "[Training Epoch 2] Batch 119, Loss 0.2915845513343811\n",
      "[Training Epoch 2] Batch 120, Loss 0.28841400146484375\n",
      "[Training Epoch 2] Batch 121, Loss 0.26722872257232666\n",
      "[Training Epoch 2] Batch 122, Loss 0.271904855966568\n",
      "[Training Epoch 2] Batch 123, Loss 0.2684589624404907\n",
      "[Training Epoch 2] Batch 124, Loss 0.2877674698829651\n",
      "[Training Epoch 2] Batch 125, Loss 0.2808550298213959\n",
      "[Training Epoch 2] Batch 126, Loss 0.2875957489013672\n",
      "[Training Epoch 2] Batch 127, Loss 0.2793163061141968\n",
      "[Training Epoch 2] Batch 128, Loss 0.2966134250164032\n",
      "[Training Epoch 2] Batch 129, Loss 0.27784112095832825\n",
      "[Training Epoch 2] Batch 130, Loss 0.3023133873939514\n",
      "[Training Epoch 2] Batch 131, Loss 0.2731552720069885\n",
      "[Training Epoch 2] Batch 132, Loss 0.2586154341697693\n",
      "[Training Epoch 2] Batch 133, Loss 0.2683904767036438\n",
      "[Training Epoch 2] Batch 134, Loss 0.27607911825180054\n",
      "[Training Epoch 2] Batch 135, Loss 0.2787279188632965\n",
      "[Training Epoch 2] Batch 136, Loss 0.28207993507385254\n",
      "[Training Epoch 2] Batch 137, Loss 0.28438800573349\n",
      "[Training Epoch 2] Batch 138, Loss 0.25570347905158997\n",
      "[Training Epoch 2] Batch 139, Loss 0.2725134491920471\n",
      "[Training Epoch 2] Batch 140, Loss 0.30547118186950684\n",
      "[Training Epoch 2] Batch 141, Loss 0.2894033193588257\n",
      "[Training Epoch 2] Batch 142, Loss 0.2863655686378479\n",
      "[Training Epoch 2] Batch 143, Loss 0.2764260768890381\n",
      "[Training Epoch 2] Batch 144, Loss 0.2808898091316223\n",
      "[Training Epoch 2] Batch 145, Loss 0.2845016121864319\n",
      "[Training Epoch 2] Batch 146, Loss 0.25382164120674133\n",
      "[Training Epoch 2] Batch 147, Loss 0.2981632351875305\n",
      "[Training Epoch 2] Batch 148, Loss 0.3010580241680145\n",
      "[Training Epoch 2] Batch 149, Loss 0.2776622176170349\n",
      "[Training Epoch 2] Batch 150, Loss 0.300078809261322\n",
      "[Training Epoch 2] Batch 151, Loss 0.2718749940395355\n",
      "[Training Epoch 2] Batch 152, Loss 0.2748832106590271\n",
      "[Training Epoch 2] Batch 153, Loss 0.28047847747802734\n",
      "[Training Epoch 2] Batch 154, Loss 0.2578060030937195\n",
      "[Training Epoch 2] Batch 155, Loss 0.2625507712364197\n",
      "[Training Epoch 2] Batch 156, Loss 0.2799205482006073\n",
      "[Training Epoch 2] Batch 157, Loss 0.28742048144340515\n",
      "[Training Epoch 2] Batch 158, Loss 0.3141624629497528\n",
      "[Training Epoch 2] Batch 159, Loss 0.3005448281764984\n",
      "[Training Epoch 2] Batch 160, Loss 0.28106892108917236\n",
      "[Training Epoch 2] Batch 161, Loss 0.2611854374408722\n",
      "[Training Epoch 2] Batch 162, Loss 0.2770049273967743\n",
      "[Training Epoch 2] Batch 163, Loss 0.3103015422821045\n",
      "[Training Epoch 2] Batch 164, Loss 0.2977902293205261\n",
      "[Training Epoch 2] Batch 165, Loss 0.2487671822309494\n",
      "[Training Epoch 2] Batch 166, Loss 0.2860771715641022\n",
      "[Training Epoch 2] Batch 167, Loss 0.31797438859939575\n",
      "[Training Epoch 2] Batch 168, Loss 0.2811650037765503\n",
      "[Training Epoch 2] Batch 169, Loss 0.2974417507648468\n",
      "[Training Epoch 2] Batch 170, Loss 0.27957797050476074\n",
      "[Training Epoch 2] Batch 171, Loss 0.2758907079696655\n",
      "[Training Epoch 2] Batch 172, Loss 0.2970196306705475\n",
      "[Training Epoch 2] Batch 173, Loss 0.2642750144004822\n",
      "[Training Epoch 2] Batch 174, Loss 0.2820022702217102\n",
      "[Training Epoch 2] Batch 175, Loss 0.2679886221885681\n",
      "[Training Epoch 2] Batch 176, Loss 0.30223214626312256\n",
      "[Training Epoch 2] Batch 177, Loss 0.3034871816635132\n",
      "[Training Epoch 2] Batch 178, Loss 0.3043989837169647\n",
      "[Training Epoch 2] Batch 179, Loss 0.27744024991989136\n",
      "[Training Epoch 2] Batch 180, Loss 0.2719706892967224\n",
      "[Training Epoch 2] Batch 181, Loss 0.27561885118484497\n",
      "[Training Epoch 2] Batch 182, Loss 0.2867199778556824\n",
      "[Training Epoch 2] Batch 183, Loss 0.2795756459236145\n",
      "[Training Epoch 2] Batch 184, Loss 0.2813645601272583\n",
      "[Training Epoch 2] Batch 185, Loss 0.2593225836753845\n",
      "[Training Epoch 2] Batch 186, Loss 0.28757575154304504\n",
      "[Training Epoch 2] Batch 187, Loss 0.28032639622688293\n",
      "[Training Epoch 2] Batch 188, Loss 0.30783891677856445\n",
      "[Training Epoch 2] Batch 189, Loss 0.2981427311897278\n",
      "[Training Epoch 2] Batch 190, Loss 0.2701962888240814\n",
      "[Training Epoch 2] Batch 191, Loss 0.29525524377822876\n",
      "[Training Epoch 2] Batch 192, Loss 0.2969488501548767\n",
      "[Training Epoch 2] Batch 193, Loss 0.28471457958221436\n",
      "[Training Epoch 2] Batch 194, Loss 0.26543939113616943\n",
      "[Training Epoch 2] Batch 195, Loss 0.29138195514678955\n",
      "[Training Epoch 2] Batch 196, Loss 0.3023158311843872\n",
      "[Training Epoch 2] Batch 197, Loss 0.2584262490272522\n",
      "[Training Epoch 2] Batch 198, Loss 0.2930143475532532\n",
      "[Training Epoch 2] Batch 199, Loss 0.29043227434158325\n",
      "[Training Epoch 2] Batch 200, Loss 0.2774113416671753\n",
      "[Training Epoch 2] Batch 201, Loss 0.274445116519928\n",
      "[Training Epoch 2] Batch 202, Loss 0.27284759283065796\n",
      "[Training Epoch 2] Batch 203, Loss 0.28352510929107666\n",
      "[Training Epoch 2] Batch 204, Loss 0.2766525149345398\n",
      "[Training Epoch 2] Batch 205, Loss 0.28114601969718933\n",
      "[Training Epoch 2] Batch 206, Loss 0.29820626974105835\n",
      "[Training Epoch 2] Batch 207, Loss 0.29902365803718567\n",
      "[Training Epoch 2] Batch 208, Loss 0.2752760350704193\n",
      "[Training Epoch 2] Batch 209, Loss 0.2797964811325073\n",
      "[Training Epoch 2] Batch 210, Loss 0.27779990434646606\n",
      "[Training Epoch 2] Batch 211, Loss 0.3052826523780823\n",
      "[Training Epoch 2] Batch 212, Loss 0.27450069785118103\n",
      "[Training Epoch 2] Batch 213, Loss 0.2760925590991974\n",
      "[Training Epoch 2] Batch 214, Loss 0.2838539481163025\n",
      "[Training Epoch 2] Batch 215, Loss 0.282805860042572\n",
      "[Training Epoch 2] Batch 216, Loss 0.30206581950187683\n",
      "[Training Epoch 2] Batch 217, Loss 0.28680846095085144\n",
      "[Training Epoch 2] Batch 218, Loss 0.2622591257095337\n",
      "[Training Epoch 2] Batch 219, Loss 0.287925660610199\n",
      "[Training Epoch 2] Batch 220, Loss 0.2487810105085373\n",
      "[Training Epoch 2] Batch 221, Loss 0.304521381855011\n",
      "[Training Epoch 2] Batch 222, Loss 0.2761468291282654\n",
      "[Training Epoch 2] Batch 223, Loss 0.2757342755794525\n",
      "[Training Epoch 2] Batch 224, Loss 0.2734881043434143\n",
      "[Training Epoch 2] Batch 225, Loss 0.27406102418899536\n",
      "[Training Epoch 2] Batch 226, Loss 0.28655678033828735\n",
      "[Training Epoch 2] Batch 227, Loss 0.30308327078819275\n",
      "[Training Epoch 2] Batch 228, Loss 0.2758817672729492\n",
      "[Training Epoch 2] Batch 229, Loss 0.3048449754714966\n",
      "[Training Epoch 2] Batch 230, Loss 0.2670917212963104\n",
      "[Training Epoch 2] Batch 231, Loss 0.3049568831920624\n",
      "[Training Epoch 2] Batch 232, Loss 0.2739178240299225\n",
      "[Training Epoch 2] Batch 233, Loss 0.3024420142173767\n",
      "[Training Epoch 2] Batch 234, Loss 0.2988625764846802\n",
      "[Training Epoch 2] Batch 235, Loss 0.2745606005191803\n",
      "[Training Epoch 2] Batch 236, Loss 0.30032843351364136\n",
      "[Training Epoch 2] Batch 237, Loss 0.2977484166622162\n",
      "[Training Epoch 2] Batch 238, Loss 0.27454957365989685\n",
      "[Training Epoch 2] Batch 239, Loss 0.2898646593093872\n",
      "[Training Epoch 2] Batch 240, Loss 0.2735215127468109\n",
      "[Training Epoch 2] Batch 241, Loss 0.2673153281211853\n",
      "[Training Epoch 2] Batch 242, Loss 0.3041451871395111\n",
      "[Training Epoch 2] Batch 243, Loss 0.29853740334510803\n",
      "[Training Epoch 2] Batch 244, Loss 0.2826150059700012\n",
      "[Training Epoch 2] Batch 245, Loss 0.29334235191345215\n",
      "[Training Epoch 2] Batch 246, Loss 0.263312429189682\n",
      "[Training Epoch 2] Batch 247, Loss 0.28210747241973877\n",
      "[Training Epoch 2] Batch 248, Loss 0.28950342535972595\n",
      "[Training Epoch 2] Batch 249, Loss 0.2889852523803711\n",
      "[Training Epoch 2] Batch 250, Loss 0.26600438356399536\n",
      "[Training Epoch 2] Batch 251, Loss 0.27450746297836304\n",
      "[Training Epoch 2] Batch 252, Loss 0.28072822093963623\n",
      "[Training Epoch 2] Batch 253, Loss 0.27484193444252014\n",
      "[Training Epoch 2] Batch 254, Loss 0.2712584137916565\n",
      "[Training Epoch 2] Batch 255, Loss 0.27542221546173096\n",
      "[Training Epoch 2] Batch 256, Loss 0.27986401319503784\n",
      "[Training Epoch 2] Batch 257, Loss 0.30266863107681274\n",
      "[Training Epoch 2] Batch 258, Loss 0.2627439498901367\n",
      "[Training Epoch 2] Batch 259, Loss 0.2862224578857422\n",
      "[Training Epoch 2] Batch 260, Loss 0.274705708026886\n",
      "[Training Epoch 2] Batch 261, Loss 0.29040443897247314\n",
      "[Training Epoch 2] Batch 262, Loss 0.26909172534942627\n",
      "[Training Epoch 2] Batch 263, Loss 0.2504725754261017\n",
      "[Training Epoch 2] Batch 264, Loss 0.2673383951187134\n",
      "[Training Epoch 2] Batch 265, Loss 0.2796982526779175\n",
      "[Training Epoch 2] Batch 266, Loss 0.26768624782562256\n",
      "[Training Epoch 2] Batch 267, Loss 0.2757580280303955\n",
      "[Training Epoch 2] Batch 268, Loss 0.2939903140068054\n",
      "[Training Epoch 2] Batch 269, Loss 0.26096677780151367\n",
      "[Training Epoch 2] Batch 270, Loss 0.30754631757736206\n",
      "[Training Epoch 2] Batch 271, Loss 0.2833101153373718\n",
      "[Training Epoch 2] Batch 272, Loss 0.311945378780365\n",
      "[Training Epoch 2] Batch 273, Loss 0.2975480556488037\n",
      "[Training Epoch 2] Batch 274, Loss 0.26652300357818604\n",
      "[Training Epoch 2] Batch 275, Loss 0.2895659804344177\n",
      "[Training Epoch 2] Batch 276, Loss 0.2890624403953552\n",
      "[Training Epoch 2] Batch 277, Loss 0.3275270462036133\n",
      "[Training Epoch 2] Batch 278, Loss 0.293062686920166\n",
      "[Training Epoch 2] Batch 279, Loss 0.26232120394706726\n",
      "[Training Epoch 2] Batch 280, Loss 0.26598358154296875\n",
      "[Training Epoch 2] Batch 281, Loss 0.2606923282146454\n",
      "[Training Epoch 2] Batch 282, Loss 0.2977784276008606\n",
      "[Training Epoch 2] Batch 283, Loss 0.30403661727905273\n",
      "[Training Epoch 2] Batch 284, Loss 0.2947552800178528\n",
      "[Training Epoch 2] Batch 285, Loss 0.2751437723636627\n",
      "[Training Epoch 2] Batch 286, Loss 0.27171093225479126\n",
      "[Training Epoch 2] Batch 287, Loss 0.28886809945106506\n",
      "[Training Epoch 2] Batch 288, Loss 0.2636839747428894\n",
      "[Training Epoch 2] Batch 289, Loss 0.272564172744751\n",
      "[Training Epoch 2] Batch 290, Loss 0.2868586778640747\n",
      "[Training Epoch 2] Batch 291, Loss 0.27987486124038696\n",
      "[Training Epoch 2] Batch 292, Loss 0.28937602043151855\n",
      "[Training Epoch 2] Batch 293, Loss 0.3181820809841156\n",
      "[Training Epoch 2] Batch 294, Loss 0.2701404094696045\n",
      "[Training Epoch 2] Batch 295, Loss 0.28777873516082764\n",
      "[Training Epoch 2] Batch 296, Loss 0.26271873712539673\n",
      "[Training Epoch 2] Batch 297, Loss 0.2569572627544403\n",
      "[Training Epoch 2] Batch 298, Loss 0.2855069935321808\n",
      "[Training Epoch 2] Batch 299, Loss 0.2729521691799164\n",
      "[Training Epoch 2] Batch 300, Loss 0.2687905430793762\n",
      "[Training Epoch 2] Batch 301, Loss 0.2654775381088257\n",
      "[Training Epoch 2] Batch 302, Loss 0.2895756661891937\n",
      "[Training Epoch 2] Batch 303, Loss 0.2931710183620453\n",
      "[Training Epoch 2] Batch 304, Loss 0.28878629207611084\n",
      "[Training Epoch 2] Batch 305, Loss 0.26211225986480713\n",
      "[Training Epoch 2] Batch 306, Loss 0.29709529876708984\n",
      "[Training Epoch 2] Batch 307, Loss 0.2770649194717407\n",
      "[Training Epoch 2] Batch 308, Loss 0.2827547490596771\n",
      "[Training Epoch 2] Batch 309, Loss 0.2619810104370117\n",
      "[Training Epoch 2] Batch 310, Loss 0.26849281787872314\n",
      "[Training Epoch 2] Batch 311, Loss 0.26806503534317017\n",
      "[Training Epoch 2] Batch 312, Loss 0.2741430997848511\n",
      "[Training Epoch 2] Batch 313, Loss 0.3099408745765686\n",
      "[Training Epoch 2] Batch 314, Loss 0.2814926505088806\n",
      "[Training Epoch 2] Batch 315, Loss 0.2788146138191223\n",
      "[Training Epoch 2] Batch 316, Loss 0.29752296209335327\n",
      "[Training Epoch 2] Batch 317, Loss 0.26530683040618896\n",
      "[Training Epoch 2] Batch 318, Loss 0.2921755611896515\n",
      "[Training Epoch 2] Batch 319, Loss 0.2676964998245239\n",
      "[Training Epoch 2] Batch 320, Loss 0.2963212728500366\n",
      "[Training Epoch 2] Batch 321, Loss 0.28870880603790283\n",
      "[Training Epoch 2] Batch 322, Loss 0.29678136110305786\n",
      "[Training Epoch 2] Batch 323, Loss 0.24770572781562805\n",
      "[Training Epoch 2] Batch 324, Loss 0.2833322584629059\n",
      "[Training Epoch 2] Batch 325, Loss 0.28495609760284424\n",
      "[Training Epoch 2] Batch 326, Loss 0.27020227909088135\n",
      "[Training Epoch 2] Batch 327, Loss 0.30735719203948975\n",
      "[Training Epoch 2] Batch 328, Loss 0.29044309258461\n",
      "[Training Epoch 2] Batch 329, Loss 0.28723326325416565\n",
      "[Training Epoch 2] Batch 330, Loss 0.28041332960128784\n",
      "[Training Epoch 2] Batch 331, Loss 0.28686749935150146\n",
      "[Training Epoch 2] Batch 332, Loss 0.30169281363487244\n",
      "[Training Epoch 2] Batch 333, Loss 0.26371854543685913\n",
      "[Training Epoch 2] Batch 334, Loss 0.2982921600341797\n",
      "[Training Epoch 2] Batch 335, Loss 0.272955060005188\n",
      "[Training Epoch 2] Batch 336, Loss 0.2339225858449936\n",
      "[Training Epoch 2] Batch 337, Loss 0.2929908335208893\n",
      "[Training Epoch 2] Batch 338, Loss 0.271396666765213\n",
      "[Training Epoch 2] Batch 339, Loss 0.2694380581378937\n",
      "[Training Epoch 2] Batch 340, Loss 0.28597205877304077\n",
      "[Training Epoch 2] Batch 341, Loss 0.2507178783416748\n",
      "[Training Epoch 2] Batch 342, Loss 0.2983255386352539\n",
      "[Training Epoch 2] Batch 343, Loss 0.2629668712615967\n",
      "[Training Epoch 2] Batch 344, Loss 0.29920583963394165\n",
      "[Training Epoch 2] Batch 345, Loss 0.3153567910194397\n",
      "[Training Epoch 2] Batch 346, Loss 0.2668297588825226\n",
      "[Training Epoch 2] Batch 347, Loss 0.2916589081287384\n",
      "[Training Epoch 2] Batch 348, Loss 0.27977272868156433\n",
      "[Training Epoch 2] Batch 349, Loss 0.2655227482318878\n",
      "[Training Epoch 2] Batch 350, Loss 0.27080297470092773\n",
      "[Training Epoch 2] Batch 351, Loss 0.29052355885505676\n",
      "[Training Epoch 2] Batch 352, Loss 0.27061712741851807\n",
      "[Training Epoch 2] Batch 353, Loss 0.3038649559020996\n",
      "[Training Epoch 2] Batch 354, Loss 0.30478551983833313\n",
      "[Training Epoch 2] Batch 355, Loss 0.29521316289901733\n",
      "[Training Epoch 2] Batch 356, Loss 0.2540401220321655\n",
      "[Training Epoch 2] Batch 357, Loss 0.29440033435821533\n",
      "[Training Epoch 2] Batch 358, Loss 0.27456608414649963\n",
      "[Training Epoch 2] Batch 359, Loss 0.2942860722541809\n",
      "[Training Epoch 2] Batch 360, Loss 0.2870429754257202\n",
      "[Training Epoch 2] Batch 361, Loss 0.2895658016204834\n",
      "[Training Epoch 2] Batch 362, Loss 0.27244889736175537\n",
      "[Training Epoch 2] Batch 363, Loss 0.2941495478153229\n",
      "[Training Epoch 2] Batch 364, Loss 0.2647132873535156\n",
      "[Training Epoch 2] Batch 365, Loss 0.2797885537147522\n",
      "[Training Epoch 2] Batch 366, Loss 0.30448710918426514\n",
      "[Training Epoch 2] Batch 367, Loss 0.3082263469696045\n",
      "[Training Epoch 2] Batch 368, Loss 0.2957364320755005\n",
      "[Training Epoch 2] Batch 369, Loss 0.27699142694473267\n",
      "[Training Epoch 2] Batch 370, Loss 0.27849531173706055\n",
      "[Training Epoch 2] Batch 371, Loss 0.27908676862716675\n",
      "[Training Epoch 2] Batch 372, Loss 0.27373063564300537\n",
      "[Training Epoch 2] Batch 373, Loss 0.3038971424102783\n",
      "[Training Epoch 2] Batch 374, Loss 0.30022764205932617\n",
      "[Training Epoch 2] Batch 375, Loss 0.27607592940330505\n",
      "[Training Epoch 2] Batch 376, Loss 0.26679134368896484\n",
      "[Training Epoch 2] Batch 377, Loss 0.3262443542480469\n",
      "[Training Epoch 2] Batch 378, Loss 0.29140082001686096\n",
      "[Training Epoch 2] Batch 379, Loss 0.2933262288570404\n",
      "[Training Epoch 2] Batch 380, Loss 0.27750781178474426\n",
      "[Training Epoch 2] Batch 381, Loss 0.27238115668296814\n",
      "[Training Epoch 2] Batch 382, Loss 0.2841801643371582\n",
      "[Training Epoch 2] Batch 383, Loss 0.2932068407535553\n",
      "[Training Epoch 2] Batch 384, Loss 0.2872826159000397\n",
      "[Training Epoch 2] Batch 385, Loss 0.28795596957206726\n",
      "[Training Epoch 2] Batch 386, Loss 0.27782100439071655\n",
      "[Training Epoch 2] Batch 387, Loss 0.2637820243835449\n",
      "[Training Epoch 2] Batch 388, Loss 0.26470381021499634\n",
      "[Training Epoch 2] Batch 389, Loss 0.2969980537891388\n",
      "[Training Epoch 2] Batch 390, Loss 0.298170804977417\n",
      "[Training Epoch 2] Batch 391, Loss 0.3008934259414673\n",
      "[Training Epoch 2] Batch 392, Loss 0.30041950941085815\n",
      "[Training Epoch 2] Batch 393, Loss 0.3119138479232788\n",
      "[Training Epoch 2] Batch 394, Loss 0.2699522376060486\n",
      "[Training Epoch 2] Batch 395, Loss 0.2990889847278595\n",
      "[Training Epoch 2] Batch 396, Loss 0.27160102128982544\n",
      "[Training Epoch 2] Batch 397, Loss 0.28731289505958557\n",
      "[Training Epoch 2] Batch 398, Loss 0.26929908990859985\n",
      "[Training Epoch 2] Batch 399, Loss 0.2824099063873291\n",
      "[Training Epoch 2] Batch 400, Loss 0.2978155016899109\n",
      "[Training Epoch 2] Batch 401, Loss 0.28964850306510925\n",
      "[Training Epoch 2] Batch 402, Loss 0.29424619674682617\n",
      "[Training Epoch 2] Batch 403, Loss 0.2870537042617798\n",
      "[Training Epoch 2] Batch 404, Loss 0.28014659881591797\n",
      "[Training Epoch 2] Batch 405, Loss 0.30056872963905334\n",
      "[Training Epoch 2] Batch 406, Loss 0.29662948846817017\n",
      "[Training Epoch 2] Batch 407, Loss 0.27327683568000793\n",
      "[Training Epoch 2] Batch 408, Loss 0.3164089024066925\n",
      "[Training Epoch 2] Batch 409, Loss 0.2586348056793213\n",
      "[Training Epoch 2] Batch 410, Loss 0.289048969745636\n",
      "[Training Epoch 2] Batch 411, Loss 0.2909754812717438\n",
      "[Training Epoch 2] Batch 412, Loss 0.2882766127586365\n",
      "[Training Epoch 2] Batch 413, Loss 0.27526944875717163\n",
      "[Training Epoch 2] Batch 414, Loss 0.2815883159637451\n",
      "[Training Epoch 2] Batch 415, Loss 0.2574091851711273\n",
      "[Training Epoch 2] Batch 416, Loss 0.269570529460907\n",
      "[Training Epoch 2] Batch 417, Loss 0.24998626112937927\n",
      "[Training Epoch 2] Batch 418, Loss 0.27886635065078735\n",
      "[Training Epoch 2] Batch 419, Loss 0.2925437390804291\n",
      "[Training Epoch 2] Batch 420, Loss 0.29056885838508606\n",
      "[Training Epoch 2] Batch 421, Loss 0.29732441902160645\n",
      "[Training Epoch 2] Batch 422, Loss 0.2767047882080078\n",
      "[Training Epoch 2] Batch 423, Loss 0.2543907165527344\n",
      "[Training Epoch 2] Batch 424, Loss 0.2752094864845276\n",
      "[Training Epoch 2] Batch 425, Loss 0.2860909700393677\n",
      "[Training Epoch 2] Batch 426, Loss 0.30482786893844604\n",
      "[Training Epoch 2] Batch 427, Loss 0.29994431138038635\n",
      "[Training Epoch 2] Batch 428, Loss 0.2710312008857727\n",
      "[Training Epoch 2] Batch 429, Loss 0.2780037522315979\n",
      "[Training Epoch 2] Batch 430, Loss 0.26850855350494385\n",
      "[Training Epoch 2] Batch 431, Loss 0.26444852352142334\n",
      "[Training Epoch 2] Batch 432, Loss 0.2763863503932953\n",
      "[Training Epoch 2] Batch 433, Loss 0.2959560453891754\n",
      "[Training Epoch 2] Batch 434, Loss 0.2917444109916687\n",
      "[Training Epoch 2] Batch 435, Loss 0.29977232217788696\n",
      "[Training Epoch 2] Batch 436, Loss 0.29857414960861206\n",
      "[Training Epoch 2] Batch 437, Loss 0.2894437909126282\n",
      "[Training Epoch 2] Batch 438, Loss 0.2705691456794739\n",
      "[Training Epoch 2] Batch 439, Loss 0.259084552526474\n",
      "[Training Epoch 2] Batch 440, Loss 0.27262061834335327\n",
      "[Training Epoch 2] Batch 441, Loss 0.30023717880249023\n",
      "[Training Epoch 2] Batch 442, Loss 0.27580299973487854\n",
      "[Training Epoch 2] Batch 443, Loss 0.27642732858657837\n",
      "[Training Epoch 2] Batch 444, Loss 0.26843225955963135\n",
      "[Training Epoch 2] Batch 445, Loss 0.2538068890571594\n",
      "[Training Epoch 2] Batch 446, Loss 0.28566741943359375\n",
      "[Training Epoch 2] Batch 447, Loss 0.28555697202682495\n",
      "[Training Epoch 2] Batch 448, Loss 0.25647252798080444\n",
      "[Training Epoch 2] Batch 449, Loss 0.29238829016685486\n",
      "[Training Epoch 2] Batch 450, Loss 0.2724118232727051\n",
      "[Training Epoch 2] Batch 451, Loss 0.2597542405128479\n",
      "[Training Epoch 2] Batch 452, Loss 0.27360305190086365\n",
      "[Training Epoch 2] Batch 453, Loss 0.24058806896209717\n",
      "[Training Epoch 2] Batch 454, Loss 0.28267359733581543\n",
      "[Training Epoch 2] Batch 455, Loss 0.2625051736831665\n",
      "[Training Epoch 2] Batch 456, Loss 0.2912634015083313\n",
      "[Training Epoch 2] Batch 457, Loss 0.2597782611846924\n",
      "[Training Epoch 2] Batch 458, Loss 0.2846691608428955\n",
      "[Training Epoch 2] Batch 459, Loss 0.3272484540939331\n",
      "[Training Epoch 2] Batch 460, Loss 0.28271543979644775\n",
      "[Training Epoch 2] Batch 461, Loss 0.2998274266719818\n",
      "[Training Epoch 2] Batch 462, Loss 0.2831875681877136\n",
      "[Training Epoch 2] Batch 463, Loss 0.29319536685943604\n",
      "[Training Epoch 2] Batch 464, Loss 0.2946782410144806\n",
      "[Training Epoch 2] Batch 465, Loss 0.3001079857349396\n",
      "[Training Epoch 2] Batch 466, Loss 0.28809118270874023\n",
      "[Training Epoch 2] Batch 467, Loss 0.26106151938438416\n",
      "[Training Epoch 2] Batch 468, Loss 0.32555773854255676\n",
      "[Training Epoch 2] Batch 469, Loss 0.27344581484794617\n",
      "[Training Epoch 2] Batch 470, Loss 0.2891814708709717\n",
      "[Training Epoch 2] Batch 471, Loss 0.2880393862724304\n",
      "[Training Epoch 2] Batch 472, Loss 0.2894366979598999\n",
      "[Training Epoch 2] Batch 473, Loss 0.2803414463996887\n",
      "[Training Epoch 2] Batch 474, Loss 0.27274900674819946\n",
      "[Training Epoch 2] Batch 475, Loss 0.3033345937728882\n",
      "[Training Epoch 2] Batch 476, Loss 0.3101319670677185\n",
      "[Training Epoch 2] Batch 477, Loss 0.2928793430328369\n",
      "[Training Epoch 2] Batch 478, Loss 0.3089092969894409\n",
      "[Training Epoch 2] Batch 479, Loss 0.2880106270313263\n",
      "[Training Epoch 2] Batch 480, Loss 0.2886869013309479\n",
      "[Training Epoch 2] Batch 481, Loss 0.24791955947875977\n",
      "[Training Epoch 2] Batch 482, Loss 0.29457107186317444\n",
      "[Training Epoch 2] Batch 483, Loss 0.3059012293815613\n",
      "[Training Epoch 2] Batch 484, Loss 0.27959001064300537\n",
      "[Training Epoch 2] Batch 485, Loss 0.28338301181793213\n",
      "[Training Epoch 2] Batch 486, Loss 0.27944496273994446\n",
      "[Training Epoch 2] Batch 487, Loss 0.28084349632263184\n",
      "[Training Epoch 2] Batch 488, Loss 0.24570408463478088\n",
      "[Training Epoch 2] Batch 489, Loss 0.2888721227645874\n",
      "[Training Epoch 2] Batch 490, Loss 0.2709321975708008\n",
      "[Training Epoch 2] Batch 491, Loss 0.25142014026641846\n",
      "[Training Epoch 2] Batch 492, Loss 0.28733012080192566\n",
      "[Training Epoch 2] Batch 493, Loss 0.30820322036743164\n",
      "[Training Epoch 2] Batch 494, Loss 0.28369587659835815\n",
      "[Training Epoch 2] Batch 495, Loss 0.26354795694351196\n",
      "[Training Epoch 2] Batch 496, Loss 0.26463499665260315\n",
      "[Training Epoch 2] Batch 497, Loss 0.26794907450675964\n",
      "[Training Epoch 2] Batch 498, Loss 0.27116382122039795\n",
      "[Training Epoch 2] Batch 499, Loss 0.27047234773635864\n",
      "[Training Epoch 2] Batch 500, Loss 0.3062255382537842\n",
      "[Training Epoch 2] Batch 501, Loss 0.2822292149066925\n",
      "[Training Epoch 2] Batch 502, Loss 0.2790927290916443\n",
      "[Training Epoch 2] Batch 503, Loss 0.2815474271774292\n",
      "[Training Epoch 2] Batch 504, Loss 0.27111825346946716\n",
      "[Training Epoch 2] Batch 505, Loss 0.311309278011322\n",
      "[Training Epoch 2] Batch 506, Loss 0.2685924768447876\n",
      "[Training Epoch 2] Batch 507, Loss 0.2720179259777069\n",
      "[Training Epoch 2] Batch 508, Loss 0.26592767238616943\n",
      "[Training Epoch 2] Batch 509, Loss 0.29119235277175903\n",
      "[Training Epoch 2] Batch 510, Loss 0.2639792561531067\n",
      "[Training Epoch 2] Batch 511, Loss 0.29269999265670776\n",
      "[Training Epoch 2] Batch 512, Loss 0.2927809953689575\n",
      "[Training Epoch 2] Batch 513, Loss 0.2954198122024536\n",
      "[Training Epoch 2] Batch 514, Loss 0.2835707664489746\n",
      "[Training Epoch 2] Batch 515, Loss 0.27308976650238037\n",
      "[Training Epoch 2] Batch 516, Loss 0.2821013629436493\n",
      "[Training Epoch 2] Batch 517, Loss 0.3036041557788849\n",
      "[Training Epoch 2] Batch 518, Loss 0.26378849148750305\n",
      "[Training Epoch 2] Batch 519, Loss 0.28687694668769836\n",
      "[Training Epoch 2] Batch 520, Loss 0.2970162630081177\n",
      "[Training Epoch 2] Batch 521, Loss 0.29473209381103516\n",
      "[Training Epoch 2] Batch 522, Loss 0.31217557191848755\n",
      "[Training Epoch 2] Batch 523, Loss 0.2972550094127655\n",
      "[Training Epoch 2] Batch 524, Loss 0.3247895836830139\n",
      "[Training Epoch 2] Batch 525, Loss 0.27956509590148926\n",
      "[Training Epoch 2] Batch 526, Loss 0.250583291053772\n",
      "[Training Epoch 2] Batch 527, Loss 0.24650682508945465\n",
      "[Training Epoch 2] Batch 528, Loss 0.2738139033317566\n",
      "[Training Epoch 2] Batch 529, Loss 0.3042481541633606\n",
      "[Training Epoch 2] Batch 530, Loss 0.2801772356033325\n",
      "[Training Epoch 2] Batch 531, Loss 0.3029918968677521\n",
      "[Training Epoch 2] Batch 532, Loss 0.2718053162097931\n",
      "[Training Epoch 2] Batch 533, Loss 0.28799572587013245\n",
      "[Training Epoch 2] Batch 534, Loss 0.30688127875328064\n",
      "[Training Epoch 2] Batch 535, Loss 0.29882580041885376\n",
      "[Training Epoch 2] Batch 536, Loss 0.26859933137893677\n",
      "[Training Epoch 2] Batch 537, Loss 0.29820823669433594\n",
      "[Training Epoch 2] Batch 538, Loss 0.2781848907470703\n",
      "[Training Epoch 2] Batch 539, Loss 0.2829223871231079\n",
      "[Training Epoch 2] Batch 540, Loss 0.28571903705596924\n",
      "[Training Epoch 2] Batch 541, Loss 0.2947325110435486\n",
      "[Training Epoch 2] Batch 542, Loss 0.2627439498901367\n",
      "[Training Epoch 2] Batch 543, Loss 0.297232449054718\n",
      "[Training Epoch 2] Batch 544, Loss 0.33581072092056274\n",
      "[Training Epoch 2] Batch 545, Loss 0.2817804217338562\n",
      "[Training Epoch 2] Batch 546, Loss 0.2873205244541168\n",
      "[Training Epoch 2] Batch 547, Loss 0.2985210418701172\n",
      "[Training Epoch 2] Batch 548, Loss 0.263485312461853\n",
      "[Training Epoch 2] Batch 549, Loss 0.2763713002204895\n",
      "[Training Epoch 2] Batch 550, Loss 0.2796992063522339\n",
      "[Training Epoch 2] Batch 551, Loss 0.2681038975715637\n",
      "[Training Epoch 2] Batch 552, Loss 0.2875557839870453\n",
      "[Training Epoch 2] Batch 553, Loss 0.304053395986557\n",
      "[Training Epoch 2] Batch 554, Loss 0.2612788677215576\n",
      "[Training Epoch 2] Batch 555, Loss 0.2616596221923828\n",
      "[Training Epoch 2] Batch 556, Loss 0.26973193883895874\n",
      "[Training Epoch 2] Batch 557, Loss 0.3137539029121399\n",
      "[Training Epoch 2] Batch 558, Loss 0.2827157974243164\n",
      "[Training Epoch 2] Batch 559, Loss 0.26113247871398926\n",
      "[Training Epoch 2] Batch 560, Loss 0.2534135580062866\n",
      "[Training Epoch 2] Batch 561, Loss 0.3163490295410156\n",
      "[Training Epoch 2] Batch 562, Loss 0.30299413204193115\n",
      "[Training Epoch 2] Batch 563, Loss 0.28928324580192566\n",
      "[Training Epoch 2] Batch 564, Loss 0.28679677844047546\n",
      "[Training Epoch 2] Batch 565, Loss 0.2820085883140564\n",
      "[Training Epoch 2] Batch 566, Loss 0.3162839114665985\n",
      "[Training Epoch 2] Batch 567, Loss 0.3033832907676697\n",
      "[Training Epoch 2] Batch 568, Loss 0.30651915073394775\n",
      "[Training Epoch 2] Batch 569, Loss 0.2752065360546112\n",
      "[Training Epoch 2] Batch 570, Loss 0.2677951455116272\n",
      "[Training Epoch 2] Batch 571, Loss 0.29810842871665955\n",
      "[Training Epoch 2] Batch 572, Loss 0.28379663825035095\n",
      "[Training Epoch 2] Batch 573, Loss 0.30562466382980347\n",
      "[Training Epoch 2] Batch 574, Loss 0.2879062294960022\n",
      "[Training Epoch 2] Batch 575, Loss 0.284989595413208\n",
      "[Training Epoch 2] Batch 576, Loss 0.30229127407073975\n",
      "[Training Epoch 2] Batch 577, Loss 0.23659177124500275\n",
      "[Training Epoch 2] Batch 578, Loss 0.2727435529232025\n",
      "[Training Epoch 2] Batch 579, Loss 0.27653253078460693\n",
      "[Training Epoch 2] Batch 580, Loss 0.289224237203598\n",
      "[Training Epoch 2] Batch 581, Loss 0.2655984163284302\n",
      "[Training Epoch 2] Batch 582, Loss 0.2772418260574341\n",
      "[Training Epoch 2] Batch 583, Loss 0.30141085386276245\n",
      "[Training Epoch 2] Batch 584, Loss 0.2702544927597046\n",
      "[Training Epoch 2] Batch 585, Loss 0.2723013162612915\n",
      "[Training Epoch 2] Batch 586, Loss 0.2783607840538025\n",
      "[Training Epoch 2] Batch 587, Loss 0.2963811159133911\n",
      "[Training Epoch 2] Batch 588, Loss 0.28931382298469543\n",
      "[Training Epoch 2] Batch 589, Loss 0.2734571099281311\n",
      "[Training Epoch 2] Batch 590, Loss 0.30466511845588684\n",
      "[Training Epoch 2] Batch 591, Loss 0.26965194940567017\n",
      "[Training Epoch 2] Batch 592, Loss 0.2980797290802002\n",
      "[Training Epoch 2] Batch 593, Loss 0.29486674070358276\n",
      "[Training Epoch 2] Batch 594, Loss 0.25667622685432434\n",
      "[Training Epoch 2] Batch 595, Loss 0.2729846239089966\n",
      "[Training Epoch 2] Batch 596, Loss 0.2490425407886505\n",
      "[Training Epoch 2] Batch 597, Loss 0.2800033688545227\n",
      "[Training Epoch 2] Batch 598, Loss 0.29737281799316406\n",
      "[Training Epoch 2] Batch 599, Loss 0.2581821382045746\n",
      "[Training Epoch 2] Batch 600, Loss 0.32907381653785706\n",
      "[Training Epoch 2] Batch 601, Loss 0.2696099579334259\n",
      "[Training Epoch 2] Batch 602, Loss 0.2685931921005249\n",
      "[Training Epoch 2] Batch 603, Loss 0.31805670261383057\n",
      "[Training Epoch 2] Batch 604, Loss 0.29139676690101624\n",
      "[Training Epoch 2] Batch 605, Loss 0.2602212131023407\n",
      "[Training Epoch 2] Batch 606, Loss 0.2936677932739258\n",
      "[Training Epoch 2] Batch 607, Loss 0.28776758909225464\n",
      "[Training Epoch 2] Batch 608, Loss 0.2534253001213074\n",
      "[Training Epoch 2] Batch 609, Loss 0.29178744554519653\n",
      "[Training Epoch 2] Batch 610, Loss 0.30642616748809814\n",
      "[Training Epoch 2] Batch 611, Loss 0.27119290828704834\n",
      "[Training Epoch 2] Batch 612, Loss 0.28550171852111816\n",
      "[Training Epoch 2] Batch 613, Loss 0.27772796154022217\n",
      "[Training Epoch 2] Batch 614, Loss 0.27099791169166565\n",
      "[Training Epoch 2] Batch 615, Loss 0.27490389347076416\n",
      "[Training Epoch 2] Batch 616, Loss 0.29305654764175415\n",
      "[Training Epoch 2] Batch 617, Loss 0.27363014221191406\n",
      "[Training Epoch 2] Batch 618, Loss 0.2777358293533325\n",
      "[Training Epoch 2] Batch 619, Loss 0.28802093863487244\n",
      "[Training Epoch 2] Batch 620, Loss 0.2862415909767151\n",
      "[Training Epoch 2] Batch 621, Loss 0.29755300283432007\n",
      "[Training Epoch 2] Batch 622, Loss 0.3003290593624115\n",
      "[Training Epoch 2] Batch 623, Loss 0.31569546461105347\n",
      "[Training Epoch 2] Batch 624, Loss 0.28113555908203125\n",
      "[Training Epoch 2] Batch 625, Loss 0.2839106321334839\n",
      "[Training Epoch 2] Batch 626, Loss 0.2898837924003601\n",
      "[Training Epoch 2] Batch 627, Loss 0.2662157416343689\n",
      "[Training Epoch 2] Batch 628, Loss 0.26591163873672485\n",
      "[Training Epoch 2] Batch 629, Loss 0.28566086292266846\n",
      "[Training Epoch 2] Batch 630, Loss 0.26674315333366394\n",
      "[Training Epoch 2] Batch 631, Loss 0.293705552816391\n",
      "[Training Epoch 2] Batch 632, Loss 0.29683953523635864\n",
      "[Training Epoch 2] Batch 633, Loss 0.29834455251693726\n",
      "[Training Epoch 2] Batch 634, Loss 0.28868624567985535\n",
      "[Training Epoch 2] Batch 635, Loss 0.2666088938713074\n",
      "[Training Epoch 2] Batch 636, Loss 0.257016658782959\n",
      "[Training Epoch 2] Batch 637, Loss 0.27352374792099\n",
      "[Training Epoch 2] Batch 638, Loss 0.2892796993255615\n",
      "[Training Epoch 2] Batch 639, Loss 0.2986653447151184\n",
      "[Training Epoch 2] Batch 640, Loss 0.2826412320137024\n",
      "[Training Epoch 2] Batch 641, Loss 0.2905887961387634\n",
      "[Training Epoch 2] Batch 642, Loss 0.2670537233352661\n",
      "[Training Epoch 2] Batch 643, Loss 0.2769685685634613\n",
      "[Training Epoch 2] Batch 644, Loss 0.2601228356361389\n",
      "[Training Epoch 2] Batch 645, Loss 0.2748623490333557\n",
      "[Training Epoch 2] Batch 646, Loss 0.28381022810935974\n",
      "[Training Epoch 2] Batch 647, Loss 0.2614094913005829\n",
      "[Training Epoch 2] Batch 648, Loss 0.301786869764328\n",
      "[Training Epoch 2] Batch 649, Loss 0.2844732403755188\n",
      "[Training Epoch 2] Batch 650, Loss 0.28026553988456726\n",
      "[Training Epoch 2] Batch 651, Loss 0.2727423310279846\n",
      "[Training Epoch 2] Batch 652, Loss 0.276780903339386\n",
      "[Training Epoch 2] Batch 653, Loss 0.307190865278244\n",
      "[Training Epoch 2] Batch 654, Loss 0.28064826130867004\n",
      "[Training Epoch 2] Batch 655, Loss 0.31565937399864197\n",
      "[Training Epoch 2] Batch 656, Loss 0.2779879570007324\n",
      "[Training Epoch 2] Batch 657, Loss 0.27566957473754883\n",
      "[Training Epoch 2] Batch 658, Loss 0.28606873750686646\n",
      "[Training Epoch 2] Batch 659, Loss 0.2623412311077118\n",
      "[Training Epoch 2] Batch 660, Loss 0.2667357623577118\n",
      "[Training Epoch 2] Batch 661, Loss 0.30087628960609436\n",
      "[Training Epoch 2] Batch 662, Loss 0.2910078763961792\n",
      "[Training Epoch 2] Batch 663, Loss 0.2923118770122528\n",
      "[Training Epoch 2] Batch 664, Loss 0.301474392414093\n",
      "[Training Epoch 2] Batch 665, Loss 0.3162711262702942\n",
      "[Training Epoch 2] Batch 666, Loss 0.28635841608047485\n",
      "[Training Epoch 2] Batch 667, Loss 0.2878968119621277\n",
      "[Training Epoch 2] Batch 668, Loss 0.3018762767314911\n",
      "[Training Epoch 2] Batch 669, Loss 0.26987963914871216\n",
      "[Training Epoch 2] Batch 670, Loss 0.2803767919540405\n",
      "[Training Epoch 2] Batch 671, Loss 0.2978089451789856\n",
      "[Training Epoch 2] Batch 672, Loss 0.30645132064819336\n",
      "[Training Epoch 2] Batch 673, Loss 0.29307687282562256\n",
      "[Training Epoch 2] Batch 674, Loss 0.27926501631736755\n",
      "[Training Epoch 2] Batch 675, Loss 0.2698403000831604\n",
      "[Training Epoch 2] Batch 676, Loss 0.28288355469703674\n",
      "[Training Epoch 2] Batch 677, Loss 0.2801377773284912\n",
      "[Training Epoch 2] Batch 678, Loss 0.2729504108428955\n",
      "[Training Epoch 2] Batch 679, Loss 0.261325478553772\n",
      "[Training Epoch 2] Batch 680, Loss 0.30751022696495056\n",
      "[Training Epoch 2] Batch 681, Loss 0.2942662537097931\n",
      "[Training Epoch 2] Batch 682, Loss 0.2635335326194763\n",
      "[Training Epoch 2] Batch 683, Loss 0.2865182161331177\n",
      "[Training Epoch 2] Batch 684, Loss 0.3178144097328186\n",
      "[Training Epoch 2] Batch 685, Loss 0.2794940769672394\n",
      "[Training Epoch 2] Batch 686, Loss 0.2685854136943817\n",
      "[Training Epoch 2] Batch 687, Loss 0.29669272899627686\n",
      "[Training Epoch 2] Batch 688, Loss 0.28342387080192566\n",
      "[Training Epoch 2] Batch 689, Loss 0.29837003350257874\n",
      "[Training Epoch 2] Batch 690, Loss 0.30392876267433167\n",
      "[Training Epoch 2] Batch 691, Loss 0.3119666874408722\n",
      "[Training Epoch 2] Batch 692, Loss 0.29913079738616943\n",
      "[Training Epoch 2] Batch 693, Loss 0.29290616512298584\n",
      "[Training Epoch 2] Batch 694, Loss 0.2550451159477234\n",
      "[Training Epoch 2] Batch 695, Loss 0.30834078788757324\n",
      "[Training Epoch 2] Batch 696, Loss 0.2592295706272125\n",
      "[Training Epoch 2] Batch 697, Loss 0.26808565855026245\n",
      "[Training Epoch 2] Batch 698, Loss 0.28902244567871094\n",
      "[Training Epoch 2] Batch 699, Loss 0.28177666664123535\n",
      "[Training Epoch 2] Batch 700, Loss 0.2804161012172699\n",
      "[Training Epoch 2] Batch 701, Loss 0.2849229574203491\n",
      "[Training Epoch 2] Batch 702, Loss 0.28038668632507324\n",
      "[Training Epoch 2] Batch 703, Loss 0.2942776083946228\n",
      "[Training Epoch 2] Batch 704, Loss 0.2500090003013611\n",
      "[Training Epoch 2] Batch 705, Loss 0.28124094009399414\n",
      "[Training Epoch 2] Batch 706, Loss 0.3054327368736267\n",
      "[Training Epoch 2] Batch 707, Loss 0.27646404504776\n",
      "[Training Epoch 2] Batch 708, Loss 0.26944857835769653\n",
      "[Training Epoch 2] Batch 709, Loss 0.2834201455116272\n",
      "[Training Epoch 2] Batch 710, Loss 0.28839367628097534\n",
      "[Training Epoch 2] Batch 711, Loss 0.2812158465385437\n",
      "[Training Epoch 2] Batch 712, Loss 0.29172444343566895\n",
      "[Training Epoch 2] Batch 713, Loss 0.2770668566226959\n",
      "[Training Epoch 2] Batch 714, Loss 0.2960401177406311\n",
      "[Training Epoch 2] Batch 715, Loss 0.2869560122489929\n",
      "[Training Epoch 2] Batch 716, Loss 0.2732195556163788\n",
      "[Training Epoch 2] Batch 717, Loss 0.29081863164901733\n",
      "[Training Epoch 2] Batch 718, Loss 0.3092082142829895\n",
      "[Training Epoch 2] Batch 719, Loss 0.28918641805648804\n",
      "[Training Epoch 2] Batch 720, Loss 0.24989455938339233\n",
      "[Training Epoch 2] Batch 721, Loss 0.2925611138343811\n",
      "[Training Epoch 2] Batch 722, Loss 0.2790220379829407\n",
      "[Training Epoch 2] Batch 723, Loss 0.2696523070335388\n",
      "[Training Epoch 2] Batch 724, Loss 0.3299179673194885\n",
      "[Training Epoch 2] Batch 725, Loss 0.2677192687988281\n",
      "[Training Epoch 2] Batch 726, Loss 0.2522740960121155\n",
      "[Training Epoch 2] Batch 727, Loss 0.28833484649658203\n",
      "[Training Epoch 2] Batch 728, Loss 0.2681530714035034\n",
      "[Training Epoch 2] Batch 729, Loss 0.2918406128883362\n",
      "[Training Epoch 2] Batch 730, Loss 0.28550204634666443\n",
      "[Training Epoch 2] Batch 731, Loss 0.2938767671585083\n",
      "[Training Epoch 2] Batch 732, Loss 0.29309016466140747\n",
      "[Training Epoch 2] Batch 733, Loss 0.30198246240615845\n",
      "[Training Epoch 2] Batch 734, Loss 0.2730947732925415\n",
      "[Training Epoch 2] Batch 735, Loss 0.28847551345825195\n",
      "[Training Epoch 2] Batch 736, Loss 0.2963626980781555\n",
      "[Training Epoch 2] Batch 737, Loss 0.26773351430892944\n",
      "[Training Epoch 2] Batch 738, Loss 0.2734706997871399\n",
      "[Training Epoch 2] Batch 739, Loss 0.30830809473991394\n",
      "[Training Epoch 2] Batch 740, Loss 0.29345637559890747\n",
      "[Training Epoch 2] Batch 741, Loss 0.24483242630958557\n",
      "[Training Epoch 2] Batch 742, Loss 0.2744041085243225\n",
      "[Training Epoch 2] Batch 743, Loss 0.2913379669189453\n",
      "[Training Epoch 2] Batch 744, Loss 0.23545020818710327\n",
      "[Training Epoch 2] Batch 745, Loss 0.26319044828414917\n",
      "[Training Epoch 2] Batch 746, Loss 0.2921874523162842\n",
      "[Training Epoch 2] Batch 747, Loss 0.29331186413764954\n",
      "[Training Epoch 2] Batch 748, Loss 0.29819047451019287\n",
      "[Training Epoch 2] Batch 749, Loss 0.2708907723426819\n",
      "[Training Epoch 2] Batch 750, Loss 0.2800371050834656\n",
      "[Training Epoch 2] Batch 751, Loss 0.26612377166748047\n",
      "[Training Epoch 2] Batch 752, Loss 0.26591551303863525\n",
      "[Training Epoch 2] Batch 753, Loss 0.29338693618774414\n",
      "[Training Epoch 2] Batch 754, Loss 0.2532000243663788\n",
      "[Training Epoch 2] Batch 755, Loss 0.2965127229690552\n",
      "[Training Epoch 2] Batch 756, Loss 0.2769679129123688\n",
      "[Training Epoch 2] Batch 757, Loss 0.30272147059440613\n",
      "[Training Epoch 2] Batch 758, Loss 0.286374032497406\n",
      "[Training Epoch 2] Batch 759, Loss 0.28723961114883423\n",
      "[Training Epoch 2] Batch 760, Loss 0.3089211583137512\n",
      "[Training Epoch 2] Batch 761, Loss 0.2844093143939972\n",
      "[Training Epoch 2] Batch 762, Loss 0.2711121439933777\n",
      "[Training Epoch 2] Batch 763, Loss 0.2941688895225525\n",
      "[Training Epoch 2] Batch 764, Loss 0.29410457611083984\n",
      "[Training Epoch 2] Batch 765, Loss 0.28277134895324707\n",
      "[Training Epoch 2] Batch 766, Loss 0.2832844853401184\n",
      "[Training Epoch 2] Batch 767, Loss 0.28104931116104126\n",
      "[Training Epoch 2] Batch 768, Loss 0.29843801259994507\n",
      "[Training Epoch 2] Batch 769, Loss 0.2867624759674072\n",
      "[Training Epoch 2] Batch 770, Loss 0.28665798902511597\n",
      "[Training Epoch 2] Batch 771, Loss 0.260306715965271\n",
      "[Training Epoch 2] Batch 772, Loss 0.2852368950843811\n",
      "[Training Epoch 2] Batch 773, Loss 0.2558801472187042\n",
      "[Training Epoch 2] Batch 774, Loss 0.26648396253585815\n",
      "[Training Epoch 2] Batch 775, Loss 0.2700567841529846\n",
      "[Training Epoch 2] Batch 776, Loss 0.2932215929031372\n",
      "[Training Epoch 2] Batch 777, Loss 0.2753685712814331\n",
      "[Training Epoch 2] Batch 778, Loss 0.30996978282928467\n",
      "[Training Epoch 2] Batch 779, Loss 0.24931302666664124\n",
      "[Training Epoch 2] Batch 780, Loss 0.27822792530059814\n",
      "[Training Epoch 2] Batch 781, Loss 0.2650066912174225\n",
      "[Training Epoch 2] Batch 782, Loss 0.2721262574195862\n",
      "[Training Epoch 2] Batch 783, Loss 0.2679235339164734\n",
      "[Training Epoch 2] Batch 784, Loss 0.29823485016822815\n",
      "[Training Epoch 2] Batch 785, Loss 0.29721659421920776\n",
      "[Training Epoch 2] Batch 786, Loss 0.28519731760025024\n",
      "[Training Epoch 2] Batch 787, Loss 0.3034171462059021\n",
      "[Training Epoch 2] Batch 788, Loss 0.2733882665634155\n",
      "[Training Epoch 2] Batch 789, Loss 0.29567527770996094\n",
      "[Training Epoch 2] Batch 790, Loss 0.2509276866912842\n",
      "[Training Epoch 2] Batch 791, Loss 0.2732287049293518\n",
      "[Training Epoch 2] Batch 792, Loss 0.2964773178100586\n",
      "[Training Epoch 2] Batch 793, Loss 0.25637853145599365\n",
      "[Training Epoch 2] Batch 794, Loss 0.2729146480560303\n",
      "[Training Epoch 2] Batch 795, Loss 0.28651660680770874\n",
      "[Training Epoch 2] Batch 796, Loss 0.2884596884250641\n",
      "[Training Epoch 2] Batch 797, Loss 0.2924683094024658\n",
      "[Training Epoch 2] Batch 798, Loss 0.27015647292137146\n",
      "[Training Epoch 2] Batch 799, Loss 0.2724945843219757\n",
      "[Training Epoch 2] Batch 800, Loss 0.27759402990341187\n",
      "[Training Epoch 2] Batch 801, Loss 0.2675706446170807\n",
      "[Training Epoch 2] Batch 802, Loss 0.27759310603141785\n",
      "[Training Epoch 2] Batch 803, Loss 0.274591863155365\n",
      "[Training Epoch 2] Batch 804, Loss 0.28638535737991333\n",
      "[Training Epoch 2] Batch 805, Loss 0.2778511643409729\n",
      "[Training Epoch 2] Batch 806, Loss 0.2809009253978729\n",
      "[Training Epoch 2] Batch 807, Loss 0.3035154342651367\n",
      "[Training Epoch 2] Batch 808, Loss 0.3064899742603302\n",
      "[Training Epoch 2] Batch 809, Loss 0.2678503096103668\n",
      "[Training Epoch 2] Batch 810, Loss 0.2790968716144562\n",
      "[Training Epoch 2] Batch 811, Loss 0.29833686351776123\n",
      "[Training Epoch 2] Batch 812, Loss 0.27496859431266785\n",
      "[Training Epoch 2] Batch 813, Loss 0.23736919462680817\n",
      "[Training Epoch 2] Batch 814, Loss 0.2844052314758301\n",
      "[Training Epoch 2] Batch 815, Loss 0.322542667388916\n",
      "[Training Epoch 2] Batch 816, Loss 0.2835586965084076\n",
      "[Training Epoch 2] Batch 817, Loss 0.3068822920322418\n",
      "[Training Epoch 2] Batch 818, Loss 0.2696726620197296\n",
      "[Training Epoch 2] Batch 819, Loss 0.2910400629043579\n",
      "[Training Epoch 2] Batch 820, Loss 0.27035075426101685\n",
      "[Training Epoch 2] Batch 821, Loss 0.29512330889701843\n",
      "[Training Epoch 2] Batch 822, Loss 0.2933952808380127\n",
      "[Training Epoch 2] Batch 823, Loss 0.2947985529899597\n",
      "[Training Epoch 2] Batch 824, Loss 0.28058651089668274\n",
      "[Training Epoch 2] Batch 825, Loss 0.2807536721229553\n",
      "[Training Epoch 2] Batch 826, Loss 0.29223567247390747\n",
      "[Training Epoch 2] Batch 827, Loss 0.2747950255870819\n",
      "[Training Epoch 2] Batch 828, Loss 0.29201793670654297\n",
      "[Training Epoch 2] Batch 829, Loss 0.2875296175479889\n",
      "[Training Epoch 2] Batch 830, Loss 0.2977827489376068\n",
      "[Training Epoch 2] Batch 831, Loss 0.2870393693447113\n",
      "[Training Epoch 2] Batch 832, Loss 0.28251346945762634\n",
      "[Training Epoch 2] Batch 833, Loss 0.2700193524360657\n",
      "[Training Epoch 2] Batch 834, Loss 0.28283458948135376\n",
      "[Training Epoch 2] Batch 835, Loss 0.31724077463150024\n",
      "[Training Epoch 2] Batch 836, Loss 0.281353235244751\n",
      "[Training Epoch 2] Batch 837, Loss 0.3050801455974579\n",
      "[Training Epoch 2] Batch 838, Loss 0.30509549379348755\n",
      "[Training Epoch 2] Batch 839, Loss 0.31717273592948914\n",
      "[Training Epoch 2] Batch 840, Loss 0.27066925168037415\n",
      "[Training Epoch 2] Batch 841, Loss 0.2845156788825989\n",
      "[Training Epoch 2] Batch 842, Loss 0.25937867164611816\n",
      "[Training Epoch 2] Batch 843, Loss 0.2691706418991089\n",
      "[Training Epoch 2] Batch 844, Loss 0.2813768982887268\n",
      "[Training Epoch 2] Batch 845, Loss 0.2824445068836212\n",
      "[Training Epoch 2] Batch 846, Loss 0.26998886466026306\n",
      "[Training Epoch 2] Batch 847, Loss 0.268598735332489\n",
      "[Training Epoch 2] Batch 848, Loss 0.28393739461898804\n",
      "[Training Epoch 2] Batch 849, Loss 0.2871238589286804\n",
      "[Training Epoch 2] Batch 850, Loss 0.25229325890541077\n",
      "[Training Epoch 2] Batch 851, Loss 0.2954261898994446\n",
      "[Training Epoch 2] Batch 852, Loss 0.25649672746658325\n",
      "[Training Epoch 2] Batch 853, Loss 0.2740764915943146\n",
      "[Training Epoch 2] Batch 854, Loss 0.266554057598114\n",
      "[Training Epoch 2] Batch 855, Loss 0.27318283915519714\n",
      "[Training Epoch 2] Batch 856, Loss 0.27195388078689575\n",
      "[Training Epoch 2] Batch 857, Loss 0.28949469327926636\n",
      "[Training Epoch 2] Batch 858, Loss 0.26073187589645386\n",
      "[Training Epoch 2] Batch 859, Loss 0.3362528681755066\n",
      "[Training Epoch 2] Batch 860, Loss 0.28233107924461365\n",
      "[Training Epoch 2] Batch 861, Loss 0.3092491924762726\n",
      "[Training Epoch 2] Batch 862, Loss 0.26527300477027893\n",
      "[Training Epoch 2] Batch 863, Loss 0.28151923418045044\n",
      "[Training Epoch 2] Batch 864, Loss 0.27485236525535583\n",
      "[Training Epoch 2] Batch 865, Loss 0.283071905374527\n",
      "[Training Epoch 2] Batch 866, Loss 0.2934792637825012\n",
      "[Training Epoch 2] Batch 867, Loss 0.31336456537246704\n",
      "[Training Epoch 2] Batch 868, Loss 0.2732861340045929\n",
      "[Training Epoch 2] Batch 869, Loss 0.2928421199321747\n",
      "[Training Epoch 2] Batch 870, Loss 0.2791014313697815\n",
      "[Training Epoch 2] Batch 871, Loss 0.3104189336299896\n",
      "[Training Epoch 2] Batch 872, Loss 0.30484628677368164\n",
      "[Training Epoch 2] Batch 873, Loss 0.3007550835609436\n",
      "[Training Epoch 2] Batch 874, Loss 0.26691216230392456\n",
      "[Training Epoch 2] Batch 875, Loss 0.28907594084739685\n",
      "[Training Epoch 2] Batch 876, Loss 0.27037543058395386\n",
      "[Training Epoch 2] Batch 877, Loss 0.31186026334762573\n",
      "[Training Epoch 2] Batch 878, Loss 0.27579861879348755\n",
      "[Training Epoch 2] Batch 879, Loss 0.3098219037055969\n",
      "[Training Epoch 2] Batch 880, Loss 0.28924307227134705\n",
      "[Training Epoch 2] Batch 881, Loss 0.24297499656677246\n",
      "[Training Epoch 2] Batch 882, Loss 0.2446872740983963\n",
      "[Training Epoch 2] Batch 883, Loss 0.27494075894355774\n",
      "[Training Epoch 2] Batch 884, Loss 0.27172279357910156\n",
      "[Training Epoch 2] Batch 885, Loss 0.2629268765449524\n",
      "[Training Epoch 2] Batch 886, Loss 0.2657184600830078\n",
      "[Training Epoch 2] Batch 887, Loss 0.2814226448535919\n",
      "[Training Epoch 2] Batch 888, Loss 0.2883577346801758\n",
      "[Training Epoch 2] Batch 889, Loss 0.3125573992729187\n",
      "[Training Epoch 2] Batch 890, Loss 0.2847389876842499\n",
      "[Training Epoch 2] Batch 891, Loss 0.27887529134750366\n",
      "[Training Epoch 2] Batch 892, Loss 0.31783872842788696\n",
      "[Training Epoch 2] Batch 893, Loss 0.33583056926727295\n",
      "[Training Epoch 2] Batch 894, Loss 0.2709616422653198\n",
      "[Training Epoch 2] Batch 895, Loss 0.27939003705978394\n",
      "[Training Epoch 2] Batch 896, Loss 0.2718160152435303\n",
      "[Training Epoch 2] Batch 897, Loss 0.303839772939682\n",
      "[Training Epoch 2] Batch 898, Loss 0.24678605794906616\n",
      "[Training Epoch 2] Batch 899, Loss 0.26956433057785034\n",
      "[Training Epoch 2] Batch 900, Loss 0.2938825786113739\n",
      "[Training Epoch 2] Batch 901, Loss 0.27805840969085693\n",
      "[Training Epoch 2] Batch 902, Loss 0.2671947479248047\n",
      "[Training Epoch 2] Batch 903, Loss 0.2933647632598877\n",
      "[Training Epoch 2] Batch 904, Loss 0.24831616878509521\n",
      "[Training Epoch 2] Batch 905, Loss 0.3096449375152588\n",
      "[Training Epoch 2] Batch 906, Loss 0.29389336705207825\n",
      "[Training Epoch 2] Batch 907, Loss 0.2613220512866974\n",
      "[Training Epoch 2] Batch 908, Loss 0.300653874874115\n",
      "[Training Epoch 2] Batch 909, Loss 0.28781503438949585\n",
      "[Training Epoch 2] Batch 910, Loss 0.27490687370300293\n",
      "[Training Epoch 2] Batch 911, Loss 0.3029874563217163\n",
      "[Training Epoch 2] Batch 912, Loss 0.3219585418701172\n",
      "[Training Epoch 2] Batch 913, Loss 0.2739689350128174\n",
      "[Training Epoch 2] Batch 914, Loss 0.3091466426849365\n",
      "[Training Epoch 2] Batch 915, Loss 0.3077591061592102\n",
      "[Training Epoch 2] Batch 916, Loss 0.30671757459640503\n",
      "[Training Epoch 2] Batch 917, Loss 0.2598838210105896\n",
      "[Training Epoch 2] Batch 918, Loss 0.27029162645339966\n",
      "[Training Epoch 2] Batch 919, Loss 0.2907700836658478\n",
      "[Training Epoch 2] Batch 920, Loss 0.2687516510486603\n",
      "[Training Epoch 2] Batch 921, Loss 0.3032207489013672\n",
      "[Training Epoch 2] Batch 922, Loss 0.32010284066200256\n",
      "[Training Epoch 2] Batch 923, Loss 0.2625443935394287\n",
      "[Training Epoch 2] Batch 924, Loss 0.29682087898254395\n",
      "[Training Epoch 2] Batch 925, Loss 0.30879953503608704\n",
      "[Training Epoch 2] Batch 926, Loss 0.3028731048107147\n",
      "[Training Epoch 2] Batch 927, Loss 0.3079442083835602\n",
      "[Training Epoch 2] Batch 928, Loss 0.31570878624916077\n",
      "[Training Epoch 2] Batch 929, Loss 0.2972176671028137\n",
      "[Training Epoch 2] Batch 930, Loss 0.27634429931640625\n",
      "[Training Epoch 2] Batch 931, Loss 0.28590595722198486\n",
      "[Training Epoch 2] Batch 932, Loss 0.30805790424346924\n",
      "[Training Epoch 2] Batch 933, Loss 0.24345533549785614\n",
      "[Training Epoch 2] Batch 934, Loss 0.27834123373031616\n",
      "[Training Epoch 2] Batch 935, Loss 0.2891848683357239\n",
      "[Training Epoch 2] Batch 936, Loss 0.26863792538642883\n",
      "[Training Epoch 2] Batch 937, Loss 0.2784597873687744\n",
      "[Training Epoch 2] Batch 938, Loss 0.249486044049263\n",
      "[Training Epoch 2] Batch 939, Loss 0.2806493043899536\n",
      "[Training Epoch 2] Batch 940, Loss 0.27836427092552185\n",
      "[Training Epoch 2] Batch 941, Loss 0.23550346493721008\n",
      "[Training Epoch 2] Batch 942, Loss 0.3055543899536133\n",
      "[Training Epoch 2] Batch 943, Loss 0.29378896951675415\n",
      "[Training Epoch 2] Batch 944, Loss 0.26680988073349\n",
      "[Training Epoch 2] Batch 945, Loss 0.2852824926376343\n",
      "[Training Epoch 2] Batch 946, Loss 0.27532505989074707\n",
      "[Training Epoch 2] Batch 947, Loss 0.2838473320007324\n",
      "[Training Epoch 2] Batch 948, Loss 0.3064180314540863\n",
      "[Training Epoch 2] Batch 949, Loss 0.29421401023864746\n",
      "[Training Epoch 2] Batch 950, Loss 0.2694600820541382\n",
      "[Training Epoch 2] Batch 951, Loss 0.24596701562404633\n",
      "[Training Epoch 2] Batch 952, Loss 0.27995866537094116\n",
      "[Training Epoch 2] Batch 953, Loss 0.31649404764175415\n",
      "[Training Epoch 2] Batch 954, Loss 0.2941054701805115\n",
      "[Training Epoch 2] Batch 955, Loss 0.30084913969039917\n",
      "[Training Epoch 2] Batch 956, Loss 0.300150603055954\n",
      "[Training Epoch 2] Batch 957, Loss 0.28703394532203674\n",
      "[Training Epoch 2] Batch 958, Loss 0.31632283329963684\n",
      "[Training Epoch 2] Batch 959, Loss 0.2743668854236603\n",
      "[Training Epoch 2] Batch 960, Loss 0.32511264085769653\n",
      "[Training Epoch 2] Batch 961, Loss 0.26986008882522583\n",
      "[Training Epoch 2] Batch 962, Loss 0.2729409337043762\n",
      "[Training Epoch 2] Batch 963, Loss 0.29594409465789795\n",
      "[Training Epoch 2] Batch 964, Loss 0.2903682589530945\n",
      "[Training Epoch 2] Batch 965, Loss 0.29287511110305786\n",
      "[Training Epoch 2] Batch 966, Loss 0.29591190814971924\n",
      "[Training Epoch 2] Batch 967, Loss 0.30167531967163086\n",
      "[Training Epoch 2] Batch 968, Loss 0.2734680771827698\n",
      "[Training Epoch 2] Batch 969, Loss 0.27773362398147583\n",
      "[Training Epoch 2] Batch 970, Loss 0.27879899740219116\n",
      "[Training Epoch 2] Batch 971, Loss 0.29324182868003845\n",
      "[Training Epoch 2] Batch 972, Loss 0.2687641382217407\n",
      "[Training Epoch 2] Batch 973, Loss 0.256634920835495\n",
      "[Training Epoch 2] Batch 974, Loss 0.2664075791835785\n",
      "[Training Epoch 2] Batch 975, Loss 0.29189372062683105\n",
      "[Training Epoch 2] Batch 976, Loss 0.25783973932266235\n",
      "[Training Epoch 2] Batch 977, Loss 0.29836875200271606\n",
      "[Training Epoch 2] Batch 978, Loss 0.277995228767395\n",
      "[Training Epoch 2] Batch 979, Loss 0.2709845006465912\n",
      "[Training Epoch 2] Batch 980, Loss 0.2554692029953003\n",
      "[Training Epoch 2] Batch 981, Loss 0.2712544798851013\n",
      "[Training Epoch 2] Batch 982, Loss 0.24254846572875977\n",
      "[Training Epoch 2] Batch 983, Loss 0.28262782096862793\n",
      "[Training Epoch 2] Batch 984, Loss 0.2747543752193451\n",
      "[Training Epoch 2] Batch 985, Loss 0.2614818513393402\n",
      "[Training Epoch 2] Batch 986, Loss 0.25610390305519104\n",
      "[Training Epoch 2] Batch 987, Loss 0.2665524184703827\n",
      "[Training Epoch 2] Batch 988, Loss 0.30492934584617615\n",
      "[Training Epoch 2] Batch 989, Loss 0.2832739055156708\n",
      "[Training Epoch 2] Batch 990, Loss 0.2782081961631775\n",
      "[Training Epoch 2] Batch 991, Loss 0.3064027428627014\n",
      "[Training Epoch 2] Batch 992, Loss 0.3154919743537903\n",
      "[Training Epoch 2] Batch 993, Loss 0.27927759289741516\n",
      "[Training Epoch 2] Batch 994, Loss 0.2590436339378357\n",
      "[Training Epoch 2] Batch 995, Loss 0.2869391441345215\n",
      "[Training Epoch 2] Batch 996, Loss 0.2825203239917755\n",
      "[Training Epoch 2] Batch 997, Loss 0.2752426266670227\n",
      "[Training Epoch 2] Batch 998, Loss 0.27059563994407654\n",
      "[Training Epoch 2] Batch 999, Loss 0.2971816658973694\n",
      "[Training Epoch 2] Batch 1000, Loss 0.30970877408981323\n",
      "[Training Epoch 2] Batch 1001, Loss 0.253842294216156\n",
      "[Training Epoch 2] Batch 1002, Loss 0.3195449113845825\n",
      "[Training Epoch 2] Batch 1003, Loss 0.27780207991600037\n",
      "[Training Epoch 2] Batch 1004, Loss 0.29350778460502625\n",
      "[Training Epoch 2] Batch 1005, Loss 0.2683452069759369\n",
      "[Training Epoch 2] Batch 1006, Loss 0.27535194158554077\n",
      "[Training Epoch 2] Batch 1007, Loss 0.26136836409568787\n",
      "[Training Epoch 2] Batch 1008, Loss 0.2510318160057068\n",
      "[Training Epoch 2] Batch 1009, Loss 0.2738555669784546\n",
      "[Training Epoch 2] Batch 1010, Loss 0.2985513508319855\n",
      "[Training Epoch 2] Batch 1011, Loss 0.27603504061698914\n",
      "[Training Epoch 2] Batch 1012, Loss 0.31540176272392273\n",
      "[Training Epoch 2] Batch 1013, Loss 0.25274741649627686\n",
      "[Training Epoch 2] Batch 1014, Loss 0.2986326813697815\n",
      "[Training Epoch 2] Batch 1015, Loss 0.2892919182777405\n",
      "[Training Epoch 2] Batch 1016, Loss 0.29226240515708923\n",
      "[Training Epoch 2] Batch 1017, Loss 0.26691845059394836\n",
      "[Training Epoch 2] Batch 1018, Loss 0.2506689429283142\n",
      "[Training Epoch 2] Batch 1019, Loss 0.2648405432701111\n",
      "[Training Epoch 2] Batch 1020, Loss 0.3023381233215332\n",
      "[Training Epoch 2] Batch 1021, Loss 0.2931252121925354\n",
      "[Training Epoch 2] Batch 1022, Loss 0.2927490770816803\n",
      "[Training Epoch 2] Batch 1023, Loss 0.291132390499115\n",
      "[Training Epoch 2] Batch 1024, Loss 0.28952354192733765\n",
      "[Training Epoch 2] Batch 1025, Loss 0.3112468719482422\n",
      "[Training Epoch 2] Batch 1026, Loss 0.30278944969177246\n",
      "[Training Epoch 2] Batch 1027, Loss 0.2527446746826172\n",
      "[Training Epoch 2] Batch 1028, Loss 0.26854947209358215\n",
      "[Training Epoch 2] Batch 1029, Loss 0.2828187942504883\n",
      "[Training Epoch 2] Batch 1030, Loss 0.2659665048122406\n",
      "[Training Epoch 2] Batch 1031, Loss 0.2727581262588501\n",
      "[Training Epoch 2] Batch 1032, Loss 0.28071391582489014\n",
      "[Training Epoch 2] Batch 1033, Loss 0.3092982769012451\n",
      "[Training Epoch 2] Batch 1034, Loss 0.2796070873737335\n",
      "[Training Epoch 2] Batch 1035, Loss 0.2766014039516449\n",
      "[Training Epoch 2] Batch 1036, Loss 0.28679680824279785\n",
      "[Training Epoch 2] Batch 1037, Loss 0.29736030101776123\n",
      "[Training Epoch 2] Batch 1038, Loss 0.2835693955421448\n",
      "[Training Epoch 2] Batch 1039, Loss 0.25208812952041626\n",
      "[Training Epoch 2] Batch 1040, Loss 0.28429555892944336\n",
      "[Training Epoch 2] Batch 1041, Loss 0.2756339907646179\n",
      "[Training Epoch 2] Batch 1042, Loss 0.2754709720611572\n",
      "[Training Epoch 2] Batch 1043, Loss 0.26798367500305176\n",
      "[Training Epoch 2] Batch 1044, Loss 0.31858891248703003\n",
      "[Training Epoch 2] Batch 1045, Loss 0.27435433864593506\n",
      "[Training Epoch 2] Batch 1046, Loss 0.33280330896377563\n",
      "[Training Epoch 2] Batch 1047, Loss 0.263058602809906\n",
      "[Training Epoch 2] Batch 1048, Loss 0.2946748733520508\n",
      "[Training Epoch 2] Batch 1049, Loss 0.30735325813293457\n",
      "[Training Epoch 2] Batch 1050, Loss 0.2905152440071106\n",
      "[Training Epoch 2] Batch 1051, Loss 0.2793179750442505\n",
      "[Training Epoch 2] Batch 1052, Loss 0.2982959747314453\n",
      "[Training Epoch 2] Batch 1053, Loss 0.278470903635025\n",
      "[Training Epoch 2] Batch 1054, Loss 0.2763165235519409\n",
      "[Training Epoch 2] Batch 1055, Loss 0.2783730626106262\n",
      "[Training Epoch 2] Batch 1056, Loss 0.262177050113678\n",
      "[Training Epoch 2] Batch 1057, Loss 0.28803306818008423\n",
      "[Training Epoch 2] Batch 1058, Loss 0.291276216506958\n",
      "[Training Epoch 2] Batch 1059, Loss 0.28772735595703125\n",
      "[Training Epoch 2] Batch 1060, Loss 0.2738148868083954\n",
      "[Training Epoch 2] Batch 1061, Loss 0.27290040254592896\n",
      "[Training Epoch 2] Batch 1062, Loss 0.27360814809799194\n",
      "[Training Epoch 2] Batch 1063, Loss 0.28973594307899475\n",
      "[Training Epoch 2] Batch 1064, Loss 0.2735133767127991\n",
      "[Training Epoch 2] Batch 1065, Loss 0.276955246925354\n",
      "[Training Epoch 2] Batch 1066, Loss 0.2842925488948822\n",
      "[Training Epoch 2] Batch 1067, Loss 0.28483936190605164\n",
      "[Training Epoch 2] Batch 1068, Loss 0.3224708139896393\n",
      "[Training Epoch 2] Batch 1069, Loss 0.2809101641178131\n",
      "[Training Epoch 2] Batch 1070, Loss 0.28665465116500854\n",
      "[Training Epoch 2] Batch 1071, Loss 0.2963825464248657\n",
      "[Training Epoch 2] Batch 1072, Loss 0.3109853267669678\n",
      "[Training Epoch 2] Batch 1073, Loss 0.3128577470779419\n",
      "[Training Epoch 2] Batch 1074, Loss 0.2974573075771332\n",
      "[Training Epoch 2] Batch 1075, Loss 0.2657868266105652\n",
      "[Training Epoch 2] Batch 1076, Loss 0.29610776901245117\n",
      "[Training Epoch 2] Batch 1077, Loss 0.27717047929763794\n",
      "[Training Epoch 2] Batch 1078, Loss 0.28516849875450134\n",
      "[Training Epoch 2] Batch 1079, Loss 0.29193130135536194\n",
      "[Training Epoch 2] Batch 1080, Loss 0.26777154207229614\n",
      "[Training Epoch 2] Batch 1081, Loss 0.3050118386745453\n",
      "[Training Epoch 2] Batch 1082, Loss 0.28084948658943176\n",
      "[Training Epoch 2] Batch 1083, Loss 0.3051624298095703\n",
      "[Training Epoch 2] Batch 1084, Loss 0.26791033148765564\n",
      "[Training Epoch 2] Batch 1085, Loss 0.2843804657459259\n",
      "[Training Epoch 2] Batch 1086, Loss 0.27255743741989136\n",
      "[Training Epoch 2] Batch 1087, Loss 0.3043214678764343\n",
      "[Training Epoch 2] Batch 1088, Loss 0.25639185309410095\n",
      "[Training Epoch 2] Batch 1089, Loss 0.28381749987602234\n",
      "[Training Epoch 2] Batch 1090, Loss 0.2666481137275696\n",
      "[Training Epoch 2] Batch 1091, Loss 0.2971041798591614\n",
      "[Training Epoch 2] Batch 1092, Loss 0.2863088846206665\n",
      "[Training Epoch 2] Batch 1093, Loss 0.2838159501552582\n",
      "[Training Epoch 2] Batch 1094, Loss 0.30715686082839966\n",
      "[Training Epoch 2] Batch 1095, Loss 0.2718939185142517\n",
      "[Training Epoch 2] Batch 1096, Loss 0.2847108244895935\n",
      "[Training Epoch 2] Batch 1097, Loss 0.2866341769695282\n",
      "[Training Epoch 2] Batch 1098, Loss 0.2935476303100586\n",
      "[Training Epoch 2] Batch 1099, Loss 0.2687456011772156\n",
      "[Training Epoch 2] Batch 1100, Loss 0.2896968722343445\n",
      "[Training Epoch 2] Batch 1101, Loss 0.2960667014122009\n",
      "[Training Epoch 2] Batch 1102, Loss 0.3163192868232727\n",
      "[Training Epoch 2] Batch 1103, Loss 0.2923319935798645\n",
      "[Training Epoch 2] Batch 1104, Loss 0.28774645924568176\n",
      "[Training Epoch 2] Batch 1105, Loss 0.2771250009536743\n",
      "[Training Epoch 2] Batch 1106, Loss 0.2734085023403168\n",
      "[Training Epoch 2] Batch 1107, Loss 0.26950451731681824\n",
      "[Training Epoch 2] Batch 1108, Loss 0.2719329595565796\n",
      "[Training Epoch 2] Batch 1109, Loss 0.27176427841186523\n",
      "[Training Epoch 2] Batch 1110, Loss 0.31711581349372864\n",
      "[Training Epoch 2] Batch 1111, Loss 0.30327051877975464\n",
      "[Training Epoch 2] Batch 1112, Loss 0.28919732570648193\n",
      "[Training Epoch 2] Batch 1113, Loss 0.28552043437957764\n",
      "[Training Epoch 2] Batch 1114, Loss 0.2954515218734741\n",
      "[Training Epoch 2] Batch 1115, Loss 0.2723926901817322\n",
      "[Training Epoch 2] Batch 1116, Loss 0.2926943302154541\n",
      "[Training Epoch 2] Batch 1117, Loss 0.27723413705825806\n",
      "[Training Epoch 2] Batch 1118, Loss 0.25522369146347046\n",
      "[Training Epoch 2] Batch 1119, Loss 0.32296180725097656\n",
      "[Training Epoch 2] Batch 1120, Loss 0.2817407250404358\n",
      "[Training Epoch 2] Batch 1121, Loss 0.28581029176712036\n",
      "[Training Epoch 2] Batch 1122, Loss 0.2811681926250458\n",
      "[Training Epoch 2] Batch 1123, Loss 0.2609631419181824\n",
      "[Training Epoch 2] Batch 1124, Loss 0.2504429817199707\n",
      "[Training Epoch 2] Batch 1125, Loss 0.3147389888763428\n",
      "[Training Epoch 2] Batch 1126, Loss 0.27132540941238403\n",
      "[Training Epoch 2] Batch 1127, Loss 0.28658807277679443\n",
      "[Training Epoch 2] Batch 1128, Loss 0.29628875851631165\n",
      "[Training Epoch 2] Batch 1129, Loss 0.265021950006485\n",
      "[Training Epoch 2] Batch 1130, Loss 0.2915257513523102\n",
      "[Training Epoch 2] Batch 1131, Loss 0.28965622186660767\n",
      "[Training Epoch 2] Batch 1132, Loss 0.30342549085617065\n",
      "[Training Epoch 2] Batch 1133, Loss 0.26046332716941833\n",
      "[Training Epoch 2] Batch 1134, Loss 0.28755617141723633\n",
      "[Training Epoch 2] Batch 1135, Loss 0.30384230613708496\n",
      "[Training Epoch 2] Batch 1136, Loss 0.27920734882354736\n",
      "[Training Epoch 2] Batch 1137, Loss 0.30162471532821655\n",
      "[Training Epoch 2] Batch 1138, Loss 0.26888400316238403\n",
      "[Training Epoch 2] Batch 1139, Loss 0.26629042625427246\n",
      "[Training Epoch 2] Batch 1140, Loss 0.26009973883628845\n",
      "[Training Epoch 2] Batch 1141, Loss 0.29312172532081604\n",
      "[Training Epoch 2] Batch 1142, Loss 0.2907642126083374\n",
      "[Training Epoch 2] Batch 1143, Loss 0.26112547516822815\n",
      "[Training Epoch 2] Batch 1144, Loss 0.24144518375396729\n",
      "[Training Epoch 2] Batch 1145, Loss 0.31059592962265015\n",
      "[Training Epoch 2] Batch 1146, Loss 0.268800288438797\n",
      "[Training Epoch 2] Batch 1147, Loss 0.2912794351577759\n",
      "[Training Epoch 2] Batch 1148, Loss 0.2887333035469055\n",
      "[Training Epoch 2] Batch 1149, Loss 0.2700636088848114\n",
      "[Training Epoch 2] Batch 1150, Loss 0.3146090507507324\n",
      "[Training Epoch 2] Batch 1151, Loss 0.30353838205337524\n",
      "[Training Epoch 2] Batch 1152, Loss 0.2840830683708191\n",
      "[Training Epoch 2] Batch 1153, Loss 0.28518813848495483\n",
      "[Training Epoch 2] Batch 1154, Loss 0.2611929774284363\n",
      "[Training Epoch 2] Batch 1155, Loss 0.28341761231422424\n",
      "[Training Epoch 2] Batch 1156, Loss 0.29221808910369873\n",
      "[Training Epoch 2] Batch 1157, Loss 0.3083524703979492\n",
      "[Training Epoch 2] Batch 1158, Loss 0.28422075510025024\n",
      "[Training Epoch 2] Batch 1159, Loss 0.308180034160614\n",
      "[Training Epoch 2] Batch 1160, Loss 0.2589246928691864\n",
      "[Training Epoch 2] Batch 1161, Loss 0.2676393985748291\n",
      "[Training Epoch 2] Batch 1162, Loss 0.31369614601135254\n",
      "[Training Epoch 2] Batch 1163, Loss 0.3217334747314453\n",
      "[Training Epoch 2] Batch 1164, Loss 0.2786955237388611\n",
      "[Training Epoch 2] Batch 1165, Loss 0.3096598982810974\n",
      "[Training Epoch 2] Batch 1166, Loss 0.3026168942451477\n",
      "[Training Epoch 2] Batch 1167, Loss 0.2961885929107666\n",
      "[Training Epoch 2] Batch 1168, Loss 0.28596383333206177\n",
      "[Training Epoch 2] Batch 1169, Loss 0.27462345361709595\n",
      "[Training Epoch 2] Batch 1170, Loss 0.2823200821876526\n",
      "[Training Epoch 2] Batch 1171, Loss 0.2881758213043213\n",
      "[Training Epoch 2] Batch 1172, Loss 0.2990608811378479\n",
      "[Training Epoch 2] Batch 1173, Loss 0.2863978445529938\n",
      "[Training Epoch 2] Batch 1174, Loss 0.2878256142139435\n",
      "[Training Epoch 2] Batch 1175, Loss 0.27432793378829956\n",
      "[Training Epoch 2] Batch 1176, Loss 0.29390090703964233\n",
      "[Training Epoch 2] Batch 1177, Loss 0.2943790555000305\n",
      "[Training Epoch 2] Batch 1178, Loss 0.3055727481842041\n",
      "[Training Epoch 2] Batch 1179, Loss 0.32676124572753906\n",
      "[Training Epoch 2] Batch 1180, Loss 0.2792549133300781\n",
      "[Training Epoch 2] Batch 1181, Loss 0.27734339237213135\n",
      "[Training Epoch 2] Batch 1182, Loss 0.26860296726226807\n",
      "[Training Epoch 2] Batch 1183, Loss 0.290019690990448\n",
      "[Training Epoch 2] Batch 1184, Loss 0.2714088559150696\n",
      "[Training Epoch 2] Batch 1185, Loss 0.2880382835865021\n",
      "[Training Epoch 2] Batch 1186, Loss 0.3084833323955536\n",
      "[Training Epoch 2] Batch 1187, Loss 0.2766396999359131\n",
      "[Training Epoch 2] Batch 1188, Loss 0.3008536696434021\n",
      "[Training Epoch 2] Batch 1189, Loss 0.2884542942047119\n",
      "[Training Epoch 2] Batch 1190, Loss 0.2869088351726532\n",
      "[Training Epoch 2] Batch 1191, Loss 0.2804220914840698\n",
      "[Training Epoch 2] Batch 1192, Loss 0.3099692463874817\n",
      "[Training Epoch 2] Batch 1193, Loss 0.2480991631746292\n",
      "[Training Epoch 2] Batch 1194, Loss 0.27276065945625305\n",
      "[Training Epoch 2] Batch 1195, Loss 0.2705479562282562\n",
      "[Training Epoch 2] Batch 1196, Loss 0.27887582778930664\n",
      "[Training Epoch 2] Batch 1197, Loss 0.2454816997051239\n",
      "[Training Epoch 2] Batch 1198, Loss 0.25622519850730896\n",
      "[Training Epoch 2] Batch 1199, Loss 0.29448431730270386\n",
      "[Training Epoch 2] Batch 1200, Loss 0.295539915561676\n",
      "[Training Epoch 2] Batch 1201, Loss 0.28811395168304443\n",
      "[Training Epoch 2] Batch 1202, Loss 0.2985343635082245\n",
      "[Training Epoch 2] Batch 1203, Loss 0.2633357048034668\n",
      "[Training Epoch 2] Batch 1204, Loss 0.27940332889556885\n",
      "[Training Epoch 2] Batch 1205, Loss 0.3064400255680084\n",
      "[Training Epoch 2] Batch 1206, Loss 0.28910326957702637\n",
      "[Training Epoch 2] Batch 1207, Loss 0.29845064878463745\n",
      "[Training Epoch 2] Batch 1208, Loss 0.3157247304916382\n",
      "[Training Epoch 2] Batch 1209, Loss 0.2936413884162903\n",
      "[Training Epoch 2] Batch 1210, Loss 0.28338176012039185\n",
      "[Training Epoch 2] Batch 1211, Loss 0.264394611120224\n",
      "[Training Epoch 2] Batch 1212, Loss 0.2591458261013031\n",
      "[Training Epoch 2] Batch 1213, Loss 0.30203676223754883\n",
      "[Training Epoch 2] Batch 1214, Loss 0.2576673626899719\n",
      "[Training Epoch 2] Batch 1215, Loss 0.28472670912742615\n",
      "[Training Epoch 2] Batch 1216, Loss 0.3146438002586365\n",
      "[Training Epoch 2] Batch 1217, Loss 0.29868119955062866\n",
      "[Training Epoch 2] Batch 1218, Loss 0.27030885219573975\n",
      "[Training Epoch 2] Batch 1219, Loss 0.30201131105422974\n",
      "[Training Epoch 2] Batch 1220, Loss 0.28994220495224\n",
      "[Training Epoch 2] Batch 1221, Loss 0.28303995728492737\n",
      "[Training Epoch 2] Batch 1222, Loss 0.28640756011009216\n",
      "[Training Epoch 2] Batch 1223, Loss 0.24657098948955536\n",
      "[Training Epoch 2] Batch 1224, Loss 0.2934419810771942\n",
      "[Training Epoch 2] Batch 1225, Loss 0.2675112187862396\n",
      "[Training Epoch 2] Batch 1226, Loss 0.29674404859542847\n",
      "[Training Epoch 2] Batch 1227, Loss 0.2729114294052124\n",
      "[Training Epoch 2] Batch 1228, Loss 0.28083860874176025\n",
      "[Training Epoch 2] Batch 1229, Loss 0.29004448652267456\n",
      "[Training Epoch 2] Batch 1230, Loss 0.29717326164245605\n",
      "[Training Epoch 2] Batch 1231, Loss 0.29626181721687317\n",
      "[Training Epoch 2] Batch 1232, Loss 0.3215886056423187\n",
      "[Training Epoch 2] Batch 1233, Loss 0.28121548891067505\n",
      "[Training Epoch 2] Batch 1234, Loss 0.2951321303844452\n",
      "[Training Epoch 2] Batch 1235, Loss 0.25976964831352234\n",
      "[Training Epoch 2] Batch 1236, Loss 0.3296682834625244\n",
      "[Training Epoch 2] Batch 1237, Loss 0.2600380480289459\n",
      "[Training Epoch 2] Batch 1238, Loss 0.28012046217918396\n",
      "[Training Epoch 2] Batch 1239, Loss 0.30571186542510986\n",
      "[Training Epoch 2] Batch 1240, Loss 0.28232908248901367\n",
      "[Training Epoch 2] Batch 1241, Loss 0.29151612520217896\n",
      "[Training Epoch 2] Batch 1242, Loss 0.30692172050476074\n",
      "[Training Epoch 2] Batch 1243, Loss 0.2810867726802826\n",
      "[Training Epoch 2] Batch 1244, Loss 0.2795078754425049\n",
      "[Training Epoch 2] Batch 1245, Loss 0.2815518379211426\n",
      "[Training Epoch 2] Batch 1246, Loss 0.2731001377105713\n",
      "[Training Epoch 2] Batch 1247, Loss 0.28226253390312195\n",
      "[Training Epoch 2] Batch 1248, Loss 0.27429312467575073\n",
      "[Training Epoch 2] Batch 1249, Loss 0.3015379309654236\n",
      "[Training Epoch 2] Batch 1250, Loss 0.3182090222835541\n",
      "[Training Epoch 2] Batch 1251, Loss 0.2713082432746887\n",
      "[Training Epoch 2] Batch 1252, Loss 0.283319354057312\n",
      "[Training Epoch 2] Batch 1253, Loss 0.29177403450012207\n",
      "[Training Epoch 2] Batch 1254, Loss 0.3044106066226959\n",
      "[Training Epoch 2] Batch 1255, Loss 0.27941399812698364\n",
      "[Training Epoch 2] Batch 1256, Loss 0.26613056659698486\n",
      "[Training Epoch 2] Batch 1257, Loss 0.29383108019828796\n",
      "[Training Epoch 2] Batch 1258, Loss 0.33294111490249634\n",
      "[Training Epoch 2] Batch 1259, Loss 0.3098425567150116\n",
      "[Training Epoch 2] Batch 1260, Loss 0.2702411413192749\n",
      "[Training Epoch 2] Batch 1261, Loss 0.2726857364177704\n",
      "[Training Epoch 2] Batch 1262, Loss 0.3029802739620209\n",
      "[Training Epoch 2] Batch 1263, Loss 0.3166635036468506\n",
      "[Training Epoch 2] Batch 1264, Loss 0.291640043258667\n",
      "[Training Epoch 2] Batch 1265, Loss 0.28825122117996216\n",
      "[Training Epoch 2] Batch 1266, Loss 0.27576321363449097\n",
      "[Training Epoch 2] Batch 1267, Loss 0.28145119547843933\n",
      "[Training Epoch 2] Batch 1268, Loss 0.2844860553741455\n",
      "[Training Epoch 2] Batch 1269, Loss 0.2930346429347992\n",
      "[Training Epoch 2] Batch 1270, Loss 0.2757835388183594\n",
      "[Training Epoch 2] Batch 1271, Loss 0.30279621481895447\n",
      "[Training Epoch 2] Batch 1272, Loss 0.30057722330093384\n",
      "[Training Epoch 2] Batch 1273, Loss 0.28665870428085327\n",
      "[Training Epoch 2] Batch 1274, Loss 0.2683382034301758\n",
      "[Training Epoch 2] Batch 1275, Loss 0.2822929322719574\n",
      "[Training Epoch 2] Batch 1276, Loss 0.27474433183670044\n",
      "[Training Epoch 2] Batch 1277, Loss 0.26217544078826904\n",
      "[Training Epoch 2] Batch 1278, Loss 0.2737036943435669\n",
      "[Training Epoch 2] Batch 1279, Loss 0.29202231764793396\n",
      "[Training Epoch 2] Batch 1280, Loss 0.2726926803588867\n",
      "[Training Epoch 2] Batch 1281, Loss 0.30891871452331543\n",
      "[Training Epoch 2] Batch 1282, Loss 0.3129894733428955\n",
      "[Training Epoch 2] Batch 1283, Loss 0.31692415475845337\n",
      "[Training Epoch 2] Batch 1284, Loss 0.2944032549858093\n",
      "[Training Epoch 2] Batch 1285, Loss 0.3011202812194824\n",
      "[Training Epoch 2] Batch 1286, Loss 0.28785640001296997\n",
      "[Training Epoch 2] Batch 1287, Loss 0.27135154604911804\n",
      "[Training Epoch 2] Batch 1288, Loss 0.265726774930954\n",
      "[Training Epoch 2] Batch 1289, Loss 0.3046165406703949\n",
      "[Training Epoch 2] Batch 1290, Loss 0.27835753560066223\n",
      "[Training Epoch 2] Batch 1291, Loss 0.28994619846343994\n",
      "[Training Epoch 2] Batch 1292, Loss 0.25849202275276184\n",
      "[Training Epoch 2] Batch 1293, Loss 0.27526646852493286\n",
      "[Training Epoch 2] Batch 1294, Loss 0.28193023800849915\n",
      "[Training Epoch 2] Batch 1295, Loss 0.2799052894115448\n",
      "[Training Epoch 2] Batch 1296, Loss 0.2861306667327881\n",
      "[Training Epoch 2] Batch 1297, Loss 0.2986588478088379\n",
      "[Training Epoch 2] Batch 1298, Loss 0.2674909234046936\n",
      "[Training Epoch 2] Batch 1299, Loss 0.2701089382171631\n",
      "[Training Epoch 2] Batch 1300, Loss 0.2855679392814636\n",
      "[Training Epoch 2] Batch 1301, Loss 0.27881714701652527\n",
      "[Training Epoch 2] Batch 1302, Loss 0.29569709300994873\n",
      "[Training Epoch 2] Batch 1303, Loss 0.2843720614910126\n",
      "[Training Epoch 2] Batch 1304, Loss 0.2900197207927704\n",
      "[Training Epoch 2] Batch 1305, Loss 0.29921889305114746\n",
      "[Training Epoch 2] Batch 1306, Loss 0.28128713369369507\n",
      "[Training Epoch 2] Batch 1307, Loss 0.2668250799179077\n",
      "[Training Epoch 2] Batch 1308, Loss 0.3098464906215668\n",
      "[Training Epoch 2] Batch 1309, Loss 0.2656629681587219\n",
      "[Training Epoch 2] Batch 1310, Loss 0.26275435090065\n",
      "[Training Epoch 2] Batch 1311, Loss 0.29714682698249817\n",
      "[Training Epoch 2] Batch 1312, Loss 0.27123862504959106\n",
      "[Training Epoch 2] Batch 1313, Loss 0.2791171073913574\n",
      "[Training Epoch 2] Batch 1314, Loss 0.27333569526672363\n",
      "[Training Epoch 2] Batch 1315, Loss 0.2820621430873871\n",
      "[Training Epoch 2] Batch 1316, Loss 0.28522825241088867\n",
      "[Training Epoch 2] Batch 1317, Loss 0.2965289354324341\n",
      "[Training Epoch 2] Batch 1318, Loss 0.275430291891098\n",
      "[Training Epoch 2] Batch 1319, Loss 0.2975829243659973\n",
      "[Training Epoch 2] Batch 1320, Loss 0.2925296723842621\n",
      "[Training Epoch 2] Batch 1321, Loss 0.2636394500732422\n",
      "[Training Epoch 2] Batch 1322, Loss 0.2887749671936035\n",
      "[Training Epoch 2] Batch 1323, Loss 0.264064222574234\n",
      "[Training Epoch 2] Batch 1324, Loss 0.26636892557144165\n",
      "[Training Epoch 2] Batch 1325, Loss 0.2640104293823242\n",
      "[Training Epoch 2] Batch 1326, Loss 0.27653610706329346\n",
      "[Training Epoch 2] Batch 1327, Loss 0.2895742952823639\n",
      "[Training Epoch 2] Batch 1328, Loss 0.30085039138793945\n",
      "[Training Epoch 2] Batch 1329, Loss 0.27174660563468933\n",
      "[Training Epoch 2] Batch 1330, Loss 0.28321003913879395\n",
      "[Training Epoch 2] Batch 1331, Loss 0.2620835304260254\n",
      "[Training Epoch 2] Batch 1332, Loss 0.2593595087528229\n",
      "[Training Epoch 2] Batch 1333, Loss 0.2838844656944275\n",
      "[Training Epoch 2] Batch 1334, Loss 0.2903088331222534\n",
      "[Training Epoch 2] Batch 1335, Loss 0.2896668016910553\n",
      "[Training Epoch 2] Batch 1336, Loss 0.29813259840011597\n",
      "[Training Epoch 2] Batch 1337, Loss 0.29809337854385376\n",
      "[Training Epoch 2] Batch 1338, Loss 0.2930331826210022\n",
      "[Training Epoch 2] Batch 1339, Loss 0.27273935079574585\n",
      "[Training Epoch 2] Batch 1340, Loss 0.2812980115413666\n",
      "[Training Epoch 2] Batch 1341, Loss 0.29753270745277405\n",
      "[Training Epoch 2] Batch 1342, Loss 0.2703859806060791\n",
      "[Training Epoch 2] Batch 1343, Loss 0.2601514756679535\n",
      "[Training Epoch 2] Batch 1344, Loss 0.26960569620132446\n",
      "[Training Epoch 2] Batch 1345, Loss 0.297149121761322\n",
      "[Training Epoch 2] Batch 1346, Loss 0.276655375957489\n",
      "[Training Epoch 2] Batch 1347, Loss 0.27640610933303833\n",
      "[Training Epoch 2] Batch 1348, Loss 0.27586668729782104\n",
      "[Training Epoch 2] Batch 1349, Loss 0.27480506896972656\n",
      "[Training Epoch 2] Batch 1350, Loss 0.2801402509212494\n",
      "[Training Epoch 2] Batch 1351, Loss 0.27233967185020447\n",
      "[Training Epoch 2] Batch 1352, Loss 0.28955966234207153\n",
      "[Training Epoch 2] Batch 1353, Loss 0.2866194248199463\n",
      "[Training Epoch 2] Batch 1354, Loss 0.26510074734687805\n",
      "[Training Epoch 2] Batch 1355, Loss 0.2534746527671814\n",
      "[Training Epoch 2] Batch 1356, Loss 0.2781342566013336\n",
      "[Training Epoch 2] Batch 1357, Loss 0.29899871349334717\n",
      "[Training Epoch 2] Batch 1358, Loss 0.3207249939441681\n",
      "[Training Epoch 2] Batch 1359, Loss 0.2908003032207489\n",
      "[Training Epoch 2] Batch 1360, Loss 0.28780925273895264\n",
      "[Training Epoch 2] Batch 1361, Loss 0.26876771450042725\n",
      "[Training Epoch 2] Batch 1362, Loss 0.2839824855327606\n",
      "[Training Epoch 2] Batch 1363, Loss 0.2763688266277313\n",
      "[Training Epoch 2] Batch 1364, Loss 0.27490949630737305\n",
      "[Training Epoch 2] Batch 1365, Loss 0.3003597557544708\n",
      "[Training Epoch 2] Batch 1366, Loss 0.29846715927124023\n",
      "[Training Epoch 2] Batch 1367, Loss 0.28172969818115234\n",
      "[Training Epoch 2] Batch 1368, Loss 0.29614055156707764\n",
      "[Training Epoch 2] Batch 1369, Loss 0.30138397216796875\n",
      "[Training Epoch 2] Batch 1370, Loss 0.29515597224235535\n",
      "[Training Epoch 2] Batch 1371, Loss 0.24789954721927643\n",
      "[Training Epoch 2] Batch 1372, Loss 0.29583895206451416\n",
      "[Training Epoch 2] Batch 1373, Loss 0.27494096755981445\n",
      "[Training Epoch 2] Batch 1374, Loss 0.3224685788154602\n",
      "[Training Epoch 2] Batch 1375, Loss 0.26930421590805054\n",
      "[Training Epoch 2] Batch 1376, Loss 0.28730088472366333\n",
      "[Training Epoch 2] Batch 1377, Loss 0.28280892968177795\n",
      "[Training Epoch 2] Batch 1378, Loss 0.3061407804489136\n",
      "[Training Epoch 2] Batch 1379, Loss 0.29000306129455566\n",
      "[Training Epoch 2] Batch 1380, Loss 0.27060234546661377\n",
      "[Training Epoch 2] Batch 1381, Loss 0.2817091941833496\n",
      "[Training Epoch 2] Batch 1382, Loss 0.29162347316741943\n",
      "[Training Epoch 2] Batch 1383, Loss 0.3099382519721985\n",
      "[Training Epoch 2] Batch 1384, Loss 0.26730436086654663\n",
      "[Training Epoch 2] Batch 1385, Loss 0.27626878023147583\n",
      "[Training Epoch 2] Batch 1386, Loss 0.2808252274990082\n",
      "[Training Epoch 2] Batch 1387, Loss 0.2722351849079132\n",
      "[Training Epoch 2] Batch 1388, Loss 0.26360884308815\n",
      "[Training Epoch 2] Batch 1389, Loss 0.2820529341697693\n",
      "[Training Epoch 2] Batch 1390, Loss 0.2944711148738861\n",
      "[Training Epoch 2] Batch 1391, Loss 0.2601096034049988\n",
      "[Training Epoch 2] Batch 1392, Loss 0.29784268140792847\n",
      "[Training Epoch 2] Batch 1393, Loss 0.3081253170967102\n",
      "[Training Epoch 2] Batch 1394, Loss 0.27365559339523315\n",
      "[Training Epoch 2] Batch 1395, Loss 0.267154723405838\n",
      "[Training Epoch 2] Batch 1396, Loss 0.2894505262374878\n",
      "[Training Epoch 2] Batch 1397, Loss 0.2870029807090759\n",
      "[Training Epoch 2] Batch 1398, Loss 0.2584942579269409\n",
      "[Training Epoch 2] Batch 1399, Loss 0.26559025049209595\n",
      "[Training Epoch 2] Batch 1400, Loss 0.28038907051086426\n",
      "[Training Epoch 2] Batch 1401, Loss 0.2662900984287262\n",
      "[Training Epoch 2] Batch 1402, Loss 0.27313777804374695\n",
      "[Training Epoch 2] Batch 1403, Loss 0.3093298077583313\n",
      "[Training Epoch 2] Batch 1404, Loss 0.2587963342666626\n",
      "[Training Epoch 2] Batch 1405, Loss 0.2621757388114929\n",
      "[Training Epoch 2] Batch 1406, Loss 0.271287202835083\n",
      "[Training Epoch 2] Batch 1407, Loss 0.292058527469635\n",
      "[Training Epoch 2] Batch 1408, Loss 0.2613387405872345\n",
      "[Training Epoch 2] Batch 1409, Loss 0.2963484227657318\n",
      "[Training Epoch 2] Batch 1410, Loss 0.29593372344970703\n",
      "[Training Epoch 2] Batch 1411, Loss 0.27591854333877563\n",
      "[Training Epoch 2] Batch 1412, Loss 0.2856576442718506\n",
      "[Training Epoch 2] Batch 1413, Loss 0.2930606007575989\n",
      "[Training Epoch 2] Batch 1414, Loss 0.300498366355896\n",
      "[Training Epoch 2] Batch 1415, Loss 0.2624116539955139\n",
      "[Training Epoch 2] Batch 1416, Loss 0.25842878222465515\n",
      "[Training Epoch 2] Batch 1417, Loss 0.2988024950027466\n",
      "[Training Epoch 2] Batch 1418, Loss 0.25763681530952454\n",
      "[Training Epoch 2] Batch 1419, Loss 0.2813587486743927\n",
      "[Training Epoch 2] Batch 1420, Loss 0.25186607241630554\n",
      "[Training Epoch 2] Batch 1421, Loss 0.2746695876121521\n",
      "[Training Epoch 2] Batch 1422, Loss 0.2894010841846466\n",
      "[Training Epoch 2] Batch 1423, Loss 0.2875053882598877\n",
      "[Training Epoch 2] Batch 1424, Loss 0.2721342444419861\n",
      "[Training Epoch 2] Batch 1425, Loss 0.26977694034576416\n",
      "[Training Epoch 2] Batch 1426, Loss 0.27026140689849854\n",
      "[Training Epoch 2] Batch 1427, Loss 0.26995909214019775\n",
      "[Training Epoch 2] Batch 1428, Loss 0.30454522371292114\n",
      "[Training Epoch 2] Batch 1429, Loss 0.2747383713722229\n",
      "[Training Epoch 2] Batch 1430, Loss 0.2966691255569458\n",
      "[Training Epoch 2] Batch 1431, Loss 0.2787911295890808\n",
      "[Training Epoch 2] Batch 1432, Loss 0.2880094051361084\n",
      "[Training Epoch 2] Batch 1433, Loss 0.29236090183258057\n",
      "[Training Epoch 2] Batch 1434, Loss 0.29838839173316956\n",
      "[Training Epoch 2] Batch 1435, Loss 0.27419212460517883\n",
      "[Training Epoch 2] Batch 1436, Loss 0.27463752031326294\n",
      "[Training Epoch 2] Batch 1437, Loss 0.2904304265975952\n",
      "[Training Epoch 2] Batch 1438, Loss 0.297763854265213\n",
      "[Training Epoch 2] Batch 1439, Loss 0.2802291512489319\n",
      "[Training Epoch 2] Batch 1440, Loss 0.28803887963294983\n",
      "[Training Epoch 2] Batch 1441, Loss 0.2981058359146118\n",
      "[Training Epoch 2] Batch 1442, Loss 0.27758195996284485\n",
      "[Training Epoch 2] Batch 1443, Loss 0.2921428978443146\n",
      "[Training Epoch 2] Batch 1444, Loss 0.27330681681632996\n",
      "[Training Epoch 2] Batch 1445, Loss 0.3157532811164856\n",
      "[Training Epoch 2] Batch 1446, Loss 0.2870648205280304\n",
      "[Training Epoch 2] Batch 1447, Loss 0.3180294632911682\n",
      "[Training Epoch 2] Batch 1448, Loss 0.26023194193840027\n",
      "[Training Epoch 2] Batch 1449, Loss 0.2978769838809967\n",
      "[Training Epoch 2] Batch 1450, Loss 0.28008636832237244\n",
      "[Training Epoch 2] Batch 1451, Loss 0.26801347732543945\n",
      "[Training Epoch 2] Batch 1452, Loss 0.2800405025482178\n",
      "[Training Epoch 2] Batch 1453, Loss 0.28120705485343933\n",
      "[Training Epoch 2] Batch 1454, Loss 0.2771707773208618\n",
      "[Training Epoch 2] Batch 1455, Loss 0.27434927225112915\n",
      "[Training Epoch 2] Batch 1456, Loss 0.2934093475341797\n",
      "[Training Epoch 2] Batch 1457, Loss 0.2854946255683899\n",
      "[Training Epoch 2] Batch 1458, Loss 0.2813768982887268\n",
      "[Training Epoch 2] Batch 1459, Loss 0.2611086070537567\n",
      "[Training Epoch 2] Batch 1460, Loss 0.28985610604286194\n",
      "[Training Epoch 2] Batch 1461, Loss 0.2872467637062073\n",
      "[Training Epoch 2] Batch 1462, Loss 0.2496527135372162\n",
      "[Training Epoch 2] Batch 1463, Loss 0.27536672353744507\n",
      "[Training Epoch 2] Batch 1464, Loss 0.2854596972465515\n",
      "[Training Epoch 2] Batch 1465, Loss 0.2966166138648987\n",
      "[Training Epoch 2] Batch 1466, Loss 0.26550528407096863\n",
      "[Training Epoch 2] Batch 1467, Loss 0.26877039670944214\n",
      "[Training Epoch 2] Batch 1468, Loss 0.2996474504470825\n",
      "[Training Epoch 2] Batch 1469, Loss 0.25933465361595154\n",
      "[Training Epoch 2] Batch 1470, Loss 0.25005605816841125\n",
      "[Training Epoch 2] Batch 1471, Loss 0.2804046869277954\n",
      "[Training Epoch 2] Batch 1472, Loss 0.3121185600757599\n",
      "[Training Epoch 2] Batch 1473, Loss 0.27780288457870483\n",
      "[Training Epoch 2] Batch 1474, Loss 0.30762779712677\n",
      "[Training Epoch 2] Batch 1475, Loss 0.25424250960350037\n",
      "[Training Epoch 2] Batch 1476, Loss 0.27142855525016785\n",
      "[Training Epoch 2] Batch 1477, Loss 0.30752432346343994\n",
      "[Training Epoch 2] Batch 1478, Loss 0.2628174424171448\n",
      "[Training Epoch 2] Batch 1479, Loss 0.25667279958724976\n",
      "[Training Epoch 2] Batch 1480, Loss 0.295225590467453\n",
      "[Training Epoch 2] Batch 1481, Loss 0.27185454964637756\n",
      "[Training Epoch 2] Batch 1482, Loss 0.2744096517562866\n",
      "[Training Epoch 2] Batch 1483, Loss 0.31696152687072754\n",
      "[Training Epoch 2] Batch 1484, Loss 0.2782173454761505\n",
      "[Training Epoch 2] Batch 1485, Loss 0.28685426712036133\n",
      "[Training Epoch 2] Batch 1486, Loss 0.29731157422065735\n",
      "[Training Epoch 2] Batch 1487, Loss 0.23402562737464905\n",
      "[Training Epoch 2] Batch 1488, Loss 0.31386494636535645\n",
      "[Training Epoch 2] Batch 1489, Loss 0.26833826303482056\n",
      "[Training Epoch 2] Batch 1490, Loss 0.24495002627372742\n",
      "[Training Epoch 2] Batch 1491, Loss 0.2740429937839508\n",
      "[Training Epoch 2] Batch 1492, Loss 0.26922351121902466\n",
      "[Training Epoch 2] Batch 1493, Loss 0.2607082426548004\n",
      "[Training Epoch 2] Batch 1494, Loss 0.27821213006973267\n",
      "[Training Epoch 2] Batch 1495, Loss 0.2839254140853882\n",
      "[Training Epoch 2] Batch 1496, Loss 0.27810749411582947\n",
      "[Training Epoch 2] Batch 1497, Loss 0.2769292891025543\n",
      "[Training Epoch 2] Batch 1498, Loss 0.26895320415496826\n",
      "[Training Epoch 2] Batch 1499, Loss 0.2876858115196228\n",
      "[Training Epoch 2] Batch 1500, Loss 0.28874343633651733\n",
      "[Training Epoch 2] Batch 1501, Loss 0.29320037364959717\n",
      "[Training Epoch 2] Batch 1502, Loss 0.2777569890022278\n",
      "[Training Epoch 2] Batch 1503, Loss 0.27801960706710815\n",
      "[Training Epoch 2] Batch 1504, Loss 0.2823670208454132\n",
      "[Training Epoch 2] Batch 1505, Loss 0.28786876797676086\n",
      "[Training Epoch 2] Batch 1506, Loss 0.25716716051101685\n",
      "[Training Epoch 2] Batch 1507, Loss 0.2851976156234741\n",
      "[Training Epoch 2] Batch 1508, Loss 0.29098546504974365\n",
      "[Training Epoch 2] Batch 1509, Loss 0.25439026951789856\n",
      "[Training Epoch 2] Batch 1510, Loss 0.2728632688522339\n",
      "[Training Epoch 2] Batch 1511, Loss 0.26835840940475464\n",
      "[Training Epoch 2] Batch 1512, Loss 0.25191837549209595\n",
      "[Training Epoch 2] Batch 1513, Loss 0.2851170599460602\n",
      "[Training Epoch 2] Batch 1514, Loss 0.2876773178577423\n",
      "[Training Epoch 2] Batch 1515, Loss 0.27025920152664185\n",
      "[Training Epoch 2] Batch 1516, Loss 0.3020767867565155\n",
      "[Training Epoch 2] Batch 1517, Loss 0.3067539930343628\n",
      "[Training Epoch 2] Batch 1518, Loss 0.29912227392196655\n",
      "[Training Epoch 2] Batch 1519, Loss 0.2808155417442322\n",
      "[Training Epoch 2] Batch 1520, Loss 0.28597235679626465\n",
      "[Training Epoch 2] Batch 1521, Loss 0.2808074951171875\n",
      "[Training Epoch 2] Batch 1522, Loss 0.272018700838089\n",
      "[Training Epoch 2] Batch 1523, Loss 0.2710610330104828\n",
      "[Training Epoch 2] Batch 1524, Loss 0.27310335636138916\n",
      "[Training Epoch 2] Batch 1525, Loss 0.27777019143104553\n",
      "[Training Epoch 2] Batch 1526, Loss 0.2951394021511078\n",
      "[Training Epoch 2] Batch 1527, Loss 0.27891606092453003\n",
      "[Training Epoch 2] Batch 1528, Loss 0.29575690627098083\n",
      "[Training Epoch 2] Batch 1529, Loss 0.24533334374427795\n",
      "[Training Epoch 2] Batch 1530, Loss 0.28835397958755493\n",
      "[Training Epoch 2] Batch 1531, Loss 0.28251761198043823\n",
      "[Training Epoch 2] Batch 1532, Loss 0.2547445297241211\n",
      "[Training Epoch 2] Batch 1533, Loss 0.29034483432769775\n",
      "[Training Epoch 2] Batch 1534, Loss 0.2987957000732422\n",
      "[Training Epoch 2] Batch 1535, Loss 0.27355659008026123\n",
      "[Training Epoch 2] Batch 1536, Loss 0.30438023805618286\n",
      "[Training Epoch 2] Batch 1537, Loss 0.3016611337661743\n",
      "[Training Epoch 2] Batch 1538, Loss 0.2974874973297119\n",
      "[Training Epoch 2] Batch 1539, Loss 0.3156213164329529\n",
      "[Training Epoch 2] Batch 1540, Loss 0.28897756338119507\n",
      "[Training Epoch 2] Batch 1541, Loss 0.2769433856010437\n",
      "[Training Epoch 2] Batch 1542, Loss 0.2489451915025711\n",
      "[Training Epoch 2] Batch 1543, Loss 0.3012423813343048\n",
      "[Training Epoch 2] Batch 1544, Loss 0.2726256847381592\n",
      "[Training Epoch 2] Batch 1545, Loss 0.2656707465648651\n",
      "[Training Epoch 2] Batch 1546, Loss 0.29661163687705994\n",
      "[Training Epoch 2] Batch 1547, Loss 0.2980288863182068\n",
      "[Training Epoch 2] Batch 1548, Loss 0.2730671763420105\n",
      "[Training Epoch 2] Batch 1549, Loss 0.30020976066589355\n",
      "[Training Epoch 2] Batch 1550, Loss 0.2644449472427368\n",
      "[Training Epoch 2] Batch 1551, Loss 0.27644094824790955\n",
      "[Training Epoch 2] Batch 1552, Loss 0.2651026248931885\n",
      "[Training Epoch 2] Batch 1553, Loss 0.28284549713134766\n",
      "[Training Epoch 2] Batch 1554, Loss 0.27793747186660767\n",
      "[Training Epoch 2] Batch 1555, Loss 0.2954225540161133\n",
      "[Training Epoch 2] Batch 1556, Loss 0.2691360414028168\n",
      "[Training Epoch 2] Batch 1557, Loss 0.31274622678756714\n",
      "[Training Epoch 2] Batch 1558, Loss 0.24829092621803284\n",
      "[Training Epoch 2] Batch 1559, Loss 0.25069689750671387\n",
      "[Training Epoch 2] Batch 1560, Loss 0.30337023735046387\n",
      "[Training Epoch 2] Batch 1561, Loss 0.28721314668655396\n",
      "[Training Epoch 2] Batch 1562, Loss 0.2816042900085449\n",
      "[Training Epoch 2] Batch 1563, Loss 0.27397358417510986\n",
      "[Training Epoch 2] Batch 1564, Loss 0.292972594499588\n",
      "[Training Epoch 2] Batch 1565, Loss 0.2970096468925476\n",
      "[Training Epoch 2] Batch 1566, Loss 0.254841148853302\n",
      "[Training Epoch 2] Batch 1567, Loss 0.2618889808654785\n",
      "[Training Epoch 2] Batch 1568, Loss 0.2856557369232178\n",
      "[Training Epoch 2] Batch 1569, Loss 0.29532429575920105\n",
      "[Training Epoch 2] Batch 1570, Loss 0.2743147015571594\n",
      "[Training Epoch 2] Batch 1571, Loss 0.30684393644332886\n",
      "[Training Epoch 2] Batch 1572, Loss 0.30885133147239685\n",
      "[Training Epoch 2] Batch 1573, Loss 0.26567542552948\n",
      "[Training Epoch 2] Batch 1574, Loss 0.2770472764968872\n",
      "[Training Epoch 2] Batch 1575, Loss 0.27601704001426697\n",
      "[Training Epoch 2] Batch 1576, Loss 0.30216920375823975\n",
      "[Training Epoch 2] Batch 1577, Loss 0.3123115003108978\n",
      "[Training Epoch 2] Batch 1578, Loss 0.2906494438648224\n",
      "[Training Epoch 2] Batch 1579, Loss 0.28853070735931396\n",
      "[Training Epoch 2] Batch 1580, Loss 0.2800898849964142\n",
      "[Training Epoch 2] Batch 1581, Loss 0.26298898458480835\n",
      "[Training Epoch 2] Batch 1582, Loss 0.28053945302963257\n",
      "[Training Epoch 2] Batch 1583, Loss 0.27842944860458374\n",
      "[Training Epoch 2] Batch 1584, Loss 0.29690998792648315\n",
      "[Training Epoch 2] Batch 1585, Loss 0.250064492225647\n",
      "[Training Epoch 2] Batch 1586, Loss 0.2770361304283142\n",
      "[Training Epoch 2] Batch 1587, Loss 0.2658565044403076\n",
      "[Training Epoch 2] Batch 1588, Loss 0.2832911014556885\n",
      "[Training Epoch 2] Batch 1589, Loss 0.29277414083480835\n",
      "[Training Epoch 2] Batch 1590, Loss 0.26903700828552246\n",
      "[Training Epoch 2] Batch 1591, Loss 0.26546937227249146\n",
      "[Training Epoch 2] Batch 1592, Loss 0.2975665330886841\n",
      "[Training Epoch 2] Batch 1593, Loss 0.2378261387348175\n",
      "[Training Epoch 2] Batch 1594, Loss 0.29087454080581665\n",
      "[Training Epoch 2] Batch 1595, Loss 0.2713491916656494\n",
      "[Training Epoch 2] Batch 1596, Loss 0.3058585524559021\n",
      "[Training Epoch 2] Batch 1597, Loss 0.28080156445503235\n",
      "[Training Epoch 2] Batch 1598, Loss 0.312421053647995\n",
      "[Training Epoch 2] Batch 1599, Loss 0.26075679063796997\n",
      "[Training Epoch 2] Batch 1600, Loss 0.2963477671146393\n",
      "[Training Epoch 2] Batch 1601, Loss 0.28907981514930725\n",
      "[Training Epoch 2] Batch 1602, Loss 0.25753408670425415\n",
      "[Training Epoch 2] Batch 1603, Loss 0.2747894525527954\n",
      "[Training Epoch 2] Batch 1604, Loss 0.2884499728679657\n",
      "[Training Epoch 2] Batch 1605, Loss 0.30154871940612793\n",
      "[Training Epoch 2] Batch 1606, Loss 0.272787868976593\n",
      "[Training Epoch 2] Batch 1607, Loss 0.2858566641807556\n",
      "[Training Epoch 2] Batch 1608, Loss 0.2669055461883545\n",
      "[Training Epoch 2] Batch 1609, Loss 0.27187469601631165\n",
      "[Training Epoch 2] Batch 1610, Loss 0.2597231864929199\n",
      "[Training Epoch 2] Batch 1611, Loss 0.2633509039878845\n",
      "[Training Epoch 2] Batch 1612, Loss 0.2593929171562195\n",
      "[Training Epoch 2] Batch 1613, Loss 0.2622320055961609\n",
      "[Training Epoch 2] Batch 1614, Loss 0.2731022536754608\n",
      "[Training Epoch 2] Batch 1615, Loss 0.27041369676589966\n",
      "[Training Epoch 2] Batch 1616, Loss 0.2940671145915985\n",
      "[Training Epoch 2] Batch 1617, Loss 0.2805083394050598\n",
      "[Training Epoch 2] Batch 1618, Loss 0.2842952609062195\n",
      "[Training Epoch 2] Batch 1619, Loss 0.27801844477653503\n",
      "[Training Epoch 2] Batch 1620, Loss 0.2865149676799774\n",
      "[Training Epoch 2] Batch 1621, Loss 0.29576677083969116\n",
      "[Training Epoch 2] Batch 1622, Loss 0.32640087604522705\n",
      "[Training Epoch 2] Batch 1623, Loss 0.3117218315601349\n",
      "[Training Epoch 2] Batch 1624, Loss 0.28480595350265503\n",
      "[Training Epoch 2] Batch 1625, Loss 0.28392159938812256\n",
      "[Training Epoch 2] Batch 1626, Loss 0.281782329082489\n",
      "[Training Epoch 2] Batch 1627, Loss 0.3200470209121704\n",
      "[Training Epoch 2] Batch 1628, Loss 0.2818240523338318\n",
      "[Training Epoch 2] Batch 1629, Loss 0.27776122093200684\n",
      "[Training Epoch 2] Batch 1630, Loss 0.29247087240219116\n",
      "[Training Epoch 2] Batch 1631, Loss 0.2888023555278778\n",
      "[Training Epoch 2] Batch 1632, Loss 0.29242417216300964\n",
      "[Training Epoch 2] Batch 1633, Loss 0.2885690927505493\n",
      "[Training Epoch 2] Batch 1634, Loss 0.27617913484573364\n",
      "[Training Epoch 2] Batch 1635, Loss 0.2681378126144409\n",
      "[Training Epoch 2] Batch 1636, Loss 0.25892966985702515\n",
      "[Training Epoch 2] Batch 1637, Loss 0.2811192572116852\n",
      "[Training Epoch 2] Batch 1638, Loss 0.28274989128112793\n",
      "[Training Epoch 2] Batch 1639, Loss 0.2656201720237732\n",
      "[Training Epoch 2] Batch 1640, Loss 0.2740175426006317\n",
      "[Training Epoch 2] Batch 1641, Loss 0.24846522510051727\n",
      "[Training Epoch 2] Batch 1642, Loss 0.3044828474521637\n",
      "[Training Epoch 2] Batch 1643, Loss 0.28930673003196716\n",
      "[Training Epoch 2] Batch 1644, Loss 0.30092400312423706\n",
      "[Training Epoch 2] Batch 1645, Loss 0.23513200879096985\n",
      "[Training Epoch 2] Batch 1646, Loss 0.3061426877975464\n",
      "[Training Epoch 2] Batch 1647, Loss 0.2855098247528076\n",
      "[Training Epoch 2] Batch 1648, Loss 0.2989963889122009\n",
      "[Training Epoch 2] Batch 1649, Loss 0.26806995272636414\n",
      "[Training Epoch 2] Batch 1650, Loss 0.29323601722717285\n",
      "[Training Epoch 2] Batch 1651, Loss 0.27561524510383606\n",
      "[Training Epoch 2] Batch 1652, Loss 0.24680081009864807\n",
      "[Training Epoch 2] Batch 1653, Loss 0.2742716372013092\n",
      "[Training Epoch 2] Batch 1654, Loss 0.27000415325164795\n",
      "[Training Epoch 2] Batch 1655, Loss 0.3113113343715668\n",
      "[Training Epoch 2] Batch 1656, Loss 0.26007795333862305\n",
      "[Training Epoch 2] Batch 1657, Loss 0.27949631214141846\n",
      "[Training Epoch 2] Batch 1658, Loss 0.2552187442779541\n",
      "[Training Epoch 2] Batch 1659, Loss 0.2877172529697418\n",
      "[Training Epoch 2] Batch 1660, Loss 0.27619054913520813\n",
      "[Training Epoch 2] Batch 1661, Loss 0.28544554114341736\n",
      "[Training Epoch 2] Batch 1662, Loss 0.2771074175834656\n",
      "[Training Epoch 2] Batch 1663, Loss 0.2668572664260864\n",
      "[Training Epoch 2] Batch 1664, Loss 0.27886927127838135\n",
      "[Training Epoch 2] Batch 1665, Loss 0.26916879415512085\n",
      "[Training Epoch 2] Batch 1666, Loss 0.2788844108581543\n",
      "[Training Epoch 2] Batch 1667, Loss 0.2884340286254883\n",
      "[Training Epoch 2] Batch 1668, Loss 0.28044605255126953\n",
      "[Training Epoch 2] Batch 1669, Loss 0.2387874871492386\n",
      "[Training Epoch 2] Batch 1670, Loss 0.30143702030181885\n",
      "[Training Epoch 2] Batch 1671, Loss 0.272038996219635\n",
      "[Training Epoch 2] Batch 1672, Loss 0.3071630001068115\n",
      "[Training Epoch 2] Batch 1673, Loss 0.32873260974884033\n",
      "[Training Epoch 2] Batch 1674, Loss 0.29748713970184326\n",
      "[Training Epoch 2] Batch 1675, Loss 0.28253239393234253\n",
      "[Training Epoch 2] Batch 1676, Loss 0.26185160875320435\n",
      "[Training Epoch 2] Batch 1677, Loss 0.2431011199951172\n",
      "[Training Epoch 2] Batch 1678, Loss 0.24293413758277893\n",
      "[Training Epoch 2] Batch 1679, Loss 0.29462990164756775\n",
      "[Training Epoch 2] Batch 1680, Loss 0.2804141044616699\n",
      "[Training Epoch 2] Batch 1681, Loss 0.28694361448287964\n",
      "[Training Epoch 2] Batch 1682, Loss 0.25857850909233093\n",
      "[Training Epoch 2] Batch 1683, Loss 0.31077495217323303\n",
      "[Training Epoch 2] Batch 1684, Loss 0.2768443822860718\n",
      "[Training Epoch 2] Batch 1685, Loss 0.28853145241737366\n",
      "[Training Epoch 2] Batch 1686, Loss 0.2900829315185547\n",
      "[Training Epoch 2] Batch 1687, Loss 0.26890209317207336\n",
      "[Training Epoch 2] Batch 1688, Loss 0.28298288583755493\n",
      "[Training Epoch 2] Batch 1689, Loss 0.29510581493377686\n",
      "[Training Epoch 2] Batch 1690, Loss 0.2642865777015686\n",
      "[Training Epoch 2] Batch 1691, Loss 0.2832469642162323\n",
      "[Training Epoch 2] Batch 1692, Loss 0.28798866271972656\n",
      "[Training Epoch 2] Batch 1693, Loss 0.2732604742050171\n",
      "[Training Epoch 2] Batch 1694, Loss 0.2756691575050354\n",
      "[Training Epoch 2] Batch 1695, Loss 0.27929437160491943\n",
      "[Training Epoch 2] Batch 1696, Loss 0.2775765061378479\n",
      "[Training Epoch 2] Batch 1697, Loss 0.28901615738868713\n",
      "[Training Epoch 2] Batch 1698, Loss 0.289681077003479\n",
      "[Training Epoch 2] Batch 1699, Loss 0.2633504867553711\n",
      "[Training Epoch 2] Batch 1700, Loss 0.2806541621685028\n",
      "[Training Epoch 2] Batch 1701, Loss 0.3133155107498169\n",
      "[Training Epoch 2] Batch 1702, Loss 0.2886422872543335\n",
      "[Training Epoch 2] Batch 1703, Loss 0.26633965969085693\n",
      "[Training Epoch 2] Batch 1704, Loss 0.2413553148508072\n",
      "[Training Epoch 2] Batch 1705, Loss 0.2527275085449219\n",
      "[Training Epoch 2] Batch 1706, Loss 0.2825261652469635\n",
      "[Training Epoch 2] Batch 1707, Loss 0.2817493975162506\n",
      "[Training Epoch 2] Batch 1708, Loss 0.29958808422088623\n",
      "[Training Epoch 2] Batch 1709, Loss 0.3320978879928589\n",
      "[Training Epoch 2] Batch 1710, Loss 0.2659090757369995\n",
      "[Training Epoch 2] Batch 1711, Loss 0.31151092052459717\n",
      "[Training Epoch 2] Batch 1712, Loss 0.30650800466537476\n",
      "[Training Epoch 2] Batch 1713, Loss 0.2898627519607544\n",
      "[Training Epoch 2] Batch 1714, Loss 0.3095473051071167\n",
      "[Training Epoch 2] Batch 1715, Loss 0.27129238843917847\n",
      "[Training Epoch 2] Batch 1716, Loss 0.2835216522216797\n",
      "[Training Epoch 2] Batch 1717, Loss 0.2632080614566803\n",
      "[Training Epoch 2] Batch 1718, Loss 0.28780531883239746\n",
      "[Training Epoch 2] Batch 1719, Loss 0.26481378078460693\n",
      "[Training Epoch 2] Batch 1720, Loss 0.27764058113098145\n",
      "[Training Epoch 2] Batch 1721, Loss 0.29426711797714233\n",
      "[Training Epoch 2] Batch 1722, Loss 0.26686179637908936\n",
      "[Training Epoch 2] Batch 1723, Loss 0.27717629075050354\n",
      "[Training Epoch 2] Batch 1724, Loss 0.2725349962711334\n",
      "[Training Epoch 2] Batch 1725, Loss 0.3057694435119629\n",
      "[Training Epoch 2] Batch 1726, Loss 0.2814101576805115\n",
      "[Training Epoch 2] Batch 1727, Loss 0.25502172112464905\n",
      "[Training Epoch 2] Batch 1728, Loss 0.2604076862335205\n",
      "[Training Epoch 2] Batch 1729, Loss 0.26450902223587036\n",
      "[Training Epoch 2] Batch 1730, Loss 0.2565566599369049\n",
      "[Training Epoch 2] Batch 1731, Loss 0.3007048964500427\n",
      "[Training Epoch 2] Batch 1732, Loss 0.2662845849990845\n",
      "[Training Epoch 2] Batch 1733, Loss 0.3010212481021881\n",
      "[Training Epoch 2] Batch 1734, Loss 0.2750329077243805\n",
      "[Training Epoch 2] Batch 1735, Loss 0.2849876880645752\n",
      "[Training Epoch 2] Batch 1736, Loss 0.2592525780200958\n",
      "[Training Epoch 2] Batch 1737, Loss 0.28078746795654297\n",
      "[Training Epoch 2] Batch 1738, Loss 0.28363311290740967\n",
      "[Training Epoch 2] Batch 1739, Loss 0.2919469475746155\n",
      "[Training Epoch 2] Batch 1740, Loss 0.29129791259765625\n",
      "[Training Epoch 2] Batch 1741, Loss 0.24860116839408875\n",
      "[Training Epoch 2] Batch 1742, Loss 0.2719351649284363\n",
      "[Training Epoch 2] Batch 1743, Loss 0.2815958261489868\n",
      "[Training Epoch 2] Batch 1744, Loss 0.30290722846984863\n",
      "[Training Epoch 2] Batch 1745, Loss 0.2770845293998718\n",
      "[Training Epoch 2] Batch 1746, Loss 0.279843807220459\n",
      "[Training Epoch 2] Batch 1747, Loss 0.2927202880382538\n",
      "[Training Epoch 2] Batch 1748, Loss 0.3173529803752899\n",
      "[Training Epoch 2] Batch 1749, Loss 0.2762872576713562\n",
      "[Training Epoch 2] Batch 1750, Loss 0.27914994955062866\n",
      "[Training Epoch 2] Batch 1751, Loss 0.26813796162605286\n",
      "[Training Epoch 2] Batch 1752, Loss 0.2697766423225403\n",
      "[Training Epoch 2] Batch 1753, Loss 0.28556960821151733\n",
      "[Training Epoch 2] Batch 1754, Loss 0.26831257343292236\n",
      "[Training Epoch 2] Batch 1755, Loss 0.30207324028015137\n",
      "[Training Epoch 2] Batch 1756, Loss 0.2579740285873413\n",
      "[Training Epoch 2] Batch 1757, Loss 0.2704930305480957\n",
      "[Training Epoch 2] Batch 1758, Loss 0.2916151285171509\n",
      "[Training Epoch 2] Batch 1759, Loss 0.32747453451156616\n",
      "[Training Epoch 2] Batch 1760, Loss 0.2556619346141815\n",
      "[Training Epoch 2] Batch 1761, Loss 0.2565852701663971\n",
      "[Training Epoch 2] Batch 1762, Loss 0.2969730794429779\n",
      "[Training Epoch 2] Batch 1763, Loss 0.3049956262111664\n",
      "[Training Epoch 2] Batch 1764, Loss 0.29167088866233826\n",
      "[Training Epoch 2] Batch 1765, Loss 0.288323312997818\n",
      "[Training Epoch 2] Batch 1766, Loss 0.27660953998565674\n",
      "[Training Epoch 2] Batch 1767, Loss 0.2960168123245239\n",
      "[Training Epoch 2] Batch 1768, Loss 0.27720290422439575\n",
      "[Training Epoch 2] Batch 1769, Loss 0.3193259537220001\n",
      "[Training Epoch 2] Batch 1770, Loss 0.28520119190216064\n",
      "[Training Epoch 2] Batch 1771, Loss 0.2725869417190552\n",
      "[Training Epoch 2] Batch 1772, Loss 0.270039439201355\n",
      "[Training Epoch 2] Batch 1773, Loss 0.2689538598060608\n",
      "[Training Epoch 2] Batch 1774, Loss 0.2801833152770996\n",
      "[Training Epoch 2] Batch 1775, Loss 0.2935129404067993\n",
      "[Training Epoch 2] Batch 1776, Loss 0.2759747803211212\n",
      "[Training Epoch 2] Batch 1777, Loss 0.25724756717681885\n",
      "[Training Epoch 2] Batch 1778, Loss 0.3221684694290161\n",
      "[Training Epoch 2] Batch 1779, Loss 0.29288336634635925\n",
      "[Training Epoch 2] Batch 1780, Loss 0.2933835983276367\n",
      "[Training Epoch 2] Batch 1781, Loss 0.27189207077026367\n",
      "[Training Epoch 2] Batch 1782, Loss 0.2969546318054199\n",
      "[Training Epoch 2] Batch 1783, Loss 0.2682189345359802\n",
      "[Training Epoch 2] Batch 1784, Loss 0.28602710366249084\n",
      "[Training Epoch 2] Batch 1785, Loss 0.25390100479125977\n",
      "[Training Epoch 2] Batch 1786, Loss 0.2790383994579315\n",
      "[Training Epoch 2] Batch 1787, Loss 0.29315072298049927\n",
      "[Training Epoch 2] Batch 1788, Loss 0.28945744037628174\n",
      "[Training Epoch 2] Batch 1789, Loss 0.30170655250549316\n",
      "[Training Epoch 2] Batch 1790, Loss 0.2913864850997925\n",
      "[Training Epoch 2] Batch 1791, Loss 0.2718167006969452\n",
      "[Training Epoch 2] Batch 1792, Loss 0.30786243081092834\n",
      "[Training Epoch 2] Batch 1793, Loss 0.28272897005081177\n",
      "[Training Epoch 2] Batch 1794, Loss 0.25944843888282776\n",
      "[Training Epoch 2] Batch 1795, Loss 0.27212411165237427\n",
      "[Training Epoch 2] Batch 1796, Loss 0.2702902555465698\n",
      "[Training Epoch 2] Batch 1797, Loss 0.270978182554245\n",
      "[Training Epoch 2] Batch 1798, Loss 0.3072378635406494\n",
      "[Training Epoch 2] Batch 1799, Loss 0.281249463558197\n",
      "[Training Epoch 2] Batch 1800, Loss 0.29242029786109924\n",
      "[Training Epoch 2] Batch 1801, Loss 0.28346699476242065\n",
      "[Training Epoch 2] Batch 1802, Loss 0.29196876287460327\n",
      "[Training Epoch 2] Batch 1803, Loss 0.29678285121917725\n",
      "[Training Epoch 2] Batch 1804, Loss 0.2914317846298218\n",
      "[Training Epoch 2] Batch 1805, Loss 0.28864920139312744\n",
      "[Training Epoch 2] Batch 1806, Loss 0.255207896232605\n",
      "[Training Epoch 2] Batch 1807, Loss 0.30516958236694336\n",
      "[Training Epoch 2] Batch 1808, Loss 0.3021222651004791\n",
      "[Training Epoch 2] Batch 1809, Loss 0.2790926694869995\n",
      "[Training Epoch 2] Batch 1810, Loss 0.2883011996746063\n",
      "[Training Epoch 2] Batch 1811, Loss 0.2942328453063965\n",
      "[Training Epoch 2] Batch 1812, Loss 0.26803261041641235\n",
      "[Training Epoch 2] Batch 1813, Loss 0.2655753791332245\n",
      "[Training Epoch 2] Batch 1814, Loss 0.26863548159599304\n",
      "[Training Epoch 2] Batch 1815, Loss 0.2866557836532593\n",
      "[Training Epoch 2] Batch 1816, Loss 0.26355665922164917\n",
      "[Training Epoch 2] Batch 1817, Loss 0.28875732421875\n",
      "[Training Epoch 2] Batch 1818, Loss 0.27723759412765503\n",
      "[Training Epoch 2] Batch 1819, Loss 0.28887295722961426\n",
      "[Training Epoch 2] Batch 1820, Loss 0.24044516682624817\n",
      "[Training Epoch 2] Batch 1821, Loss 0.2674185633659363\n",
      "[Training Epoch 2] Batch 1822, Loss 0.28227779269218445\n",
      "[Training Epoch 2] Batch 1823, Loss 0.27856725454330444\n",
      "[Training Epoch 2] Batch 1824, Loss 0.27440184354782104\n",
      "[Training Epoch 2] Batch 1825, Loss 0.2885136008262634\n",
      "[Training Epoch 2] Batch 1826, Loss 0.27400246262550354\n",
      "[Training Epoch 2] Batch 1827, Loss 0.2922046482563019\n",
      "[Training Epoch 2] Batch 1828, Loss 0.31018930673599243\n",
      "[Training Epoch 2] Batch 1829, Loss 0.2865292727947235\n",
      "[Training Epoch 2] Batch 1830, Loss 0.27894431352615356\n",
      "[Training Epoch 2] Batch 1831, Loss 0.30307483673095703\n",
      "[Training Epoch 2] Batch 1832, Loss 0.2774573862552643\n",
      "[Training Epoch 2] Batch 1833, Loss 0.26486846804618835\n",
      "[Training Epoch 2] Batch 1834, Loss 0.29938191175460815\n",
      "[Training Epoch 2] Batch 1835, Loss 0.26980605721473694\n",
      "[Training Epoch 2] Batch 1836, Loss 0.2729070782661438\n",
      "[Training Epoch 2] Batch 1837, Loss 0.29025599360466003\n",
      "[Training Epoch 2] Batch 1838, Loss 0.2780730724334717\n",
      "[Training Epoch 2] Batch 1839, Loss 0.27934369444847107\n",
      "[Training Epoch 2] Batch 1840, Loss 0.29898104071617126\n",
      "[Training Epoch 2] Batch 1841, Loss 0.34029513597488403\n",
      "[Training Epoch 2] Batch 1842, Loss 0.2605988085269928\n",
      "[Training Epoch 2] Batch 1843, Loss 0.26041144132614136\n",
      "[Training Epoch 2] Batch 1844, Loss 0.29035133123397827\n",
      "[Training Epoch 2] Batch 1845, Loss 0.27576160430908203\n",
      "[Training Epoch 2] Batch 1846, Loss 0.27118176221847534\n",
      "[Training Epoch 2] Batch 1847, Loss 0.2653447389602661\n",
      "[Training Epoch 2] Batch 1848, Loss 0.279313862323761\n",
      "[Training Epoch 2] Batch 1849, Loss 0.2753177583217621\n",
      "[Training Epoch 2] Batch 1850, Loss 0.27337631583213806\n",
      "[Training Epoch 2] Batch 1851, Loss 0.2828669548034668\n",
      "[Training Epoch 2] Batch 1852, Loss 0.27109307050704956\n",
      "[Training Epoch 2] Batch 1853, Loss 0.25928008556365967\n",
      "[Training Epoch 2] Batch 1854, Loss 0.2815864086151123\n",
      "[Training Epoch 2] Batch 1855, Loss 0.3056207597255707\n",
      "[Training Epoch 2] Batch 1856, Loss 0.2511320412158966\n",
      "[Training Epoch 2] Batch 1857, Loss 0.26908302307128906\n",
      "[Training Epoch 2] Batch 1858, Loss 0.3051707148551941\n",
      "[Training Epoch 2] Batch 1859, Loss 0.2901926636695862\n",
      "[Training Epoch 2] Batch 1860, Loss 0.26467254757881165\n",
      "[Training Epoch 2] Batch 1861, Loss 0.25981026887893677\n",
      "[Training Epoch 2] Batch 1862, Loss 0.3038770258426666\n",
      "[Training Epoch 2] Batch 1863, Loss 0.30595827102661133\n",
      "[Training Epoch 2] Batch 1864, Loss 0.3120529353618622\n",
      "[Training Epoch 2] Batch 1865, Loss 0.2971045970916748\n",
      "[Training Epoch 2] Batch 1866, Loss 0.27317261695861816\n",
      "[Training Epoch 2] Batch 1867, Loss 0.291867196559906\n",
      "[Training Epoch 2] Batch 1868, Loss 0.26849913597106934\n",
      "[Training Epoch 2] Batch 1869, Loss 0.266960084438324\n",
      "[Training Epoch 2] Batch 1870, Loss 0.2921735346317291\n",
      "[Training Epoch 2] Batch 1871, Loss 0.2955506443977356\n",
      "[Training Epoch 2] Batch 1872, Loss 0.29382964968681335\n",
      "[Training Epoch 2] Batch 1873, Loss 0.2772282361984253\n",
      "[Training Epoch 2] Batch 1874, Loss 0.2529560327529907\n",
      "[Training Epoch 2] Batch 1875, Loss 0.2574108839035034\n",
      "[Training Epoch 2] Batch 1876, Loss 0.2852402925491333\n",
      "[Training Epoch 2] Batch 1877, Loss 0.2590155005455017\n",
      "[Training Epoch 2] Batch 1878, Loss 0.2780640125274658\n",
      "[Training Epoch 2] Batch 1879, Loss 0.2716313600540161\n",
      "[Training Epoch 2] Batch 1880, Loss 0.29179227352142334\n",
      "[Training Epoch 2] Batch 1881, Loss 0.29689714312553406\n",
      "[Training Epoch 2] Batch 1882, Loss 0.2657121419906616\n",
      "[Training Epoch 2] Batch 1883, Loss 0.2819538116455078\n",
      "[Training Epoch 2] Batch 1884, Loss 0.28912413120269775\n",
      "[Training Epoch 2] Batch 1885, Loss 0.2736339569091797\n",
      "[Training Epoch 2] Batch 1886, Loss 0.2729215621948242\n",
      "[Training Epoch 2] Batch 1887, Loss 0.28549349308013916\n",
      "[Training Epoch 2] Batch 1888, Loss 0.26928868889808655\n",
      "[Training Epoch 2] Batch 1889, Loss 0.2766912281513214\n",
      "[Training Epoch 2] Batch 1890, Loss 0.2598112225532532\n",
      "[Training Epoch 2] Batch 1891, Loss 0.3153156042098999\n",
      "[Training Epoch 2] Batch 1892, Loss 0.27987420558929443\n",
      "[Training Epoch 2] Batch 1893, Loss 0.29386770725250244\n",
      "[Training Epoch 2] Batch 1894, Loss 0.2922571301460266\n",
      "[Training Epoch 2] Batch 1895, Loss 0.3036865293979645\n",
      "[Training Epoch 2] Batch 1896, Loss 0.2708134651184082\n",
      "[Training Epoch 2] Batch 1897, Loss 0.24427014589309692\n",
      "[Training Epoch 2] Batch 1898, Loss 0.2844700217247009\n",
      "[Training Epoch 2] Batch 1899, Loss 0.2619342505931854\n",
      "[Training Epoch 2] Batch 1900, Loss 0.29063090682029724\n",
      "[Training Epoch 2] Batch 1901, Loss 0.3015187978744507\n",
      "[Training Epoch 2] Batch 1902, Loss 0.25503718852996826\n",
      "[Training Epoch 2] Batch 1903, Loss 0.30291029810905457\n",
      "[Training Epoch 2] Batch 1904, Loss 0.2841815650463104\n",
      "[Training Epoch 2] Batch 1905, Loss 0.2789768874645233\n",
      "[Training Epoch 2] Batch 1906, Loss 0.23715530335903168\n",
      "[Training Epoch 2] Batch 1907, Loss 0.27272236347198486\n",
      "[Training Epoch 2] Batch 1908, Loss 0.30262255668640137\n",
      "[Training Epoch 2] Batch 1909, Loss 0.25130587816238403\n",
      "[Training Epoch 2] Batch 1910, Loss 0.27728864550590515\n",
      "[Training Epoch 2] Batch 1911, Loss 0.32560670375823975\n",
      "[Training Epoch 2] Batch 1912, Loss 0.2820076644420624\n",
      "[Training Epoch 2] Batch 1913, Loss 0.2980037331581116\n",
      "[Training Epoch 2] Batch 1914, Loss 0.2684749662876129\n",
      "[Training Epoch 2] Batch 1915, Loss 0.308844655752182\n",
      "[Training Epoch 2] Batch 1916, Loss 0.3028828501701355\n",
      "[Training Epoch 2] Batch 1917, Loss 0.26318952441215515\n",
      "[Training Epoch 2] Batch 1918, Loss 0.2961311638355255\n",
      "[Training Epoch 2] Batch 1919, Loss 0.27316248416900635\n",
      "[Training Epoch 2] Batch 1920, Loss 0.29324477910995483\n",
      "[Training Epoch 2] Batch 1921, Loss 0.291452556848526\n",
      "[Training Epoch 2] Batch 1922, Loss 0.2655533254146576\n",
      "[Training Epoch 2] Batch 1923, Loss 0.2780824899673462\n",
      "[Training Epoch 2] Batch 1924, Loss 0.2919883131980896\n",
      "[Training Epoch 2] Batch 1925, Loss 0.2752484381198883\n",
      "[Training Epoch 2] Batch 1926, Loss 0.29036760330200195\n",
      "[Training Epoch 2] Batch 1927, Loss 0.2827926278114319\n",
      "[Training Epoch 2] Batch 1928, Loss 0.27742189168930054\n",
      "[Training Epoch 2] Batch 1929, Loss 0.28778591752052307\n",
      "[Training Epoch 2] Batch 1930, Loss 0.25776398181915283\n",
      "[Training Epoch 2] Batch 1931, Loss 0.26783859729766846\n",
      "[Training Epoch 2] Batch 1932, Loss 0.2800215780735016\n",
      "[Training Epoch 2] Batch 1933, Loss 0.25929534435272217\n",
      "[Training Epoch 2] Batch 1934, Loss 0.2787899971008301\n",
      "[Training Epoch 2] Batch 1935, Loss 0.3015751838684082\n",
      "[Training Epoch 2] Batch 1936, Loss 0.2945060133934021\n",
      "[Training Epoch 2] Batch 1937, Loss 0.2803027629852295\n",
      "[Training Epoch 2] Batch 1938, Loss 0.27806001901626587\n",
      "[Training Epoch 2] Batch 1939, Loss 0.2780385911464691\n",
      "[Training Epoch 2] Batch 1940, Loss 0.274442195892334\n",
      "[Training Epoch 2] Batch 1941, Loss 0.30753475427627563\n",
      "[Training Epoch 2] Batch 1942, Loss 0.3181425929069519\n",
      "[Training Epoch 2] Batch 1943, Loss 0.27524226903915405\n",
      "[Training Epoch 2] Batch 1944, Loss 0.28172004222869873\n",
      "[Training Epoch 2] Batch 1945, Loss 0.3100741505622864\n",
      "[Training Epoch 2] Batch 1946, Loss 0.2649114727973938\n",
      "[Training Epoch 2] Batch 1947, Loss 0.2603071331977844\n",
      "[Training Epoch 2] Batch 1948, Loss 0.29527750611305237\n",
      "[Training Epoch 2] Batch 1949, Loss 0.2876615822315216\n",
      "[Training Epoch 2] Batch 1950, Loss 0.28572261333465576\n",
      "[Training Epoch 2] Batch 1951, Loss 0.2759091854095459\n",
      "[Training Epoch 2] Batch 1952, Loss 0.2693299651145935\n",
      "[Training Epoch 2] Batch 1953, Loss 0.2528347074985504\n",
      "[Training Epoch 2] Batch 1954, Loss 0.29185914993286133\n",
      "[Training Epoch 2] Batch 1955, Loss 0.27909421920776367\n",
      "[Training Epoch 2] Batch 1956, Loss 0.29690319299697876\n",
      "[Training Epoch 2] Batch 1957, Loss 0.27196967601776123\n",
      "[Training Epoch 2] Batch 1958, Loss 0.2727063298225403\n",
      "[Training Epoch 2] Batch 1959, Loss 0.2812246084213257\n",
      "[Training Epoch 2] Batch 1960, Loss 0.25197863578796387\n",
      "[Training Epoch 2] Batch 1961, Loss 0.2916955351829529\n",
      "[Training Epoch 2] Batch 1962, Loss 0.2561380863189697\n",
      "[Training Epoch 2] Batch 1963, Loss 0.30163806676864624\n",
      "[Training Epoch 2] Batch 1964, Loss 0.2804015278816223\n",
      "[Training Epoch 2] Batch 1965, Loss 0.3010486960411072\n",
      "[Training Epoch 2] Batch 1966, Loss 0.26654529571533203\n",
      "[Training Epoch 2] Batch 1967, Loss 0.28400254249572754\n",
      "[Training Epoch 2] Batch 1968, Loss 0.2972715198993683\n",
      "[Training Epoch 2] Batch 1969, Loss 0.2900145947933197\n",
      "[Training Epoch 2] Batch 1970, Loss 0.26574599742889404\n",
      "[Training Epoch 2] Batch 1971, Loss 0.2773361802101135\n",
      "[Training Epoch 2] Batch 1972, Loss 0.273037850856781\n",
      "[Training Epoch 2] Batch 1973, Loss 0.2967000901699066\n",
      "[Training Epoch 2] Batch 1974, Loss 0.2656826376914978\n",
      "[Training Epoch 2] Batch 1975, Loss 0.28885722160339355\n",
      "[Training Epoch 2] Batch 1976, Loss 0.2726209759712219\n",
      "[Training Epoch 2] Batch 1977, Loss 0.29224464297294617\n",
      "[Training Epoch 2] Batch 1978, Loss 0.2742099165916443\n",
      "[Training Epoch 2] Batch 1979, Loss 0.2691345512866974\n",
      "[Training Epoch 2] Batch 1980, Loss 0.3061107397079468\n",
      "[Training Epoch 2] Batch 1981, Loss 0.26324474811553955\n",
      "[Training Epoch 2] Batch 1982, Loss 0.27959489822387695\n",
      "[Training Epoch 2] Batch 1983, Loss 0.2956703305244446\n",
      "[Training Epoch 2] Batch 1984, Loss 0.2999308109283447\n",
      "[Training Epoch 2] Batch 1985, Loss 0.275382936000824\n",
      "[Training Epoch 2] Batch 1986, Loss 0.2690903842449188\n",
      "[Training Epoch 2] Batch 1987, Loss 0.27056458592414856\n",
      "[Training Epoch 2] Batch 1988, Loss 0.285400390625\n",
      "[Training Epoch 2] Batch 1989, Loss 0.2963196635246277\n",
      "[Training Epoch 2] Batch 1990, Loss 0.3069145977497101\n",
      "[Training Epoch 2] Batch 1991, Loss 0.2662959098815918\n",
      "[Training Epoch 2] Batch 1992, Loss 0.26947736740112305\n",
      "[Training Epoch 2] Batch 1993, Loss 0.2837226390838623\n",
      "[Training Epoch 2] Batch 1994, Loss 0.2975807785987854\n",
      "[Training Epoch 2] Batch 1995, Loss 0.24952715635299683\n",
      "[Training Epoch 2] Batch 1996, Loss 0.2775423526763916\n",
      "[Training Epoch 2] Batch 1997, Loss 0.27812907099723816\n",
      "[Training Epoch 2] Batch 1998, Loss 0.25931793451309204\n",
      "[Training Epoch 2] Batch 1999, Loss 0.2848018407821655\n",
      "[Training Epoch 2] Batch 2000, Loss 0.28202521800994873\n",
      "[Training Epoch 2] Batch 2001, Loss 0.2993398606777191\n",
      "[Training Epoch 2] Batch 2002, Loss 0.2778491973876953\n",
      "[Training Epoch 2] Batch 2003, Loss 0.30020660161972046\n",
      "[Training Epoch 2] Batch 2004, Loss 0.2792823314666748\n",
      "[Training Epoch 2] Batch 2005, Loss 0.3070266842842102\n",
      "[Training Epoch 2] Batch 2006, Loss 0.322864830493927\n",
      "[Training Epoch 2] Batch 2007, Loss 0.2591506838798523\n",
      "[Training Epoch 2] Batch 2008, Loss 0.2713688015937805\n",
      "[Training Epoch 2] Batch 2009, Loss 0.2483108937740326\n",
      "[Training Epoch 2] Batch 2010, Loss 0.2565120458602905\n",
      "[Training Epoch 2] Batch 2011, Loss 0.3222063183784485\n",
      "[Training Epoch 2] Batch 2012, Loss 0.27609217166900635\n",
      "[Training Epoch 2] Batch 2013, Loss 0.26039764285087585\n",
      "[Training Epoch 2] Batch 2014, Loss 0.28136223554611206\n",
      "[Training Epoch 2] Batch 2015, Loss 0.2966986894607544\n",
      "[Training Epoch 2] Batch 2016, Loss 0.29609131813049316\n",
      "[Training Epoch 2] Batch 2017, Loss 0.27271562814712524\n",
      "[Training Epoch 2] Batch 2018, Loss 0.28079670667648315\n",
      "[Training Epoch 2] Batch 2019, Loss 0.2825847864151001\n",
      "[Training Epoch 2] Batch 2020, Loss 0.2826496660709381\n",
      "[Training Epoch 2] Batch 2021, Loss 0.2991272211074829\n",
      "[Training Epoch 2] Batch 2022, Loss 0.3018381595611572\n",
      "[Training Epoch 2] Batch 2023, Loss 0.2806963324546814\n",
      "[Training Epoch 2] Batch 2024, Loss 0.27360761165618896\n",
      "[Training Epoch 2] Batch 2025, Loss 0.2628059387207031\n",
      "[Training Epoch 2] Batch 2026, Loss 0.2702302038669586\n",
      "[Training Epoch 2] Batch 2027, Loss 0.29015544056892395\n",
      "[Training Epoch 2] Batch 2028, Loss 0.28850769996643066\n",
      "[Training Epoch 2] Batch 2029, Loss 0.30926889181137085\n",
      "[Training Epoch 2] Batch 2030, Loss 0.2872885465621948\n",
      "[Training Epoch 2] Batch 2031, Loss 0.26591819524765015\n",
      "[Training Epoch 2] Batch 2032, Loss 0.30628708004951477\n",
      "[Training Epoch 2] Batch 2033, Loss 0.2877315878868103\n",
      "[Training Epoch 2] Batch 2034, Loss 0.3323516845703125\n",
      "[Training Epoch 2] Batch 2035, Loss 0.28961408138275146\n",
      "[Training Epoch 2] Batch 2036, Loss 0.26283273100852966\n",
      "[Training Epoch 2] Batch 2037, Loss 0.2590593099594116\n",
      "[Training Epoch 2] Batch 2038, Loss 0.27732688188552856\n",
      "[Training Epoch 2] Batch 2039, Loss 0.2702900469303131\n",
      "[Training Epoch 2] Batch 2040, Loss 0.2853185534477234\n",
      "[Training Epoch 2] Batch 2041, Loss 0.29451748728752136\n",
      "[Training Epoch 2] Batch 2042, Loss 0.296137273311615\n",
      "[Training Epoch 2] Batch 2043, Loss 0.24220100045204163\n",
      "[Training Epoch 2] Batch 2044, Loss 0.2808358669281006\n",
      "[Training Epoch 2] Batch 2045, Loss 0.2791532278060913\n",
      "[Training Epoch 2] Batch 2046, Loss 0.28656071424484253\n",
      "[Training Epoch 2] Batch 2047, Loss 0.27222657203674316\n",
      "[Training Epoch 2] Batch 2048, Loss 0.3174545168876648\n",
      "[Training Epoch 2] Batch 2049, Loss 0.2684342861175537\n",
      "[Training Epoch 2] Batch 2050, Loss 0.27340638637542725\n",
      "[Training Epoch 2] Batch 2051, Loss 0.2735840082168579\n",
      "[Training Epoch 2] Batch 2052, Loss 0.287441223859787\n",
      "[Training Epoch 2] Batch 2053, Loss 0.2693715989589691\n",
      "[Training Epoch 2] Batch 2054, Loss 0.2985897362232208\n",
      "[Training Epoch 2] Batch 2055, Loss 0.25980544090270996\n",
      "[Training Epoch 2] Batch 2056, Loss 0.2917135953903198\n",
      "[Training Epoch 2] Batch 2057, Loss 0.2823413610458374\n",
      "[Training Epoch 2] Batch 2058, Loss 0.27362602949142456\n",
      "[Training Epoch 2] Batch 2059, Loss 0.298933207988739\n",
      "[Training Epoch 2] Batch 2060, Loss 0.2909114360809326\n",
      "[Training Epoch 2] Batch 2061, Loss 0.27749425172805786\n",
      "[Training Epoch 2] Batch 2062, Loss 0.2797132432460785\n",
      "[Training Epoch 2] Batch 2063, Loss 0.26739683747291565\n",
      "[Training Epoch 2] Batch 2064, Loss 0.2713698148727417\n",
      "[Training Epoch 2] Batch 2065, Loss 0.25403183698654175\n",
      "[Training Epoch 2] Batch 2066, Loss 0.2595336437225342\n",
      "[Training Epoch 2] Batch 2067, Loss 0.28871726989746094\n",
      "[Training Epoch 2] Batch 2068, Loss 0.2783231735229492\n",
      "[Training Epoch 2] Batch 2069, Loss 0.27594172954559326\n",
      "[Training Epoch 2] Batch 2070, Loss 0.25996339321136475\n",
      "[Training Epoch 2] Batch 2071, Loss 0.2780245840549469\n",
      "[Training Epoch 2] Batch 2072, Loss 0.25638940930366516\n",
      "[Training Epoch 2] Batch 2073, Loss 0.27778860926628113\n",
      "[Training Epoch 2] Batch 2074, Loss 0.29414239525794983\n",
      "[Training Epoch 2] Batch 2075, Loss 0.28212419152259827\n",
      "[Training Epoch 2] Batch 2076, Loss 0.27750223875045776\n",
      "[Training Epoch 2] Batch 2077, Loss 0.2913130819797516\n",
      "[Training Epoch 2] Batch 2078, Loss 0.2686832547187805\n",
      "[Training Epoch 2] Batch 2079, Loss 0.316935658454895\n",
      "[Training Epoch 2] Batch 2080, Loss 0.31743112206459045\n",
      "[Training Epoch 2] Batch 2081, Loss 0.2638995051383972\n",
      "[Training Epoch 2] Batch 2082, Loss 0.260116308927536\n",
      "[Training Epoch 2] Batch 2083, Loss 0.24012136459350586\n",
      "[Training Epoch 2] Batch 2084, Loss 0.2880266010761261\n",
      "[Training Epoch 2] Batch 2085, Loss 0.27119073271751404\n",
      "[Training Epoch 2] Batch 2086, Loss 0.2824835181236267\n",
      "[Training Epoch 2] Batch 2087, Loss 0.28472644090652466\n",
      "[Training Epoch 2] Batch 2088, Loss 0.2932756543159485\n",
      "[Training Epoch 2] Batch 2089, Loss 0.28379392623901367\n",
      "[Training Epoch 2] Batch 2090, Loss 0.2723953127861023\n",
      "[Training Epoch 2] Batch 2091, Loss 0.30188125371932983\n",
      "[Training Epoch 2] Batch 2092, Loss 0.30087876319885254\n",
      "[Training Epoch 2] Batch 2093, Loss 0.2915602922439575\n",
      "[Training Epoch 2] Batch 2094, Loss 0.27961933612823486\n",
      "[Training Epoch 2] Batch 2095, Loss 0.28095340728759766\n",
      "[Training Epoch 2] Batch 2096, Loss 0.28659284114837646\n",
      "[Training Epoch 2] Batch 2097, Loss 0.3061862587928772\n",
      "[Training Epoch 2] Batch 2098, Loss 0.29699671268463135\n",
      "[Training Epoch 2] Batch 2099, Loss 0.2849760353565216\n",
      "[Training Epoch 2] Batch 2100, Loss 0.24670612812042236\n",
      "[Training Epoch 2] Batch 2101, Loss 0.291517972946167\n",
      "[Training Epoch 2] Batch 2102, Loss 0.26453617215156555\n",
      "[Training Epoch 2] Batch 2103, Loss 0.32530081272125244\n",
      "[Training Epoch 2] Batch 2104, Loss 0.28885871171951294\n",
      "[Training Epoch 2] Batch 2105, Loss 0.2891022562980652\n",
      "[Training Epoch 2] Batch 2106, Loss 0.26408764719963074\n",
      "[Training Epoch 2] Batch 2107, Loss 0.27206072211265564\n",
      "[Training Epoch 2] Batch 2108, Loss 0.2811320424079895\n",
      "[Training Epoch 2] Batch 2109, Loss 0.28627508878707886\n",
      "[Training Epoch 2] Batch 2110, Loss 0.2724442481994629\n",
      "[Training Epoch 2] Batch 2111, Loss 0.2684291899204254\n",
      "[Training Epoch 2] Batch 2112, Loss 0.26403963565826416\n",
      "[Training Epoch 2] Batch 2113, Loss 0.2970051169395447\n",
      "[Training Epoch 2] Batch 2114, Loss 0.28384867310523987\n",
      "[Training Epoch 2] Batch 2115, Loss 0.27240562438964844\n",
      "[Training Epoch 2] Batch 2116, Loss 0.28536707162857056\n",
      "[Training Epoch 2] Batch 2117, Loss 0.28588810563087463\n",
      "[Training Epoch 2] Batch 2118, Loss 0.30865994095802307\n",
      "[Training Epoch 2] Batch 2119, Loss 0.2702261209487915\n",
      "[Training Epoch 2] Batch 2120, Loss 0.2416876256465912\n",
      "[Training Epoch 2] Batch 2121, Loss 0.2863656282424927\n",
      "[Training Epoch 2] Batch 2122, Loss 0.270760178565979\n",
      "[Training Epoch 2] Batch 2123, Loss 0.24674612283706665\n",
      "[Training Epoch 2] Batch 2124, Loss 0.29485684633255005\n",
      "[Training Epoch 2] Batch 2125, Loss 0.26739922165870667\n",
      "[Training Epoch 2] Batch 2126, Loss 0.2756829261779785\n",
      "[Training Epoch 2] Batch 2127, Loss 0.3030749261379242\n",
      "[Training Epoch 2] Batch 2128, Loss 0.2677338719367981\n",
      "[Training Epoch 2] Batch 2129, Loss 0.26493632793426514\n",
      "[Training Epoch 2] Batch 2130, Loss 0.2801775336265564\n",
      "[Training Epoch 2] Batch 2131, Loss 0.2879081964492798\n",
      "[Training Epoch 2] Batch 2132, Loss 0.2756289839744568\n",
      "[Training Epoch 2] Batch 2133, Loss 0.29042112827301025\n",
      "[Training Epoch 2] Batch 2134, Loss 0.29091885685920715\n",
      "[Training Epoch 2] Batch 2135, Loss 0.2855801582336426\n",
      "[Training Epoch 2] Batch 2136, Loss 0.2903384268283844\n",
      "[Training Epoch 2] Batch 2137, Loss 0.2563021183013916\n",
      "[Training Epoch 2] Batch 2138, Loss 0.2970670461654663\n",
      "[Training Epoch 2] Batch 2139, Loss 0.27669572830200195\n",
      "[Training Epoch 2] Batch 2140, Loss 0.24620310962200165\n",
      "[Training Epoch 2] Batch 2141, Loss 0.25163039565086365\n",
      "[Training Epoch 2] Batch 2142, Loss 0.263248085975647\n",
      "[Training Epoch 2] Batch 2143, Loss 0.28987663984298706\n",
      "[Training Epoch 2] Batch 2144, Loss 0.286279559135437\n",
      "[Training Epoch 2] Batch 2145, Loss 0.27399516105651855\n",
      "[Training Epoch 2] Batch 2146, Loss 0.2968546152114868\n",
      "[Training Epoch 2] Batch 2147, Loss 0.3049573302268982\n",
      "[Training Epoch 2] Batch 2148, Loss 0.28362083435058594\n",
      "[Training Epoch 2] Batch 2149, Loss 0.30176424980163574\n",
      "[Training Epoch 2] Batch 2150, Loss 0.2913878560066223\n",
      "[Training Epoch 2] Batch 2151, Loss 0.3038068413734436\n",
      "[Training Epoch 2] Batch 2152, Loss 0.27226006984710693\n",
      "[Training Epoch 2] Batch 2153, Loss 0.26805776357650757\n",
      "[Training Epoch 2] Batch 2154, Loss 0.2652416229248047\n",
      "[Training Epoch 2] Batch 2155, Loss 0.27766650915145874\n",
      "[Training Epoch 2] Batch 2156, Loss 0.2703656256198883\n",
      "[Training Epoch 2] Batch 2157, Loss 0.25716453790664673\n",
      "[Training Epoch 2] Batch 2158, Loss 0.2704693377017975\n",
      "[Training Epoch 2] Batch 2159, Loss 0.28918129205703735\n",
      "[Training Epoch 2] Batch 2160, Loss 0.27331268787384033\n",
      "[Training Epoch 2] Batch 2161, Loss 0.27390262484550476\n",
      "[Training Epoch 2] Batch 2162, Loss 0.28413125872612\n",
      "[Training Epoch 2] Batch 2163, Loss 0.2959572970867157\n",
      "[Training Epoch 2] Batch 2164, Loss 0.2950841784477234\n",
      "[Training Epoch 2] Batch 2165, Loss 0.2800903618335724\n",
      "[Training Epoch 2] Batch 2166, Loss 0.2928101420402527\n",
      "[Training Epoch 2] Batch 2167, Loss 0.2947516143321991\n",
      "[Training Epoch 2] Batch 2168, Loss 0.27388080954551697\n",
      "[Training Epoch 2] Batch 2169, Loss 0.28908151388168335\n",
      "[Training Epoch 2] Batch 2170, Loss 0.28977397084236145\n",
      "[Training Epoch 2] Batch 2171, Loss 0.2879706025123596\n",
      "[Training Epoch 2] Batch 2172, Loss 0.28692901134490967\n",
      "[Training Epoch 2] Batch 2173, Loss 0.25451597571372986\n",
      "[Training Epoch 2] Batch 2174, Loss 0.28764814138412476\n",
      "[Training Epoch 2] Batch 2175, Loss 0.26042088866233826\n",
      "[Training Epoch 2] Batch 2176, Loss 0.2780626714229584\n",
      "[Training Epoch 2] Batch 2177, Loss 0.2629786431789398\n",
      "[Training Epoch 2] Batch 2178, Loss 0.30634433031082153\n",
      "[Training Epoch 2] Batch 2179, Loss 0.25912022590637207\n",
      "[Training Epoch 2] Batch 2180, Loss 0.28645023703575134\n",
      "[Training Epoch 2] Batch 2181, Loss 0.27037492394447327\n",
      "[Training Epoch 2] Batch 2182, Loss 0.3005112409591675\n",
      "[Training Epoch 2] Batch 2183, Loss 0.2656870186328888\n",
      "[Training Epoch 2] Batch 2184, Loss 0.2716869115829468\n",
      "[Training Epoch 2] Batch 2185, Loss 0.28672850131988525\n",
      "[Training Epoch 2] Batch 2186, Loss 0.3032161295413971\n",
      "[Training Epoch 2] Batch 2187, Loss 0.2742454707622528\n",
      "[Training Epoch 2] Batch 2188, Loss 0.3055182695388794\n",
      "[Training Epoch 2] Batch 2189, Loss 0.27294379472732544\n",
      "[Training Epoch 2] Batch 2190, Loss 0.3044915795326233\n",
      "[Training Epoch 2] Batch 2191, Loss 0.277474582195282\n",
      "[Training Epoch 2] Batch 2192, Loss 0.28372982144355774\n",
      "[Training Epoch 2] Batch 2193, Loss 0.3054666519165039\n",
      "[Training Epoch 2] Batch 2194, Loss 0.3029363751411438\n",
      "[Training Epoch 2] Batch 2195, Loss 0.26697662472724915\n",
      "[Training Epoch 2] Batch 2196, Loss 0.30251413583755493\n",
      "[Training Epoch 2] Batch 2197, Loss 0.26347774267196655\n",
      "[Training Epoch 2] Batch 2198, Loss 0.29639899730682373\n",
      "[Training Epoch 2] Batch 2199, Loss 0.29289621114730835\n",
      "[Training Epoch 2] Batch 2200, Loss 0.27263230085372925\n",
      "[Training Epoch 2] Batch 2201, Loss 0.2619079351425171\n",
      "[Training Epoch 2] Batch 2202, Loss 0.29738444089889526\n",
      "[Training Epoch 2] Batch 2203, Loss 0.267886757850647\n",
      "[Training Epoch 2] Batch 2204, Loss 0.314703106880188\n",
      "[Training Epoch 2] Batch 2205, Loss 0.24062588810920715\n",
      "[Training Epoch 2] Batch 2206, Loss 0.26070988178253174\n",
      "[Training Epoch 2] Batch 2207, Loss 0.29798445105552673\n",
      "[Training Epoch 2] Batch 2208, Loss 0.3044195771217346\n",
      "[Training Epoch 2] Batch 2209, Loss 0.2487935721874237\n",
      "[Training Epoch 2] Batch 2210, Loss 0.2482323795557022\n",
      "[Training Epoch 2] Batch 2211, Loss 0.27880406379699707\n",
      "[Training Epoch 2] Batch 2212, Loss 0.2861860394477844\n",
      "[Training Epoch 2] Batch 2213, Loss 0.2607351839542389\n",
      "[Training Epoch 2] Batch 2214, Loss 0.27695393562316895\n",
      "[Training Epoch 2] Batch 2215, Loss 0.29136988520622253\n",
      "[Training Epoch 2] Batch 2216, Loss 0.25566360354423523\n",
      "[Training Epoch 2] Batch 2217, Loss 0.29823777079582214\n",
      "[Training Epoch 2] Batch 2218, Loss 0.274393767118454\n",
      "[Training Epoch 2] Batch 2219, Loss 0.2832850217819214\n",
      "[Training Epoch 2] Batch 2220, Loss 0.29853397607803345\n",
      "[Training Epoch 2] Batch 2221, Loss 0.29793602228164673\n",
      "[Training Epoch 2] Batch 2222, Loss 0.28437480330467224\n",
      "[Training Epoch 2] Batch 2223, Loss 0.2780625522136688\n",
      "[Training Epoch 2] Batch 2224, Loss 0.285229355096817\n",
      "[Training Epoch 2] Batch 2225, Loss 0.2723656892776489\n",
      "[Training Epoch 2] Batch 2226, Loss 0.2892301082611084\n",
      "[Training Epoch 2] Batch 2227, Loss 0.24659046530723572\n",
      "[Training Epoch 2] Batch 2228, Loss 0.2809775769710541\n",
      "[Training Epoch 2] Batch 2229, Loss 0.2669391334056854\n",
      "[Training Epoch 2] Batch 2230, Loss 0.2972725033760071\n",
      "[Training Epoch 2] Batch 2231, Loss 0.2688148319721222\n",
      "[Training Epoch 2] Batch 2232, Loss 0.24308383464813232\n",
      "[Training Epoch 2] Batch 2233, Loss 0.2803117632865906\n",
      "[Training Epoch 2] Batch 2234, Loss 0.3010207414627075\n",
      "[Training Epoch 2] Batch 2235, Loss 0.2806701064109802\n",
      "[Training Epoch 2] Batch 2236, Loss 0.24537859857082367\n",
      "[Training Epoch 2] Batch 2237, Loss 0.3147825598716736\n",
      "[Training Epoch 2] Batch 2238, Loss 0.2739316523075104\n",
      "[Training Epoch 2] Batch 2239, Loss 0.2746594548225403\n",
      "[Training Epoch 2] Batch 2240, Loss 0.2659451961517334\n",
      "[Training Epoch 2] Batch 2241, Loss 0.2262439727783203\n",
      "[Training Epoch 2] Batch 2242, Loss 0.28646183013916016\n",
      "[Training Epoch 2] Batch 2243, Loss 0.2886425852775574\n",
      "[Training Epoch 2] Batch 2244, Loss 0.2659960389137268\n",
      "[Training Epoch 2] Batch 2245, Loss 0.26562145352363586\n",
      "[Training Epoch 2] Batch 2246, Loss 0.24279941618442535\n",
      "[Training Epoch 2] Batch 2247, Loss 0.3003949224948883\n",
      "[Training Epoch 2] Batch 2248, Loss 0.27602308988571167\n",
      "[Training Epoch 2] Batch 2249, Loss 0.29422447085380554\n",
      "[Training Epoch 2] Batch 2250, Loss 0.27728068828582764\n",
      "[Training Epoch 2] Batch 2251, Loss 0.3025769591331482\n",
      "[Training Epoch 2] Batch 2252, Loss 0.32690632343292236\n",
      "[Training Epoch 2] Batch 2253, Loss 0.2729801535606384\n",
      "[Training Epoch 2] Batch 2254, Loss 0.29000502824783325\n",
      "[Training Epoch 2] Batch 2255, Loss 0.28361645340919495\n",
      "[Training Epoch 2] Batch 2256, Loss 0.25742971897125244\n",
      "[Training Epoch 2] Batch 2257, Loss 0.25133228302001953\n",
      "[Training Epoch 2] Batch 2258, Loss 0.28048044443130493\n",
      "[Training Epoch 2] Batch 2259, Loss 0.2510637640953064\n",
      "[Training Epoch 2] Batch 2260, Loss 0.2700185775756836\n",
      "[Training Epoch 2] Batch 2261, Loss 0.30159425735473633\n",
      "[Training Epoch 2] Batch 2262, Loss 0.2670067846775055\n",
      "[Training Epoch 2] Batch 2263, Loss 0.27246618270874023\n",
      "[Training Epoch 2] Batch 2264, Loss 0.2942436933517456\n",
      "[Training Epoch 2] Batch 2265, Loss 0.25454074144363403\n",
      "[Training Epoch 2] Batch 2266, Loss 0.3099237084388733\n",
      "[Training Epoch 2] Batch 2267, Loss 0.30471745133399963\n",
      "[Training Epoch 2] Batch 2268, Loss 0.2750677466392517\n",
      "[Training Epoch 2] Batch 2269, Loss 0.26261094212532043\n",
      "[Training Epoch 2] Batch 2270, Loss 0.27942565083503723\n",
      "[Training Epoch 2] Batch 2271, Loss 0.3058101236820221\n",
      "[Training Epoch 2] Batch 2272, Loss 0.26883310079574585\n",
      "[Training Epoch 2] Batch 2273, Loss 0.2643197774887085\n",
      "[Training Epoch 2] Batch 2274, Loss 0.2516447901725769\n",
      "[Training Epoch 2] Batch 2275, Loss 0.29845142364501953\n",
      "[Training Epoch 2] Batch 2276, Loss 0.2518135607242584\n",
      "[Training Epoch 2] Batch 2277, Loss 0.2909749448299408\n",
      "[Training Epoch 2] Batch 2278, Loss 0.2600342035293579\n",
      "[Training Epoch 2] Batch 2279, Loss 0.30928945541381836\n",
      "[Training Epoch 2] Batch 2280, Loss 0.27027228474617004\n",
      "[Training Epoch 2] Batch 2281, Loss 0.2686992287635803\n",
      "[Training Epoch 2] Batch 2282, Loss 0.3186706304550171\n",
      "[Training Epoch 2] Batch 2283, Loss 0.26900607347488403\n",
      "[Training Epoch 2] Batch 2284, Loss 0.30806630849838257\n",
      "[Training Epoch 2] Batch 2285, Loss 0.3251684606075287\n",
      "[Training Epoch 2] Batch 2286, Loss 0.3013764023780823\n",
      "[Training Epoch 2] Batch 2287, Loss 0.27548909187316895\n",
      "[Training Epoch 2] Batch 2288, Loss 0.2698838710784912\n",
      "[Training Epoch 2] Batch 2289, Loss 0.2805311679840088\n",
      "[Training Epoch 2] Batch 2290, Loss 0.2771676778793335\n",
      "[Training Epoch 2] Batch 2291, Loss 0.2762105464935303\n",
      "[Training Epoch 2] Batch 2292, Loss 0.28032249212265015\n",
      "[Training Epoch 2] Batch 2293, Loss 0.2917322516441345\n",
      "[Training Epoch 2] Batch 2294, Loss 0.2841029167175293\n",
      "[Training Epoch 2] Batch 2295, Loss 0.284348726272583\n",
      "[Training Epoch 2] Batch 2296, Loss 0.2988539934158325\n",
      "[Training Epoch 2] Batch 2297, Loss 0.28476306796073914\n",
      "[Training Epoch 2] Batch 2298, Loss 0.28937944769859314\n",
      "[Training Epoch 2] Batch 2299, Loss 0.2901756465435028\n",
      "[Training Epoch 2] Batch 2300, Loss 0.2730451822280884\n",
      "[Training Epoch 2] Batch 2301, Loss 0.285954087972641\n",
      "[Training Epoch 2] Batch 2302, Loss 0.29968249797821045\n",
      "[Training Epoch 2] Batch 2303, Loss 0.2695041000843048\n",
      "[Training Epoch 2] Batch 2304, Loss 0.3038884699344635\n",
      "[Training Epoch 2] Batch 2305, Loss 0.29409775137901306\n",
      "[Training Epoch 2] Batch 2306, Loss 0.2716134488582611\n",
      "[Training Epoch 2] Batch 2307, Loss 0.30272048711776733\n",
      "[Training Epoch 2] Batch 2308, Loss 0.25091466307640076\n",
      "[Training Epoch 2] Batch 2309, Loss 0.2863803207874298\n",
      "[Training Epoch 2] Batch 2310, Loss 0.26253634691238403\n",
      "[Training Epoch 2] Batch 2311, Loss 0.23904626071453094\n",
      "[Training Epoch 2] Batch 2312, Loss 0.2861972451210022\n",
      "[Training Epoch 2] Batch 2313, Loss 0.25402185320854187\n",
      "[Training Epoch 2] Batch 2314, Loss 0.2995292842388153\n",
      "[Training Epoch 2] Batch 2315, Loss 0.3090091347694397\n",
      "[Training Epoch 2] Batch 2316, Loss 0.27727246284484863\n",
      "[Training Epoch 2] Batch 2317, Loss 0.30685603618621826\n",
      "[Training Epoch 2] Batch 2318, Loss 0.2684709131717682\n",
      "[Training Epoch 2] Batch 2319, Loss 0.30305129289627075\n",
      "[Training Epoch 2] Batch 2320, Loss 0.29472213983535767\n",
      "[Training Epoch 2] Batch 2321, Loss 0.2819463908672333\n",
      "[Training Epoch 2] Batch 2322, Loss 0.2834416627883911\n",
      "[Training Epoch 2] Batch 2323, Loss 0.27552470564842224\n",
      "[Training Epoch 2] Batch 2324, Loss 0.2628468871116638\n",
      "[Training Epoch 2] Batch 2325, Loss 0.28465646505355835\n",
      "[Training Epoch 2] Batch 2326, Loss 0.2825188934803009\n",
      "[Training Epoch 2] Batch 2327, Loss 0.2847757637500763\n",
      "[Training Epoch 2] Batch 2328, Loss 0.28599026799201965\n",
      "[Training Epoch 2] Batch 2329, Loss 0.2794197201728821\n",
      "[Training Epoch 2] Batch 2330, Loss 0.2627427577972412\n",
      "[Training Epoch 2] Batch 2331, Loss 0.2920868992805481\n",
      "[Training Epoch 2] Batch 2332, Loss 0.26440754532814026\n",
      "[Training Epoch 2] Batch 2333, Loss 0.26912376284599304\n",
      "[Training Epoch 2] Batch 2334, Loss 0.30016377568244934\n",
      "[Training Epoch 2] Batch 2335, Loss 0.3102930784225464\n",
      "[Training Epoch 2] Batch 2336, Loss 0.2589266300201416\n",
      "[Training Epoch 2] Batch 2337, Loss 0.25611090660095215\n",
      "[Training Epoch 2] Batch 2338, Loss 0.282244473695755\n",
      "[Training Epoch 2] Batch 2339, Loss 0.27710920572280884\n",
      "[Training Epoch 2] Batch 2340, Loss 0.2741248607635498\n",
      "[Training Epoch 2] Batch 2341, Loss 0.30709439516067505\n",
      "[Training Epoch 2] Batch 2342, Loss 0.29483264684677124\n",
      "[Training Epoch 2] Batch 2343, Loss 0.25875502824783325\n",
      "[Training Epoch 2] Batch 2344, Loss 0.27126145362854004\n",
      "[Training Epoch 2] Batch 2345, Loss 0.32293111085891724\n",
      "[Training Epoch 2] Batch 2346, Loss 0.2883659601211548\n",
      "[Training Epoch 2] Batch 2347, Loss 0.2665230631828308\n",
      "[Training Epoch 2] Batch 2348, Loss 0.26535797119140625\n",
      "[Training Epoch 2] Batch 2349, Loss 0.26937705278396606\n",
      "[Training Epoch 2] Batch 2350, Loss 0.26190051436424255\n",
      "[Training Epoch 2] Batch 2351, Loss 0.2657623887062073\n",
      "[Training Epoch 2] Batch 2352, Loss 0.26876822113990784\n",
      "[Training Epoch 2] Batch 2353, Loss 0.29315152764320374\n",
      "[Training Epoch 2] Batch 2354, Loss 0.2582527697086334\n",
      "[Training Epoch 2] Batch 2355, Loss 0.2911475896835327\n",
      "[Training Epoch 2] Batch 2356, Loss 0.2984044551849365\n",
      "[Training Epoch 2] Batch 2357, Loss 0.30619144439697266\n",
      "[Training Epoch 2] Batch 2358, Loss 0.26297861337661743\n",
      "[Training Epoch 2] Batch 2359, Loss 0.2805679142475128\n",
      "[Training Epoch 2] Batch 2360, Loss 0.3082228899002075\n",
      "[Training Epoch 2] Batch 2361, Loss 0.2760659456253052\n",
      "[Training Epoch 2] Batch 2362, Loss 0.2898571789264679\n",
      "[Training Epoch 2] Batch 2363, Loss 0.3031213879585266\n",
      "[Training Epoch 2] Batch 2364, Loss 0.2899121046066284\n",
      "[Training Epoch 2] Batch 2365, Loss 0.28470391035079956\n",
      "[Training Epoch 2] Batch 2366, Loss 0.2608698904514313\n",
      "[Training Epoch 2] Batch 2367, Loss 0.27964138984680176\n",
      "[Training Epoch 2] Batch 2368, Loss 0.2719196081161499\n",
      "[Training Epoch 2] Batch 2369, Loss 0.27203086018562317\n",
      "[Training Epoch 2] Batch 2370, Loss 0.26538020372390747\n",
      "[Training Epoch 2] Batch 2371, Loss 0.26848310232162476\n",
      "[Training Epoch 2] Batch 2372, Loss 0.2629547715187073\n",
      "[Training Epoch 2] Batch 2373, Loss 0.28958457708358765\n",
      "[Training Epoch 2] Batch 2374, Loss 0.2784309685230255\n",
      "[Training Epoch 2] Batch 2375, Loss 0.28473925590515137\n",
      "[Training Epoch 2] Batch 2376, Loss 0.29084140062332153\n",
      "[Training Epoch 2] Batch 2377, Loss 0.2836335301399231\n",
      "[Training Epoch 2] Batch 2378, Loss 0.29548966884613037\n",
      "[Training Epoch 2] Batch 2379, Loss 0.2679484188556671\n",
      "[Training Epoch 2] Batch 2380, Loss 0.31621408462524414\n",
      "[Training Epoch 2] Batch 2381, Loss 0.2706356942653656\n",
      "[Training Epoch 2] Batch 2382, Loss 0.30851292610168457\n",
      "[Training Epoch 2] Batch 2383, Loss 0.27262037992477417\n",
      "[Training Epoch 2] Batch 2384, Loss 0.27318865060806274\n",
      "[Training Epoch 2] Batch 2385, Loss 0.26993292570114136\n",
      "[Training Epoch 2] Batch 2386, Loss 0.26779529452323914\n",
      "[Training Epoch 2] Batch 2387, Loss 0.29464206099510193\n",
      "[Training Epoch 2] Batch 2388, Loss 0.2784824073314667\n",
      "[Training Epoch 2] Batch 2389, Loss 0.3042544722557068\n",
      "[Training Epoch 2] Batch 2390, Loss 0.27895259857177734\n",
      "[Training Epoch 2] Batch 2391, Loss 0.2539701461791992\n",
      "[Training Epoch 2] Batch 2392, Loss 0.2658969759941101\n",
      "[Training Epoch 2] Batch 2393, Loss 0.26973849534988403\n",
      "[Training Epoch 2] Batch 2394, Loss 0.2790149450302124\n",
      "[Training Epoch 2] Batch 2395, Loss 0.25952810049057007\n",
      "[Training Epoch 2] Batch 2396, Loss 0.29124248027801514\n",
      "[Training Epoch 2] Batch 2397, Loss 0.30130863189697266\n",
      "[Training Epoch 2] Batch 2398, Loss 0.2599712014198303\n",
      "[Training Epoch 2] Batch 2399, Loss 0.2698405385017395\n",
      "[Training Epoch 2] Batch 2400, Loss 0.2616719901561737\n",
      "[Training Epoch 2] Batch 2401, Loss 0.25911271572113037\n",
      "[Training Epoch 2] Batch 2402, Loss 0.28223299980163574\n",
      "[Training Epoch 2] Batch 2403, Loss 0.266448974609375\n",
      "[Training Epoch 2] Batch 2404, Loss 0.2521940767765045\n",
      "[Training Epoch 2] Batch 2405, Loss 0.3045560121536255\n",
      "[Training Epoch 2] Batch 2406, Loss 0.2874065935611725\n",
      "[Training Epoch 2] Batch 2407, Loss 0.2858610451221466\n",
      "[Training Epoch 2] Batch 2408, Loss 0.2876317501068115\n",
      "[Training Epoch 2] Batch 2409, Loss 0.2691148519515991\n",
      "[Training Epoch 2] Batch 2410, Loss 0.2864116430282593\n",
      "[Training Epoch 2] Batch 2411, Loss 0.2713841497898102\n",
      "[Training Epoch 2] Batch 2412, Loss 0.31613558530807495\n",
      "[Training Epoch 2] Batch 2413, Loss 0.28894954919815063\n",
      "[Training Epoch 2] Batch 2414, Loss 0.30480238795280457\n",
      "[Training Epoch 2] Batch 2415, Loss 0.25013381242752075\n",
      "[Training Epoch 2] Batch 2416, Loss 0.28148776292800903\n",
      "[Training Epoch 2] Batch 2417, Loss 0.2805255353450775\n",
      "[Training Epoch 2] Batch 2418, Loss 0.2846316993236542\n",
      "[Training Epoch 2] Batch 2419, Loss 0.30299150943756104\n",
      "[Training Epoch 2] Batch 2420, Loss 0.2710126042366028\n",
      "[Training Epoch 2] Batch 2421, Loss 0.2803730070590973\n",
      "[Training Epoch 2] Batch 2422, Loss 0.26503604650497437\n",
      "[Training Epoch 2] Batch 2423, Loss 0.2608136236667633\n",
      "[Training Epoch 2] Batch 2424, Loss 0.26784491539001465\n",
      "[Training Epoch 2] Batch 2425, Loss 0.25710543990135193\n",
      "[Training Epoch 2] Batch 2426, Loss 0.2885175049304962\n",
      "[Training Epoch 2] Batch 2427, Loss 0.29115772247314453\n",
      "[Training Epoch 2] Batch 2428, Loss 0.26837867498397827\n",
      "[Training Epoch 2] Batch 2429, Loss 0.28660401701927185\n",
      "[Training Epoch 2] Batch 2430, Loss 0.28879842162132263\n",
      "[Training Epoch 2] Batch 2431, Loss 0.29477939009666443\n",
      "[Training Epoch 2] Batch 2432, Loss 0.27035677433013916\n",
      "[Training Epoch 2] Batch 2433, Loss 0.299430787563324\n",
      "[Training Epoch 2] Batch 2434, Loss 0.28826001286506653\n",
      "[Training Epoch 2] Batch 2435, Loss 0.3022151589393616\n",
      "[Training Epoch 2] Batch 2436, Loss 0.27851754426956177\n",
      "[Training Epoch 2] Batch 2437, Loss 0.28440284729003906\n",
      "[Training Epoch 2] Batch 2438, Loss 0.30856773257255554\n",
      "[Training Epoch 2] Batch 2439, Loss 0.2517661452293396\n",
      "[Training Epoch 2] Batch 2440, Loss 0.2707797586917877\n",
      "[Training Epoch 2] Batch 2441, Loss 0.26802679896354675\n",
      "[Training Epoch 2] Batch 2442, Loss 0.2555108070373535\n",
      "[Training Epoch 2] Batch 2443, Loss 0.27302736043930054\n",
      "[Training Epoch 2] Batch 2444, Loss 0.2645930051803589\n",
      "[Training Epoch 2] Batch 2445, Loss 0.27647554874420166\n",
      "[Training Epoch 2] Batch 2446, Loss 0.28429150581359863\n",
      "[Training Epoch 2] Batch 2447, Loss 0.264657199382782\n",
      "[Training Epoch 2] Batch 2448, Loss 0.2667737901210785\n",
      "[Training Epoch 2] Batch 2449, Loss 0.29043275117874146\n",
      "[Training Epoch 2] Batch 2450, Loss 0.2993331551551819\n",
      "[Training Epoch 2] Batch 2451, Loss 0.2564181089401245\n",
      "[Training Epoch 2] Batch 2452, Loss 0.2601386308670044\n",
      "[Training Epoch 2] Batch 2453, Loss 0.2951432466506958\n",
      "[Training Epoch 2] Batch 2454, Loss 0.27252450585365295\n",
      "[Training Epoch 2] Batch 2455, Loss 0.27464964985847473\n",
      "[Training Epoch 2] Batch 2456, Loss 0.2992854416370392\n",
      "[Training Epoch 2] Batch 2457, Loss 0.2868614196777344\n",
      "[Training Epoch 2] Batch 2458, Loss 0.2886961102485657\n",
      "[Training Epoch 2] Batch 2459, Loss 0.27715563774108887\n",
      "[Training Epoch 2] Batch 2460, Loss 0.27879321575164795\n",
      "[Training Epoch 2] Batch 2461, Loss 0.27604204416275024\n",
      "[Training Epoch 2] Batch 2462, Loss 0.301301509141922\n",
      "[Training Epoch 2] Batch 2463, Loss 0.2608948349952698\n",
      "[Training Epoch 2] Batch 2464, Loss 0.2843775749206543\n",
      "[Training Epoch 2] Batch 2465, Loss 0.26424261927604675\n",
      "[Training Epoch 2] Batch 2466, Loss 0.2881576418876648\n",
      "[Training Epoch 2] Batch 2467, Loss 0.28291094303131104\n",
      "[Training Epoch 2] Batch 2468, Loss 0.2611590623855591\n",
      "[Training Epoch 2] Batch 2469, Loss 0.28209418058395386\n",
      "[Training Epoch 2] Batch 2470, Loss 0.29546862840652466\n",
      "[Training Epoch 2] Batch 2471, Loss 0.28087860345840454\n",
      "[Training Epoch 2] Batch 2472, Loss 0.24062132835388184\n",
      "[Training Epoch 2] Batch 2473, Loss 0.2989221513271332\n",
      "[Training Epoch 2] Batch 2474, Loss 0.3083626627922058\n",
      "[Training Epoch 2] Batch 2475, Loss 0.2627337574958801\n",
      "[Training Epoch 2] Batch 2476, Loss 0.2571389079093933\n",
      "[Training Epoch 2] Batch 2477, Loss 0.2626216411590576\n",
      "[Training Epoch 2] Batch 2478, Loss 0.2707465887069702\n",
      "[Training Epoch 2] Batch 2479, Loss 0.28519368171691895\n",
      "[Training Epoch 2] Batch 2480, Loss 0.26312777400016785\n",
      "[Training Epoch 2] Batch 2481, Loss 0.26080402731895447\n",
      "[Training Epoch 2] Batch 2482, Loss 0.28725144267082214\n",
      "[Training Epoch 2] Batch 2483, Loss 0.29595962166786194\n",
      "[Training Epoch 2] Batch 2484, Loss 0.2597138285636902\n",
      "[Training Epoch 2] Batch 2485, Loss 0.26433587074279785\n",
      "[Training Epoch 2] Batch 2486, Loss 0.27584126591682434\n",
      "[Training Epoch 2] Batch 2487, Loss 0.2797606289386749\n",
      "[Training Epoch 2] Batch 2488, Loss 0.2820813059806824\n",
      "[Training Epoch 2] Batch 2489, Loss 0.3136445879936218\n",
      "[Training Epoch 2] Batch 2490, Loss 0.27496498823165894\n",
      "[Training Epoch 2] Batch 2491, Loss 0.28409814834594727\n",
      "[Training Epoch 2] Batch 2492, Loss 0.2702440917491913\n",
      "[Training Epoch 2] Batch 2493, Loss 0.3019249439239502\n",
      "[Training Epoch 2] Batch 2494, Loss 0.28101247549057007\n",
      "[Training Epoch 2] Batch 2495, Loss 0.31158870458602905\n",
      "[Training Epoch 2] Batch 2496, Loss 0.29937055706977844\n",
      "[Training Epoch 2] Batch 2497, Loss 0.28079867362976074\n",
      "[Training Epoch 2] Batch 2498, Loss 0.2564646601676941\n",
      "[Training Epoch 2] Batch 2499, Loss 0.2832490801811218\n",
      "[Training Epoch 2] Batch 2500, Loss 0.26895397901535034\n",
      "[Training Epoch 2] Batch 2501, Loss 0.2989048361778259\n",
      "[Training Epoch 2] Batch 2502, Loss 0.27146846055984497\n",
      "[Training Epoch 2] Batch 2503, Loss 0.2865978479385376\n",
      "[Training Epoch 2] Batch 2504, Loss 0.2605675458908081\n",
      "[Training Epoch 2] Batch 2505, Loss 0.28966325521469116\n",
      "[Training Epoch 2] Batch 2506, Loss 0.29385289549827576\n",
      "[Training Epoch 2] Batch 2507, Loss 0.29068523645401\n",
      "[Training Epoch 2] Batch 2508, Loss 0.30094218254089355\n",
      "[Training Epoch 2] Batch 2509, Loss 0.2924540042877197\n",
      "[Training Epoch 2] Batch 2510, Loss 0.2658383548259735\n",
      "[Training Epoch 2] Batch 2511, Loss 0.24638573825359344\n",
      "[Training Epoch 2] Batch 2512, Loss 0.2676604390144348\n",
      "[Training Epoch 2] Batch 2513, Loss 0.2939131259918213\n",
      "[Training Epoch 2] Batch 2514, Loss 0.2887212038040161\n",
      "[Training Epoch 2] Batch 2515, Loss 0.2623295187950134\n",
      "[Training Epoch 2] Batch 2516, Loss 0.3003586530685425\n",
      "[Training Epoch 2] Batch 2517, Loss 0.27536046504974365\n",
      "[Training Epoch 2] Batch 2518, Loss 0.2698036730289459\n",
      "[Training Epoch 2] Batch 2519, Loss 0.2559761106967926\n",
      "[Training Epoch 2] Batch 2520, Loss 0.2993716597557068\n",
      "[Training Epoch 2] Batch 2521, Loss 0.30777889490127563\n",
      "[Training Epoch 2] Batch 2522, Loss 0.27848392724990845\n",
      "[Training Epoch 2] Batch 2523, Loss 0.27899497747421265\n",
      "[Training Epoch 2] Batch 2524, Loss 0.2843092679977417\n",
      "[Training Epoch 2] Batch 2525, Loss 0.2597017288208008\n",
      "[Training Epoch 2] Batch 2526, Loss 0.29779309034347534\n",
      "[Training Epoch 2] Batch 2527, Loss 0.29156795144081116\n",
      "[Training Epoch 2] Batch 2528, Loss 0.3014259338378906\n",
      "[Training Epoch 2] Batch 2529, Loss 0.28794604539871216\n",
      "[Training Epoch 2] Batch 2530, Loss 0.2526240646839142\n",
      "[Training Epoch 2] Batch 2531, Loss 0.2826560139656067\n",
      "[Training Epoch 2] Batch 2532, Loss 0.3284461796283722\n",
      "[Training Epoch 2] Batch 2533, Loss 0.28690236806869507\n",
      "[Training Epoch 2] Batch 2534, Loss 0.2853969633579254\n",
      "[Training Epoch 2] Batch 2535, Loss 0.3004063665866852\n",
      "[Training Epoch 2] Batch 2536, Loss 0.2954930067062378\n",
      "[Training Epoch 2] Batch 2537, Loss 0.3038599491119385\n",
      "[Training Epoch 2] Batch 2538, Loss 0.2943277359008789\n",
      "[Training Epoch 2] Batch 2539, Loss 0.27033770084381104\n",
      "[Training Epoch 2] Batch 2540, Loss 0.2828354835510254\n",
      "[Training Epoch 2] Batch 2541, Loss 0.2717147767543793\n",
      "[Training Epoch 2] Batch 2542, Loss 0.26436829566955566\n",
      "[Training Epoch 2] Batch 2543, Loss 0.27140843868255615\n",
      "[Training Epoch 2] Batch 2544, Loss 0.273782342672348\n",
      "[Training Epoch 2] Batch 2545, Loss 0.3077489733695984\n",
      "[Training Epoch 2] Batch 2546, Loss 0.2528839707374573\n",
      "[Training Epoch 2] Batch 2547, Loss 0.2913578152656555\n",
      "[Training Epoch 2] Batch 2548, Loss 0.28623273968696594\n",
      "[Training Epoch 2] Batch 2549, Loss 0.27256500720977783\n",
      "[Training Epoch 2] Batch 2550, Loss 0.25148963928222656\n",
      "[Training Epoch 2] Batch 2551, Loss 0.27137136459350586\n",
      "[Training Epoch 2] Batch 2552, Loss 0.29799479246139526\n",
      "[Training Epoch 2] Batch 2553, Loss 0.2775753140449524\n",
      "[Training Epoch 2] Batch 2554, Loss 0.28508633375167847\n",
      "[Training Epoch 2] Batch 2555, Loss 0.30316460132598877\n",
      "[Training Epoch 2] Batch 2556, Loss 0.2854458689689636\n",
      "[Training Epoch 2] Batch 2557, Loss 0.28868335485458374\n",
      "[Training Epoch 2] Batch 2558, Loss 0.2901651859283447\n",
      "[Training Epoch 2] Batch 2559, Loss 0.27564263343811035\n",
      "[Training Epoch 2] Batch 2560, Loss 0.2908247113227844\n",
      "[Training Epoch 2] Batch 2561, Loss 0.2859894037246704\n",
      "[Training Epoch 2] Batch 2562, Loss 0.2841462790966034\n",
      "[Training Epoch 2] Batch 2563, Loss 0.27144181728363037\n",
      "[Training Epoch 2] Batch 2564, Loss 0.2886987328529358\n",
      "[Training Epoch 2] Batch 2565, Loss 0.29429274797439575\n",
      "[Training Epoch 2] Batch 2566, Loss 0.30012357234954834\n",
      "[Training Epoch 2] Batch 2567, Loss 0.29808759689331055\n",
      "[Training Epoch 2] Batch 2568, Loss 0.2886446714401245\n",
      "[Training Epoch 2] Batch 2569, Loss 0.2758445739746094\n",
      "[Training Epoch 2] Batch 2570, Loss 0.3054050803184509\n",
      "[Training Epoch 2] Batch 2571, Loss 0.25547707080841064\n",
      "[Training Epoch 2] Batch 2572, Loss 0.25982022285461426\n",
      "[Training Epoch 2] Batch 2573, Loss 0.2920309007167816\n",
      "[Training Epoch 2] Batch 2574, Loss 0.27579811215400696\n",
      "[Training Epoch 2] Batch 2575, Loss 0.2636180520057678\n",
      "[Training Epoch 2] Batch 2576, Loss 0.2850780785083771\n",
      "[Training Epoch 2] Batch 2577, Loss 0.2427535355091095\n",
      "[Training Epoch 2] Batch 2578, Loss 0.29783347249031067\n",
      "[Training Epoch 2] Batch 2579, Loss 0.2709301710128784\n",
      "[Training Epoch 2] Batch 2580, Loss 0.2796103358268738\n",
      "[Training Epoch 2] Batch 2581, Loss 0.264432430267334\n",
      "[Training Epoch 2] Batch 2582, Loss 0.299977570772171\n",
      "[Training Epoch 2] Batch 2583, Loss 0.2872716784477234\n",
      "[Training Epoch 2] Batch 2584, Loss 0.24556821584701538\n",
      "[Training Epoch 2] Batch 2585, Loss 0.283051073551178\n",
      "[Training Epoch 2] Batch 2586, Loss 0.2585625648498535\n",
      "[Training Epoch 2] Batch 2587, Loss 0.2701680064201355\n",
      "[Training Epoch 2] Batch 2588, Loss 0.24926698207855225\n",
      "[Training Epoch 2] Batch 2589, Loss 0.2876288592815399\n",
      "[Training Epoch 2] Batch 2590, Loss 0.2844706177711487\n",
      "[Training Epoch 2] Batch 2591, Loss 0.3052033483982086\n",
      "[Training Epoch 2] Batch 2592, Loss 0.29568880796432495\n",
      "[Training Epoch 2] Batch 2593, Loss 0.275265634059906\n",
      "[Training Epoch 2] Batch 2594, Loss 0.26406872272491455\n",
      "[Training Epoch 2] Batch 2595, Loss 0.27965760231018066\n",
      "[Training Epoch 2] Batch 2596, Loss 0.27322661876678467\n",
      "[Training Epoch 2] Batch 2597, Loss 0.27700144052505493\n",
      "[Training Epoch 2] Batch 2598, Loss 0.2972734272480011\n",
      "[Training Epoch 2] Batch 2599, Loss 0.26300692558288574\n",
      "[Training Epoch 2] Batch 2600, Loss 0.28328946232795715\n",
      "[Training Epoch 2] Batch 2601, Loss 0.315329372882843\n",
      "[Training Epoch 2] Batch 2602, Loss 0.29247382283210754\n",
      "[Training Epoch 2] Batch 2603, Loss 0.2783054709434509\n",
      "[Training Epoch 2] Batch 2604, Loss 0.3050451874732971\n",
      "[Training Epoch 2] Batch 2605, Loss 0.29064619541168213\n",
      "[Training Epoch 2] Batch 2606, Loss 0.30328303575515747\n",
      "[Training Epoch 2] Batch 2607, Loss 0.31191208958625793\n",
      "[Training Epoch 2] Batch 2608, Loss 0.28488674759864807\n",
      "[Training Epoch 2] Batch 2609, Loss 0.2983335256576538\n",
      "[Training Epoch 2] Batch 2610, Loss 0.2698306441307068\n",
      "[Training Epoch 2] Batch 2611, Loss 0.29059481620788574\n",
      "[Training Epoch 2] Batch 2612, Loss 0.2534105181694031\n",
      "[Training Epoch 2] Batch 2613, Loss 0.2802119255065918\n",
      "[Training Epoch 2] Batch 2614, Loss 0.2726937234401703\n",
      "[Training Epoch 2] Batch 2615, Loss 0.2697983384132385\n",
      "[Training Epoch 2] Batch 2616, Loss 0.2730002999305725\n",
      "[Training Epoch 2] Batch 2617, Loss 0.30836743116378784\n",
      "[Training Epoch 2] Batch 2618, Loss 0.27750498056411743\n",
      "[Training Epoch 2] Batch 2619, Loss 0.25735998153686523\n",
      "[Training Epoch 2] Batch 2620, Loss 0.24529540538787842\n",
      "[Training Epoch 2] Batch 2621, Loss 0.301984041929245\n",
      "[Training Epoch 2] Batch 2622, Loss 0.2846662402153015\n",
      "[Training Epoch 2] Batch 2623, Loss 0.28772401809692383\n",
      "[Training Epoch 2] Batch 2624, Loss 0.31249111890792847\n",
      "[Training Epoch 2] Batch 2625, Loss 0.3023267984390259\n",
      "[Training Epoch 2] Batch 2626, Loss 0.2870783507823944\n",
      "[Training Epoch 2] Batch 2627, Loss 0.27807947993278503\n",
      "[Training Epoch 2] Batch 2628, Loss 0.3076096773147583\n",
      "[Training Epoch 2] Batch 2629, Loss 0.29822567105293274\n",
      "[Training Epoch 2] Batch 2630, Loss 0.28628215193748474\n",
      "[Training Epoch 2] Batch 2631, Loss 0.2595144212245941\n",
      "[Training Epoch 2] Batch 2632, Loss 0.2850439250469208\n",
      "[Training Epoch 2] Batch 2633, Loss 0.2763403356075287\n",
      "[Training Epoch 2] Batch 2634, Loss 0.29616230726242065\n",
      "[Training Epoch 2] Batch 2635, Loss 0.2698498070240021\n",
      "[Training Epoch 2] Batch 2636, Loss 0.2718958556652069\n",
      "[Training Epoch 2] Batch 2637, Loss 0.28237083554267883\n",
      "[Training Epoch 2] Batch 2638, Loss 0.27528703212738037\n",
      "[Training Epoch 2] Batch 2639, Loss 0.24433845281600952\n",
      "[Training Epoch 2] Batch 2640, Loss 0.24604354798793793\n",
      "[Training Epoch 2] Batch 2641, Loss 0.2564622759819031\n",
      "[Training Epoch 2] Batch 2642, Loss 0.2793552279472351\n",
      "[Training Epoch 2] Batch 2643, Loss 0.278759241104126\n",
      "[Training Epoch 2] Batch 2644, Loss 0.2925258278846741\n",
      "[Training Epoch 2] Batch 2645, Loss 0.28865307569503784\n",
      "[Training Epoch 2] Batch 2646, Loss 0.2823830544948578\n",
      "[Training Epoch 2] Batch 2647, Loss 0.2789161503314972\n",
      "[Training Epoch 2] Batch 2648, Loss 0.2950058877468109\n",
      "[Training Epoch 2] Batch 2649, Loss 0.24275052547454834\n",
      "[Training Epoch 2] Batch 2650, Loss 0.2750770151615143\n",
      "[Training Epoch 2] Batch 2651, Loss 0.2472161054611206\n",
      "[Training Epoch 2] Batch 2652, Loss 0.25651443004608154\n",
      "[Training Epoch 2] Batch 2653, Loss 0.2909926772117615\n",
      "[Training Epoch 2] Batch 2654, Loss 0.2989034354686737\n",
      "[Training Epoch 2] Batch 2655, Loss 0.29173868894577026\n",
      "[Training Epoch 2] Batch 2656, Loss 0.25255680084228516\n",
      "[Training Epoch 2] Batch 2657, Loss 0.27902549505233765\n",
      "[Training Epoch 2] Batch 2658, Loss 0.27451080083847046\n",
      "[Training Epoch 2] Batch 2659, Loss 0.2778857946395874\n",
      "[Training Epoch 2] Batch 2660, Loss 0.28454288840293884\n",
      "[Training Epoch 2] Batch 2661, Loss 0.2897605001926422\n",
      "[Training Epoch 2] Batch 2662, Loss 0.28727805614471436\n",
      "[Training Epoch 2] Batch 2663, Loss 0.27866095304489136\n",
      "[Training Epoch 2] Batch 2664, Loss 0.26875796914100647\n",
      "[Training Epoch 2] Batch 2665, Loss 0.23582538962364197\n",
      "[Training Epoch 2] Batch 2666, Loss 0.26834553480148315\n",
      "[Training Epoch 2] Batch 2667, Loss 0.2833852469921112\n",
      "[Training Epoch 2] Batch 2668, Loss 0.2849186658859253\n",
      "[Training Epoch 2] Batch 2669, Loss 0.2783616781234741\n",
      "[Training Epoch 2] Batch 2670, Loss 0.2676485776901245\n",
      "[Training Epoch 2] Batch 2671, Loss 0.2792099118232727\n",
      "[Training Epoch 2] Batch 2672, Loss 0.29379981756210327\n",
      "[Training Epoch 2] Batch 2673, Loss 0.26318204402923584\n",
      "[Training Epoch 2] Batch 2674, Loss 0.3104446232318878\n",
      "[Training Epoch 2] Batch 2675, Loss 0.2630924582481384\n",
      "[Training Epoch 2] Batch 2676, Loss 0.2792201042175293\n",
      "[Training Epoch 2] Batch 2677, Loss 0.2856915593147278\n",
      "[Training Epoch 2] Batch 2678, Loss 0.26550501585006714\n",
      "[Training Epoch 2] Batch 2679, Loss 0.2854544520378113\n",
      "[Training Epoch 2] Batch 2680, Loss 0.25684621930122375\n",
      "[Training Epoch 2] Batch 2681, Loss 0.2581605911254883\n",
      "[Training Epoch 2] Batch 2682, Loss 0.251630961894989\n",
      "[Training Epoch 2] Batch 2683, Loss 0.28246650099754333\n",
      "[Training Epoch 2] Batch 2684, Loss 0.28035393357276917\n",
      "[Training Epoch 2] Batch 2685, Loss 0.2563372552394867\n",
      "[Training Epoch 2] Batch 2686, Loss 0.26619333028793335\n",
      "[Training Epoch 2] Batch 2687, Loss 0.26114410161972046\n",
      "[Training Epoch 2] Batch 2688, Loss 0.28367459774017334\n",
      "[Training Epoch 2] Batch 2689, Loss 0.27890998125076294\n",
      "[Training Epoch 2] Batch 2690, Loss 0.260378897190094\n",
      "[Training Epoch 2] Batch 2691, Loss 0.2965419888496399\n",
      "[Training Epoch 2] Batch 2692, Loss 0.28578728437423706\n",
      "[Training Epoch 2] Batch 2693, Loss 0.26588648557662964\n",
      "[Training Epoch 2] Batch 2694, Loss 0.27040132880210876\n",
      "[Training Epoch 2] Batch 2695, Loss 0.26915907859802246\n",
      "[Training Epoch 2] Batch 2696, Loss 0.2850743532180786\n",
      "[Training Epoch 2] Batch 2697, Loss 0.2674131989479065\n",
      "[Training Epoch 2] Batch 2698, Loss 0.28166335821151733\n",
      "[Training Epoch 2] Batch 2699, Loss 0.26613837480545044\n",
      "[Training Epoch 2] Batch 2700, Loss 0.29398268461227417\n",
      "[Training Epoch 2] Batch 2701, Loss 0.29488569498062134\n",
      "[Training Epoch 2] Batch 2702, Loss 0.3046073019504547\n",
      "[Training Epoch 2] Batch 2703, Loss 0.30001434683799744\n",
      "[Training Epoch 2] Batch 2704, Loss 0.24969202280044556\n",
      "[Training Epoch 2] Batch 2705, Loss 0.2693808972835541\n",
      "[Training Epoch 2] Batch 2706, Loss 0.2751849293708801\n",
      "[Training Epoch 2] Batch 2707, Loss 0.2958318591117859\n",
      "[Training Epoch 2] Batch 2708, Loss 0.3124728798866272\n",
      "[Training Epoch 2] Batch 2709, Loss 0.27835679054260254\n",
      "[Training Epoch 2] Batch 2710, Loss 0.2852737605571747\n",
      "[Training Epoch 2] Batch 2711, Loss 0.28876233100891113\n",
      "[Training Epoch 2] Batch 2712, Loss 0.2750914692878723\n",
      "[Training Epoch 2] Batch 2713, Loss 0.3114815950393677\n",
      "[Training Epoch 2] Batch 2714, Loss 0.32893282175064087\n",
      "[Training Epoch 2] Batch 2715, Loss 0.2589632272720337\n",
      "[Training Epoch 2] Batch 2716, Loss 0.2734012007713318\n",
      "[Training Epoch 2] Batch 2717, Loss 0.24086716771125793\n",
      "[Training Epoch 2] Batch 2718, Loss 0.27405160665512085\n",
      "[Training Epoch 2] Batch 2719, Loss 0.28069037199020386\n",
      "[Training Epoch 2] Batch 2720, Loss 0.29116731882095337\n",
      "[Training Epoch 2] Batch 2721, Loss 0.26448798179626465\n",
      "[Training Epoch 2] Batch 2722, Loss 0.29227694869041443\n",
      "[Training Epoch 2] Batch 2723, Loss 0.2842777669429779\n",
      "[Training Epoch 2] Batch 2724, Loss 0.26578089594841003\n",
      "[Training Epoch 2] Batch 2725, Loss 0.2879604697227478\n",
      "[Training Epoch 2] Batch 2726, Loss 0.2895127534866333\n",
      "[Training Epoch 2] Batch 2727, Loss 0.2770957052707672\n",
      "[Training Epoch 2] Batch 2728, Loss 0.2865128517150879\n",
      "[Training Epoch 2] Batch 2729, Loss 0.26726144552230835\n",
      "[Training Epoch 2] Batch 2730, Loss 0.29099637269973755\n",
      "[Training Epoch 2] Batch 2731, Loss 0.26177841424942017\n",
      "[Training Epoch 2] Batch 2732, Loss 0.30534473061561584\n",
      "[Training Epoch 2] Batch 2733, Loss 0.31143754720687866\n",
      "[Training Epoch 2] Batch 2734, Loss 0.26278403401374817\n",
      "[Training Epoch 2] Batch 2735, Loss 0.2759559154510498\n",
      "[Training Epoch 2] Batch 2736, Loss 0.2662898898124695\n",
      "[Training Epoch 2] Batch 2737, Loss 0.29201334714889526\n",
      "[Training Epoch 2] Batch 2738, Loss 0.27677246928215027\n",
      "[Training Epoch 2] Batch 2739, Loss 0.30879712104797363\n",
      "[Training Epoch 2] Batch 2740, Loss 0.25850847363471985\n",
      "[Training Epoch 2] Batch 2741, Loss 0.27811723947525024\n",
      "[Training Epoch 2] Batch 2742, Loss 0.2682337760925293\n",
      "[Training Epoch 2] Batch 2743, Loss 0.26961666345596313\n",
      "[Training Epoch 2] Batch 2744, Loss 0.3238178789615631\n",
      "[Training Epoch 2] Batch 2745, Loss 0.3041989207267761\n",
      "[Training Epoch 2] Batch 2746, Loss 0.2840190529823303\n",
      "[Training Epoch 2] Batch 2747, Loss 0.24915805459022522\n",
      "[Training Epoch 2] Batch 2748, Loss 0.27925580739974976\n",
      "[Training Epoch 2] Batch 2749, Loss 0.30368560552597046\n",
      "[Training Epoch 2] Batch 2750, Loss 0.29362788796424866\n",
      "[Training Epoch 2] Batch 2751, Loss 0.27116286754608154\n",
      "[Training Epoch 2] Batch 2752, Loss 0.2752987742424011\n",
      "[Training Epoch 2] Batch 2753, Loss 0.2531152367591858\n",
      "[Training Epoch 2] Batch 2754, Loss 0.2568471431732178\n",
      "[Training Epoch 2] Batch 2755, Loss 0.2557378113269806\n",
      "[Training Epoch 2] Batch 2756, Loss 0.29982367157936096\n",
      "[Training Epoch 2] Batch 2757, Loss 0.2980943024158478\n",
      "[Training Epoch 2] Batch 2758, Loss 0.293987512588501\n",
      "[Training Epoch 2] Batch 2759, Loss 0.28932422399520874\n",
      "[Training Epoch 2] Batch 2760, Loss 0.2969229817390442\n",
      "[Training Epoch 2] Batch 2761, Loss 0.28074806928634644\n",
      "[Training Epoch 2] Batch 2762, Loss 0.25840121507644653\n",
      "[Training Epoch 2] Batch 2763, Loss 0.2595866322517395\n",
      "[Training Epoch 2] Batch 2764, Loss 0.27040910720825195\n",
      "[Training Epoch 2] Batch 2765, Loss 0.28830331563949585\n",
      "[Training Epoch 2] Batch 2766, Loss 0.28428447246551514\n",
      "[Training Epoch 2] Batch 2767, Loss 0.2615797519683838\n",
      "[Training Epoch 2] Batch 2768, Loss 0.27358120679855347\n",
      "[Training Epoch 2] Batch 2769, Loss 0.2626316547393799\n",
      "[Training Epoch 2] Batch 2770, Loss 0.2847254276275635\n",
      "[Training Epoch 2] Batch 2771, Loss 0.2938942015171051\n",
      "[Training Epoch 2] Batch 2772, Loss 0.28242576122283936\n",
      "[Training Epoch 2] Batch 2773, Loss 0.29002612829208374\n",
      "[Training Epoch 2] Batch 2774, Loss 0.2725878357887268\n",
      "[Training Epoch 2] Batch 2775, Loss 0.3182791471481323\n",
      "[Training Epoch 2] Batch 2776, Loss 0.28166434168815613\n",
      "[Training Epoch 2] Batch 2777, Loss 0.30587106943130493\n",
      "[Training Epoch 2] Batch 2778, Loss 0.25128716230392456\n",
      "[Training Epoch 2] Batch 2779, Loss 0.3132275342941284\n",
      "[Training Epoch 2] Batch 2780, Loss 0.28871607780456543\n",
      "[Training Epoch 2] Batch 2781, Loss 0.28450024127960205\n",
      "[Training Epoch 2] Batch 2782, Loss 0.2898726761341095\n",
      "[Training Epoch 2] Batch 2783, Loss 0.26808521151542664\n",
      "[Training Epoch 2] Batch 2784, Loss 0.29280394315719604\n",
      "[Training Epoch 2] Batch 2785, Loss 0.2716716527938843\n",
      "[Training Epoch 2] Batch 2786, Loss 0.28092458844184875\n",
      "[Training Epoch 2] Batch 2787, Loss 0.2948322594165802\n",
      "[Training Epoch 2] Batch 2788, Loss 0.2975740432739258\n",
      "[Training Epoch 2] Batch 2789, Loss 0.2950412631034851\n",
      "[Training Epoch 2] Batch 2790, Loss 0.283138245344162\n",
      "[Training Epoch 2] Batch 2791, Loss 0.31236961483955383\n",
      "[Training Epoch 2] Batch 2792, Loss 0.2862182557582855\n",
      "[Training Epoch 2] Batch 2793, Loss 0.28152406215667725\n",
      "[Training Epoch 2] Batch 2794, Loss 0.30027586221694946\n",
      "[Training Epoch 2] Batch 2795, Loss 0.24576285481452942\n",
      "[Training Epoch 2] Batch 2796, Loss 0.2646861672401428\n",
      "[Training Epoch 2] Batch 2797, Loss 0.27600061893463135\n",
      "[Training Epoch 2] Batch 2798, Loss 0.3064980208873749\n",
      "[Training Epoch 2] Batch 2799, Loss 0.27843162417411804\n",
      "[Training Epoch 2] Batch 2800, Loss 0.2816372215747833\n",
      "[Training Epoch 2] Batch 2801, Loss 0.27217531204223633\n",
      "[Training Epoch 2] Batch 2802, Loss 0.29637837409973145\n",
      "[Training Epoch 2] Batch 2803, Loss 0.2708699703216553\n",
      "[Training Epoch 2] Batch 2804, Loss 0.2610923647880554\n",
      "[Training Epoch 2] Batch 2805, Loss 0.26980334520339966\n",
      "[Training Epoch 2] Batch 2806, Loss 0.2766290605068207\n",
      "[Training Epoch 2] Batch 2807, Loss 0.2356950044631958\n",
      "[Training Epoch 2] Batch 2808, Loss 0.2528875768184662\n",
      "[Training Epoch 2] Batch 2809, Loss 0.30696403980255127\n",
      "[Training Epoch 2] Batch 2810, Loss 0.25959694385528564\n",
      "[Training Epoch 2] Batch 2811, Loss 0.2816791236400604\n",
      "[Training Epoch 2] Batch 2812, Loss 0.28089582920074463\n",
      "[Training Epoch 2] Batch 2813, Loss 0.2489001452922821\n",
      "[Training Epoch 2] Batch 2814, Loss 0.30296456813812256\n",
      "[Training Epoch 2] Batch 2815, Loss 0.27284008264541626\n",
      "[Training Epoch 2] Batch 2816, Loss 0.28642088174819946\n",
      "[Training Epoch 2] Batch 2817, Loss 0.3034712076187134\n",
      "[Training Epoch 2] Batch 2818, Loss 0.29848796129226685\n",
      "[Training Epoch 2] Batch 2819, Loss 0.2575676441192627\n",
      "[Training Epoch 2] Batch 2820, Loss 0.26834383606910706\n",
      "[Training Epoch 2] Batch 2821, Loss 0.2529911398887634\n",
      "[Training Epoch 2] Batch 2822, Loss 0.28413039445877075\n",
      "[Training Epoch 2] Batch 2823, Loss 0.2559533715248108\n",
      "[Training Epoch 2] Batch 2824, Loss 0.28359442949295044\n",
      "[Training Epoch 2] Batch 2825, Loss 0.2855570614337921\n",
      "[Training Epoch 2] Batch 2826, Loss 0.2940247058868408\n",
      "[Training Epoch 2] Batch 2827, Loss 0.28519779443740845\n",
      "[Training Epoch 2] Batch 2828, Loss 0.2615061402320862\n",
      "[Training Epoch 2] Batch 2829, Loss 0.2753717303276062\n",
      "[Training Epoch 2] Batch 2830, Loss 0.3034825921058655\n",
      "[Training Epoch 2] Batch 2831, Loss 0.28364717960357666\n",
      "[Training Epoch 2] Batch 2832, Loss 0.2685483396053314\n",
      "[Training Epoch 2] Batch 2833, Loss 0.2717764377593994\n",
      "[Training Epoch 2] Batch 2834, Loss 0.2630125880241394\n",
      "[Training Epoch 2] Batch 2835, Loss 0.30395588278770447\n",
      "[Training Epoch 2] Batch 2836, Loss 0.2780357897281647\n",
      "[Training Epoch 2] Batch 2837, Loss 0.3179652690887451\n",
      "[Training Epoch 2] Batch 2838, Loss 0.30795156955718994\n",
      "[Training Epoch 2] Batch 2839, Loss 0.25592100620269775\n",
      "[Training Epoch 2] Batch 2840, Loss 0.2808922529220581\n",
      "[Training Epoch 2] Batch 2841, Loss 0.3153115510940552\n",
      "[Training Epoch 2] Batch 2842, Loss 0.2596103549003601\n",
      "[Training Epoch 2] Batch 2843, Loss 0.2798399329185486\n",
      "[Training Epoch 2] Batch 2844, Loss 0.27384868264198303\n",
      "[Training Epoch 2] Batch 2845, Loss 0.2901998460292816\n",
      "[Training Epoch 2] Batch 2846, Loss 0.2681728005409241\n",
      "[Training Epoch 2] Batch 2847, Loss 0.2500792145729065\n",
      "[Training Epoch 2] Batch 2848, Loss 0.28944432735443115\n",
      "[Training Epoch 2] Batch 2849, Loss 0.26797187328338623\n",
      "[Training Epoch 2] Batch 2850, Loss 0.27786147594451904\n",
      "[Training Epoch 2] Batch 2851, Loss 0.2787403464317322\n",
      "[Training Epoch 2] Batch 2852, Loss 0.2522483468055725\n",
      "[Training Epoch 2] Batch 2853, Loss 0.2904049754142761\n",
      "[Training Epoch 2] Batch 2854, Loss 0.2814311385154724\n",
      "[Training Epoch 2] Batch 2855, Loss 0.28227555751800537\n",
      "[Training Epoch 2] Batch 2856, Loss 0.30038756132125854\n",
      "[Training Epoch 2] Batch 2857, Loss 0.2477828413248062\n",
      "[Training Epoch 2] Batch 2858, Loss 0.30093640089035034\n",
      "[Training Epoch 2] Batch 2859, Loss 0.2703450322151184\n",
      "[Training Epoch 2] Batch 2860, Loss 0.2788565158843994\n",
      "[Training Epoch 2] Batch 2861, Loss 0.2938576638698578\n",
      "[Training Epoch 2] Batch 2862, Loss 0.28595975041389465\n",
      "[Training Epoch 2] Batch 2863, Loss 0.2810809016227722\n",
      "[Training Epoch 2] Batch 2864, Loss 0.27263426780700684\n",
      "[Training Epoch 2] Batch 2865, Loss 0.2892456352710724\n",
      "[Training Epoch 2] Batch 2866, Loss 0.2566792964935303\n",
      "[Training Epoch 2] Batch 2867, Loss 0.286599338054657\n",
      "[Training Epoch 2] Batch 2868, Loss 0.2545185089111328\n",
      "[Training Epoch 2] Batch 2869, Loss 0.2936347723007202\n",
      "[Training Epoch 2] Batch 2870, Loss 0.28344404697418213\n",
      "[Training Epoch 2] Batch 2871, Loss 0.2878941297531128\n",
      "[Training Epoch 2] Batch 2872, Loss 0.2791558504104614\n",
      "[Training Epoch 2] Batch 2873, Loss 0.2752688527107239\n",
      "[Training Epoch 2] Batch 2874, Loss 0.3150023818016052\n",
      "[Training Epoch 2] Batch 2875, Loss 0.273696631193161\n",
      "[Training Epoch 2] Batch 2876, Loss 0.2866661548614502\n",
      "[Training Epoch 2] Batch 2877, Loss 0.2921925187110901\n",
      "[Training Epoch 2] Batch 2878, Loss 0.2684342861175537\n",
      "[Training Epoch 2] Batch 2879, Loss 0.30817297101020813\n",
      "[Training Epoch 2] Batch 2880, Loss 0.25411808490753174\n",
      "[Training Epoch 2] Batch 2881, Loss 0.26700103282928467\n",
      "[Training Epoch 2] Batch 2882, Loss 0.28409984707832336\n",
      "[Training Epoch 2] Batch 2883, Loss 0.29514795541763306\n",
      "[Training Epoch 2] Batch 2884, Loss 0.2740197777748108\n",
      "[Training Epoch 2] Batch 2885, Loss 0.2763732671737671\n",
      "[Training Epoch 2] Batch 2886, Loss 0.27862846851348877\n",
      "[Training Epoch 2] Batch 2887, Loss 0.27207279205322266\n",
      "[Training Epoch 2] Batch 2888, Loss 0.27975183725357056\n",
      "[Training Epoch 2] Batch 2889, Loss 0.3091292083263397\n",
      "[Training Epoch 2] Batch 2890, Loss 0.28824275732040405\n",
      "[Training Epoch 2] Batch 2891, Loss 0.2883753478527069\n",
      "[Training Epoch 2] Batch 2892, Loss 0.2599308490753174\n",
      "[Training Epoch 2] Batch 2893, Loss 0.2512650787830353\n",
      "[Training Epoch 2] Batch 2894, Loss 0.2799966633319855\n",
      "[Training Epoch 2] Batch 2895, Loss 0.2714424133300781\n",
      "[Training Epoch 2] Batch 2896, Loss 0.2897670269012451\n",
      "[Training Epoch 2] Batch 2897, Loss 0.2922012209892273\n",
      "[Training Epoch 2] Batch 2898, Loss 0.28318971395492554\n",
      "[Training Epoch 2] Batch 2899, Loss 0.2841361165046692\n",
      "[Training Epoch 2] Batch 2900, Loss 0.27999889850616455\n",
      "[Training Epoch 2] Batch 2901, Loss 0.26511716842651367\n",
      "[Training Epoch 2] Batch 2902, Loss 0.28551405668258667\n",
      "[Training Epoch 2] Batch 2903, Loss 0.28725460171699524\n",
      "[Training Epoch 2] Batch 2904, Loss 0.26767677068710327\n",
      "[Training Epoch 2] Batch 2905, Loss 0.2772142291069031\n",
      "[Training Epoch 2] Batch 2906, Loss 0.25592148303985596\n",
      "[Training Epoch 2] Batch 2907, Loss 0.25795596837997437\n",
      "[Training Epoch 2] Batch 2908, Loss 0.273335337638855\n",
      "[Training Epoch 2] Batch 2909, Loss 0.27457141876220703\n",
      "[Training Epoch 2] Batch 2910, Loss 0.2428933084011078\n",
      "[Training Epoch 2] Batch 2911, Loss 0.2797681391239166\n",
      "[Training Epoch 2] Batch 2912, Loss 0.30093422532081604\n",
      "[Training Epoch 2] Batch 2913, Loss 0.2792680263519287\n",
      "[Training Epoch 2] Batch 2914, Loss 0.2750858664512634\n",
      "[Training Epoch 2] Batch 2915, Loss 0.3092840313911438\n",
      "[Training Epoch 2] Batch 2916, Loss 0.2934720516204834\n",
      "[Training Epoch 2] Batch 2917, Loss 0.2722305655479431\n",
      "[Training Epoch 2] Batch 2918, Loss 0.28656262159347534\n",
      "[Training Epoch 2] Batch 2919, Loss 0.2922367751598358\n",
      "[Training Epoch 2] Batch 2920, Loss 0.3037283420562744\n",
      "[Training Epoch 2] Batch 2921, Loss 0.2812221646308899\n",
      "[Training Epoch 2] Batch 2922, Loss 0.29593467712402344\n",
      "[Training Epoch 2] Batch 2923, Loss 0.2543473541736603\n",
      "[Training Epoch 2] Batch 2924, Loss 0.2892860174179077\n",
      "[Training Epoch 2] Batch 2925, Loss 0.24306222796440125\n",
      "[Training Epoch 2] Batch 2926, Loss 0.26543712615966797\n",
      "[Training Epoch 2] Batch 2927, Loss 0.269095242023468\n",
      "[Training Epoch 2] Batch 2928, Loss 0.2924354672431946\n",
      "[Training Epoch 2] Batch 2929, Loss 0.27250659465789795\n",
      "[Training Epoch 2] Batch 2930, Loss 0.2981254458427429\n",
      "[Training Epoch 2] Batch 2931, Loss 0.2611581087112427\n",
      "[Training Epoch 2] Batch 2932, Loss 0.27400094270706177\n",
      "[Training Epoch 2] Batch 2933, Loss 0.2929431200027466\n",
      "[Training Epoch 2] Batch 2934, Loss 0.27119970321655273\n",
      "[Training Epoch 2] Batch 2935, Loss 0.28344589471817017\n",
      "[Training Epoch 2] Batch 2936, Loss 0.2711387276649475\n",
      "[Training Epoch 2] Batch 2937, Loss 0.28710368275642395\n",
      "[Training Epoch 2] Batch 2938, Loss 0.2743302881717682\n",
      "[Training Epoch 2] Batch 2939, Loss 0.3005690276622772\n",
      "[Training Epoch 2] Batch 2940, Loss 0.2927529811859131\n",
      "[Training Epoch 2] Batch 2941, Loss 0.26000887155532837\n",
      "[Training Epoch 2] Batch 2942, Loss 0.26930758357048035\n",
      "[Training Epoch 2] Batch 2943, Loss 0.27417999505996704\n",
      "[Training Epoch 2] Batch 2944, Loss 0.2904318571090698\n",
      "[Training Epoch 2] Batch 2945, Loss 0.282162070274353\n",
      "[Training Epoch 2] Batch 2946, Loss 0.2743045389652252\n",
      "[Training Epoch 2] Batch 2947, Loss 0.27554851770401\n",
      "[Training Epoch 2] Batch 2948, Loss 0.30197352170944214\n",
      "[Training Epoch 2] Batch 2949, Loss 0.28402018547058105\n",
      "[Training Epoch 2] Batch 2950, Loss 0.2655949890613556\n",
      "[Training Epoch 2] Batch 2951, Loss 0.26555314660072327\n",
      "[Training Epoch 2] Batch 2952, Loss 0.2892500162124634\n",
      "[Training Epoch 2] Batch 2953, Loss 0.3047568202018738\n",
      "[Training Epoch 2] Batch 2954, Loss 0.2761925458908081\n",
      "[Training Epoch 2] Batch 2955, Loss 0.2774309813976288\n",
      "[Training Epoch 2] Batch 2956, Loss 0.3185584545135498\n",
      "[Training Epoch 2] Batch 2957, Loss 0.2986316680908203\n",
      "[Training Epoch 2] Batch 2958, Loss 0.25137871503829956\n",
      "[Training Epoch 2] Batch 2959, Loss 0.2790391445159912\n",
      "[Training Epoch 2] Batch 2960, Loss 0.3085442781448364\n",
      "[Training Epoch 2] Batch 2961, Loss 0.2852543592453003\n",
      "[Training Epoch 2] Batch 2962, Loss 0.2550584077835083\n",
      "[Training Epoch 2] Batch 2963, Loss 0.26480698585510254\n",
      "[Training Epoch 2] Batch 2964, Loss 0.25830304622650146\n",
      "[Training Epoch 2] Batch 2965, Loss 0.2775697410106659\n",
      "[Training Epoch 2] Batch 2966, Loss 0.27307450771331787\n",
      "[Training Epoch 2] Batch 2967, Loss 0.26336005330085754\n",
      "[Training Epoch 2] Batch 2968, Loss 0.29930931329727173\n",
      "[Training Epoch 2] Batch 2969, Loss 0.24810320138931274\n",
      "[Training Epoch 2] Batch 2970, Loss 0.25210291147232056\n",
      "[Training Epoch 2] Batch 2971, Loss 0.28889232873916626\n",
      "[Training Epoch 2] Batch 2972, Loss 0.282547265291214\n",
      "[Training Epoch 2] Batch 2973, Loss 0.2907453179359436\n",
      "[Training Epoch 2] Batch 2974, Loss 0.26932740211486816\n",
      "[Training Epoch 2] Batch 2975, Loss 0.27818262577056885\n",
      "[Training Epoch 2] Batch 2976, Loss 0.2538289725780487\n",
      "[Training Epoch 2] Batch 2977, Loss 0.29737794399261475\n",
      "[Training Epoch 2] Batch 2978, Loss 0.2672792971134186\n",
      "[Training Epoch 2] Batch 2979, Loss 0.2783967852592468\n",
      "[Training Epoch 2] Batch 2980, Loss 0.3007522225379944\n",
      "[Training Epoch 2] Batch 2981, Loss 0.2589724063873291\n",
      "[Training Epoch 2] Batch 2982, Loss 0.27268779277801514\n",
      "[Training Epoch 2] Batch 2983, Loss 0.2637290060520172\n",
      "[Training Epoch 2] Batch 2984, Loss 0.3089991509914398\n",
      "[Training Epoch 2] Batch 2985, Loss 0.2678891122341156\n",
      "[Training Epoch 2] Batch 2986, Loss 0.25861358642578125\n",
      "[Training Epoch 2] Batch 2987, Loss 0.27607104182243347\n",
      "[Training Epoch 2] Batch 2988, Loss 0.28454673290252686\n",
      "[Training Epoch 2] Batch 2989, Loss 0.26185673475265503\n",
      "[Training Epoch 2] Batch 2990, Loss 0.2796167731285095\n",
      "[Training Epoch 2] Batch 2991, Loss 0.254851758480072\n",
      "[Training Epoch 2] Batch 2992, Loss 0.28803586959838867\n",
      "[Training Epoch 2] Batch 2993, Loss 0.2672048509120941\n",
      "[Training Epoch 2] Batch 2994, Loss 0.28150835633277893\n",
      "[Training Epoch 2] Batch 2995, Loss 0.2738807797431946\n",
      "[Training Epoch 2] Batch 2996, Loss 0.29398590326309204\n",
      "[Training Epoch 2] Batch 2997, Loss 0.29580697417259216\n",
      "[Training Epoch 2] Batch 2998, Loss 0.28476712107658386\n",
      "[Training Epoch 2] Batch 2999, Loss 0.2840394973754883\n",
      "[Training Epoch 2] Batch 3000, Loss 0.2740054726600647\n",
      "[Training Epoch 2] Batch 3001, Loss 0.28517425060272217\n",
      "[Training Epoch 2] Batch 3002, Loss 0.2756311297416687\n",
      "[Training Epoch 2] Batch 3003, Loss 0.2953330874443054\n",
      "[Training Epoch 2] Batch 3004, Loss 0.2796485126018524\n",
      "[Training Epoch 2] Batch 3005, Loss 0.30555638670921326\n",
      "[Training Epoch 2] Batch 3006, Loss 0.2676054835319519\n",
      "[Training Epoch 2] Batch 3007, Loss 0.2593303322792053\n",
      "[Training Epoch 2] Batch 3008, Loss 0.2958594560623169\n",
      "[Training Epoch 2] Batch 3009, Loss 0.2735756039619446\n",
      "[Training Epoch 2] Batch 3010, Loss 0.25476229190826416\n",
      "[Training Epoch 2] Batch 3011, Loss 0.2830200791358948\n",
      "[Training Epoch 2] Batch 3012, Loss 0.25465360283851624\n",
      "[Training Epoch 2] Batch 3013, Loss 0.27764201164245605\n",
      "[Training Epoch 2] Batch 3014, Loss 0.2868059277534485\n",
      "[Training Epoch 2] Batch 3015, Loss 0.30421799421310425\n",
      "[Training Epoch 2] Batch 3016, Loss 0.2874865233898163\n",
      "[Training Epoch 2] Batch 3017, Loss 0.2791523337364197\n",
      "[Training Epoch 2] Batch 3018, Loss 0.25833117961883545\n",
      "[Training Epoch 2] Batch 3019, Loss 0.26603639125823975\n",
      "[Training Epoch 2] Batch 3020, Loss 0.28421878814697266\n",
      "[Training Epoch 2] Batch 3021, Loss 0.248501718044281\n",
      "[Training Epoch 2] Batch 3022, Loss 0.2777140140533447\n",
      "[Training Epoch 2] Batch 3023, Loss 0.2805987298488617\n",
      "[Training Epoch 2] Batch 3024, Loss 0.27480238676071167\n",
      "[Training Epoch 2] Batch 3025, Loss 0.30584824085235596\n",
      "[Training Epoch 2] Batch 3026, Loss 0.2917531132698059\n",
      "[Training Epoch 2] Batch 3027, Loss 0.3076924681663513\n",
      "[Training Epoch 2] Batch 3028, Loss 0.2719871401786804\n",
      "[Training Epoch 2] Batch 3029, Loss 0.29930227994918823\n",
      "[Training Epoch 2] Batch 3030, Loss 0.3008698523044586\n",
      "[Training Epoch 2] Batch 3031, Loss 0.29953861236572266\n",
      "[Training Epoch 2] Batch 3032, Loss 0.2789868414402008\n",
      "[Training Epoch 2] Batch 3033, Loss 0.28652501106262207\n",
      "[Training Epoch 2] Batch 3034, Loss 0.28463947772979736\n",
      "[Training Epoch 2] Batch 3035, Loss 0.2884006202220917\n",
      "[Training Epoch 2] Batch 3036, Loss 0.27129319310188293\n",
      "[Training Epoch 2] Batch 3037, Loss 0.25582921504974365\n",
      "[Training Epoch 2] Batch 3038, Loss 0.30468595027923584\n",
      "[Training Epoch 2] Batch 3039, Loss 0.2645806670188904\n",
      "[Training Epoch 2] Batch 3040, Loss 0.26991093158721924\n",
      "[Training Epoch 2] Batch 3041, Loss 0.30558329820632935\n",
      "[Training Epoch 2] Batch 3042, Loss 0.2850080132484436\n",
      "[Training Epoch 2] Batch 3043, Loss 0.2614396810531616\n",
      "[Training Epoch 2] Batch 3044, Loss 0.27877265214920044\n",
      "[Training Epoch 2] Batch 3045, Loss 0.28220662474632263\n",
      "[Training Epoch 2] Batch 3046, Loss 0.25964421033859253\n",
      "[Training Epoch 2] Batch 3047, Loss 0.27785539627075195\n",
      "[Training Epoch 2] Batch 3048, Loss 0.2928363084793091\n",
      "[Training Epoch 2] Batch 3049, Loss 0.27160364389419556\n",
      "[Training Epoch 2] Batch 3050, Loss 0.28757813572883606\n",
      "[Training Epoch 2] Batch 3051, Loss 0.28624358773231506\n",
      "[Training Epoch 2] Batch 3052, Loss 0.2804020047187805\n",
      "[Training Epoch 2] Batch 3053, Loss 0.26997435092926025\n",
      "[Training Epoch 2] Batch 3054, Loss 0.27427592873573303\n",
      "[Training Epoch 2] Batch 3055, Loss 0.2678358852863312\n",
      "[Training Epoch 2] Batch 3056, Loss 0.27172136306762695\n",
      "[Training Epoch 2] Batch 3057, Loss 0.2772732973098755\n",
      "[Training Epoch 2] Batch 3058, Loss 0.2638261914253235\n",
      "[Training Epoch 2] Batch 3059, Loss 0.32228437066078186\n",
      "[Training Epoch 2] Batch 3060, Loss 0.25734469294548035\n",
      "[Training Epoch 2] Batch 3061, Loss 0.2840871214866638\n",
      "[Training Epoch 2] Batch 3062, Loss 0.2701648473739624\n",
      "[Training Epoch 2] Batch 3063, Loss 0.2868584990501404\n",
      "[Training Epoch 2] Batch 3064, Loss 0.25551968812942505\n",
      "[Training Epoch 2] Batch 3065, Loss 0.28225603699684143\n",
      "[Training Epoch 2] Batch 3066, Loss 0.2918767035007477\n",
      "[Training Epoch 2] Batch 3067, Loss 0.2719383239746094\n",
      "[Training Epoch 2] Batch 3068, Loss 0.2596626579761505\n",
      "[Training Epoch 2] Batch 3069, Loss 0.24131476879119873\n",
      "[Training Epoch 2] Batch 3070, Loss 0.2835296392440796\n",
      "[Training Epoch 2] Batch 3071, Loss 0.2621515989303589\n",
      "[Training Epoch 2] Batch 3072, Loss 0.27503636479377747\n",
      "[Training Epoch 2] Batch 3073, Loss 0.29902875423431396\n",
      "[Training Epoch 2] Batch 3074, Loss 0.3142378330230713\n",
      "[Training Epoch 2] Batch 3075, Loss 0.31010913848876953\n",
      "[Training Epoch 2] Batch 3076, Loss 0.2810474634170532\n",
      "[Training Epoch 2] Batch 3077, Loss 0.29239898920059204\n",
      "[Training Epoch 2] Batch 3078, Loss 0.2721066474914551\n",
      "[Training Epoch 2] Batch 3079, Loss 0.2500826120376587\n",
      "[Training Epoch 2] Batch 3080, Loss 0.2627369463443756\n",
      "[Training Epoch 2] Batch 3081, Loss 0.2641757130622864\n",
      "[Training Epoch 2] Batch 3082, Loss 0.2630471885204315\n",
      "[Training Epoch 2] Batch 3083, Loss 0.28345492482185364\n",
      "[Training Epoch 2] Batch 3084, Loss 0.26847130060195923\n",
      "[Training Epoch 2] Batch 3085, Loss 0.28430652618408203\n",
      "[Training Epoch 2] Batch 3086, Loss 0.27242618799209595\n",
      "[Training Epoch 2] Batch 3087, Loss 0.2597341537475586\n",
      "[Training Epoch 2] Batch 3088, Loss 0.2733961045742035\n",
      "[Training Epoch 2] Batch 3089, Loss 0.2938759922981262\n",
      "[Training Epoch 2] Batch 3090, Loss 0.290086567401886\n",
      "[Training Epoch 2] Batch 3091, Loss 0.2745558023452759\n",
      "[Training Epoch 2] Batch 3092, Loss 0.2636466324329376\n",
      "[Training Epoch 2] Batch 3093, Loss 0.2509021759033203\n",
      "[Training Epoch 2] Batch 3094, Loss 0.2828659415245056\n",
      "[Training Epoch 2] Batch 3095, Loss 0.2837618887424469\n",
      "[Training Epoch 2] Batch 3096, Loss 0.26846081018447876\n",
      "[Training Epoch 2] Batch 3097, Loss 0.28269338607788086\n",
      "[Training Epoch 2] Batch 3098, Loss 0.2799030542373657\n",
      "[Training Epoch 2] Batch 3099, Loss 0.28010398149490356\n",
      "[Training Epoch 2] Batch 3100, Loss 0.28194403648376465\n",
      "[Training Epoch 2] Batch 3101, Loss 0.2954501509666443\n",
      "[Training Epoch 2] Batch 3102, Loss 0.2575511932373047\n",
      "[Training Epoch 2] Batch 3103, Loss 0.287893146276474\n",
      "[Training Epoch 2] Batch 3104, Loss 0.2652788758277893\n",
      "[Training Epoch 2] Batch 3105, Loss 0.2808622717857361\n",
      "[Training Epoch 2] Batch 3106, Loss 0.2694156765937805\n",
      "[Training Epoch 2] Batch 3107, Loss 0.2907894253730774\n",
      "[Training Epoch 2] Batch 3108, Loss 0.261014461517334\n",
      "[Training Epoch 2] Batch 3109, Loss 0.29253268241882324\n",
      "[Training Epoch 2] Batch 3110, Loss 0.25728046894073486\n",
      "[Training Epoch 2] Batch 3111, Loss 0.28333398699760437\n",
      "[Training Epoch 2] Batch 3112, Loss 0.30163925886154175\n",
      "[Training Epoch 2] Batch 3113, Loss 0.2815086245536804\n",
      "[Training Epoch 2] Batch 3114, Loss 0.30046871304512024\n",
      "[Training Epoch 2] Batch 3115, Loss 0.26565682888031006\n",
      "[Training Epoch 2] Batch 3116, Loss 0.3021491467952728\n",
      "[Training Epoch 2] Batch 3117, Loss 0.29971736669540405\n",
      "[Training Epoch 2] Batch 3118, Loss 0.27667003870010376\n",
      "[Training Epoch 2] Batch 3119, Loss 0.2952820658683777\n",
      "[Training Epoch 2] Batch 3120, Loss 0.2796812951564789\n",
      "[Training Epoch 2] Batch 3121, Loss 0.2954571843147278\n",
      "[Training Epoch 2] Batch 3122, Loss 0.29602405428886414\n",
      "[Training Epoch 2] Batch 3123, Loss 0.277103990316391\n",
      "[Training Epoch 2] Batch 3124, Loss 0.2711593508720398\n",
      "[Training Epoch 2] Batch 3125, Loss 0.2926896810531616\n",
      "[Training Epoch 2] Batch 3126, Loss 0.3006134033203125\n",
      "[Training Epoch 2] Batch 3127, Loss 0.2804783582687378\n",
      "[Training Epoch 2] Batch 3128, Loss 0.2803249955177307\n",
      "[Training Epoch 2] Batch 3129, Loss 0.26026445627212524\n",
      "[Training Epoch 2] Batch 3130, Loss 0.2833186984062195\n",
      "[Training Epoch 2] Batch 3131, Loss 0.2768535912036896\n",
      "[Training Epoch 2] Batch 3132, Loss 0.28350454568862915\n",
      "[Training Epoch 2] Batch 3133, Loss 0.28493767976760864\n",
      "[Training Epoch 2] Batch 3134, Loss 0.2759619951248169\n",
      "[Training Epoch 2] Batch 3135, Loss 0.29293471574783325\n",
      "[Training Epoch 2] Batch 3136, Loss 0.26412433385849\n",
      "[Training Epoch 2] Batch 3137, Loss 0.29792314767837524\n",
      "[Training Epoch 2] Batch 3138, Loss 0.2645964026451111\n",
      "[Training Epoch 2] Batch 3139, Loss 0.27975383400917053\n",
      "[Training Epoch 2] Batch 3140, Loss 0.23767685890197754\n",
      "[Training Epoch 2] Batch 3141, Loss 0.23974984884262085\n",
      "[Training Epoch 2] Batch 3142, Loss 0.2785540521144867\n",
      "[Training Epoch 2] Batch 3143, Loss 0.31183984875679016\n",
      "[Training Epoch 2] Batch 3144, Loss 0.283670037984848\n",
      "[Training Epoch 2] Batch 3145, Loss 0.2621311545372009\n",
      "[Training Epoch 2] Batch 3146, Loss 0.28589439392089844\n",
      "[Training Epoch 2] Batch 3147, Loss 0.293996125459671\n",
      "[Training Epoch 2] Batch 3148, Loss 0.2990143895149231\n",
      "[Training Epoch 2] Batch 3149, Loss 0.26750051975250244\n",
      "[Training Epoch 2] Batch 3150, Loss 0.2776682376861572\n",
      "[Training Epoch 2] Batch 3151, Loss 0.2528393268585205\n",
      "[Training Epoch 2] Batch 3152, Loss 0.2909797430038452\n",
      "[Training Epoch 2] Batch 3153, Loss 0.27045583724975586\n",
      "[Training Epoch 2] Batch 3154, Loss 0.2683984339237213\n",
      "[Training Epoch 2] Batch 3155, Loss 0.2767273783683777\n",
      "[Training Epoch 2] Batch 3156, Loss 0.28645697236061096\n",
      "[Training Epoch 2] Batch 3157, Loss 0.29952603578567505\n",
      "[Training Epoch 2] Batch 3158, Loss 0.3164866268634796\n",
      "[Training Epoch 2] Batch 3159, Loss 0.2762085795402527\n",
      "[Training Epoch 2] Batch 3160, Loss 0.28316253423690796\n",
      "[Training Epoch 2] Batch 3161, Loss 0.2807580232620239\n",
      "[Training Epoch 2] Batch 3162, Loss 0.2577778398990631\n",
      "[Training Epoch 2] Batch 3163, Loss 0.26060962677001953\n",
      "[Training Epoch 2] Batch 3164, Loss 0.27459263801574707\n",
      "[Training Epoch 2] Batch 3165, Loss 0.2595829367637634\n",
      "[Training Epoch 2] Batch 3166, Loss 0.29761233925819397\n",
      "[Training Epoch 2] Batch 3167, Loss 0.2686702609062195\n",
      "[Training Epoch 2] Batch 3168, Loss 0.21634840965270996\n",
      "[Training Epoch 2] Batch 3169, Loss 0.2930912971496582\n",
      "[Training Epoch 2] Batch 3170, Loss 0.25533607602119446\n",
      "[Training Epoch 2] Batch 3171, Loss 0.2968910038471222\n",
      "[Training Epoch 2] Batch 3172, Loss 0.25754672288894653\n",
      "[Training Epoch 2] Batch 3173, Loss 0.28676438331604004\n",
      "[Training Epoch 2] Batch 3174, Loss 0.29505664110183716\n",
      "[Training Epoch 2] Batch 3175, Loss 0.26511722803115845\n",
      "[Training Epoch 2] Batch 3176, Loss 0.2627639174461365\n",
      "[Training Epoch 2] Batch 3177, Loss 0.24581632018089294\n",
      "[Training Epoch 2] Batch 3178, Loss 0.26320281624794006\n",
      "[Training Epoch 2] Batch 3179, Loss 0.2568928599357605\n",
      "[Training Epoch 2] Batch 3180, Loss 0.2533722221851349\n",
      "[Training Epoch 2] Batch 3181, Loss 0.31690818071365356\n",
      "[Training Epoch 2] Batch 3182, Loss 0.26544925570487976\n",
      "[Training Epoch 2] Batch 3183, Loss 0.27389782667160034\n",
      "[Training Epoch 2] Batch 3184, Loss 0.28739380836486816\n",
      "[Training Epoch 2] Batch 3185, Loss 0.27654770016670227\n",
      "[Training Epoch 2] Batch 3186, Loss 0.2593543529510498\n",
      "[Training Epoch 2] Batch 3187, Loss 0.2754393219947815\n",
      "[Training Epoch 2] Batch 3188, Loss 0.3035236597061157\n",
      "[Training Epoch 2] Batch 3189, Loss 0.28238075971603394\n",
      "[Training Epoch 2] Batch 3190, Loss 0.26004454493522644\n",
      "[Training Epoch 2] Batch 3191, Loss 0.2754538655281067\n",
      "[Training Epoch 2] Batch 3192, Loss 0.25996336340904236\n",
      "[Training Epoch 2] Batch 3193, Loss 0.2623518705368042\n",
      "[Training Epoch 2] Batch 3194, Loss 0.25353604555130005\n",
      "[Training Epoch 2] Batch 3195, Loss 0.27932095527648926\n",
      "[Training Epoch 2] Batch 3196, Loss 0.29432153701782227\n",
      "[Training Epoch 2] Batch 3197, Loss 0.27494490146636963\n",
      "[Training Epoch 2] Batch 3198, Loss 0.24010463058948517\n",
      "[Training Epoch 2] Batch 3199, Loss 0.2567007541656494\n",
      "[Training Epoch 2] Batch 3200, Loss 0.2849887013435364\n",
      "[Training Epoch 2] Batch 3201, Loss 0.26784831285476685\n",
      "[Training Epoch 2] Batch 3202, Loss 0.2826656103134155\n",
      "[Training Epoch 2] Batch 3203, Loss 0.26063916087150574\n",
      "[Training Epoch 2] Batch 3204, Loss 0.3080539107322693\n",
      "[Training Epoch 2] Batch 3205, Loss 0.2843968868255615\n",
      "[Training Epoch 2] Batch 3206, Loss 0.25322601199150085\n",
      "[Training Epoch 2] Batch 3207, Loss 0.27805107831954956\n",
      "[Training Epoch 2] Batch 3208, Loss 0.2838432192802429\n",
      "[Training Epoch 2] Batch 3209, Loss 0.2538872957229614\n",
      "[Training Epoch 2] Batch 3210, Loss 0.2667294442653656\n",
      "[Training Epoch 2] Batch 3211, Loss 0.2917637825012207\n",
      "[Training Epoch 2] Batch 3212, Loss 0.2697465419769287\n",
      "[Training Epoch 2] Batch 3213, Loss 0.2732089161872864\n",
      "[Training Epoch 2] Batch 3214, Loss 0.27215927839279175\n",
      "[Training Epoch 2] Batch 3215, Loss 0.2442188858985901\n",
      "[Training Epoch 2] Batch 3216, Loss 0.28683191537857056\n",
      "[Training Epoch 2] Batch 3217, Loss 0.3045106530189514\n",
      "[Training Epoch 2] Batch 3218, Loss 0.293423056602478\n",
      "[Training Epoch 2] Batch 3219, Loss 0.3075246810913086\n",
      "[Training Epoch 2] Batch 3220, Loss 0.28719252347946167\n",
      "[Training Epoch 2] Batch 3221, Loss 0.2620227336883545\n",
      "[Training Epoch 2] Batch 3222, Loss 0.2979047894477844\n",
      "[Training Epoch 2] Batch 3223, Loss 0.26488035917282104\n",
      "[Training Epoch 2] Batch 3224, Loss 0.27214422821998596\n",
      "[Training Epoch 2] Batch 3225, Loss 0.25880324840545654\n",
      "[Training Epoch 2] Batch 3226, Loss 0.2500888407230377\n",
      "[Training Epoch 2] Batch 3227, Loss 0.2931927740573883\n",
      "[Training Epoch 2] Batch 3228, Loss 0.2759292423725128\n",
      "[Training Epoch 2] Batch 3229, Loss 0.25984880328178406\n",
      "[Training Epoch 2] Batch 3230, Loss 0.26419597864151\n",
      "[Training Epoch 2] Batch 3231, Loss 0.28352922201156616\n",
      "[Training Epoch 2] Batch 3232, Loss 0.2634488344192505\n",
      "[Training Epoch 2] Batch 3233, Loss 0.26804354786872864\n",
      "[Training Epoch 2] Batch 3234, Loss 0.26919734477996826\n",
      "[Training Epoch 2] Batch 3235, Loss 0.25529688596725464\n",
      "[Training Epoch 2] Batch 3236, Loss 0.2569954991340637\n",
      "[Training Epoch 2] Batch 3237, Loss 0.2822014391422272\n",
      "[Training Epoch 2] Batch 3238, Loss 0.2731216251850128\n",
      "[Training Epoch 2] Batch 3239, Loss 0.2776622772216797\n",
      "[Training Epoch 2] Batch 3240, Loss 0.27431413531303406\n",
      "[Training Epoch 2] Batch 3241, Loss 0.25875765085220337\n",
      "[Training Epoch 2] Batch 3242, Loss 0.2693316638469696\n",
      "[Training Epoch 2] Batch 3243, Loss 0.265470415353775\n",
      "[Training Epoch 2] Batch 3244, Loss 0.27768972516059875\n",
      "[Training Epoch 2] Batch 3245, Loss 0.28634127974510193\n",
      "[Training Epoch 2] Batch 3246, Loss 0.29488348960876465\n",
      "[Training Epoch 2] Batch 3247, Loss 0.3052559196949005\n",
      "[Training Epoch 2] Batch 3248, Loss 0.3034270405769348\n",
      "[Training Epoch 2] Batch 3249, Loss 0.27371999621391296\n",
      "[Training Epoch 2] Batch 3250, Loss 0.29911112785339355\n",
      "[Training Epoch 2] Batch 3251, Loss 0.26878589391708374\n",
      "[Training Epoch 2] Batch 3252, Loss 0.25393933057785034\n",
      "[Training Epoch 2] Batch 3253, Loss 0.26604360342025757\n",
      "[Training Epoch 2] Batch 3254, Loss 0.280711829662323\n",
      "[Training Epoch 2] Batch 3255, Loss 0.2735097408294678\n",
      "[Training Epoch 2] Batch 3256, Loss 0.295200914144516\n",
      "[Training Epoch 2] Batch 3257, Loss 0.26074618101119995\n",
      "[Training Epoch 2] Batch 3258, Loss 0.2745485305786133\n",
      "[Training Epoch 2] Batch 3259, Loss 0.29343265295028687\n",
      "[Training Epoch 2] Batch 3260, Loss 0.278175950050354\n",
      "[Training Epoch 2] Batch 3261, Loss 0.28149598836898804\n",
      "[Training Epoch 2] Batch 3262, Loss 0.2766953706741333\n",
      "[Training Epoch 2] Batch 3263, Loss 0.2876380980014801\n",
      "[Training Epoch 2] Batch 3264, Loss 0.28476813435554504\n",
      "[Training Epoch 2] Batch 3265, Loss 0.26162150502204895\n",
      "[Training Epoch 2] Batch 3266, Loss 0.28149309754371643\n",
      "[Training Epoch 2] Batch 3267, Loss 0.269977867603302\n",
      "[Training Epoch 2] Batch 3268, Loss 0.2486172318458557\n",
      "[Training Epoch 2] Batch 3269, Loss 0.27649974822998047\n",
      "[Training Epoch 2] Batch 3270, Loss 0.24325670301914215\n",
      "[Training Epoch 2] Batch 3271, Loss 0.28001466393470764\n",
      "[Training Epoch 2] Batch 3272, Loss 0.2585965096950531\n",
      "[Training Epoch 2] Batch 3273, Loss 0.2768789529800415\n",
      "[Training Epoch 2] Batch 3274, Loss 0.29225289821624756\n",
      "[Training Epoch 2] Batch 3275, Loss 0.2763041853904724\n",
      "[Training Epoch 2] Batch 3276, Loss 0.2514207065105438\n",
      "[Training Epoch 2] Batch 3277, Loss 0.2643009424209595\n",
      "[Training Epoch 2] Batch 3278, Loss 0.28752925992012024\n",
      "[Training Epoch 2] Batch 3279, Loss 0.2723076045513153\n",
      "[Training Epoch 2] Batch 3280, Loss 0.28920209407806396\n",
      "[Training Epoch 2] Batch 3281, Loss 0.28039440512657166\n",
      "[Training Epoch 2] Batch 3282, Loss 0.2775791883468628\n",
      "[Training Epoch 2] Batch 3283, Loss 0.2660463750362396\n",
      "[Training Epoch 2] Batch 3284, Loss 0.26488763093948364\n",
      "[Training Epoch 2] Batch 3285, Loss 0.26592111587524414\n",
      "[Training Epoch 2] Batch 3286, Loss 0.25674745440483093\n",
      "[Training Epoch 2] Batch 3287, Loss 0.2972370982170105\n",
      "[Training Epoch 2] Batch 3288, Loss 0.284159392118454\n",
      "[Training Epoch 2] Batch 3289, Loss 0.2844933867454529\n",
      "[Training Epoch 2] Batch 3290, Loss 0.2737027406692505\n",
      "[Training Epoch 2] Batch 3291, Loss 0.3225956857204437\n",
      "[Training Epoch 2] Batch 3292, Loss 0.2548902630805969\n",
      "[Training Epoch 2] Batch 3293, Loss 0.28861144185066223\n",
      "[Training Epoch 2] Batch 3294, Loss 0.24758291244506836\n",
      "[Training Epoch 2] Batch 3295, Loss 0.28168627619743347\n",
      "[Training Epoch 2] Batch 3296, Loss 0.2978665232658386\n",
      "[Training Epoch 2] Batch 3297, Loss 0.27702030539512634\n",
      "[Training Epoch 2] Batch 3298, Loss 0.276220440864563\n",
      "[Training Epoch 2] Batch 3299, Loss 0.2723255157470703\n",
      "[Training Epoch 2] Batch 3300, Loss 0.2701038122177124\n",
      "[Training Epoch 2] Batch 3301, Loss 0.25830256938934326\n",
      "[Training Epoch 2] Batch 3302, Loss 0.2731662094593048\n",
      "[Training Epoch 2] Batch 3303, Loss 0.2843131422996521\n",
      "[Training Epoch 2] Batch 3304, Loss 0.2754993438720703\n",
      "[Training Epoch 2] Batch 3305, Loss 0.28641247749328613\n",
      "[Training Epoch 2] Batch 3306, Loss 0.2657860517501831\n",
      "[Training Epoch 2] Batch 3307, Loss 0.2795048654079437\n",
      "[Training Epoch 2] Batch 3308, Loss 0.27750062942504883\n",
      "[Training Epoch 2] Batch 3309, Loss 0.27811020612716675\n",
      "[Training Epoch 2] Batch 3310, Loss 0.2763729691505432\n",
      "[Training Epoch 2] Batch 3311, Loss 0.26126113533973694\n",
      "[Training Epoch 2] Batch 3312, Loss 0.26982584595680237\n",
      "[Training Epoch 2] Batch 3313, Loss 0.3136034309864044\n",
      "[Training Epoch 2] Batch 3314, Loss 0.2589094042778015\n",
      "[Training Epoch 2] Batch 3315, Loss 0.2527657747268677\n",
      "[Training Epoch 2] Batch 3316, Loss 0.27094316482543945\n",
      "[Training Epoch 2] Batch 3317, Loss 0.2747200131416321\n",
      "[Training Epoch 2] Batch 3318, Loss 0.28241831064224243\n",
      "[Training Epoch 2] Batch 3319, Loss 0.27982908487319946\n",
      "[Training Epoch 2] Batch 3320, Loss 0.2874903082847595\n",
      "[Training Epoch 2] Batch 3321, Loss 0.2916351556777954\n",
      "[Training Epoch 2] Batch 3322, Loss 0.27166497707366943\n",
      "[Training Epoch 2] Batch 3323, Loss 0.2949695587158203\n",
      "[Training Epoch 2] Batch 3324, Loss 0.30942848324775696\n",
      "[Training Epoch 2] Batch 3325, Loss 0.3016681373119354\n",
      "[Training Epoch 2] Batch 3326, Loss 0.2875508666038513\n",
      "[Training Epoch 2] Batch 3327, Loss 0.2575334310531616\n",
      "[Training Epoch 2] Batch 3328, Loss 0.2945846617221832\n",
      "[Training Epoch 2] Batch 3329, Loss 0.2914932072162628\n",
      "[Training Epoch 2] Batch 3330, Loss 0.26316705346107483\n",
      "[Training Epoch 2] Batch 3331, Loss 0.28704315423965454\n",
      "[Training Epoch 2] Batch 3332, Loss 0.27703553438186646\n",
      "[Training Epoch 2] Batch 3333, Loss 0.28710368275642395\n",
      "[Training Epoch 2] Batch 3334, Loss 0.29470571875572205\n",
      "[Training Epoch 2] Batch 3335, Loss 0.28224366903305054\n",
      "[Training Epoch 2] Batch 3336, Loss 0.2851344645023346\n",
      "[Training Epoch 2] Batch 3337, Loss 0.29017817974090576\n",
      "[Training Epoch 2] Batch 3338, Loss 0.27281612157821655\n",
      "[Training Epoch 2] Batch 3339, Loss 0.2525094151496887\n",
      "[Training Epoch 2] Batch 3340, Loss 0.2641341984272003\n",
      "[Training Epoch 2] Batch 3341, Loss 0.2943604588508606\n",
      "[Training Epoch 2] Batch 3342, Loss 0.2666008472442627\n",
      "[Training Epoch 2] Batch 3343, Loss 0.2848469316959381\n",
      "[Training Epoch 2] Batch 3344, Loss 0.2775987386703491\n",
      "[Training Epoch 2] Batch 3345, Loss 0.27296802401542664\n",
      "[Training Epoch 2] Batch 3346, Loss 0.2673225700855255\n",
      "[Training Epoch 2] Batch 3347, Loss 0.264220267534256\n",
      "[Training Epoch 2] Batch 3348, Loss 0.29036250710487366\n",
      "[Training Epoch 2] Batch 3349, Loss 0.2739386558532715\n",
      "[Training Epoch 2] Batch 3350, Loss 0.2932432293891907\n",
      "[Training Epoch 2] Batch 3351, Loss 0.2943575382232666\n",
      "[Training Epoch 2] Batch 3352, Loss 0.26915356516838074\n",
      "[Training Epoch 2] Batch 3353, Loss 0.2818880081176758\n",
      "[Training Epoch 2] Batch 3354, Loss 0.2928923964500427\n",
      "[Training Epoch 2] Batch 3355, Loss 0.25220948457717896\n",
      "[Training Epoch 2] Batch 3356, Loss 0.29772457480430603\n",
      "[Training Epoch 2] Batch 3357, Loss 0.2765258848667145\n",
      "[Training Epoch 2] Batch 3358, Loss 0.2629091739654541\n",
      "[Training Epoch 2] Batch 3359, Loss 0.2855973243713379\n",
      "[Training Epoch 2] Batch 3360, Loss 0.2934255003929138\n",
      "[Training Epoch 2] Batch 3361, Loss 0.2596113383769989\n",
      "[Training Epoch 2] Batch 3362, Loss 0.27751195430755615\n",
      "[Training Epoch 2] Batch 3363, Loss 0.2688967287540436\n",
      "[Training Epoch 2] Batch 3364, Loss 0.2851801812648773\n",
      "[Training Epoch 2] Batch 3365, Loss 0.29463014006614685\n",
      "[Training Epoch 2] Batch 3366, Loss 0.28314995765686035\n",
      "[Training Epoch 2] Batch 3367, Loss 0.2508704960346222\n",
      "[Training Epoch 2] Batch 3368, Loss 0.26956886053085327\n",
      "[Training Epoch 2] Batch 3369, Loss 0.27052274346351624\n",
      "[Training Epoch 2] Batch 3370, Loss 0.28490790724754333\n",
      "[Training Epoch 2] Batch 3371, Loss 0.26381543278694153\n",
      "[Training Epoch 2] Batch 3372, Loss 0.2716267704963684\n",
      "[Training Epoch 2] Batch 3373, Loss 0.2683737277984619\n",
      "[Training Epoch 2] Batch 3374, Loss 0.2794714570045471\n",
      "[Training Epoch 2] Batch 3375, Loss 0.28424346446990967\n",
      "[Training Epoch 2] Batch 3376, Loss 0.24104627966880798\n",
      "[Training Epoch 2] Batch 3377, Loss 0.28989699482917786\n",
      "[Training Epoch 2] Batch 3378, Loss 0.30096086859703064\n",
      "[Training Epoch 2] Batch 3379, Loss 0.30368882417678833\n",
      "[Training Epoch 2] Batch 3380, Loss 0.3036256730556488\n",
      "[Training Epoch 2] Batch 3381, Loss 0.27660107612609863\n",
      "[Training Epoch 2] Batch 3382, Loss 0.2572794556617737\n",
      "[Training Epoch 2] Batch 3383, Loss 0.28908246755599976\n",
      "[Training Epoch 2] Batch 3384, Loss 0.29240691661834717\n",
      "[Training Epoch 2] Batch 3385, Loss 0.29879313707351685\n",
      "[Training Epoch 2] Batch 3386, Loss 0.2700975239276886\n",
      "[Training Epoch 2] Batch 3387, Loss 0.26179760694503784\n",
      "[Training Epoch 2] Batch 3388, Loss 0.29895418882369995\n",
      "[Training Epoch 2] Batch 3389, Loss 0.30778539180755615\n",
      "[Training Epoch 2] Batch 3390, Loss 0.29032987356185913\n",
      "[Training Epoch 2] Batch 3391, Loss 0.2814077138900757\n",
      "[Training Epoch 2] Batch 3392, Loss 0.2543260455131531\n",
      "[Training Epoch 2] Batch 3393, Loss 0.27931705117225647\n",
      "[Training Epoch 2] Batch 3394, Loss 0.26549822092056274\n",
      "[Training Epoch 2] Batch 3395, Loss 0.26896947622299194\n",
      "[Training Epoch 2] Batch 3396, Loss 0.27481022477149963\n",
      "[Training Epoch 2] Batch 3397, Loss 0.2819814085960388\n",
      "[Training Epoch 2] Batch 3398, Loss 0.27122098207473755\n",
      "[Training Epoch 2] Batch 3399, Loss 0.2906138598918915\n",
      "[Training Epoch 2] Batch 3400, Loss 0.29099106788635254\n",
      "[Training Epoch 2] Batch 3401, Loss 0.2572792172431946\n",
      "[Training Epoch 2] Batch 3402, Loss 0.29021915793418884\n",
      "[Training Epoch 2] Batch 3403, Loss 0.27520662546157837\n",
      "[Training Epoch 2] Batch 3404, Loss 0.30140572786331177\n",
      "[Training Epoch 2] Batch 3405, Loss 0.2596673369407654\n",
      "[Training Epoch 2] Batch 3406, Loss 0.27417126297950745\n",
      "[Training Epoch 2] Batch 3407, Loss 0.27063822746276855\n",
      "[Training Epoch 2] Batch 3408, Loss 0.3028986155986786\n",
      "[Training Epoch 2] Batch 3409, Loss 0.28118282556533813\n",
      "[Training Epoch 2] Batch 3410, Loss 0.26445019245147705\n",
      "[Training Epoch 2] Batch 3411, Loss 0.30783599615097046\n",
      "[Training Epoch 2] Batch 3412, Loss 0.3038010895252228\n",
      "[Training Epoch 2] Batch 3413, Loss 0.27158376574516296\n",
      "[Training Epoch 2] Batch 3414, Loss 0.2867886424064636\n",
      "[Training Epoch 2] Batch 3415, Loss 0.2956223487854004\n",
      "[Training Epoch 2] Batch 3416, Loss 0.2891279458999634\n",
      "[Training Epoch 2] Batch 3417, Loss 0.2764981985092163\n",
      "[Training Epoch 2] Batch 3418, Loss 0.27563056349754333\n",
      "[Training Epoch 2] Batch 3419, Loss 0.25383633375167847\n",
      "[Training Epoch 2] Batch 3420, Loss 0.2876371145248413\n",
      "[Training Epoch 2] Batch 3421, Loss 0.2559751868247986\n",
      "[Training Epoch 2] Batch 3422, Loss 0.26111263036727905\n",
      "[Training Epoch 2] Batch 3423, Loss 0.2761296033859253\n",
      "[Training Epoch 2] Batch 3424, Loss 0.28844913840293884\n",
      "[Training Epoch 2] Batch 3425, Loss 0.3045980930328369\n",
      "[Training Epoch 2] Batch 3426, Loss 0.27576175332069397\n",
      "[Training Epoch 2] Batch 3427, Loss 0.2898271381855011\n",
      "[Training Epoch 2] Batch 3428, Loss 0.2819793224334717\n",
      "[Training Epoch 2] Batch 3429, Loss 0.2790325880050659\n",
      "[Training Epoch 2] Batch 3430, Loss 0.262125700712204\n",
      "[Training Epoch 2] Batch 3431, Loss 0.24711817502975464\n",
      "[Training Epoch 2] Batch 3432, Loss 0.2901698052883148\n",
      "[Training Epoch 2] Batch 3433, Loss 0.26696503162384033\n",
      "[Training Epoch 2] Batch 3434, Loss 0.3081645369529724\n",
      "[Training Epoch 2] Batch 3435, Loss 0.3033542037010193\n",
      "[Training Epoch 2] Batch 3436, Loss 0.26084622740745544\n",
      "[Training Epoch 2] Batch 3437, Loss 0.25716352462768555\n",
      "[Training Epoch 2] Batch 3438, Loss 0.2913188934326172\n",
      "[Training Epoch 2] Batch 3439, Loss 0.2615683376789093\n",
      "[Training Epoch 2] Batch 3440, Loss 0.3011147975921631\n",
      "[Training Epoch 2] Batch 3441, Loss 0.28005167841911316\n",
      "[Training Epoch 2] Batch 3442, Loss 0.3016761541366577\n",
      "[Training Epoch 2] Batch 3443, Loss 0.3007584512233734\n",
      "[Training Epoch 2] Batch 3444, Loss 0.25854772329330444\n",
      "[Training Epoch 2] Batch 3445, Loss 0.28853151202201843\n",
      "[Training Epoch 2] Batch 3446, Loss 0.29376643896102905\n",
      "[Training Epoch 2] Batch 3447, Loss 0.31876787543296814\n",
      "[Training Epoch 2] Batch 3448, Loss 0.2894899845123291\n",
      "[Training Epoch 2] Batch 3449, Loss 0.30004435777664185\n",
      "[Training Epoch 2] Batch 3450, Loss 0.30457043647766113\n",
      "[Training Epoch 2] Batch 3451, Loss 0.26526540517807007\n",
      "[Training Epoch 2] Batch 3452, Loss 0.2801547050476074\n",
      "[Training Epoch 2] Batch 3453, Loss 0.29477331042289734\n",
      "[Training Epoch 2] Batch 3454, Loss 0.29430800676345825\n",
      "[Training Epoch 2] Batch 3455, Loss 0.2844945788383484\n",
      "[Training Epoch 2] Batch 3456, Loss 0.29787957668304443\n",
      "[Training Epoch 2] Batch 3457, Loss 0.26486092805862427\n",
      "[Training Epoch 2] Batch 3458, Loss 0.3092852830886841\n",
      "[Training Epoch 2] Batch 3459, Loss 0.2567989230155945\n",
      "[Training Epoch 2] Batch 3460, Loss 0.27053022384643555\n",
      "[Training Epoch 2] Batch 3461, Loss 0.26115527749061584\n",
      "[Training Epoch 2] Batch 3462, Loss 0.30988502502441406\n",
      "[Training Epoch 2] Batch 3463, Loss 0.26822659373283386\n",
      "[Training Epoch 2] Batch 3464, Loss 0.2969900369644165\n",
      "[Training Epoch 2] Batch 3465, Loss 0.26152706146240234\n",
      "[Training Epoch 2] Batch 3466, Loss 0.27625712752342224\n",
      "[Training Epoch 2] Batch 3467, Loss 0.2963045835494995\n",
      "[Training Epoch 2] Batch 3468, Loss 0.25742387771606445\n",
      "[Training Epoch 2] Batch 3469, Loss 0.27950018644332886\n",
      "[Training Epoch 2] Batch 3470, Loss 0.2696322798728943\n",
      "[Training Epoch 2] Batch 3471, Loss 0.2713453769683838\n",
      "[Training Epoch 2] Batch 3472, Loss 0.2837445139884949\n",
      "[Training Epoch 2] Batch 3473, Loss 0.3163717985153198\n",
      "[Training Epoch 2] Batch 3474, Loss 0.26950132846832275\n",
      "[Training Epoch 2] Batch 3475, Loss 0.2674943208694458\n",
      "[Training Epoch 2] Batch 3476, Loss 0.2530953586101532\n",
      "[Training Epoch 2] Batch 3477, Loss 0.30272790789604187\n",
      "[Training Epoch 2] Batch 3478, Loss 0.266263484954834\n",
      "[Training Epoch 2] Batch 3479, Loss 0.2671600878238678\n",
      "[Training Epoch 2] Batch 3480, Loss 0.2605757713317871\n",
      "[Training Epoch 2] Batch 3481, Loss 0.25035786628723145\n",
      "[Training Epoch 2] Batch 3482, Loss 0.2531065344810486\n",
      "[Training Epoch 2] Batch 3483, Loss 0.2674293518066406\n",
      "[Training Epoch 2] Batch 3484, Loss 0.26305094361305237\n",
      "[Training Epoch 2] Batch 3485, Loss 0.2775009572505951\n",
      "[Training Epoch 2] Batch 3486, Loss 0.2727026641368866\n",
      "[Training Epoch 2] Batch 3487, Loss 0.2610066831111908\n",
      "[Training Epoch 2] Batch 3488, Loss 0.28556498885154724\n",
      "[Training Epoch 2] Batch 3489, Loss 0.28993508219718933\n",
      "[Training Epoch 2] Batch 3490, Loss 0.2687058448791504\n",
      "[Training Epoch 2] Batch 3491, Loss 0.27771955728530884\n",
      "[Training Epoch 2] Batch 3492, Loss 0.27496764063835144\n",
      "[Training Epoch 2] Batch 3493, Loss 0.26263362169265747\n",
      "[Training Epoch 2] Batch 3494, Loss 0.28507399559020996\n",
      "[Training Epoch 2] Batch 3495, Loss 0.2890351116657257\n",
      "[Training Epoch 2] Batch 3496, Loss 0.280017226934433\n",
      "[Training Epoch 2] Batch 3497, Loss 0.2851255536079407\n",
      "[Training Epoch 2] Batch 3498, Loss 0.2596205472946167\n",
      "[Training Epoch 2] Batch 3499, Loss 0.2728212773799896\n",
      "[Training Epoch 2] Batch 3500, Loss 0.29988664388656616\n",
      "[Training Epoch 2] Batch 3501, Loss 0.2810271978378296\n",
      "[Training Epoch 2] Batch 3502, Loss 0.27856624126434326\n",
      "[Training Epoch 2] Batch 3503, Loss 0.2843305766582489\n",
      "[Training Epoch 2] Batch 3504, Loss 0.269145131111145\n",
      "[Training Epoch 2] Batch 3505, Loss 0.293780654668808\n",
      "[Training Epoch 2] Batch 3506, Loss 0.2784828543663025\n",
      "[Training Epoch 2] Batch 3507, Loss 0.2950580418109894\n",
      "[Training Epoch 2] Batch 3508, Loss 0.25804463028907776\n",
      "[Training Epoch 2] Batch 3509, Loss 0.29607951641082764\n",
      "[Training Epoch 2] Batch 3510, Loss 0.2941116690635681\n",
      "[Training Epoch 2] Batch 3511, Loss 0.29171448945999146\n",
      "[Training Epoch 2] Batch 3512, Loss 0.27066826820373535\n",
      "[Training Epoch 2] Batch 3513, Loss 0.3013842701911926\n",
      "[Training Epoch 2] Batch 3514, Loss 0.28878358006477356\n",
      "[Training Epoch 2] Batch 3515, Loss 0.24975985288619995\n",
      "[Training Epoch 2] Batch 3516, Loss 0.27829205989837646\n",
      "[Training Epoch 2] Batch 3517, Loss 0.27766239643096924\n",
      "[Training Epoch 2] Batch 3518, Loss 0.25939369201660156\n",
      "[Training Epoch 2] Batch 3519, Loss 0.2629017233848572\n",
      "[Training Epoch 2] Batch 3520, Loss 0.2645166218280792\n",
      "[Training Epoch 2] Batch 3521, Loss 0.2898867130279541\n",
      "[Training Epoch 2] Batch 3522, Loss 0.28300172090530396\n",
      "[Training Epoch 2] Batch 3523, Loss 0.2988044023513794\n",
      "[Training Epoch 2] Batch 3524, Loss 0.31282228231430054\n",
      "[Training Epoch 2] Batch 3525, Loss 0.27717751264572144\n",
      "[Training Epoch 2] Batch 3526, Loss 0.2640443444252014\n",
      "[Training Epoch 2] Batch 3527, Loss 0.26948070526123047\n",
      "[Training Epoch 2] Batch 3528, Loss 0.30595970153808594\n",
      "[Training Epoch 2] Batch 3529, Loss 0.2797107696533203\n",
      "[Training Epoch 2] Batch 3530, Loss 0.2866363823413849\n",
      "[Training Epoch 2] Batch 3531, Loss 0.33033058047294617\n",
      "[Training Epoch 2] Batch 3532, Loss 0.2784210443496704\n",
      "[Training Epoch 2] Batch 3533, Loss 0.27423426508903503\n",
      "[Training Epoch 2] Batch 3534, Loss 0.2548966705799103\n",
      "[Training Epoch 2] Batch 3535, Loss 0.2541980445384979\n",
      "[Training Epoch 2] Batch 3536, Loss 0.2794153690338135\n",
      "[Training Epoch 2] Batch 3537, Loss 0.2928100824356079\n",
      "[Training Epoch 2] Batch 3538, Loss 0.25035393238067627\n",
      "[Training Epoch 2] Batch 3539, Loss 0.26836806535720825\n",
      "[Training Epoch 2] Batch 3540, Loss 0.2801213264465332\n",
      "[Training Epoch 2] Batch 3541, Loss 0.2803303897380829\n",
      "[Training Epoch 2] Batch 3542, Loss 0.28591346740722656\n",
      "[Training Epoch 2] Batch 3543, Loss 0.28850531578063965\n",
      "[Training Epoch 2] Batch 3544, Loss 0.28934383392333984\n",
      "[Training Epoch 2] Batch 3545, Loss 0.26730841398239136\n",
      "[Training Epoch 2] Batch 3546, Loss 0.2824801206588745\n",
      "[Training Epoch 2] Batch 3547, Loss 0.2951650619506836\n",
      "[Training Epoch 2] Batch 3548, Loss 0.2658499479293823\n",
      "[Training Epoch 2] Batch 3549, Loss 0.2733333706855774\n",
      "[Training Epoch 2] Batch 3550, Loss 0.23912186920642853\n",
      "[Training Epoch 2] Batch 3551, Loss 0.28269410133361816\n",
      "[Training Epoch 2] Batch 3552, Loss 0.2823963463306427\n",
      "[Training Epoch 2] Batch 3553, Loss 0.27391934394836426\n",
      "[Training Epoch 2] Batch 3554, Loss 0.31478962302207947\n",
      "[Training Epoch 2] Batch 3555, Loss 0.31075602769851685\n",
      "[Training Epoch 2] Batch 3556, Loss 0.25293222069740295\n",
      "[Training Epoch 2] Batch 3557, Loss 0.2743781507015228\n",
      "[Training Epoch 2] Batch 3558, Loss 0.2746681571006775\n",
      "[Training Epoch 2] Batch 3559, Loss 0.2808721661567688\n",
      "[Training Epoch 2] Batch 3560, Loss 0.26204612851142883\n",
      "[Training Epoch 2] Batch 3561, Loss 0.25523892045021057\n",
      "[Training Epoch 2] Batch 3562, Loss 0.27809810638427734\n",
      "[Training Epoch 2] Batch 3563, Loss 0.2905442416667938\n",
      "[Training Epoch 2] Batch 3564, Loss 0.27734559774398804\n",
      "[Training Epoch 2] Batch 3565, Loss 0.2826942205429077\n",
      "[Training Epoch 2] Batch 3566, Loss 0.2815215587615967\n",
      "[Training Epoch 2] Batch 3567, Loss 0.28817611932754517\n",
      "[Training Epoch 2] Batch 3568, Loss 0.2779592275619507\n",
      "[Training Epoch 2] Batch 3569, Loss 0.2573838531970978\n",
      "[Training Epoch 2] Batch 3570, Loss 0.3005264103412628\n",
      "[Training Epoch 2] Batch 3571, Loss 0.2886244058609009\n",
      "[Training Epoch 2] Batch 3572, Loss 0.2794034481048584\n",
      "[Training Epoch 2] Batch 3573, Loss 0.3058069944381714\n",
      "[Training Epoch 2] Batch 3574, Loss 0.286068320274353\n",
      "[Training Epoch 2] Batch 3575, Loss 0.27432483434677124\n",
      "[Training Epoch 2] Batch 3576, Loss 0.29719120264053345\n",
      "[Training Epoch 2] Batch 3577, Loss 0.26569876074790955\n",
      "[Training Epoch 2] Batch 3578, Loss 0.2909087538719177\n",
      "[Training Epoch 2] Batch 3579, Loss 0.293707013130188\n",
      "[Training Epoch 2] Batch 3580, Loss 0.30549195408821106\n",
      "[Training Epoch 2] Batch 3581, Loss 0.26122990250587463\n",
      "[Training Epoch 2] Batch 3582, Loss 0.27197718620300293\n",
      "[Training Epoch 2] Batch 3583, Loss 0.2876667380332947\n",
      "[Training Epoch 2] Batch 3584, Loss 0.28398650884628296\n",
      "[Training Epoch 2] Batch 3585, Loss 0.2805596590042114\n",
      "[Training Epoch 2] Batch 3586, Loss 0.24751272797584534\n",
      "[Training Epoch 2] Batch 3587, Loss 0.29328063130378723\n",
      "[Training Epoch 2] Batch 3588, Loss 0.2843741178512573\n",
      "[Training Epoch 2] Batch 3589, Loss 0.27744537591934204\n",
      "[Training Epoch 2] Batch 3590, Loss 0.2526344358921051\n",
      "[Training Epoch 2] Batch 3591, Loss 0.3104115128517151\n",
      "[Training Epoch 2] Batch 3592, Loss 0.28051894903182983\n",
      "[Training Epoch 2] Batch 3593, Loss 0.25782155990600586\n",
      "[Training Epoch 2] Batch 3594, Loss 0.29327908158302307\n",
      "[Training Epoch 2] Batch 3595, Loss 0.2952655851840973\n",
      "[Training Epoch 2] Batch 3596, Loss 0.28400400280952454\n",
      "[Training Epoch 2] Batch 3597, Loss 0.28581738471984863\n",
      "[Training Epoch 2] Batch 3598, Loss 0.2903139591217041\n",
      "[Training Epoch 2] Batch 3599, Loss 0.2619805335998535\n",
      "[Training Epoch 2] Batch 3600, Loss 0.2807356119155884\n",
      "[Training Epoch 2] Batch 3601, Loss 0.2649964690208435\n",
      "[Training Epoch 2] Batch 3602, Loss 0.3001343607902527\n",
      "[Training Epoch 2] Batch 3603, Loss 0.26831522583961487\n",
      "[Training Epoch 2] Batch 3604, Loss 0.2758191227912903\n",
      "[Training Epoch 2] Batch 3605, Loss 0.29242196679115295\n",
      "[Training Epoch 2] Batch 3606, Loss 0.2900348901748657\n",
      "[Training Epoch 2] Batch 3607, Loss 0.2978200912475586\n",
      "[Training Epoch 2] Batch 3608, Loss 0.2979128956794739\n",
      "[Training Epoch 2] Batch 3609, Loss 0.28361037373542786\n",
      "[Training Epoch 2] Batch 3610, Loss 0.27780234813690186\n",
      "[Training Epoch 2] Batch 3611, Loss 0.276969850063324\n",
      "[Training Epoch 2] Batch 3612, Loss 0.276189923286438\n",
      "[Training Epoch 2] Batch 3613, Loss 0.27410638332366943\n",
      "[Training Epoch 2] Batch 3614, Loss 0.27719807624816895\n",
      "[Training Epoch 2] Batch 3615, Loss 0.31051042675971985\n",
      "[Training Epoch 2] Batch 3616, Loss 0.289124071598053\n",
      "[Training Epoch 2] Batch 3617, Loss 0.2594711184501648\n",
      "[Training Epoch 2] Batch 3618, Loss 0.28843367099761963\n",
      "[Training Epoch 2] Batch 3619, Loss 0.2474900782108307\n",
      "[Training Epoch 2] Batch 3620, Loss 0.28852543234825134\n",
      "[Training Epoch 2] Batch 3621, Loss 0.2711960971355438\n",
      "[Training Epoch 2] Batch 3622, Loss 0.2791513204574585\n",
      "[Training Epoch 2] Batch 3623, Loss 0.2981347441673279\n",
      "[Training Epoch 2] Batch 3624, Loss 0.27190470695495605\n",
      "[Training Epoch 2] Batch 3625, Loss 0.26273566484451294\n",
      "[Training Epoch 2] Batch 3626, Loss 0.2689070999622345\n",
      "[Training Epoch 2] Batch 3627, Loss 0.25179868936538696\n",
      "[Training Epoch 2] Batch 3628, Loss 0.25994354486465454\n",
      "[Training Epoch 2] Batch 3629, Loss 0.2681765556335449\n",
      "[Training Epoch 2] Batch 3630, Loss 0.2704092264175415\n",
      "[Training Epoch 2] Batch 3631, Loss 0.26152169704437256\n",
      "[Training Epoch 2] Batch 3632, Loss 0.299199640750885\n",
      "[Training Epoch 2] Batch 3633, Loss 0.24707075953483582\n",
      "[Training Epoch 2] Batch 3634, Loss 0.2960108518600464\n",
      "[Training Epoch 2] Batch 3635, Loss 0.29479163885116577\n",
      "[Training Epoch 2] Batch 3636, Loss 0.25686436891555786\n",
      "[Training Epoch 2] Batch 3637, Loss 0.30428314208984375\n",
      "[Training Epoch 2] Batch 3638, Loss 0.28816571831703186\n",
      "[Training Epoch 2] Batch 3639, Loss 0.3015380799770355\n",
      "[Training Epoch 2] Batch 3640, Loss 0.2479785531759262\n",
      "[Training Epoch 2] Batch 3641, Loss 0.29543179273605347\n",
      "[Training Epoch 2] Batch 3642, Loss 0.263653427362442\n",
      "[Training Epoch 2] Batch 3643, Loss 0.28232046961784363\n",
      "[Training Epoch 2] Batch 3644, Loss 0.2604382634162903\n",
      "[Training Epoch 2] Batch 3645, Loss 0.277116060256958\n",
      "[Training Epoch 2] Batch 3646, Loss 0.2595669627189636\n",
      "[Training Epoch 2] Batch 3647, Loss 0.28427475690841675\n",
      "[Training Epoch 2] Batch 3648, Loss 0.2903580665588379\n",
      "[Training Epoch 2] Batch 3649, Loss 0.2924618124961853\n",
      "[Training Epoch 2] Batch 3650, Loss 0.28063780069351196\n",
      "[Training Epoch 2] Batch 3651, Loss 0.25175750255584717\n",
      "[Training Epoch 2] Batch 3652, Loss 0.2966468632221222\n",
      "[Training Epoch 2] Batch 3653, Loss 0.25325894355773926\n",
      "[Training Epoch 2] Batch 3654, Loss 0.2703999876976013\n",
      "[Training Epoch 2] Batch 3655, Loss 0.277493953704834\n",
      "[Training Epoch 2] Batch 3656, Loss 0.22553768754005432\n",
      "[Training Epoch 2] Batch 3657, Loss 0.26512137055397034\n",
      "[Training Epoch 2] Batch 3658, Loss 0.28896960616111755\n",
      "[Training Epoch 2] Batch 3659, Loss 0.30403462052345276\n",
      "[Training Epoch 2] Batch 3660, Loss 0.2870655059814453\n",
      "[Training Epoch 2] Batch 3661, Loss 0.2744511067867279\n",
      "[Training Epoch 2] Batch 3662, Loss 0.28419724106788635\n",
      "[Training Epoch 2] Batch 3663, Loss 0.26545119285583496\n",
      "[Training Epoch 2] Batch 3664, Loss 0.2827106714248657\n",
      "[Training Epoch 2] Batch 3665, Loss 0.2920340895652771\n",
      "[Training Epoch 2] Batch 3666, Loss 0.2791594862937927\n",
      "[Training Epoch 2] Batch 3667, Loss 0.2869122624397278\n",
      "[Training Epoch 2] Batch 3668, Loss 0.2637806534767151\n",
      "[Training Epoch 2] Batch 3669, Loss 0.2798830270767212\n",
      "[Training Epoch 2] Batch 3670, Loss 0.25735098123550415\n",
      "[Training Epoch 2] Batch 3671, Loss 0.2493235170841217\n",
      "[Training Epoch 2] Batch 3672, Loss 0.28275591135025024\n",
      "[Training Epoch 2] Batch 3673, Loss 0.2291085571050644\n",
      "[Training Epoch 2] Batch 3674, Loss 0.25929075479507446\n",
      "[Training Epoch 2] Batch 3675, Loss 0.28510910272598267\n",
      "[Training Epoch 2] Batch 3676, Loss 0.28505373001098633\n",
      "[Training Epoch 2] Batch 3677, Loss 0.2686360478401184\n",
      "[Training Epoch 2] Batch 3678, Loss 0.2836870551109314\n",
      "[Training Epoch 2] Batch 3679, Loss 0.29214248061180115\n",
      "[Training Epoch 2] Batch 3680, Loss 0.26912346482276917\n",
      "[Training Epoch 2] Batch 3681, Loss 0.2641671299934387\n",
      "[Training Epoch 2] Batch 3682, Loss 0.2786402106285095\n",
      "[Training Epoch 2] Batch 3683, Loss 0.26979589462280273\n",
      "[Training Epoch 2] Batch 3684, Loss 0.2948395609855652\n",
      "[Training Epoch 2] Batch 3685, Loss 0.29130566120147705\n",
      "[Training Epoch 2] Batch 3686, Loss 0.3054913282394409\n",
      "[Training Epoch 2] Batch 3687, Loss 0.2625974714756012\n",
      "[Training Epoch 2] Batch 3688, Loss 0.2981495261192322\n",
      "[Training Epoch 2] Batch 3689, Loss 0.26861435174942017\n",
      "[Training Epoch 2] Batch 3690, Loss 0.2791321277618408\n",
      "[Training Epoch 2] Batch 3691, Loss 0.29551732540130615\n",
      "[Training Epoch 2] Batch 3692, Loss 0.291260302066803\n",
      "[Training Epoch 2] Batch 3693, Loss 0.2760443091392517\n",
      "[Training Epoch 2] Batch 3694, Loss 0.29432213306427\n",
      "[Training Epoch 2] Batch 3695, Loss 0.27387088537216187\n",
      "[Training Epoch 2] Batch 3696, Loss 0.28433024883270264\n",
      "[Training Epoch 2] Batch 3697, Loss 0.29428306221961975\n",
      "[Training Epoch 2] Batch 3698, Loss 0.323539137840271\n",
      "[Training Epoch 2] Batch 3699, Loss 0.2740362286567688\n",
      "[Training Epoch 2] Batch 3700, Loss 0.29055851697921753\n",
      "[Training Epoch 2] Batch 3701, Loss 0.2849160134792328\n",
      "[Training Epoch 2] Batch 3702, Loss 0.27380120754241943\n",
      "[Training Epoch 2] Batch 3703, Loss 0.2753009796142578\n",
      "[Training Epoch 2] Batch 3704, Loss 0.29579442739486694\n",
      "[Training Epoch 2] Batch 3705, Loss 0.27725422382354736\n",
      "[Training Epoch 2] Batch 3706, Loss 0.2783939838409424\n",
      "[Training Epoch 2] Batch 3707, Loss 0.25990962982177734\n",
      "[Training Epoch 2] Batch 3708, Loss 0.25499942898750305\n",
      "[Training Epoch 2] Batch 3709, Loss 0.3095906674861908\n",
      "[Training Epoch 2] Batch 3710, Loss 0.2958608567714691\n",
      "[Training Epoch 2] Batch 3711, Loss 0.2674404978752136\n",
      "[Training Epoch 2] Batch 3712, Loss 0.273000568151474\n",
      "[Training Epoch 2] Batch 3713, Loss 0.30181992053985596\n",
      "[Training Epoch 2] Batch 3714, Loss 0.32403722405433655\n",
      "[Training Epoch 2] Batch 3715, Loss 0.29296284914016724\n",
      "[Training Epoch 2] Batch 3716, Loss 0.2838612198829651\n",
      "[Training Epoch 2] Batch 3717, Loss 0.3161013722419739\n",
      "[Training Epoch 2] Batch 3718, Loss 0.2690542936325073\n",
      "[Training Epoch 2] Batch 3719, Loss 0.29987847805023193\n",
      "[Training Epoch 2] Batch 3720, Loss 0.2762019634246826\n",
      "[Training Epoch 2] Batch 3721, Loss 0.28171271085739136\n",
      "[Training Epoch 2] Batch 3722, Loss 0.2735176086425781\n",
      "[Training Epoch 2] Batch 3723, Loss 0.26894640922546387\n",
      "[Training Epoch 2] Batch 3724, Loss 0.30498847365379333\n",
      "[Training Epoch 2] Batch 3725, Loss 0.2765160799026489\n",
      "[Training Epoch 2] Batch 3726, Loss 0.2803722620010376\n",
      "[Training Epoch 2] Batch 3727, Loss 0.2781454920768738\n",
      "[Training Epoch 2] Batch 3728, Loss 0.2576693892478943\n",
      "[Training Epoch 2] Batch 3729, Loss 0.2855713367462158\n",
      "[Training Epoch 2] Batch 3730, Loss 0.2727847695350647\n",
      "[Training Epoch 2] Batch 3731, Loss 0.30846089124679565\n",
      "[Training Epoch 2] Batch 3732, Loss 0.2868120074272156\n",
      "[Training Epoch 2] Batch 3733, Loss 0.26398766040802\n",
      "[Training Epoch 2] Batch 3734, Loss 0.2958976626396179\n",
      "[Training Epoch 2] Batch 3735, Loss 0.28713542222976685\n",
      "[Training Epoch 2] Batch 3736, Loss 0.265245258808136\n",
      "[Training Epoch 2] Batch 3737, Loss 0.25293397903442383\n",
      "[Training Epoch 2] Batch 3738, Loss 0.2575295567512512\n",
      "[Training Epoch 2] Batch 3739, Loss 0.27511483430862427\n",
      "[Training Epoch 2] Batch 3740, Loss 0.25819629430770874\n",
      "[Training Epoch 2] Batch 3741, Loss 0.2850818634033203\n",
      "[Training Epoch 2] Batch 3742, Loss 0.30090266466140747\n",
      "[Training Epoch 2] Batch 3743, Loss 0.2932146489620209\n",
      "[Training Epoch 2] Batch 3744, Loss 0.2921340763568878\n",
      "[Training Epoch 2] Batch 3745, Loss 0.25932082533836365\n",
      "[Training Epoch 2] Batch 3746, Loss 0.3006941080093384\n",
      "[Training Epoch 2] Batch 3747, Loss 0.3081265985965729\n",
      "[Training Epoch 2] Batch 3748, Loss 0.2868579030036926\n",
      "[Training Epoch 2] Batch 3749, Loss 0.26840442419052124\n",
      "[Training Epoch 2] Batch 3750, Loss 0.29328393936157227\n",
      "[Training Epoch 2] Batch 3751, Loss 0.2778283357620239\n",
      "[Training Epoch 2] Batch 3752, Loss 0.2756097912788391\n",
      "[Training Epoch 2] Batch 3753, Loss 0.25819024443626404\n",
      "[Training Epoch 2] Batch 3754, Loss 0.26018571853637695\n",
      "[Training Epoch 2] Batch 3755, Loss 0.2817339599132538\n",
      "[Training Epoch 2] Batch 3756, Loss 0.24403345584869385\n",
      "[Training Epoch 2] Batch 3757, Loss 0.25221365690231323\n",
      "[Training Epoch 2] Batch 3758, Loss 0.2793882489204407\n",
      "[Training Epoch 2] Batch 3759, Loss 0.28881680965423584\n",
      "[Training Epoch 2] Batch 3760, Loss 0.2975621819496155\n",
      "[Training Epoch 2] Batch 3761, Loss 0.2618022561073303\n",
      "[Training Epoch 2] Batch 3762, Loss 0.2793229818344116\n",
      "[Training Epoch 2] Batch 3763, Loss 0.28205013275146484\n",
      "[Training Epoch 2] Batch 3764, Loss 0.2878831624984741\n",
      "[Training Epoch 2] Batch 3765, Loss 0.2632156014442444\n",
      "[Training Epoch 2] Batch 3766, Loss 0.28702569007873535\n",
      "[Training Epoch 2] Batch 3767, Loss 0.2619243264198303\n",
      "[Training Epoch 2] Batch 3768, Loss 0.2545311450958252\n",
      "[Training Epoch 2] Batch 3769, Loss 0.2921378016471863\n",
      "[Training Epoch 2] Batch 3770, Loss 0.2896944284439087\n",
      "[Training Epoch 2] Batch 3771, Loss 0.28156018257141113\n",
      "[Training Epoch 2] Batch 3772, Loss 0.2828514575958252\n",
      "[Training Epoch 2] Batch 3773, Loss 0.2934364676475525\n",
      "[Training Epoch 2] Batch 3774, Loss 0.28674665093421936\n",
      "[Training Epoch 2] Batch 3775, Loss 0.26857268810272217\n",
      "[Training Epoch 2] Batch 3776, Loss 0.26219579577445984\n",
      "[Training Epoch 2] Batch 3777, Loss 0.30635178089141846\n",
      "[Training Epoch 2] Batch 3778, Loss 0.2912302613258362\n",
      "[Training Epoch 2] Batch 3779, Loss 0.28689515590667725\n",
      "[Training Epoch 2] Batch 3780, Loss 0.2932353615760803\n",
      "[Training Epoch 2] Batch 3781, Loss 0.30945533514022827\n",
      "[Training Epoch 2] Batch 3782, Loss 0.28003668785095215\n",
      "[Training Epoch 2] Batch 3783, Loss 0.2785888910293579\n",
      "[Training Epoch 2] Batch 3784, Loss 0.3120763301849365\n",
      "[Training Epoch 2] Batch 3785, Loss 0.25138920545578003\n",
      "[Training Epoch 2] Batch 3786, Loss 0.2749878168106079\n",
      "[Training Epoch 2] Batch 3787, Loss 0.2642259895801544\n",
      "[Training Epoch 2] Batch 3788, Loss 0.28191694617271423\n",
      "[Training Epoch 2] Batch 3789, Loss 0.2421029955148697\n",
      "[Training Epoch 2] Batch 3790, Loss 0.2737489938735962\n",
      "[Training Epoch 2] Batch 3791, Loss 0.26467499136924744\n",
      "[Training Epoch 2] Batch 3792, Loss 0.310061514377594\n",
      "[Training Epoch 2] Batch 3793, Loss 0.2655486762523651\n",
      "[Training Epoch 2] Batch 3794, Loss 0.2951503098011017\n",
      "[Training Epoch 2] Batch 3795, Loss 0.27375566959381104\n",
      "[Training Epoch 2] Batch 3796, Loss 0.2706877589225769\n",
      "[Training Epoch 2] Batch 3797, Loss 0.31507861614227295\n",
      "[Training Epoch 2] Batch 3798, Loss 0.2559688091278076\n",
      "[Training Epoch 2] Batch 3799, Loss 0.2518250644207001\n",
      "[Training Epoch 2] Batch 3800, Loss 0.25723063945770264\n",
      "[Training Epoch 2] Batch 3801, Loss 0.2522808313369751\n",
      "[Training Epoch 2] Batch 3802, Loss 0.253911554813385\n",
      "[Training Epoch 2] Batch 3803, Loss 0.27163630723953247\n",
      "[Training Epoch 2] Batch 3804, Loss 0.28941747546195984\n",
      "[Training Epoch 2] Batch 3805, Loss 0.27073514461517334\n",
      "[Training Epoch 2] Batch 3806, Loss 0.24726390838623047\n",
      "[Training Epoch 2] Batch 3807, Loss 0.2776171565055847\n",
      "[Training Epoch 2] Batch 3808, Loss 0.257845938205719\n",
      "[Training Epoch 2] Batch 3809, Loss 0.2659154534339905\n",
      "[Training Epoch 2] Batch 3810, Loss 0.3087931275367737\n",
      "[Training Epoch 2] Batch 3811, Loss 0.27457335591316223\n",
      "[Training Epoch 2] Batch 3812, Loss 0.28094643354415894\n",
      "[Training Epoch 2] Batch 3813, Loss 0.28687572479248047\n",
      "[Training Epoch 2] Batch 3814, Loss 0.29317590594291687\n",
      "[Training Epoch 2] Batch 3815, Loss 0.26623016595840454\n",
      "[Training Epoch 2] Batch 3816, Loss 0.27658361196517944\n",
      "[Training Epoch 2] Batch 3817, Loss 0.2810240089893341\n",
      "[Training Epoch 2] Batch 3818, Loss 0.2936727702617645\n",
      "[Training Epoch 2] Batch 3819, Loss 0.28609511256217957\n",
      "[Training Epoch 2] Batch 3820, Loss 0.25513342022895813\n",
      "[Training Epoch 2] Batch 3821, Loss 0.28092142939567566\n",
      "[Training Epoch 2] Batch 3822, Loss 0.29685044288635254\n",
      "[Training Epoch 2] Batch 3823, Loss 0.2824617028236389\n",
      "[Training Epoch 2] Batch 3824, Loss 0.27725058794021606\n",
      "[Training Epoch 2] Batch 3825, Loss 0.26961639523506165\n",
      "[Training Epoch 2] Batch 3826, Loss 0.25671184062957764\n",
      "[Training Epoch 2] Batch 3827, Loss 0.24655145406723022\n",
      "[Training Epoch 2] Batch 3828, Loss 0.25733059644699097\n",
      "[Training Epoch 2] Batch 3829, Loss 0.2718547284603119\n",
      "[Training Epoch 2] Batch 3830, Loss 0.2623292803764343\n",
      "[Training Epoch 2] Batch 3831, Loss 0.2815019190311432\n",
      "[Training Epoch 2] Batch 3832, Loss 0.2564423084259033\n",
      "[Training Epoch 2] Batch 3833, Loss 0.2726948857307434\n",
      "[Training Epoch 2] Batch 3834, Loss 0.2816166579723358\n",
      "[Training Epoch 2] Batch 3835, Loss 0.2641928195953369\n",
      "[Training Epoch 2] Batch 3836, Loss 0.2642778754234314\n",
      "[Training Epoch 2] Batch 3837, Loss 0.2972962260246277\n",
      "[Training Epoch 2] Batch 3838, Loss 0.2992904782295227\n",
      "[Training Epoch 2] Batch 3839, Loss 0.27843159437179565\n",
      "[Training Epoch 2] Batch 3840, Loss 0.2478444129228592\n",
      "[Training Epoch 2] Batch 3841, Loss 0.2372252643108368\n",
      "[Training Epoch 2] Batch 3842, Loss 0.2911851406097412\n",
      "[Training Epoch 2] Batch 3843, Loss 0.25833860039711\n",
      "[Training Epoch 2] Batch 3844, Loss 0.302668958902359\n",
      "[Training Epoch 2] Batch 3845, Loss 0.2859245538711548\n",
      "[Training Epoch 2] Batch 3846, Loss 0.27241164445877075\n",
      "[Training Epoch 2] Batch 3847, Loss 0.27605944871902466\n",
      "[Training Epoch 2] Batch 3848, Loss 0.2808911204338074\n",
      "[Training Epoch 2] Batch 3849, Loss 0.27541404962539673\n",
      "[Training Epoch 2] Batch 3850, Loss 0.25003811717033386\n",
      "[Training Epoch 2] Batch 3851, Loss 0.2800787091255188\n",
      "[Training Epoch 2] Batch 3852, Loss 0.30119943618774414\n",
      "[Training Epoch 2] Batch 3853, Loss 0.2826417088508606\n",
      "[Training Epoch 2] Batch 3854, Loss 0.2724646329879761\n",
      "[Training Epoch 2] Batch 3855, Loss 0.2881932258605957\n",
      "[Training Epoch 2] Batch 3856, Loss 0.2900734841823578\n",
      "[Training Epoch 2] Batch 3857, Loss 0.2834555208683014\n",
      "[Training Epoch 2] Batch 3858, Loss 0.29416972398757935\n",
      "[Training Epoch 2] Batch 3859, Loss 0.25896456837654114\n",
      "[Training Epoch 2] Batch 3860, Loss 0.30158066749572754\n",
      "[Training Epoch 2] Batch 3861, Loss 0.28333956003189087\n",
      "[Training Epoch 2] Batch 3862, Loss 0.27770495414733887\n",
      "[Training Epoch 2] Batch 3863, Loss 0.2673231065273285\n",
      "[Training Epoch 2] Batch 3864, Loss 0.2703705132007599\n",
      "[Training Epoch 2] Batch 3865, Loss 0.2788386344909668\n",
      "[Training Epoch 2] Batch 3866, Loss 0.278242290019989\n",
      "[Training Epoch 2] Batch 3867, Loss 0.26412907242774963\n",
      "[Training Epoch 2] Batch 3868, Loss 0.2838834524154663\n",
      "[Training Epoch 2] Batch 3869, Loss 0.28817474842071533\n",
      "[Training Epoch 2] Batch 3870, Loss 0.27080973982810974\n",
      "[Training Epoch 2] Batch 3871, Loss 0.27017083764076233\n",
      "[Training Epoch 2] Batch 3872, Loss 0.27425873279571533\n",
      "[Training Epoch 2] Batch 3873, Loss 0.27565664052963257\n",
      "[Training Epoch 2] Batch 3874, Loss 0.2704228162765503\n",
      "[Training Epoch 2] Batch 3875, Loss 0.2649112343788147\n",
      "[Training Epoch 2] Batch 3876, Loss 0.29458028078079224\n",
      "[Training Epoch 2] Batch 3877, Loss 0.2939763069152832\n",
      "[Training Epoch 2] Batch 3878, Loss 0.29935789108276367\n",
      "[Training Epoch 2] Batch 3879, Loss 0.2624029517173767\n",
      "[Training Epoch 2] Batch 3880, Loss 0.2945554256439209\n",
      "[Training Epoch 2] Batch 3881, Loss 0.24591991305351257\n",
      "[Training Epoch 2] Batch 3882, Loss 0.2779839038848877\n",
      "[Training Epoch 2] Batch 3883, Loss 0.27465951442718506\n",
      "[Training Epoch 2] Batch 3884, Loss 0.24565720558166504\n",
      "[Training Epoch 2] Batch 3885, Loss 0.2988808751106262\n",
      "[Training Epoch 2] Batch 3886, Loss 0.27631282806396484\n",
      "[Training Epoch 2] Batch 3887, Loss 0.2709762454032898\n",
      "[Training Epoch 2] Batch 3888, Loss 0.26813262701034546\n",
      "[Training Epoch 2] Batch 3889, Loss 0.27174103260040283\n",
      "[Training Epoch 2] Batch 3890, Loss 0.26581117510795593\n",
      "[Training Epoch 2] Batch 3891, Loss 0.27149444818496704\n",
      "[Training Epoch 2] Batch 3892, Loss 0.2550541162490845\n",
      "[Training Epoch 2] Batch 3893, Loss 0.2580372989177704\n",
      "[Training Epoch 2] Batch 3894, Loss 0.2930256724357605\n",
      "[Training Epoch 2] Batch 3895, Loss 0.25809359550476074\n",
      "[Training Epoch 2] Batch 3896, Loss 0.2938387989997864\n",
      "[Training Epoch 2] Batch 3897, Loss 0.25392425060272217\n",
      "[Training Epoch 2] Batch 3898, Loss 0.3005409240722656\n",
      "[Training Epoch 2] Batch 3899, Loss 0.2461307793855667\n",
      "[Training Epoch 2] Batch 3900, Loss 0.25311559438705444\n",
      "[Training Epoch 2] Batch 3901, Loss 0.29112082719802856\n",
      "[Training Epoch 2] Batch 3902, Loss 0.2530180513858795\n",
      "[Training Epoch 2] Batch 3903, Loss 0.264453649520874\n",
      "[Training Epoch 2] Batch 3904, Loss 0.27637583017349243\n",
      "[Training Epoch 2] Batch 3905, Loss 0.29210203886032104\n",
      "[Training Epoch 2] Batch 3906, Loss 0.30821675062179565\n",
      "[Training Epoch 2] Batch 3907, Loss 0.2639840245246887\n",
      "[Training Epoch 2] Batch 3908, Loss 0.2866326570510864\n",
      "[Training Epoch 2] Batch 3909, Loss 0.30123722553253174\n",
      "[Training Epoch 2] Batch 3910, Loss 0.25815248489379883\n",
      "[Training Epoch 2] Batch 3911, Loss 0.24691103398799896\n",
      "[Training Epoch 2] Batch 3912, Loss 0.2898954749107361\n",
      "[Training Epoch 2] Batch 3913, Loss 0.2755606770515442\n",
      "[Training Epoch 2] Batch 3914, Loss 0.3036482036113739\n",
      "[Training Epoch 2] Batch 3915, Loss 0.2690012753009796\n",
      "[Training Epoch 2] Batch 3916, Loss 0.2626212537288666\n",
      "[Training Epoch 2] Batch 3917, Loss 0.2746386229991913\n",
      "[Training Epoch 2] Batch 3918, Loss 0.28659534454345703\n",
      "[Training Epoch 2] Batch 3919, Loss 0.25690847635269165\n",
      "[Training Epoch 2] Batch 3920, Loss 0.2896309494972229\n",
      "[Training Epoch 2] Batch 3921, Loss 0.28149157762527466\n",
      "[Training Epoch 2] Batch 3922, Loss 0.29975882172584534\n",
      "[Training Epoch 2] Batch 3923, Loss 0.27286285161972046\n",
      "[Training Epoch 2] Batch 3924, Loss 0.263248085975647\n",
      "[Training Epoch 2] Batch 3925, Loss 0.292800635099411\n",
      "[Training Epoch 2] Batch 3926, Loss 0.2747330069541931\n",
      "[Training Epoch 2] Batch 3927, Loss 0.2606036365032196\n",
      "[Training Epoch 2] Batch 3928, Loss 0.280733585357666\n",
      "[Training Epoch 2] Batch 3929, Loss 0.30248984694480896\n",
      "[Training Epoch 2] Batch 3930, Loss 0.26298195123672485\n",
      "[Training Epoch 2] Batch 3931, Loss 0.2660331130027771\n",
      "[Training Epoch 2] Batch 3932, Loss 0.25024962425231934\n",
      "[Training Epoch 2] Batch 3933, Loss 0.27447688579559326\n",
      "[Training Epoch 2] Batch 3934, Loss 0.26042598485946655\n",
      "[Training Epoch 2] Batch 3935, Loss 0.2691306173801422\n",
      "[Training Epoch 2] Batch 3936, Loss 0.30119970440864563\n",
      "[Training Epoch 2] Batch 3937, Loss 0.2714625895023346\n",
      "[Training Epoch 2] Batch 3938, Loss 0.2617700695991516\n",
      "[Training Epoch 2] Batch 3939, Loss 0.2698154151439667\n",
      "[Training Epoch 2] Batch 3940, Loss 0.27294057607650757\n",
      "[Training Epoch 2] Batch 3941, Loss 0.28214767575263977\n",
      "[Training Epoch 2] Batch 3942, Loss 0.27135786414146423\n",
      "[Training Epoch 2] Batch 3943, Loss 0.29528242349624634\n",
      "[Training Epoch 2] Batch 3944, Loss 0.2873440384864807\n",
      "[Training Epoch 2] Batch 3945, Loss 0.281838059425354\n",
      "[Training Epoch 2] Batch 3946, Loss 0.29078346490859985\n",
      "[Training Epoch 2] Batch 3947, Loss 0.2787849009037018\n",
      "[Training Epoch 2] Batch 3948, Loss 0.2674586772918701\n",
      "[Training Epoch 2] Batch 3949, Loss 0.27198535203933716\n",
      "[Training Epoch 2] Batch 3950, Loss 0.2644205093383789\n",
      "[Training Epoch 2] Batch 3951, Loss 0.31032830476760864\n",
      "[Training Epoch 2] Batch 3952, Loss 0.2795802056789398\n",
      "[Training Epoch 2] Batch 3953, Loss 0.29268813133239746\n",
      "[Training Epoch 2] Batch 3954, Loss 0.2901856303215027\n",
      "[Training Epoch 2] Batch 3955, Loss 0.25756144523620605\n",
      "[Training Epoch 2] Batch 3956, Loss 0.29449141025543213\n",
      "[Training Epoch 2] Batch 3957, Loss 0.24088339507579803\n",
      "[Training Epoch 2] Batch 3958, Loss 0.2809147834777832\n",
      "[Training Epoch 2] Batch 3959, Loss 0.2580713629722595\n",
      "[Training Epoch 2] Batch 3960, Loss 0.2531543970108032\n",
      "[Training Epoch 2] Batch 3961, Loss 0.2920167148113251\n",
      "[Training Epoch 2] Batch 3962, Loss 0.28385666012763977\n",
      "[Training Epoch 2] Batch 3963, Loss 0.2579105496406555\n",
      "[Training Epoch 2] Batch 3964, Loss 0.27517685294151306\n",
      "[Training Epoch 2] Batch 3965, Loss 0.29546475410461426\n",
      "[Training Epoch 2] Batch 3966, Loss 0.23660162091255188\n",
      "[Training Epoch 2] Batch 3967, Loss 0.28565502166748047\n",
      "[Training Epoch 2] Batch 3968, Loss 0.2635096311569214\n",
      "[Training Epoch 2] Batch 3969, Loss 0.2626342177391052\n",
      "[Training Epoch 2] Batch 3970, Loss 0.2614182233810425\n",
      "[Training Epoch 2] Batch 3971, Loss 0.2856954038143158\n",
      "[Training Epoch 2] Batch 3972, Loss 0.27979323267936707\n",
      "[Training Epoch 2] Batch 3973, Loss 0.2873769998550415\n",
      "[Training Epoch 2] Batch 3974, Loss 0.28388798236846924\n",
      "[Training Epoch 2] Batch 3975, Loss 0.2521859407424927\n",
      "[Training Epoch 2] Batch 3976, Loss 0.2867719531059265\n",
      "[Training Epoch 2] Batch 3977, Loss 0.2738224267959595\n",
      "[Training Epoch 2] Batch 3978, Loss 0.2835735082626343\n",
      "[Training Epoch 2] Batch 3979, Loss 0.2848511338233948\n",
      "[Training Epoch 2] Batch 3980, Loss 0.31392520666122437\n",
      "[Training Epoch 2] Batch 3981, Loss 0.2607318162918091\n",
      "[Training Epoch 2] Batch 3982, Loss 0.2618124783039093\n",
      "[Training Epoch 2] Batch 3983, Loss 0.270119309425354\n",
      "[Training Epoch 2] Batch 3984, Loss 0.26259779930114746\n",
      "[Training Epoch 2] Batch 3985, Loss 0.2720921039581299\n",
      "[Training Epoch 2] Batch 3986, Loss 0.27131620049476624\n",
      "[Training Epoch 2] Batch 3987, Loss 0.27538180351257324\n",
      "[Training Epoch 2] Batch 3988, Loss 0.29180610179901123\n",
      "[Training Epoch 2] Batch 3989, Loss 0.2987329363822937\n",
      "[Training Epoch 2] Batch 3990, Loss 0.30499228835105896\n",
      "[Training Epoch 2] Batch 3991, Loss 0.26429107785224915\n",
      "[Training Epoch 2] Batch 3992, Loss 0.2843743860721588\n",
      "[Training Epoch 2] Batch 3993, Loss 0.26672735810279846\n",
      "[Training Epoch 2] Batch 3994, Loss 0.2846657931804657\n",
      "[Training Epoch 2] Batch 3995, Loss 0.29218459129333496\n",
      "[Training Epoch 2] Batch 3996, Loss 0.2645196318626404\n",
      "[Training Epoch 2] Batch 3997, Loss 0.26870495080947876\n",
      "[Training Epoch 2] Batch 3998, Loss 0.28798264265060425\n",
      "[Training Epoch 2] Batch 3999, Loss 0.27595627307891846\n",
      "[Training Epoch 2] Batch 4000, Loss 0.2784977853298187\n",
      "[Training Epoch 2] Batch 4001, Loss 0.26954013109207153\n",
      "[Training Epoch 2] Batch 4002, Loss 0.29417943954467773\n",
      "[Training Epoch 2] Batch 4003, Loss 0.2771279811859131\n",
      "[Training Epoch 2] Batch 4004, Loss 0.2621879577636719\n",
      "[Training Epoch 2] Batch 4005, Loss 0.28714796900749207\n",
      "[Training Epoch 2] Batch 4006, Loss 0.2555956244468689\n",
      "[Training Epoch 2] Batch 4007, Loss 0.24008159339427948\n",
      "[Training Epoch 2] Batch 4008, Loss 0.2900550365447998\n",
      "[Training Epoch 2] Batch 4009, Loss 0.3054826855659485\n",
      "[Training Epoch 2] Batch 4010, Loss 0.28700315952301025\n",
      "[Training Epoch 2] Batch 4011, Loss 0.3059902787208557\n",
      "[Training Epoch 2] Batch 4012, Loss 0.2801263928413391\n",
      "[Training Epoch 2] Batch 4013, Loss 0.26560285687446594\n",
      "[Training Epoch 2] Batch 4014, Loss 0.2880030572414398\n",
      "[Training Epoch 2] Batch 4015, Loss 0.2796701192855835\n",
      "[Training Epoch 2] Batch 4016, Loss 0.2713935375213623\n",
      "[Training Epoch 2] Batch 4017, Loss 0.2941865622997284\n",
      "[Training Epoch 2] Batch 4018, Loss 0.2891855239868164\n",
      "[Training Epoch 2] Batch 4019, Loss 0.27714803814888\n",
      "[Training Epoch 2] Batch 4020, Loss 0.2791561484336853\n",
      "[Training Epoch 2] Batch 4021, Loss 0.29073548316955566\n",
      "[Training Epoch 2] Batch 4022, Loss 0.2606094479560852\n",
      "[Training Epoch 2] Batch 4023, Loss 0.32934683561325073\n",
      "[Training Epoch 2] Batch 4024, Loss 0.2777066230773926\n",
      "[Training Epoch 2] Batch 4025, Loss 0.27664464712142944\n",
      "[Training Epoch 2] Batch 4026, Loss 0.3065858781337738\n",
      "[Training Epoch 2] Batch 4027, Loss 0.26527148485183716\n",
      "[Training Epoch 2] Batch 4028, Loss 0.2586362957954407\n",
      "[Training Epoch 2] Batch 4029, Loss 0.2795039415359497\n",
      "[Training Epoch 2] Batch 4030, Loss 0.30303755402565\n",
      "[Training Epoch 2] Batch 4031, Loss 0.2853161096572876\n",
      "[Training Epoch 2] Batch 4032, Loss 0.26993054151535034\n",
      "[Training Epoch 2] Batch 4033, Loss 0.2856189012527466\n",
      "[Training Epoch 2] Batch 4034, Loss 0.26084980368614197\n",
      "[Training Epoch 2] Batch 4035, Loss 0.25900667905807495\n",
      "[Training Epoch 2] Batch 4036, Loss 0.25685322284698486\n",
      "[Training Epoch 2] Batch 4037, Loss 0.30691689252853394\n",
      "[Training Epoch 2] Batch 4038, Loss 0.2809956967830658\n",
      "[Training Epoch 2] Batch 4039, Loss 0.2439246028661728\n",
      "[Training Epoch 2] Batch 4040, Loss 0.29448455572128296\n",
      "[Training Epoch 2] Batch 4041, Loss 0.26195767521858215\n",
      "[Training Epoch 2] Batch 4042, Loss 0.29557979106903076\n",
      "[Training Epoch 2] Batch 4043, Loss 0.30688774585723877\n",
      "[Training Epoch 2] Batch 4044, Loss 0.30679047107696533\n",
      "[Training Epoch 2] Batch 4045, Loss 0.2768455445766449\n",
      "[Training Epoch 2] Batch 4046, Loss 0.2865793704986572\n",
      "[Training Epoch 2] Batch 4047, Loss 0.25384652614593506\n",
      "[Training Epoch 2] Batch 4048, Loss 0.2751155197620392\n",
      "[Training Epoch 2] Batch 4049, Loss 0.2821522355079651\n",
      "[Training Epoch 2] Batch 4050, Loss 0.27013736963272095\n",
      "[Training Epoch 2] Batch 4051, Loss 0.25827014446258545\n",
      "[Training Epoch 2] Batch 4052, Loss 0.30064406991004944\n",
      "[Training Epoch 2] Batch 4053, Loss 0.2831437289714813\n",
      "[Training Epoch 2] Batch 4054, Loss 0.261671781539917\n",
      "[Training Epoch 2] Batch 4055, Loss 0.26005470752716064\n",
      "[Training Epoch 2] Batch 4056, Loss 0.27839040756225586\n",
      "[Training Epoch 2] Batch 4057, Loss 0.25684934854507446\n",
      "[Training Epoch 2] Batch 4058, Loss 0.2782069444656372\n",
      "[Training Epoch 2] Batch 4059, Loss 0.29102152585983276\n",
      "[Training Epoch 2] Batch 4060, Loss 0.294343501329422\n",
      "[Training Epoch 2] Batch 4061, Loss 0.28581446409225464\n",
      "[Training Epoch 2] Batch 4062, Loss 0.29605230689048767\n",
      "[Training Epoch 2] Batch 4063, Loss 0.26217228174209595\n",
      "[Training Epoch 2] Batch 4064, Loss 0.2836386561393738\n",
      "[Training Epoch 2] Batch 4065, Loss 0.2970205545425415\n",
      "[Training Epoch 2] Batch 4066, Loss 0.3026251792907715\n",
      "[Training Epoch 2] Batch 4067, Loss 0.2880520522594452\n",
      "[Training Epoch 2] Batch 4068, Loss 0.26409000158309937\n",
      "[Training Epoch 2] Batch 4069, Loss 0.25922179222106934\n",
      "[Training Epoch 2] Batch 4070, Loss 0.2917868494987488\n",
      "[Training Epoch 2] Batch 4071, Loss 0.290012001991272\n",
      "[Training Epoch 2] Batch 4072, Loss 0.2743836045265198\n",
      "[Training Epoch 2] Batch 4073, Loss 0.2804133892059326\n",
      "[Training Epoch 2] Batch 4074, Loss 0.2575799822807312\n",
      "[Training Epoch 2] Batch 4075, Loss 0.29296964406967163\n",
      "[Training Epoch 2] Batch 4076, Loss 0.2936115264892578\n",
      "[Training Epoch 2] Batch 4077, Loss 0.2660667300224304\n",
      "[Training Epoch 2] Batch 4078, Loss 0.2872222661972046\n",
      "[Training Epoch 2] Batch 4079, Loss 0.26211869716644287\n",
      "[Training Epoch 2] Batch 4080, Loss 0.2578718662261963\n",
      "[Training Epoch 2] Batch 4081, Loss 0.26361769437789917\n",
      "[Training Epoch 2] Batch 4082, Loss 0.2690150737762451\n",
      "[Training Epoch 2] Batch 4083, Loss 0.28377610445022583\n",
      "[Training Epoch 2] Batch 4084, Loss 0.2548752427101135\n",
      "[Training Epoch 2] Batch 4085, Loss 0.2803386449813843\n",
      "[Training Epoch 2] Batch 4086, Loss 0.2566283643245697\n",
      "[Training Epoch 2] Batch 4087, Loss 0.2922004163265228\n",
      "[Training Epoch 2] Batch 4088, Loss 0.2631937861442566\n",
      "[Training Epoch 2] Batch 4089, Loss 0.24754486978054047\n",
      "[Training Epoch 2] Batch 4090, Loss 0.2787317633628845\n",
      "[Training Epoch 2] Batch 4091, Loss 0.2858882546424866\n",
      "[Training Epoch 2] Batch 4092, Loss 0.2534436583518982\n",
      "[Training Epoch 2] Batch 4093, Loss 0.2847692370414734\n",
      "[Training Epoch 2] Batch 4094, Loss 0.29139411449432373\n",
      "[Training Epoch 2] Batch 4095, Loss 0.28617197275161743\n",
      "[Training Epoch 2] Batch 4096, Loss 0.2572382688522339\n",
      "[Training Epoch 2] Batch 4097, Loss 0.274519681930542\n",
      "[Training Epoch 2] Batch 4098, Loss 0.27813324332237244\n",
      "[Training Epoch 2] Batch 4099, Loss 0.2775309085845947\n",
      "[Training Epoch 2] Batch 4100, Loss 0.26968124508857727\n",
      "[Training Epoch 2] Batch 4101, Loss 0.2653159499168396\n",
      "[Training Epoch 2] Batch 4102, Loss 0.2596392333507538\n",
      "[Training Epoch 2] Batch 4103, Loss 0.26644763350486755\n",
      "[Training Epoch 2] Batch 4104, Loss 0.2821848392486572\n",
      "[Training Epoch 2] Batch 4105, Loss 0.2655858099460602\n",
      "[Training Epoch 2] Batch 4106, Loss 0.32059311866760254\n",
      "[Training Epoch 2] Batch 4107, Loss 0.28132158517837524\n",
      "[Training Epoch 2] Batch 4108, Loss 0.27351993322372437\n",
      "[Training Epoch 2] Batch 4109, Loss 0.2618454098701477\n",
      "[Training Epoch 2] Batch 4110, Loss 0.3077646493911743\n",
      "[Training Epoch 2] Batch 4111, Loss 0.30092373490333557\n",
      "[Training Epoch 2] Batch 4112, Loss 0.2704942524433136\n",
      "[Training Epoch 2] Batch 4113, Loss 0.2779786288738251\n",
      "[Training Epoch 2] Batch 4114, Loss 0.2735897898674011\n",
      "[Training Epoch 2] Batch 4115, Loss 0.2836417257785797\n",
      "[Training Epoch 2] Batch 4116, Loss 0.2579704225063324\n",
      "[Training Epoch 2] Batch 4117, Loss 0.2669486999511719\n",
      "[Training Epoch 2] Batch 4118, Loss 0.2887601852416992\n",
      "[Training Epoch 2] Batch 4119, Loss 0.2841736078262329\n",
      "[Training Epoch 2] Batch 4120, Loss 0.2608111500740051\n",
      "[Training Epoch 2] Batch 4121, Loss 0.24919968843460083\n",
      "[Training Epoch 2] Batch 4122, Loss 0.30994805693626404\n",
      "[Training Epoch 2] Batch 4123, Loss 0.2814435362815857\n",
      "[Training Epoch 2] Batch 4124, Loss 0.2833186686038971\n",
      "[Training Epoch 2] Batch 4125, Loss 0.27195706963539124\n",
      "[Training Epoch 2] Batch 4126, Loss 0.27087292075157166\n",
      "[Training Epoch 2] Batch 4127, Loss 0.26701438426971436\n",
      "[Training Epoch 2] Batch 4128, Loss 0.26924222707748413\n",
      "[Training Epoch 2] Batch 4129, Loss 0.2996959090232849\n",
      "[Training Epoch 2] Batch 4130, Loss 0.2641696333885193\n",
      "[Training Epoch 2] Batch 4131, Loss 0.2785778045654297\n",
      "[Training Epoch 2] Batch 4132, Loss 0.23555706441402435\n",
      "[Training Epoch 2] Batch 4133, Loss 0.2677968442440033\n",
      "[Training Epoch 2] Batch 4134, Loss 0.26508986949920654\n",
      "[Training Epoch 2] Batch 4135, Loss 0.26690420508384705\n",
      "[Training Epoch 2] Batch 4136, Loss 0.29666829109191895\n",
      "[Training Epoch 2] Batch 4137, Loss 0.2891266942024231\n",
      "[Training Epoch 2] Batch 4138, Loss 0.31102848052978516\n",
      "[Training Epoch 2] Batch 4139, Loss 0.29005172848701477\n",
      "[Training Epoch 2] Batch 4140, Loss 0.2528836131095886\n",
      "[Training Epoch 2] Batch 4141, Loss 0.26781803369522095\n",
      "[Training Epoch 2] Batch 4142, Loss 0.252705842256546\n",
      "[Training Epoch 2] Batch 4143, Loss 0.24911946058273315\n",
      "[Training Epoch 2] Batch 4144, Loss 0.27983009815216064\n",
      "[Training Epoch 2] Batch 4145, Loss 0.25750336050987244\n",
      "[Training Epoch 2] Batch 4146, Loss 0.2635374069213867\n",
      "[Training Epoch 2] Batch 4147, Loss 0.28053733706474304\n",
      "[Training Epoch 2] Batch 4148, Loss 0.2689589262008667\n",
      "[Training Epoch 2] Batch 4149, Loss 0.28617680072784424\n",
      "[Training Epoch 2] Batch 4150, Loss 0.25804150104522705\n",
      "[Training Epoch 2] Batch 4151, Loss 0.245907261967659\n",
      "[Training Epoch 2] Batch 4152, Loss 0.2600751519203186\n",
      "[Training Epoch 2] Batch 4153, Loss 0.26945197582244873\n",
      "[Training Epoch 2] Batch 4154, Loss 0.2899020314216614\n",
      "[Training Epoch 2] Batch 4155, Loss 0.28273385763168335\n",
      "[Training Epoch 2] Batch 4156, Loss 0.2855881452560425\n",
      "[Training Epoch 2] Batch 4157, Loss 0.2734472155570984\n",
      "[Training Epoch 2] Batch 4158, Loss 0.28946638107299805\n",
      "[Training Epoch 2] Batch 4159, Loss 0.26937544345855713\n",
      "[Training Epoch 2] Batch 4160, Loss 0.27346330881118774\n",
      "[Training Epoch 2] Batch 4161, Loss 0.26043012738227844\n",
      "[Training Epoch 2] Batch 4162, Loss 0.2778341472148895\n",
      "[Training Epoch 2] Batch 4163, Loss 0.27203965187072754\n",
      "[Training Epoch 2] Batch 4164, Loss 0.2995373606681824\n",
      "[Training Epoch 2] Batch 4165, Loss 0.2584904730319977\n",
      "[Training Epoch 2] Batch 4166, Loss 0.27181440591812134\n",
      "[Training Epoch 2] Batch 4167, Loss 0.2437877655029297\n",
      "[Training Epoch 2] Batch 4168, Loss 0.26002514362335205\n",
      "[Training Epoch 2] Batch 4169, Loss 0.23283600807189941\n",
      "[Training Epoch 2] Batch 4170, Loss 0.27158689498901367\n",
      "[Training Epoch 2] Batch 4171, Loss 0.28165215253829956\n",
      "[Training Epoch 2] Batch 4172, Loss 0.27394378185272217\n",
      "[Training Epoch 2] Batch 4173, Loss 0.2696666121482849\n",
      "[Training Epoch 2] Batch 4174, Loss 0.28440457582473755\n",
      "[Training Epoch 2] Batch 4175, Loss 0.23971375823020935\n",
      "[Training Epoch 2] Batch 4176, Loss 0.26988619565963745\n",
      "[Training Epoch 2] Batch 4177, Loss 0.28134632110595703\n",
      "[Training Epoch 2] Batch 4178, Loss 0.2803296446800232\n",
      "[Training Epoch 2] Batch 4179, Loss 0.26940038800239563\n",
      "[Training Epoch 2] Batch 4180, Loss 0.26838183403015137\n",
      "[Training Epoch 2] Batch 4181, Loss 0.2788556218147278\n",
      "[Training Epoch 2] Batch 4182, Loss 0.276788592338562\n",
      "[Training Epoch 2] Batch 4183, Loss 0.2778330445289612\n",
      "[Training Epoch 2] Batch 4184, Loss 0.2700827717781067\n",
      "[Training Epoch 2] Batch 4185, Loss 0.25956398248672485\n",
      "[Training Epoch 2] Batch 4186, Loss 0.26412075757980347\n",
      "[Training Epoch 2] Batch 4187, Loss 0.2693396508693695\n",
      "[Training Epoch 2] Batch 4188, Loss 0.2957698106765747\n",
      "[Training Epoch 2] Batch 4189, Loss 0.28442007303237915\n",
      "[Training Epoch 2] Batch 4190, Loss 0.2768239676952362\n",
      "[Training Epoch 2] Batch 4191, Loss 0.2793924808502197\n",
      "[Training Epoch 2] Batch 4192, Loss 0.2851581275463104\n",
      "[Training Epoch 2] Batch 4193, Loss 0.2687157392501831\n",
      "[Training Epoch 2] Batch 4194, Loss 0.2794399857521057\n",
      "[Training Epoch 2] Batch 4195, Loss 0.30020710825920105\n",
      "[Training Epoch 2] Batch 4196, Loss 0.2645873725414276\n",
      "[Training Epoch 2] Batch 4197, Loss 0.3072587549686432\n",
      "[Training Epoch 2] Batch 4198, Loss 0.2943214178085327\n",
      "[Training Epoch 2] Batch 4199, Loss 0.2772766351699829\n",
      "[Training Epoch 2] Batch 4200, Loss 0.27554547786712646\n",
      "[Training Epoch 2] Batch 4201, Loss 0.2631527781486511\n",
      "[Training Epoch 2] Batch 4202, Loss 0.2636362910270691\n",
      "[Training Epoch 2] Batch 4203, Loss 0.2753854990005493\n",
      "[Training Epoch 2] Batch 4204, Loss 0.2604360580444336\n",
      "[Training Epoch 2] Batch 4205, Loss 0.28647711873054504\n",
      "[Training Epoch 2] Batch 4206, Loss 0.28577274084091187\n",
      "[Training Epoch 2] Batch 4207, Loss 0.2629324793815613\n",
      "[Training Epoch 2] Batch 4208, Loss 0.282440185546875\n",
      "[Training Epoch 2] Batch 4209, Loss 0.2317720353603363\n",
      "[Training Epoch 2] Batch 4210, Loss 0.26284071803092957\n",
      "[Training Epoch 2] Batch 4211, Loss 0.2815983295440674\n",
      "[Training Epoch 2] Batch 4212, Loss 0.2804281711578369\n",
      "[Training Epoch 2] Batch 4213, Loss 0.29906994104385376\n",
      "[Training Epoch 2] Batch 4214, Loss 0.29396796226501465\n",
      "[Training Epoch 2] Batch 4215, Loss 0.30592048168182373\n",
      "[Training Epoch 2] Batch 4216, Loss 0.2783701419830322\n",
      "[Training Epoch 2] Batch 4217, Loss 0.29926446080207825\n",
      "[Training Epoch 2] Batch 4218, Loss 0.25648581981658936\n",
      "[Training Epoch 2] Batch 4219, Loss 0.29452890157699585\n",
      "[Training Epoch 2] Batch 4220, Loss 0.2577216327190399\n",
      "[Training Epoch 2] Batch 4221, Loss 0.27748408913612366\n",
      "[Training Epoch 2] Batch 4222, Loss 0.24465924501419067\n",
      "[Training Epoch 2] Batch 4223, Loss 0.2779330313205719\n",
      "[Training Epoch 2] Batch 4224, Loss 0.28094303607940674\n",
      "[Training Epoch 2] Batch 4225, Loss 0.29292741417884827\n",
      "[Training Epoch 2] Batch 4226, Loss 0.30629539489746094\n",
      "[Training Epoch 2] Batch 4227, Loss 0.2652483582496643\n",
      "[Training Epoch 2] Batch 4228, Loss 0.2632625102996826\n",
      "[Training Epoch 2] Batch 4229, Loss 0.28908732533454895\n",
      "[Training Epoch 2] Batch 4230, Loss 0.264198899269104\n",
      "[Training Epoch 2] Batch 4231, Loss 0.273052453994751\n",
      "[Training Epoch 2] Batch 4232, Loss 0.2919711768627167\n",
      "[Training Epoch 2] Batch 4233, Loss 0.2821713089942932\n",
      "[Training Epoch 2] Batch 4234, Loss 0.2816435694694519\n",
      "[Training Epoch 2] Batch 4235, Loss 0.26191699504852295\n",
      "[Training Epoch 2] Batch 4236, Loss 0.3002440631389618\n",
      "[Training Epoch 2] Batch 4237, Loss 0.26665955781936646\n",
      "[Training Epoch 2] Batch 4238, Loss 0.2654276490211487\n",
      "[Training Epoch 2] Batch 4239, Loss 0.2538025379180908\n",
      "[Training Epoch 2] Batch 4240, Loss 0.253338485956192\n",
      "[Training Epoch 2] Batch 4241, Loss 0.3091333210468292\n",
      "[Training Epoch 2] Batch 4242, Loss 0.28556203842163086\n",
      "[Training Epoch 2] Batch 4243, Loss 0.28164297342300415\n",
      "[Training Epoch 2] Batch 4244, Loss 0.26468759775161743\n",
      "[Training Epoch 2] Batch 4245, Loss 0.3106326460838318\n",
      "[Training Epoch 2] Batch 4246, Loss 0.24877135455608368\n",
      "[Training Epoch 2] Batch 4247, Loss 0.29838842153549194\n",
      "[Training Epoch 2] Batch 4248, Loss 0.281061053276062\n",
      "[Training Epoch 2] Batch 4249, Loss 0.27427035570144653\n",
      "[Training Epoch 2] Batch 4250, Loss 0.28832781314849854\n",
      "[Training Epoch 2] Batch 4251, Loss 0.2878235876560211\n",
      "[Training Epoch 2] Batch 4252, Loss 0.3039955496788025\n",
      "[Training Epoch 2] Batch 4253, Loss 0.27040770649909973\n",
      "[Training Epoch 2] Batch 4254, Loss 0.2775987982749939\n",
      "[Training Epoch 2] Batch 4255, Loss 0.2830222249031067\n",
      "[Training Epoch 2] Batch 4256, Loss 0.2629832625389099\n",
      "[Training Epoch 2] Batch 4257, Loss 0.29465946555137634\n",
      "[Training Epoch 2] Batch 4258, Loss 0.2851853370666504\n",
      "[Training Epoch 2] Batch 4259, Loss 0.29937124252319336\n",
      "[Training Epoch 2] Batch 4260, Loss 0.28154823184013367\n",
      "[Training Epoch 2] Batch 4261, Loss 0.2860775589942932\n",
      "[Training Epoch 2] Batch 4262, Loss 0.2881365418434143\n",
      "[Training Epoch 2] Batch 4263, Loss 0.3020670413970947\n",
      "[Training Epoch 2] Batch 4264, Loss 0.2940285801887512\n",
      "[Training Epoch 2] Batch 4265, Loss 0.24619269371032715\n",
      "[Training Epoch 2] Batch 4266, Loss 0.2798418402671814\n",
      "[Training Epoch 2] Batch 4267, Loss 0.26041659712791443\n",
      "[Training Epoch 2] Batch 4268, Loss 0.27063268423080444\n",
      "[Training Epoch 2] Batch 4269, Loss 0.2959771454334259\n",
      "[Training Epoch 2] Batch 4270, Loss 0.28609350323677063\n",
      "[Training Epoch 2] Batch 4271, Loss 0.26999038457870483\n",
      "[Training Epoch 2] Batch 4272, Loss 0.28047993779182434\n",
      "[Training Epoch 2] Batch 4273, Loss 0.2525602877140045\n",
      "[Training Epoch 2] Batch 4274, Loss 0.29680243134498596\n",
      "[Training Epoch 2] Batch 4275, Loss 0.2873861789703369\n",
      "[Training Epoch 2] Batch 4276, Loss 0.28324970602989197\n",
      "[Training Epoch 2] Batch 4277, Loss 0.2969743609428406\n",
      "[Training Epoch 2] Batch 4278, Loss 0.28725576400756836\n",
      "[Training Epoch 2] Batch 4279, Loss 0.29309216141700745\n",
      "[Training Epoch 2] Batch 4280, Loss 0.28315913677215576\n",
      "[Training Epoch 2] Batch 4281, Loss 0.28578975796699524\n",
      "[Training Epoch 2] Batch 4282, Loss 0.2691698670387268\n",
      "[Training Epoch 2] Batch 4283, Loss 0.2740978002548218\n",
      "[Training Epoch 2] Batch 4284, Loss 0.2708926498889923\n",
      "[Training Epoch 2] Batch 4285, Loss 0.2989617884159088\n",
      "[Training Epoch 2] Batch 4286, Loss 0.2580224275588989\n",
      "[Training Epoch 2] Batch 4287, Loss 0.2746509611606598\n",
      "[Training Epoch 2] Batch 4288, Loss 0.2737158238887787\n",
      "[Training Epoch 2] Batch 4289, Loss 0.2267710566520691\n",
      "[Training Epoch 2] Batch 4290, Loss 0.2862119674682617\n",
      "[Training Epoch 2] Batch 4291, Loss 0.2687472999095917\n",
      "[Training Epoch 2] Batch 4292, Loss 0.2953800559043884\n",
      "[Training Epoch 2] Batch 4293, Loss 0.29109567403793335\n",
      "[Training Epoch 2] Batch 4294, Loss 0.2554331421852112\n",
      "[Training Epoch 2] Batch 4295, Loss 0.29515528678894043\n",
      "[Training Epoch 2] Batch 4296, Loss 0.26265084743499756\n",
      "[Training Epoch 2] Batch 4297, Loss 0.29407280683517456\n",
      "[Training Epoch 2] Batch 4298, Loss 0.2741415202617645\n",
      "[Training Epoch 2] Batch 4299, Loss 0.2763693928718567\n",
      "[Training Epoch 2] Batch 4300, Loss 0.27244114875793457\n",
      "[Training Epoch 2] Batch 4301, Loss 0.24768748879432678\n",
      "[Training Epoch 2] Batch 4302, Loss 0.2665322422981262\n",
      "[Training Epoch 2] Batch 4303, Loss 0.2678402066230774\n",
      "[Training Epoch 2] Batch 4304, Loss 0.29835593700408936\n",
      "[Training Epoch 2] Batch 4305, Loss 0.26969262957572937\n",
      "[Training Epoch 2] Batch 4306, Loss 0.2719736099243164\n",
      "[Training Epoch 2] Batch 4307, Loss 0.26275134086608887\n",
      "[Training Epoch 2] Batch 4308, Loss 0.2903633713722229\n",
      "[Training Epoch 2] Batch 4309, Loss 0.28402769565582275\n",
      "[Training Epoch 2] Batch 4310, Loss 0.3008453845977783\n",
      "[Training Epoch 2] Batch 4311, Loss 0.3151920437812805\n",
      "[Training Epoch 2] Batch 4312, Loss 0.26388829946517944\n",
      "[Training Epoch 2] Batch 4313, Loss 0.2888253331184387\n",
      "[Training Epoch 2] Batch 4314, Loss 0.25615715980529785\n",
      "[Training Epoch 2] Batch 4315, Loss 0.25749462842941284\n",
      "[Training Epoch 2] Batch 4316, Loss 0.27975884079933167\n",
      "[Training Epoch 2] Batch 4317, Loss 0.25334447622299194\n",
      "[Training Epoch 2] Batch 4318, Loss 0.2853086590766907\n",
      "[Training Epoch 2] Batch 4319, Loss 0.26597052812576294\n",
      "[Training Epoch 2] Batch 4320, Loss 0.26421433687210083\n",
      "[Training Epoch 2] Batch 4321, Loss 0.2995109558105469\n",
      "[Training Epoch 2] Batch 4322, Loss 0.2829732596874237\n",
      "[Training Epoch 2] Batch 4323, Loss 0.2658569812774658\n",
      "[Training Epoch 2] Batch 4324, Loss 0.26460570096969604\n",
      "[Training Epoch 2] Batch 4325, Loss 0.2636045217514038\n",
      "[Training Epoch 2] Batch 4326, Loss 0.2795386016368866\n",
      "[Training Epoch 2] Batch 4327, Loss 0.2825668454170227\n",
      "[Training Epoch 2] Batch 4328, Loss 0.297527551651001\n",
      "[Training Epoch 2] Batch 4329, Loss 0.2953281104564667\n",
      "[Training Epoch 2] Batch 4330, Loss 0.29205119609832764\n",
      "[Training Epoch 2] Batch 4331, Loss 0.26262468099594116\n",
      "[Training Epoch 2] Batch 4332, Loss 0.2655988037586212\n",
      "[Training Epoch 2] Batch 4333, Loss 0.2653186321258545\n",
      "[Training Epoch 2] Batch 4334, Loss 0.29061949253082275\n",
      "[Training Epoch 2] Batch 4335, Loss 0.2881244421005249\n",
      "[Training Epoch 2] Batch 4336, Loss 0.28133055567741394\n",
      "[Training Epoch 2] Batch 4337, Loss 0.27295851707458496\n",
      "[Training Epoch 2] Batch 4338, Loss 0.26004964113235474\n",
      "[Training Epoch 2] Batch 4339, Loss 0.2864684462547302\n",
      "[Training Epoch 2] Batch 4340, Loss 0.25785398483276367\n",
      "[Training Epoch 2] Batch 4341, Loss 0.2746320366859436\n",
      "[Training Epoch 2] Batch 4342, Loss 0.24976395070552826\n",
      "[Training Epoch 2] Batch 4343, Loss 0.25108328461647034\n",
      "[Training Epoch 2] Batch 4344, Loss 0.29751601815223694\n",
      "[Training Epoch 2] Batch 4345, Loss 0.2585289478302002\n",
      "[Training Epoch 2] Batch 4346, Loss 0.26954779028892517\n",
      "[Training Epoch 2] Batch 4347, Loss 0.2861388027667999\n",
      "[Training Epoch 2] Batch 4348, Loss 0.2613774538040161\n",
      "[Training Epoch 2] Batch 4349, Loss 0.26441794633865356\n",
      "[Training Epoch 2] Batch 4350, Loss 0.27337557077407837\n",
      "[Training Epoch 2] Batch 4351, Loss 0.27781596779823303\n",
      "[Training Epoch 2] Batch 4352, Loss 0.2734782099723816\n",
      "[Training Epoch 2] Batch 4353, Loss 0.32837748527526855\n",
      "[Training Epoch 2] Batch 4354, Loss 0.28293073177337646\n",
      "[Training Epoch 2] Batch 4355, Loss 0.27687740325927734\n",
      "[Training Epoch 2] Batch 4356, Loss 0.2835918962955475\n",
      "[Training Epoch 2] Batch 4357, Loss 0.28054219484329224\n",
      "[Training Epoch 2] Batch 4358, Loss 0.27748245000839233\n",
      "[Training Epoch 2] Batch 4359, Loss 0.25699448585510254\n",
      "[Training Epoch 2] Batch 4360, Loss 0.2603120505809784\n",
      "[Training Epoch 2] Batch 4361, Loss 0.255219042301178\n",
      "[Training Epoch 2] Batch 4362, Loss 0.2789953947067261\n",
      "[Training Epoch 2] Batch 4363, Loss 0.274295449256897\n",
      "[Training Epoch 2] Batch 4364, Loss 0.2878126800060272\n",
      "[Training Epoch 2] Batch 4365, Loss 0.30547308921813965\n",
      "[Training Epoch 2] Batch 4366, Loss 0.29483476281166077\n",
      "[Training Epoch 2] Batch 4367, Loss 0.28850704431533813\n",
      "[Training Epoch 2] Batch 4368, Loss 0.30571895837783813\n",
      "[Training Epoch 2] Batch 4369, Loss 0.27035677433013916\n",
      "[Training Epoch 2] Batch 4370, Loss 0.300314724445343\n",
      "[Training Epoch 2] Batch 4371, Loss 0.26268309354782104\n",
      "[Training Epoch 2] Batch 4372, Loss 0.27859190106391907\n",
      "[Training Epoch 2] Batch 4373, Loss 0.251705139875412\n",
      "[Training Epoch 2] Batch 4374, Loss 0.25909194350242615\n",
      "[Training Epoch 2] Batch 4375, Loss 0.2715350389480591\n",
      "[Training Epoch 2] Batch 4376, Loss 0.2675468921661377\n",
      "[Training Epoch 2] Batch 4377, Loss 0.2719740569591522\n",
      "[Training Epoch 2] Batch 4378, Loss 0.2663397789001465\n",
      "[Training Epoch 2] Batch 4379, Loss 0.27363014221191406\n",
      "[Training Epoch 2] Batch 4380, Loss 0.28455325961112976\n",
      "[Training Epoch 2] Batch 4381, Loss 0.27696114778518677\n",
      "[Training Epoch 2] Batch 4382, Loss 0.3949224650859833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:05<00:00, 1982.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 2] Precision = 0.2618, Recall = 0.7739\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.25511133670806885\n",
      "[Training Epoch 3] Batch 1, Loss 0.2802758812904358\n",
      "[Training Epoch 3] Batch 2, Loss 0.2823371887207031\n",
      "[Training Epoch 3] Batch 3, Loss 0.2376820147037506\n",
      "[Training Epoch 3] Batch 4, Loss 0.2570372223854065\n",
      "[Training Epoch 3] Batch 5, Loss 0.2648353576660156\n",
      "[Training Epoch 3] Batch 6, Loss 0.25178927183151245\n",
      "[Training Epoch 3] Batch 7, Loss 0.28661486506462097\n",
      "[Training Epoch 3] Batch 8, Loss 0.26621532440185547\n",
      "[Training Epoch 3] Batch 9, Loss 0.27830183506011963\n",
      "[Training Epoch 3] Batch 10, Loss 0.2709539532661438\n",
      "[Training Epoch 3] Batch 11, Loss 0.2533772587776184\n",
      "[Training Epoch 3] Batch 12, Loss 0.24482455849647522\n",
      "[Training Epoch 3] Batch 13, Loss 0.2822796404361725\n",
      "[Training Epoch 3] Batch 14, Loss 0.30497753620147705\n",
      "[Training Epoch 3] Batch 15, Loss 0.29799792170524597\n",
      "[Training Epoch 3] Batch 16, Loss 0.27524253726005554\n",
      "[Training Epoch 3] Batch 17, Loss 0.28262433409690857\n",
      "[Training Epoch 3] Batch 18, Loss 0.2737847566604614\n",
      "[Training Epoch 3] Batch 19, Loss 0.2672795355319977\n",
      "[Training Epoch 3] Batch 20, Loss 0.27785754203796387\n",
      "[Training Epoch 3] Batch 21, Loss 0.24451392889022827\n",
      "[Training Epoch 3] Batch 22, Loss 0.2529165744781494\n",
      "[Training Epoch 3] Batch 23, Loss 0.2490408718585968\n",
      "[Training Epoch 3] Batch 24, Loss 0.2771061360836029\n",
      "[Training Epoch 3] Batch 25, Loss 0.2629238963127136\n",
      "[Training Epoch 3] Batch 26, Loss 0.26043346524238586\n",
      "[Training Epoch 3] Batch 27, Loss 0.2814203202724457\n",
      "[Training Epoch 3] Batch 28, Loss 0.2674422860145569\n",
      "[Training Epoch 3] Batch 29, Loss 0.2811756134033203\n",
      "[Training Epoch 3] Batch 30, Loss 0.2827249765396118\n",
      "[Training Epoch 3] Batch 31, Loss 0.24662968516349792\n",
      "[Training Epoch 3] Batch 32, Loss 0.27328377962112427\n",
      "[Training Epoch 3] Batch 33, Loss 0.2589847147464752\n",
      "[Training Epoch 3] Batch 34, Loss 0.2841923236846924\n",
      "[Training Epoch 3] Batch 35, Loss 0.28975680470466614\n",
      "[Training Epoch 3] Batch 36, Loss 0.2786475419998169\n",
      "[Training Epoch 3] Batch 37, Loss 0.2584458589553833\n",
      "[Training Epoch 3] Batch 38, Loss 0.27078723907470703\n",
      "[Training Epoch 3] Batch 39, Loss 0.2692220211029053\n",
      "[Training Epoch 3] Batch 40, Loss 0.28947770595550537\n",
      "[Training Epoch 3] Batch 41, Loss 0.2738649845123291\n",
      "[Training Epoch 3] Batch 42, Loss 0.2875119149684906\n",
      "[Training Epoch 3] Batch 43, Loss 0.27149698138237\n",
      "[Training Epoch 3] Batch 44, Loss 0.2649751603603363\n",
      "[Training Epoch 3] Batch 45, Loss 0.24057364463806152\n",
      "[Training Epoch 3] Batch 46, Loss 0.25637421011924744\n",
      "[Training Epoch 3] Batch 47, Loss 0.30029967427253723\n",
      "[Training Epoch 3] Batch 48, Loss 0.255571186542511\n",
      "[Training Epoch 3] Batch 49, Loss 0.27479296922683716\n",
      "[Training Epoch 3] Batch 50, Loss 0.273311048746109\n",
      "[Training Epoch 3] Batch 51, Loss 0.27101069688796997\n",
      "[Training Epoch 3] Batch 52, Loss 0.26658427715301514\n",
      "[Training Epoch 3] Batch 53, Loss 0.2705477476119995\n",
      "[Training Epoch 3] Batch 54, Loss 0.256158709526062\n",
      "[Training Epoch 3] Batch 55, Loss 0.29533857107162476\n",
      "[Training Epoch 3] Batch 56, Loss 0.28303971886634827\n",
      "[Training Epoch 3] Batch 57, Loss 0.29022544622421265\n",
      "[Training Epoch 3] Batch 58, Loss 0.25851598381996155\n",
      "[Training Epoch 3] Batch 59, Loss 0.285514771938324\n",
      "[Training Epoch 3] Batch 60, Loss 0.2766340970993042\n",
      "[Training Epoch 3] Batch 61, Loss 0.257490336894989\n",
      "[Training Epoch 3] Batch 62, Loss 0.2640221118927002\n",
      "[Training Epoch 3] Batch 63, Loss 0.26792454719543457\n",
      "[Training Epoch 3] Batch 64, Loss 0.29710763692855835\n",
      "[Training Epoch 3] Batch 65, Loss 0.2713543176651001\n",
      "[Training Epoch 3] Batch 66, Loss 0.27325665950775146\n",
      "[Training Epoch 3] Batch 67, Loss 0.26278191804885864\n",
      "[Training Epoch 3] Batch 68, Loss 0.29497045278549194\n",
      "[Training Epoch 3] Batch 69, Loss 0.29121214151382446\n",
      "[Training Epoch 3] Batch 70, Loss 0.3076285123825073\n",
      "[Training Epoch 3] Batch 71, Loss 0.2639296054840088\n",
      "[Training Epoch 3] Batch 72, Loss 0.2495339810848236\n",
      "[Training Epoch 3] Batch 73, Loss 0.27122730016708374\n",
      "[Training Epoch 3] Batch 74, Loss 0.26121485233306885\n",
      "[Training Epoch 3] Batch 75, Loss 0.28306281566619873\n",
      "[Training Epoch 3] Batch 76, Loss 0.2575492560863495\n",
      "[Training Epoch 3] Batch 77, Loss 0.26120519638061523\n",
      "[Training Epoch 3] Batch 78, Loss 0.2728936970233917\n",
      "[Training Epoch 3] Batch 79, Loss 0.28452879190444946\n",
      "[Training Epoch 3] Batch 80, Loss 0.2785995900630951\n",
      "[Training Epoch 3] Batch 81, Loss 0.24688750505447388\n",
      "[Training Epoch 3] Batch 82, Loss 0.25098884105682373\n",
      "[Training Epoch 3] Batch 83, Loss 0.2684665024280548\n",
      "[Training Epoch 3] Batch 84, Loss 0.23804402351379395\n",
      "[Training Epoch 3] Batch 85, Loss 0.25970304012298584\n",
      "[Training Epoch 3] Batch 86, Loss 0.2747553884983063\n",
      "[Training Epoch 3] Batch 87, Loss 0.2777213454246521\n",
      "[Training Epoch 3] Batch 88, Loss 0.2541190981864929\n",
      "[Training Epoch 3] Batch 89, Loss 0.2821875214576721\n",
      "[Training Epoch 3] Batch 90, Loss 0.29405951499938965\n",
      "[Training Epoch 3] Batch 91, Loss 0.2501092553138733\n",
      "[Training Epoch 3] Batch 92, Loss 0.28724294900894165\n",
      "[Training Epoch 3] Batch 93, Loss 0.2431028187274933\n",
      "[Training Epoch 3] Batch 94, Loss 0.2565012574195862\n",
      "[Training Epoch 3] Batch 95, Loss 0.28765368461608887\n",
      "[Training Epoch 3] Batch 96, Loss 0.25174182653427124\n",
      "[Training Epoch 3] Batch 97, Loss 0.23693729937076569\n",
      "[Training Epoch 3] Batch 98, Loss 0.26522770524024963\n",
      "[Training Epoch 3] Batch 99, Loss 0.30536702275276184\n",
      "[Training Epoch 3] Batch 100, Loss 0.2889731228351593\n",
      "[Training Epoch 3] Batch 101, Loss 0.24942784011363983\n",
      "[Training Epoch 3] Batch 102, Loss 0.30336248874664307\n",
      "[Training Epoch 3] Batch 103, Loss 0.2677329182624817\n",
      "[Training Epoch 3] Batch 104, Loss 0.2477821260690689\n",
      "[Training Epoch 3] Batch 105, Loss 0.2785843312740326\n",
      "[Training Epoch 3] Batch 106, Loss 0.25078386068344116\n",
      "[Training Epoch 3] Batch 107, Loss 0.27211475372314453\n",
      "[Training Epoch 3] Batch 108, Loss 0.2736891508102417\n",
      "[Training Epoch 3] Batch 109, Loss 0.27294397354125977\n",
      "[Training Epoch 3] Batch 110, Loss 0.26585814356803894\n",
      "[Training Epoch 3] Batch 111, Loss 0.2902671694755554\n",
      "[Training Epoch 3] Batch 112, Loss 0.256072461605072\n",
      "[Training Epoch 3] Batch 113, Loss 0.2535647451877594\n",
      "[Training Epoch 3] Batch 114, Loss 0.2783881425857544\n",
      "[Training Epoch 3] Batch 115, Loss 0.30022406578063965\n",
      "[Training Epoch 3] Batch 116, Loss 0.29697543382644653\n",
      "[Training Epoch 3] Batch 117, Loss 0.290748655796051\n",
      "[Training Epoch 3] Batch 118, Loss 0.2550666928291321\n",
      "[Training Epoch 3] Batch 119, Loss 0.27324146032333374\n",
      "[Training Epoch 3] Batch 120, Loss 0.25543212890625\n",
      "[Training Epoch 3] Batch 121, Loss 0.24769243597984314\n",
      "[Training Epoch 3] Batch 122, Loss 0.2599940598011017\n",
      "[Training Epoch 3] Batch 123, Loss 0.2695735692977905\n",
      "[Training Epoch 3] Batch 124, Loss 0.2638510763645172\n",
      "[Training Epoch 3] Batch 125, Loss 0.26759427785873413\n",
      "[Training Epoch 3] Batch 126, Loss 0.24723520874977112\n",
      "[Training Epoch 3] Batch 127, Loss 0.2586003243923187\n",
      "[Training Epoch 3] Batch 128, Loss 0.23390302062034607\n",
      "[Training Epoch 3] Batch 129, Loss 0.268219530582428\n",
      "[Training Epoch 3] Batch 130, Loss 0.24598726630210876\n",
      "[Training Epoch 3] Batch 131, Loss 0.25337347388267517\n",
      "[Training Epoch 3] Batch 132, Loss 0.2569235563278198\n",
      "[Training Epoch 3] Batch 133, Loss 0.27483007311820984\n",
      "[Training Epoch 3] Batch 134, Loss 0.25323015451431274\n",
      "[Training Epoch 3] Batch 135, Loss 0.2985081970691681\n",
      "[Training Epoch 3] Batch 136, Loss 0.29911842942237854\n",
      "[Training Epoch 3] Batch 137, Loss 0.2648252844810486\n",
      "[Training Epoch 3] Batch 138, Loss 0.24657617509365082\n",
      "[Training Epoch 3] Batch 139, Loss 0.26353225111961365\n",
      "[Training Epoch 3] Batch 140, Loss 0.27753788232803345\n",
      "[Training Epoch 3] Batch 141, Loss 0.2857900857925415\n",
      "[Training Epoch 3] Batch 142, Loss 0.27052396535873413\n",
      "[Training Epoch 3] Batch 143, Loss 0.29894885420799255\n",
      "[Training Epoch 3] Batch 144, Loss 0.267697274684906\n",
      "[Training Epoch 3] Batch 145, Loss 0.27326783537864685\n",
      "[Training Epoch 3] Batch 146, Loss 0.23622187972068787\n",
      "[Training Epoch 3] Batch 147, Loss 0.2901769280433655\n",
      "[Training Epoch 3] Batch 148, Loss 0.29074233770370483\n",
      "[Training Epoch 3] Batch 149, Loss 0.25046104192733765\n",
      "[Training Epoch 3] Batch 150, Loss 0.24735577404499054\n",
      "[Training Epoch 3] Batch 151, Loss 0.30562055110931396\n",
      "[Training Epoch 3] Batch 152, Loss 0.28395646810531616\n",
      "[Training Epoch 3] Batch 153, Loss 0.24930991232395172\n",
      "[Training Epoch 3] Batch 154, Loss 0.26452988386154175\n",
      "[Training Epoch 3] Batch 155, Loss 0.2781350016593933\n",
      "[Training Epoch 3] Batch 156, Loss 0.28217852115631104\n",
      "[Training Epoch 3] Batch 157, Loss 0.2824039161205292\n",
      "[Training Epoch 3] Batch 158, Loss 0.2716203033924103\n",
      "[Training Epoch 3] Batch 159, Loss 0.25531643629074097\n",
      "[Training Epoch 3] Batch 160, Loss 0.27819305658340454\n",
      "[Training Epoch 3] Batch 161, Loss 0.2730861306190491\n",
      "[Training Epoch 3] Batch 162, Loss 0.2730776071548462\n",
      "[Training Epoch 3] Batch 163, Loss 0.2962762415409088\n",
      "[Training Epoch 3] Batch 164, Loss 0.28229179978370667\n",
      "[Training Epoch 3] Batch 165, Loss 0.24586668610572815\n",
      "[Training Epoch 3] Batch 166, Loss 0.27666425704956055\n",
      "[Training Epoch 3] Batch 167, Loss 0.2697584629058838\n",
      "[Training Epoch 3] Batch 168, Loss 0.28606104850769043\n",
      "[Training Epoch 3] Batch 169, Loss 0.26194071769714355\n",
      "[Training Epoch 3] Batch 170, Loss 0.2585746645927429\n",
      "[Training Epoch 3] Batch 171, Loss 0.30164486169815063\n",
      "[Training Epoch 3] Batch 172, Loss 0.28089362382888794\n",
      "[Training Epoch 3] Batch 173, Loss 0.23140302300453186\n",
      "[Training Epoch 3] Batch 174, Loss 0.27856960892677307\n",
      "[Training Epoch 3] Batch 175, Loss 0.2479054033756256\n",
      "[Training Epoch 3] Batch 176, Loss 0.2659517824649811\n",
      "[Training Epoch 3] Batch 177, Loss 0.24600523710250854\n",
      "[Training Epoch 3] Batch 178, Loss 0.2751957178115845\n",
      "[Training Epoch 3] Batch 179, Loss 0.2770000398159027\n",
      "[Training Epoch 3] Batch 180, Loss 0.28821152448654175\n",
      "[Training Epoch 3] Batch 181, Loss 0.24945418536663055\n",
      "[Training Epoch 3] Batch 182, Loss 0.26139384508132935\n",
      "[Training Epoch 3] Batch 183, Loss 0.29735344648361206\n",
      "[Training Epoch 3] Batch 184, Loss 0.2685509920120239\n",
      "[Training Epoch 3] Batch 185, Loss 0.2825426459312439\n",
      "[Training Epoch 3] Batch 186, Loss 0.2653999328613281\n",
      "[Training Epoch 3] Batch 187, Loss 0.2502790689468384\n",
      "[Training Epoch 3] Batch 188, Loss 0.2481825202703476\n",
      "[Training Epoch 3] Batch 189, Loss 0.26731938123703003\n",
      "[Training Epoch 3] Batch 190, Loss 0.2991498112678528\n",
      "[Training Epoch 3] Batch 191, Loss 0.2921416163444519\n",
      "[Training Epoch 3] Batch 192, Loss 0.2686820328235626\n",
      "[Training Epoch 3] Batch 193, Loss 0.26579782366752625\n",
      "[Training Epoch 3] Batch 194, Loss 0.2707671523094177\n",
      "[Training Epoch 3] Batch 195, Loss 0.24331910908222198\n",
      "[Training Epoch 3] Batch 196, Loss 0.265900194644928\n",
      "[Training Epoch 3] Batch 197, Loss 0.2842091917991638\n",
      "[Training Epoch 3] Batch 198, Loss 0.29756060242652893\n",
      "[Training Epoch 3] Batch 199, Loss 0.28102627396583557\n",
      "[Training Epoch 3] Batch 200, Loss 0.26290014386177063\n",
      "[Training Epoch 3] Batch 201, Loss 0.250321626663208\n",
      "[Training Epoch 3] Batch 202, Loss 0.28733372688293457\n",
      "[Training Epoch 3] Batch 203, Loss 0.2609516382217407\n",
      "[Training Epoch 3] Batch 204, Loss 0.2751162648200989\n",
      "[Training Epoch 3] Batch 205, Loss 0.26996520161628723\n",
      "[Training Epoch 3] Batch 206, Loss 0.2662888169288635\n",
      "[Training Epoch 3] Batch 207, Loss 0.28545039892196655\n",
      "[Training Epoch 3] Batch 208, Loss 0.2663936913013458\n",
      "[Training Epoch 3] Batch 209, Loss 0.2706211805343628\n",
      "[Training Epoch 3] Batch 210, Loss 0.2445966601371765\n",
      "[Training Epoch 3] Batch 211, Loss 0.2671339213848114\n",
      "[Training Epoch 3] Batch 212, Loss 0.27584683895111084\n",
      "[Training Epoch 3] Batch 213, Loss 0.2668865919113159\n",
      "[Training Epoch 3] Batch 214, Loss 0.2873504161834717\n",
      "[Training Epoch 3] Batch 215, Loss 0.27860593795776367\n",
      "[Training Epoch 3] Batch 216, Loss 0.2940414249897003\n",
      "[Training Epoch 3] Batch 217, Loss 0.2991105318069458\n",
      "[Training Epoch 3] Batch 218, Loss 0.29085269570350647\n",
      "[Training Epoch 3] Batch 219, Loss 0.25982964038848877\n",
      "[Training Epoch 3] Batch 220, Loss 0.2620276212692261\n",
      "[Training Epoch 3] Batch 221, Loss 0.2689642906188965\n",
      "[Training Epoch 3] Batch 222, Loss 0.2637225389480591\n",
      "[Training Epoch 3] Batch 223, Loss 0.2677288353443146\n",
      "[Training Epoch 3] Batch 224, Loss 0.244370236992836\n",
      "[Training Epoch 3] Batch 225, Loss 0.27324798703193665\n",
      "[Training Epoch 3] Batch 226, Loss 0.2502121031284332\n",
      "[Training Epoch 3] Batch 227, Loss 0.3069826066493988\n",
      "[Training Epoch 3] Batch 228, Loss 0.27096807956695557\n",
      "[Training Epoch 3] Batch 229, Loss 0.2768009901046753\n",
      "[Training Epoch 3] Batch 230, Loss 0.2737267315387726\n",
      "[Training Epoch 3] Batch 231, Loss 0.2554520070552826\n",
      "[Training Epoch 3] Batch 232, Loss 0.2504327893257141\n",
      "[Training Epoch 3] Batch 233, Loss 0.2738005220890045\n",
      "[Training Epoch 3] Batch 234, Loss 0.25064757466316223\n",
      "[Training Epoch 3] Batch 235, Loss 0.2743259072303772\n",
      "[Training Epoch 3] Batch 236, Loss 0.2551319897174835\n",
      "[Training Epoch 3] Batch 237, Loss 0.28626948595046997\n",
      "[Training Epoch 3] Batch 238, Loss 0.2593074142932892\n",
      "[Training Epoch 3] Batch 239, Loss 0.2683655917644501\n",
      "[Training Epoch 3] Batch 240, Loss 0.27057135105133057\n",
      "[Training Epoch 3] Batch 241, Loss 0.27585846185684204\n",
      "[Training Epoch 3] Batch 242, Loss 0.27813488245010376\n",
      "[Training Epoch 3] Batch 243, Loss 0.2696130871772766\n",
      "[Training Epoch 3] Batch 244, Loss 0.27409446239471436\n",
      "[Training Epoch 3] Batch 245, Loss 0.26236751675605774\n",
      "[Training Epoch 3] Batch 246, Loss 0.24707412719726562\n",
      "[Training Epoch 3] Batch 247, Loss 0.2761521339416504\n",
      "[Training Epoch 3] Batch 248, Loss 0.2530229687690735\n",
      "[Training Epoch 3] Batch 249, Loss 0.2787718176841736\n",
      "[Training Epoch 3] Batch 250, Loss 0.27566301822662354\n",
      "[Training Epoch 3] Batch 251, Loss 0.2739182412624359\n",
      "[Training Epoch 3] Batch 252, Loss 0.27295881509780884\n",
      "[Training Epoch 3] Batch 253, Loss 0.2523985505104065\n",
      "[Training Epoch 3] Batch 254, Loss 0.2954508662223816\n",
      "[Training Epoch 3] Batch 255, Loss 0.28443196415901184\n",
      "[Training Epoch 3] Batch 256, Loss 0.26909664273262024\n",
      "[Training Epoch 3] Batch 257, Loss 0.26547756791114807\n",
      "[Training Epoch 3] Batch 258, Loss 0.25653934478759766\n",
      "[Training Epoch 3] Batch 259, Loss 0.2447565793991089\n",
      "[Training Epoch 3] Batch 260, Loss 0.29192841053009033\n",
      "[Training Epoch 3] Batch 261, Loss 0.26842808723449707\n",
      "[Training Epoch 3] Batch 262, Loss 0.260210782289505\n",
      "[Training Epoch 3] Batch 263, Loss 0.2582624554634094\n",
      "[Training Epoch 3] Batch 264, Loss 0.2650049924850464\n",
      "[Training Epoch 3] Batch 265, Loss 0.2621321678161621\n",
      "[Training Epoch 3] Batch 266, Loss 0.25851792097091675\n",
      "[Training Epoch 3] Batch 267, Loss 0.30763012170791626\n",
      "[Training Epoch 3] Batch 268, Loss 0.26021552085876465\n",
      "[Training Epoch 3] Batch 269, Loss 0.2780269384384155\n",
      "[Training Epoch 3] Batch 270, Loss 0.24987809360027313\n",
      "[Training Epoch 3] Batch 271, Loss 0.2537105083465576\n",
      "[Training Epoch 3] Batch 272, Loss 0.28133124113082886\n",
      "[Training Epoch 3] Batch 273, Loss 0.2436104416847229\n",
      "[Training Epoch 3] Batch 274, Loss 0.2768726944923401\n",
      "[Training Epoch 3] Batch 275, Loss 0.28777265548706055\n",
      "[Training Epoch 3] Batch 276, Loss 0.2844829857349396\n",
      "[Training Epoch 3] Batch 277, Loss 0.28790783882141113\n",
      "[Training Epoch 3] Batch 278, Loss 0.2837480306625366\n",
      "[Training Epoch 3] Batch 279, Loss 0.28860148787498474\n",
      "[Training Epoch 3] Batch 280, Loss 0.2641142010688782\n",
      "[Training Epoch 3] Batch 281, Loss 0.2546529769897461\n",
      "[Training Epoch 3] Batch 282, Loss 0.26880133152008057\n",
      "[Training Epoch 3] Batch 283, Loss 0.30497196316719055\n",
      "[Training Epoch 3] Batch 284, Loss 0.26524800062179565\n",
      "[Training Epoch 3] Batch 285, Loss 0.26255741715431213\n",
      "[Training Epoch 3] Batch 286, Loss 0.27007758617401123\n",
      "[Training Epoch 3] Batch 287, Loss 0.3096820116043091\n",
      "[Training Epoch 3] Batch 288, Loss 0.2626240849494934\n",
      "[Training Epoch 3] Batch 289, Loss 0.31371307373046875\n",
      "[Training Epoch 3] Batch 290, Loss 0.27392062544822693\n",
      "[Training Epoch 3] Batch 291, Loss 0.28327006101608276\n",
      "[Training Epoch 3] Batch 292, Loss 0.26364701986312866\n",
      "[Training Epoch 3] Batch 293, Loss 0.27782514691352844\n",
      "[Training Epoch 3] Batch 294, Loss 0.31327205896377563\n",
      "[Training Epoch 3] Batch 295, Loss 0.28914469480514526\n",
      "[Training Epoch 3] Batch 296, Loss 0.2925407886505127\n",
      "[Training Epoch 3] Batch 297, Loss 0.2488183081150055\n",
      "[Training Epoch 3] Batch 298, Loss 0.28963330388069153\n",
      "[Training Epoch 3] Batch 299, Loss 0.2766583561897278\n",
      "[Training Epoch 3] Batch 300, Loss 0.2587534785270691\n",
      "[Training Epoch 3] Batch 301, Loss 0.25918400287628174\n",
      "[Training Epoch 3] Batch 302, Loss 0.2545441687107086\n",
      "[Training Epoch 3] Batch 303, Loss 0.27372032403945923\n",
      "[Training Epoch 3] Batch 304, Loss 0.2663443386554718\n",
      "[Training Epoch 3] Batch 305, Loss 0.25463342666625977\n",
      "[Training Epoch 3] Batch 306, Loss 0.29875504970550537\n",
      "[Training Epoch 3] Batch 307, Loss 0.2550428509712219\n",
      "[Training Epoch 3] Batch 308, Loss 0.28825581073760986\n",
      "[Training Epoch 3] Batch 309, Loss 0.262847363948822\n",
      "[Training Epoch 3] Batch 310, Loss 0.27736181020736694\n",
      "[Training Epoch 3] Batch 311, Loss 0.2707633972167969\n",
      "[Training Epoch 3] Batch 312, Loss 0.28967365622520447\n",
      "[Training Epoch 3] Batch 313, Loss 0.2580866515636444\n",
      "[Training Epoch 3] Batch 314, Loss 0.28535228967666626\n",
      "[Training Epoch 3] Batch 315, Loss 0.26339778304100037\n",
      "[Training Epoch 3] Batch 316, Loss 0.2557448446750641\n",
      "[Training Epoch 3] Batch 317, Loss 0.25322210788726807\n",
      "[Training Epoch 3] Batch 318, Loss 0.26837682723999023\n",
      "[Training Epoch 3] Batch 319, Loss 0.280958354473114\n",
      "[Training Epoch 3] Batch 320, Loss 0.2725638747215271\n",
      "[Training Epoch 3] Batch 321, Loss 0.2509300410747528\n",
      "[Training Epoch 3] Batch 322, Loss 0.2796507775783539\n",
      "[Training Epoch 3] Batch 323, Loss 0.2791978120803833\n",
      "[Training Epoch 3] Batch 324, Loss 0.27741947770118713\n",
      "[Training Epoch 3] Batch 325, Loss 0.26825231313705444\n",
      "[Training Epoch 3] Batch 326, Loss 0.28880971670150757\n",
      "[Training Epoch 3] Batch 327, Loss 0.3052537441253662\n",
      "[Training Epoch 3] Batch 328, Loss 0.2439165711402893\n",
      "[Training Epoch 3] Batch 329, Loss 0.2697921395301819\n",
      "[Training Epoch 3] Batch 330, Loss 0.2807263731956482\n",
      "[Training Epoch 3] Batch 331, Loss 0.27278023958206177\n",
      "[Training Epoch 3] Batch 332, Loss 0.280247300863266\n",
      "[Training Epoch 3] Batch 333, Loss 0.279843807220459\n",
      "[Training Epoch 3] Batch 334, Loss 0.2872738242149353\n",
      "[Training Epoch 3] Batch 335, Loss 0.3039809465408325\n",
      "[Training Epoch 3] Batch 336, Loss 0.2608661353588104\n",
      "[Training Epoch 3] Batch 337, Loss 0.2871484160423279\n",
      "[Training Epoch 3] Batch 338, Loss 0.27185875177383423\n",
      "[Training Epoch 3] Batch 339, Loss 0.257276326417923\n",
      "[Training Epoch 3] Batch 340, Loss 0.2699231505393982\n",
      "[Training Epoch 3] Batch 341, Loss 0.25769978761672974\n",
      "[Training Epoch 3] Batch 342, Loss 0.26278501749038696\n",
      "[Training Epoch 3] Batch 343, Loss 0.24152597784996033\n",
      "[Training Epoch 3] Batch 344, Loss 0.2865622639656067\n",
      "[Training Epoch 3] Batch 345, Loss 0.27073004841804504\n",
      "[Training Epoch 3] Batch 346, Loss 0.24574317038059235\n",
      "[Training Epoch 3] Batch 347, Loss 0.2616988718509674\n",
      "[Training Epoch 3] Batch 348, Loss 0.24686266481876373\n",
      "[Training Epoch 3] Batch 349, Loss 0.2964777946472168\n",
      "[Training Epoch 3] Batch 350, Loss 0.27167052030563354\n",
      "[Training Epoch 3] Batch 351, Loss 0.2589796185493469\n",
      "[Training Epoch 3] Batch 352, Loss 0.27132004499435425\n",
      "[Training Epoch 3] Batch 353, Loss 0.2507137060165405\n",
      "[Training Epoch 3] Batch 354, Loss 0.26770251989364624\n",
      "[Training Epoch 3] Batch 355, Loss 0.30311593413352966\n",
      "[Training Epoch 3] Batch 356, Loss 0.26900777220726013\n",
      "[Training Epoch 3] Batch 357, Loss 0.25631487369537354\n",
      "[Training Epoch 3] Batch 358, Loss 0.2764360308647156\n",
      "[Training Epoch 3] Batch 359, Loss 0.24423182010650635\n",
      "[Training Epoch 3] Batch 360, Loss 0.28071457147598267\n",
      "[Training Epoch 3] Batch 361, Loss 0.2603920102119446\n",
      "[Training Epoch 3] Batch 362, Loss 0.2476009875535965\n",
      "[Training Epoch 3] Batch 363, Loss 0.27028512954711914\n",
      "[Training Epoch 3] Batch 364, Loss 0.26107126474380493\n",
      "[Training Epoch 3] Batch 365, Loss 0.25398290157318115\n",
      "[Training Epoch 3] Batch 366, Loss 0.2897930145263672\n",
      "[Training Epoch 3] Batch 367, Loss 0.25802066922187805\n",
      "[Training Epoch 3] Batch 368, Loss 0.25881657004356384\n",
      "[Training Epoch 3] Batch 369, Loss 0.2728816270828247\n",
      "[Training Epoch 3] Batch 370, Loss 0.24770210683345795\n",
      "[Training Epoch 3] Batch 371, Loss 0.26120367646217346\n",
      "[Training Epoch 3] Batch 372, Loss 0.2789190709590912\n",
      "[Training Epoch 3] Batch 373, Loss 0.27546799182891846\n",
      "[Training Epoch 3] Batch 374, Loss 0.2552368938922882\n",
      "[Training Epoch 3] Batch 375, Loss 0.27348533272743225\n",
      "[Training Epoch 3] Batch 376, Loss 0.26712527871131897\n",
      "[Training Epoch 3] Batch 377, Loss 0.2557743191719055\n",
      "[Training Epoch 3] Batch 378, Loss 0.28404784202575684\n",
      "[Training Epoch 3] Batch 379, Loss 0.2667683959007263\n",
      "[Training Epoch 3] Batch 380, Loss 0.2851121425628662\n",
      "[Training Epoch 3] Batch 381, Loss 0.2642268240451813\n",
      "[Training Epoch 3] Batch 382, Loss 0.2261769026517868\n",
      "[Training Epoch 3] Batch 383, Loss 0.24973616003990173\n",
      "[Training Epoch 3] Batch 384, Loss 0.2939268946647644\n",
      "[Training Epoch 3] Batch 385, Loss 0.282134473323822\n",
      "[Training Epoch 3] Batch 386, Loss 0.26839786767959595\n",
      "[Training Epoch 3] Batch 387, Loss 0.3022613525390625\n",
      "[Training Epoch 3] Batch 388, Loss 0.2803414463996887\n",
      "[Training Epoch 3] Batch 389, Loss 0.2840801477432251\n",
      "[Training Epoch 3] Batch 390, Loss 0.2570311427116394\n",
      "[Training Epoch 3] Batch 391, Loss 0.26452046632766724\n",
      "[Training Epoch 3] Batch 392, Loss 0.26906391978263855\n",
      "[Training Epoch 3] Batch 393, Loss 0.26659464836120605\n",
      "[Training Epoch 3] Batch 394, Loss 0.2764385938644409\n",
      "[Training Epoch 3] Batch 395, Loss 0.26260024309158325\n",
      "[Training Epoch 3] Batch 396, Loss 0.24718952178955078\n",
      "[Training Epoch 3] Batch 397, Loss 0.25472986698150635\n",
      "[Training Epoch 3] Batch 398, Loss 0.25962451100349426\n",
      "[Training Epoch 3] Batch 399, Loss 0.26537448167800903\n",
      "[Training Epoch 3] Batch 400, Loss 0.2681095004081726\n",
      "[Training Epoch 3] Batch 401, Loss 0.25276505947113037\n",
      "[Training Epoch 3] Batch 402, Loss 0.2985428273677826\n",
      "[Training Epoch 3] Batch 403, Loss 0.28703057765960693\n",
      "[Training Epoch 3] Batch 404, Loss 0.2720622420310974\n",
      "[Training Epoch 3] Batch 405, Loss 0.27386242151260376\n",
      "[Training Epoch 3] Batch 406, Loss 0.2619803845882416\n",
      "[Training Epoch 3] Batch 407, Loss 0.2923378646373749\n",
      "[Training Epoch 3] Batch 408, Loss 0.29413503408432007\n",
      "[Training Epoch 3] Batch 409, Loss 0.26278266310691833\n",
      "[Training Epoch 3] Batch 410, Loss 0.24873298406600952\n",
      "[Training Epoch 3] Batch 411, Loss 0.2716105580329895\n",
      "[Training Epoch 3] Batch 412, Loss 0.26834332942962646\n",
      "[Training Epoch 3] Batch 413, Loss 0.26953399181365967\n",
      "[Training Epoch 3] Batch 414, Loss 0.24841099977493286\n",
      "[Training Epoch 3] Batch 415, Loss 0.24826309084892273\n",
      "[Training Epoch 3] Batch 416, Loss 0.24931348860263824\n",
      "[Training Epoch 3] Batch 417, Loss 0.2674052119255066\n",
      "[Training Epoch 3] Batch 418, Loss 0.2983669340610504\n",
      "[Training Epoch 3] Batch 419, Loss 0.28148409724235535\n",
      "[Training Epoch 3] Batch 420, Loss 0.26081904768943787\n",
      "[Training Epoch 3] Batch 421, Loss 0.2607368230819702\n",
      "[Training Epoch 3] Batch 422, Loss 0.2751263678073883\n",
      "[Training Epoch 3] Batch 423, Loss 0.2740944027900696\n",
      "[Training Epoch 3] Batch 424, Loss 0.26856088638305664\n",
      "[Training Epoch 3] Batch 425, Loss 0.2613145112991333\n",
      "[Training Epoch 3] Batch 426, Loss 0.27915069460868835\n",
      "[Training Epoch 3] Batch 427, Loss 0.2984815239906311\n",
      "[Training Epoch 3] Batch 428, Loss 0.261569082736969\n",
      "[Training Epoch 3] Batch 429, Loss 0.2697688341140747\n",
      "[Training Epoch 3] Batch 430, Loss 0.261346697807312\n",
      "[Training Epoch 3] Batch 431, Loss 0.2935606837272644\n",
      "[Training Epoch 3] Batch 432, Loss 0.23613578081130981\n",
      "[Training Epoch 3] Batch 433, Loss 0.2664638161659241\n",
      "[Training Epoch 3] Batch 434, Loss 0.2978397309780121\n",
      "[Training Epoch 3] Batch 435, Loss 0.26468920707702637\n",
      "[Training Epoch 3] Batch 436, Loss 0.2657586336135864\n",
      "[Training Epoch 3] Batch 437, Loss 0.2667480707168579\n",
      "[Training Epoch 3] Batch 438, Loss 0.2818145751953125\n",
      "[Training Epoch 3] Batch 439, Loss 0.2823105752468109\n",
      "[Training Epoch 3] Batch 440, Loss 0.24831688404083252\n",
      "[Training Epoch 3] Batch 441, Loss 0.27084243297576904\n",
      "[Training Epoch 3] Batch 442, Loss 0.2501009702682495\n",
      "[Training Epoch 3] Batch 443, Loss 0.30122512578964233\n",
      "[Training Epoch 3] Batch 444, Loss 0.2777944803237915\n",
      "[Training Epoch 3] Batch 445, Loss 0.2555750012397766\n",
      "[Training Epoch 3] Batch 446, Loss 0.2826174795627594\n",
      "[Training Epoch 3] Batch 447, Loss 0.2727587819099426\n",
      "[Training Epoch 3] Batch 448, Loss 0.2647700309753418\n",
      "[Training Epoch 3] Batch 449, Loss 0.3033429682254791\n",
      "[Training Epoch 3] Batch 450, Loss 0.2548742890357971\n",
      "[Training Epoch 3] Batch 451, Loss 0.3016415238380432\n",
      "[Training Epoch 3] Batch 452, Loss 0.2963533401489258\n",
      "[Training Epoch 3] Batch 453, Loss 0.24973276257514954\n",
      "[Training Epoch 3] Batch 454, Loss 0.2611802816390991\n",
      "[Training Epoch 3] Batch 455, Loss 0.2653743624687195\n",
      "[Training Epoch 3] Batch 456, Loss 0.3003336787223816\n",
      "[Training Epoch 3] Batch 457, Loss 0.28663116693496704\n",
      "[Training Epoch 3] Batch 458, Loss 0.2629692256450653\n",
      "[Training Epoch 3] Batch 459, Loss 0.2807174324989319\n",
      "[Training Epoch 3] Batch 460, Loss 0.2708280682563782\n",
      "[Training Epoch 3] Batch 461, Loss 0.24577659368515015\n",
      "[Training Epoch 3] Batch 462, Loss 0.2654304504394531\n",
      "[Training Epoch 3] Batch 463, Loss 0.2608844041824341\n",
      "[Training Epoch 3] Batch 464, Loss 0.2929161787033081\n",
      "[Training Epoch 3] Batch 465, Loss 0.25245851278305054\n",
      "[Training Epoch 3] Batch 466, Loss 0.25517624616622925\n",
      "[Training Epoch 3] Batch 467, Loss 0.26032450795173645\n",
      "[Training Epoch 3] Batch 468, Loss 0.27793437242507935\n",
      "[Training Epoch 3] Batch 469, Loss 0.2831907272338867\n",
      "[Training Epoch 3] Batch 470, Loss 0.26420140266418457\n",
      "[Training Epoch 3] Batch 471, Loss 0.2795347571372986\n",
      "[Training Epoch 3] Batch 472, Loss 0.26901787519454956\n",
      "[Training Epoch 3] Batch 473, Loss 0.2492867410182953\n",
      "[Training Epoch 3] Batch 474, Loss 0.28133848309516907\n",
      "[Training Epoch 3] Batch 475, Loss 0.24316683411598206\n",
      "[Training Epoch 3] Batch 476, Loss 0.2422972321510315\n",
      "[Training Epoch 3] Batch 477, Loss 0.3000165820121765\n",
      "[Training Epoch 3] Batch 478, Loss 0.29101288318634033\n",
      "[Training Epoch 3] Batch 479, Loss 0.26218855381011963\n",
      "[Training Epoch 3] Batch 480, Loss 0.2867833971977234\n",
      "[Training Epoch 3] Batch 481, Loss 0.2541115880012512\n",
      "[Training Epoch 3] Batch 482, Loss 0.2579081058502197\n",
      "[Training Epoch 3] Batch 483, Loss 0.26828405261039734\n",
      "[Training Epoch 3] Batch 484, Loss 0.2756880223751068\n",
      "[Training Epoch 3] Batch 485, Loss 0.26291966438293457\n",
      "[Training Epoch 3] Batch 486, Loss 0.27288466691970825\n",
      "[Training Epoch 3] Batch 487, Loss 0.2674180567264557\n",
      "[Training Epoch 3] Batch 488, Loss 0.2884775400161743\n",
      "[Training Epoch 3] Batch 489, Loss 0.2777988314628601\n",
      "[Training Epoch 3] Batch 490, Loss 0.2962779104709625\n",
      "[Training Epoch 3] Batch 491, Loss 0.24427898228168488\n",
      "[Training Epoch 3] Batch 492, Loss 0.25984007120132446\n",
      "[Training Epoch 3] Batch 493, Loss 0.2696504592895508\n",
      "[Training Epoch 3] Batch 494, Loss 0.25035011768341064\n",
      "[Training Epoch 3] Batch 495, Loss 0.2522035837173462\n",
      "[Training Epoch 3] Batch 496, Loss 0.2778472900390625\n",
      "[Training Epoch 3] Batch 497, Loss 0.2795770466327667\n",
      "[Training Epoch 3] Batch 498, Loss 0.26137152314186096\n",
      "[Training Epoch 3] Batch 499, Loss 0.2917431592941284\n",
      "[Training Epoch 3] Batch 500, Loss 0.2806061804294586\n",
      "[Training Epoch 3] Batch 501, Loss 0.2815512418746948\n",
      "[Training Epoch 3] Batch 502, Loss 0.2737869620323181\n",
      "[Training Epoch 3] Batch 503, Loss 0.28075599670410156\n",
      "[Training Epoch 3] Batch 504, Loss 0.2857920527458191\n",
      "[Training Epoch 3] Batch 505, Loss 0.2947159707546234\n",
      "[Training Epoch 3] Batch 506, Loss 0.2850242257118225\n",
      "[Training Epoch 3] Batch 507, Loss 0.2556312680244446\n",
      "[Training Epoch 3] Batch 508, Loss 0.24326926469802856\n",
      "[Training Epoch 3] Batch 509, Loss 0.2600528597831726\n",
      "[Training Epoch 3] Batch 510, Loss 0.25509247183799744\n",
      "[Training Epoch 3] Batch 511, Loss 0.2659153640270233\n",
      "[Training Epoch 3] Batch 512, Loss 0.2682667076587677\n",
      "[Training Epoch 3] Batch 513, Loss 0.27945390343666077\n",
      "[Training Epoch 3] Batch 514, Loss 0.25806283950805664\n",
      "[Training Epoch 3] Batch 515, Loss 0.23083651065826416\n",
      "[Training Epoch 3] Batch 516, Loss 0.2775736451148987\n",
      "[Training Epoch 3] Batch 517, Loss 0.2515190839767456\n",
      "[Training Epoch 3] Batch 518, Loss 0.2686174213886261\n",
      "[Training Epoch 3] Batch 519, Loss 0.24622821807861328\n",
      "[Training Epoch 3] Batch 520, Loss 0.26075461506843567\n",
      "[Training Epoch 3] Batch 521, Loss 0.28601765632629395\n",
      "[Training Epoch 3] Batch 522, Loss 0.250179648399353\n",
      "[Training Epoch 3] Batch 523, Loss 0.26889967918395996\n",
      "[Training Epoch 3] Batch 524, Loss 0.27211469411849976\n",
      "[Training Epoch 3] Batch 525, Loss 0.24297529458999634\n",
      "[Training Epoch 3] Batch 526, Loss 0.25849831104278564\n",
      "[Training Epoch 3] Batch 527, Loss 0.28831011056900024\n",
      "[Training Epoch 3] Batch 528, Loss 0.2722352147102356\n",
      "[Training Epoch 3] Batch 529, Loss 0.27206698060035706\n",
      "[Training Epoch 3] Batch 530, Loss 0.2772992253303528\n",
      "[Training Epoch 3] Batch 531, Loss 0.3059591054916382\n",
      "[Training Epoch 3] Batch 532, Loss 0.2696121037006378\n",
      "[Training Epoch 3] Batch 533, Loss 0.23389261960983276\n",
      "[Training Epoch 3] Batch 534, Loss 0.30508100986480713\n",
      "[Training Epoch 3] Batch 535, Loss 0.25943535566329956\n",
      "[Training Epoch 3] Batch 536, Loss 0.26575174927711487\n",
      "[Training Epoch 3] Batch 537, Loss 0.2694724202156067\n",
      "[Training Epoch 3] Batch 538, Loss 0.2778461277484894\n",
      "[Training Epoch 3] Batch 539, Loss 0.2736722230911255\n",
      "[Training Epoch 3] Batch 540, Loss 0.26880306005477905\n",
      "[Training Epoch 3] Batch 541, Loss 0.2859645485877991\n",
      "[Training Epoch 3] Batch 542, Loss 0.26211148500442505\n",
      "[Training Epoch 3] Batch 543, Loss 0.26266545057296753\n",
      "[Training Epoch 3] Batch 544, Loss 0.3016393780708313\n",
      "[Training Epoch 3] Batch 545, Loss 0.2861887812614441\n",
      "[Training Epoch 3] Batch 546, Loss 0.2656141519546509\n",
      "[Training Epoch 3] Batch 547, Loss 0.2767733335494995\n",
      "[Training Epoch 3] Batch 548, Loss 0.28454726934432983\n",
      "[Training Epoch 3] Batch 549, Loss 0.29085665941238403\n",
      "[Training Epoch 3] Batch 550, Loss 0.2831483483314514\n",
      "[Training Epoch 3] Batch 551, Loss 0.25544264912605286\n",
      "[Training Epoch 3] Batch 552, Loss 0.28099220991134644\n",
      "[Training Epoch 3] Batch 553, Loss 0.266590416431427\n",
      "[Training Epoch 3] Batch 554, Loss 0.2710745930671692\n",
      "[Training Epoch 3] Batch 555, Loss 0.2572094798088074\n",
      "[Training Epoch 3] Batch 556, Loss 0.28252559900283813\n",
      "[Training Epoch 3] Batch 557, Loss 0.26809948682785034\n",
      "[Training Epoch 3] Batch 558, Loss 0.2994540333747864\n",
      "[Training Epoch 3] Batch 559, Loss 0.25960487127304077\n",
      "[Training Epoch 3] Batch 560, Loss 0.29115474224090576\n",
      "[Training Epoch 3] Batch 561, Loss 0.26935574412345886\n",
      "[Training Epoch 3] Batch 562, Loss 0.26996302604675293\n",
      "[Training Epoch 3] Batch 563, Loss 0.247371107339859\n",
      "[Training Epoch 3] Batch 564, Loss 0.29651278257369995\n",
      "[Training Epoch 3] Batch 565, Loss 0.27068275213241577\n",
      "[Training Epoch 3] Batch 566, Loss 0.2603580355644226\n",
      "[Training Epoch 3] Batch 567, Loss 0.2657523453235626\n",
      "[Training Epoch 3] Batch 568, Loss 0.2729550898075104\n",
      "[Training Epoch 3] Batch 569, Loss 0.23946315050125122\n",
      "[Training Epoch 3] Batch 570, Loss 0.28241103887557983\n",
      "[Training Epoch 3] Batch 571, Loss 0.275584876537323\n",
      "[Training Epoch 3] Batch 572, Loss 0.2856636643409729\n",
      "[Training Epoch 3] Batch 573, Loss 0.25277066230773926\n",
      "[Training Epoch 3] Batch 574, Loss 0.30106544494628906\n",
      "[Training Epoch 3] Batch 575, Loss 0.27997419238090515\n",
      "[Training Epoch 3] Batch 576, Loss 0.2487335503101349\n",
      "[Training Epoch 3] Batch 577, Loss 0.2701033651828766\n",
      "[Training Epoch 3] Batch 578, Loss 0.28197401762008667\n",
      "[Training Epoch 3] Batch 579, Loss 0.2371535301208496\n",
      "[Training Epoch 3] Batch 580, Loss 0.2617506980895996\n",
      "[Training Epoch 3] Batch 581, Loss 0.25338107347488403\n",
      "[Training Epoch 3] Batch 582, Loss 0.2703383266925812\n",
      "[Training Epoch 3] Batch 583, Loss 0.24930524826049805\n",
      "[Training Epoch 3] Batch 584, Loss 0.2564859986305237\n",
      "[Training Epoch 3] Batch 585, Loss 0.297757089138031\n",
      "[Training Epoch 3] Batch 586, Loss 0.26451489329338074\n",
      "[Training Epoch 3] Batch 587, Loss 0.2714724540710449\n",
      "[Training Epoch 3] Batch 588, Loss 0.2923034727573395\n",
      "[Training Epoch 3] Batch 589, Loss 0.2637292742729187\n",
      "[Training Epoch 3] Batch 590, Loss 0.26676714420318604\n",
      "[Training Epoch 3] Batch 591, Loss 0.2813648581504822\n",
      "[Training Epoch 3] Batch 592, Loss 0.23934270441532135\n",
      "[Training Epoch 3] Batch 593, Loss 0.25044888257980347\n",
      "[Training Epoch 3] Batch 594, Loss 0.2571348547935486\n",
      "[Training Epoch 3] Batch 595, Loss 0.2528783082962036\n",
      "[Training Epoch 3] Batch 596, Loss 0.26545625925064087\n",
      "[Training Epoch 3] Batch 597, Loss 0.29387062788009644\n",
      "[Training Epoch 3] Batch 598, Loss 0.2799578309059143\n",
      "[Training Epoch 3] Batch 599, Loss 0.27764448523521423\n",
      "[Training Epoch 3] Batch 600, Loss 0.26782864332199097\n",
      "[Training Epoch 3] Batch 601, Loss 0.2737123668193817\n",
      "[Training Epoch 3] Batch 602, Loss 0.259317547082901\n",
      "[Training Epoch 3] Batch 603, Loss 0.2557084560394287\n",
      "[Training Epoch 3] Batch 604, Loss 0.2776246666908264\n",
      "[Training Epoch 3] Batch 605, Loss 0.3110215961933136\n",
      "[Training Epoch 3] Batch 606, Loss 0.261441707611084\n",
      "[Training Epoch 3] Batch 607, Loss 0.289766788482666\n",
      "[Training Epoch 3] Batch 608, Loss 0.28391027450561523\n",
      "[Training Epoch 3] Batch 609, Loss 0.2553613781929016\n",
      "[Training Epoch 3] Batch 610, Loss 0.2696946859359741\n",
      "[Training Epoch 3] Batch 611, Loss 0.27469903230667114\n",
      "[Training Epoch 3] Batch 612, Loss 0.2901250720024109\n",
      "[Training Epoch 3] Batch 613, Loss 0.28018152713775635\n",
      "[Training Epoch 3] Batch 614, Loss 0.2439626157283783\n",
      "[Training Epoch 3] Batch 615, Loss 0.27016863226890564\n",
      "[Training Epoch 3] Batch 616, Loss 0.24279500544071198\n",
      "[Training Epoch 3] Batch 617, Loss 0.25692665576934814\n",
      "[Training Epoch 3] Batch 618, Loss 0.2840906083583832\n",
      "[Training Epoch 3] Batch 619, Loss 0.2925512194633484\n",
      "[Training Epoch 3] Batch 620, Loss 0.26997414231300354\n",
      "[Training Epoch 3] Batch 621, Loss 0.2591407895088196\n",
      "[Training Epoch 3] Batch 622, Loss 0.29299214482307434\n",
      "[Training Epoch 3] Batch 623, Loss 0.3008483350276947\n",
      "[Training Epoch 3] Batch 624, Loss 0.2673491835594177\n",
      "[Training Epoch 3] Batch 625, Loss 0.26062077283859253\n",
      "[Training Epoch 3] Batch 626, Loss 0.2637619078159332\n",
      "[Training Epoch 3] Batch 627, Loss 0.29228124022483826\n",
      "[Training Epoch 3] Batch 628, Loss 0.26388755440711975\n",
      "[Training Epoch 3] Batch 629, Loss 0.25705403089523315\n",
      "[Training Epoch 3] Batch 630, Loss 0.27790337800979614\n",
      "[Training Epoch 3] Batch 631, Loss 0.23785074055194855\n",
      "[Training Epoch 3] Batch 632, Loss 0.26042670011520386\n",
      "[Training Epoch 3] Batch 633, Loss 0.26694226264953613\n",
      "[Training Epoch 3] Batch 634, Loss 0.2408856302499771\n",
      "[Training Epoch 3] Batch 635, Loss 0.28656578063964844\n",
      "[Training Epoch 3] Batch 636, Loss 0.2914355993270874\n",
      "[Training Epoch 3] Batch 637, Loss 0.2648966908454895\n",
      "[Training Epoch 3] Batch 638, Loss 0.2674272954463959\n",
      "[Training Epoch 3] Batch 639, Loss 0.271971732378006\n",
      "[Training Epoch 3] Batch 640, Loss 0.25786763429641724\n",
      "[Training Epoch 3] Batch 641, Loss 0.2925218939781189\n",
      "[Training Epoch 3] Batch 642, Loss 0.26239150762557983\n",
      "[Training Epoch 3] Batch 643, Loss 0.28394484519958496\n",
      "[Training Epoch 3] Batch 644, Loss 0.3133575916290283\n",
      "[Training Epoch 3] Batch 645, Loss 0.2770439386367798\n",
      "[Training Epoch 3] Batch 646, Loss 0.24070334434509277\n",
      "[Training Epoch 3] Batch 647, Loss 0.29701435565948486\n",
      "[Training Epoch 3] Batch 648, Loss 0.2777200937271118\n",
      "[Training Epoch 3] Batch 649, Loss 0.28200748562812805\n",
      "[Training Epoch 3] Batch 650, Loss 0.2495841085910797\n",
      "[Training Epoch 3] Batch 651, Loss 0.2808719277381897\n",
      "[Training Epoch 3] Batch 652, Loss 0.2353256344795227\n",
      "[Training Epoch 3] Batch 653, Loss 0.2638826072216034\n",
      "[Training Epoch 3] Batch 654, Loss 0.2812301516532898\n",
      "[Training Epoch 3] Batch 655, Loss 0.2831459641456604\n",
      "[Training Epoch 3] Batch 656, Loss 0.24819445610046387\n",
      "[Training Epoch 3] Batch 657, Loss 0.26177269220352173\n",
      "[Training Epoch 3] Batch 658, Loss 0.29194337129592896\n",
      "[Training Epoch 3] Batch 659, Loss 0.251049667596817\n",
      "[Training Epoch 3] Batch 660, Loss 0.2676616907119751\n",
      "[Training Epoch 3] Batch 661, Loss 0.3192139267921448\n",
      "[Training Epoch 3] Batch 662, Loss 0.276821494102478\n",
      "[Training Epoch 3] Batch 663, Loss 0.27388566732406616\n",
      "[Training Epoch 3] Batch 664, Loss 0.27445968985557556\n",
      "[Training Epoch 3] Batch 665, Loss 0.2875747084617615\n",
      "[Training Epoch 3] Batch 666, Loss 0.2949303388595581\n",
      "[Training Epoch 3] Batch 667, Loss 0.2585776746273041\n",
      "[Training Epoch 3] Batch 668, Loss 0.25953614711761475\n",
      "[Training Epoch 3] Batch 669, Loss 0.284687876701355\n",
      "[Training Epoch 3] Batch 670, Loss 0.26208043098449707\n",
      "[Training Epoch 3] Batch 671, Loss 0.27151116728782654\n",
      "[Training Epoch 3] Batch 672, Loss 0.29638221859931946\n",
      "[Training Epoch 3] Batch 673, Loss 0.2683016061782837\n",
      "[Training Epoch 3] Batch 674, Loss 0.2881470322608948\n",
      "[Training Epoch 3] Batch 675, Loss 0.30172085762023926\n",
      "[Training Epoch 3] Batch 676, Loss 0.29872772097587585\n",
      "[Training Epoch 3] Batch 677, Loss 0.26129984855651855\n",
      "[Training Epoch 3] Batch 678, Loss 0.2668299078941345\n",
      "[Training Epoch 3] Batch 679, Loss 0.253256618976593\n",
      "[Training Epoch 3] Batch 680, Loss 0.2810920476913452\n",
      "[Training Epoch 3] Batch 681, Loss 0.25927188992500305\n",
      "[Training Epoch 3] Batch 682, Loss 0.26103317737579346\n",
      "[Training Epoch 3] Batch 683, Loss 0.26410919427871704\n",
      "[Training Epoch 3] Batch 684, Loss 0.2623637616634369\n",
      "[Training Epoch 3] Batch 685, Loss 0.27236130833625793\n",
      "[Training Epoch 3] Batch 686, Loss 0.27329832315444946\n",
      "[Training Epoch 3] Batch 687, Loss 0.32093727588653564\n",
      "[Training Epoch 3] Batch 688, Loss 0.260185182094574\n",
      "[Training Epoch 3] Batch 689, Loss 0.275096595287323\n",
      "[Training Epoch 3] Batch 690, Loss 0.287139892578125\n",
      "[Training Epoch 3] Batch 691, Loss 0.2786708474159241\n",
      "[Training Epoch 3] Batch 692, Loss 0.26595497131347656\n",
      "[Training Epoch 3] Batch 693, Loss 0.2749635875225067\n",
      "[Training Epoch 3] Batch 694, Loss 0.26631778478622437\n",
      "[Training Epoch 3] Batch 695, Loss 0.29229921102523804\n",
      "[Training Epoch 3] Batch 696, Loss 0.2713930606842041\n",
      "[Training Epoch 3] Batch 697, Loss 0.2763318717479706\n",
      "[Training Epoch 3] Batch 698, Loss 0.2612478733062744\n",
      "[Training Epoch 3] Batch 699, Loss 0.28198444843292236\n",
      "[Training Epoch 3] Batch 700, Loss 0.2971875071525574\n",
      "[Training Epoch 3] Batch 701, Loss 0.27576982975006104\n",
      "[Training Epoch 3] Batch 702, Loss 0.29069554805755615\n",
      "[Training Epoch 3] Batch 703, Loss 0.27448636293411255\n",
      "[Training Epoch 3] Batch 704, Loss 0.24872294068336487\n",
      "[Training Epoch 3] Batch 705, Loss 0.26108506321907043\n",
      "[Training Epoch 3] Batch 706, Loss 0.28651684522628784\n",
      "[Training Epoch 3] Batch 707, Loss 0.2908771336078644\n",
      "[Training Epoch 3] Batch 708, Loss 0.28205838799476624\n",
      "[Training Epoch 3] Batch 709, Loss 0.2684316635131836\n",
      "[Training Epoch 3] Batch 710, Loss 0.2801164984703064\n",
      "[Training Epoch 3] Batch 711, Loss 0.25980979204177856\n",
      "[Training Epoch 3] Batch 712, Loss 0.28425660729408264\n",
      "[Training Epoch 3] Batch 713, Loss 0.28904348611831665\n",
      "[Training Epoch 3] Batch 714, Loss 0.26691845059394836\n",
      "[Training Epoch 3] Batch 715, Loss 0.2650003731250763\n",
      "[Training Epoch 3] Batch 716, Loss 0.2660996913909912\n",
      "[Training Epoch 3] Batch 717, Loss 0.29052621126174927\n",
      "[Training Epoch 3] Batch 718, Loss 0.2773750126361847\n",
      "[Training Epoch 3] Batch 719, Loss 0.267949640750885\n",
      "[Training Epoch 3] Batch 720, Loss 0.2841894030570984\n",
      "[Training Epoch 3] Batch 721, Loss 0.2771509289741516\n",
      "[Training Epoch 3] Batch 722, Loss 0.28614920377731323\n",
      "[Training Epoch 3] Batch 723, Loss 0.2666659951210022\n",
      "[Training Epoch 3] Batch 724, Loss 0.25436124205589294\n",
      "[Training Epoch 3] Batch 725, Loss 0.29833561182022095\n",
      "[Training Epoch 3] Batch 726, Loss 0.26567038893699646\n",
      "[Training Epoch 3] Batch 727, Loss 0.2600494921207428\n",
      "[Training Epoch 3] Batch 728, Loss 0.2717795968055725\n",
      "[Training Epoch 3] Batch 729, Loss 0.29935920238494873\n",
      "[Training Epoch 3] Batch 730, Loss 0.22967112064361572\n",
      "[Training Epoch 3] Batch 731, Loss 0.29249608516693115\n",
      "[Training Epoch 3] Batch 732, Loss 0.27648627758026123\n",
      "[Training Epoch 3] Batch 733, Loss 0.2545926570892334\n",
      "[Training Epoch 3] Batch 734, Loss 0.26718640327453613\n",
      "[Training Epoch 3] Batch 735, Loss 0.2917483448982239\n",
      "[Training Epoch 3] Batch 736, Loss 0.26117971539497375\n",
      "[Training Epoch 3] Batch 737, Loss 0.27247148752212524\n",
      "[Training Epoch 3] Batch 738, Loss 0.27976861596107483\n",
      "[Training Epoch 3] Batch 739, Loss 0.2640818953514099\n",
      "[Training Epoch 3] Batch 740, Loss 0.2660283148288727\n",
      "[Training Epoch 3] Batch 741, Loss 0.2761283218860626\n",
      "[Training Epoch 3] Batch 742, Loss 0.3215993046760559\n",
      "[Training Epoch 3] Batch 743, Loss 0.26024776697158813\n",
      "[Training Epoch 3] Batch 744, Loss 0.2702386677265167\n",
      "[Training Epoch 3] Batch 745, Loss 0.28233569860458374\n",
      "[Training Epoch 3] Batch 746, Loss 0.2527614235877991\n",
      "[Training Epoch 3] Batch 747, Loss 0.27379268407821655\n",
      "[Training Epoch 3] Batch 748, Loss 0.2816174030303955\n",
      "[Training Epoch 3] Batch 749, Loss 0.26110774278640747\n",
      "[Training Epoch 3] Batch 750, Loss 0.26419615745544434\n",
      "[Training Epoch 3] Batch 751, Loss 0.26048851013183594\n",
      "[Training Epoch 3] Batch 752, Loss 0.2840990424156189\n",
      "[Training Epoch 3] Batch 753, Loss 0.271651029586792\n",
      "[Training Epoch 3] Batch 754, Loss 0.25272440910339355\n",
      "[Training Epoch 3] Batch 755, Loss 0.2448229193687439\n",
      "[Training Epoch 3] Batch 756, Loss 0.2607780992984772\n",
      "[Training Epoch 3] Batch 757, Loss 0.2516922056674957\n",
      "[Training Epoch 3] Batch 758, Loss 0.2825188636779785\n",
      "[Training Epoch 3] Batch 759, Loss 0.28698229789733887\n",
      "[Training Epoch 3] Batch 760, Loss 0.26962000131607056\n",
      "[Training Epoch 3] Batch 761, Loss 0.2674770653247833\n",
      "[Training Epoch 3] Batch 762, Loss 0.29750365018844604\n",
      "[Training Epoch 3] Batch 763, Loss 0.2633102238178253\n",
      "[Training Epoch 3] Batch 764, Loss 0.2751654386520386\n",
      "[Training Epoch 3] Batch 765, Loss 0.2656398415565491\n",
      "[Training Epoch 3] Batch 766, Loss 0.2661077380180359\n",
      "[Training Epoch 3] Batch 767, Loss 0.2565852999687195\n",
      "[Training Epoch 3] Batch 768, Loss 0.26490500569343567\n",
      "[Training Epoch 3] Batch 769, Loss 0.28080806136131287\n",
      "[Training Epoch 3] Batch 770, Loss 0.2561270296573639\n",
      "[Training Epoch 3] Batch 771, Loss 0.2706618010997772\n",
      "[Training Epoch 3] Batch 772, Loss 0.2647630572319031\n",
      "[Training Epoch 3] Batch 773, Loss 0.27206769585609436\n",
      "[Training Epoch 3] Batch 774, Loss 0.2689988613128662\n",
      "[Training Epoch 3] Batch 775, Loss 0.2584134340286255\n",
      "[Training Epoch 3] Batch 776, Loss 0.2503431737422943\n",
      "[Training Epoch 3] Batch 777, Loss 0.28774887323379517\n",
      "[Training Epoch 3] Batch 778, Loss 0.24240344762802124\n",
      "[Training Epoch 3] Batch 779, Loss 0.263868510723114\n",
      "[Training Epoch 3] Batch 780, Loss 0.258626252412796\n",
      "[Training Epoch 3] Batch 781, Loss 0.2735557556152344\n",
      "[Training Epoch 3] Batch 782, Loss 0.28435206413269043\n",
      "[Training Epoch 3] Batch 783, Loss 0.26979732513427734\n",
      "[Training Epoch 3] Batch 784, Loss 0.2727271616458893\n",
      "[Training Epoch 3] Batch 785, Loss 0.2470332682132721\n",
      "[Training Epoch 3] Batch 786, Loss 0.28130093216896057\n",
      "[Training Epoch 3] Batch 787, Loss 0.2529715299606323\n",
      "[Training Epoch 3] Batch 788, Loss 0.2710752487182617\n",
      "[Training Epoch 3] Batch 789, Loss 0.26623696088790894\n",
      "[Training Epoch 3] Batch 790, Loss 0.3021644353866577\n",
      "[Training Epoch 3] Batch 791, Loss 0.28701668977737427\n",
      "[Training Epoch 3] Batch 792, Loss 0.2712271809577942\n",
      "[Training Epoch 3] Batch 793, Loss 0.2879469096660614\n",
      "[Training Epoch 3] Batch 794, Loss 0.25108540058135986\n",
      "[Training Epoch 3] Batch 795, Loss 0.27766022086143494\n",
      "[Training Epoch 3] Batch 796, Loss 0.26333796977996826\n",
      "[Training Epoch 3] Batch 797, Loss 0.2303267866373062\n",
      "[Training Epoch 3] Batch 798, Loss 0.2938659191131592\n",
      "[Training Epoch 3] Batch 799, Loss 0.2648296356201172\n",
      "[Training Epoch 3] Batch 800, Loss 0.2921692132949829\n",
      "[Training Epoch 3] Batch 801, Loss 0.26405972242355347\n",
      "[Training Epoch 3] Batch 802, Loss 0.27283328771591187\n",
      "[Training Epoch 3] Batch 803, Loss 0.27236706018447876\n",
      "[Training Epoch 3] Batch 804, Loss 0.2811831831932068\n",
      "[Training Epoch 3] Batch 805, Loss 0.28511422872543335\n",
      "[Training Epoch 3] Batch 806, Loss 0.276920348405838\n",
      "[Training Epoch 3] Batch 807, Loss 0.2705433666706085\n",
      "[Training Epoch 3] Batch 808, Loss 0.2756215035915375\n",
      "[Training Epoch 3] Batch 809, Loss 0.28502312302589417\n",
      "[Training Epoch 3] Batch 810, Loss 0.26453840732574463\n",
      "[Training Epoch 3] Batch 811, Loss 0.2800150513648987\n",
      "[Training Epoch 3] Batch 812, Loss 0.284370481967926\n",
      "[Training Epoch 3] Batch 813, Loss 0.27423587441444397\n",
      "[Training Epoch 3] Batch 814, Loss 0.3172130286693573\n",
      "[Training Epoch 3] Batch 815, Loss 0.25598597526550293\n",
      "[Training Epoch 3] Batch 816, Loss 0.28071534633636475\n",
      "[Training Epoch 3] Batch 817, Loss 0.30594348907470703\n",
      "[Training Epoch 3] Batch 818, Loss 0.2621478736400604\n",
      "[Training Epoch 3] Batch 819, Loss 0.24927735328674316\n",
      "[Training Epoch 3] Batch 820, Loss 0.2589859962463379\n",
      "[Training Epoch 3] Batch 821, Loss 0.257942795753479\n",
      "[Training Epoch 3] Batch 822, Loss 0.27214813232421875\n",
      "[Training Epoch 3] Batch 823, Loss 0.2660364806652069\n",
      "[Training Epoch 3] Batch 824, Loss 0.2816476821899414\n",
      "[Training Epoch 3] Batch 825, Loss 0.25737008452415466\n",
      "[Training Epoch 3] Batch 826, Loss 0.2920761704444885\n",
      "[Training Epoch 3] Batch 827, Loss 0.2782514691352844\n",
      "[Training Epoch 3] Batch 828, Loss 0.2636713981628418\n",
      "[Training Epoch 3] Batch 829, Loss 0.2593536972999573\n",
      "[Training Epoch 3] Batch 830, Loss 0.24895623326301575\n",
      "[Training Epoch 3] Batch 831, Loss 0.2728096842765808\n",
      "[Training Epoch 3] Batch 832, Loss 0.2909262180328369\n",
      "[Training Epoch 3] Batch 833, Loss 0.28465503454208374\n",
      "[Training Epoch 3] Batch 834, Loss 0.29815658926963806\n",
      "[Training Epoch 3] Batch 835, Loss 0.27389615774154663\n",
      "[Training Epoch 3] Batch 836, Loss 0.2648829221725464\n",
      "[Training Epoch 3] Batch 837, Loss 0.2919475734233856\n",
      "[Training Epoch 3] Batch 838, Loss 0.29151904582977295\n",
      "[Training Epoch 3] Batch 839, Loss 0.2789457440376282\n",
      "[Training Epoch 3] Batch 840, Loss 0.25985535979270935\n",
      "[Training Epoch 3] Batch 841, Loss 0.27587443590164185\n",
      "[Training Epoch 3] Batch 842, Loss 0.2645593285560608\n",
      "[Training Epoch 3] Batch 843, Loss 0.2910732328891754\n",
      "[Training Epoch 3] Batch 844, Loss 0.28392061591148376\n",
      "[Training Epoch 3] Batch 845, Loss 0.29528599977493286\n",
      "[Training Epoch 3] Batch 846, Loss 0.2727510333061218\n",
      "[Training Epoch 3] Batch 847, Loss 0.2563606798648834\n",
      "[Training Epoch 3] Batch 848, Loss 0.2884170711040497\n",
      "[Training Epoch 3] Batch 849, Loss 0.26250797510147095\n",
      "[Training Epoch 3] Batch 850, Loss 0.28861314058303833\n",
      "[Training Epoch 3] Batch 851, Loss 0.2432401180267334\n",
      "[Training Epoch 3] Batch 852, Loss 0.286124587059021\n",
      "[Training Epoch 3] Batch 853, Loss 0.283369243144989\n",
      "[Training Epoch 3] Batch 854, Loss 0.2621135711669922\n",
      "[Training Epoch 3] Batch 855, Loss 0.28284555673599243\n",
      "[Training Epoch 3] Batch 856, Loss 0.2666201591491699\n",
      "[Training Epoch 3] Batch 857, Loss 0.2563263773918152\n",
      "[Training Epoch 3] Batch 858, Loss 0.25935354828834534\n",
      "[Training Epoch 3] Batch 859, Loss 0.2529721260070801\n",
      "[Training Epoch 3] Batch 860, Loss 0.2978857159614563\n",
      "[Training Epoch 3] Batch 861, Loss 0.27268165349960327\n",
      "[Training Epoch 3] Batch 862, Loss 0.26709312200546265\n",
      "[Training Epoch 3] Batch 863, Loss 0.2716734707355499\n",
      "[Training Epoch 3] Batch 864, Loss 0.3038611114025116\n",
      "[Training Epoch 3] Batch 865, Loss 0.28463056683540344\n",
      "[Training Epoch 3] Batch 866, Loss 0.26607364416122437\n",
      "[Training Epoch 3] Batch 867, Loss 0.24810650944709778\n",
      "[Training Epoch 3] Batch 868, Loss 0.2572821378707886\n",
      "[Training Epoch 3] Batch 869, Loss 0.24458497762680054\n",
      "[Training Epoch 3] Batch 870, Loss 0.26239123940467834\n",
      "[Training Epoch 3] Batch 871, Loss 0.27004826068878174\n",
      "[Training Epoch 3] Batch 872, Loss 0.2857012152671814\n",
      "[Training Epoch 3] Batch 873, Loss 0.2713530361652374\n",
      "[Training Epoch 3] Batch 874, Loss 0.299832820892334\n",
      "[Training Epoch 3] Batch 875, Loss 0.2576557397842407\n",
      "[Training Epoch 3] Batch 876, Loss 0.28955549001693726\n",
      "[Training Epoch 3] Batch 877, Loss 0.2934930920600891\n",
      "[Training Epoch 3] Batch 878, Loss 0.2501649260520935\n",
      "[Training Epoch 3] Batch 879, Loss 0.2782236337661743\n",
      "[Training Epoch 3] Batch 880, Loss 0.24094529449939728\n",
      "[Training Epoch 3] Batch 881, Loss 0.2506747841835022\n",
      "[Training Epoch 3] Batch 882, Loss 0.29108813405036926\n",
      "[Training Epoch 3] Batch 883, Loss 0.2520793676376343\n",
      "[Training Epoch 3] Batch 884, Loss 0.2870018482208252\n",
      "[Training Epoch 3] Batch 885, Loss 0.2511299252510071\n",
      "[Training Epoch 3] Batch 886, Loss 0.2589089870452881\n",
      "[Training Epoch 3] Batch 887, Loss 0.2713962197303772\n",
      "[Training Epoch 3] Batch 888, Loss 0.2976730465888977\n",
      "[Training Epoch 3] Batch 889, Loss 0.25706976652145386\n",
      "[Training Epoch 3] Batch 890, Loss 0.2966824173927307\n",
      "[Training Epoch 3] Batch 891, Loss 0.2807486653327942\n",
      "[Training Epoch 3] Batch 892, Loss 0.289825439453125\n",
      "[Training Epoch 3] Batch 893, Loss 0.2794281244277954\n",
      "[Training Epoch 3] Batch 894, Loss 0.2640436887741089\n",
      "[Training Epoch 3] Batch 895, Loss 0.286956250667572\n",
      "[Training Epoch 3] Batch 896, Loss 0.2701851725578308\n",
      "[Training Epoch 3] Batch 897, Loss 0.2736061215400696\n",
      "[Training Epoch 3] Batch 898, Loss 0.29060983657836914\n",
      "[Training Epoch 3] Batch 899, Loss 0.28529998660087585\n",
      "[Training Epoch 3] Batch 900, Loss 0.26408112049102783\n",
      "[Training Epoch 3] Batch 901, Loss 0.29160386323928833\n",
      "[Training Epoch 3] Batch 902, Loss 0.2497868537902832\n",
      "[Training Epoch 3] Batch 903, Loss 0.27109503746032715\n",
      "[Training Epoch 3] Batch 904, Loss 0.2940543293952942\n",
      "[Training Epoch 3] Batch 905, Loss 0.26382237672805786\n",
      "[Training Epoch 3] Batch 906, Loss 0.27569854259490967\n",
      "[Training Epoch 3] Batch 907, Loss 0.26450109481811523\n",
      "[Training Epoch 3] Batch 908, Loss 0.2647753357887268\n",
      "[Training Epoch 3] Batch 909, Loss 0.2774839997291565\n",
      "[Training Epoch 3] Batch 910, Loss 0.2564011514186859\n",
      "[Training Epoch 3] Batch 911, Loss 0.30109691619873047\n",
      "[Training Epoch 3] Batch 912, Loss 0.27357685565948486\n",
      "[Training Epoch 3] Batch 913, Loss 0.2443152815103531\n",
      "[Training Epoch 3] Batch 914, Loss 0.26259997487068176\n",
      "[Training Epoch 3] Batch 915, Loss 0.2819046974182129\n",
      "[Training Epoch 3] Batch 916, Loss 0.2578344941139221\n",
      "[Training Epoch 3] Batch 917, Loss 0.25303614139556885\n",
      "[Training Epoch 3] Batch 918, Loss 0.2875474989414215\n",
      "[Training Epoch 3] Batch 919, Loss 0.2490624338388443\n",
      "[Training Epoch 3] Batch 920, Loss 0.26505622267723083\n",
      "[Training Epoch 3] Batch 921, Loss 0.2771494388580322\n",
      "[Training Epoch 3] Batch 922, Loss 0.2713508605957031\n",
      "[Training Epoch 3] Batch 923, Loss 0.27183806896209717\n",
      "[Training Epoch 3] Batch 924, Loss 0.2933056354522705\n",
      "[Training Epoch 3] Batch 925, Loss 0.2651025056838989\n",
      "[Training Epoch 3] Batch 926, Loss 0.2689078450202942\n",
      "[Training Epoch 3] Batch 927, Loss 0.2644805610179901\n",
      "[Training Epoch 3] Batch 928, Loss 0.28304436802864075\n",
      "[Training Epoch 3] Batch 929, Loss 0.28073516488075256\n",
      "[Training Epoch 3] Batch 930, Loss 0.2938136160373688\n",
      "[Training Epoch 3] Batch 931, Loss 0.23551753163337708\n",
      "[Training Epoch 3] Batch 932, Loss 0.2675101161003113\n",
      "[Training Epoch 3] Batch 933, Loss 0.2555423974990845\n",
      "[Training Epoch 3] Batch 934, Loss 0.26518648862838745\n",
      "[Training Epoch 3] Batch 935, Loss 0.2800755500793457\n",
      "[Training Epoch 3] Batch 936, Loss 0.27123311161994934\n",
      "[Training Epoch 3] Batch 937, Loss 0.26570308208465576\n",
      "[Training Epoch 3] Batch 938, Loss 0.293146550655365\n",
      "[Training Epoch 3] Batch 939, Loss 0.2735895812511444\n",
      "[Training Epoch 3] Batch 940, Loss 0.250556081533432\n",
      "[Training Epoch 3] Batch 941, Loss 0.264883816242218\n",
      "[Training Epoch 3] Batch 942, Loss 0.2569718360900879\n",
      "[Training Epoch 3] Batch 943, Loss 0.26156601309776306\n",
      "[Training Epoch 3] Batch 944, Loss 0.2596149444580078\n",
      "[Training Epoch 3] Batch 945, Loss 0.29166939854621887\n",
      "[Training Epoch 3] Batch 946, Loss 0.2775684893131256\n",
      "[Training Epoch 3] Batch 947, Loss 0.28065285086631775\n",
      "[Training Epoch 3] Batch 948, Loss 0.272914320230484\n",
      "[Training Epoch 3] Batch 949, Loss 0.2610885500907898\n",
      "[Training Epoch 3] Batch 950, Loss 0.2614838480949402\n",
      "[Training Epoch 3] Batch 951, Loss 0.2849825620651245\n",
      "[Training Epoch 3] Batch 952, Loss 0.28586095571517944\n",
      "[Training Epoch 3] Batch 953, Loss 0.27332603931427\n",
      "[Training Epoch 3] Batch 954, Loss 0.2670901119709015\n",
      "[Training Epoch 3] Batch 955, Loss 0.27342286705970764\n",
      "[Training Epoch 3] Batch 956, Loss 0.2596896290779114\n",
      "[Training Epoch 3] Batch 957, Loss 0.259784996509552\n",
      "[Training Epoch 3] Batch 958, Loss 0.28041550517082214\n",
      "[Training Epoch 3] Batch 959, Loss 0.27316856384277344\n",
      "[Training Epoch 3] Batch 960, Loss 0.2308083176612854\n",
      "[Training Epoch 3] Batch 961, Loss 0.31276968121528625\n",
      "[Training Epoch 3] Batch 962, Loss 0.2914951741695404\n",
      "[Training Epoch 3] Batch 963, Loss 0.26632195711135864\n",
      "[Training Epoch 3] Batch 964, Loss 0.2674204409122467\n",
      "[Training Epoch 3] Batch 965, Loss 0.28629493713378906\n",
      "[Training Epoch 3] Batch 966, Loss 0.31045758724212646\n",
      "[Training Epoch 3] Batch 967, Loss 0.2742087244987488\n",
      "[Training Epoch 3] Batch 968, Loss 0.29132771492004395\n",
      "[Training Epoch 3] Batch 969, Loss 0.27469536662101746\n",
      "[Training Epoch 3] Batch 970, Loss 0.2716827690601349\n",
      "[Training Epoch 3] Batch 971, Loss 0.24950815737247467\n",
      "[Training Epoch 3] Batch 972, Loss 0.2768751382827759\n",
      "[Training Epoch 3] Batch 973, Loss 0.257720410823822\n",
      "[Training Epoch 3] Batch 974, Loss 0.28239011764526367\n",
      "[Training Epoch 3] Batch 975, Loss 0.2823355495929718\n",
      "[Training Epoch 3] Batch 976, Loss 0.254592627286911\n",
      "[Training Epoch 3] Batch 977, Loss 0.2617383599281311\n",
      "[Training Epoch 3] Batch 978, Loss 0.25832855701446533\n",
      "[Training Epoch 3] Batch 979, Loss 0.27608874440193176\n",
      "[Training Epoch 3] Batch 980, Loss 0.27262187004089355\n",
      "[Training Epoch 3] Batch 981, Loss 0.2681867480278015\n",
      "[Training Epoch 3] Batch 982, Loss 0.2671958804130554\n",
      "[Training Epoch 3] Batch 983, Loss 0.2556798458099365\n",
      "[Training Epoch 3] Batch 984, Loss 0.26931872963905334\n",
      "[Training Epoch 3] Batch 985, Loss 0.27985823154449463\n",
      "[Training Epoch 3] Batch 986, Loss 0.25489529967308044\n",
      "[Training Epoch 3] Batch 987, Loss 0.25886374711990356\n",
      "[Training Epoch 3] Batch 988, Loss 0.275897353887558\n",
      "[Training Epoch 3] Batch 989, Loss 0.29997989535331726\n",
      "[Training Epoch 3] Batch 990, Loss 0.2787623405456543\n",
      "[Training Epoch 3] Batch 991, Loss 0.2837449312210083\n",
      "[Training Epoch 3] Batch 992, Loss 0.28922396898269653\n",
      "[Training Epoch 3] Batch 993, Loss 0.2504868805408478\n",
      "[Training Epoch 3] Batch 994, Loss 0.28703010082244873\n",
      "[Training Epoch 3] Batch 995, Loss 0.27442073822021484\n",
      "[Training Epoch 3] Batch 996, Loss 0.2894044816493988\n",
      "[Training Epoch 3] Batch 997, Loss 0.26733189821243286\n",
      "[Training Epoch 3] Batch 998, Loss 0.2643924355506897\n",
      "[Training Epoch 3] Batch 999, Loss 0.25887125730514526\n",
      "[Training Epoch 3] Batch 1000, Loss 0.25024592876434326\n",
      "[Training Epoch 3] Batch 1001, Loss 0.2841261029243469\n",
      "[Training Epoch 3] Batch 1002, Loss 0.24080684781074524\n",
      "[Training Epoch 3] Batch 1003, Loss 0.2640717625617981\n",
      "[Training Epoch 3] Batch 1004, Loss 0.27245721220970154\n",
      "[Training Epoch 3] Batch 1005, Loss 0.2657838463783264\n",
      "[Training Epoch 3] Batch 1006, Loss 0.2750055491924286\n",
      "[Training Epoch 3] Batch 1007, Loss 0.28201210498809814\n",
      "[Training Epoch 3] Batch 1008, Loss 0.26699066162109375\n",
      "[Training Epoch 3] Batch 1009, Loss 0.2667883634567261\n",
      "[Training Epoch 3] Batch 1010, Loss 0.2767272889614105\n",
      "[Training Epoch 3] Batch 1011, Loss 0.2740701138973236\n",
      "[Training Epoch 3] Batch 1012, Loss 0.2749439477920532\n",
      "[Training Epoch 3] Batch 1013, Loss 0.2602806091308594\n",
      "[Training Epoch 3] Batch 1014, Loss 0.2770448327064514\n",
      "[Training Epoch 3] Batch 1015, Loss 0.2629562020301819\n",
      "[Training Epoch 3] Batch 1016, Loss 0.29421767592430115\n",
      "[Training Epoch 3] Batch 1017, Loss 0.27367937564849854\n",
      "[Training Epoch 3] Batch 1018, Loss 0.28101518750190735\n",
      "[Training Epoch 3] Batch 1019, Loss 0.2786387503147125\n",
      "[Training Epoch 3] Batch 1020, Loss 0.2507268190383911\n",
      "[Training Epoch 3] Batch 1021, Loss 0.2269040197134018\n",
      "[Training Epoch 3] Batch 1022, Loss 0.25670671463012695\n",
      "[Training Epoch 3] Batch 1023, Loss 0.27775585651397705\n",
      "[Training Epoch 3] Batch 1024, Loss 0.2808247208595276\n",
      "[Training Epoch 3] Batch 1025, Loss 0.26315224170684814\n",
      "[Training Epoch 3] Batch 1026, Loss 0.2964131534099579\n",
      "[Training Epoch 3] Batch 1027, Loss 0.2496633529663086\n",
      "[Training Epoch 3] Batch 1028, Loss 0.26592692732810974\n",
      "[Training Epoch 3] Batch 1029, Loss 0.2828599214553833\n",
      "[Training Epoch 3] Batch 1030, Loss 0.2637338936328888\n",
      "[Training Epoch 3] Batch 1031, Loss 0.2775226831436157\n",
      "[Training Epoch 3] Batch 1032, Loss 0.2999803423881531\n",
      "[Training Epoch 3] Batch 1033, Loss 0.2692801058292389\n",
      "[Training Epoch 3] Batch 1034, Loss 0.25098729133605957\n",
      "[Training Epoch 3] Batch 1035, Loss 0.24636968970298767\n",
      "[Training Epoch 3] Batch 1036, Loss 0.25909197330474854\n",
      "[Training Epoch 3] Batch 1037, Loss 0.2778446078300476\n",
      "[Training Epoch 3] Batch 1038, Loss 0.28127655386924744\n",
      "[Training Epoch 3] Batch 1039, Loss 0.2850651144981384\n",
      "[Training Epoch 3] Batch 1040, Loss 0.25805193185806274\n",
      "[Training Epoch 3] Batch 1041, Loss 0.2673596143722534\n",
      "[Training Epoch 3] Batch 1042, Loss 0.27787914872169495\n",
      "[Training Epoch 3] Batch 1043, Loss 0.277843713760376\n",
      "[Training Epoch 3] Batch 1044, Loss 0.28043973445892334\n",
      "[Training Epoch 3] Batch 1045, Loss 0.2876822352409363\n",
      "[Training Epoch 3] Batch 1046, Loss 0.2834010422229767\n",
      "[Training Epoch 3] Batch 1047, Loss 0.30389606952667236\n",
      "[Training Epoch 3] Batch 1048, Loss 0.254699170589447\n",
      "[Training Epoch 3] Batch 1049, Loss 0.2729676365852356\n",
      "[Training Epoch 3] Batch 1050, Loss 0.2800447940826416\n",
      "[Training Epoch 3] Batch 1051, Loss 0.26799580454826355\n",
      "[Training Epoch 3] Batch 1052, Loss 0.26912155747413635\n",
      "[Training Epoch 3] Batch 1053, Loss 0.25943872332572937\n",
      "[Training Epoch 3] Batch 1054, Loss 0.2402729094028473\n",
      "[Training Epoch 3] Batch 1055, Loss 0.2838655114173889\n",
      "[Training Epoch 3] Batch 1056, Loss 0.27054929733276367\n",
      "[Training Epoch 3] Batch 1057, Loss 0.2510038912296295\n",
      "[Training Epoch 3] Batch 1058, Loss 0.2834651470184326\n",
      "[Training Epoch 3] Batch 1059, Loss 0.27374759316444397\n",
      "[Training Epoch 3] Batch 1060, Loss 0.24403241276741028\n",
      "[Training Epoch 3] Batch 1061, Loss 0.26833200454711914\n",
      "[Training Epoch 3] Batch 1062, Loss 0.2483888566493988\n",
      "[Training Epoch 3] Batch 1063, Loss 0.2760036587715149\n",
      "[Training Epoch 3] Batch 1064, Loss 0.2703842222690582\n",
      "[Training Epoch 3] Batch 1065, Loss 0.254647433757782\n",
      "[Training Epoch 3] Batch 1066, Loss 0.2718350291252136\n",
      "[Training Epoch 3] Batch 1067, Loss 0.26537346839904785\n",
      "[Training Epoch 3] Batch 1068, Loss 0.2630555033683777\n",
      "[Training Epoch 3] Batch 1069, Loss 0.2716597318649292\n",
      "[Training Epoch 3] Batch 1070, Loss 0.24448344111442566\n",
      "[Training Epoch 3] Batch 1071, Loss 0.2485119104385376\n",
      "[Training Epoch 3] Batch 1072, Loss 0.29498034715652466\n",
      "[Training Epoch 3] Batch 1073, Loss 0.2830207943916321\n",
      "[Training Epoch 3] Batch 1074, Loss 0.26773908734321594\n",
      "[Training Epoch 3] Batch 1075, Loss 0.27821534872055054\n",
      "[Training Epoch 3] Batch 1076, Loss 0.2886938452720642\n",
      "[Training Epoch 3] Batch 1077, Loss 0.255248486995697\n",
      "[Training Epoch 3] Batch 1078, Loss 0.2925185561180115\n",
      "[Training Epoch 3] Batch 1079, Loss 0.27148371934890747\n",
      "[Training Epoch 3] Batch 1080, Loss 0.29120883345603943\n",
      "[Training Epoch 3] Batch 1081, Loss 0.27391257882118225\n",
      "[Training Epoch 3] Batch 1082, Loss 0.2677592635154724\n",
      "[Training Epoch 3] Batch 1083, Loss 0.29147255420684814\n",
      "[Training Epoch 3] Batch 1084, Loss 0.2838593125343323\n",
      "[Training Epoch 3] Batch 1085, Loss 0.2731323838233948\n",
      "[Training Epoch 3] Batch 1086, Loss 0.255135715007782\n",
      "[Training Epoch 3] Batch 1087, Loss 0.26813334226608276\n",
      "[Training Epoch 3] Batch 1088, Loss 0.27995097637176514\n",
      "[Training Epoch 3] Batch 1089, Loss 0.2774994969367981\n",
      "[Training Epoch 3] Batch 1090, Loss 0.2516680657863617\n",
      "[Training Epoch 3] Batch 1091, Loss 0.2905007004737854\n",
      "[Training Epoch 3] Batch 1092, Loss 0.28493449091911316\n",
      "[Training Epoch 3] Batch 1093, Loss 0.26616352796554565\n",
      "[Training Epoch 3] Batch 1094, Loss 0.2882865071296692\n",
      "[Training Epoch 3] Batch 1095, Loss 0.26337915658950806\n",
      "[Training Epoch 3] Batch 1096, Loss 0.2677502930164337\n",
      "[Training Epoch 3] Batch 1097, Loss 0.24695423245429993\n",
      "[Training Epoch 3] Batch 1098, Loss 0.27629154920578003\n",
      "[Training Epoch 3] Batch 1099, Loss 0.27410292625427246\n",
      "[Training Epoch 3] Batch 1100, Loss 0.3051757216453552\n",
      "[Training Epoch 3] Batch 1101, Loss 0.2840394079685211\n",
      "[Training Epoch 3] Batch 1102, Loss 0.26412278413772583\n",
      "[Training Epoch 3] Batch 1103, Loss 0.2507809102535248\n",
      "[Training Epoch 3] Batch 1104, Loss 0.27987077832221985\n",
      "[Training Epoch 3] Batch 1105, Loss 0.2760348916053772\n",
      "[Training Epoch 3] Batch 1106, Loss 0.2951444685459137\n",
      "[Training Epoch 3] Batch 1107, Loss 0.29919978976249695\n",
      "[Training Epoch 3] Batch 1108, Loss 0.26568663120269775\n",
      "[Training Epoch 3] Batch 1109, Loss 0.27769580483436584\n",
      "[Training Epoch 3] Batch 1110, Loss 0.2774851322174072\n",
      "[Training Epoch 3] Batch 1111, Loss 0.27919191122055054\n",
      "[Training Epoch 3] Batch 1112, Loss 0.24185913801193237\n",
      "[Training Epoch 3] Batch 1113, Loss 0.2506408989429474\n",
      "[Training Epoch 3] Batch 1114, Loss 0.266262948513031\n",
      "[Training Epoch 3] Batch 1115, Loss 0.25376707315444946\n",
      "[Training Epoch 3] Batch 1116, Loss 0.25437474250793457\n",
      "[Training Epoch 3] Batch 1117, Loss 0.2843865156173706\n",
      "[Training Epoch 3] Batch 1118, Loss 0.27720531821250916\n",
      "[Training Epoch 3] Batch 1119, Loss 0.2741544842720032\n",
      "[Training Epoch 3] Batch 1120, Loss 0.2623539865016937\n",
      "[Training Epoch 3] Batch 1121, Loss 0.25332966446876526\n",
      "[Training Epoch 3] Batch 1122, Loss 0.27530384063720703\n",
      "[Training Epoch 3] Batch 1123, Loss 0.24686725437641144\n",
      "[Training Epoch 3] Batch 1124, Loss 0.2581060826778412\n",
      "[Training Epoch 3] Batch 1125, Loss 0.2727968096733093\n",
      "[Training Epoch 3] Batch 1126, Loss 0.2821236848831177\n",
      "[Training Epoch 3] Batch 1127, Loss 0.26147210597991943\n",
      "[Training Epoch 3] Batch 1128, Loss 0.2479211986064911\n",
      "[Training Epoch 3] Batch 1129, Loss 0.256679892539978\n",
      "[Training Epoch 3] Batch 1130, Loss 0.32168126106262207\n",
      "[Training Epoch 3] Batch 1131, Loss 0.2999430298805237\n",
      "[Training Epoch 3] Batch 1132, Loss 0.28008323907852173\n",
      "[Training Epoch 3] Batch 1133, Loss 0.27190306782722473\n",
      "[Training Epoch 3] Batch 1134, Loss 0.2726452350616455\n",
      "[Training Epoch 3] Batch 1135, Loss 0.2930567264556885\n",
      "[Training Epoch 3] Batch 1136, Loss 0.29271286725997925\n",
      "[Training Epoch 3] Batch 1137, Loss 0.26903036236763\n",
      "[Training Epoch 3] Batch 1138, Loss 0.27492791414260864\n",
      "[Training Epoch 3] Batch 1139, Loss 0.2627527713775635\n",
      "[Training Epoch 3] Batch 1140, Loss 0.28574663400650024\n",
      "[Training Epoch 3] Batch 1141, Loss 0.25303542613983154\n",
      "[Training Epoch 3] Batch 1142, Loss 0.28039854764938354\n",
      "[Training Epoch 3] Batch 1143, Loss 0.2880600690841675\n",
      "[Training Epoch 3] Batch 1144, Loss 0.2929989695549011\n",
      "[Training Epoch 3] Batch 1145, Loss 0.2840239405632019\n",
      "[Training Epoch 3] Batch 1146, Loss 0.2577321529388428\n",
      "[Training Epoch 3] Batch 1147, Loss 0.2792212963104248\n",
      "[Training Epoch 3] Batch 1148, Loss 0.24690353870391846\n",
      "[Training Epoch 3] Batch 1149, Loss 0.25439712405204773\n",
      "[Training Epoch 3] Batch 1150, Loss 0.30662763118743896\n",
      "[Training Epoch 3] Batch 1151, Loss 0.2710554599761963\n",
      "[Training Epoch 3] Batch 1152, Loss 0.2692466378211975\n",
      "[Training Epoch 3] Batch 1153, Loss 0.2608112692832947\n",
      "[Training Epoch 3] Batch 1154, Loss 0.273978590965271\n",
      "[Training Epoch 3] Batch 1155, Loss 0.29461103677749634\n",
      "[Training Epoch 3] Batch 1156, Loss 0.26441872119903564\n",
      "[Training Epoch 3] Batch 1157, Loss 0.2780895233154297\n",
      "[Training Epoch 3] Batch 1158, Loss 0.2603629231452942\n",
      "[Training Epoch 3] Batch 1159, Loss 0.2918453812599182\n",
      "[Training Epoch 3] Batch 1160, Loss 0.2904531955718994\n",
      "[Training Epoch 3] Batch 1161, Loss 0.23183999955654144\n",
      "[Training Epoch 3] Batch 1162, Loss 0.2847036123275757\n",
      "[Training Epoch 3] Batch 1163, Loss 0.2683544456958771\n",
      "[Training Epoch 3] Batch 1164, Loss 0.2670700252056122\n",
      "[Training Epoch 3] Batch 1165, Loss 0.2768431305885315\n",
      "[Training Epoch 3] Batch 1166, Loss 0.2692759931087494\n",
      "[Training Epoch 3] Batch 1167, Loss 0.2817641496658325\n",
      "[Training Epoch 3] Batch 1168, Loss 0.2570022940635681\n",
      "[Training Epoch 3] Batch 1169, Loss 0.26684147119522095\n",
      "[Training Epoch 3] Batch 1170, Loss 0.26990196108818054\n",
      "[Training Epoch 3] Batch 1171, Loss 0.2738526463508606\n",
      "[Training Epoch 3] Batch 1172, Loss 0.3017317056655884\n",
      "[Training Epoch 3] Batch 1173, Loss 0.27530691027641296\n",
      "[Training Epoch 3] Batch 1174, Loss 0.27099016308784485\n",
      "[Training Epoch 3] Batch 1175, Loss 0.24335142970085144\n",
      "[Training Epoch 3] Batch 1176, Loss 0.27331066131591797\n",
      "[Training Epoch 3] Batch 1177, Loss 0.27211371064186096\n",
      "[Training Epoch 3] Batch 1178, Loss 0.2665473222732544\n",
      "[Training Epoch 3] Batch 1179, Loss 0.260994553565979\n",
      "[Training Epoch 3] Batch 1180, Loss 0.2848725914955139\n",
      "[Training Epoch 3] Batch 1181, Loss 0.252080500125885\n",
      "[Training Epoch 3] Batch 1182, Loss 0.2614850401878357\n",
      "[Training Epoch 3] Batch 1183, Loss 0.2835749387741089\n",
      "[Training Epoch 3] Batch 1184, Loss 0.26570847630500793\n",
      "[Training Epoch 3] Batch 1185, Loss 0.26155057549476624\n",
      "[Training Epoch 3] Batch 1186, Loss 0.2820095121860504\n",
      "[Training Epoch 3] Batch 1187, Loss 0.2842505872249603\n",
      "[Training Epoch 3] Batch 1188, Loss 0.29002612829208374\n",
      "[Training Epoch 3] Batch 1189, Loss 0.2959979772567749\n",
      "[Training Epoch 3] Batch 1190, Loss 0.2722806930541992\n",
      "[Training Epoch 3] Batch 1191, Loss 0.27895259857177734\n",
      "[Training Epoch 3] Batch 1192, Loss 0.27872973680496216\n",
      "[Training Epoch 3] Batch 1193, Loss 0.24615910649299622\n",
      "[Training Epoch 3] Batch 1194, Loss 0.24686941504478455\n",
      "[Training Epoch 3] Batch 1195, Loss 0.262983500957489\n",
      "[Training Epoch 3] Batch 1196, Loss 0.29935091733932495\n",
      "[Training Epoch 3] Batch 1197, Loss 0.2647736966609955\n",
      "[Training Epoch 3] Batch 1198, Loss 0.2757956385612488\n",
      "[Training Epoch 3] Batch 1199, Loss 0.270558625459671\n",
      "[Training Epoch 3] Batch 1200, Loss 0.29712679982185364\n",
      "[Training Epoch 3] Batch 1201, Loss 0.2623681426048279\n",
      "[Training Epoch 3] Batch 1202, Loss 0.3001154661178589\n",
      "[Training Epoch 3] Batch 1203, Loss 0.2715744972229004\n",
      "[Training Epoch 3] Batch 1204, Loss 0.2752649486064911\n",
      "[Training Epoch 3] Batch 1205, Loss 0.2668851613998413\n",
      "[Training Epoch 3] Batch 1206, Loss 0.2466159611940384\n",
      "[Training Epoch 3] Batch 1207, Loss 0.26255735754966736\n",
      "[Training Epoch 3] Batch 1208, Loss 0.2861144244670868\n",
      "[Training Epoch 3] Batch 1209, Loss 0.2521612048149109\n",
      "[Training Epoch 3] Batch 1210, Loss 0.2792779505252838\n",
      "[Training Epoch 3] Batch 1211, Loss 0.2916390597820282\n",
      "[Training Epoch 3] Batch 1212, Loss 0.2637946605682373\n",
      "[Training Epoch 3] Batch 1213, Loss 0.29173505306243896\n",
      "[Training Epoch 3] Batch 1214, Loss 0.2713853120803833\n",
      "[Training Epoch 3] Batch 1215, Loss 0.26317790150642395\n",
      "[Training Epoch 3] Batch 1216, Loss 0.26894983649253845\n",
      "[Training Epoch 3] Batch 1217, Loss 0.25702863931655884\n",
      "[Training Epoch 3] Batch 1218, Loss 0.2696343660354614\n",
      "[Training Epoch 3] Batch 1219, Loss 0.25889262557029724\n",
      "[Training Epoch 3] Batch 1220, Loss 0.28300732374191284\n",
      "[Training Epoch 3] Batch 1221, Loss 0.25239378213882446\n",
      "[Training Epoch 3] Batch 1222, Loss 0.2660791277885437\n",
      "[Training Epoch 3] Batch 1223, Loss 0.26733845472335815\n",
      "[Training Epoch 3] Batch 1224, Loss 0.30366915464401245\n",
      "[Training Epoch 3] Batch 1225, Loss 0.2454925924539566\n",
      "[Training Epoch 3] Batch 1226, Loss 0.2909204661846161\n",
      "[Training Epoch 3] Batch 1227, Loss 0.29711630940437317\n",
      "[Training Epoch 3] Batch 1228, Loss 0.2622588276863098\n",
      "[Training Epoch 3] Batch 1229, Loss 0.2623405456542969\n",
      "[Training Epoch 3] Batch 1230, Loss 0.29676583409309387\n",
      "[Training Epoch 3] Batch 1231, Loss 0.287982702255249\n",
      "[Training Epoch 3] Batch 1232, Loss 0.27213209867477417\n",
      "[Training Epoch 3] Batch 1233, Loss 0.2721223831176758\n",
      "[Training Epoch 3] Batch 1234, Loss 0.3030173182487488\n",
      "[Training Epoch 3] Batch 1235, Loss 0.25331953167915344\n",
      "[Training Epoch 3] Batch 1236, Loss 0.2570890188217163\n",
      "[Training Epoch 3] Batch 1237, Loss 0.29106414318084717\n",
      "[Training Epoch 3] Batch 1238, Loss 0.28792405128479004\n",
      "[Training Epoch 3] Batch 1239, Loss 0.27059125900268555\n",
      "[Training Epoch 3] Batch 1240, Loss 0.25923484563827515\n",
      "[Training Epoch 3] Batch 1241, Loss 0.282853901386261\n",
      "[Training Epoch 3] Batch 1242, Loss 0.26437246799468994\n",
      "[Training Epoch 3] Batch 1243, Loss 0.2608160376548767\n",
      "[Training Epoch 3] Batch 1244, Loss 0.2842441499233246\n",
      "[Training Epoch 3] Batch 1245, Loss 0.2713513970375061\n",
      "[Training Epoch 3] Batch 1246, Loss 0.24263399839401245\n",
      "[Training Epoch 3] Batch 1247, Loss 0.2596431076526642\n",
      "[Training Epoch 3] Batch 1248, Loss 0.2862768769264221\n",
      "[Training Epoch 3] Batch 1249, Loss 0.2705821394920349\n",
      "[Training Epoch 3] Batch 1250, Loss 0.27075454592704773\n",
      "[Training Epoch 3] Batch 1251, Loss 0.2883169651031494\n",
      "[Training Epoch 3] Batch 1252, Loss 0.2819172441959381\n",
      "[Training Epoch 3] Batch 1253, Loss 0.2854248881340027\n",
      "[Training Epoch 3] Batch 1254, Loss 0.2782783806324005\n",
      "[Training Epoch 3] Batch 1255, Loss 0.26251477003097534\n",
      "[Training Epoch 3] Batch 1256, Loss 0.2600407004356384\n",
      "[Training Epoch 3] Batch 1257, Loss 0.3041403293609619\n",
      "[Training Epoch 3] Batch 1258, Loss 0.2627754807472229\n",
      "[Training Epoch 3] Batch 1259, Loss 0.27954062819480896\n",
      "[Training Epoch 3] Batch 1260, Loss 0.2527414560317993\n",
      "[Training Epoch 3] Batch 1261, Loss 0.2899421453475952\n",
      "[Training Epoch 3] Batch 1262, Loss 0.2872505187988281\n",
      "[Training Epoch 3] Batch 1263, Loss 0.26049894094467163\n",
      "[Training Epoch 3] Batch 1264, Loss 0.26599106192588806\n",
      "[Training Epoch 3] Batch 1265, Loss 0.25314512848854065\n",
      "[Training Epoch 3] Batch 1266, Loss 0.27943673729896545\n",
      "[Training Epoch 3] Batch 1267, Loss 0.2635560631752014\n",
      "[Training Epoch 3] Batch 1268, Loss 0.2503817677497864\n",
      "[Training Epoch 3] Batch 1269, Loss 0.2764170169830322\n",
      "[Training Epoch 3] Batch 1270, Loss 0.2854727804660797\n",
      "[Training Epoch 3] Batch 1271, Loss 0.2897185683250427\n",
      "[Training Epoch 3] Batch 1272, Loss 0.25410544872283936\n",
      "[Training Epoch 3] Batch 1273, Loss 0.30101534724235535\n",
      "[Training Epoch 3] Batch 1274, Loss 0.29645437002182007\n",
      "[Training Epoch 3] Batch 1275, Loss 0.29720523953437805\n",
      "[Training Epoch 3] Batch 1276, Loss 0.24565105140209198\n",
      "[Training Epoch 3] Batch 1277, Loss 0.27372437715530396\n",
      "[Training Epoch 3] Batch 1278, Loss 0.2712131142616272\n",
      "[Training Epoch 3] Batch 1279, Loss 0.2694465219974518\n",
      "[Training Epoch 3] Batch 1280, Loss 0.24611687660217285\n",
      "[Training Epoch 3] Batch 1281, Loss 0.30013394355773926\n",
      "[Training Epoch 3] Batch 1282, Loss 0.25237348675727844\n",
      "[Training Epoch 3] Batch 1283, Loss 0.26423174142837524\n",
      "[Training Epoch 3] Batch 1284, Loss 0.25570178031921387\n",
      "[Training Epoch 3] Batch 1285, Loss 0.27726519107818604\n",
      "[Training Epoch 3] Batch 1286, Loss 0.29355669021606445\n",
      "[Training Epoch 3] Batch 1287, Loss 0.24596460163593292\n",
      "[Training Epoch 3] Batch 1288, Loss 0.27665191888809204\n",
      "[Training Epoch 3] Batch 1289, Loss 0.28925952315330505\n",
      "[Training Epoch 3] Batch 1290, Loss 0.29802846908569336\n",
      "[Training Epoch 3] Batch 1291, Loss 0.25414979457855225\n",
      "[Training Epoch 3] Batch 1292, Loss 0.27095699310302734\n",
      "[Training Epoch 3] Batch 1293, Loss 0.30346617102622986\n",
      "[Training Epoch 3] Batch 1294, Loss 0.2691677212715149\n",
      "[Training Epoch 3] Batch 1295, Loss 0.2753913402557373\n",
      "[Training Epoch 3] Batch 1296, Loss 0.2756250202655792\n",
      "[Training Epoch 3] Batch 1297, Loss 0.2694265842437744\n",
      "[Training Epoch 3] Batch 1298, Loss 0.26594072580337524\n",
      "[Training Epoch 3] Batch 1299, Loss 0.2694508731365204\n",
      "[Training Epoch 3] Batch 1300, Loss 0.2753949463367462\n",
      "[Training Epoch 3] Batch 1301, Loss 0.29733821749687195\n",
      "[Training Epoch 3] Batch 1302, Loss 0.2464015781879425\n",
      "[Training Epoch 3] Batch 1303, Loss 0.2632538080215454\n",
      "[Training Epoch 3] Batch 1304, Loss 0.27215060591697693\n",
      "[Training Epoch 3] Batch 1305, Loss 0.2528960108757019\n",
      "[Training Epoch 3] Batch 1306, Loss 0.2641497552394867\n",
      "[Training Epoch 3] Batch 1307, Loss 0.2753903865814209\n",
      "[Training Epoch 3] Batch 1308, Loss 0.2666381001472473\n",
      "[Training Epoch 3] Batch 1309, Loss 0.26354607939720154\n",
      "[Training Epoch 3] Batch 1310, Loss 0.30063754320144653\n",
      "[Training Epoch 3] Batch 1311, Loss 0.26757633686065674\n",
      "[Training Epoch 3] Batch 1312, Loss 0.25897613167762756\n",
      "[Training Epoch 3] Batch 1313, Loss 0.24862438440322876\n",
      "[Training Epoch 3] Batch 1314, Loss 0.2551412582397461\n",
      "[Training Epoch 3] Batch 1315, Loss 0.27950921654701233\n",
      "[Training Epoch 3] Batch 1316, Loss 0.2780768573284149\n",
      "[Training Epoch 3] Batch 1317, Loss 0.2613036036491394\n",
      "[Training Epoch 3] Batch 1318, Loss 0.27772852778434753\n",
      "[Training Epoch 3] Batch 1319, Loss 0.2625136077404022\n",
      "[Training Epoch 3] Batch 1320, Loss 0.28129029273986816\n",
      "[Training Epoch 3] Batch 1321, Loss 0.27012884616851807\n",
      "[Training Epoch 3] Batch 1322, Loss 0.2753138244152069\n",
      "[Training Epoch 3] Batch 1323, Loss 0.2683161199092865\n",
      "[Training Epoch 3] Batch 1324, Loss 0.299613356590271\n",
      "[Training Epoch 3] Batch 1325, Loss 0.2717669904232025\n",
      "[Training Epoch 3] Batch 1326, Loss 0.26554566621780396\n",
      "[Training Epoch 3] Batch 1327, Loss 0.24714983999729156\n",
      "[Training Epoch 3] Batch 1328, Loss 0.2813630998134613\n",
      "[Training Epoch 3] Batch 1329, Loss 0.30070704221725464\n",
      "[Training Epoch 3] Batch 1330, Loss 0.28587669134140015\n",
      "[Training Epoch 3] Batch 1331, Loss 0.29055553674697876\n",
      "[Training Epoch 3] Batch 1332, Loss 0.2569999098777771\n",
      "[Training Epoch 3] Batch 1333, Loss 0.29108017683029175\n",
      "[Training Epoch 3] Batch 1334, Loss 0.2755378782749176\n",
      "[Training Epoch 3] Batch 1335, Loss 0.2431672215461731\n",
      "[Training Epoch 3] Batch 1336, Loss 0.2661783695220947\n",
      "[Training Epoch 3] Batch 1337, Loss 0.27446991205215454\n",
      "[Training Epoch 3] Batch 1338, Loss 0.3082694113254547\n",
      "[Training Epoch 3] Batch 1339, Loss 0.3019922375679016\n",
      "[Training Epoch 3] Batch 1340, Loss 0.26609551906585693\n",
      "[Training Epoch 3] Batch 1341, Loss 0.2829746603965759\n",
      "[Training Epoch 3] Batch 1342, Loss 0.2900809049606323\n",
      "[Training Epoch 3] Batch 1343, Loss 0.26621487736701965\n",
      "[Training Epoch 3] Batch 1344, Loss 0.26398995518684387\n",
      "[Training Epoch 3] Batch 1345, Loss 0.27995628118515015\n",
      "[Training Epoch 3] Batch 1346, Loss 0.2595095634460449\n",
      "[Training Epoch 3] Batch 1347, Loss 0.2643355131149292\n",
      "[Training Epoch 3] Batch 1348, Loss 0.2647054195404053\n",
      "[Training Epoch 3] Batch 1349, Loss 0.25845474004745483\n",
      "[Training Epoch 3] Batch 1350, Loss 0.26133596897125244\n",
      "[Training Epoch 3] Batch 1351, Loss 0.2553759217262268\n",
      "[Training Epoch 3] Batch 1352, Loss 0.29055771231651306\n",
      "[Training Epoch 3] Batch 1353, Loss 0.2775256931781769\n",
      "[Training Epoch 3] Batch 1354, Loss 0.28833603858947754\n",
      "[Training Epoch 3] Batch 1355, Loss 0.2637604773044586\n",
      "[Training Epoch 3] Batch 1356, Loss 0.2855386435985565\n",
      "[Training Epoch 3] Batch 1357, Loss 0.2633209228515625\n",
      "[Training Epoch 3] Batch 1358, Loss 0.28904491662979126\n",
      "[Training Epoch 3] Batch 1359, Loss 0.27392637729644775\n",
      "[Training Epoch 3] Batch 1360, Loss 0.27661508321762085\n",
      "[Training Epoch 3] Batch 1361, Loss 0.2583019733428955\n",
      "[Training Epoch 3] Batch 1362, Loss 0.2894173264503479\n",
      "[Training Epoch 3] Batch 1363, Loss 0.25360873341560364\n",
      "[Training Epoch 3] Batch 1364, Loss 0.2312890589237213\n",
      "[Training Epoch 3] Batch 1365, Loss 0.2933505177497864\n",
      "[Training Epoch 3] Batch 1366, Loss 0.2598300576210022\n",
      "[Training Epoch 3] Batch 1367, Loss 0.2908906936645508\n",
      "[Training Epoch 3] Batch 1368, Loss 0.2557275593280792\n",
      "[Training Epoch 3] Batch 1369, Loss 0.2704622745513916\n",
      "[Training Epoch 3] Batch 1370, Loss 0.3033006191253662\n",
      "[Training Epoch 3] Batch 1371, Loss 0.2386256754398346\n",
      "[Training Epoch 3] Batch 1372, Loss 0.2871822714805603\n",
      "[Training Epoch 3] Batch 1373, Loss 0.26611554622650146\n",
      "[Training Epoch 3] Batch 1374, Loss 0.2597211003303528\n",
      "[Training Epoch 3] Batch 1375, Loss 0.28792107105255127\n",
      "[Training Epoch 3] Batch 1376, Loss 0.2922619879245758\n",
      "[Training Epoch 3] Batch 1377, Loss 0.2824106812477112\n",
      "[Training Epoch 3] Batch 1378, Loss 0.2686379551887512\n",
      "[Training Epoch 3] Batch 1379, Loss 0.2736009955406189\n",
      "[Training Epoch 3] Batch 1380, Loss 0.2833614945411682\n",
      "[Training Epoch 3] Batch 1381, Loss 0.24237944185733795\n",
      "[Training Epoch 3] Batch 1382, Loss 0.24295353889465332\n",
      "[Training Epoch 3] Batch 1383, Loss 0.2966102957725525\n",
      "[Training Epoch 3] Batch 1384, Loss 0.25957223773002625\n",
      "[Training Epoch 3] Batch 1385, Loss 0.24992266297340393\n",
      "[Training Epoch 3] Batch 1386, Loss 0.23412227630615234\n",
      "[Training Epoch 3] Batch 1387, Loss 0.28359174728393555\n",
      "[Training Epoch 3] Batch 1388, Loss 0.2631964087486267\n",
      "[Training Epoch 3] Batch 1389, Loss 0.2674219608306885\n",
      "[Training Epoch 3] Batch 1390, Loss 0.263964980840683\n",
      "[Training Epoch 3] Batch 1391, Loss 0.28277844190597534\n",
      "[Training Epoch 3] Batch 1392, Loss 0.2545815706253052\n",
      "[Training Epoch 3] Batch 1393, Loss 0.2890229821205139\n",
      "[Training Epoch 3] Batch 1394, Loss 0.27605968713760376\n",
      "[Training Epoch 3] Batch 1395, Loss 0.27773424983024597\n",
      "[Training Epoch 3] Batch 1396, Loss 0.256568044424057\n",
      "[Training Epoch 3] Batch 1397, Loss 0.24698558449745178\n",
      "[Training Epoch 3] Batch 1398, Loss 0.28874391317367554\n",
      "[Training Epoch 3] Batch 1399, Loss 0.2903793454170227\n",
      "[Training Epoch 3] Batch 1400, Loss 0.27517181634902954\n",
      "[Training Epoch 3] Batch 1401, Loss 0.2282763123512268\n",
      "[Training Epoch 3] Batch 1402, Loss 0.26164090633392334\n",
      "[Training Epoch 3] Batch 1403, Loss 0.2536770701408386\n",
      "[Training Epoch 3] Batch 1404, Loss 0.27747979760169983\n",
      "[Training Epoch 3] Batch 1405, Loss 0.2493765652179718\n",
      "[Training Epoch 3] Batch 1406, Loss 0.2965809106826782\n",
      "[Training Epoch 3] Batch 1407, Loss 0.24998748302459717\n",
      "[Training Epoch 3] Batch 1408, Loss 0.24587126076221466\n",
      "[Training Epoch 3] Batch 1409, Loss 0.24514172971248627\n",
      "[Training Epoch 3] Batch 1410, Loss 0.26548075675964355\n",
      "[Training Epoch 3] Batch 1411, Loss 0.266655296087265\n",
      "[Training Epoch 3] Batch 1412, Loss 0.2854570746421814\n",
      "[Training Epoch 3] Batch 1413, Loss 0.2594526708126068\n",
      "[Training Epoch 3] Batch 1414, Loss 0.2715374827384949\n",
      "[Training Epoch 3] Batch 1415, Loss 0.2820487320423126\n",
      "[Training Epoch 3] Batch 1416, Loss 0.2701859772205353\n",
      "[Training Epoch 3] Batch 1417, Loss 0.2875964641571045\n",
      "[Training Epoch 3] Batch 1418, Loss 0.24846147000789642\n",
      "[Training Epoch 3] Batch 1419, Loss 0.25198447704315186\n",
      "[Training Epoch 3] Batch 1420, Loss 0.2711617648601532\n",
      "[Training Epoch 3] Batch 1421, Loss 0.28311657905578613\n",
      "[Training Epoch 3] Batch 1422, Loss 0.25107553601264954\n",
      "[Training Epoch 3] Batch 1423, Loss 0.31359437108039856\n",
      "[Training Epoch 3] Batch 1424, Loss 0.2664804756641388\n",
      "[Training Epoch 3] Batch 1425, Loss 0.24400073289871216\n",
      "[Training Epoch 3] Batch 1426, Loss 0.3108271062374115\n",
      "[Training Epoch 3] Batch 1427, Loss 0.2780953645706177\n",
      "[Training Epoch 3] Batch 1428, Loss 0.28522178530693054\n",
      "[Training Epoch 3] Batch 1429, Loss 0.26246076822280884\n",
      "[Training Epoch 3] Batch 1430, Loss 0.284381240606308\n",
      "[Training Epoch 3] Batch 1431, Loss 0.26098647713661194\n",
      "[Training Epoch 3] Batch 1432, Loss 0.2906121611595154\n",
      "[Training Epoch 3] Batch 1433, Loss 0.27638453245162964\n",
      "[Training Epoch 3] Batch 1434, Loss 0.2624596953392029\n",
      "[Training Epoch 3] Batch 1435, Loss 0.2869667410850525\n",
      "[Training Epoch 3] Batch 1436, Loss 0.26037541031837463\n",
      "[Training Epoch 3] Batch 1437, Loss 0.2507733404636383\n",
      "[Training Epoch 3] Batch 1438, Loss 0.2624160945415497\n",
      "[Training Epoch 3] Batch 1439, Loss 0.2970566749572754\n",
      "[Training Epoch 3] Batch 1440, Loss 0.30440056324005127\n",
      "[Training Epoch 3] Batch 1441, Loss 0.28323599696159363\n",
      "[Training Epoch 3] Batch 1442, Loss 0.2708553075790405\n",
      "[Training Epoch 3] Batch 1443, Loss 0.2771584987640381\n",
      "[Training Epoch 3] Batch 1444, Loss 0.27589237689971924\n",
      "[Training Epoch 3] Batch 1445, Loss 0.29003679752349854\n",
      "[Training Epoch 3] Batch 1446, Loss 0.25549736618995667\n",
      "[Training Epoch 3] Batch 1447, Loss 0.27801766991615295\n",
      "[Training Epoch 3] Batch 1448, Loss 0.2825326919555664\n",
      "[Training Epoch 3] Batch 1449, Loss 0.25854289531707764\n",
      "[Training Epoch 3] Batch 1450, Loss 0.29303333163261414\n",
      "[Training Epoch 3] Batch 1451, Loss 0.2522290050983429\n",
      "[Training Epoch 3] Batch 1452, Loss 0.2837677001953125\n",
      "[Training Epoch 3] Batch 1453, Loss 0.2704259753227234\n",
      "[Training Epoch 3] Batch 1454, Loss 0.29858219623565674\n",
      "[Training Epoch 3] Batch 1455, Loss 0.2583668828010559\n",
      "[Training Epoch 3] Batch 1456, Loss 0.2603452205657959\n",
      "[Training Epoch 3] Batch 1457, Loss 0.2571054697036743\n",
      "[Training Epoch 3] Batch 1458, Loss 0.2401149570941925\n",
      "[Training Epoch 3] Batch 1459, Loss 0.24808089435100555\n",
      "[Training Epoch 3] Batch 1460, Loss 0.25146740674972534\n",
      "[Training Epoch 3] Batch 1461, Loss 0.29161107540130615\n",
      "[Training Epoch 3] Batch 1462, Loss 0.27752941846847534\n",
      "[Training Epoch 3] Batch 1463, Loss 0.28330692648887634\n",
      "[Training Epoch 3] Batch 1464, Loss 0.2795170545578003\n",
      "[Training Epoch 3] Batch 1465, Loss 0.28302836418151855\n",
      "[Training Epoch 3] Batch 1466, Loss 0.2555348873138428\n",
      "[Training Epoch 3] Batch 1467, Loss 0.2676045298576355\n",
      "[Training Epoch 3] Batch 1468, Loss 0.28432151675224304\n",
      "[Training Epoch 3] Batch 1469, Loss 0.28209683299064636\n",
      "[Training Epoch 3] Batch 1470, Loss 0.33567672967910767\n",
      "[Training Epoch 3] Batch 1471, Loss 0.2777838110923767\n",
      "[Training Epoch 3] Batch 1472, Loss 0.2629351317882538\n",
      "[Training Epoch 3] Batch 1473, Loss 0.2798600196838379\n",
      "[Training Epoch 3] Batch 1474, Loss 0.2850474715232849\n",
      "[Training Epoch 3] Batch 1475, Loss 0.3238184452056885\n",
      "[Training Epoch 3] Batch 1476, Loss 0.27167123556137085\n",
      "[Training Epoch 3] Batch 1477, Loss 0.2740132212638855\n",
      "[Training Epoch 3] Batch 1478, Loss 0.24112355709075928\n",
      "[Training Epoch 3] Batch 1479, Loss 0.29409804940223694\n",
      "[Training Epoch 3] Batch 1480, Loss 0.2735323905944824\n",
      "[Training Epoch 3] Batch 1481, Loss 0.26608896255493164\n",
      "[Training Epoch 3] Batch 1482, Loss 0.30890533328056335\n",
      "[Training Epoch 3] Batch 1483, Loss 0.2593771517276764\n",
      "[Training Epoch 3] Batch 1484, Loss 0.27160167694091797\n",
      "[Training Epoch 3] Batch 1485, Loss 0.25366348028182983\n",
      "[Training Epoch 3] Batch 1486, Loss 0.2921341061592102\n",
      "[Training Epoch 3] Batch 1487, Loss 0.27611225843429565\n",
      "[Training Epoch 3] Batch 1488, Loss 0.2519780993461609\n",
      "[Training Epoch 3] Batch 1489, Loss 0.27983158826828003\n",
      "[Training Epoch 3] Batch 1490, Loss 0.26723289489746094\n",
      "[Training Epoch 3] Batch 1491, Loss 0.26739567518234253\n",
      "[Training Epoch 3] Batch 1492, Loss 0.2780956029891968\n",
      "[Training Epoch 3] Batch 1493, Loss 0.281759113073349\n",
      "[Training Epoch 3] Batch 1494, Loss 0.27023473381996155\n",
      "[Training Epoch 3] Batch 1495, Loss 0.2626458406448364\n",
      "[Training Epoch 3] Batch 1496, Loss 0.28096193075180054\n",
      "[Training Epoch 3] Batch 1497, Loss 0.23652347922325134\n",
      "[Training Epoch 3] Batch 1498, Loss 0.28395915031433105\n",
      "[Training Epoch 3] Batch 1499, Loss 0.2595475912094116\n",
      "[Training Epoch 3] Batch 1500, Loss 0.25193923711776733\n",
      "[Training Epoch 3] Batch 1501, Loss 0.2562297582626343\n",
      "[Training Epoch 3] Batch 1502, Loss 0.25279754400253296\n",
      "[Training Epoch 3] Batch 1503, Loss 0.26861315965652466\n",
      "[Training Epoch 3] Batch 1504, Loss 0.2589843273162842\n",
      "[Training Epoch 3] Batch 1505, Loss 0.2438231259584427\n",
      "[Training Epoch 3] Batch 1506, Loss 0.2844139039516449\n",
      "[Training Epoch 3] Batch 1507, Loss 0.2874515652656555\n",
      "[Training Epoch 3] Batch 1508, Loss 0.2831544876098633\n",
      "[Training Epoch 3] Batch 1509, Loss 0.26576972007751465\n",
      "[Training Epoch 3] Batch 1510, Loss 0.28952258825302124\n",
      "[Training Epoch 3] Batch 1511, Loss 0.2679237723350525\n",
      "[Training Epoch 3] Batch 1512, Loss 0.25658756494522095\n",
      "[Training Epoch 3] Batch 1513, Loss 0.25876694917678833\n",
      "[Training Epoch 3] Batch 1514, Loss 0.26414039731025696\n",
      "[Training Epoch 3] Batch 1515, Loss 0.28651461005210876\n",
      "[Training Epoch 3] Batch 1516, Loss 0.2495822310447693\n",
      "[Training Epoch 3] Batch 1517, Loss 0.2662505805492401\n",
      "[Training Epoch 3] Batch 1518, Loss 0.2862529158592224\n",
      "[Training Epoch 3] Batch 1519, Loss 0.2554231286048889\n",
      "[Training Epoch 3] Batch 1520, Loss 0.2745260000228882\n",
      "[Training Epoch 3] Batch 1521, Loss 0.26423224806785583\n",
      "[Training Epoch 3] Batch 1522, Loss 0.26391857862472534\n",
      "[Training Epoch 3] Batch 1523, Loss 0.3060857653617859\n",
      "[Training Epoch 3] Batch 1524, Loss 0.2682285010814667\n",
      "[Training Epoch 3] Batch 1525, Loss 0.28169727325439453\n",
      "[Training Epoch 3] Batch 1526, Loss 0.2699078619480133\n",
      "[Training Epoch 3] Batch 1527, Loss 0.2522417902946472\n",
      "[Training Epoch 3] Batch 1528, Loss 0.28882062435150146\n",
      "[Training Epoch 3] Batch 1529, Loss 0.2564556300640106\n",
      "[Training Epoch 3] Batch 1530, Loss 0.28405216336250305\n",
      "[Training Epoch 3] Batch 1531, Loss 0.2683020830154419\n",
      "[Training Epoch 3] Batch 1532, Loss 0.29203495383262634\n",
      "[Training Epoch 3] Batch 1533, Loss 0.28265246748924255\n",
      "[Training Epoch 3] Batch 1534, Loss 0.28612321615219116\n",
      "[Training Epoch 3] Batch 1535, Loss 0.28484487533569336\n",
      "[Training Epoch 3] Batch 1536, Loss 0.2458256632089615\n",
      "[Training Epoch 3] Batch 1537, Loss 0.2805575728416443\n",
      "[Training Epoch 3] Batch 1538, Loss 0.3229999542236328\n",
      "[Training Epoch 3] Batch 1539, Loss 0.24902747571468353\n",
      "[Training Epoch 3] Batch 1540, Loss 0.2770533263683319\n",
      "[Training Epoch 3] Batch 1541, Loss 0.3025038242340088\n",
      "[Training Epoch 3] Batch 1542, Loss 0.27199119329452515\n",
      "[Training Epoch 3] Batch 1543, Loss 0.26019957661628723\n",
      "[Training Epoch 3] Batch 1544, Loss 0.2928798496723175\n",
      "[Training Epoch 3] Batch 1545, Loss 0.2510867118835449\n",
      "[Training Epoch 3] Batch 1546, Loss 0.266242653131485\n",
      "[Training Epoch 3] Batch 1547, Loss 0.26452574133872986\n",
      "[Training Epoch 3] Batch 1548, Loss 0.25836220383644104\n",
      "[Training Epoch 3] Batch 1549, Loss 0.28667935729026794\n",
      "[Training Epoch 3] Batch 1550, Loss 0.2725484073162079\n",
      "[Training Epoch 3] Batch 1551, Loss 0.25607508420944214\n",
      "[Training Epoch 3] Batch 1552, Loss 0.26770201325416565\n",
      "[Training Epoch 3] Batch 1553, Loss 0.25665411353111267\n",
      "[Training Epoch 3] Batch 1554, Loss 0.25961217284202576\n",
      "[Training Epoch 3] Batch 1555, Loss 0.26371249556541443\n",
      "[Training Epoch 3] Batch 1556, Loss 0.26834338903427124\n",
      "[Training Epoch 3] Batch 1557, Loss 0.2759208381175995\n",
      "[Training Epoch 3] Batch 1558, Loss 0.25037530064582825\n",
      "[Training Epoch 3] Batch 1559, Loss 0.28094321489334106\n",
      "[Training Epoch 3] Batch 1560, Loss 0.25899621844291687\n",
      "[Training Epoch 3] Batch 1561, Loss 0.27429714798927307\n",
      "[Training Epoch 3] Batch 1562, Loss 0.2824341952800751\n",
      "[Training Epoch 3] Batch 1563, Loss 0.28424179553985596\n",
      "[Training Epoch 3] Batch 1564, Loss 0.31307029724121094\n",
      "[Training Epoch 3] Batch 1565, Loss 0.26345640420913696\n",
      "[Training Epoch 3] Batch 1566, Loss 0.26587238907814026\n",
      "[Training Epoch 3] Batch 1567, Loss 0.2827259302139282\n",
      "[Training Epoch 3] Batch 1568, Loss 0.2687559127807617\n",
      "[Training Epoch 3] Batch 1569, Loss 0.2647096514701843\n",
      "[Training Epoch 3] Batch 1570, Loss 0.31148865818977356\n",
      "[Training Epoch 3] Batch 1571, Loss 0.3059453070163727\n",
      "[Training Epoch 3] Batch 1572, Loss 0.29413920640945435\n",
      "[Training Epoch 3] Batch 1573, Loss 0.25295865535736084\n",
      "[Training Epoch 3] Batch 1574, Loss 0.27269989252090454\n",
      "[Training Epoch 3] Batch 1575, Loss 0.25106149911880493\n",
      "[Training Epoch 3] Batch 1576, Loss 0.2680620551109314\n",
      "[Training Epoch 3] Batch 1577, Loss 0.2868993878364563\n",
      "[Training Epoch 3] Batch 1578, Loss 0.2617935836315155\n",
      "[Training Epoch 3] Batch 1579, Loss 0.270565927028656\n",
      "[Training Epoch 3] Batch 1580, Loss 0.25849658250808716\n",
      "[Training Epoch 3] Batch 1581, Loss 0.3022323250770569\n",
      "[Training Epoch 3] Batch 1582, Loss 0.2829717993736267\n",
      "[Training Epoch 3] Batch 1583, Loss 0.29273372888565063\n",
      "[Training Epoch 3] Batch 1584, Loss 0.26233723759651184\n",
      "[Training Epoch 3] Batch 1585, Loss 0.2891036570072174\n",
      "[Training Epoch 3] Batch 1586, Loss 0.2810367941856384\n",
      "[Training Epoch 3] Batch 1587, Loss 0.28205397725105286\n",
      "[Training Epoch 3] Batch 1588, Loss 0.2653908431529999\n",
      "[Training Epoch 3] Batch 1589, Loss 0.25384289026260376\n",
      "[Training Epoch 3] Batch 1590, Loss 0.24434679746627808\n",
      "[Training Epoch 3] Batch 1591, Loss 0.28869298100471497\n",
      "[Training Epoch 3] Batch 1592, Loss 0.265485942363739\n",
      "[Training Epoch 3] Batch 1593, Loss 0.2990884482860565\n",
      "[Training Epoch 3] Batch 1594, Loss 0.26805996894836426\n",
      "[Training Epoch 3] Batch 1595, Loss 0.2978363633155823\n",
      "[Training Epoch 3] Batch 1596, Loss 0.2522604465484619\n",
      "[Training Epoch 3] Batch 1597, Loss 0.2609763741493225\n",
      "[Training Epoch 3] Batch 1598, Loss 0.2440837025642395\n",
      "[Training Epoch 3] Batch 1599, Loss 0.26255056262016296\n",
      "[Training Epoch 3] Batch 1600, Loss 0.2758025825023651\n",
      "[Training Epoch 3] Batch 1601, Loss 0.27338600158691406\n",
      "[Training Epoch 3] Batch 1602, Loss 0.22415240108966827\n",
      "[Training Epoch 3] Batch 1603, Loss 0.2529616951942444\n",
      "[Training Epoch 3] Batch 1604, Loss 0.28584104776382446\n",
      "[Training Epoch 3] Batch 1605, Loss 0.2763519883155823\n",
      "[Training Epoch 3] Batch 1606, Loss 0.2693997621536255\n",
      "[Training Epoch 3] Batch 1607, Loss 0.276048481464386\n",
      "[Training Epoch 3] Batch 1608, Loss 0.2677684426307678\n",
      "[Training Epoch 3] Batch 1609, Loss 0.2676635682582855\n",
      "[Training Epoch 3] Batch 1610, Loss 0.28602540493011475\n",
      "[Training Epoch 3] Batch 1611, Loss 0.25111648440361023\n",
      "[Training Epoch 3] Batch 1612, Loss 0.2705947756767273\n",
      "[Training Epoch 3] Batch 1613, Loss 0.26242727041244507\n",
      "[Training Epoch 3] Batch 1614, Loss 0.25575438141822815\n",
      "[Training Epoch 3] Batch 1615, Loss 0.2638750672340393\n",
      "[Training Epoch 3] Batch 1616, Loss 0.29132789373397827\n",
      "[Training Epoch 3] Batch 1617, Loss 0.26073092222213745\n",
      "[Training Epoch 3] Batch 1618, Loss 0.26493775844573975\n",
      "[Training Epoch 3] Batch 1619, Loss 0.24368055164813995\n",
      "[Training Epoch 3] Batch 1620, Loss 0.25963306427001953\n",
      "[Training Epoch 3] Batch 1621, Loss 0.27481842041015625\n",
      "[Training Epoch 3] Batch 1622, Loss 0.2726701498031616\n",
      "[Training Epoch 3] Batch 1623, Loss 0.2827676236629486\n",
      "[Training Epoch 3] Batch 1624, Loss 0.27897536754608154\n",
      "[Training Epoch 3] Batch 1625, Loss 0.2778428792953491\n",
      "[Training Epoch 3] Batch 1626, Loss 0.24790070950984955\n",
      "[Training Epoch 3] Batch 1627, Loss 0.2782963514328003\n",
      "[Training Epoch 3] Batch 1628, Loss 0.26569199562072754\n",
      "[Training Epoch 3] Batch 1629, Loss 0.2535771429538727\n",
      "[Training Epoch 3] Batch 1630, Loss 0.24700358510017395\n",
      "[Training Epoch 3] Batch 1631, Loss 0.25647473335266113\n",
      "[Training Epoch 3] Batch 1632, Loss 0.2580706775188446\n",
      "[Training Epoch 3] Batch 1633, Loss 0.28658944368362427\n",
      "[Training Epoch 3] Batch 1634, Loss 0.2735515236854553\n",
      "[Training Epoch 3] Batch 1635, Loss 0.2608199715614319\n",
      "[Training Epoch 3] Batch 1636, Loss 0.2763916254043579\n",
      "[Training Epoch 3] Batch 1637, Loss 0.2730235457420349\n",
      "[Training Epoch 3] Batch 1638, Loss 0.2385849952697754\n",
      "[Training Epoch 3] Batch 1639, Loss 0.267143189907074\n",
      "[Training Epoch 3] Batch 1640, Loss 0.261404812335968\n",
      "[Training Epoch 3] Batch 1641, Loss 0.2591789960861206\n",
      "[Training Epoch 3] Batch 1642, Loss 0.31358039379119873\n",
      "[Training Epoch 3] Batch 1643, Loss 0.24910442531108856\n",
      "[Training Epoch 3] Batch 1644, Loss 0.25053712725639343\n",
      "[Training Epoch 3] Batch 1645, Loss 0.25132614374160767\n",
      "[Training Epoch 3] Batch 1646, Loss 0.28121235966682434\n",
      "[Training Epoch 3] Batch 1647, Loss 0.28438514471054077\n",
      "[Training Epoch 3] Batch 1648, Loss 0.2688915431499481\n",
      "[Training Epoch 3] Batch 1649, Loss 0.27142173051834106\n",
      "[Training Epoch 3] Batch 1650, Loss 0.26221156120300293\n",
      "[Training Epoch 3] Batch 1651, Loss 0.28346681594848633\n",
      "[Training Epoch 3] Batch 1652, Loss 0.25809457898139954\n",
      "[Training Epoch 3] Batch 1653, Loss 0.2725951671600342\n",
      "[Training Epoch 3] Batch 1654, Loss 0.26059335470199585\n",
      "[Training Epoch 3] Batch 1655, Loss 0.2560921907424927\n",
      "[Training Epoch 3] Batch 1656, Loss 0.252363383769989\n",
      "[Training Epoch 3] Batch 1657, Loss 0.2681971788406372\n",
      "[Training Epoch 3] Batch 1658, Loss 0.2606809139251709\n",
      "[Training Epoch 3] Batch 1659, Loss 0.2311324030160904\n",
      "[Training Epoch 3] Batch 1660, Loss 0.26935142278671265\n",
      "[Training Epoch 3] Batch 1661, Loss 0.26275748014450073\n",
      "[Training Epoch 3] Batch 1662, Loss 0.2733210325241089\n",
      "[Training Epoch 3] Batch 1663, Loss 0.26225546002388\n",
      "[Training Epoch 3] Batch 1664, Loss 0.2632253170013428\n",
      "[Training Epoch 3] Batch 1665, Loss 0.2817954421043396\n",
      "[Training Epoch 3] Batch 1666, Loss 0.28412750363349915\n",
      "[Training Epoch 3] Batch 1667, Loss 0.2666650116443634\n",
      "[Training Epoch 3] Batch 1668, Loss 0.2611691951751709\n",
      "[Training Epoch 3] Batch 1669, Loss 0.27254411578178406\n",
      "[Training Epoch 3] Batch 1670, Loss 0.24985168874263763\n",
      "[Training Epoch 3] Batch 1671, Loss 0.27517372369766235\n",
      "[Training Epoch 3] Batch 1672, Loss 0.26860445737838745\n",
      "[Training Epoch 3] Batch 1673, Loss 0.26969343423843384\n",
      "[Training Epoch 3] Batch 1674, Loss 0.27117490768432617\n",
      "[Training Epoch 3] Batch 1675, Loss 0.2704046368598938\n",
      "[Training Epoch 3] Batch 1676, Loss 0.26675254106521606\n",
      "[Training Epoch 3] Batch 1677, Loss 0.2863147258758545\n",
      "[Training Epoch 3] Batch 1678, Loss 0.26491785049438477\n",
      "[Training Epoch 3] Batch 1679, Loss 0.2453405261039734\n",
      "[Training Epoch 3] Batch 1680, Loss 0.2683787941932678\n",
      "[Training Epoch 3] Batch 1681, Loss 0.28836876153945923\n",
      "[Training Epoch 3] Batch 1682, Loss 0.29324740171432495\n",
      "[Training Epoch 3] Batch 1683, Loss 0.29285287857055664\n",
      "[Training Epoch 3] Batch 1684, Loss 0.258544921875\n",
      "[Training Epoch 3] Batch 1685, Loss 0.26083865761756897\n",
      "[Training Epoch 3] Batch 1686, Loss 0.262855589389801\n",
      "[Training Epoch 3] Batch 1687, Loss 0.2675664722919464\n",
      "[Training Epoch 3] Batch 1688, Loss 0.27177828550338745\n",
      "[Training Epoch 3] Batch 1689, Loss 0.27346187829971313\n",
      "[Training Epoch 3] Batch 1690, Loss 0.24662551283836365\n",
      "[Training Epoch 3] Batch 1691, Loss 0.26752620935440063\n",
      "[Training Epoch 3] Batch 1692, Loss 0.24010658264160156\n",
      "[Training Epoch 3] Batch 1693, Loss 0.2902764678001404\n",
      "[Training Epoch 3] Batch 1694, Loss 0.24410933256149292\n",
      "[Training Epoch 3] Batch 1695, Loss 0.2728078365325928\n",
      "[Training Epoch 3] Batch 1696, Loss 0.2538038492202759\n",
      "[Training Epoch 3] Batch 1697, Loss 0.24515780806541443\n",
      "[Training Epoch 3] Batch 1698, Loss 0.2670190930366516\n",
      "[Training Epoch 3] Batch 1699, Loss 0.29416948556900024\n",
      "[Training Epoch 3] Batch 1700, Loss 0.26014846563339233\n",
      "[Training Epoch 3] Batch 1701, Loss 0.26874423027038574\n",
      "[Training Epoch 3] Batch 1702, Loss 0.26514750719070435\n",
      "[Training Epoch 3] Batch 1703, Loss 0.29742246866226196\n",
      "[Training Epoch 3] Batch 1704, Loss 0.27707093954086304\n",
      "[Training Epoch 3] Batch 1705, Loss 0.2396986484527588\n",
      "[Training Epoch 3] Batch 1706, Loss 0.28102052211761475\n",
      "[Training Epoch 3] Batch 1707, Loss 0.26260533928871155\n",
      "[Training Epoch 3] Batch 1708, Loss 0.2886647582054138\n",
      "[Training Epoch 3] Batch 1709, Loss 0.24305260181427002\n",
      "[Training Epoch 3] Batch 1710, Loss 0.272763729095459\n",
      "[Training Epoch 3] Batch 1711, Loss 0.29120898246765137\n",
      "[Training Epoch 3] Batch 1712, Loss 0.27072346210479736\n",
      "[Training Epoch 3] Batch 1713, Loss 0.25556468963623047\n",
      "[Training Epoch 3] Batch 1714, Loss 0.29222166538238525\n",
      "[Training Epoch 3] Batch 1715, Loss 0.26729220151901245\n",
      "[Training Epoch 3] Batch 1716, Loss 0.28949421644210815\n",
      "[Training Epoch 3] Batch 1717, Loss 0.2551678419113159\n",
      "[Training Epoch 3] Batch 1718, Loss 0.2716776728630066\n",
      "[Training Epoch 3] Batch 1719, Loss 0.28978878259658813\n",
      "[Training Epoch 3] Batch 1720, Loss 0.3062216639518738\n",
      "[Training Epoch 3] Batch 1721, Loss 0.2770005464553833\n",
      "[Training Epoch 3] Batch 1722, Loss 0.271306574344635\n",
      "[Training Epoch 3] Batch 1723, Loss 0.26686009764671326\n",
      "[Training Epoch 3] Batch 1724, Loss 0.2829377055168152\n",
      "[Training Epoch 3] Batch 1725, Loss 0.28967276215553284\n",
      "[Training Epoch 3] Batch 1726, Loss 0.2737599015235901\n",
      "[Training Epoch 3] Batch 1727, Loss 0.3189300298690796\n",
      "[Training Epoch 3] Batch 1728, Loss 0.2634955942630768\n",
      "[Training Epoch 3] Batch 1729, Loss 0.27328750491142273\n",
      "[Training Epoch 3] Batch 1730, Loss 0.27310699224472046\n",
      "[Training Epoch 3] Batch 1731, Loss 0.2791038751602173\n",
      "[Training Epoch 3] Batch 1732, Loss 0.282876193523407\n",
      "[Training Epoch 3] Batch 1733, Loss 0.2789028584957123\n",
      "[Training Epoch 3] Batch 1734, Loss 0.2737598717212677\n",
      "[Training Epoch 3] Batch 1735, Loss 0.25832995772361755\n",
      "[Training Epoch 3] Batch 1736, Loss 0.28186488151550293\n",
      "[Training Epoch 3] Batch 1737, Loss 0.2723844051361084\n",
      "[Training Epoch 3] Batch 1738, Loss 0.30091744661331177\n",
      "[Training Epoch 3] Batch 1739, Loss 0.2867124676704407\n",
      "[Training Epoch 3] Batch 1740, Loss 0.26403191685676575\n",
      "[Training Epoch 3] Batch 1741, Loss 0.2818818688392639\n",
      "[Training Epoch 3] Batch 1742, Loss 0.24801956117153168\n",
      "[Training Epoch 3] Batch 1743, Loss 0.2580898106098175\n",
      "[Training Epoch 3] Batch 1744, Loss 0.2862611413002014\n",
      "[Training Epoch 3] Batch 1745, Loss 0.2812606692314148\n",
      "[Training Epoch 3] Batch 1746, Loss 0.26396095752716064\n",
      "[Training Epoch 3] Batch 1747, Loss 0.30256035923957825\n",
      "[Training Epoch 3] Batch 1748, Loss 0.27079451084136963\n",
      "[Training Epoch 3] Batch 1749, Loss 0.28552407026290894\n",
      "[Training Epoch 3] Batch 1750, Loss 0.30199024081230164\n",
      "[Training Epoch 3] Batch 1751, Loss 0.27713721990585327\n",
      "[Training Epoch 3] Batch 1752, Loss 0.26738816499710083\n",
      "[Training Epoch 3] Batch 1753, Loss 0.27282360196113586\n",
      "[Training Epoch 3] Batch 1754, Loss 0.26498305797576904\n",
      "[Training Epoch 3] Batch 1755, Loss 0.2809433937072754\n",
      "[Training Epoch 3] Batch 1756, Loss 0.27625197172164917\n",
      "[Training Epoch 3] Batch 1757, Loss 0.2788015902042389\n",
      "[Training Epoch 3] Batch 1758, Loss 0.2688532769680023\n",
      "[Training Epoch 3] Batch 1759, Loss 0.2720237076282501\n",
      "[Training Epoch 3] Batch 1760, Loss 0.2571095824241638\n",
      "[Training Epoch 3] Batch 1761, Loss 0.2489968240261078\n",
      "[Training Epoch 3] Batch 1762, Loss 0.2823595702648163\n",
      "[Training Epoch 3] Batch 1763, Loss 0.304826557636261\n",
      "[Training Epoch 3] Batch 1764, Loss 0.272915780544281\n",
      "[Training Epoch 3] Batch 1765, Loss 0.2560080587863922\n",
      "[Training Epoch 3] Batch 1766, Loss 0.29930371046066284\n",
      "[Training Epoch 3] Batch 1767, Loss 0.26876556873321533\n",
      "[Training Epoch 3] Batch 1768, Loss 0.27258604764938354\n",
      "[Training Epoch 3] Batch 1769, Loss 0.2857336401939392\n",
      "[Training Epoch 3] Batch 1770, Loss 0.2917550802230835\n",
      "[Training Epoch 3] Batch 1771, Loss 0.29249444603919983\n",
      "[Training Epoch 3] Batch 1772, Loss 0.2812161445617676\n",
      "[Training Epoch 3] Batch 1773, Loss 0.26519131660461426\n",
      "[Training Epoch 3] Batch 1774, Loss 0.280291885137558\n",
      "[Training Epoch 3] Batch 1775, Loss 0.2864079475402832\n",
      "[Training Epoch 3] Batch 1776, Loss 0.23989638686180115\n",
      "[Training Epoch 3] Batch 1777, Loss 0.2662372589111328\n",
      "[Training Epoch 3] Batch 1778, Loss 0.2630099058151245\n",
      "[Training Epoch 3] Batch 1779, Loss 0.28717342019081116\n",
      "[Training Epoch 3] Batch 1780, Loss 0.28586727380752563\n",
      "[Training Epoch 3] Batch 1781, Loss 0.2527657151222229\n",
      "[Training Epoch 3] Batch 1782, Loss 0.26738160848617554\n",
      "[Training Epoch 3] Batch 1783, Loss 0.28187471628189087\n",
      "[Training Epoch 3] Batch 1784, Loss 0.25534528493881226\n",
      "[Training Epoch 3] Batch 1785, Loss 0.27269288897514343\n",
      "[Training Epoch 3] Batch 1786, Loss 0.28338491916656494\n",
      "[Training Epoch 3] Batch 1787, Loss 0.2723473012447357\n",
      "[Training Epoch 3] Batch 1788, Loss 0.2912519574165344\n",
      "[Training Epoch 3] Batch 1789, Loss 0.25891003012657166\n",
      "[Training Epoch 3] Batch 1790, Loss 0.28158292174339294\n",
      "[Training Epoch 3] Batch 1791, Loss 0.3010990023612976\n",
      "[Training Epoch 3] Batch 1792, Loss 0.27215322852134705\n",
      "[Training Epoch 3] Batch 1793, Loss 0.2742823362350464\n",
      "[Training Epoch 3] Batch 1794, Loss 0.26769453287124634\n",
      "[Training Epoch 3] Batch 1795, Loss 0.28342342376708984\n",
      "[Training Epoch 3] Batch 1796, Loss 0.28441542387008667\n",
      "[Training Epoch 3] Batch 1797, Loss 0.233134925365448\n",
      "[Training Epoch 3] Batch 1798, Loss 0.2951244115829468\n",
      "[Training Epoch 3] Batch 1799, Loss 0.2944837808609009\n",
      "[Training Epoch 3] Batch 1800, Loss 0.2476343959569931\n",
      "[Training Epoch 3] Batch 1801, Loss 0.2768539786338806\n",
      "[Training Epoch 3] Batch 1802, Loss 0.27718591690063477\n",
      "[Training Epoch 3] Batch 1803, Loss 0.2797527015209198\n",
      "[Training Epoch 3] Batch 1804, Loss 0.26344001293182373\n",
      "[Training Epoch 3] Batch 1805, Loss 0.2693079710006714\n",
      "[Training Epoch 3] Batch 1806, Loss 0.2784020006656647\n",
      "[Training Epoch 3] Batch 1807, Loss 0.2714696228504181\n",
      "[Training Epoch 3] Batch 1808, Loss 0.28825366497039795\n",
      "[Training Epoch 3] Batch 1809, Loss 0.24447199702262878\n",
      "[Training Epoch 3] Batch 1810, Loss 0.2821601629257202\n",
      "[Training Epoch 3] Batch 1811, Loss 0.27246755361557007\n",
      "[Training Epoch 3] Batch 1812, Loss 0.2900252342224121\n",
      "[Training Epoch 3] Batch 1813, Loss 0.287811815738678\n",
      "[Training Epoch 3] Batch 1814, Loss 0.270504891872406\n",
      "[Training Epoch 3] Batch 1815, Loss 0.24754445254802704\n",
      "[Training Epoch 3] Batch 1816, Loss 0.24594227969646454\n",
      "[Training Epoch 3] Batch 1817, Loss 0.2647523581981659\n",
      "[Training Epoch 3] Batch 1818, Loss 0.2585797905921936\n",
      "[Training Epoch 3] Batch 1819, Loss 0.2598574757575989\n",
      "[Training Epoch 3] Batch 1820, Loss 0.25511616468429565\n",
      "[Training Epoch 3] Batch 1821, Loss 0.26297813653945923\n",
      "[Training Epoch 3] Batch 1822, Loss 0.2542886435985565\n",
      "[Training Epoch 3] Batch 1823, Loss 0.28047358989715576\n",
      "[Training Epoch 3] Batch 1824, Loss 0.249119833111763\n",
      "[Training Epoch 3] Batch 1825, Loss 0.2769813537597656\n",
      "[Training Epoch 3] Batch 1826, Loss 0.2420693188905716\n",
      "[Training Epoch 3] Batch 1827, Loss 0.2932193875312805\n",
      "[Training Epoch 3] Batch 1828, Loss 0.2591027617454529\n",
      "[Training Epoch 3] Batch 1829, Loss 0.24733194708824158\n",
      "[Training Epoch 3] Batch 1830, Loss 0.2754191756248474\n",
      "[Training Epoch 3] Batch 1831, Loss 0.24839085340499878\n",
      "[Training Epoch 3] Batch 1832, Loss 0.2635394036769867\n",
      "[Training Epoch 3] Batch 1833, Loss 0.31625765562057495\n",
      "[Training Epoch 3] Batch 1834, Loss 0.27501094341278076\n",
      "[Training Epoch 3] Batch 1835, Loss 0.278020441532135\n",
      "[Training Epoch 3] Batch 1836, Loss 0.26993390917778015\n",
      "[Training Epoch 3] Batch 1837, Loss 0.25803714990615845\n",
      "[Training Epoch 3] Batch 1838, Loss 0.25354039669036865\n",
      "[Training Epoch 3] Batch 1839, Loss 0.27199241518974304\n",
      "[Training Epoch 3] Batch 1840, Loss 0.26415932178497314\n",
      "[Training Epoch 3] Batch 1841, Loss 0.28287720680236816\n",
      "[Training Epoch 3] Batch 1842, Loss 0.2746117413043976\n",
      "[Training Epoch 3] Batch 1843, Loss 0.2666637897491455\n",
      "[Training Epoch 3] Batch 1844, Loss 0.26936042308807373\n",
      "[Training Epoch 3] Batch 1845, Loss 0.2638000249862671\n",
      "[Training Epoch 3] Batch 1846, Loss 0.24904905259609222\n",
      "[Training Epoch 3] Batch 1847, Loss 0.2622382938861847\n",
      "[Training Epoch 3] Batch 1848, Loss 0.29161500930786133\n",
      "[Training Epoch 3] Batch 1849, Loss 0.26494190096855164\n",
      "[Training Epoch 3] Batch 1850, Loss 0.3021717369556427\n",
      "[Training Epoch 3] Batch 1851, Loss 0.28114187717437744\n",
      "[Training Epoch 3] Batch 1852, Loss 0.2624322772026062\n",
      "[Training Epoch 3] Batch 1853, Loss 0.26404285430908203\n",
      "[Training Epoch 3] Batch 1854, Loss 0.2712829113006592\n",
      "[Training Epoch 3] Batch 1855, Loss 0.2690850496292114\n",
      "[Training Epoch 3] Batch 1856, Loss 0.2857275903224945\n",
      "[Training Epoch 3] Batch 1857, Loss 0.2804374694824219\n",
      "[Training Epoch 3] Batch 1858, Loss 0.2681328058242798\n",
      "[Training Epoch 3] Batch 1859, Loss 0.27907297015190125\n",
      "[Training Epoch 3] Batch 1860, Loss 0.2511177062988281\n",
      "[Training Epoch 3] Batch 1861, Loss 0.28509941697120667\n",
      "[Training Epoch 3] Batch 1862, Loss 0.2927463948726654\n",
      "[Training Epoch 3] Batch 1863, Loss 0.27642467617988586\n",
      "[Training Epoch 3] Batch 1864, Loss 0.25792601704597473\n",
      "[Training Epoch 3] Batch 1865, Loss 0.2902531623840332\n",
      "[Training Epoch 3] Batch 1866, Loss 0.26514410972595215\n",
      "[Training Epoch 3] Batch 1867, Loss 0.27350717782974243\n",
      "[Training Epoch 3] Batch 1868, Loss 0.28139394521713257\n",
      "[Training Epoch 3] Batch 1869, Loss 0.27036717534065247\n",
      "[Training Epoch 3] Batch 1870, Loss 0.2669706344604492\n",
      "[Training Epoch 3] Batch 1871, Loss 0.26527225971221924\n",
      "[Training Epoch 3] Batch 1872, Loss 0.26767849922180176\n",
      "[Training Epoch 3] Batch 1873, Loss 0.27253368496894836\n",
      "[Training Epoch 3] Batch 1874, Loss 0.2647242844104767\n",
      "[Training Epoch 3] Batch 1875, Loss 0.2618401348590851\n",
      "[Training Epoch 3] Batch 1876, Loss 0.2643696069717407\n",
      "[Training Epoch 3] Batch 1877, Loss 0.28434672951698303\n",
      "[Training Epoch 3] Batch 1878, Loss 0.2381778061389923\n",
      "[Training Epoch 3] Batch 1879, Loss 0.2872794270515442\n",
      "[Training Epoch 3] Batch 1880, Loss 0.30296561121940613\n",
      "[Training Epoch 3] Batch 1881, Loss 0.29426950216293335\n",
      "[Training Epoch 3] Batch 1882, Loss 0.29572591185569763\n",
      "[Training Epoch 3] Batch 1883, Loss 0.27263349294662476\n",
      "[Training Epoch 3] Batch 1884, Loss 0.27033430337905884\n",
      "[Training Epoch 3] Batch 1885, Loss 0.26408979296684265\n",
      "[Training Epoch 3] Batch 1886, Loss 0.27308496832847595\n",
      "[Training Epoch 3] Batch 1887, Loss 0.28217989206314087\n",
      "[Training Epoch 3] Batch 1888, Loss 0.26260945200920105\n",
      "[Training Epoch 3] Batch 1889, Loss 0.24262799322605133\n",
      "[Training Epoch 3] Batch 1890, Loss 0.28618383407592773\n",
      "[Training Epoch 3] Batch 1891, Loss 0.29140108823776245\n",
      "[Training Epoch 3] Batch 1892, Loss 0.2821189761161804\n",
      "[Training Epoch 3] Batch 1893, Loss 0.3002628684043884\n",
      "[Training Epoch 3] Batch 1894, Loss 0.2948801815509796\n",
      "[Training Epoch 3] Batch 1895, Loss 0.2748597264289856\n",
      "[Training Epoch 3] Batch 1896, Loss 0.26005297899246216\n",
      "[Training Epoch 3] Batch 1897, Loss 0.26176801323890686\n",
      "[Training Epoch 3] Batch 1898, Loss 0.26492586731910706\n",
      "[Training Epoch 3] Batch 1899, Loss 0.2716108560562134\n",
      "[Training Epoch 3] Batch 1900, Loss 0.25458571314811707\n",
      "[Training Epoch 3] Batch 1901, Loss 0.25932127237319946\n",
      "[Training Epoch 3] Batch 1902, Loss 0.271671324968338\n",
      "[Training Epoch 3] Batch 1903, Loss 0.26437684893608093\n",
      "[Training Epoch 3] Batch 1904, Loss 0.2806495428085327\n",
      "[Training Epoch 3] Batch 1905, Loss 0.2818366289138794\n",
      "[Training Epoch 3] Batch 1906, Loss 0.27986329793930054\n",
      "[Training Epoch 3] Batch 1907, Loss 0.2726563811302185\n",
      "[Training Epoch 3] Batch 1908, Loss 0.2719390392303467\n",
      "[Training Epoch 3] Batch 1909, Loss 0.2600896954536438\n",
      "[Training Epoch 3] Batch 1910, Loss 0.27139413356781006\n",
      "[Training Epoch 3] Batch 1911, Loss 0.2567318081855774\n",
      "[Training Epoch 3] Batch 1912, Loss 0.2824490964412689\n",
      "[Training Epoch 3] Batch 1913, Loss 0.2744755744934082\n",
      "[Training Epoch 3] Batch 1914, Loss 0.2726905643939972\n",
      "[Training Epoch 3] Batch 1915, Loss 0.25981104373931885\n",
      "[Training Epoch 3] Batch 1916, Loss 0.2959582209587097\n",
      "[Training Epoch 3] Batch 1917, Loss 0.28055107593536377\n",
      "[Training Epoch 3] Batch 1918, Loss 0.26812177896499634\n",
      "[Training Epoch 3] Batch 1919, Loss 0.2687169909477234\n",
      "[Training Epoch 3] Batch 1920, Loss 0.27669715881347656\n",
      "[Training Epoch 3] Batch 1921, Loss 0.2652519941329956\n",
      "[Training Epoch 3] Batch 1922, Loss 0.26717936992645264\n",
      "[Training Epoch 3] Batch 1923, Loss 0.2782056927680969\n",
      "[Training Epoch 3] Batch 1924, Loss 0.26466235518455505\n",
      "[Training Epoch 3] Batch 1925, Loss 0.2975158989429474\n",
      "[Training Epoch 3] Batch 1926, Loss 0.26437243819236755\n",
      "[Training Epoch 3] Batch 1927, Loss 0.30151283740997314\n",
      "[Training Epoch 3] Batch 1928, Loss 0.25618699193000793\n",
      "[Training Epoch 3] Batch 1929, Loss 0.26975634694099426\n",
      "[Training Epoch 3] Batch 1930, Loss 0.3013114333152771\n",
      "[Training Epoch 3] Batch 1931, Loss 0.3003445863723755\n",
      "[Training Epoch 3] Batch 1932, Loss 0.25519683957099915\n",
      "[Training Epoch 3] Batch 1933, Loss 0.2897084951400757\n",
      "[Training Epoch 3] Batch 1934, Loss 0.28360897302627563\n",
      "[Training Epoch 3] Batch 1935, Loss 0.2606216073036194\n",
      "[Training Epoch 3] Batch 1936, Loss 0.2845344543457031\n",
      "[Training Epoch 3] Batch 1937, Loss 0.2615210711956024\n",
      "[Training Epoch 3] Batch 1938, Loss 0.27934497594833374\n",
      "[Training Epoch 3] Batch 1939, Loss 0.24742946028709412\n",
      "[Training Epoch 3] Batch 1940, Loss 0.2619510293006897\n",
      "[Training Epoch 3] Batch 1941, Loss 0.24938654899597168\n",
      "[Training Epoch 3] Batch 1942, Loss 0.2788854241371155\n",
      "[Training Epoch 3] Batch 1943, Loss 0.2936956584453583\n",
      "[Training Epoch 3] Batch 1944, Loss 0.27568894624710083\n",
      "[Training Epoch 3] Batch 1945, Loss 0.23659394681453705\n",
      "[Training Epoch 3] Batch 1946, Loss 0.26375460624694824\n",
      "[Training Epoch 3] Batch 1947, Loss 0.2420729547739029\n",
      "[Training Epoch 3] Batch 1948, Loss 0.2849982678890228\n",
      "[Training Epoch 3] Batch 1949, Loss 0.26193714141845703\n",
      "[Training Epoch 3] Batch 1950, Loss 0.26458364725112915\n",
      "[Training Epoch 3] Batch 1951, Loss 0.2566024661064148\n",
      "[Training Epoch 3] Batch 1952, Loss 0.25897079706192017\n",
      "[Training Epoch 3] Batch 1953, Loss 0.2712361216545105\n",
      "[Training Epoch 3] Batch 1954, Loss 0.29439252614974976\n",
      "[Training Epoch 3] Batch 1955, Loss 0.2506580352783203\n",
      "[Training Epoch 3] Batch 1956, Loss 0.2645772695541382\n",
      "[Training Epoch 3] Batch 1957, Loss 0.25519227981567383\n",
      "[Training Epoch 3] Batch 1958, Loss 0.24784104526042938\n",
      "[Training Epoch 3] Batch 1959, Loss 0.26139068603515625\n",
      "[Training Epoch 3] Batch 1960, Loss 0.2625080943107605\n",
      "[Training Epoch 3] Batch 1961, Loss 0.28699344396591187\n",
      "[Training Epoch 3] Batch 1962, Loss 0.2668190598487854\n",
      "[Training Epoch 3] Batch 1963, Loss 0.26110050082206726\n",
      "[Training Epoch 3] Batch 1964, Loss 0.26622024178504944\n",
      "[Training Epoch 3] Batch 1965, Loss 0.2703559994697571\n",
      "[Training Epoch 3] Batch 1966, Loss 0.27237796783447266\n",
      "[Training Epoch 3] Batch 1967, Loss 0.28168314695358276\n",
      "[Training Epoch 3] Batch 1968, Loss 0.2624289393424988\n",
      "[Training Epoch 3] Batch 1969, Loss 0.24764928221702576\n",
      "[Training Epoch 3] Batch 1970, Loss 0.2614319324493408\n",
      "[Training Epoch 3] Batch 1971, Loss 0.2774105370044708\n",
      "[Training Epoch 3] Batch 1972, Loss 0.25128746032714844\n",
      "[Training Epoch 3] Batch 1973, Loss 0.2806633710861206\n",
      "[Training Epoch 3] Batch 1974, Loss 0.2789822816848755\n",
      "[Training Epoch 3] Batch 1975, Loss 0.2764207422733307\n",
      "[Training Epoch 3] Batch 1976, Loss 0.2866664230823517\n",
      "[Training Epoch 3] Batch 1977, Loss 0.26348215341567993\n",
      "[Training Epoch 3] Batch 1978, Loss 0.27723973989486694\n",
      "[Training Epoch 3] Batch 1979, Loss 0.2684885859489441\n",
      "[Training Epoch 3] Batch 1980, Loss 0.2575351595878601\n",
      "[Training Epoch 3] Batch 1981, Loss 0.29847970604896545\n",
      "[Training Epoch 3] Batch 1982, Loss 0.2689739167690277\n",
      "[Training Epoch 3] Batch 1983, Loss 0.2667388916015625\n",
      "[Training Epoch 3] Batch 1984, Loss 0.26470667123794556\n",
      "[Training Epoch 3] Batch 1985, Loss 0.27004531025886536\n",
      "[Training Epoch 3] Batch 1986, Loss 0.26736193895339966\n",
      "[Training Epoch 3] Batch 1987, Loss 0.2932789623737335\n",
      "[Training Epoch 3] Batch 1988, Loss 0.2788206934928894\n",
      "[Training Epoch 3] Batch 1989, Loss 0.27836012840270996\n",
      "[Training Epoch 3] Batch 1990, Loss 0.27997082471847534\n",
      "[Training Epoch 3] Batch 1991, Loss 0.28920042514801025\n",
      "[Training Epoch 3] Batch 1992, Loss 0.2566283345222473\n",
      "[Training Epoch 3] Batch 1993, Loss 0.2670755982398987\n",
      "[Training Epoch 3] Batch 1994, Loss 0.2628001570701599\n",
      "[Training Epoch 3] Batch 1995, Loss 0.27704939246177673\n",
      "[Training Epoch 3] Batch 1996, Loss 0.26755261421203613\n",
      "[Training Epoch 3] Batch 1997, Loss 0.2753193974494934\n",
      "[Training Epoch 3] Batch 1998, Loss 0.275119423866272\n",
      "[Training Epoch 3] Batch 1999, Loss 0.27485477924346924\n",
      "[Training Epoch 3] Batch 2000, Loss 0.28217944502830505\n",
      "[Training Epoch 3] Batch 2001, Loss 0.288053959608078\n",
      "[Training Epoch 3] Batch 2002, Loss 0.27987271547317505\n",
      "[Training Epoch 3] Batch 2003, Loss 0.2731512784957886\n",
      "[Training Epoch 3] Batch 2004, Loss 0.24692213535308838\n",
      "[Training Epoch 3] Batch 2005, Loss 0.2843770980834961\n",
      "[Training Epoch 3] Batch 2006, Loss 0.24319325387477875\n",
      "[Training Epoch 3] Batch 2007, Loss 0.2488994002342224\n",
      "[Training Epoch 3] Batch 2008, Loss 0.2906298041343689\n",
      "[Training Epoch 3] Batch 2009, Loss 0.29474586248397827\n",
      "[Training Epoch 3] Batch 2010, Loss 0.29511308670043945\n",
      "[Training Epoch 3] Batch 2011, Loss 0.2718615233898163\n",
      "[Training Epoch 3] Batch 2012, Loss 0.2763406038284302\n",
      "[Training Epoch 3] Batch 2013, Loss 0.25828877091407776\n",
      "[Training Epoch 3] Batch 2014, Loss 0.2657117545604706\n",
      "[Training Epoch 3] Batch 2015, Loss 0.26293349266052246\n",
      "[Training Epoch 3] Batch 2016, Loss 0.30273109674453735\n",
      "[Training Epoch 3] Batch 2017, Loss 0.25890159606933594\n",
      "[Training Epoch 3] Batch 2018, Loss 0.2463892698287964\n",
      "[Training Epoch 3] Batch 2019, Loss 0.2734379172325134\n",
      "[Training Epoch 3] Batch 2020, Loss 0.29437193274497986\n",
      "[Training Epoch 3] Batch 2021, Loss 0.2981110215187073\n",
      "[Training Epoch 3] Batch 2022, Loss 0.27680832147598267\n",
      "[Training Epoch 3] Batch 2023, Loss 0.2793821394443512\n",
      "[Training Epoch 3] Batch 2024, Loss 0.27046188712120056\n",
      "[Training Epoch 3] Batch 2025, Loss 0.2961956560611725\n",
      "[Training Epoch 3] Batch 2026, Loss 0.2655792534351349\n",
      "[Training Epoch 3] Batch 2027, Loss 0.2662668228149414\n",
      "[Training Epoch 3] Batch 2028, Loss 0.25614872574806213\n",
      "[Training Epoch 3] Batch 2029, Loss 0.26376938819885254\n",
      "[Training Epoch 3] Batch 2030, Loss 0.2831342816352844\n",
      "[Training Epoch 3] Batch 2031, Loss 0.269107460975647\n",
      "[Training Epoch 3] Batch 2032, Loss 0.28418871760368347\n",
      "[Training Epoch 3] Batch 2033, Loss 0.24236544966697693\n",
      "[Training Epoch 3] Batch 2034, Loss 0.3036884665489197\n",
      "[Training Epoch 3] Batch 2035, Loss 0.26250526309013367\n",
      "[Training Epoch 3] Batch 2036, Loss 0.2711631953716278\n",
      "[Training Epoch 3] Batch 2037, Loss 0.2814207673072815\n",
      "[Training Epoch 3] Batch 2038, Loss 0.26195377111434937\n",
      "[Training Epoch 3] Batch 2039, Loss 0.2818809151649475\n",
      "[Training Epoch 3] Batch 2040, Loss 0.2571665048599243\n",
      "[Training Epoch 3] Batch 2041, Loss 0.2544582486152649\n",
      "[Training Epoch 3] Batch 2042, Loss 0.2890118360519409\n",
      "[Training Epoch 3] Batch 2043, Loss 0.26283007860183716\n",
      "[Training Epoch 3] Batch 2044, Loss 0.28823378682136536\n",
      "[Training Epoch 3] Batch 2045, Loss 0.2614511251449585\n",
      "[Training Epoch 3] Batch 2046, Loss 0.2802460491657257\n",
      "[Training Epoch 3] Batch 2047, Loss 0.2718517780303955\n",
      "[Training Epoch 3] Batch 2048, Loss 0.2909853458404541\n",
      "[Training Epoch 3] Batch 2049, Loss 0.26752039790153503\n",
      "[Training Epoch 3] Batch 2050, Loss 0.28277623653411865\n",
      "[Training Epoch 3] Batch 2051, Loss 0.2724974751472473\n",
      "[Training Epoch 3] Batch 2052, Loss 0.2557334899902344\n",
      "[Training Epoch 3] Batch 2053, Loss 0.26693302392959595\n",
      "[Training Epoch 3] Batch 2054, Loss 0.2927969992160797\n",
      "[Training Epoch 3] Batch 2055, Loss 0.2785292863845825\n",
      "[Training Epoch 3] Batch 2056, Loss 0.24573829770088196\n",
      "[Training Epoch 3] Batch 2057, Loss 0.26131319999694824\n",
      "[Training Epoch 3] Batch 2058, Loss 0.26262468099594116\n",
      "[Training Epoch 3] Batch 2059, Loss 0.2827075719833374\n",
      "[Training Epoch 3] Batch 2060, Loss 0.2750012278556824\n",
      "[Training Epoch 3] Batch 2061, Loss 0.2720721960067749\n",
      "[Training Epoch 3] Batch 2062, Loss 0.29681113362312317\n",
      "[Training Epoch 3] Batch 2063, Loss 0.2684948444366455\n",
      "[Training Epoch 3] Batch 2064, Loss 0.27695131301879883\n",
      "[Training Epoch 3] Batch 2065, Loss 0.2961888909339905\n",
      "[Training Epoch 3] Batch 2066, Loss 0.2653871476650238\n",
      "[Training Epoch 3] Batch 2067, Loss 0.26361680030822754\n",
      "[Training Epoch 3] Batch 2068, Loss 0.27912360429763794\n",
      "[Training Epoch 3] Batch 2069, Loss 0.25576868653297424\n",
      "[Training Epoch 3] Batch 2070, Loss 0.2729189991950989\n",
      "[Training Epoch 3] Batch 2071, Loss 0.27439072728157043\n",
      "[Training Epoch 3] Batch 2072, Loss 0.2564014792442322\n",
      "[Training Epoch 3] Batch 2073, Loss 0.2539694905281067\n",
      "[Training Epoch 3] Batch 2074, Loss 0.2813494801521301\n",
      "[Training Epoch 3] Batch 2075, Loss 0.25598764419555664\n",
      "[Training Epoch 3] Batch 2076, Loss 0.25027042627334595\n",
      "[Training Epoch 3] Batch 2077, Loss 0.275395929813385\n",
      "[Training Epoch 3] Batch 2078, Loss 0.27518564462661743\n",
      "[Training Epoch 3] Batch 2079, Loss 0.2595687508583069\n",
      "[Training Epoch 3] Batch 2080, Loss 0.2845134139060974\n",
      "[Training Epoch 3] Batch 2081, Loss 0.2580510079860687\n",
      "[Training Epoch 3] Batch 2082, Loss 0.24704952538013458\n",
      "[Training Epoch 3] Batch 2083, Loss 0.26621273159980774\n",
      "[Training Epoch 3] Batch 2084, Loss 0.27569153904914856\n",
      "[Training Epoch 3] Batch 2085, Loss 0.28129512071609497\n",
      "[Training Epoch 3] Batch 2086, Loss 0.2506759762763977\n",
      "[Training Epoch 3] Batch 2087, Loss 0.2606480121612549\n",
      "[Training Epoch 3] Batch 2088, Loss 0.2548651099205017\n",
      "[Training Epoch 3] Batch 2089, Loss 0.2561098337173462\n",
      "[Training Epoch 3] Batch 2090, Loss 0.2595868706703186\n",
      "[Training Epoch 3] Batch 2091, Loss 0.2590264678001404\n",
      "[Training Epoch 3] Batch 2092, Loss 0.24540746212005615\n",
      "[Training Epoch 3] Batch 2093, Loss 0.2458929866552353\n",
      "[Training Epoch 3] Batch 2094, Loss 0.24835681915283203\n",
      "[Training Epoch 3] Batch 2095, Loss 0.2684478163719177\n",
      "[Training Epoch 3] Batch 2096, Loss 0.22103102505207062\n",
      "[Training Epoch 3] Batch 2097, Loss 0.25162360072135925\n",
      "[Training Epoch 3] Batch 2098, Loss 0.2667863070964813\n",
      "[Training Epoch 3] Batch 2099, Loss 0.24915777146816254\n",
      "[Training Epoch 3] Batch 2100, Loss 0.27474915981292725\n",
      "[Training Epoch 3] Batch 2101, Loss 0.2545458972454071\n",
      "[Training Epoch 3] Batch 2102, Loss 0.23427797853946686\n",
      "[Training Epoch 3] Batch 2103, Loss 0.290960431098938\n",
      "[Training Epoch 3] Batch 2104, Loss 0.26559868454933167\n",
      "[Training Epoch 3] Batch 2105, Loss 0.2726398706436157\n",
      "[Training Epoch 3] Batch 2106, Loss 0.3018881678581238\n",
      "[Training Epoch 3] Batch 2107, Loss 0.27948081493377686\n",
      "[Training Epoch 3] Batch 2108, Loss 0.2999175786972046\n",
      "[Training Epoch 3] Batch 2109, Loss 0.2656863331794739\n",
      "[Training Epoch 3] Batch 2110, Loss 0.28937873244285583\n",
      "[Training Epoch 3] Batch 2111, Loss 0.28206944465637207\n",
      "[Training Epoch 3] Batch 2112, Loss 0.2860395312309265\n",
      "[Training Epoch 3] Batch 2113, Loss 0.30102989077568054\n",
      "[Training Epoch 3] Batch 2114, Loss 0.2370046228170395\n",
      "[Training Epoch 3] Batch 2115, Loss 0.24715571105480194\n",
      "[Training Epoch 3] Batch 2116, Loss 0.2504951059818268\n",
      "[Training Epoch 3] Batch 2117, Loss 0.2727392315864563\n",
      "[Training Epoch 3] Batch 2118, Loss 0.2778424024581909\n",
      "[Training Epoch 3] Batch 2119, Loss 0.27515581250190735\n",
      "[Training Epoch 3] Batch 2120, Loss 0.2522119879722595\n",
      "[Training Epoch 3] Batch 2121, Loss 0.2601916491985321\n",
      "[Training Epoch 3] Batch 2122, Loss 0.24737998843193054\n",
      "[Training Epoch 3] Batch 2123, Loss 0.2626304626464844\n",
      "[Training Epoch 3] Batch 2124, Loss 0.2516138255596161\n",
      "[Training Epoch 3] Batch 2125, Loss 0.26441043615341187\n",
      "[Training Epoch 3] Batch 2126, Loss 0.2648199498653412\n",
      "[Training Epoch 3] Batch 2127, Loss 0.27103090286254883\n",
      "[Training Epoch 3] Batch 2128, Loss 0.25694775581359863\n",
      "[Training Epoch 3] Batch 2129, Loss 0.28246259689331055\n",
      "[Training Epoch 3] Batch 2130, Loss 0.30485737323760986\n",
      "[Training Epoch 3] Batch 2131, Loss 0.28618600964546204\n",
      "[Training Epoch 3] Batch 2132, Loss 0.2627692222595215\n",
      "[Training Epoch 3] Batch 2133, Loss 0.2671282887458801\n",
      "[Training Epoch 3] Batch 2134, Loss 0.2617109417915344\n",
      "[Training Epoch 3] Batch 2135, Loss 0.26206785440444946\n",
      "[Training Epoch 3] Batch 2136, Loss 0.2786865830421448\n",
      "[Training Epoch 3] Batch 2137, Loss 0.2910342216491699\n",
      "[Training Epoch 3] Batch 2138, Loss 0.24656188488006592\n",
      "[Training Epoch 3] Batch 2139, Loss 0.2661343514919281\n",
      "[Training Epoch 3] Batch 2140, Loss 0.2797657251358032\n",
      "[Training Epoch 3] Batch 2141, Loss 0.26870179176330566\n",
      "[Training Epoch 3] Batch 2142, Loss 0.24348613619804382\n",
      "[Training Epoch 3] Batch 2143, Loss 0.2658948302268982\n",
      "[Training Epoch 3] Batch 2144, Loss 0.2505037784576416\n",
      "[Training Epoch 3] Batch 2145, Loss 0.24955707788467407\n",
      "[Training Epoch 3] Batch 2146, Loss 0.27289247512817383\n",
      "[Training Epoch 3] Batch 2147, Loss 0.2753373384475708\n",
      "[Training Epoch 3] Batch 2148, Loss 0.221973717212677\n",
      "[Training Epoch 3] Batch 2149, Loss 0.2708461880683899\n",
      "[Training Epoch 3] Batch 2150, Loss 0.29485833644866943\n",
      "[Training Epoch 3] Batch 2151, Loss 0.29736393690109253\n",
      "[Training Epoch 3] Batch 2152, Loss 0.25328177213668823\n",
      "[Training Epoch 3] Batch 2153, Loss 0.2524808645248413\n",
      "[Training Epoch 3] Batch 2154, Loss 0.26603415608406067\n",
      "[Training Epoch 3] Batch 2155, Loss 0.2749674916267395\n",
      "[Training Epoch 3] Batch 2156, Loss 0.24808180332183838\n",
      "[Training Epoch 3] Batch 2157, Loss 0.24812522530555725\n",
      "[Training Epoch 3] Batch 2158, Loss 0.27855271100997925\n",
      "[Training Epoch 3] Batch 2159, Loss 0.2707492709159851\n",
      "[Training Epoch 3] Batch 2160, Loss 0.26538825035095215\n",
      "[Training Epoch 3] Batch 2161, Loss 0.26893436908721924\n",
      "[Training Epoch 3] Batch 2162, Loss 0.25860047340393066\n",
      "[Training Epoch 3] Batch 2163, Loss 0.2735179662704468\n",
      "[Training Epoch 3] Batch 2164, Loss 0.2842698097229004\n",
      "[Training Epoch 3] Batch 2165, Loss 0.25988975167274475\n",
      "[Training Epoch 3] Batch 2166, Loss 0.2726913094520569\n",
      "[Training Epoch 3] Batch 2167, Loss 0.267983615398407\n",
      "[Training Epoch 3] Batch 2168, Loss 0.26693475246429443\n",
      "[Training Epoch 3] Batch 2169, Loss 0.2535151541233063\n",
      "[Training Epoch 3] Batch 2170, Loss 0.27415281534194946\n",
      "[Training Epoch 3] Batch 2171, Loss 0.26913073658943176\n",
      "[Training Epoch 3] Batch 2172, Loss 0.2659105956554413\n",
      "[Training Epoch 3] Batch 2173, Loss 0.2981340289115906\n",
      "[Training Epoch 3] Batch 2174, Loss 0.28090471029281616\n",
      "[Training Epoch 3] Batch 2175, Loss 0.2689545154571533\n",
      "[Training Epoch 3] Batch 2176, Loss 0.2254551351070404\n",
      "[Training Epoch 3] Batch 2177, Loss 0.2773836851119995\n",
      "[Training Epoch 3] Batch 2178, Loss 0.2756485939025879\n",
      "[Training Epoch 3] Batch 2179, Loss 0.26036322116851807\n",
      "[Training Epoch 3] Batch 2180, Loss 0.3090730607509613\n",
      "[Training Epoch 3] Batch 2181, Loss 0.25953248143196106\n",
      "[Training Epoch 3] Batch 2182, Loss 0.2541893720626831\n",
      "[Training Epoch 3] Batch 2183, Loss 0.2813072204589844\n",
      "[Training Epoch 3] Batch 2184, Loss 0.2777312397956848\n",
      "[Training Epoch 3] Batch 2185, Loss 0.2888619899749756\n",
      "[Training Epoch 3] Batch 2186, Loss 0.2774694859981537\n",
      "[Training Epoch 3] Batch 2187, Loss 0.24529194831848145\n",
      "[Training Epoch 3] Batch 2188, Loss 0.25979965925216675\n",
      "[Training Epoch 3] Batch 2189, Loss 0.2846786379814148\n",
      "[Training Epoch 3] Batch 2190, Loss 0.24621473252773285\n",
      "[Training Epoch 3] Batch 2191, Loss 0.26460105180740356\n",
      "[Training Epoch 3] Batch 2192, Loss 0.29514220356941223\n",
      "[Training Epoch 3] Batch 2193, Loss 0.30066001415252686\n",
      "[Training Epoch 3] Batch 2194, Loss 0.2438313364982605\n",
      "[Training Epoch 3] Batch 2195, Loss 0.26296135783195496\n",
      "[Training Epoch 3] Batch 2196, Loss 0.2652967572212219\n",
      "[Training Epoch 3] Batch 2197, Loss 0.27741798758506775\n",
      "[Training Epoch 3] Batch 2198, Loss 0.26850247383117676\n",
      "[Training Epoch 3] Batch 2199, Loss 0.2511098384857178\n",
      "[Training Epoch 3] Batch 2200, Loss 0.2493361532688141\n",
      "[Training Epoch 3] Batch 2201, Loss 0.2893068790435791\n",
      "[Training Epoch 3] Batch 2202, Loss 0.2610277533531189\n",
      "[Training Epoch 3] Batch 2203, Loss 0.2457541823387146\n",
      "[Training Epoch 3] Batch 2204, Loss 0.30292707681655884\n",
      "[Training Epoch 3] Batch 2205, Loss 0.2640964984893799\n",
      "[Training Epoch 3] Batch 2206, Loss 0.29765310883522034\n",
      "[Training Epoch 3] Batch 2207, Loss 0.29699283838272095\n",
      "[Training Epoch 3] Batch 2208, Loss 0.26195743680000305\n",
      "[Training Epoch 3] Batch 2209, Loss 0.28898513317108154\n",
      "[Training Epoch 3] Batch 2210, Loss 0.26777973771095276\n",
      "[Training Epoch 3] Batch 2211, Loss 0.28384554386138916\n",
      "[Training Epoch 3] Batch 2212, Loss 0.28051742911338806\n",
      "[Training Epoch 3] Batch 2213, Loss 0.28552940487861633\n",
      "[Training Epoch 3] Batch 2214, Loss 0.3031056821346283\n",
      "[Training Epoch 3] Batch 2215, Loss 0.27680838108062744\n",
      "[Training Epoch 3] Batch 2216, Loss 0.25370243191719055\n",
      "[Training Epoch 3] Batch 2217, Loss 0.2610608637332916\n",
      "[Training Epoch 3] Batch 2218, Loss 0.2536740005016327\n",
      "[Training Epoch 3] Batch 2219, Loss 0.24585041403770447\n",
      "[Training Epoch 3] Batch 2220, Loss 0.26933977007865906\n",
      "[Training Epoch 3] Batch 2221, Loss 0.2748393714427948\n",
      "[Training Epoch 3] Batch 2222, Loss 0.26782312989234924\n",
      "[Training Epoch 3] Batch 2223, Loss 0.2731046676635742\n",
      "[Training Epoch 3] Batch 2224, Loss 0.27747249603271484\n",
      "[Training Epoch 3] Batch 2225, Loss 0.26059991121292114\n",
      "[Training Epoch 3] Batch 2226, Loss 0.2711009383201599\n",
      "[Training Epoch 3] Batch 2227, Loss 0.2648177742958069\n",
      "[Training Epoch 3] Batch 2228, Loss 0.24484777450561523\n",
      "[Training Epoch 3] Batch 2229, Loss 0.2798008322715759\n",
      "[Training Epoch 3] Batch 2230, Loss 0.3134322762489319\n",
      "[Training Epoch 3] Batch 2231, Loss 0.2625943422317505\n",
      "[Training Epoch 3] Batch 2232, Loss 0.2860437035560608\n",
      "[Training Epoch 3] Batch 2233, Loss 0.2613373398780823\n",
      "[Training Epoch 3] Batch 2234, Loss 0.2838593125343323\n",
      "[Training Epoch 3] Batch 2235, Loss 0.25102728605270386\n",
      "[Training Epoch 3] Batch 2236, Loss 0.2833889126777649\n",
      "[Training Epoch 3] Batch 2237, Loss 0.2789287567138672\n",
      "[Training Epoch 3] Batch 2238, Loss 0.2732100784778595\n",
      "[Training Epoch 3] Batch 2239, Loss 0.2594325542449951\n",
      "[Training Epoch 3] Batch 2240, Loss 0.284750372171402\n",
      "[Training Epoch 3] Batch 2241, Loss 0.2601900100708008\n",
      "[Training Epoch 3] Batch 2242, Loss 0.26147711277008057\n",
      "[Training Epoch 3] Batch 2243, Loss 0.270001620054245\n",
      "[Training Epoch 3] Batch 2244, Loss 0.23439785838127136\n",
      "[Training Epoch 3] Batch 2245, Loss 0.2755257189273834\n",
      "[Training Epoch 3] Batch 2246, Loss 0.26633724570274353\n",
      "[Training Epoch 3] Batch 2247, Loss 0.292514443397522\n",
      "[Training Epoch 3] Batch 2248, Loss 0.2563757300376892\n",
      "[Training Epoch 3] Batch 2249, Loss 0.264386385679245\n",
      "[Training Epoch 3] Batch 2250, Loss 0.2745012044906616\n",
      "[Training Epoch 3] Batch 2251, Loss 0.2691029906272888\n",
      "[Training Epoch 3] Batch 2252, Loss 0.25699466466903687\n",
      "[Training Epoch 3] Batch 2253, Loss 0.27636730670928955\n",
      "[Training Epoch 3] Batch 2254, Loss 0.27797532081604004\n",
      "[Training Epoch 3] Batch 2255, Loss 0.28359854221343994\n",
      "[Training Epoch 3] Batch 2256, Loss 0.26306673884391785\n",
      "[Training Epoch 3] Batch 2257, Loss 0.268011212348938\n",
      "[Training Epoch 3] Batch 2258, Loss 0.3056963086128235\n",
      "[Training Epoch 3] Batch 2259, Loss 0.28894567489624023\n",
      "[Training Epoch 3] Batch 2260, Loss 0.25365573167800903\n",
      "[Training Epoch 3] Batch 2261, Loss 0.28021538257598877\n",
      "[Training Epoch 3] Batch 2262, Loss 0.256247878074646\n",
      "[Training Epoch 3] Batch 2263, Loss 0.27615755796432495\n",
      "[Training Epoch 3] Batch 2264, Loss 0.25029104948043823\n",
      "[Training Epoch 3] Batch 2265, Loss 0.23477932810783386\n",
      "[Training Epoch 3] Batch 2266, Loss 0.2829245924949646\n",
      "[Training Epoch 3] Batch 2267, Loss 0.2612355947494507\n",
      "[Training Epoch 3] Batch 2268, Loss 0.2458716630935669\n",
      "[Training Epoch 3] Batch 2269, Loss 0.22653616964817047\n",
      "[Training Epoch 3] Batch 2270, Loss 0.25883376598358154\n",
      "[Training Epoch 3] Batch 2271, Loss 0.27526503801345825\n",
      "[Training Epoch 3] Batch 2272, Loss 0.2724052369594574\n",
      "[Training Epoch 3] Batch 2273, Loss 0.28349241614341736\n",
      "[Training Epoch 3] Batch 2274, Loss 0.3005005121231079\n",
      "[Training Epoch 3] Batch 2275, Loss 0.2726096510887146\n",
      "[Training Epoch 3] Batch 2276, Loss 0.27487826347351074\n",
      "[Training Epoch 3] Batch 2277, Loss 0.24396821856498718\n",
      "[Training Epoch 3] Batch 2278, Loss 0.26900866627693176\n",
      "[Training Epoch 3] Batch 2279, Loss 0.2517668902873993\n",
      "[Training Epoch 3] Batch 2280, Loss 0.2568192481994629\n",
      "[Training Epoch 3] Batch 2281, Loss 0.30045896768569946\n",
      "[Training Epoch 3] Batch 2282, Loss 0.2610844671726227\n",
      "[Training Epoch 3] Batch 2283, Loss 0.30846619606018066\n",
      "[Training Epoch 3] Batch 2284, Loss 0.2652968168258667\n",
      "[Training Epoch 3] Batch 2285, Loss 0.27067118883132935\n",
      "[Training Epoch 3] Batch 2286, Loss 0.271954208612442\n",
      "[Training Epoch 3] Batch 2287, Loss 0.2841517925262451\n",
      "[Training Epoch 3] Batch 2288, Loss 0.28041237592697144\n",
      "[Training Epoch 3] Batch 2289, Loss 0.2818147540092468\n",
      "[Training Epoch 3] Batch 2290, Loss 0.2529706060886383\n",
      "[Training Epoch 3] Batch 2291, Loss 0.2969575524330139\n",
      "[Training Epoch 3] Batch 2292, Loss 0.245724618434906\n",
      "[Training Epoch 3] Batch 2293, Loss 0.27901405096054077\n",
      "[Training Epoch 3] Batch 2294, Loss 0.28412431478500366\n",
      "[Training Epoch 3] Batch 2295, Loss 0.28864338994026184\n",
      "[Training Epoch 3] Batch 2296, Loss 0.3015316426753998\n",
      "[Training Epoch 3] Batch 2297, Loss 0.2703056335449219\n",
      "[Training Epoch 3] Batch 2298, Loss 0.24929800629615784\n",
      "[Training Epoch 3] Batch 2299, Loss 0.2976110279560089\n",
      "[Training Epoch 3] Batch 2300, Loss 0.26563721895217896\n",
      "[Training Epoch 3] Batch 2301, Loss 0.28510305285453796\n",
      "[Training Epoch 3] Batch 2302, Loss 0.2740945816040039\n",
      "[Training Epoch 3] Batch 2303, Loss 0.2529991865158081\n",
      "[Training Epoch 3] Batch 2304, Loss 0.2604198455810547\n",
      "[Training Epoch 3] Batch 2305, Loss 0.27954304218292236\n",
      "[Training Epoch 3] Batch 2306, Loss 0.2576178014278412\n",
      "[Training Epoch 3] Batch 2307, Loss 0.23298555612564087\n",
      "[Training Epoch 3] Batch 2308, Loss 0.24453601241111755\n",
      "[Training Epoch 3] Batch 2309, Loss 0.2621701955795288\n",
      "[Training Epoch 3] Batch 2310, Loss 0.262934148311615\n",
      "[Training Epoch 3] Batch 2311, Loss 0.2521224617958069\n",
      "[Training Epoch 3] Batch 2312, Loss 0.26446259021759033\n",
      "[Training Epoch 3] Batch 2313, Loss 0.258430540561676\n",
      "[Training Epoch 3] Batch 2314, Loss 0.2753959596157074\n",
      "[Training Epoch 3] Batch 2315, Loss 0.27462515234947205\n",
      "[Training Epoch 3] Batch 2316, Loss 0.24947279691696167\n",
      "[Training Epoch 3] Batch 2317, Loss 0.3041209876537323\n",
      "[Training Epoch 3] Batch 2318, Loss 0.27583107352256775\n",
      "[Training Epoch 3] Batch 2319, Loss 0.2839386463165283\n",
      "[Training Epoch 3] Batch 2320, Loss 0.2733392119407654\n",
      "[Training Epoch 3] Batch 2321, Loss 0.2849268913269043\n",
      "[Training Epoch 3] Batch 2322, Loss 0.2711726725101471\n",
      "[Training Epoch 3] Batch 2323, Loss 0.27534371614456177\n",
      "[Training Epoch 3] Batch 2324, Loss 0.28953319787979126\n",
      "[Training Epoch 3] Batch 2325, Loss 0.26661527156829834\n",
      "[Training Epoch 3] Batch 2326, Loss 0.27056294679641724\n",
      "[Training Epoch 3] Batch 2327, Loss 0.29012197256088257\n",
      "[Training Epoch 3] Batch 2328, Loss 0.2621512711048126\n",
      "[Training Epoch 3] Batch 2329, Loss 0.2575717568397522\n",
      "[Training Epoch 3] Batch 2330, Loss 0.24212764203548431\n",
      "[Training Epoch 3] Batch 2331, Loss 0.27654504776000977\n",
      "[Training Epoch 3] Batch 2332, Loss 0.28962117433547974\n",
      "[Training Epoch 3] Batch 2333, Loss 0.24216705560684204\n",
      "[Training Epoch 3] Batch 2334, Loss 0.30438703298568726\n",
      "[Training Epoch 3] Batch 2335, Loss 0.2679816484451294\n",
      "[Training Epoch 3] Batch 2336, Loss 0.26811280846595764\n",
      "[Training Epoch 3] Batch 2337, Loss 0.28814658522605896\n",
      "[Training Epoch 3] Batch 2338, Loss 0.2588556408882141\n",
      "[Training Epoch 3] Batch 2339, Loss 0.2714482545852661\n",
      "[Training Epoch 3] Batch 2340, Loss 0.26910316944122314\n",
      "[Training Epoch 3] Batch 2341, Loss 0.27588650584220886\n",
      "[Training Epoch 3] Batch 2342, Loss 0.2724910080432892\n",
      "[Training Epoch 3] Batch 2343, Loss 0.2611863911151886\n",
      "[Training Epoch 3] Batch 2344, Loss 0.2929188013076782\n",
      "[Training Epoch 3] Batch 2345, Loss 0.2532799541950226\n",
      "[Training Epoch 3] Batch 2346, Loss 0.2610675096511841\n",
      "[Training Epoch 3] Batch 2347, Loss 0.31668201088905334\n",
      "[Training Epoch 3] Batch 2348, Loss 0.2658250331878662\n",
      "[Training Epoch 3] Batch 2349, Loss 0.288143515586853\n",
      "[Training Epoch 3] Batch 2350, Loss 0.2774312496185303\n",
      "[Training Epoch 3] Batch 2351, Loss 0.2807735204696655\n",
      "[Training Epoch 3] Batch 2352, Loss 0.2982567548751831\n",
      "[Training Epoch 3] Batch 2353, Loss 0.24747446179389954\n",
      "[Training Epoch 3] Batch 2354, Loss 0.28945329785346985\n",
      "[Training Epoch 3] Batch 2355, Loss 0.24420976638793945\n",
      "[Training Epoch 3] Batch 2356, Loss 0.27864667773246765\n",
      "[Training Epoch 3] Batch 2357, Loss 0.2813558280467987\n",
      "[Training Epoch 3] Batch 2358, Loss 0.2863330841064453\n",
      "[Training Epoch 3] Batch 2359, Loss 0.24067310988903046\n",
      "[Training Epoch 3] Batch 2360, Loss 0.28543952107429504\n",
      "[Training Epoch 3] Batch 2361, Loss 0.25596219301223755\n",
      "[Training Epoch 3] Batch 2362, Loss 0.2579931616783142\n",
      "[Training Epoch 3] Batch 2363, Loss 0.3007251024246216\n",
      "[Training Epoch 3] Batch 2364, Loss 0.2554396390914917\n",
      "[Training Epoch 3] Batch 2365, Loss 0.25219154357910156\n",
      "[Training Epoch 3] Batch 2366, Loss 0.2790186405181885\n",
      "[Training Epoch 3] Batch 2367, Loss 0.26536381244659424\n",
      "[Training Epoch 3] Batch 2368, Loss 0.2623134255409241\n",
      "[Training Epoch 3] Batch 2369, Loss 0.26709508895874023\n",
      "[Training Epoch 3] Batch 2370, Loss 0.2433638572692871\n",
      "[Training Epoch 3] Batch 2371, Loss 0.3163442015647888\n",
      "[Training Epoch 3] Batch 2372, Loss 0.2913704514503479\n",
      "[Training Epoch 3] Batch 2373, Loss 0.2330983281135559\n",
      "[Training Epoch 3] Batch 2374, Loss 0.2598685622215271\n",
      "[Training Epoch 3] Batch 2375, Loss 0.2683908939361572\n",
      "[Training Epoch 3] Batch 2376, Loss 0.275928795337677\n",
      "[Training Epoch 3] Batch 2377, Loss 0.2754098176956177\n",
      "[Training Epoch 3] Batch 2378, Loss 0.2635349631309509\n",
      "[Training Epoch 3] Batch 2379, Loss 0.2731238305568695\n",
      "[Training Epoch 3] Batch 2380, Loss 0.2490936815738678\n",
      "[Training Epoch 3] Batch 2381, Loss 0.26671797037124634\n",
      "[Training Epoch 3] Batch 2382, Loss 0.2556510269641876\n",
      "[Training Epoch 3] Batch 2383, Loss 0.24596285820007324\n",
      "[Training Epoch 3] Batch 2384, Loss 0.2546772360801697\n",
      "[Training Epoch 3] Batch 2385, Loss 0.2937138080596924\n",
      "[Training Epoch 3] Batch 2386, Loss 0.26095932722091675\n",
      "[Training Epoch 3] Batch 2387, Loss 0.26773497462272644\n",
      "[Training Epoch 3] Batch 2388, Loss 0.300168514251709\n",
      "[Training Epoch 3] Batch 2389, Loss 0.26298683881759644\n",
      "[Training Epoch 3] Batch 2390, Loss 0.24023570120334625\n",
      "[Training Epoch 3] Batch 2391, Loss 0.25522106885910034\n",
      "[Training Epoch 3] Batch 2392, Loss 0.29055216908454895\n",
      "[Training Epoch 3] Batch 2393, Loss 0.30779963731765747\n",
      "[Training Epoch 3] Batch 2394, Loss 0.2502554655075073\n",
      "[Training Epoch 3] Batch 2395, Loss 0.2774979770183563\n",
      "[Training Epoch 3] Batch 2396, Loss 0.2650817930698395\n",
      "[Training Epoch 3] Batch 2397, Loss 0.27811428904533386\n",
      "[Training Epoch 3] Batch 2398, Loss 0.27195602655410767\n",
      "[Training Epoch 3] Batch 2399, Loss 0.29361116886138916\n",
      "[Training Epoch 3] Batch 2400, Loss 0.2807263135910034\n",
      "[Training Epoch 3] Batch 2401, Loss 0.264069139957428\n",
      "[Training Epoch 3] Batch 2402, Loss 0.2719576954841614\n",
      "[Training Epoch 3] Batch 2403, Loss 0.2725922763347626\n",
      "[Training Epoch 3] Batch 2404, Loss 0.25904005765914917\n",
      "[Training Epoch 3] Batch 2405, Loss 0.2420351803302765\n",
      "[Training Epoch 3] Batch 2406, Loss 0.26355186104774475\n",
      "[Training Epoch 3] Batch 2407, Loss 0.2665630578994751\n",
      "[Training Epoch 3] Batch 2408, Loss 0.2709403932094574\n",
      "[Training Epoch 3] Batch 2409, Loss 0.2674606442451477\n",
      "[Training Epoch 3] Batch 2410, Loss 0.31813371181488037\n",
      "[Training Epoch 3] Batch 2411, Loss 0.2557479441165924\n",
      "[Training Epoch 3] Batch 2412, Loss 0.2636331617832184\n",
      "[Training Epoch 3] Batch 2413, Loss 0.2805822491645813\n",
      "[Training Epoch 3] Batch 2414, Loss 0.2710915803909302\n",
      "[Training Epoch 3] Batch 2415, Loss 0.27220574021339417\n",
      "[Training Epoch 3] Batch 2416, Loss 0.25781598687171936\n",
      "[Training Epoch 3] Batch 2417, Loss 0.25945138931274414\n",
      "[Training Epoch 3] Batch 2418, Loss 0.3056420087814331\n",
      "[Training Epoch 3] Batch 2419, Loss 0.24759605526924133\n",
      "[Training Epoch 3] Batch 2420, Loss 0.2589534521102905\n",
      "[Training Epoch 3] Batch 2421, Loss 0.24796688556671143\n",
      "[Training Epoch 3] Batch 2422, Loss 0.23160409927368164\n",
      "[Training Epoch 3] Batch 2423, Loss 0.2685359716415405\n",
      "[Training Epoch 3] Batch 2424, Loss 0.270174503326416\n",
      "[Training Epoch 3] Batch 2425, Loss 0.2863161563873291\n",
      "[Training Epoch 3] Batch 2426, Loss 0.2655814588069916\n",
      "[Training Epoch 3] Batch 2427, Loss 0.2932649850845337\n",
      "[Training Epoch 3] Batch 2428, Loss 0.2861539125442505\n",
      "[Training Epoch 3] Batch 2429, Loss 0.263924241065979\n",
      "[Training Epoch 3] Batch 2430, Loss 0.2742452025413513\n",
      "[Training Epoch 3] Batch 2431, Loss 0.2549811899662018\n",
      "[Training Epoch 3] Batch 2432, Loss 0.2639785408973694\n",
      "[Training Epoch 3] Batch 2433, Loss 0.2858356833457947\n",
      "[Training Epoch 3] Batch 2434, Loss 0.28419968485832214\n",
      "[Training Epoch 3] Batch 2435, Loss 0.2685837149620056\n",
      "[Training Epoch 3] Batch 2436, Loss 0.30366599559783936\n",
      "[Training Epoch 3] Batch 2437, Loss 0.29218244552612305\n",
      "[Training Epoch 3] Batch 2438, Loss 0.2658076286315918\n",
      "[Training Epoch 3] Batch 2439, Loss 0.2724601626396179\n",
      "[Training Epoch 3] Batch 2440, Loss 0.2834151089191437\n",
      "[Training Epoch 3] Batch 2441, Loss 0.2632112205028534\n",
      "[Training Epoch 3] Batch 2442, Loss 0.25452131032943726\n",
      "[Training Epoch 3] Batch 2443, Loss 0.2748492956161499\n",
      "[Training Epoch 3] Batch 2444, Loss 0.27427351474761963\n",
      "[Training Epoch 3] Batch 2445, Loss 0.26995861530303955\n",
      "[Training Epoch 3] Batch 2446, Loss 0.2713302671909332\n",
      "[Training Epoch 3] Batch 2447, Loss 0.2896506190299988\n",
      "[Training Epoch 3] Batch 2448, Loss 0.2714621424674988\n",
      "[Training Epoch 3] Batch 2449, Loss 0.27801835536956787\n",
      "[Training Epoch 3] Batch 2450, Loss 0.2509323060512543\n",
      "[Training Epoch 3] Batch 2451, Loss 0.271744966506958\n",
      "[Training Epoch 3] Batch 2452, Loss 0.27571892738342285\n",
      "[Training Epoch 3] Batch 2453, Loss 0.32099372148513794\n",
      "[Training Epoch 3] Batch 2454, Loss 0.27255523204803467\n",
      "[Training Epoch 3] Batch 2455, Loss 0.29219236969947815\n",
      "[Training Epoch 3] Batch 2456, Loss 0.2727408707141876\n",
      "[Training Epoch 3] Batch 2457, Loss 0.3114149570465088\n",
      "[Training Epoch 3] Batch 2458, Loss 0.26975730061531067\n",
      "[Training Epoch 3] Batch 2459, Loss 0.2668038308620453\n",
      "[Training Epoch 3] Batch 2460, Loss 0.2714017629623413\n",
      "[Training Epoch 3] Batch 2461, Loss 0.25171780586242676\n",
      "[Training Epoch 3] Batch 2462, Loss 0.24799948930740356\n",
      "[Training Epoch 3] Batch 2463, Loss 0.30139341950416565\n",
      "[Training Epoch 3] Batch 2464, Loss 0.24560067057609558\n",
      "[Training Epoch 3] Batch 2465, Loss 0.2776201367378235\n",
      "[Training Epoch 3] Batch 2466, Loss 0.2627722918987274\n",
      "[Training Epoch 3] Batch 2467, Loss 0.27764829993247986\n",
      "[Training Epoch 3] Batch 2468, Loss 0.24676497280597687\n",
      "[Training Epoch 3] Batch 2469, Loss 0.2958020567893982\n",
      "[Training Epoch 3] Batch 2470, Loss 0.2519233226776123\n",
      "[Training Epoch 3] Batch 2471, Loss 0.2551000118255615\n",
      "[Training Epoch 3] Batch 2472, Loss 0.2629161477088928\n",
      "[Training Epoch 3] Batch 2473, Loss 0.2920337915420532\n",
      "[Training Epoch 3] Batch 2474, Loss 0.26380378007888794\n",
      "[Training Epoch 3] Batch 2475, Loss 0.2758716940879822\n",
      "[Training Epoch 3] Batch 2476, Loss 0.26713526248931885\n",
      "[Training Epoch 3] Batch 2477, Loss 0.2825095057487488\n",
      "[Training Epoch 3] Batch 2478, Loss 0.319374144077301\n",
      "[Training Epoch 3] Batch 2479, Loss 0.26928311586380005\n",
      "[Training Epoch 3] Batch 2480, Loss 0.2585671842098236\n",
      "[Training Epoch 3] Batch 2481, Loss 0.28629356622695923\n",
      "[Training Epoch 3] Batch 2482, Loss 0.28649455308914185\n",
      "[Training Epoch 3] Batch 2483, Loss 0.24857711791992188\n",
      "[Training Epoch 3] Batch 2484, Loss 0.2843521237373352\n",
      "[Training Epoch 3] Batch 2485, Loss 0.2762065529823303\n",
      "[Training Epoch 3] Batch 2486, Loss 0.2415449619293213\n",
      "[Training Epoch 3] Batch 2487, Loss 0.28168970346450806\n",
      "[Training Epoch 3] Batch 2488, Loss 0.24366126954555511\n",
      "[Training Epoch 3] Batch 2489, Loss 0.2830800414085388\n",
      "[Training Epoch 3] Batch 2490, Loss 0.28916609287261963\n",
      "[Training Epoch 3] Batch 2491, Loss 0.26802152395248413\n",
      "[Training Epoch 3] Batch 2492, Loss 0.25828808546066284\n",
      "[Training Epoch 3] Batch 2493, Loss 0.25608518719673157\n",
      "[Training Epoch 3] Batch 2494, Loss 0.29974985122680664\n",
      "[Training Epoch 3] Batch 2495, Loss 0.2719278037548065\n",
      "[Training Epoch 3] Batch 2496, Loss 0.29330748319625854\n",
      "[Training Epoch 3] Batch 2497, Loss 0.2622820734977722\n",
      "[Training Epoch 3] Batch 2498, Loss 0.2633680999279022\n",
      "[Training Epoch 3] Batch 2499, Loss 0.24914684891700745\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2688256502151489\n",
      "[Training Epoch 3] Batch 2501, Loss 0.2815987169742584\n",
      "[Training Epoch 3] Batch 2502, Loss 0.29956695437431335\n",
      "[Training Epoch 3] Batch 2503, Loss 0.2907630205154419\n",
      "[Training Epoch 3] Batch 2504, Loss 0.27648359537124634\n",
      "[Training Epoch 3] Batch 2505, Loss 0.27870941162109375\n",
      "[Training Epoch 3] Batch 2506, Loss 0.25386354327201843\n",
      "[Training Epoch 3] Batch 2507, Loss 0.28481635451316833\n",
      "[Training Epoch 3] Batch 2508, Loss 0.2494335025548935\n",
      "[Training Epoch 3] Batch 2509, Loss 0.24412383139133453\n",
      "[Training Epoch 3] Batch 2510, Loss 0.26005280017852783\n",
      "[Training Epoch 3] Batch 2511, Loss 0.2661716938018799\n",
      "[Training Epoch 3] Batch 2512, Loss 0.2619909644126892\n",
      "[Training Epoch 3] Batch 2513, Loss 0.28166139125823975\n",
      "[Training Epoch 3] Batch 2514, Loss 0.24701043963432312\n",
      "[Training Epoch 3] Batch 2515, Loss 0.2960847020149231\n",
      "[Training Epoch 3] Batch 2516, Loss 0.27925583720207214\n",
      "[Training Epoch 3] Batch 2517, Loss 0.2720474898815155\n",
      "[Training Epoch 3] Batch 2518, Loss 0.2718290686607361\n",
      "[Training Epoch 3] Batch 2519, Loss 0.2807506024837494\n",
      "[Training Epoch 3] Batch 2520, Loss 0.2577214241027832\n",
      "[Training Epoch 3] Batch 2521, Loss 0.23885974287986755\n",
      "[Training Epoch 3] Batch 2522, Loss 0.27345848083496094\n",
      "[Training Epoch 3] Batch 2523, Loss 0.26679500937461853\n",
      "[Training Epoch 3] Batch 2524, Loss 0.2459443062543869\n",
      "[Training Epoch 3] Batch 2525, Loss 0.28490641713142395\n",
      "[Training Epoch 3] Batch 2526, Loss 0.26757919788360596\n",
      "[Training Epoch 3] Batch 2527, Loss 0.25834178924560547\n",
      "[Training Epoch 3] Batch 2528, Loss 0.244731143116951\n",
      "[Training Epoch 3] Batch 2529, Loss 0.28505557775497437\n",
      "[Training Epoch 3] Batch 2530, Loss 0.26968905329704285\n",
      "[Training Epoch 3] Batch 2531, Loss 0.27472439408302307\n",
      "[Training Epoch 3] Batch 2532, Loss 0.2722386121749878\n",
      "[Training Epoch 3] Batch 2533, Loss 0.2843676805496216\n",
      "[Training Epoch 3] Batch 2534, Loss 0.24312376976013184\n",
      "[Training Epoch 3] Batch 2535, Loss 0.268149197101593\n",
      "[Training Epoch 3] Batch 2536, Loss 0.2822140157222748\n",
      "[Training Epoch 3] Batch 2537, Loss 0.28024399280548096\n",
      "[Training Epoch 3] Batch 2538, Loss 0.2822030782699585\n",
      "[Training Epoch 3] Batch 2539, Loss 0.27385175228118896\n",
      "[Training Epoch 3] Batch 2540, Loss 0.3006778955459595\n",
      "[Training Epoch 3] Batch 2541, Loss 0.2633652091026306\n",
      "[Training Epoch 3] Batch 2542, Loss 0.28440460562705994\n",
      "[Training Epoch 3] Batch 2543, Loss 0.2870831787586212\n",
      "[Training Epoch 3] Batch 2544, Loss 0.24309614300727844\n",
      "[Training Epoch 3] Batch 2545, Loss 0.2650216221809387\n",
      "[Training Epoch 3] Batch 2546, Loss 0.2593209147453308\n",
      "[Training Epoch 3] Batch 2547, Loss 0.26057857275009155\n",
      "[Training Epoch 3] Batch 2548, Loss 0.23607106506824493\n",
      "[Training Epoch 3] Batch 2549, Loss 0.2830583155155182\n",
      "[Training Epoch 3] Batch 2550, Loss 0.25201985239982605\n",
      "[Training Epoch 3] Batch 2551, Loss 0.265741765499115\n",
      "[Training Epoch 3] Batch 2552, Loss 0.2838519215583801\n",
      "[Training Epoch 3] Batch 2553, Loss 0.28474146127700806\n",
      "[Training Epoch 3] Batch 2554, Loss 0.27201417088508606\n",
      "[Training Epoch 3] Batch 2555, Loss 0.25900256633758545\n",
      "[Training Epoch 3] Batch 2556, Loss 0.26543259620666504\n",
      "[Training Epoch 3] Batch 2557, Loss 0.28054314851760864\n",
      "[Training Epoch 3] Batch 2558, Loss 0.284351646900177\n",
      "[Training Epoch 3] Batch 2559, Loss 0.31876254081726074\n",
      "[Training Epoch 3] Batch 2560, Loss 0.27125996351242065\n",
      "[Training Epoch 3] Batch 2561, Loss 0.2775363624095917\n",
      "[Training Epoch 3] Batch 2562, Loss 0.27628064155578613\n",
      "[Training Epoch 3] Batch 2563, Loss 0.262651264667511\n",
      "[Training Epoch 3] Batch 2564, Loss 0.2724500298500061\n",
      "[Training Epoch 3] Batch 2565, Loss 0.26331833004951477\n",
      "[Training Epoch 3] Batch 2566, Loss 0.251022070646286\n",
      "[Training Epoch 3] Batch 2567, Loss 0.26477015018463135\n",
      "[Training Epoch 3] Batch 2568, Loss 0.2791489362716675\n",
      "[Training Epoch 3] Batch 2569, Loss 0.28594595193862915\n",
      "[Training Epoch 3] Batch 2570, Loss 0.2738395929336548\n",
      "[Training Epoch 3] Batch 2571, Loss 0.2716798186302185\n",
      "[Training Epoch 3] Batch 2572, Loss 0.2747724652290344\n",
      "[Training Epoch 3] Batch 2573, Loss 0.22608190774917603\n",
      "[Training Epoch 3] Batch 2574, Loss 0.26817458868026733\n",
      "[Training Epoch 3] Batch 2575, Loss 0.26440173387527466\n",
      "[Training Epoch 3] Batch 2576, Loss 0.2791230082511902\n",
      "[Training Epoch 3] Batch 2577, Loss 0.27894824743270874\n",
      "[Training Epoch 3] Batch 2578, Loss 0.2526482939720154\n",
      "[Training Epoch 3] Batch 2579, Loss 0.2749166488647461\n",
      "[Training Epoch 3] Batch 2580, Loss 0.2722533941268921\n",
      "[Training Epoch 3] Batch 2581, Loss 0.2697044014930725\n",
      "[Training Epoch 3] Batch 2582, Loss 0.24566969275474548\n",
      "[Training Epoch 3] Batch 2583, Loss 0.24777086079120636\n",
      "[Training Epoch 3] Batch 2584, Loss 0.25582486391067505\n",
      "[Training Epoch 3] Batch 2585, Loss 0.2798300087451935\n",
      "[Training Epoch 3] Batch 2586, Loss 0.2567974925041199\n",
      "[Training Epoch 3] Batch 2587, Loss 0.282833456993103\n",
      "[Training Epoch 3] Batch 2588, Loss 0.271152138710022\n",
      "[Training Epoch 3] Batch 2589, Loss 0.27620041370391846\n",
      "[Training Epoch 3] Batch 2590, Loss 0.2584351897239685\n",
      "[Training Epoch 3] Batch 2591, Loss 0.2610415518283844\n",
      "[Training Epoch 3] Batch 2592, Loss 0.31729310750961304\n",
      "[Training Epoch 3] Batch 2593, Loss 0.29243436455726624\n",
      "[Training Epoch 3] Batch 2594, Loss 0.27239668369293213\n",
      "[Training Epoch 3] Batch 2595, Loss 0.2942318022251129\n",
      "[Training Epoch 3] Batch 2596, Loss 0.24321125447750092\n",
      "[Training Epoch 3] Batch 2597, Loss 0.24719911813735962\n",
      "[Training Epoch 3] Batch 2598, Loss 0.25517743825912476\n",
      "[Training Epoch 3] Batch 2599, Loss 0.2761935591697693\n",
      "[Training Epoch 3] Batch 2600, Loss 0.28994181752204895\n",
      "[Training Epoch 3] Batch 2601, Loss 0.2804097533226013\n",
      "[Training Epoch 3] Batch 2602, Loss 0.28296318650245667\n",
      "[Training Epoch 3] Batch 2603, Loss 0.27027565240859985\n",
      "[Training Epoch 3] Batch 2604, Loss 0.2712799906730652\n",
      "[Training Epoch 3] Batch 2605, Loss 0.29325294494628906\n",
      "[Training Epoch 3] Batch 2606, Loss 0.25548356771469116\n",
      "[Training Epoch 3] Batch 2607, Loss 0.26711928844451904\n",
      "[Training Epoch 3] Batch 2608, Loss 0.2818254232406616\n",
      "[Training Epoch 3] Batch 2609, Loss 0.2850028872489929\n",
      "[Training Epoch 3] Batch 2610, Loss 0.2673228979110718\n",
      "[Training Epoch 3] Batch 2611, Loss 0.28329336643218994\n",
      "[Training Epoch 3] Batch 2612, Loss 0.25106626749038696\n",
      "[Training Epoch 3] Batch 2613, Loss 0.2337908148765564\n",
      "[Training Epoch 3] Batch 2614, Loss 0.26387444138526917\n",
      "[Training Epoch 3] Batch 2615, Loss 0.27970948815345764\n",
      "[Training Epoch 3] Batch 2616, Loss 0.2918691039085388\n",
      "[Training Epoch 3] Batch 2617, Loss 0.284079372882843\n",
      "[Training Epoch 3] Batch 2618, Loss 0.2815024256706238\n",
      "[Training Epoch 3] Batch 2619, Loss 0.2651750445365906\n",
      "[Training Epoch 3] Batch 2620, Loss 0.2363525778055191\n",
      "[Training Epoch 3] Batch 2621, Loss 0.26387834548950195\n",
      "[Training Epoch 3] Batch 2622, Loss 0.27602747082710266\n",
      "[Training Epoch 3] Batch 2623, Loss 0.2664058208465576\n",
      "[Training Epoch 3] Batch 2624, Loss 0.27006790041923523\n",
      "[Training Epoch 3] Batch 2625, Loss 0.28515303134918213\n",
      "[Training Epoch 3] Batch 2626, Loss 0.266965389251709\n",
      "[Training Epoch 3] Batch 2627, Loss 0.23985213041305542\n",
      "[Training Epoch 3] Batch 2628, Loss 0.23454873263835907\n",
      "[Training Epoch 3] Batch 2629, Loss 0.2803908586502075\n",
      "[Training Epoch 3] Batch 2630, Loss 0.2802591621875763\n",
      "[Training Epoch 3] Batch 2631, Loss 0.26631689071655273\n",
      "[Training Epoch 3] Batch 2632, Loss 0.2708151936531067\n",
      "[Training Epoch 3] Batch 2633, Loss 0.27655279636383057\n",
      "[Training Epoch 3] Batch 2634, Loss 0.24861684441566467\n",
      "[Training Epoch 3] Batch 2635, Loss 0.28448382019996643\n",
      "[Training Epoch 3] Batch 2636, Loss 0.28897446393966675\n",
      "[Training Epoch 3] Batch 2637, Loss 0.25983959436416626\n",
      "[Training Epoch 3] Batch 2638, Loss 0.28019195795059204\n",
      "[Training Epoch 3] Batch 2639, Loss 0.23034706711769104\n",
      "[Training Epoch 3] Batch 2640, Loss 0.2771080434322357\n",
      "[Training Epoch 3] Batch 2641, Loss 0.2990415096282959\n",
      "[Training Epoch 3] Batch 2642, Loss 0.2648650109767914\n",
      "[Training Epoch 3] Batch 2643, Loss 0.2813621163368225\n",
      "[Training Epoch 3] Batch 2644, Loss 0.2811536192893982\n",
      "[Training Epoch 3] Batch 2645, Loss 0.28125467896461487\n",
      "[Training Epoch 3] Batch 2646, Loss 0.2797586917877197\n",
      "[Training Epoch 3] Batch 2647, Loss 0.2640894055366516\n",
      "[Training Epoch 3] Batch 2648, Loss 0.24948519468307495\n",
      "[Training Epoch 3] Batch 2649, Loss 0.25557780265808105\n",
      "[Training Epoch 3] Batch 2650, Loss 0.27456772327423096\n",
      "[Training Epoch 3] Batch 2651, Loss 0.26872962713241577\n",
      "[Training Epoch 3] Batch 2652, Loss 0.2816672921180725\n",
      "[Training Epoch 3] Batch 2653, Loss 0.28315186500549316\n",
      "[Training Epoch 3] Batch 2654, Loss 0.262682169675827\n",
      "[Training Epoch 3] Batch 2655, Loss 0.2557048797607422\n",
      "[Training Epoch 3] Batch 2656, Loss 0.2693459391593933\n",
      "[Training Epoch 3] Batch 2657, Loss 0.25349700450897217\n",
      "[Training Epoch 3] Batch 2658, Loss 0.2945603132247925\n",
      "[Training Epoch 3] Batch 2659, Loss 0.27798280119895935\n",
      "[Training Epoch 3] Batch 2660, Loss 0.27904433012008667\n",
      "[Training Epoch 3] Batch 2661, Loss 0.2739448547363281\n",
      "[Training Epoch 3] Batch 2662, Loss 0.25849175453186035\n",
      "[Training Epoch 3] Batch 2663, Loss 0.29864272475242615\n",
      "[Training Epoch 3] Batch 2664, Loss 0.2777230143547058\n",
      "[Training Epoch 3] Batch 2665, Loss 0.2777014970779419\n",
      "[Training Epoch 3] Batch 2666, Loss 0.2513607144355774\n",
      "[Training Epoch 3] Batch 2667, Loss 0.27458059787750244\n",
      "[Training Epoch 3] Batch 2668, Loss 0.29445400834083557\n",
      "[Training Epoch 3] Batch 2669, Loss 0.2956039309501648\n",
      "[Training Epoch 3] Batch 2670, Loss 0.2775072455406189\n",
      "[Training Epoch 3] Batch 2671, Loss 0.27249813079833984\n",
      "[Training Epoch 3] Batch 2672, Loss 0.2576179802417755\n",
      "[Training Epoch 3] Batch 2673, Loss 0.28915122151374817\n",
      "[Training Epoch 3] Batch 2674, Loss 0.25717565417289734\n",
      "[Training Epoch 3] Batch 2675, Loss 0.2852247357368469\n",
      "[Training Epoch 3] Batch 2676, Loss 0.28496915102005005\n",
      "[Training Epoch 3] Batch 2677, Loss 0.26262927055358887\n",
      "[Training Epoch 3] Batch 2678, Loss 0.29752466082572937\n",
      "[Training Epoch 3] Batch 2679, Loss 0.2663494348526001\n",
      "[Training Epoch 3] Batch 2680, Loss 0.2920088768005371\n",
      "[Training Epoch 3] Batch 2681, Loss 0.2866501212120056\n",
      "[Training Epoch 3] Batch 2682, Loss 0.2885388135910034\n",
      "[Training Epoch 3] Batch 2683, Loss 0.27616986632347107\n",
      "[Training Epoch 3] Batch 2684, Loss 0.25328630208969116\n",
      "[Training Epoch 3] Batch 2685, Loss 0.2551490068435669\n",
      "[Training Epoch 3] Batch 2686, Loss 0.24301472306251526\n",
      "[Training Epoch 3] Batch 2687, Loss 0.29075607657432556\n",
      "[Training Epoch 3] Batch 2688, Loss 0.2470906376838684\n",
      "[Training Epoch 3] Batch 2689, Loss 0.2681092917919159\n",
      "[Training Epoch 3] Batch 2690, Loss 0.29398292303085327\n",
      "[Training Epoch 3] Batch 2691, Loss 0.2837175726890564\n",
      "[Training Epoch 3] Batch 2692, Loss 0.296032190322876\n",
      "[Training Epoch 3] Batch 2693, Loss 0.2809932827949524\n",
      "[Training Epoch 3] Batch 2694, Loss 0.2727987468242645\n",
      "[Training Epoch 3] Batch 2695, Loss 0.2505335509777069\n",
      "[Training Epoch 3] Batch 2696, Loss 0.27894702553749084\n",
      "[Training Epoch 3] Batch 2697, Loss 0.2826367914676666\n",
      "[Training Epoch 3] Batch 2698, Loss 0.2878974676132202\n",
      "[Training Epoch 3] Batch 2699, Loss 0.2516418695449829\n",
      "[Training Epoch 3] Batch 2700, Loss 0.2717667818069458\n",
      "[Training Epoch 3] Batch 2701, Loss 0.2575465440750122\n",
      "[Training Epoch 3] Batch 2702, Loss 0.2860383987426758\n",
      "[Training Epoch 3] Batch 2703, Loss 0.2643372416496277\n",
      "[Training Epoch 3] Batch 2704, Loss 0.2724575400352478\n",
      "[Training Epoch 3] Batch 2705, Loss 0.27378058433532715\n",
      "[Training Epoch 3] Batch 2706, Loss 0.2820812165737152\n",
      "[Training Epoch 3] Batch 2707, Loss 0.2555589973926544\n",
      "[Training Epoch 3] Batch 2708, Loss 0.27178168296813965\n",
      "[Training Epoch 3] Batch 2709, Loss 0.2849426865577698\n",
      "[Training Epoch 3] Batch 2710, Loss 0.2411406934261322\n",
      "[Training Epoch 3] Batch 2711, Loss 0.25370824337005615\n",
      "[Training Epoch 3] Batch 2712, Loss 0.2630685567855835\n",
      "[Training Epoch 3] Batch 2713, Loss 0.2929168939590454\n",
      "[Training Epoch 3] Batch 2714, Loss 0.2804678678512573\n",
      "[Training Epoch 3] Batch 2715, Loss 0.2882201671600342\n",
      "[Training Epoch 3] Batch 2716, Loss 0.2962321937084198\n",
      "[Training Epoch 3] Batch 2717, Loss 0.2886216640472412\n",
      "[Training Epoch 3] Batch 2718, Loss 0.26086968183517456\n",
      "[Training Epoch 3] Batch 2719, Loss 0.28282415866851807\n",
      "[Training Epoch 3] Batch 2720, Loss 0.2876398265361786\n",
      "[Training Epoch 3] Batch 2721, Loss 0.2642059922218323\n",
      "[Training Epoch 3] Batch 2722, Loss 0.27994588017463684\n",
      "[Training Epoch 3] Batch 2723, Loss 0.2614239454269409\n",
      "[Training Epoch 3] Batch 2724, Loss 0.262046217918396\n",
      "[Training Epoch 3] Batch 2725, Loss 0.3042599558830261\n",
      "[Training Epoch 3] Batch 2726, Loss 0.26708781719207764\n",
      "[Training Epoch 3] Batch 2727, Loss 0.30219924449920654\n",
      "[Training Epoch 3] Batch 2728, Loss 0.2455674409866333\n",
      "[Training Epoch 3] Batch 2729, Loss 0.27046316862106323\n",
      "[Training Epoch 3] Batch 2730, Loss 0.26207590103149414\n",
      "[Training Epoch 3] Batch 2731, Loss 0.2667481303215027\n",
      "[Training Epoch 3] Batch 2732, Loss 0.26552945375442505\n",
      "[Training Epoch 3] Batch 2733, Loss 0.27188149094581604\n",
      "[Training Epoch 3] Batch 2734, Loss 0.2483072280883789\n",
      "[Training Epoch 3] Batch 2735, Loss 0.27290529012680054\n",
      "[Training Epoch 3] Batch 2736, Loss 0.2735460698604584\n",
      "[Training Epoch 3] Batch 2737, Loss 0.27935928106307983\n",
      "[Training Epoch 3] Batch 2738, Loss 0.237709641456604\n",
      "[Training Epoch 3] Batch 2739, Loss 0.26996225118637085\n",
      "[Training Epoch 3] Batch 2740, Loss 0.25750768184661865\n",
      "[Training Epoch 3] Batch 2741, Loss 0.27664798498153687\n",
      "[Training Epoch 3] Batch 2742, Loss 0.26028189063072205\n",
      "[Training Epoch 3] Batch 2743, Loss 0.28420254588127136\n",
      "[Training Epoch 3] Batch 2744, Loss 0.2692890465259552\n",
      "[Training Epoch 3] Batch 2745, Loss 0.26685038208961487\n",
      "[Training Epoch 3] Batch 2746, Loss 0.2946394085884094\n",
      "[Training Epoch 3] Batch 2747, Loss 0.2501547932624817\n",
      "[Training Epoch 3] Batch 2748, Loss 0.2841637432575226\n",
      "[Training Epoch 3] Batch 2749, Loss 0.27681994438171387\n",
      "[Training Epoch 3] Batch 2750, Loss 0.2765689492225647\n",
      "[Training Epoch 3] Batch 2751, Loss 0.2421211451292038\n",
      "[Training Epoch 3] Batch 2752, Loss 0.2613682150840759\n",
      "[Training Epoch 3] Batch 2753, Loss 0.28311172127723694\n",
      "[Training Epoch 3] Batch 2754, Loss 0.2929660379886627\n",
      "[Training Epoch 3] Batch 2755, Loss 0.27661633491516113\n",
      "[Training Epoch 3] Batch 2756, Loss 0.2552264332771301\n",
      "[Training Epoch 3] Batch 2757, Loss 0.26373687386512756\n",
      "[Training Epoch 3] Batch 2758, Loss 0.30165642499923706\n",
      "[Training Epoch 3] Batch 2759, Loss 0.2894360423088074\n",
      "[Training Epoch 3] Batch 2760, Loss 0.25165796279907227\n",
      "[Training Epoch 3] Batch 2761, Loss 0.3069482743740082\n",
      "[Training Epoch 3] Batch 2762, Loss 0.25704044103622437\n",
      "[Training Epoch 3] Batch 2763, Loss 0.26519542932510376\n",
      "[Training Epoch 3] Batch 2764, Loss 0.2428121119737625\n",
      "[Training Epoch 3] Batch 2765, Loss 0.2660876512527466\n",
      "[Training Epoch 3] Batch 2766, Loss 0.24911028146743774\n",
      "[Training Epoch 3] Batch 2767, Loss 0.2582203149795532\n",
      "[Training Epoch 3] Batch 2768, Loss 0.24821078777313232\n",
      "[Training Epoch 3] Batch 2769, Loss 0.30076971650123596\n",
      "[Training Epoch 3] Batch 2770, Loss 0.23923981189727783\n",
      "[Training Epoch 3] Batch 2771, Loss 0.28610098361968994\n",
      "[Training Epoch 3] Batch 2772, Loss 0.23753486573696136\n",
      "[Training Epoch 3] Batch 2773, Loss 0.2781029939651489\n",
      "[Training Epoch 3] Batch 2774, Loss 0.28420794010162354\n",
      "[Training Epoch 3] Batch 2775, Loss 0.28515470027923584\n",
      "[Training Epoch 3] Batch 2776, Loss 0.2840074896812439\n",
      "[Training Epoch 3] Batch 2777, Loss 0.26488882303237915\n",
      "[Training Epoch 3] Batch 2778, Loss 0.27748024463653564\n",
      "[Training Epoch 3] Batch 2779, Loss 0.24428458511829376\n",
      "[Training Epoch 3] Batch 2780, Loss 0.24640382826328278\n",
      "[Training Epoch 3] Batch 2781, Loss 0.26281869411468506\n",
      "[Training Epoch 3] Batch 2782, Loss 0.28859618306159973\n",
      "[Training Epoch 3] Batch 2783, Loss 0.28680211305618286\n",
      "[Training Epoch 3] Batch 2784, Loss 0.2565210163593292\n",
      "[Training Epoch 3] Batch 2785, Loss 0.250077486038208\n",
      "[Training Epoch 3] Batch 2786, Loss 0.32566434144973755\n",
      "[Training Epoch 3] Batch 2787, Loss 0.2806127667427063\n",
      "[Training Epoch 3] Batch 2788, Loss 0.2692759037017822\n",
      "[Training Epoch 3] Batch 2789, Loss 0.2689400315284729\n",
      "[Training Epoch 3] Batch 2790, Loss 0.2809091806411743\n",
      "[Training Epoch 3] Batch 2791, Loss 0.2674470841884613\n",
      "[Training Epoch 3] Batch 2792, Loss 0.24660363793373108\n",
      "[Training Epoch 3] Batch 2793, Loss 0.27725476026535034\n",
      "[Training Epoch 3] Batch 2794, Loss 0.2920946478843689\n",
      "[Training Epoch 3] Batch 2795, Loss 0.2884058356285095\n",
      "[Training Epoch 3] Batch 2796, Loss 0.25040698051452637\n",
      "[Training Epoch 3] Batch 2797, Loss 0.25627630949020386\n",
      "[Training Epoch 3] Batch 2798, Loss 0.2706812620162964\n",
      "[Training Epoch 3] Batch 2799, Loss 0.2604694962501526\n",
      "[Training Epoch 3] Batch 2800, Loss 0.2594849765300751\n",
      "[Training Epoch 3] Batch 2801, Loss 0.28711384534835815\n",
      "[Training Epoch 3] Batch 2802, Loss 0.25622066855430603\n",
      "[Training Epoch 3] Batch 2803, Loss 0.26845499873161316\n",
      "[Training Epoch 3] Batch 2804, Loss 0.2528734803199768\n",
      "[Training Epoch 3] Batch 2805, Loss 0.2550985813140869\n",
      "[Training Epoch 3] Batch 2806, Loss 0.26323485374450684\n",
      "[Training Epoch 3] Batch 2807, Loss 0.25493502616882324\n",
      "[Training Epoch 3] Batch 2808, Loss 0.2733372151851654\n",
      "[Training Epoch 3] Batch 2809, Loss 0.2564787268638611\n",
      "[Training Epoch 3] Batch 2810, Loss 0.2748571038246155\n",
      "[Training Epoch 3] Batch 2811, Loss 0.2517569959163666\n",
      "[Training Epoch 3] Batch 2812, Loss 0.25905218720436096\n",
      "[Training Epoch 3] Batch 2813, Loss 0.3085983395576477\n",
      "[Training Epoch 3] Batch 2814, Loss 0.2625683844089508\n",
      "[Training Epoch 3] Batch 2815, Loss 0.24739345908164978\n",
      "[Training Epoch 3] Batch 2816, Loss 0.29197901487350464\n",
      "[Training Epoch 3] Batch 2817, Loss 0.2656082212924957\n",
      "[Training Epoch 3] Batch 2818, Loss 0.2655535042285919\n",
      "[Training Epoch 3] Batch 2819, Loss 0.27529752254486084\n",
      "[Training Epoch 3] Batch 2820, Loss 0.28958985209465027\n",
      "[Training Epoch 3] Batch 2821, Loss 0.26900598406791687\n",
      "[Training Epoch 3] Batch 2822, Loss 0.2610800266265869\n",
      "[Training Epoch 3] Batch 2823, Loss 0.23742660880088806\n",
      "[Training Epoch 3] Batch 2824, Loss 0.271564781665802\n",
      "[Training Epoch 3] Batch 2825, Loss 0.2800314128398895\n",
      "[Training Epoch 3] Batch 2826, Loss 0.278213232755661\n",
      "[Training Epoch 3] Batch 2827, Loss 0.255535751581192\n",
      "[Training Epoch 3] Batch 2828, Loss 0.2781483232975006\n",
      "[Training Epoch 3] Batch 2829, Loss 0.2830778956413269\n",
      "[Training Epoch 3] Batch 2830, Loss 0.2484440803527832\n",
      "[Training Epoch 3] Batch 2831, Loss 0.2605603039264679\n",
      "[Training Epoch 3] Batch 2832, Loss 0.30380648374557495\n",
      "[Training Epoch 3] Batch 2833, Loss 0.2702493369579315\n",
      "[Training Epoch 3] Batch 2834, Loss 0.3035089373588562\n",
      "[Training Epoch 3] Batch 2835, Loss 0.24853555858135223\n",
      "[Training Epoch 3] Batch 2836, Loss 0.2746999263763428\n",
      "[Training Epoch 3] Batch 2837, Loss 0.25078606605529785\n",
      "[Training Epoch 3] Batch 2838, Loss 0.2513769268989563\n",
      "[Training Epoch 3] Batch 2839, Loss 0.2781097888946533\n",
      "[Training Epoch 3] Batch 2840, Loss 0.255514532327652\n",
      "[Training Epoch 3] Batch 2841, Loss 0.2659597396850586\n",
      "[Training Epoch 3] Batch 2842, Loss 0.2876991629600525\n",
      "[Training Epoch 3] Batch 2843, Loss 0.25687700510025024\n",
      "[Training Epoch 3] Batch 2844, Loss 0.25360947847366333\n",
      "[Training Epoch 3] Batch 2845, Loss 0.2554986774921417\n",
      "[Training Epoch 3] Batch 2846, Loss 0.3157612979412079\n",
      "[Training Epoch 3] Batch 2847, Loss 0.28448042273521423\n",
      "[Training Epoch 3] Batch 2848, Loss 0.26130402088165283\n",
      "[Training Epoch 3] Batch 2849, Loss 0.29723483324050903\n",
      "[Training Epoch 3] Batch 2850, Loss 0.2706078290939331\n",
      "[Training Epoch 3] Batch 2851, Loss 0.26915132999420166\n",
      "[Training Epoch 3] Batch 2852, Loss 0.2833845019340515\n",
      "[Training Epoch 3] Batch 2853, Loss 0.268446683883667\n",
      "[Training Epoch 3] Batch 2854, Loss 0.27677398920059204\n",
      "[Training Epoch 3] Batch 2855, Loss 0.26722368597984314\n",
      "[Training Epoch 3] Batch 2856, Loss 0.2671191096305847\n",
      "[Training Epoch 3] Batch 2857, Loss 0.27639275789260864\n",
      "[Training Epoch 3] Batch 2858, Loss 0.2629246711730957\n",
      "[Training Epoch 3] Batch 2859, Loss 0.2753255069255829\n",
      "[Training Epoch 3] Batch 2860, Loss 0.29767003655433655\n",
      "[Training Epoch 3] Batch 2861, Loss 0.2485731542110443\n",
      "[Training Epoch 3] Batch 2862, Loss 0.24719075858592987\n",
      "[Training Epoch 3] Batch 2863, Loss 0.2629241347312927\n",
      "[Training Epoch 3] Batch 2864, Loss 0.27347952127456665\n",
      "[Training Epoch 3] Batch 2865, Loss 0.28247880935668945\n",
      "[Training Epoch 3] Batch 2866, Loss 0.2771139144897461\n",
      "[Training Epoch 3] Batch 2867, Loss 0.2743811011314392\n",
      "[Training Epoch 3] Batch 2868, Loss 0.26614782214164734\n",
      "[Training Epoch 3] Batch 2869, Loss 0.2950907349586487\n",
      "[Training Epoch 3] Batch 2870, Loss 0.2370162308216095\n",
      "[Training Epoch 3] Batch 2871, Loss 0.2733297049999237\n",
      "[Training Epoch 3] Batch 2872, Loss 0.253656268119812\n",
      "[Training Epoch 3] Batch 2873, Loss 0.2585983872413635\n",
      "[Training Epoch 3] Batch 2874, Loss 0.27806711196899414\n",
      "[Training Epoch 3] Batch 2875, Loss 0.290152370929718\n",
      "[Training Epoch 3] Batch 2876, Loss 0.2657512426376343\n",
      "[Training Epoch 3] Batch 2877, Loss 0.29184210300445557\n",
      "[Training Epoch 3] Batch 2878, Loss 0.2544366419315338\n",
      "[Training Epoch 3] Batch 2879, Loss 0.3122124671936035\n",
      "[Training Epoch 3] Batch 2880, Loss 0.2742061913013458\n",
      "[Training Epoch 3] Batch 2881, Loss 0.24974429607391357\n",
      "[Training Epoch 3] Batch 2882, Loss 0.296653687953949\n",
      "[Training Epoch 3] Batch 2883, Loss 0.2384786605834961\n",
      "[Training Epoch 3] Batch 2884, Loss 0.2427724301815033\n",
      "[Training Epoch 3] Batch 2885, Loss 0.26368987560272217\n",
      "[Training Epoch 3] Batch 2886, Loss 0.23579230904579163\n",
      "[Training Epoch 3] Batch 2887, Loss 0.2756235599517822\n",
      "[Training Epoch 3] Batch 2888, Loss 0.32384446263313293\n",
      "[Training Epoch 3] Batch 2889, Loss 0.24630628526210785\n",
      "[Training Epoch 3] Batch 2890, Loss 0.27488744258880615\n",
      "[Training Epoch 3] Batch 2891, Loss 0.275585800409317\n",
      "[Training Epoch 3] Batch 2892, Loss 0.26466232538223267\n",
      "[Training Epoch 3] Batch 2893, Loss 0.27670958638191223\n",
      "[Training Epoch 3] Batch 2894, Loss 0.2709222733974457\n",
      "[Training Epoch 3] Batch 2895, Loss 0.27778682112693787\n",
      "[Training Epoch 3] Batch 2896, Loss 0.2510764002799988\n",
      "[Training Epoch 3] Batch 2897, Loss 0.27868667244911194\n",
      "[Training Epoch 3] Batch 2898, Loss 0.2814241647720337\n",
      "[Training Epoch 3] Batch 2899, Loss 0.2527284622192383\n",
      "[Training Epoch 3] Batch 2900, Loss 0.27019113302230835\n",
      "[Training Epoch 3] Batch 2901, Loss 0.2541062831878662\n",
      "[Training Epoch 3] Batch 2902, Loss 0.26958340406417847\n",
      "[Training Epoch 3] Batch 2903, Loss 0.2541388273239136\n",
      "[Training Epoch 3] Batch 2904, Loss 0.2722766101360321\n",
      "[Training Epoch 3] Batch 2905, Loss 0.26958075165748596\n",
      "[Training Epoch 3] Batch 2906, Loss 0.2888316214084625\n",
      "[Training Epoch 3] Batch 2907, Loss 0.2521330714225769\n",
      "[Training Epoch 3] Batch 2908, Loss 0.2848406434059143\n",
      "[Training Epoch 3] Batch 2909, Loss 0.28182220458984375\n",
      "[Training Epoch 3] Batch 2910, Loss 0.2657545208930969\n",
      "[Training Epoch 3] Batch 2911, Loss 0.2770872414112091\n",
      "[Training Epoch 3] Batch 2912, Loss 0.3025209903717041\n",
      "[Training Epoch 3] Batch 2913, Loss 0.2757432460784912\n",
      "[Training Epoch 3] Batch 2914, Loss 0.273028165102005\n",
      "[Training Epoch 3] Batch 2915, Loss 0.2767336368560791\n",
      "[Training Epoch 3] Batch 2916, Loss 0.29628974199295044\n",
      "[Training Epoch 3] Batch 2917, Loss 0.2780357599258423\n",
      "[Training Epoch 3] Batch 2918, Loss 0.2726536989212036\n",
      "[Training Epoch 3] Batch 2919, Loss 0.2570438086986542\n",
      "[Training Epoch 3] Batch 2920, Loss 0.26626914739608765\n",
      "[Training Epoch 3] Batch 2921, Loss 0.2640019655227661\n",
      "[Training Epoch 3] Batch 2922, Loss 0.26745545864105225\n",
      "[Training Epoch 3] Batch 2923, Loss 0.27394136786460876\n",
      "[Training Epoch 3] Batch 2924, Loss 0.27333390712738037\n",
      "[Training Epoch 3] Batch 2925, Loss 0.2848495841026306\n",
      "[Training Epoch 3] Batch 2926, Loss 0.2795536518096924\n",
      "[Training Epoch 3] Batch 2927, Loss 0.3056730031967163\n",
      "[Training Epoch 3] Batch 2928, Loss 0.26262563467025757\n",
      "[Training Epoch 3] Batch 2929, Loss 0.2998858690261841\n",
      "[Training Epoch 3] Batch 2930, Loss 0.28416454792022705\n",
      "[Training Epoch 3] Batch 2931, Loss 0.28088533878326416\n",
      "[Training Epoch 3] Batch 2932, Loss 0.25444746017456055\n",
      "[Training Epoch 3] Batch 2933, Loss 0.2754972577095032\n",
      "[Training Epoch 3] Batch 2934, Loss 0.2433966100215912\n",
      "[Training Epoch 3] Batch 2935, Loss 0.2950555384159088\n",
      "[Training Epoch 3] Batch 2936, Loss 0.2954419255256653\n",
      "[Training Epoch 3] Batch 2937, Loss 0.2643149197101593\n",
      "[Training Epoch 3] Batch 2938, Loss 0.27632245421409607\n",
      "[Training Epoch 3] Batch 2939, Loss 0.2642750144004822\n",
      "[Training Epoch 3] Batch 2940, Loss 0.30237507820129395\n",
      "[Training Epoch 3] Batch 2941, Loss 0.25279873609542847\n",
      "[Training Epoch 3] Batch 2942, Loss 0.2462129294872284\n",
      "[Training Epoch 3] Batch 2943, Loss 0.2701500356197357\n",
      "[Training Epoch 3] Batch 2944, Loss 0.2762622535228729\n",
      "[Training Epoch 3] Batch 2945, Loss 0.2552977502346039\n",
      "[Training Epoch 3] Batch 2946, Loss 0.24973660707473755\n",
      "[Training Epoch 3] Batch 2947, Loss 0.28261035680770874\n",
      "[Training Epoch 3] Batch 2948, Loss 0.2808076739311218\n",
      "[Training Epoch 3] Batch 2949, Loss 0.25646448135375977\n",
      "[Training Epoch 3] Batch 2950, Loss 0.2581239938735962\n",
      "[Training Epoch 3] Batch 2951, Loss 0.27241042256355286\n",
      "[Training Epoch 3] Batch 2952, Loss 0.2669830024242401\n",
      "[Training Epoch 3] Batch 2953, Loss 0.26770976185798645\n",
      "[Training Epoch 3] Batch 2954, Loss 0.2731582820415497\n",
      "[Training Epoch 3] Batch 2955, Loss 0.2815761864185333\n",
      "[Training Epoch 3] Batch 2956, Loss 0.2682090997695923\n",
      "[Training Epoch 3] Batch 2957, Loss 0.2574126720428467\n",
      "[Training Epoch 3] Batch 2958, Loss 0.29543671011924744\n",
      "[Training Epoch 3] Batch 2959, Loss 0.2748411297798157\n",
      "[Training Epoch 3] Batch 2960, Loss 0.27715516090393066\n",
      "[Training Epoch 3] Batch 2961, Loss 0.27494704723358154\n",
      "[Training Epoch 3] Batch 2962, Loss 0.2581154704093933\n",
      "[Training Epoch 3] Batch 2963, Loss 0.26634863018989563\n",
      "[Training Epoch 3] Batch 2964, Loss 0.2589973509311676\n",
      "[Training Epoch 3] Batch 2965, Loss 0.25931516289711\n",
      "[Training Epoch 3] Batch 2966, Loss 0.25942590832710266\n",
      "[Training Epoch 3] Batch 2967, Loss 0.2547535300254822\n",
      "[Training Epoch 3] Batch 2968, Loss 0.27689749002456665\n",
      "[Training Epoch 3] Batch 2969, Loss 0.2817244529724121\n",
      "[Training Epoch 3] Batch 2970, Loss 0.2578951120376587\n",
      "[Training Epoch 3] Batch 2971, Loss 0.24938330054283142\n",
      "[Training Epoch 3] Batch 2972, Loss 0.25634250044822693\n",
      "[Training Epoch 3] Batch 2973, Loss 0.2713974714279175\n",
      "[Training Epoch 3] Batch 2974, Loss 0.2653248906135559\n",
      "[Training Epoch 3] Batch 2975, Loss 0.2678736448287964\n",
      "[Training Epoch 3] Batch 2976, Loss 0.2538401186466217\n",
      "[Training Epoch 3] Batch 2977, Loss 0.2699022889137268\n",
      "[Training Epoch 3] Batch 2978, Loss 0.3009834587574005\n",
      "[Training Epoch 3] Batch 2979, Loss 0.2818174660205841\n",
      "[Training Epoch 3] Batch 2980, Loss 0.26919883489608765\n",
      "[Training Epoch 3] Batch 2981, Loss 0.27543124556541443\n",
      "[Training Epoch 3] Batch 2982, Loss 0.25274693965911865\n",
      "[Training Epoch 3] Batch 2983, Loss 0.27302923798561096\n",
      "[Training Epoch 3] Batch 2984, Loss 0.27073734998703003\n",
      "[Training Epoch 3] Batch 2985, Loss 0.27925315499305725\n",
      "[Training Epoch 3] Batch 2986, Loss 0.25239092111587524\n",
      "[Training Epoch 3] Batch 2987, Loss 0.26372355222702026\n",
      "[Training Epoch 3] Batch 2988, Loss 0.24059675633907318\n",
      "[Training Epoch 3] Batch 2989, Loss 0.2692537009716034\n",
      "[Training Epoch 3] Batch 2990, Loss 0.2391386181116104\n",
      "[Training Epoch 3] Batch 2991, Loss 0.25954514741897583\n",
      "[Training Epoch 3] Batch 2992, Loss 0.2607513666152954\n",
      "[Training Epoch 3] Batch 2993, Loss 0.28806793689727783\n",
      "[Training Epoch 3] Batch 2994, Loss 0.2636794447898865\n",
      "[Training Epoch 3] Batch 2995, Loss 0.2918906807899475\n",
      "[Training Epoch 3] Batch 2996, Loss 0.2509855031967163\n",
      "[Training Epoch 3] Batch 2997, Loss 0.2598220705986023\n",
      "[Training Epoch 3] Batch 2998, Loss 0.25810033082962036\n",
      "[Training Epoch 3] Batch 2999, Loss 0.2596343159675598\n",
      "[Training Epoch 3] Batch 3000, Loss 0.2776833772659302\n",
      "[Training Epoch 3] Batch 3001, Loss 0.2626166343688965\n",
      "[Training Epoch 3] Batch 3002, Loss 0.2728971838951111\n",
      "[Training Epoch 3] Batch 3003, Loss 0.3072715103626251\n",
      "[Training Epoch 3] Batch 3004, Loss 0.25728678703308105\n",
      "[Training Epoch 3] Batch 3005, Loss 0.28758901357650757\n",
      "[Training Epoch 3] Batch 3006, Loss 0.282492458820343\n",
      "[Training Epoch 3] Batch 3007, Loss 0.2862599492073059\n",
      "[Training Epoch 3] Batch 3008, Loss 0.26318877935409546\n",
      "[Training Epoch 3] Batch 3009, Loss 0.2582322955131531\n",
      "[Training Epoch 3] Batch 3010, Loss 0.3022202253341675\n",
      "[Training Epoch 3] Batch 3011, Loss 0.2787598669528961\n",
      "[Training Epoch 3] Batch 3012, Loss 0.26769906282424927\n",
      "[Training Epoch 3] Batch 3013, Loss 0.2541859745979309\n",
      "[Training Epoch 3] Batch 3014, Loss 0.29138320684432983\n",
      "[Training Epoch 3] Batch 3015, Loss 0.2646777629852295\n",
      "[Training Epoch 3] Batch 3016, Loss 0.25851666927337646\n",
      "[Training Epoch 3] Batch 3017, Loss 0.28449612855911255\n",
      "[Training Epoch 3] Batch 3018, Loss 0.249093696475029\n",
      "[Training Epoch 3] Batch 3019, Loss 0.26192590594291687\n",
      "[Training Epoch 3] Batch 3020, Loss 0.26700058579444885\n",
      "[Training Epoch 3] Batch 3021, Loss 0.24735891819000244\n",
      "[Training Epoch 3] Batch 3022, Loss 0.28105705976486206\n",
      "[Training Epoch 3] Batch 3023, Loss 0.26561886072158813\n",
      "[Training Epoch 3] Batch 3024, Loss 0.25163978338241577\n",
      "[Training Epoch 3] Batch 3025, Loss 0.25825726985931396\n",
      "[Training Epoch 3] Batch 3026, Loss 0.2715183198451996\n",
      "[Training Epoch 3] Batch 3027, Loss 0.24082574248313904\n",
      "[Training Epoch 3] Batch 3028, Loss 0.27141159772872925\n",
      "[Training Epoch 3] Batch 3029, Loss 0.2438201606273651\n",
      "[Training Epoch 3] Batch 3030, Loss 0.2892763018608093\n",
      "[Training Epoch 3] Batch 3031, Loss 0.28211289644241333\n",
      "[Training Epoch 3] Batch 3032, Loss 0.2895643413066864\n",
      "[Training Epoch 3] Batch 3033, Loss 0.2604004144668579\n",
      "[Training Epoch 3] Batch 3034, Loss 0.3284311294555664\n",
      "[Training Epoch 3] Batch 3035, Loss 0.2407645285129547\n",
      "[Training Epoch 3] Batch 3036, Loss 0.2825945019721985\n",
      "[Training Epoch 3] Batch 3037, Loss 0.27733683586120605\n",
      "[Training Epoch 3] Batch 3038, Loss 0.30563879013061523\n",
      "[Training Epoch 3] Batch 3039, Loss 0.291140079498291\n",
      "[Training Epoch 3] Batch 3040, Loss 0.2995465397834778\n",
      "[Training Epoch 3] Batch 3041, Loss 0.27028873562812805\n",
      "[Training Epoch 3] Batch 3042, Loss 0.29416006803512573\n",
      "[Training Epoch 3] Batch 3043, Loss 0.27851366996765137\n",
      "[Training Epoch 3] Batch 3044, Loss 0.2428569793701172\n",
      "[Training Epoch 3] Batch 3045, Loss 0.28654995560646057\n",
      "[Training Epoch 3] Batch 3046, Loss 0.2930445671081543\n",
      "[Training Epoch 3] Batch 3047, Loss 0.29323774576187134\n",
      "[Training Epoch 3] Batch 3048, Loss 0.2952117919921875\n",
      "[Training Epoch 3] Batch 3049, Loss 0.2539997398853302\n",
      "[Training Epoch 3] Batch 3050, Loss 0.24532756209373474\n",
      "[Training Epoch 3] Batch 3051, Loss 0.26981550455093384\n",
      "[Training Epoch 3] Batch 3052, Loss 0.29586297273635864\n",
      "[Training Epoch 3] Batch 3053, Loss 0.2766716182231903\n",
      "[Training Epoch 3] Batch 3054, Loss 0.22126328945159912\n",
      "[Training Epoch 3] Batch 3055, Loss 0.26456713676452637\n",
      "[Training Epoch 3] Batch 3056, Loss 0.2823626399040222\n",
      "[Training Epoch 3] Batch 3057, Loss 0.2765064835548401\n",
      "[Training Epoch 3] Batch 3058, Loss 0.2646874189376831\n",
      "[Training Epoch 3] Batch 3059, Loss 0.2559608221054077\n",
      "[Training Epoch 3] Batch 3060, Loss 0.26922333240509033\n",
      "[Training Epoch 3] Batch 3061, Loss 0.2558321952819824\n",
      "[Training Epoch 3] Batch 3062, Loss 0.2787446975708008\n",
      "[Training Epoch 3] Batch 3063, Loss 0.2580176591873169\n",
      "[Training Epoch 3] Batch 3064, Loss 0.2734745740890503\n",
      "[Training Epoch 3] Batch 3065, Loss 0.26158666610717773\n",
      "[Training Epoch 3] Batch 3066, Loss 0.28006628155708313\n",
      "[Training Epoch 3] Batch 3067, Loss 0.26732856035232544\n",
      "[Training Epoch 3] Batch 3068, Loss 0.24970212578773499\n",
      "[Training Epoch 3] Batch 3069, Loss 0.26792389154434204\n",
      "[Training Epoch 3] Batch 3070, Loss 0.26371702551841736\n",
      "[Training Epoch 3] Batch 3071, Loss 0.29837289452552795\n",
      "[Training Epoch 3] Batch 3072, Loss 0.26259350776672363\n",
      "[Training Epoch 3] Batch 3073, Loss 0.26511770486831665\n",
      "[Training Epoch 3] Batch 3074, Loss 0.27845266461372375\n",
      "[Training Epoch 3] Batch 3075, Loss 0.2897379696369171\n",
      "[Training Epoch 3] Batch 3076, Loss 0.2715873718261719\n",
      "[Training Epoch 3] Batch 3077, Loss 0.261232852935791\n",
      "[Training Epoch 3] Batch 3078, Loss 0.26887404918670654\n",
      "[Training Epoch 3] Batch 3079, Loss 0.2760268449783325\n",
      "[Training Epoch 3] Batch 3080, Loss 0.25821489095687866\n",
      "[Training Epoch 3] Batch 3081, Loss 0.30842798948287964\n",
      "[Training Epoch 3] Batch 3082, Loss 0.2699243426322937\n",
      "[Training Epoch 3] Batch 3083, Loss 0.2666205167770386\n",
      "[Training Epoch 3] Batch 3084, Loss 0.2566470503807068\n",
      "[Training Epoch 3] Batch 3085, Loss 0.2666480541229248\n",
      "[Training Epoch 3] Batch 3086, Loss 0.265824556350708\n",
      "[Training Epoch 3] Batch 3087, Loss 0.2791103720664978\n",
      "[Training Epoch 3] Batch 3088, Loss 0.2819141447544098\n",
      "[Training Epoch 3] Batch 3089, Loss 0.26028239727020264\n",
      "[Training Epoch 3] Batch 3090, Loss 0.27070921659469604\n",
      "[Training Epoch 3] Batch 3091, Loss 0.2675132751464844\n",
      "[Training Epoch 3] Batch 3092, Loss 0.2628922760486603\n",
      "[Training Epoch 3] Batch 3093, Loss 0.2734139561653137\n",
      "[Training Epoch 3] Batch 3094, Loss 0.28443270921707153\n",
      "[Training Epoch 3] Batch 3095, Loss 0.24093462526798248\n",
      "[Training Epoch 3] Batch 3096, Loss 0.2912370562553406\n",
      "[Training Epoch 3] Batch 3097, Loss 0.2784131169319153\n",
      "[Training Epoch 3] Batch 3098, Loss 0.2710716724395752\n",
      "[Training Epoch 3] Batch 3099, Loss 0.26907312870025635\n",
      "[Training Epoch 3] Batch 3100, Loss 0.2716270983219147\n",
      "[Training Epoch 3] Batch 3101, Loss 0.2920486330986023\n",
      "[Training Epoch 3] Batch 3102, Loss 0.2934138774871826\n",
      "[Training Epoch 3] Batch 3103, Loss 0.292766809463501\n",
      "[Training Epoch 3] Batch 3104, Loss 0.27505919337272644\n",
      "[Training Epoch 3] Batch 3105, Loss 0.2911856770515442\n",
      "[Training Epoch 3] Batch 3106, Loss 0.2715091407299042\n",
      "[Training Epoch 3] Batch 3107, Loss 0.2508319020271301\n",
      "[Training Epoch 3] Batch 3108, Loss 0.2637736201286316\n",
      "[Training Epoch 3] Batch 3109, Loss 0.23915788531303406\n",
      "[Training Epoch 3] Batch 3110, Loss 0.2777255177497864\n",
      "[Training Epoch 3] Batch 3111, Loss 0.24339330196380615\n",
      "[Training Epoch 3] Batch 3112, Loss 0.28793296217918396\n",
      "[Training Epoch 3] Batch 3113, Loss 0.27203091979026794\n",
      "[Training Epoch 3] Batch 3114, Loss 0.2746012508869171\n",
      "[Training Epoch 3] Batch 3115, Loss 0.274193674325943\n",
      "[Training Epoch 3] Batch 3116, Loss 0.2665930390357971\n",
      "[Training Epoch 3] Batch 3117, Loss 0.25827568769454956\n",
      "[Training Epoch 3] Batch 3118, Loss 0.2662401795387268\n",
      "[Training Epoch 3] Batch 3119, Loss 0.29138267040252686\n",
      "[Training Epoch 3] Batch 3120, Loss 0.2635488510131836\n",
      "[Training Epoch 3] Batch 3121, Loss 0.2685457170009613\n",
      "[Training Epoch 3] Batch 3122, Loss 0.24679164588451385\n",
      "[Training Epoch 3] Batch 3123, Loss 0.2740119695663452\n",
      "[Training Epoch 3] Batch 3124, Loss 0.27789461612701416\n",
      "[Training Epoch 3] Batch 3125, Loss 0.27952998876571655\n",
      "[Training Epoch 3] Batch 3126, Loss 0.2941911816596985\n",
      "[Training Epoch 3] Batch 3127, Loss 0.25026869773864746\n",
      "[Training Epoch 3] Batch 3128, Loss 0.2519071698188782\n",
      "[Training Epoch 3] Batch 3129, Loss 0.26325905323028564\n",
      "[Training Epoch 3] Batch 3130, Loss 0.27742934226989746\n",
      "[Training Epoch 3] Batch 3131, Loss 0.2736373543739319\n",
      "[Training Epoch 3] Batch 3132, Loss 0.3160937428474426\n",
      "[Training Epoch 3] Batch 3133, Loss 0.2670454978942871\n",
      "[Training Epoch 3] Batch 3134, Loss 0.25748294591903687\n",
      "[Training Epoch 3] Batch 3135, Loss 0.2454390674829483\n",
      "[Training Epoch 3] Batch 3136, Loss 0.24053719639778137\n",
      "[Training Epoch 3] Batch 3137, Loss 0.2580956518650055\n",
      "[Training Epoch 3] Batch 3138, Loss 0.2616422176361084\n",
      "[Training Epoch 3] Batch 3139, Loss 0.25918713212013245\n",
      "[Training Epoch 3] Batch 3140, Loss 0.2778765857219696\n",
      "[Training Epoch 3] Batch 3141, Loss 0.2727949619293213\n",
      "[Training Epoch 3] Batch 3142, Loss 0.26732516288757324\n",
      "[Training Epoch 3] Batch 3143, Loss 0.25976836681365967\n",
      "[Training Epoch 3] Batch 3144, Loss 0.2786981761455536\n",
      "[Training Epoch 3] Batch 3145, Loss 0.25367864966392517\n",
      "[Training Epoch 3] Batch 3146, Loss 0.2679986357688904\n",
      "[Training Epoch 3] Batch 3147, Loss 0.27029797434806824\n",
      "[Training Epoch 3] Batch 3148, Loss 0.2566269040107727\n",
      "[Training Epoch 3] Batch 3149, Loss 0.2609509229660034\n",
      "[Training Epoch 3] Batch 3150, Loss 0.26420244574546814\n",
      "[Training Epoch 3] Batch 3151, Loss 0.26087066531181335\n",
      "[Training Epoch 3] Batch 3152, Loss 0.3028486371040344\n",
      "[Training Epoch 3] Batch 3153, Loss 0.2668207883834839\n",
      "[Training Epoch 3] Batch 3154, Loss 0.24717792868614197\n",
      "[Training Epoch 3] Batch 3155, Loss 0.2542579174041748\n",
      "[Training Epoch 3] Batch 3156, Loss 0.24725861847400665\n",
      "[Training Epoch 3] Batch 3157, Loss 0.258698970079422\n",
      "[Training Epoch 3] Batch 3158, Loss 0.25873690843582153\n",
      "[Training Epoch 3] Batch 3159, Loss 0.24341794848442078\n",
      "[Training Epoch 3] Batch 3160, Loss 0.271575927734375\n",
      "[Training Epoch 3] Batch 3161, Loss 0.24586865305900574\n",
      "[Training Epoch 3] Batch 3162, Loss 0.25337862968444824\n",
      "[Training Epoch 3] Batch 3163, Loss 0.2988946735858917\n",
      "[Training Epoch 3] Batch 3164, Loss 0.25766801834106445\n",
      "[Training Epoch 3] Batch 3165, Loss 0.30556926131248474\n",
      "[Training Epoch 3] Batch 3166, Loss 0.29445332288742065\n",
      "[Training Epoch 3] Batch 3167, Loss 0.24513030052185059\n",
      "[Training Epoch 3] Batch 3168, Loss 0.28747761249542236\n",
      "[Training Epoch 3] Batch 3169, Loss 0.2702787220478058\n",
      "[Training Epoch 3] Batch 3170, Loss 0.25994160771369934\n",
      "[Training Epoch 3] Batch 3171, Loss 0.26790785789489746\n",
      "[Training Epoch 3] Batch 3172, Loss 0.24917346239089966\n",
      "[Training Epoch 3] Batch 3173, Loss 0.2656738758087158\n",
      "[Training Epoch 3] Batch 3174, Loss 0.2613019645214081\n",
      "[Training Epoch 3] Batch 3175, Loss 0.2565152049064636\n",
      "[Training Epoch 3] Batch 3176, Loss 0.2719690203666687\n",
      "[Training Epoch 3] Batch 3177, Loss 0.24012188613414764\n",
      "[Training Epoch 3] Batch 3178, Loss 0.29180455207824707\n",
      "[Training Epoch 3] Batch 3179, Loss 0.2950238585472107\n",
      "[Training Epoch 3] Batch 3180, Loss 0.25505542755126953\n",
      "[Training Epoch 3] Batch 3181, Loss 0.23735594749450684\n",
      "[Training Epoch 3] Batch 3182, Loss 0.2979695200920105\n",
      "[Training Epoch 3] Batch 3183, Loss 0.28996747732162476\n",
      "[Training Epoch 3] Batch 3184, Loss 0.2790006995201111\n",
      "[Training Epoch 3] Batch 3185, Loss 0.24070340394973755\n",
      "[Training Epoch 3] Batch 3186, Loss 0.24166753888130188\n",
      "[Training Epoch 3] Batch 3187, Loss 0.2599118947982788\n",
      "[Training Epoch 3] Batch 3188, Loss 0.25295495986938477\n",
      "[Training Epoch 3] Batch 3189, Loss 0.25918105244636536\n",
      "[Training Epoch 3] Batch 3190, Loss 0.2651159167289734\n",
      "[Training Epoch 3] Batch 3191, Loss 0.27916499972343445\n",
      "[Training Epoch 3] Batch 3192, Loss 0.26990169286727905\n",
      "[Training Epoch 3] Batch 3193, Loss 0.28660446405410767\n",
      "[Training Epoch 3] Batch 3194, Loss 0.2659919261932373\n",
      "[Training Epoch 3] Batch 3195, Loss 0.29892390966415405\n",
      "[Training Epoch 3] Batch 3196, Loss 0.2656332850456238\n",
      "[Training Epoch 3] Batch 3197, Loss 0.2664946913719177\n",
      "[Training Epoch 3] Batch 3198, Loss 0.26148590445518494\n",
      "[Training Epoch 3] Batch 3199, Loss 0.25866618752479553\n",
      "[Training Epoch 3] Batch 3200, Loss 0.26967960596084595\n",
      "[Training Epoch 3] Batch 3201, Loss 0.2949070334434509\n",
      "[Training Epoch 3] Batch 3202, Loss 0.2363055944442749\n",
      "[Training Epoch 3] Batch 3203, Loss 0.2703307271003723\n",
      "[Training Epoch 3] Batch 3204, Loss 0.26641833782196045\n",
      "[Training Epoch 3] Batch 3205, Loss 0.2859201431274414\n",
      "[Training Epoch 3] Batch 3206, Loss 0.2463909089565277\n",
      "[Training Epoch 3] Batch 3207, Loss 0.2574608027935028\n",
      "[Training Epoch 3] Batch 3208, Loss 0.27450740337371826\n",
      "[Training Epoch 3] Batch 3209, Loss 0.26011067628860474\n",
      "[Training Epoch 3] Batch 3210, Loss 0.2711673378944397\n",
      "[Training Epoch 3] Batch 3211, Loss 0.28184449672698975\n",
      "[Training Epoch 3] Batch 3212, Loss 0.26793086528778076\n",
      "[Training Epoch 3] Batch 3213, Loss 0.297600656747818\n",
      "[Training Epoch 3] Batch 3214, Loss 0.283138632774353\n",
      "[Training Epoch 3] Batch 3215, Loss 0.25678128004074097\n",
      "[Training Epoch 3] Batch 3216, Loss 0.271179735660553\n",
      "[Training Epoch 3] Batch 3217, Loss 0.26542311906814575\n",
      "[Training Epoch 3] Batch 3218, Loss 0.2705901265144348\n",
      "[Training Epoch 3] Batch 3219, Loss 0.2947874069213867\n",
      "[Training Epoch 3] Batch 3220, Loss 0.27812883257865906\n",
      "[Training Epoch 3] Batch 3221, Loss 0.2864711582660675\n",
      "[Training Epoch 3] Batch 3222, Loss 0.27803272008895874\n",
      "[Training Epoch 3] Batch 3223, Loss 0.29375696182250977\n",
      "[Training Epoch 3] Batch 3224, Loss 0.2648976445198059\n",
      "[Training Epoch 3] Batch 3225, Loss 0.2808264493942261\n",
      "[Training Epoch 3] Batch 3226, Loss 0.2469988465309143\n",
      "[Training Epoch 3] Batch 3227, Loss 0.2615993022918701\n",
      "[Training Epoch 3] Batch 3228, Loss 0.27565181255340576\n",
      "[Training Epoch 3] Batch 3229, Loss 0.28357523679733276\n",
      "[Training Epoch 3] Batch 3230, Loss 0.2577473521232605\n",
      "[Training Epoch 3] Batch 3231, Loss 0.2639782130718231\n",
      "[Training Epoch 3] Batch 3232, Loss 0.27653080224990845\n",
      "[Training Epoch 3] Batch 3233, Loss 0.26958680152893066\n",
      "[Training Epoch 3] Batch 3234, Loss 0.29601484537124634\n",
      "[Training Epoch 3] Batch 3235, Loss 0.2711673080921173\n",
      "[Training Epoch 3] Batch 3236, Loss 0.28418469429016113\n",
      "[Training Epoch 3] Batch 3237, Loss 0.26752543449401855\n",
      "[Training Epoch 3] Batch 3238, Loss 0.30070066452026367\n",
      "[Training Epoch 3] Batch 3239, Loss 0.27138906717300415\n",
      "[Training Epoch 3] Batch 3240, Loss 0.26282382011413574\n",
      "[Training Epoch 3] Batch 3241, Loss 0.26433342695236206\n",
      "[Training Epoch 3] Batch 3242, Loss 0.2722746729850769\n",
      "[Training Epoch 3] Batch 3243, Loss 0.2742336392402649\n",
      "[Training Epoch 3] Batch 3244, Loss 0.2794991731643677\n",
      "[Training Epoch 3] Batch 3245, Loss 0.2908550500869751\n",
      "[Training Epoch 3] Batch 3246, Loss 0.25014573335647583\n",
      "[Training Epoch 3] Batch 3247, Loss 0.27272874116897583\n",
      "[Training Epoch 3] Batch 3248, Loss 0.262548565864563\n",
      "[Training Epoch 3] Batch 3249, Loss 0.27450811862945557\n",
      "[Training Epoch 3] Batch 3250, Loss 0.2687607407569885\n",
      "[Training Epoch 3] Batch 3251, Loss 0.2909974455833435\n",
      "[Training Epoch 3] Batch 3252, Loss 0.2720809578895569\n",
      "[Training Epoch 3] Batch 3253, Loss 0.2631780803203583\n",
      "[Training Epoch 3] Batch 3254, Loss 0.2756272554397583\n",
      "[Training Epoch 3] Batch 3255, Loss 0.2846972346305847\n",
      "[Training Epoch 3] Batch 3256, Loss 0.248726025223732\n",
      "[Training Epoch 3] Batch 3257, Loss 0.2724778950214386\n",
      "[Training Epoch 3] Batch 3258, Loss 0.27846038341522217\n",
      "[Training Epoch 3] Batch 3259, Loss 0.267162561416626\n",
      "[Training Epoch 3] Batch 3260, Loss 0.30629780888557434\n",
      "[Training Epoch 3] Batch 3261, Loss 0.2739708423614502\n",
      "[Training Epoch 3] Batch 3262, Loss 0.2678002119064331\n",
      "[Training Epoch 3] Batch 3263, Loss 0.2587786316871643\n",
      "[Training Epoch 3] Batch 3264, Loss 0.26484769582748413\n",
      "[Training Epoch 3] Batch 3265, Loss 0.2332978993654251\n",
      "[Training Epoch 3] Batch 3266, Loss 0.2846892774105072\n",
      "[Training Epoch 3] Batch 3267, Loss 0.2567550837993622\n",
      "[Training Epoch 3] Batch 3268, Loss 0.2531879246234894\n",
      "[Training Epoch 3] Batch 3269, Loss 0.2680683732032776\n",
      "[Training Epoch 3] Batch 3270, Loss 0.2800788879394531\n",
      "[Training Epoch 3] Batch 3271, Loss 0.2521486282348633\n",
      "[Training Epoch 3] Batch 3272, Loss 0.2667372226715088\n",
      "[Training Epoch 3] Batch 3273, Loss 0.2487107217311859\n",
      "[Training Epoch 3] Batch 3274, Loss 0.31393003463745117\n",
      "[Training Epoch 3] Batch 3275, Loss 0.2649286687374115\n",
      "[Training Epoch 3] Batch 3276, Loss 0.278693825006485\n",
      "[Training Epoch 3] Batch 3277, Loss 0.26846903562545776\n",
      "[Training Epoch 3] Batch 3278, Loss 0.2635844349861145\n",
      "[Training Epoch 3] Batch 3279, Loss 0.2849080562591553\n",
      "[Training Epoch 3] Batch 3280, Loss 0.245688796043396\n",
      "[Training Epoch 3] Batch 3281, Loss 0.2892744839191437\n",
      "[Training Epoch 3] Batch 3282, Loss 0.2745029926300049\n",
      "[Training Epoch 3] Batch 3283, Loss 0.25516900420188904\n",
      "[Training Epoch 3] Batch 3284, Loss 0.26404836773872375\n",
      "[Training Epoch 3] Batch 3285, Loss 0.2678404450416565\n",
      "[Training Epoch 3] Batch 3286, Loss 0.2625126242637634\n",
      "[Training Epoch 3] Batch 3287, Loss 0.2604258954524994\n",
      "[Training Epoch 3] Batch 3288, Loss 0.26059669256210327\n",
      "[Training Epoch 3] Batch 3289, Loss 0.2852284908294678\n",
      "[Training Epoch 3] Batch 3290, Loss 0.2776319980621338\n",
      "[Training Epoch 3] Batch 3291, Loss 0.27864229679107666\n",
      "[Training Epoch 3] Batch 3292, Loss 0.282903254032135\n",
      "[Training Epoch 3] Batch 3293, Loss 0.2677968740463257\n",
      "[Training Epoch 3] Batch 3294, Loss 0.2840014398097992\n",
      "[Training Epoch 3] Batch 3295, Loss 0.2683160603046417\n",
      "[Training Epoch 3] Batch 3296, Loss 0.2792966365814209\n",
      "[Training Epoch 3] Batch 3297, Loss 0.2789342999458313\n",
      "[Training Epoch 3] Batch 3298, Loss 0.2482183873653412\n",
      "[Training Epoch 3] Batch 3299, Loss 0.25529909133911133\n",
      "[Training Epoch 3] Batch 3300, Loss 0.27287036180496216\n",
      "[Training Epoch 3] Batch 3301, Loss 0.27940335869789124\n",
      "[Training Epoch 3] Batch 3302, Loss 0.28380903601646423\n",
      "[Training Epoch 3] Batch 3303, Loss 0.25794023275375366\n",
      "[Training Epoch 3] Batch 3304, Loss 0.263017863035202\n",
      "[Training Epoch 3] Batch 3305, Loss 0.2730030119419098\n",
      "[Training Epoch 3] Batch 3306, Loss 0.25128108263015747\n",
      "[Training Epoch 3] Batch 3307, Loss 0.2684847116470337\n",
      "[Training Epoch 3] Batch 3308, Loss 0.2785030007362366\n",
      "[Training Epoch 3] Batch 3309, Loss 0.29849234223365784\n",
      "[Training Epoch 3] Batch 3310, Loss 0.27420246601104736\n",
      "[Training Epoch 3] Batch 3311, Loss 0.2813955545425415\n",
      "[Training Epoch 3] Batch 3312, Loss 0.270900160074234\n",
      "[Training Epoch 3] Batch 3313, Loss 0.2517966628074646\n",
      "[Training Epoch 3] Batch 3314, Loss 0.27700239419937134\n",
      "[Training Epoch 3] Batch 3315, Loss 0.2601713538169861\n",
      "[Training Epoch 3] Batch 3316, Loss 0.2779630422592163\n",
      "[Training Epoch 3] Batch 3317, Loss 0.2939932644367218\n",
      "[Training Epoch 3] Batch 3318, Loss 0.2510765492916107\n",
      "[Training Epoch 3] Batch 3319, Loss 0.2753838300704956\n",
      "[Training Epoch 3] Batch 3320, Loss 0.3042066693305969\n",
      "[Training Epoch 3] Batch 3321, Loss 0.27195388078689575\n",
      "[Training Epoch 3] Batch 3322, Loss 0.26696664094924927\n",
      "[Training Epoch 3] Batch 3323, Loss 0.2820209264755249\n",
      "[Training Epoch 3] Batch 3324, Loss 0.245340958237648\n",
      "[Training Epoch 3] Batch 3325, Loss 0.2769726514816284\n",
      "[Training Epoch 3] Batch 3326, Loss 0.25765085220336914\n",
      "[Training Epoch 3] Batch 3327, Loss 0.2597882151603699\n",
      "[Training Epoch 3] Batch 3328, Loss 0.2770373821258545\n",
      "[Training Epoch 3] Batch 3329, Loss 0.24366077780723572\n",
      "[Training Epoch 3] Batch 3330, Loss 0.2795412242412567\n",
      "[Training Epoch 3] Batch 3331, Loss 0.2800397276878357\n",
      "[Training Epoch 3] Batch 3332, Loss 0.29407456517219543\n",
      "[Training Epoch 3] Batch 3333, Loss 0.29523134231567383\n",
      "[Training Epoch 3] Batch 3334, Loss 0.2923823297023773\n",
      "[Training Epoch 3] Batch 3335, Loss 0.24677906930446625\n",
      "[Training Epoch 3] Batch 3336, Loss 0.25923675298690796\n",
      "[Training Epoch 3] Batch 3337, Loss 0.3025318384170532\n",
      "[Training Epoch 3] Batch 3338, Loss 0.23999600112438202\n",
      "[Training Epoch 3] Batch 3339, Loss 0.2863142788410187\n",
      "[Training Epoch 3] Batch 3340, Loss 0.24011188745498657\n",
      "[Training Epoch 3] Batch 3341, Loss 0.25985947251319885\n",
      "[Training Epoch 3] Batch 3342, Loss 0.2930465042591095\n",
      "[Training Epoch 3] Batch 3343, Loss 0.2583485245704651\n",
      "[Training Epoch 3] Batch 3344, Loss 0.2616223394870758\n",
      "[Training Epoch 3] Batch 3345, Loss 0.267958402633667\n",
      "[Training Epoch 3] Batch 3346, Loss 0.24316509068012238\n",
      "[Training Epoch 3] Batch 3347, Loss 0.2588198184967041\n",
      "[Training Epoch 3] Batch 3348, Loss 0.27587294578552246\n",
      "[Training Epoch 3] Batch 3349, Loss 0.2683197259902954\n",
      "[Training Epoch 3] Batch 3350, Loss 0.2616930603981018\n",
      "[Training Epoch 3] Batch 3351, Loss 0.2742306590080261\n",
      "[Training Epoch 3] Batch 3352, Loss 0.2830888628959656\n",
      "[Training Epoch 3] Batch 3353, Loss 0.2534049153327942\n",
      "[Training Epoch 3] Batch 3354, Loss 0.26736384630203247\n",
      "[Training Epoch 3] Batch 3355, Loss 0.30038589239120483\n",
      "[Training Epoch 3] Batch 3356, Loss 0.2924150228500366\n",
      "[Training Epoch 3] Batch 3357, Loss 0.25669237971305847\n",
      "[Training Epoch 3] Batch 3358, Loss 0.29208138585090637\n",
      "[Training Epoch 3] Batch 3359, Loss 0.2512645423412323\n",
      "[Training Epoch 3] Batch 3360, Loss 0.2967863082885742\n",
      "[Training Epoch 3] Batch 3361, Loss 0.2794841527938843\n",
      "[Training Epoch 3] Batch 3362, Loss 0.3075442612171173\n",
      "[Training Epoch 3] Batch 3363, Loss 0.2553327977657318\n",
      "[Training Epoch 3] Batch 3364, Loss 0.255010187625885\n",
      "[Training Epoch 3] Batch 3365, Loss 0.2770550847053528\n",
      "[Training Epoch 3] Batch 3366, Loss 0.29451215267181396\n",
      "[Training Epoch 3] Batch 3367, Loss 0.2703990340232849\n",
      "[Training Epoch 3] Batch 3368, Loss 0.25058695673942566\n",
      "[Training Epoch 3] Batch 3369, Loss 0.25573232769966125\n",
      "[Training Epoch 3] Batch 3370, Loss 0.2581275701522827\n",
      "[Training Epoch 3] Batch 3371, Loss 0.25221848487854004\n",
      "[Training Epoch 3] Batch 3372, Loss 0.24914440512657166\n",
      "[Training Epoch 3] Batch 3373, Loss 0.26090139150619507\n",
      "[Training Epoch 3] Batch 3374, Loss 0.26783275604248047\n",
      "[Training Epoch 3] Batch 3375, Loss 0.2758338451385498\n",
      "[Training Epoch 3] Batch 3376, Loss 0.2507760226726532\n",
      "[Training Epoch 3] Batch 3377, Loss 0.26813173294067383\n",
      "[Training Epoch 3] Batch 3378, Loss 0.24527090787887573\n",
      "[Training Epoch 3] Batch 3379, Loss 0.2739669680595398\n",
      "[Training Epoch 3] Batch 3380, Loss 0.26748770475387573\n",
      "[Training Epoch 3] Batch 3381, Loss 0.276642382144928\n",
      "[Training Epoch 3] Batch 3382, Loss 0.2858783006668091\n",
      "[Training Epoch 3] Batch 3383, Loss 0.2765512764453888\n",
      "[Training Epoch 3] Batch 3384, Loss 0.2839164137840271\n",
      "[Training Epoch 3] Batch 3385, Loss 0.29437336325645447\n",
      "[Training Epoch 3] Batch 3386, Loss 0.2623622417449951\n",
      "[Training Epoch 3] Batch 3387, Loss 0.2637869715690613\n",
      "[Training Epoch 3] Batch 3388, Loss 0.28069549798965454\n",
      "[Training Epoch 3] Batch 3389, Loss 0.27323371171951294\n",
      "[Training Epoch 3] Batch 3390, Loss 0.25834980607032776\n",
      "[Training Epoch 3] Batch 3391, Loss 0.27208346128463745\n",
      "[Training Epoch 3] Batch 3392, Loss 0.28419408202171326\n",
      "[Training Epoch 3] Batch 3393, Loss 0.25512903928756714\n",
      "[Training Epoch 3] Batch 3394, Loss 0.28136810660362244\n",
      "[Training Epoch 3] Batch 3395, Loss 0.27279525995254517\n",
      "[Training Epoch 3] Batch 3396, Loss 0.26448920369148254\n",
      "[Training Epoch 3] Batch 3397, Loss 0.2713788151741028\n",
      "[Training Epoch 3] Batch 3398, Loss 0.27256298065185547\n",
      "[Training Epoch 3] Batch 3399, Loss 0.27830275893211365\n",
      "[Training Epoch 3] Batch 3400, Loss 0.2546750605106354\n",
      "[Training Epoch 3] Batch 3401, Loss 0.25002700090408325\n",
      "[Training Epoch 3] Batch 3402, Loss 0.27830970287323\n",
      "[Training Epoch 3] Batch 3403, Loss 0.255990207195282\n",
      "[Training Epoch 3] Batch 3404, Loss 0.27688199281692505\n",
      "[Training Epoch 3] Batch 3405, Loss 0.2854984700679779\n",
      "[Training Epoch 3] Batch 3406, Loss 0.29033851623535156\n",
      "[Training Epoch 3] Batch 3407, Loss 0.2732982039451599\n",
      "[Training Epoch 3] Batch 3408, Loss 0.2731490731239319\n",
      "[Training Epoch 3] Batch 3409, Loss 0.26888415217399597\n",
      "[Training Epoch 3] Batch 3410, Loss 0.26631173491477966\n",
      "[Training Epoch 3] Batch 3411, Loss 0.23821662366390228\n",
      "[Training Epoch 3] Batch 3412, Loss 0.27020955085754395\n",
      "[Training Epoch 3] Batch 3413, Loss 0.2762112021446228\n",
      "[Training Epoch 3] Batch 3414, Loss 0.25202229619026184\n",
      "[Training Epoch 3] Batch 3415, Loss 0.2690100073814392\n",
      "[Training Epoch 3] Batch 3416, Loss 0.2753763496875763\n",
      "[Training Epoch 3] Batch 3417, Loss 0.2583448588848114\n",
      "[Training Epoch 3] Batch 3418, Loss 0.27063167095184326\n",
      "[Training Epoch 3] Batch 3419, Loss 0.2706303596496582\n",
      "[Training Epoch 3] Batch 3420, Loss 0.2847450375556946\n",
      "[Training Epoch 3] Batch 3421, Loss 0.27284103631973267\n",
      "[Training Epoch 3] Batch 3422, Loss 0.27156662940979004\n",
      "[Training Epoch 3] Batch 3423, Loss 0.29127106070518494\n",
      "[Training Epoch 3] Batch 3424, Loss 0.2423420250415802\n",
      "[Training Epoch 3] Batch 3425, Loss 0.2784980535507202\n",
      "[Training Epoch 3] Batch 3426, Loss 0.29372018575668335\n",
      "[Training Epoch 3] Batch 3427, Loss 0.25681304931640625\n",
      "[Training Epoch 3] Batch 3428, Loss 0.2800723910331726\n",
      "[Training Epoch 3] Batch 3429, Loss 0.25431591272354126\n",
      "[Training Epoch 3] Batch 3430, Loss 0.3096606135368347\n",
      "[Training Epoch 3] Batch 3431, Loss 0.30246132612228394\n",
      "[Training Epoch 3] Batch 3432, Loss 0.28138473629951477\n",
      "[Training Epoch 3] Batch 3433, Loss 0.26990175247192383\n",
      "[Training Epoch 3] Batch 3434, Loss 0.27220937609672546\n",
      "[Training Epoch 3] Batch 3435, Loss 0.27379879355430603\n",
      "[Training Epoch 3] Batch 3436, Loss 0.2535320222377777\n",
      "[Training Epoch 3] Batch 3437, Loss 0.2771126329898834\n",
      "[Training Epoch 3] Batch 3438, Loss 0.2360897660255432\n",
      "[Training Epoch 3] Batch 3439, Loss 0.24217665195465088\n",
      "[Training Epoch 3] Batch 3440, Loss 0.27990978956222534\n",
      "[Training Epoch 3] Batch 3441, Loss 0.30446380376815796\n",
      "[Training Epoch 3] Batch 3442, Loss 0.23946654796600342\n",
      "[Training Epoch 3] Batch 3443, Loss 0.283300518989563\n",
      "[Training Epoch 3] Batch 3444, Loss 0.2781066298484802\n",
      "[Training Epoch 3] Batch 3445, Loss 0.2468383014202118\n",
      "[Training Epoch 3] Batch 3446, Loss 0.28182098269462585\n",
      "[Training Epoch 3] Batch 3447, Loss 0.27402323484420776\n",
      "[Training Epoch 3] Batch 3448, Loss 0.23711517453193665\n",
      "[Training Epoch 3] Batch 3449, Loss 0.24129343032836914\n",
      "[Training Epoch 3] Batch 3450, Loss 0.2752183675765991\n",
      "[Training Epoch 3] Batch 3451, Loss 0.2557876706123352\n",
      "[Training Epoch 3] Batch 3452, Loss 0.26432716846466064\n",
      "[Training Epoch 3] Batch 3453, Loss 0.25575849413871765\n",
      "[Training Epoch 3] Batch 3454, Loss 0.254911869764328\n",
      "[Training Epoch 3] Batch 3455, Loss 0.26497718691825867\n",
      "[Training Epoch 3] Batch 3456, Loss 0.31126177310943604\n",
      "[Training Epoch 3] Batch 3457, Loss 0.23092332482337952\n",
      "[Training Epoch 3] Batch 3458, Loss 0.27368563413619995\n",
      "[Training Epoch 3] Batch 3459, Loss 0.2922905683517456\n",
      "[Training Epoch 3] Batch 3460, Loss 0.26129084825515747\n",
      "[Training Epoch 3] Batch 3461, Loss 0.2685982584953308\n",
      "[Training Epoch 3] Batch 3462, Loss 0.2899370491504669\n",
      "[Training Epoch 3] Batch 3463, Loss 0.25177690386772156\n",
      "[Training Epoch 3] Batch 3464, Loss 0.2320431023836136\n",
      "[Training Epoch 3] Batch 3465, Loss 0.2553836405277252\n",
      "[Training Epoch 3] Batch 3466, Loss 0.2821079194545746\n",
      "[Training Epoch 3] Batch 3467, Loss 0.25354015827178955\n",
      "[Training Epoch 3] Batch 3468, Loss 0.27644193172454834\n",
      "[Training Epoch 3] Batch 3469, Loss 0.2692798376083374\n",
      "[Training Epoch 3] Batch 3470, Loss 0.27449357509613037\n",
      "[Training Epoch 3] Batch 3471, Loss 0.2590816915035248\n",
      "[Training Epoch 3] Batch 3472, Loss 0.278921514749527\n",
      "[Training Epoch 3] Batch 3473, Loss 0.2735620439052582\n",
      "[Training Epoch 3] Batch 3474, Loss 0.28556692600250244\n",
      "[Training Epoch 3] Batch 3475, Loss 0.2430148422718048\n",
      "[Training Epoch 3] Batch 3476, Loss 0.24856367707252502\n",
      "[Training Epoch 3] Batch 3477, Loss 0.2556774914264679\n",
      "[Training Epoch 3] Batch 3478, Loss 0.2736068367958069\n",
      "[Training Epoch 3] Batch 3479, Loss 0.25735488533973694\n",
      "[Training Epoch 3] Batch 3480, Loss 0.270721971988678\n",
      "[Training Epoch 3] Batch 3481, Loss 0.25760748982429504\n",
      "[Training Epoch 3] Batch 3482, Loss 0.27000516653060913\n",
      "[Training Epoch 3] Batch 3483, Loss 0.2600165605545044\n",
      "[Training Epoch 3] Batch 3484, Loss 0.21692591905593872\n",
      "[Training Epoch 3] Batch 3485, Loss 0.28393417596817017\n",
      "[Training Epoch 3] Batch 3486, Loss 0.26895758509635925\n",
      "[Training Epoch 3] Batch 3487, Loss 0.2592109143733978\n",
      "[Training Epoch 3] Batch 3488, Loss 0.24637699127197266\n",
      "[Training Epoch 3] Batch 3489, Loss 0.2692772150039673\n",
      "[Training Epoch 3] Batch 3490, Loss 0.2726050615310669\n",
      "[Training Epoch 3] Batch 3491, Loss 0.27710676193237305\n",
      "[Training Epoch 3] Batch 3492, Loss 0.25171536207199097\n",
      "[Training Epoch 3] Batch 3493, Loss 0.2805951237678528\n",
      "[Training Epoch 3] Batch 3494, Loss 0.28781723976135254\n",
      "[Training Epoch 3] Batch 3495, Loss 0.271121084690094\n",
      "[Training Epoch 3] Batch 3496, Loss 0.26898419857025146\n",
      "[Training Epoch 3] Batch 3497, Loss 0.2595859169960022\n",
      "[Training Epoch 3] Batch 3498, Loss 0.25263553857803345\n",
      "[Training Epoch 3] Batch 3499, Loss 0.283829927444458\n",
      "[Training Epoch 3] Batch 3500, Loss 0.2878267765045166\n",
      "[Training Epoch 3] Batch 3501, Loss 0.28633350133895874\n",
      "[Training Epoch 3] Batch 3502, Loss 0.2911817133426666\n",
      "[Training Epoch 3] Batch 3503, Loss 0.26083454489707947\n",
      "[Training Epoch 3] Batch 3504, Loss 0.2837672233581543\n",
      "[Training Epoch 3] Batch 3505, Loss 0.29023176431655884\n",
      "[Training Epoch 3] Batch 3506, Loss 0.24122385680675507\n",
      "[Training Epoch 3] Batch 3507, Loss 0.2608114182949066\n",
      "[Training Epoch 3] Batch 3508, Loss 0.2891392707824707\n",
      "[Training Epoch 3] Batch 3509, Loss 0.2703057527542114\n",
      "[Training Epoch 3] Batch 3510, Loss 0.2657364308834076\n",
      "[Training Epoch 3] Batch 3511, Loss 0.261603444814682\n",
      "[Training Epoch 3] Batch 3512, Loss 0.2935172915458679\n",
      "[Training Epoch 3] Batch 3513, Loss 0.2772560715675354\n",
      "[Training Epoch 3] Batch 3514, Loss 0.24876846373081207\n",
      "[Training Epoch 3] Batch 3515, Loss 0.28667572140693665\n",
      "[Training Epoch 3] Batch 3516, Loss 0.24962139129638672\n",
      "[Training Epoch 3] Batch 3517, Loss 0.2605127692222595\n",
      "[Training Epoch 3] Batch 3518, Loss 0.2601858973503113\n",
      "[Training Epoch 3] Batch 3519, Loss 0.27654099464416504\n",
      "[Training Epoch 3] Batch 3520, Loss 0.2566893696784973\n",
      "[Training Epoch 3] Batch 3521, Loss 0.2582723796367645\n",
      "[Training Epoch 3] Batch 3522, Loss 0.2649492919445038\n",
      "[Training Epoch 3] Batch 3523, Loss 0.2540942132472992\n",
      "[Training Epoch 3] Batch 3524, Loss 0.26987749338150024\n",
      "[Training Epoch 3] Batch 3525, Loss 0.26656287908554077\n",
      "[Training Epoch 3] Batch 3526, Loss 0.3065844178199768\n",
      "[Training Epoch 3] Batch 3527, Loss 0.24422788619995117\n",
      "[Training Epoch 3] Batch 3528, Loss 0.26587140560150146\n",
      "[Training Epoch 3] Batch 3529, Loss 0.2503114938735962\n",
      "[Training Epoch 3] Batch 3530, Loss 0.26904919743537903\n",
      "[Training Epoch 3] Batch 3531, Loss 0.27493318915367126\n",
      "[Training Epoch 3] Batch 3532, Loss 0.2719045877456665\n",
      "[Training Epoch 3] Batch 3533, Loss 0.24935312569141388\n",
      "[Training Epoch 3] Batch 3534, Loss 0.24123980104923248\n",
      "[Training Epoch 3] Batch 3535, Loss 0.2686007618904114\n",
      "[Training Epoch 3] Batch 3536, Loss 0.257015585899353\n",
      "[Training Epoch 3] Batch 3537, Loss 0.2789827585220337\n",
      "[Training Epoch 3] Batch 3538, Loss 0.27735626697540283\n",
      "[Training Epoch 3] Batch 3539, Loss 0.28745973110198975\n",
      "[Training Epoch 3] Batch 3540, Loss 0.2906249761581421\n",
      "[Training Epoch 3] Batch 3541, Loss 0.2776120901107788\n",
      "[Training Epoch 3] Batch 3542, Loss 0.24867787957191467\n",
      "[Training Epoch 3] Batch 3543, Loss 0.2682173252105713\n",
      "[Training Epoch 3] Batch 3544, Loss 0.26131999492645264\n",
      "[Training Epoch 3] Batch 3545, Loss 0.27686822414398193\n",
      "[Training Epoch 3] Batch 3546, Loss 0.2775598168373108\n",
      "[Training Epoch 3] Batch 3547, Loss 0.2739716172218323\n",
      "[Training Epoch 3] Batch 3548, Loss 0.2592008113861084\n",
      "[Training Epoch 3] Batch 3549, Loss 0.2721012830734253\n",
      "[Training Epoch 3] Batch 3550, Loss 0.3111056089401245\n",
      "[Training Epoch 3] Batch 3551, Loss 0.25412192940711975\n",
      "[Training Epoch 3] Batch 3552, Loss 0.2591015696525574\n",
      "[Training Epoch 3] Batch 3553, Loss 0.2809823453426361\n",
      "[Training Epoch 3] Batch 3554, Loss 0.24335210025310516\n",
      "[Training Epoch 3] Batch 3555, Loss 0.2528270184993744\n",
      "[Training Epoch 3] Batch 3556, Loss 0.27308040857315063\n",
      "[Training Epoch 3] Batch 3557, Loss 0.2539706528186798\n",
      "[Training Epoch 3] Batch 3558, Loss 0.26113900542259216\n",
      "[Training Epoch 3] Batch 3559, Loss 0.2685927152633667\n",
      "[Training Epoch 3] Batch 3560, Loss 0.27335283160209656\n",
      "[Training Epoch 3] Batch 3561, Loss 0.27558794617652893\n",
      "[Training Epoch 3] Batch 3562, Loss 0.2633063495159149\n",
      "[Training Epoch 3] Batch 3563, Loss 0.25166961550712585\n",
      "[Training Epoch 3] Batch 3564, Loss 0.27689293026924133\n",
      "[Training Epoch 3] Batch 3565, Loss 0.27498459815979004\n",
      "[Training Epoch 3] Batch 3566, Loss 0.2635385990142822\n",
      "[Training Epoch 3] Batch 3567, Loss 0.2952316999435425\n",
      "[Training Epoch 3] Batch 3568, Loss 0.2814837694168091\n",
      "[Training Epoch 3] Batch 3569, Loss 0.25045719742774963\n",
      "[Training Epoch 3] Batch 3570, Loss 0.24122953414916992\n",
      "[Training Epoch 3] Batch 3571, Loss 0.2773872911930084\n",
      "[Training Epoch 3] Batch 3572, Loss 0.27133941650390625\n",
      "[Training Epoch 3] Batch 3573, Loss 0.2664808928966522\n",
      "[Training Epoch 3] Batch 3574, Loss 0.28215181827545166\n",
      "[Training Epoch 3] Batch 3575, Loss 0.23925547301769257\n",
      "[Training Epoch 3] Batch 3576, Loss 0.2557260990142822\n",
      "[Training Epoch 3] Batch 3577, Loss 0.2931485176086426\n",
      "[Training Epoch 3] Batch 3578, Loss 0.26630741357803345\n",
      "[Training Epoch 3] Batch 3579, Loss 0.2967187166213989\n",
      "[Training Epoch 3] Batch 3580, Loss 0.2539428472518921\n",
      "[Training Epoch 3] Batch 3581, Loss 0.2883378863334656\n",
      "[Training Epoch 3] Batch 3582, Loss 0.3105285167694092\n",
      "[Training Epoch 3] Batch 3583, Loss 0.27027377486228943\n",
      "[Training Epoch 3] Batch 3584, Loss 0.26453039050102234\n",
      "[Training Epoch 3] Batch 3585, Loss 0.2686743140220642\n",
      "[Training Epoch 3] Batch 3586, Loss 0.2813565135002136\n",
      "[Training Epoch 3] Batch 3587, Loss 0.24832868576049805\n",
      "[Training Epoch 3] Batch 3588, Loss 0.2984295189380646\n",
      "[Training Epoch 3] Batch 3589, Loss 0.2722494602203369\n",
      "[Training Epoch 3] Batch 3590, Loss 0.25421544909477234\n",
      "[Training Epoch 3] Batch 3591, Loss 0.295637845993042\n",
      "[Training Epoch 3] Batch 3592, Loss 0.2802237868309021\n",
      "[Training Epoch 3] Batch 3593, Loss 0.2737627327442169\n",
      "[Training Epoch 3] Batch 3594, Loss 0.28769221901893616\n",
      "[Training Epoch 3] Batch 3595, Loss 0.28776875138282776\n",
      "[Training Epoch 3] Batch 3596, Loss 0.28585225343704224\n",
      "[Training Epoch 3] Batch 3597, Loss 0.2942456305027008\n",
      "[Training Epoch 3] Batch 3598, Loss 0.24485445022583008\n",
      "[Training Epoch 3] Batch 3599, Loss 0.27227139472961426\n",
      "[Training Epoch 3] Batch 3600, Loss 0.26535582542419434\n",
      "[Training Epoch 3] Batch 3601, Loss 0.3155544102191925\n",
      "[Training Epoch 3] Batch 3602, Loss 0.27180641889572144\n",
      "[Training Epoch 3] Batch 3603, Loss 0.2507473826408386\n",
      "[Training Epoch 3] Batch 3604, Loss 0.23997727036476135\n",
      "[Training Epoch 3] Batch 3605, Loss 0.26965129375457764\n",
      "[Training Epoch 3] Batch 3606, Loss 0.263943076133728\n",
      "[Training Epoch 3] Batch 3607, Loss 0.25667479634284973\n",
      "[Training Epoch 3] Batch 3608, Loss 0.2961312234401703\n",
      "[Training Epoch 3] Batch 3609, Loss 0.2661590278148651\n",
      "[Training Epoch 3] Batch 3610, Loss 0.25719594955444336\n",
      "[Training Epoch 3] Batch 3611, Loss 0.27905064821243286\n",
      "[Training Epoch 3] Batch 3612, Loss 0.2860283851623535\n",
      "[Training Epoch 3] Batch 3613, Loss 0.27930039167404175\n",
      "[Training Epoch 3] Batch 3614, Loss 0.24940764904022217\n",
      "[Training Epoch 3] Batch 3615, Loss 0.258177787065506\n",
      "[Training Epoch 3] Batch 3616, Loss 0.25194603204727173\n",
      "[Training Epoch 3] Batch 3617, Loss 0.2811805009841919\n",
      "[Training Epoch 3] Batch 3618, Loss 0.28529179096221924\n",
      "[Training Epoch 3] Batch 3619, Loss 0.2744329571723938\n",
      "[Training Epoch 3] Batch 3620, Loss 0.2739381194114685\n",
      "[Training Epoch 3] Batch 3621, Loss 0.28926506638526917\n",
      "[Training Epoch 3] Batch 3622, Loss 0.29445528984069824\n",
      "[Training Epoch 3] Batch 3623, Loss 0.2696581482887268\n",
      "[Training Epoch 3] Batch 3624, Loss 0.2578050196170807\n",
      "[Training Epoch 3] Batch 3625, Loss 0.2915189266204834\n",
      "[Training Epoch 3] Batch 3626, Loss 0.26739901304244995\n",
      "[Training Epoch 3] Batch 3627, Loss 0.26224514842033386\n",
      "[Training Epoch 3] Batch 3628, Loss 0.2696206569671631\n",
      "[Training Epoch 3] Batch 3629, Loss 0.2561822533607483\n",
      "[Training Epoch 3] Batch 3630, Loss 0.2704710364341736\n",
      "[Training Epoch 3] Batch 3631, Loss 0.2573803663253784\n",
      "[Training Epoch 3] Batch 3632, Loss 0.2773863673210144\n",
      "[Training Epoch 3] Batch 3633, Loss 0.2596997916698456\n",
      "[Training Epoch 3] Batch 3634, Loss 0.2414451539516449\n",
      "[Training Epoch 3] Batch 3635, Loss 0.25739091634750366\n",
      "[Training Epoch 3] Batch 3636, Loss 0.29122960567474365\n",
      "[Training Epoch 3] Batch 3637, Loss 0.2578970193862915\n",
      "[Training Epoch 3] Batch 3638, Loss 0.2705872356891632\n",
      "[Training Epoch 3] Batch 3639, Loss 0.2372719943523407\n",
      "[Training Epoch 3] Batch 3640, Loss 0.2924194037914276\n",
      "[Training Epoch 3] Batch 3641, Loss 0.29188525676727295\n",
      "[Training Epoch 3] Batch 3642, Loss 0.26921346783638\n",
      "[Training Epoch 3] Batch 3643, Loss 0.26416322588920593\n",
      "[Training Epoch 3] Batch 3644, Loss 0.2286236584186554\n",
      "[Training Epoch 3] Batch 3645, Loss 0.25040704011917114\n",
      "[Training Epoch 3] Batch 3646, Loss 0.2740671634674072\n",
      "[Training Epoch 3] Batch 3647, Loss 0.2625354826450348\n",
      "[Training Epoch 3] Batch 3648, Loss 0.2698095440864563\n",
      "[Training Epoch 3] Batch 3649, Loss 0.22602325677871704\n",
      "[Training Epoch 3] Batch 3650, Loss 0.26899009943008423\n",
      "[Training Epoch 3] Batch 3651, Loss 0.2723849415779114\n",
      "[Training Epoch 3] Batch 3652, Loss 0.2615166902542114\n",
      "[Training Epoch 3] Batch 3653, Loss 0.2665128707885742\n",
      "[Training Epoch 3] Batch 3654, Loss 0.23413074016571045\n",
      "[Training Epoch 3] Batch 3655, Loss 0.2503805160522461\n",
      "[Training Epoch 3] Batch 3656, Loss 0.26225435733795166\n",
      "[Training Epoch 3] Batch 3657, Loss 0.3078743517398834\n",
      "[Training Epoch 3] Batch 3658, Loss 0.27336326241493225\n",
      "[Training Epoch 3] Batch 3659, Loss 0.25345826148986816\n",
      "[Training Epoch 3] Batch 3660, Loss 0.2884175181388855\n",
      "[Training Epoch 3] Batch 3661, Loss 0.2793399691581726\n",
      "[Training Epoch 3] Batch 3662, Loss 0.2668224573135376\n",
      "[Training Epoch 3] Batch 3663, Loss 0.24728697538375854\n",
      "[Training Epoch 3] Batch 3664, Loss 0.28097814321517944\n",
      "[Training Epoch 3] Batch 3665, Loss 0.24717047810554504\n",
      "[Training Epoch 3] Batch 3666, Loss 0.24691666662693024\n",
      "[Training Epoch 3] Batch 3667, Loss 0.2673334777355194\n",
      "[Training Epoch 3] Batch 3668, Loss 0.2558477520942688\n",
      "[Training Epoch 3] Batch 3669, Loss 0.2784213423728943\n",
      "[Training Epoch 3] Batch 3670, Loss 0.2605226933956146\n",
      "[Training Epoch 3] Batch 3671, Loss 0.2661473751068115\n",
      "[Training Epoch 3] Batch 3672, Loss 0.2919621765613556\n",
      "[Training Epoch 3] Batch 3673, Loss 0.2818027138710022\n",
      "[Training Epoch 3] Batch 3674, Loss 0.2613680064678192\n",
      "[Training Epoch 3] Batch 3675, Loss 0.26716428995132446\n",
      "[Training Epoch 3] Batch 3676, Loss 0.2686307430267334\n",
      "[Training Epoch 3] Batch 3677, Loss 0.29109564423561096\n",
      "[Training Epoch 3] Batch 3678, Loss 0.28480973839759827\n",
      "[Training Epoch 3] Batch 3679, Loss 0.2657715678215027\n",
      "[Training Epoch 3] Batch 3680, Loss 0.2977193593978882\n",
      "[Training Epoch 3] Batch 3681, Loss 0.2757081091403961\n",
      "[Training Epoch 3] Batch 3682, Loss 0.24382925033569336\n",
      "[Training Epoch 3] Batch 3683, Loss 0.29042086005210876\n",
      "[Training Epoch 3] Batch 3684, Loss 0.25388282537460327\n",
      "[Training Epoch 3] Batch 3685, Loss 0.2765742540359497\n",
      "[Training Epoch 3] Batch 3686, Loss 0.28420311212539673\n",
      "[Training Epoch 3] Batch 3687, Loss 0.26362431049346924\n",
      "[Training Epoch 3] Batch 3688, Loss 0.2842421531677246\n",
      "[Training Epoch 3] Batch 3689, Loss 0.27744099497795105\n",
      "[Training Epoch 3] Batch 3690, Loss 0.2944997549057007\n",
      "[Training Epoch 3] Batch 3691, Loss 0.30025726556777954\n",
      "[Training Epoch 3] Batch 3692, Loss 0.2683470845222473\n",
      "[Training Epoch 3] Batch 3693, Loss 0.26974353194236755\n",
      "[Training Epoch 3] Batch 3694, Loss 0.2794959545135498\n",
      "[Training Epoch 3] Batch 3695, Loss 0.2503467798233032\n",
      "[Training Epoch 3] Batch 3696, Loss 0.2658534646034241\n",
      "[Training Epoch 3] Batch 3697, Loss 0.30870115756988525\n",
      "[Training Epoch 3] Batch 3698, Loss 0.2767419219017029\n",
      "[Training Epoch 3] Batch 3699, Loss 0.2552986741065979\n",
      "[Training Epoch 3] Batch 3700, Loss 0.26767677068710327\n",
      "[Training Epoch 3] Batch 3701, Loss 0.26083630323410034\n",
      "[Training Epoch 3] Batch 3702, Loss 0.2817103862762451\n",
      "[Training Epoch 3] Batch 3703, Loss 0.2795383036136627\n",
      "[Training Epoch 3] Batch 3704, Loss 0.2502076327800751\n",
      "[Training Epoch 3] Batch 3705, Loss 0.26047366857528687\n",
      "[Training Epoch 3] Batch 3706, Loss 0.27023938298225403\n",
      "[Training Epoch 3] Batch 3707, Loss 0.2657042443752289\n",
      "[Training Epoch 3] Batch 3708, Loss 0.26867741346359253\n",
      "[Training Epoch 3] Batch 3709, Loss 0.278799831867218\n",
      "[Training Epoch 3] Batch 3710, Loss 0.2683248221874237\n",
      "[Training Epoch 3] Batch 3711, Loss 0.26224496960639954\n",
      "[Training Epoch 3] Batch 3712, Loss 0.31770098209381104\n",
      "[Training Epoch 3] Batch 3713, Loss 0.2678030729293823\n",
      "[Training Epoch 3] Batch 3714, Loss 0.24737222492694855\n",
      "[Training Epoch 3] Batch 3715, Loss 0.28896766901016235\n",
      "[Training Epoch 3] Batch 3716, Loss 0.2947813868522644\n",
      "[Training Epoch 3] Batch 3717, Loss 0.27889227867126465\n",
      "[Training Epoch 3] Batch 3718, Loss 0.2762044668197632\n",
      "[Training Epoch 3] Batch 3719, Loss 0.24350422620773315\n",
      "[Training Epoch 3] Batch 3720, Loss 0.2927868366241455\n",
      "[Training Epoch 3] Batch 3721, Loss 0.25952938199043274\n",
      "[Training Epoch 3] Batch 3722, Loss 0.2563839554786682\n",
      "[Training Epoch 3] Batch 3723, Loss 0.250544011592865\n",
      "[Training Epoch 3] Batch 3724, Loss 0.24990969896316528\n",
      "[Training Epoch 3] Batch 3725, Loss 0.29510897397994995\n",
      "[Training Epoch 3] Batch 3726, Loss 0.24862241744995117\n",
      "[Training Epoch 3] Batch 3727, Loss 0.29664847254753113\n",
      "[Training Epoch 3] Batch 3728, Loss 0.2627342939376831\n",
      "[Training Epoch 3] Batch 3729, Loss 0.2758151888847351\n",
      "[Training Epoch 3] Batch 3730, Loss 0.29653364419937134\n",
      "[Training Epoch 3] Batch 3731, Loss 0.280538946390152\n",
      "[Training Epoch 3] Batch 3732, Loss 0.22546552121639252\n",
      "[Training Epoch 3] Batch 3733, Loss 0.2698512673377991\n",
      "[Training Epoch 3] Batch 3734, Loss 0.2573429346084595\n",
      "[Training Epoch 3] Batch 3735, Loss 0.26443642377853394\n",
      "[Training Epoch 3] Batch 3736, Loss 0.23349027335643768\n",
      "[Training Epoch 3] Batch 3737, Loss 0.26824283599853516\n",
      "[Training Epoch 3] Batch 3738, Loss 0.2522052526473999\n",
      "[Training Epoch 3] Batch 3739, Loss 0.2719980776309967\n",
      "[Training Epoch 3] Batch 3740, Loss 0.2571098208427429\n",
      "[Training Epoch 3] Batch 3741, Loss 0.2784130573272705\n",
      "[Training Epoch 3] Batch 3742, Loss 0.293171226978302\n",
      "[Training Epoch 3] Batch 3743, Loss 0.28840935230255127\n",
      "[Training Epoch 3] Batch 3744, Loss 0.2529115676879883\n",
      "[Training Epoch 3] Batch 3745, Loss 0.2976474165916443\n",
      "[Training Epoch 3] Batch 3746, Loss 0.24684497714042664\n",
      "[Training Epoch 3] Batch 3747, Loss 0.25729507207870483\n",
      "[Training Epoch 3] Batch 3748, Loss 0.26753130555152893\n",
      "[Training Epoch 3] Batch 3749, Loss 0.2713286280632019\n",
      "[Training Epoch 3] Batch 3750, Loss 0.26891472935676575\n",
      "[Training Epoch 3] Batch 3751, Loss 0.27300912141799927\n",
      "[Training Epoch 3] Batch 3752, Loss 0.26177138090133667\n",
      "[Training Epoch 3] Batch 3753, Loss 0.2762616276741028\n",
      "[Training Epoch 3] Batch 3754, Loss 0.25761422514915466\n",
      "[Training Epoch 3] Batch 3755, Loss 0.2827327251434326\n",
      "[Training Epoch 3] Batch 3756, Loss 0.2883341610431671\n",
      "[Training Epoch 3] Batch 3757, Loss 0.2596275210380554\n",
      "[Training Epoch 3] Batch 3758, Loss 0.2736373543739319\n",
      "[Training Epoch 3] Batch 3759, Loss 0.2588651776313782\n",
      "[Training Epoch 3] Batch 3760, Loss 0.2418784648180008\n",
      "[Training Epoch 3] Batch 3761, Loss 0.24479176104068756\n",
      "[Training Epoch 3] Batch 3762, Loss 0.24052554368972778\n",
      "[Training Epoch 3] Batch 3763, Loss 0.2742310166358948\n",
      "[Training Epoch 3] Batch 3764, Loss 0.286743700504303\n",
      "[Training Epoch 3] Batch 3765, Loss 0.25729382038116455\n",
      "[Training Epoch 3] Batch 3766, Loss 0.2679328918457031\n",
      "[Training Epoch 3] Batch 3767, Loss 0.27192479372024536\n",
      "[Training Epoch 3] Batch 3768, Loss 0.2839667797088623\n",
      "[Training Epoch 3] Batch 3769, Loss 0.2610798478126526\n",
      "[Training Epoch 3] Batch 3770, Loss 0.2855227589607239\n",
      "[Training Epoch 3] Batch 3771, Loss 0.2912256717681885\n",
      "[Training Epoch 3] Batch 3772, Loss 0.30911388993263245\n",
      "[Training Epoch 3] Batch 3773, Loss 0.25866690278053284\n",
      "[Training Epoch 3] Batch 3774, Loss 0.2737261652946472\n",
      "[Training Epoch 3] Batch 3775, Loss 0.2596495747566223\n",
      "[Training Epoch 3] Batch 3776, Loss 0.2612770199775696\n",
      "[Training Epoch 3] Batch 3777, Loss 0.2828716039657593\n",
      "[Training Epoch 3] Batch 3778, Loss 0.31834399700164795\n",
      "[Training Epoch 3] Batch 3779, Loss 0.23982150852680206\n",
      "[Training Epoch 3] Batch 3780, Loss 0.264873206615448\n",
      "[Training Epoch 3] Batch 3781, Loss 0.2911824882030487\n",
      "[Training Epoch 3] Batch 3782, Loss 0.269273966550827\n",
      "[Training Epoch 3] Batch 3783, Loss 0.27822548151016235\n",
      "[Training Epoch 3] Batch 3784, Loss 0.26002824306488037\n",
      "[Training Epoch 3] Batch 3785, Loss 0.2795623540878296\n",
      "[Training Epoch 3] Batch 3786, Loss 0.27802062034606934\n",
      "[Training Epoch 3] Batch 3787, Loss 0.24868199229240417\n",
      "[Training Epoch 3] Batch 3788, Loss 0.25494930148124695\n",
      "[Training Epoch 3] Batch 3789, Loss 0.2973020076751709\n",
      "[Training Epoch 3] Batch 3790, Loss 0.306350976228714\n",
      "[Training Epoch 3] Batch 3791, Loss 0.2615826427936554\n",
      "[Training Epoch 3] Batch 3792, Loss 0.2619137763977051\n",
      "[Training Epoch 3] Batch 3793, Loss 0.26365673542022705\n",
      "[Training Epoch 3] Batch 3794, Loss 0.28634777665138245\n",
      "[Training Epoch 3] Batch 3795, Loss 0.26512664556503296\n",
      "[Training Epoch 3] Batch 3796, Loss 0.26219820976257324\n",
      "[Training Epoch 3] Batch 3797, Loss 0.26849716901779175\n",
      "[Training Epoch 3] Batch 3798, Loss 0.2460949867963791\n",
      "[Training Epoch 3] Batch 3799, Loss 0.3025587201118469\n",
      "[Training Epoch 3] Batch 3800, Loss 0.28483113646507263\n",
      "[Training Epoch 3] Batch 3801, Loss 0.26794904470443726\n",
      "[Training Epoch 3] Batch 3802, Loss 0.266075998544693\n",
      "[Training Epoch 3] Batch 3803, Loss 0.2509102523326874\n",
      "[Training Epoch 3] Batch 3804, Loss 0.236787348985672\n",
      "[Training Epoch 3] Batch 3805, Loss 0.28287309408187866\n",
      "[Training Epoch 3] Batch 3806, Loss 0.2803943455219269\n",
      "[Training Epoch 3] Batch 3807, Loss 0.2753027677536011\n",
      "[Training Epoch 3] Batch 3808, Loss 0.2936771810054779\n",
      "[Training Epoch 3] Batch 3809, Loss 0.2752687931060791\n",
      "[Training Epoch 3] Batch 3810, Loss 0.27672603726387024\n",
      "[Training Epoch 3] Batch 3811, Loss 0.2754141092300415\n",
      "[Training Epoch 3] Batch 3812, Loss 0.29011070728302\n",
      "[Training Epoch 3] Batch 3813, Loss 0.2678217589855194\n",
      "[Training Epoch 3] Batch 3814, Loss 0.25250542163848877\n",
      "[Training Epoch 3] Batch 3815, Loss 0.266748309135437\n",
      "[Training Epoch 3] Batch 3816, Loss 0.2682383954524994\n",
      "[Training Epoch 3] Batch 3817, Loss 0.2685641646385193\n",
      "[Training Epoch 3] Batch 3818, Loss 0.2802925109863281\n",
      "[Training Epoch 3] Batch 3819, Loss 0.29455798864364624\n",
      "[Training Epoch 3] Batch 3820, Loss 0.3117256164550781\n",
      "[Training Epoch 3] Batch 3821, Loss 0.27551937103271484\n",
      "[Training Epoch 3] Batch 3822, Loss 0.24399173259735107\n",
      "[Training Epoch 3] Batch 3823, Loss 0.2650074362754822\n",
      "[Training Epoch 3] Batch 3824, Loss 0.26842057704925537\n",
      "[Training Epoch 3] Batch 3825, Loss 0.27966147661209106\n",
      "[Training Epoch 3] Batch 3826, Loss 0.2771727740764618\n",
      "[Training Epoch 3] Batch 3827, Loss 0.25149425864219666\n",
      "[Training Epoch 3] Batch 3828, Loss 0.24234828352928162\n",
      "[Training Epoch 3] Batch 3829, Loss 0.2722913324832916\n",
      "[Training Epoch 3] Batch 3830, Loss 0.2579004466533661\n",
      "[Training Epoch 3] Batch 3831, Loss 0.2519485354423523\n",
      "[Training Epoch 3] Batch 3832, Loss 0.26196956634521484\n",
      "[Training Epoch 3] Batch 3833, Loss 0.24961885809898376\n",
      "[Training Epoch 3] Batch 3834, Loss 0.266139954328537\n",
      "[Training Epoch 3] Batch 3835, Loss 0.26443278789520264\n",
      "[Training Epoch 3] Batch 3836, Loss 0.25858139991760254\n",
      "[Training Epoch 3] Batch 3837, Loss 0.2877747118473053\n",
      "[Training Epoch 3] Batch 3838, Loss 0.26472151279449463\n",
      "[Training Epoch 3] Batch 3839, Loss 0.2597932517528534\n",
      "[Training Epoch 3] Batch 3840, Loss 0.2967260479927063\n",
      "[Training Epoch 3] Batch 3841, Loss 0.27360302209854126\n",
      "[Training Epoch 3] Batch 3842, Loss 0.23656275868415833\n",
      "[Training Epoch 3] Batch 3843, Loss 0.23483821749687195\n",
      "[Training Epoch 3] Batch 3844, Loss 0.27116894721984863\n",
      "[Training Epoch 3] Batch 3845, Loss 0.26782384514808655\n",
      "[Training Epoch 3] Batch 3846, Loss 0.2481299638748169\n",
      "[Training Epoch 3] Batch 3847, Loss 0.263713538646698\n",
      "[Training Epoch 3] Batch 3848, Loss 0.2788264751434326\n",
      "[Training Epoch 3] Batch 3849, Loss 0.24398037791252136\n",
      "[Training Epoch 3] Batch 3850, Loss 0.2453557699918747\n",
      "[Training Epoch 3] Batch 3851, Loss 0.28061169385910034\n",
      "[Training Epoch 3] Batch 3852, Loss 0.28748953342437744\n",
      "[Training Epoch 3] Batch 3853, Loss 0.27357882261276245\n",
      "[Training Epoch 3] Batch 3854, Loss 0.2737284302711487\n",
      "[Training Epoch 3] Batch 3855, Loss 0.25877779722213745\n",
      "[Training Epoch 3] Batch 3856, Loss 0.25241512060165405\n",
      "[Training Epoch 3] Batch 3857, Loss 0.2732602655887604\n",
      "[Training Epoch 3] Batch 3858, Loss 0.27139827609062195\n",
      "[Training Epoch 3] Batch 3859, Loss 0.2821448743343353\n",
      "[Training Epoch 3] Batch 3860, Loss 0.2612828016281128\n",
      "[Training Epoch 3] Batch 3861, Loss 0.26459309458732605\n",
      "[Training Epoch 3] Batch 3862, Loss 0.2831226885318756\n",
      "[Training Epoch 3] Batch 3863, Loss 0.2746519446372986\n",
      "[Training Epoch 3] Batch 3864, Loss 0.3032703995704651\n",
      "[Training Epoch 3] Batch 3865, Loss 0.2733652591705322\n",
      "[Training Epoch 3] Batch 3866, Loss 0.2243698537349701\n",
      "[Training Epoch 3] Batch 3867, Loss 0.2856243848800659\n",
      "[Training Epoch 3] Batch 3868, Loss 0.2557235360145569\n",
      "[Training Epoch 3] Batch 3869, Loss 0.3095088601112366\n",
      "[Training Epoch 3] Batch 3870, Loss 0.2599082589149475\n",
      "[Training Epoch 3] Batch 3871, Loss 0.2759631872177124\n",
      "[Training Epoch 3] Batch 3872, Loss 0.266968309879303\n",
      "[Training Epoch 3] Batch 3873, Loss 0.2860701382160187\n",
      "[Training Epoch 3] Batch 3874, Loss 0.2647033929824829\n",
      "[Training Epoch 3] Batch 3875, Loss 0.2673949897289276\n",
      "[Training Epoch 3] Batch 3876, Loss 0.29503828287124634\n",
      "[Training Epoch 3] Batch 3877, Loss 0.2447677105665207\n",
      "[Training Epoch 3] Batch 3878, Loss 0.25883209705352783\n",
      "[Training Epoch 3] Batch 3879, Loss 0.26922038197517395\n",
      "[Training Epoch 3] Batch 3880, Loss 0.2829315662384033\n",
      "[Training Epoch 3] Batch 3881, Loss 0.2570447325706482\n",
      "[Training Epoch 3] Batch 3882, Loss 0.2712650001049042\n",
      "[Training Epoch 3] Batch 3883, Loss 0.263795405626297\n",
      "[Training Epoch 3] Batch 3884, Loss 0.3089214265346527\n",
      "[Training Epoch 3] Batch 3885, Loss 0.25557324290275574\n",
      "[Training Epoch 3] Batch 3886, Loss 0.2271958291530609\n",
      "[Training Epoch 3] Batch 3887, Loss 0.2662551999092102\n",
      "[Training Epoch 3] Batch 3888, Loss 0.25561606884002686\n",
      "[Training Epoch 3] Batch 3889, Loss 0.2801723778247833\n",
      "[Training Epoch 3] Batch 3890, Loss 0.24799895286560059\n",
      "[Training Epoch 3] Batch 3891, Loss 0.2828952372074127\n",
      "[Training Epoch 3] Batch 3892, Loss 0.2868075966835022\n",
      "[Training Epoch 3] Batch 3893, Loss 0.2825860381126404\n",
      "[Training Epoch 3] Batch 3894, Loss 0.2874055504798889\n",
      "[Training Epoch 3] Batch 3895, Loss 0.26224276423454285\n",
      "[Training Epoch 3] Batch 3896, Loss 0.2744576930999756\n",
      "[Training Epoch 3] Batch 3897, Loss 0.2423243522644043\n",
      "[Training Epoch 3] Batch 3898, Loss 0.2919583320617676\n",
      "[Training Epoch 3] Batch 3899, Loss 0.28478866815567017\n",
      "[Training Epoch 3] Batch 3900, Loss 0.2708141505718231\n",
      "[Training Epoch 3] Batch 3901, Loss 0.2414790689945221\n",
      "[Training Epoch 3] Batch 3902, Loss 0.24770015478134155\n",
      "[Training Epoch 3] Batch 3903, Loss 0.26966747641563416\n",
      "[Training Epoch 3] Batch 3904, Loss 0.3182516098022461\n",
      "[Training Epoch 3] Batch 3905, Loss 0.24463987350463867\n",
      "[Training Epoch 3] Batch 3906, Loss 0.2563902735710144\n",
      "[Training Epoch 3] Batch 3907, Loss 0.2731150984764099\n",
      "[Training Epoch 3] Batch 3908, Loss 0.29148930311203003\n",
      "[Training Epoch 3] Batch 3909, Loss 0.2759101688861847\n",
      "[Training Epoch 3] Batch 3910, Loss 0.25821536779403687\n",
      "[Training Epoch 3] Batch 3911, Loss 0.27750396728515625\n",
      "[Training Epoch 3] Batch 3912, Loss 0.30127108097076416\n",
      "[Training Epoch 3] Batch 3913, Loss 0.24118468165397644\n",
      "[Training Epoch 3] Batch 3914, Loss 0.26721763610839844\n",
      "[Training Epoch 3] Batch 3915, Loss 0.24854332208633423\n",
      "[Training Epoch 3] Batch 3916, Loss 0.26300084590911865\n",
      "[Training Epoch 3] Batch 3917, Loss 0.2813740670681\n",
      "[Training Epoch 3] Batch 3918, Loss 0.2516607642173767\n",
      "[Training Epoch 3] Batch 3919, Loss 0.27160805463790894\n",
      "[Training Epoch 3] Batch 3920, Loss 0.25818026065826416\n",
      "[Training Epoch 3] Batch 3921, Loss 0.2623119354248047\n",
      "[Training Epoch 3] Batch 3922, Loss 0.27996259927749634\n",
      "[Training Epoch 3] Batch 3923, Loss 0.2705128490924835\n",
      "[Training Epoch 3] Batch 3924, Loss 0.27874013781547546\n",
      "[Training Epoch 3] Batch 3925, Loss 0.2480127513408661\n",
      "[Training Epoch 3] Batch 3926, Loss 0.24423037469387054\n",
      "[Training Epoch 3] Batch 3927, Loss 0.2559164762496948\n",
      "[Training Epoch 3] Batch 3928, Loss 0.2498135268688202\n",
      "[Training Epoch 3] Batch 3929, Loss 0.28416794538497925\n",
      "[Training Epoch 3] Batch 3930, Loss 0.2579299211502075\n",
      "[Training Epoch 3] Batch 3931, Loss 0.23480378091335297\n",
      "[Training Epoch 3] Batch 3932, Loss 0.2649598717689514\n",
      "[Training Epoch 3] Batch 3933, Loss 0.2932283282279968\n",
      "[Training Epoch 3] Batch 3934, Loss 0.2533801198005676\n",
      "[Training Epoch 3] Batch 3935, Loss 0.2622724771499634\n",
      "[Training Epoch 3] Batch 3936, Loss 0.281349241733551\n",
      "[Training Epoch 3] Batch 3937, Loss 0.25728029012680054\n",
      "[Training Epoch 3] Batch 3938, Loss 0.2507946193218231\n",
      "[Training Epoch 3] Batch 3939, Loss 0.2887765169143677\n",
      "[Training Epoch 3] Batch 3940, Loss 0.27688151597976685\n",
      "[Training Epoch 3] Batch 3941, Loss 0.2841683626174927\n",
      "[Training Epoch 3] Batch 3942, Loss 0.29041701555252075\n",
      "[Training Epoch 3] Batch 3943, Loss 0.23917391896247864\n",
      "[Training Epoch 3] Batch 3944, Loss 0.2634648084640503\n",
      "[Training Epoch 3] Batch 3945, Loss 0.28237560391426086\n",
      "[Training Epoch 3] Batch 3946, Loss 0.2605438232421875\n",
      "[Training Epoch 3] Batch 3947, Loss 0.2821704149246216\n",
      "[Training Epoch 3] Batch 3948, Loss 0.26583439111709595\n",
      "[Training Epoch 3] Batch 3949, Loss 0.2630320191383362\n",
      "[Training Epoch 3] Batch 3950, Loss 0.29258352518081665\n",
      "[Training Epoch 3] Batch 3951, Loss 0.2734658420085907\n",
      "[Training Epoch 3] Batch 3952, Loss 0.24553529918193817\n",
      "[Training Epoch 3] Batch 3953, Loss 0.2375200092792511\n",
      "[Training Epoch 3] Batch 3954, Loss 0.2632712125778198\n",
      "[Training Epoch 3] Batch 3955, Loss 0.25745847821235657\n",
      "[Training Epoch 3] Batch 3956, Loss 0.2424275279045105\n",
      "[Training Epoch 3] Batch 3957, Loss 0.2665172219276428\n",
      "[Training Epoch 3] Batch 3958, Loss 0.2620406746864319\n",
      "[Training Epoch 3] Batch 3959, Loss 0.26095345616340637\n",
      "[Training Epoch 3] Batch 3960, Loss 0.2769818902015686\n",
      "[Training Epoch 3] Batch 3961, Loss 0.32356539368629456\n",
      "[Training Epoch 3] Batch 3962, Loss 0.27691566944122314\n",
      "[Training Epoch 3] Batch 3963, Loss 0.26399609446525574\n",
      "[Training Epoch 3] Batch 3964, Loss 0.28660422563552856\n",
      "[Training Epoch 3] Batch 3965, Loss 0.25825512409210205\n",
      "[Training Epoch 3] Batch 3966, Loss 0.28660136461257935\n",
      "[Training Epoch 3] Batch 3967, Loss 0.27073442935943604\n",
      "[Training Epoch 3] Batch 3968, Loss 0.2747805714607239\n",
      "[Training Epoch 3] Batch 3969, Loss 0.24308675527572632\n",
      "[Training Epoch 3] Batch 3970, Loss 0.24669474363327026\n",
      "[Training Epoch 3] Batch 3971, Loss 0.2998012900352478\n",
      "[Training Epoch 3] Batch 3972, Loss 0.2587398290634155\n",
      "[Training Epoch 3] Batch 3973, Loss 0.24576634168624878\n",
      "[Training Epoch 3] Batch 3974, Loss 0.2681468427181244\n",
      "[Training Epoch 3] Batch 3975, Loss 0.2777670621871948\n",
      "[Training Epoch 3] Batch 3976, Loss 0.28124845027923584\n",
      "[Training Epoch 3] Batch 3977, Loss 0.25657111406326294\n",
      "[Training Epoch 3] Batch 3978, Loss 0.25834405422210693\n",
      "[Training Epoch 3] Batch 3979, Loss 0.28492242097854614\n",
      "[Training Epoch 3] Batch 3980, Loss 0.25177001953125\n",
      "[Training Epoch 3] Batch 3981, Loss 0.28821131587028503\n",
      "[Training Epoch 3] Batch 3982, Loss 0.281301349401474\n",
      "[Training Epoch 3] Batch 3983, Loss 0.251691997051239\n",
      "[Training Epoch 3] Batch 3984, Loss 0.25300323963165283\n",
      "[Training Epoch 3] Batch 3985, Loss 0.2578784227371216\n",
      "[Training Epoch 3] Batch 3986, Loss 0.26945143938064575\n",
      "[Training Epoch 3] Batch 3987, Loss 0.2711806893348694\n",
      "[Training Epoch 3] Batch 3988, Loss 0.2862268090248108\n",
      "[Training Epoch 3] Batch 3989, Loss 0.2525905966758728\n",
      "[Training Epoch 3] Batch 3990, Loss 0.3036360740661621\n",
      "[Training Epoch 3] Batch 3991, Loss 0.2800517976284027\n",
      "[Training Epoch 3] Batch 3992, Loss 0.24798455834388733\n",
      "[Training Epoch 3] Batch 3993, Loss 0.2766670882701874\n",
      "[Training Epoch 3] Batch 3994, Loss 0.2414248287677765\n",
      "[Training Epoch 3] Batch 3995, Loss 0.23094528913497925\n",
      "[Training Epoch 3] Batch 3996, Loss 0.27490508556365967\n",
      "[Training Epoch 3] Batch 3997, Loss 0.24095050990581512\n",
      "[Training Epoch 3] Batch 3998, Loss 0.24587559700012207\n",
      "[Training Epoch 3] Batch 3999, Loss 0.24996685981750488\n",
      "[Training Epoch 3] Batch 4000, Loss 0.2766813635826111\n",
      "[Training Epoch 3] Batch 4001, Loss 0.26610615849494934\n",
      "[Training Epoch 3] Batch 4002, Loss 0.26551079750061035\n",
      "[Training Epoch 3] Batch 4003, Loss 0.2690503001213074\n",
      "[Training Epoch 3] Batch 4004, Loss 0.2723715305328369\n",
      "[Training Epoch 3] Batch 4005, Loss 0.2911536693572998\n",
      "[Training Epoch 3] Batch 4006, Loss 0.2843119502067566\n",
      "[Training Epoch 3] Batch 4007, Loss 0.2955598533153534\n",
      "[Training Epoch 3] Batch 4008, Loss 0.25846898555755615\n",
      "[Training Epoch 3] Batch 4009, Loss 0.2889748811721802\n",
      "[Training Epoch 3] Batch 4010, Loss 0.27098017930984497\n",
      "[Training Epoch 3] Batch 4011, Loss 0.26538553833961487\n",
      "[Training Epoch 3] Batch 4012, Loss 0.2759424149990082\n",
      "[Training Epoch 3] Batch 4013, Loss 0.29467159509658813\n",
      "[Training Epoch 3] Batch 4014, Loss 0.2673848867416382\n",
      "[Training Epoch 3] Batch 4015, Loss 0.2916030287742615\n",
      "[Training Epoch 3] Batch 4016, Loss 0.27017685770988464\n",
      "[Training Epoch 3] Batch 4017, Loss 0.25671136379241943\n",
      "[Training Epoch 3] Batch 4018, Loss 0.2642744183540344\n",
      "[Training Epoch 3] Batch 4019, Loss 0.24738453328609467\n",
      "[Training Epoch 3] Batch 4020, Loss 0.2436692714691162\n",
      "[Training Epoch 3] Batch 4021, Loss 0.280421644449234\n",
      "[Training Epoch 3] Batch 4022, Loss 0.291898250579834\n",
      "[Training Epoch 3] Batch 4023, Loss 0.25464895367622375\n",
      "[Training Epoch 3] Batch 4024, Loss 0.2906839847564697\n",
      "[Training Epoch 3] Batch 4025, Loss 0.2691667377948761\n",
      "[Training Epoch 3] Batch 4026, Loss 0.26592084765434265\n",
      "[Training Epoch 3] Batch 4027, Loss 0.27143171429634094\n",
      "[Training Epoch 3] Batch 4028, Loss 0.23047320544719696\n",
      "[Training Epoch 3] Batch 4029, Loss 0.2538371980190277\n",
      "[Training Epoch 3] Batch 4030, Loss 0.2850918173789978\n",
      "[Training Epoch 3] Batch 4031, Loss 0.2827613949775696\n",
      "[Training Epoch 3] Batch 4032, Loss 0.27612414956092834\n",
      "[Training Epoch 3] Batch 4033, Loss 0.27332285046577454\n",
      "[Training Epoch 3] Batch 4034, Loss 0.27714550495147705\n",
      "[Training Epoch 3] Batch 4035, Loss 0.28393667936325073\n",
      "[Training Epoch 3] Batch 4036, Loss 0.2825137972831726\n",
      "[Training Epoch 3] Batch 4037, Loss 0.2628372311592102\n",
      "[Training Epoch 3] Batch 4038, Loss 0.2572909891605377\n",
      "[Training Epoch 3] Batch 4039, Loss 0.27444249391555786\n",
      "[Training Epoch 3] Batch 4040, Loss 0.28673917055130005\n",
      "[Training Epoch 3] Batch 4041, Loss 0.2607751786708832\n",
      "[Training Epoch 3] Batch 4042, Loss 0.2606021463871002\n",
      "[Training Epoch 3] Batch 4043, Loss 0.3163345456123352\n",
      "[Training Epoch 3] Batch 4044, Loss 0.2738661766052246\n",
      "[Training Epoch 3] Batch 4045, Loss 0.28145831823349\n",
      "[Training Epoch 3] Batch 4046, Loss 0.28053849935531616\n",
      "[Training Epoch 3] Batch 4047, Loss 0.28159672021865845\n",
      "[Training Epoch 3] Batch 4048, Loss 0.29041463136672974\n",
      "[Training Epoch 3] Batch 4049, Loss 0.23691952228546143\n",
      "[Training Epoch 3] Batch 4050, Loss 0.2747421860694885\n",
      "[Training Epoch 3] Batch 4051, Loss 0.2942413091659546\n",
      "[Training Epoch 3] Batch 4052, Loss 0.24952641129493713\n",
      "[Training Epoch 3] Batch 4053, Loss 0.29778748750686646\n",
      "[Training Epoch 3] Batch 4054, Loss 0.25412940979003906\n",
      "[Training Epoch 3] Batch 4055, Loss 0.2649126350879669\n",
      "[Training Epoch 3] Batch 4056, Loss 0.2786775231361389\n",
      "[Training Epoch 3] Batch 4057, Loss 0.2654496431350708\n",
      "[Training Epoch 3] Batch 4058, Loss 0.2687651515007019\n",
      "[Training Epoch 3] Batch 4059, Loss 0.2456437647342682\n",
      "[Training Epoch 3] Batch 4060, Loss 0.23481342196464539\n",
      "[Training Epoch 3] Batch 4061, Loss 0.26715224981307983\n",
      "[Training Epoch 3] Batch 4062, Loss 0.27868545055389404\n",
      "[Training Epoch 3] Batch 4063, Loss 0.26513272523880005\n",
      "[Training Epoch 3] Batch 4064, Loss 0.264626145362854\n",
      "[Training Epoch 3] Batch 4065, Loss 0.280447781085968\n",
      "[Training Epoch 3] Batch 4066, Loss 0.281658411026001\n",
      "[Training Epoch 3] Batch 4067, Loss 0.2817208170890808\n",
      "[Training Epoch 3] Batch 4068, Loss 0.23635920882225037\n",
      "[Training Epoch 3] Batch 4069, Loss 0.2700532078742981\n",
      "[Training Epoch 3] Batch 4070, Loss 0.2827114462852478\n",
      "[Training Epoch 3] Batch 4071, Loss 0.24780453741550446\n",
      "[Training Epoch 3] Batch 4072, Loss 0.2898218631744385\n",
      "[Training Epoch 3] Batch 4073, Loss 0.27350914478302\n",
      "[Training Epoch 3] Batch 4074, Loss 0.28174132108688354\n",
      "[Training Epoch 3] Batch 4075, Loss 0.2564709186553955\n",
      "[Training Epoch 3] Batch 4076, Loss 0.26225829124450684\n",
      "[Training Epoch 3] Batch 4077, Loss 0.2791057229042053\n",
      "[Training Epoch 3] Batch 4078, Loss 0.22498947381973267\n",
      "[Training Epoch 3] Batch 4079, Loss 0.24034544825553894\n",
      "[Training Epoch 3] Batch 4080, Loss 0.2789897322654724\n",
      "[Training Epoch 3] Batch 4081, Loss 0.28940653800964355\n",
      "[Training Epoch 3] Batch 4082, Loss 0.27743977308273315\n",
      "[Training Epoch 3] Batch 4083, Loss 0.22244101762771606\n",
      "[Training Epoch 3] Batch 4084, Loss 0.28102362155914307\n",
      "[Training Epoch 3] Batch 4085, Loss 0.2891846001148224\n",
      "[Training Epoch 3] Batch 4086, Loss 0.2713690400123596\n",
      "[Training Epoch 3] Batch 4087, Loss 0.2693183422088623\n",
      "[Training Epoch 3] Batch 4088, Loss 0.26468032598495483\n",
      "[Training Epoch 3] Batch 4089, Loss 0.2845214605331421\n",
      "[Training Epoch 3] Batch 4090, Loss 0.2723223567008972\n",
      "[Training Epoch 3] Batch 4091, Loss 0.244773730635643\n",
      "[Training Epoch 3] Batch 4092, Loss 0.2716965675354004\n",
      "[Training Epoch 3] Batch 4093, Loss 0.2633090317249298\n",
      "[Training Epoch 3] Batch 4094, Loss 0.2624816596508026\n",
      "[Training Epoch 3] Batch 4095, Loss 0.23588252067565918\n",
      "[Training Epoch 3] Batch 4096, Loss 0.25349006056785583\n",
      "[Training Epoch 3] Batch 4097, Loss 0.26530930399894714\n",
      "[Training Epoch 3] Batch 4098, Loss 0.25817620754241943\n",
      "[Training Epoch 3] Batch 4099, Loss 0.2641768455505371\n",
      "[Training Epoch 3] Batch 4100, Loss 0.2584172785282135\n",
      "[Training Epoch 3] Batch 4101, Loss 0.24341796338558197\n",
      "[Training Epoch 3] Batch 4102, Loss 0.2749352753162384\n",
      "[Training Epoch 3] Batch 4103, Loss 0.2729490399360657\n",
      "[Training Epoch 3] Batch 4104, Loss 0.27684611082077026\n",
      "[Training Epoch 3] Batch 4105, Loss 0.3125985860824585\n",
      "[Training Epoch 3] Batch 4106, Loss 0.2546805441379547\n",
      "[Training Epoch 3] Batch 4107, Loss 0.25801870226860046\n",
      "[Training Epoch 3] Batch 4108, Loss 0.2712099850177765\n",
      "[Training Epoch 3] Batch 4109, Loss 0.25802022218704224\n",
      "[Training Epoch 3] Batch 4110, Loss 0.2762095630168915\n",
      "[Training Epoch 3] Batch 4111, Loss 0.2696456015110016\n",
      "[Training Epoch 3] Batch 4112, Loss 0.2420099973678589\n",
      "[Training Epoch 3] Batch 4113, Loss 0.25769177079200745\n",
      "[Training Epoch 3] Batch 4114, Loss 0.23185604810714722\n",
      "[Training Epoch 3] Batch 4115, Loss 0.26593759655952454\n",
      "[Training Epoch 3] Batch 4116, Loss 0.2655141353607178\n",
      "[Training Epoch 3] Batch 4117, Loss 0.2325776368379593\n",
      "[Training Epoch 3] Batch 4118, Loss 0.2754802703857422\n",
      "[Training Epoch 3] Batch 4119, Loss 0.25241681933403015\n",
      "[Training Epoch 3] Batch 4120, Loss 0.2801103889942169\n",
      "[Training Epoch 3] Batch 4121, Loss 0.295320987701416\n",
      "[Training Epoch 3] Batch 4122, Loss 0.2606920003890991\n",
      "[Training Epoch 3] Batch 4123, Loss 0.270194411277771\n",
      "[Training Epoch 3] Batch 4124, Loss 0.26488620042800903\n",
      "[Training Epoch 3] Batch 4125, Loss 0.2445177584886551\n",
      "[Training Epoch 3] Batch 4126, Loss 0.26368916034698486\n",
      "[Training Epoch 3] Batch 4127, Loss 0.2620027959346771\n",
      "[Training Epoch 3] Batch 4128, Loss 0.2567780017852783\n",
      "[Training Epoch 3] Batch 4129, Loss 0.27641618251800537\n",
      "[Training Epoch 3] Batch 4130, Loss 0.2508814036846161\n",
      "[Training Epoch 3] Batch 4131, Loss 0.26689401268959045\n",
      "[Training Epoch 3] Batch 4132, Loss 0.28801870346069336\n",
      "[Training Epoch 3] Batch 4133, Loss 0.2536581754684448\n",
      "[Training Epoch 3] Batch 4134, Loss 0.2655313313007355\n",
      "[Training Epoch 3] Batch 4135, Loss 0.23767174780368805\n",
      "[Training Epoch 3] Batch 4136, Loss 0.2520200312137604\n",
      "[Training Epoch 3] Batch 4137, Loss 0.2606537342071533\n",
      "[Training Epoch 3] Batch 4138, Loss 0.2761618494987488\n",
      "[Training Epoch 3] Batch 4139, Loss 0.2640420198440552\n",
      "[Training Epoch 3] Batch 4140, Loss 0.2926550507545471\n",
      "[Training Epoch 3] Batch 4141, Loss 0.27240970730781555\n",
      "[Training Epoch 3] Batch 4142, Loss 0.2679895758628845\n",
      "[Training Epoch 3] Batch 4143, Loss 0.2501875162124634\n",
      "[Training Epoch 3] Batch 4144, Loss 0.27354565262794495\n",
      "[Training Epoch 3] Batch 4145, Loss 0.28200048208236694\n",
      "[Training Epoch 3] Batch 4146, Loss 0.24117203056812286\n",
      "[Training Epoch 3] Batch 4147, Loss 0.26683878898620605\n",
      "[Training Epoch 3] Batch 4148, Loss 0.2813432216644287\n",
      "[Training Epoch 3] Batch 4149, Loss 0.3056240975856781\n",
      "[Training Epoch 3] Batch 4150, Loss 0.27825289964675903\n",
      "[Training Epoch 3] Batch 4151, Loss 0.2549714148044586\n",
      "[Training Epoch 3] Batch 4152, Loss 0.29132533073425293\n",
      "[Training Epoch 3] Batch 4153, Loss 0.2566811740398407\n",
      "[Training Epoch 3] Batch 4154, Loss 0.26565951108932495\n",
      "[Training Epoch 3] Batch 4155, Loss 0.3027373254299164\n",
      "[Training Epoch 3] Batch 4156, Loss 0.2600051164627075\n",
      "[Training Epoch 3] Batch 4157, Loss 0.26269012689590454\n",
      "[Training Epoch 3] Batch 4158, Loss 0.2730521559715271\n",
      "[Training Epoch 3] Batch 4159, Loss 0.2563211917877197\n",
      "[Training Epoch 3] Batch 4160, Loss 0.26176175475120544\n",
      "[Training Epoch 3] Batch 4161, Loss 0.263884574174881\n",
      "[Training Epoch 3] Batch 4162, Loss 0.2801077663898468\n",
      "[Training Epoch 3] Batch 4163, Loss 0.2549721598625183\n",
      "[Training Epoch 3] Batch 4164, Loss 0.24821583926677704\n",
      "[Training Epoch 3] Batch 4165, Loss 0.28181833028793335\n",
      "[Training Epoch 3] Batch 4166, Loss 0.25981786847114563\n",
      "[Training Epoch 3] Batch 4167, Loss 0.2581128180027008\n",
      "[Training Epoch 3] Batch 4168, Loss 0.24722746014595032\n",
      "[Training Epoch 3] Batch 4169, Loss 0.28538647294044495\n",
      "[Training Epoch 3] Batch 4170, Loss 0.27445554733276367\n",
      "[Training Epoch 3] Batch 4171, Loss 0.2754155993461609\n",
      "[Training Epoch 3] Batch 4172, Loss 0.26802653074264526\n",
      "[Training Epoch 3] Batch 4173, Loss 0.2653152644634247\n",
      "[Training Epoch 3] Batch 4174, Loss 0.23091864585876465\n",
      "[Training Epoch 3] Batch 4175, Loss 0.26787540316581726\n",
      "[Training Epoch 3] Batch 4176, Loss 0.27343112230300903\n",
      "[Training Epoch 3] Batch 4177, Loss 0.2672315239906311\n",
      "[Training Epoch 3] Batch 4178, Loss 0.26474660634994507\n",
      "[Training Epoch 3] Batch 4179, Loss 0.26490089297294617\n",
      "[Training Epoch 3] Batch 4180, Loss 0.24222178757190704\n",
      "[Training Epoch 3] Batch 4181, Loss 0.2673361301422119\n",
      "[Training Epoch 3] Batch 4182, Loss 0.26073259115219116\n",
      "[Training Epoch 3] Batch 4183, Loss 0.28087061643600464\n",
      "[Training Epoch 3] Batch 4184, Loss 0.2908743619918823\n",
      "[Training Epoch 3] Batch 4185, Loss 0.2797468900680542\n",
      "[Training Epoch 3] Batch 4186, Loss 0.30456119775772095\n",
      "[Training Epoch 3] Batch 4187, Loss 0.24687066674232483\n",
      "[Training Epoch 3] Batch 4188, Loss 0.31402134895324707\n",
      "[Training Epoch 3] Batch 4189, Loss 0.2668982148170471\n",
      "[Training Epoch 3] Batch 4190, Loss 0.294590026140213\n",
      "[Training Epoch 3] Batch 4191, Loss 0.2821277379989624\n",
      "[Training Epoch 3] Batch 4192, Loss 0.2851637601852417\n",
      "[Training Epoch 3] Batch 4193, Loss 0.2821669578552246\n",
      "[Training Epoch 3] Batch 4194, Loss 0.27370840311050415\n",
      "[Training Epoch 3] Batch 4195, Loss 0.2797815203666687\n",
      "[Training Epoch 3] Batch 4196, Loss 0.2660231590270996\n",
      "[Training Epoch 3] Batch 4197, Loss 0.2783411741256714\n",
      "[Training Epoch 3] Batch 4198, Loss 0.2957768142223358\n",
      "[Training Epoch 3] Batch 4199, Loss 0.2483830600976944\n",
      "[Training Epoch 3] Batch 4200, Loss 0.29423826932907104\n",
      "[Training Epoch 3] Batch 4201, Loss 0.2893921434879303\n",
      "[Training Epoch 3] Batch 4202, Loss 0.2508717179298401\n",
      "[Training Epoch 3] Batch 4203, Loss 0.2650872468948364\n",
      "[Training Epoch 3] Batch 4204, Loss 0.2586787939071655\n",
      "[Training Epoch 3] Batch 4205, Loss 0.2389545738697052\n",
      "[Training Epoch 3] Batch 4206, Loss 0.29597562551498413\n",
      "[Training Epoch 3] Batch 4207, Loss 0.30180394649505615\n",
      "[Training Epoch 3] Batch 4208, Loss 0.261573851108551\n",
      "[Training Epoch 3] Batch 4209, Loss 0.28219491243362427\n",
      "[Training Epoch 3] Batch 4210, Loss 0.2653532326221466\n",
      "[Training Epoch 3] Batch 4211, Loss 0.2780233919620514\n",
      "[Training Epoch 3] Batch 4212, Loss 0.26802003383636475\n",
      "[Training Epoch 3] Batch 4213, Loss 0.2931061387062073\n",
      "[Training Epoch 3] Batch 4214, Loss 0.2540234327316284\n",
      "[Training Epoch 3] Batch 4215, Loss 0.2624738812446594\n",
      "[Training Epoch 3] Batch 4216, Loss 0.2630126476287842\n",
      "[Training Epoch 3] Batch 4217, Loss 0.2684118449687958\n",
      "[Training Epoch 3] Batch 4218, Loss 0.26515108346939087\n",
      "[Training Epoch 3] Batch 4219, Loss 0.29412582516670227\n",
      "[Training Epoch 3] Batch 4220, Loss 0.2772138714790344\n",
      "[Training Epoch 3] Batch 4221, Loss 0.28946250677108765\n",
      "[Training Epoch 3] Batch 4222, Loss 0.2594960629940033\n",
      "[Training Epoch 3] Batch 4223, Loss 0.27209675312042236\n",
      "[Training Epoch 3] Batch 4224, Loss 0.28677454590797424\n",
      "[Training Epoch 3] Batch 4225, Loss 0.2511911392211914\n",
      "[Training Epoch 3] Batch 4226, Loss 0.24762731790542603\n",
      "[Training Epoch 3] Batch 4227, Loss 0.24917879700660706\n",
      "[Training Epoch 3] Batch 4228, Loss 0.2819240391254425\n",
      "[Training Epoch 3] Batch 4229, Loss 0.26739221811294556\n",
      "[Training Epoch 3] Batch 4230, Loss 0.27005016803741455\n",
      "[Training Epoch 3] Batch 4231, Loss 0.23902282118797302\n",
      "[Training Epoch 3] Batch 4232, Loss 0.2744351029396057\n",
      "[Training Epoch 3] Batch 4233, Loss 0.23860211670398712\n",
      "[Training Epoch 3] Batch 4234, Loss 0.2771605849266052\n",
      "[Training Epoch 3] Batch 4235, Loss 0.2774328589439392\n",
      "[Training Epoch 3] Batch 4236, Loss 0.2924545407295227\n",
      "[Training Epoch 3] Batch 4237, Loss 0.2624446749687195\n",
      "[Training Epoch 3] Batch 4238, Loss 0.2769938111305237\n",
      "[Training Epoch 3] Batch 4239, Loss 0.28109216690063477\n",
      "[Training Epoch 3] Batch 4240, Loss 0.24786841869354248\n",
      "[Training Epoch 3] Batch 4241, Loss 0.270132839679718\n",
      "[Training Epoch 3] Batch 4242, Loss 0.23379938304424286\n",
      "[Training Epoch 3] Batch 4243, Loss 0.276630163192749\n",
      "[Training Epoch 3] Batch 4244, Loss 0.2545114755630493\n",
      "[Training Epoch 3] Batch 4245, Loss 0.27531397342681885\n",
      "[Training Epoch 3] Batch 4246, Loss 0.2715255618095398\n",
      "[Training Epoch 3] Batch 4247, Loss 0.26660120487213135\n",
      "[Training Epoch 3] Batch 4248, Loss 0.27538013458251953\n",
      "[Training Epoch 3] Batch 4249, Loss 0.2745240330696106\n",
      "[Training Epoch 3] Batch 4250, Loss 0.25695887207984924\n",
      "[Training Epoch 3] Batch 4251, Loss 0.27993476390838623\n",
      "[Training Epoch 3] Batch 4252, Loss 0.26853451132774353\n",
      "[Training Epoch 3] Batch 4253, Loss 0.28145694732666016\n",
      "[Training Epoch 3] Batch 4254, Loss 0.2718656361103058\n",
      "[Training Epoch 3] Batch 4255, Loss 0.2492522895336151\n",
      "[Training Epoch 3] Batch 4256, Loss 0.27691400051116943\n",
      "[Training Epoch 3] Batch 4257, Loss 0.27180492877960205\n",
      "[Training Epoch 3] Batch 4258, Loss 0.2566884756088257\n",
      "[Training Epoch 3] Batch 4259, Loss 0.2865413427352905\n",
      "[Training Epoch 3] Batch 4260, Loss 0.27739134430885315\n",
      "[Training Epoch 3] Batch 4261, Loss 0.26580172777175903\n",
      "[Training Epoch 3] Batch 4262, Loss 0.2741236686706543\n",
      "[Training Epoch 3] Batch 4263, Loss 0.2522260546684265\n",
      "[Training Epoch 3] Batch 4264, Loss 0.30074548721313477\n",
      "[Training Epoch 3] Batch 4265, Loss 0.2736278176307678\n",
      "[Training Epoch 3] Batch 4266, Loss 0.2996380627155304\n",
      "[Training Epoch 3] Batch 4267, Loss 0.2730458378791809\n",
      "[Training Epoch 3] Batch 4268, Loss 0.2628335654735565\n",
      "[Training Epoch 3] Batch 4269, Loss 0.28015780448913574\n",
      "[Training Epoch 3] Batch 4270, Loss 0.28856733441352844\n",
      "[Training Epoch 3] Batch 4271, Loss 0.2681232988834381\n",
      "[Training Epoch 3] Batch 4272, Loss 0.28822556138038635\n",
      "[Training Epoch 3] Batch 4273, Loss 0.2726537883281708\n",
      "[Training Epoch 3] Batch 4274, Loss 0.2886383533477783\n",
      "[Training Epoch 3] Batch 4275, Loss 0.2839803099632263\n",
      "[Training Epoch 3] Batch 4276, Loss 0.25582820177078247\n",
      "[Training Epoch 3] Batch 4277, Loss 0.2657429575920105\n",
      "[Training Epoch 3] Batch 4278, Loss 0.2793312072753906\n",
      "[Training Epoch 3] Batch 4279, Loss 0.2552640438079834\n",
      "[Training Epoch 3] Batch 4280, Loss 0.2659211754798889\n",
      "[Training Epoch 3] Batch 4281, Loss 0.27693116664886475\n",
      "[Training Epoch 3] Batch 4282, Loss 0.2803000211715698\n",
      "[Training Epoch 3] Batch 4283, Loss 0.261993944644928\n",
      "[Training Epoch 3] Batch 4284, Loss 0.2856786847114563\n",
      "[Training Epoch 3] Batch 4285, Loss 0.2624541223049164\n",
      "[Training Epoch 3] Batch 4286, Loss 0.2875237464904785\n",
      "[Training Epoch 3] Batch 4287, Loss 0.3095582127571106\n",
      "[Training Epoch 3] Batch 4288, Loss 0.26972508430480957\n",
      "[Training Epoch 3] Batch 4289, Loss 0.2757887542247772\n",
      "[Training Epoch 3] Batch 4290, Loss 0.29316216707229614\n",
      "[Training Epoch 3] Batch 4291, Loss 0.2923462986946106\n",
      "[Training Epoch 3] Batch 4292, Loss 0.27950233221054077\n",
      "[Training Epoch 3] Batch 4293, Loss 0.28217536211013794\n",
      "[Training Epoch 3] Batch 4294, Loss 0.28361862897872925\n",
      "[Training Epoch 3] Batch 4295, Loss 0.2603093981742859\n",
      "[Training Epoch 3] Batch 4296, Loss 0.25472936034202576\n",
      "[Training Epoch 3] Batch 4297, Loss 0.27976536750793457\n",
      "[Training Epoch 3] Batch 4298, Loss 0.2379174828529358\n",
      "[Training Epoch 3] Batch 4299, Loss 0.28173863887786865\n",
      "[Training Epoch 3] Batch 4300, Loss 0.2876144051551819\n",
      "[Training Epoch 3] Batch 4301, Loss 0.2433728277683258\n",
      "[Training Epoch 3] Batch 4302, Loss 0.2705985903739929\n",
      "[Training Epoch 3] Batch 4303, Loss 0.25757312774658203\n",
      "[Training Epoch 3] Batch 4304, Loss 0.2535285949707031\n",
      "[Training Epoch 3] Batch 4305, Loss 0.2616268992424011\n",
      "[Training Epoch 3] Batch 4306, Loss 0.2612266540527344\n",
      "[Training Epoch 3] Batch 4307, Loss 0.24964329600334167\n",
      "[Training Epoch 3] Batch 4308, Loss 0.30628281831741333\n",
      "[Training Epoch 3] Batch 4309, Loss 0.2760258913040161\n",
      "[Training Epoch 3] Batch 4310, Loss 0.30247485637664795\n",
      "[Training Epoch 3] Batch 4311, Loss 0.26650017499923706\n",
      "[Training Epoch 3] Batch 4312, Loss 0.24101831018924713\n",
      "[Training Epoch 3] Batch 4313, Loss 0.23870639503002167\n",
      "[Training Epoch 3] Batch 4314, Loss 0.2665127217769623\n",
      "[Training Epoch 3] Batch 4315, Loss 0.28793126344680786\n",
      "[Training Epoch 3] Batch 4316, Loss 0.26274147629737854\n",
      "[Training Epoch 3] Batch 4317, Loss 0.2785939574241638\n",
      "[Training Epoch 3] Batch 4318, Loss 0.23832334578037262\n",
      "[Training Epoch 3] Batch 4319, Loss 0.26575082540512085\n",
      "[Training Epoch 3] Batch 4320, Loss 0.26441478729248047\n",
      "[Training Epoch 3] Batch 4321, Loss 0.27843737602233887\n",
      "[Training Epoch 3] Batch 4322, Loss 0.2759779095649719\n",
      "[Training Epoch 3] Batch 4323, Loss 0.28369376063346863\n",
      "[Training Epoch 3] Batch 4324, Loss 0.26770541071891785\n",
      "[Training Epoch 3] Batch 4325, Loss 0.29144155979156494\n",
      "[Training Epoch 3] Batch 4326, Loss 0.27184444665908813\n",
      "[Training Epoch 3] Batch 4327, Loss 0.2605476677417755\n",
      "[Training Epoch 3] Batch 4328, Loss 0.2853356897830963\n",
      "[Training Epoch 3] Batch 4329, Loss 0.2522743046283722\n",
      "[Training Epoch 3] Batch 4330, Loss 0.27012473344802856\n",
      "[Training Epoch 3] Batch 4331, Loss 0.2952848970890045\n",
      "[Training Epoch 3] Batch 4332, Loss 0.2666151225566864\n",
      "[Training Epoch 3] Batch 4333, Loss 0.276111364364624\n",
      "[Training Epoch 3] Batch 4334, Loss 0.28293633460998535\n",
      "[Training Epoch 3] Batch 4335, Loss 0.2549382150173187\n",
      "[Training Epoch 3] Batch 4336, Loss 0.2816923260688782\n",
      "[Training Epoch 3] Batch 4337, Loss 0.29264333844184875\n",
      "[Training Epoch 3] Batch 4338, Loss 0.23935502767562866\n",
      "[Training Epoch 3] Batch 4339, Loss 0.2592703402042389\n",
      "[Training Epoch 3] Batch 4340, Loss 0.27318713068962097\n",
      "[Training Epoch 3] Batch 4341, Loss 0.2770650386810303\n",
      "[Training Epoch 3] Batch 4342, Loss 0.25145310163497925\n",
      "[Training Epoch 3] Batch 4343, Loss 0.2736373841762543\n",
      "[Training Epoch 3] Batch 4344, Loss 0.29447174072265625\n",
      "[Training Epoch 3] Batch 4345, Loss 0.27834179997444153\n",
      "[Training Epoch 3] Batch 4346, Loss 0.2521967887878418\n",
      "[Training Epoch 3] Batch 4347, Loss 0.2839217782020569\n",
      "[Training Epoch 3] Batch 4348, Loss 0.2838117182254791\n",
      "[Training Epoch 3] Batch 4349, Loss 0.3028886318206787\n",
      "[Training Epoch 3] Batch 4350, Loss 0.2962212562561035\n",
      "[Training Epoch 3] Batch 4351, Loss 0.2643046975135803\n",
      "[Training Epoch 3] Batch 4352, Loss 0.2784407138824463\n",
      "[Training Epoch 3] Batch 4353, Loss 0.26412689685821533\n",
      "[Training Epoch 3] Batch 4354, Loss 0.2590150535106659\n",
      "[Training Epoch 3] Batch 4355, Loss 0.25136253237724304\n",
      "[Training Epoch 3] Batch 4356, Loss 0.2714093327522278\n",
      "[Training Epoch 3] Batch 4357, Loss 0.2904597818851471\n",
      "[Training Epoch 3] Batch 4358, Loss 0.290968656539917\n",
      "[Training Epoch 3] Batch 4359, Loss 0.2981721758842468\n",
      "[Training Epoch 3] Batch 4360, Loss 0.2545493245124817\n",
      "[Training Epoch 3] Batch 4361, Loss 0.2760508954524994\n",
      "[Training Epoch 3] Batch 4362, Loss 0.25944510102272034\n",
      "[Training Epoch 3] Batch 4363, Loss 0.2623951733112335\n",
      "[Training Epoch 3] Batch 4364, Loss 0.27288341522216797\n",
      "[Training Epoch 3] Batch 4365, Loss 0.24124427139759064\n",
      "[Training Epoch 3] Batch 4366, Loss 0.27275848388671875\n",
      "[Training Epoch 3] Batch 4367, Loss 0.2870263159275055\n",
      "[Training Epoch 3] Batch 4368, Loss 0.2875179946422577\n",
      "[Training Epoch 3] Batch 4369, Loss 0.27657589316368103\n",
      "[Training Epoch 3] Batch 4370, Loss 0.26002320647239685\n",
      "[Training Epoch 3] Batch 4371, Loss 0.23441453278064728\n",
      "[Training Epoch 3] Batch 4372, Loss 0.23526372015476227\n",
      "[Training Epoch 3] Batch 4373, Loss 0.2755599021911621\n",
      "[Training Epoch 3] Batch 4374, Loss 0.2379998117685318\n",
      "[Training Epoch 3] Batch 4375, Loss 0.26268696784973145\n",
      "[Training Epoch 3] Batch 4376, Loss 0.29983222484588623\n",
      "[Training Epoch 3] Batch 4377, Loss 0.23474276065826416\n",
      "[Training Epoch 3] Batch 4378, Loss 0.2695578336715698\n",
      "[Training Epoch 3] Batch 4379, Loss 0.25034475326538086\n",
      "[Training Epoch 3] Batch 4380, Loss 0.29157400131225586\n",
      "[Training Epoch 3] Batch 4381, Loss 0.26406288146972656\n",
      "[Training Epoch 3] Batch 4382, Loss 0.3194698691368103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2223.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 3] Precision = 0.2619, Recall = 0.7748\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.2556823492050171\n",
      "[Training Epoch 4] Batch 1, Loss 0.28345590829849243\n",
      "[Training Epoch 4] Batch 2, Loss 0.2937825620174408\n",
      "[Training Epoch 4] Batch 3, Loss 0.2649540305137634\n",
      "[Training Epoch 4] Batch 4, Loss 0.26128697395324707\n",
      "[Training Epoch 4] Batch 5, Loss 0.28206369280815125\n",
      "[Training Epoch 4] Batch 6, Loss 0.2702205181121826\n",
      "[Training Epoch 4] Batch 7, Loss 0.26273679733276367\n",
      "[Training Epoch 4] Batch 8, Loss 0.29760515689849854\n",
      "[Training Epoch 4] Batch 9, Loss 0.26975882053375244\n",
      "[Training Epoch 4] Batch 10, Loss 0.24201813340187073\n",
      "[Training Epoch 4] Batch 11, Loss 0.27197104692459106\n",
      "[Training Epoch 4] Batch 12, Loss 0.28219422698020935\n",
      "[Training Epoch 4] Batch 13, Loss 0.2484714686870575\n",
      "[Training Epoch 4] Batch 14, Loss 0.280720591545105\n",
      "[Training Epoch 4] Batch 15, Loss 0.24531835317611694\n",
      "[Training Epoch 4] Batch 16, Loss 0.26251861453056335\n",
      "[Training Epoch 4] Batch 17, Loss 0.2987615466117859\n",
      "[Training Epoch 4] Batch 18, Loss 0.263094961643219\n",
      "[Training Epoch 4] Batch 19, Loss 0.2503666281700134\n",
      "[Training Epoch 4] Batch 20, Loss 0.26336538791656494\n",
      "[Training Epoch 4] Batch 21, Loss 0.2888543903827667\n",
      "[Training Epoch 4] Batch 22, Loss 0.24419273436069489\n",
      "[Training Epoch 4] Batch 23, Loss 0.22721320390701294\n",
      "[Training Epoch 4] Batch 24, Loss 0.2682638168334961\n",
      "[Training Epoch 4] Batch 25, Loss 0.2659400999546051\n",
      "[Training Epoch 4] Batch 26, Loss 0.24879610538482666\n",
      "[Training Epoch 4] Batch 27, Loss 0.25563669204711914\n",
      "[Training Epoch 4] Batch 28, Loss 0.2743180990219116\n",
      "[Training Epoch 4] Batch 29, Loss 0.22979888319969177\n",
      "[Training Epoch 4] Batch 30, Loss 0.2750329375267029\n",
      "[Training Epoch 4] Batch 31, Loss 0.26081395149230957\n",
      "[Training Epoch 4] Batch 32, Loss 0.24954113364219666\n",
      "[Training Epoch 4] Batch 33, Loss 0.23983705043792725\n",
      "[Training Epoch 4] Batch 34, Loss 0.2383546531200409\n",
      "[Training Epoch 4] Batch 35, Loss 0.2780272960662842\n",
      "[Training Epoch 4] Batch 36, Loss 0.26118871569633484\n",
      "[Training Epoch 4] Batch 37, Loss 0.26288333535194397\n",
      "[Training Epoch 4] Batch 38, Loss 0.22529545426368713\n",
      "[Training Epoch 4] Batch 39, Loss 0.23135703802108765\n",
      "[Training Epoch 4] Batch 40, Loss 0.2825291156768799\n",
      "[Training Epoch 4] Batch 41, Loss 0.2392691820859909\n",
      "[Training Epoch 4] Batch 42, Loss 0.2673306465148926\n",
      "[Training Epoch 4] Batch 43, Loss 0.26518312096595764\n",
      "[Training Epoch 4] Batch 44, Loss 0.26165056228637695\n",
      "[Training Epoch 4] Batch 45, Loss 0.3045428395271301\n",
      "[Training Epoch 4] Batch 46, Loss 0.2704677879810333\n",
      "[Training Epoch 4] Batch 47, Loss 0.25880932807922363\n",
      "[Training Epoch 4] Batch 48, Loss 0.252732515335083\n",
      "[Training Epoch 4] Batch 49, Loss 0.27126163244247437\n",
      "[Training Epoch 4] Batch 50, Loss 0.23716962337493896\n",
      "[Training Epoch 4] Batch 51, Loss 0.2724347710609436\n",
      "[Training Epoch 4] Batch 52, Loss 0.26392823457717896\n",
      "[Training Epoch 4] Batch 53, Loss 0.24283148348331451\n",
      "[Training Epoch 4] Batch 54, Loss 0.25623074173927307\n",
      "[Training Epoch 4] Batch 55, Loss 0.27065354585647583\n",
      "[Training Epoch 4] Batch 56, Loss 0.26573827862739563\n",
      "[Training Epoch 4] Batch 57, Loss 0.2766169011592865\n",
      "[Training Epoch 4] Batch 58, Loss 0.27329349517822266\n",
      "[Training Epoch 4] Batch 59, Loss 0.2728230059146881\n",
      "[Training Epoch 4] Batch 60, Loss 0.23313619196414948\n",
      "[Training Epoch 4] Batch 61, Loss 0.3023086190223694\n",
      "[Training Epoch 4] Batch 62, Loss 0.25461116433143616\n",
      "[Training Epoch 4] Batch 63, Loss 0.23776233196258545\n",
      "[Training Epoch 4] Batch 64, Loss 0.2687910795211792\n",
      "[Training Epoch 4] Batch 65, Loss 0.27103376388549805\n",
      "[Training Epoch 4] Batch 66, Loss 0.2563035190105438\n",
      "[Training Epoch 4] Batch 67, Loss 0.277044415473938\n",
      "[Training Epoch 4] Batch 68, Loss 0.2539670467376709\n",
      "[Training Epoch 4] Batch 69, Loss 0.27084019780158997\n",
      "[Training Epoch 4] Batch 70, Loss 0.2533947825431824\n",
      "[Training Epoch 4] Batch 71, Loss 0.26273223757743835\n",
      "[Training Epoch 4] Batch 72, Loss 0.26041990518569946\n",
      "[Training Epoch 4] Batch 73, Loss 0.2877086400985718\n",
      "[Training Epoch 4] Batch 74, Loss 0.27788490056991577\n",
      "[Training Epoch 4] Batch 75, Loss 0.2710261344909668\n",
      "[Training Epoch 4] Batch 76, Loss 0.2685917019844055\n",
      "[Training Epoch 4] Batch 77, Loss 0.25298255681991577\n",
      "[Training Epoch 4] Batch 78, Loss 0.26573458313941956\n",
      "[Training Epoch 4] Batch 79, Loss 0.24744106829166412\n",
      "[Training Epoch 4] Batch 80, Loss 0.25615108013153076\n",
      "[Training Epoch 4] Batch 81, Loss 0.2837403416633606\n",
      "[Training Epoch 4] Batch 82, Loss 0.27277565002441406\n",
      "[Training Epoch 4] Batch 83, Loss 0.281686007976532\n",
      "[Training Epoch 4] Batch 84, Loss 0.2543790340423584\n",
      "[Training Epoch 4] Batch 85, Loss 0.2590920329093933\n",
      "[Training Epoch 4] Batch 86, Loss 0.24192577600479126\n",
      "[Training Epoch 4] Batch 87, Loss 0.2854074239730835\n",
      "[Training Epoch 4] Batch 88, Loss 0.24098964035511017\n",
      "[Training Epoch 4] Batch 89, Loss 0.2705878019332886\n",
      "[Training Epoch 4] Batch 90, Loss 0.2774110436439514\n",
      "[Training Epoch 4] Batch 91, Loss 0.28183043003082275\n",
      "[Training Epoch 4] Batch 92, Loss 0.24796977639198303\n",
      "[Training Epoch 4] Batch 93, Loss 0.26950740814208984\n",
      "[Training Epoch 4] Batch 94, Loss 0.2666633725166321\n",
      "[Training Epoch 4] Batch 95, Loss 0.258890300989151\n",
      "[Training Epoch 4] Batch 96, Loss 0.2608049809932709\n",
      "[Training Epoch 4] Batch 97, Loss 0.26018962264060974\n",
      "[Training Epoch 4] Batch 98, Loss 0.2529784142971039\n",
      "[Training Epoch 4] Batch 99, Loss 0.2642749547958374\n",
      "[Training Epoch 4] Batch 100, Loss 0.2722104787826538\n",
      "[Training Epoch 4] Batch 101, Loss 0.25313955545425415\n",
      "[Training Epoch 4] Batch 102, Loss 0.24630138278007507\n",
      "[Training Epoch 4] Batch 103, Loss 0.25560837984085083\n",
      "[Training Epoch 4] Batch 104, Loss 0.2458958625793457\n",
      "[Training Epoch 4] Batch 105, Loss 0.26919662952423096\n",
      "[Training Epoch 4] Batch 106, Loss 0.25980085134506226\n",
      "[Training Epoch 4] Batch 107, Loss 0.25528499484062195\n",
      "[Training Epoch 4] Batch 108, Loss 0.27934014797210693\n",
      "[Training Epoch 4] Batch 109, Loss 0.2646285593509674\n",
      "[Training Epoch 4] Batch 110, Loss 0.24167871475219727\n",
      "[Training Epoch 4] Batch 111, Loss 0.26031744480133057\n",
      "[Training Epoch 4] Batch 112, Loss 0.27667856216430664\n",
      "[Training Epoch 4] Batch 113, Loss 0.23019519448280334\n",
      "[Training Epoch 4] Batch 114, Loss 0.2755858302116394\n",
      "[Training Epoch 4] Batch 115, Loss 0.289711058139801\n",
      "[Training Epoch 4] Batch 116, Loss 0.27233558893203735\n",
      "[Training Epoch 4] Batch 117, Loss 0.25560420751571655\n",
      "[Training Epoch 4] Batch 118, Loss 0.251690149307251\n",
      "[Training Epoch 4] Batch 119, Loss 0.27555739879608154\n",
      "[Training Epoch 4] Batch 120, Loss 0.28180816769599915\n",
      "[Training Epoch 4] Batch 121, Loss 0.24992066621780396\n",
      "[Training Epoch 4] Batch 122, Loss 0.25052541494369507\n",
      "[Training Epoch 4] Batch 123, Loss 0.2519332766532898\n",
      "[Training Epoch 4] Batch 124, Loss 0.2674550712108612\n",
      "[Training Epoch 4] Batch 125, Loss 0.28241831064224243\n",
      "[Training Epoch 4] Batch 126, Loss 0.25269603729248047\n",
      "[Training Epoch 4] Batch 127, Loss 0.25040608644485474\n",
      "[Training Epoch 4] Batch 128, Loss 0.28320741653442383\n",
      "[Training Epoch 4] Batch 129, Loss 0.2542768120765686\n",
      "[Training Epoch 4] Batch 130, Loss 0.2765888571739197\n",
      "[Training Epoch 4] Batch 131, Loss 0.27998802065849304\n",
      "[Training Epoch 4] Batch 132, Loss 0.25090959668159485\n",
      "[Training Epoch 4] Batch 133, Loss 0.24605736136436462\n",
      "[Training Epoch 4] Batch 134, Loss 0.24889227747917175\n",
      "[Training Epoch 4] Batch 135, Loss 0.275995671749115\n",
      "[Training Epoch 4] Batch 136, Loss 0.27504754066467285\n",
      "[Training Epoch 4] Batch 137, Loss 0.2927665710449219\n",
      "[Training Epoch 4] Batch 138, Loss 0.2715117931365967\n",
      "[Training Epoch 4] Batch 139, Loss 0.2701265215873718\n",
      "[Training Epoch 4] Batch 140, Loss 0.24839523434638977\n",
      "[Training Epoch 4] Batch 141, Loss 0.27803733944892883\n",
      "[Training Epoch 4] Batch 142, Loss 0.2501612901687622\n",
      "[Training Epoch 4] Batch 143, Loss 0.26206135749816895\n",
      "[Training Epoch 4] Batch 144, Loss 0.273421972990036\n",
      "[Training Epoch 4] Batch 145, Loss 0.24263085424900055\n",
      "[Training Epoch 4] Batch 146, Loss 0.2420637458562851\n",
      "[Training Epoch 4] Batch 147, Loss 0.25546419620513916\n",
      "[Training Epoch 4] Batch 148, Loss 0.27167975902557373\n",
      "[Training Epoch 4] Batch 149, Loss 0.22425255179405212\n",
      "[Training Epoch 4] Batch 150, Loss 0.25442346930503845\n",
      "[Training Epoch 4] Batch 151, Loss 0.26947012543678284\n",
      "[Training Epoch 4] Batch 152, Loss 0.24724704027175903\n",
      "[Training Epoch 4] Batch 153, Loss 0.27841639518737793\n",
      "[Training Epoch 4] Batch 154, Loss 0.24831685423851013\n",
      "[Training Epoch 4] Batch 155, Loss 0.25771117210388184\n",
      "[Training Epoch 4] Batch 156, Loss 0.2575918734073639\n",
      "[Training Epoch 4] Batch 157, Loss 0.27134162187576294\n",
      "[Training Epoch 4] Batch 158, Loss 0.2623984217643738\n",
      "[Training Epoch 4] Batch 159, Loss 0.2619452476501465\n",
      "[Training Epoch 4] Batch 160, Loss 0.2678273320198059\n",
      "[Training Epoch 4] Batch 161, Loss 0.24480104446411133\n",
      "[Training Epoch 4] Batch 162, Loss 0.2798021137714386\n",
      "[Training Epoch 4] Batch 163, Loss 0.249004065990448\n",
      "[Training Epoch 4] Batch 164, Loss 0.25720691680908203\n",
      "[Training Epoch 4] Batch 165, Loss 0.29533544182777405\n",
      "[Training Epoch 4] Batch 166, Loss 0.2829877436161041\n",
      "[Training Epoch 4] Batch 167, Loss 0.28479862213134766\n",
      "[Training Epoch 4] Batch 168, Loss 0.28965920209884644\n",
      "[Training Epoch 4] Batch 169, Loss 0.2573772072792053\n",
      "[Training Epoch 4] Batch 170, Loss 0.2633988857269287\n",
      "[Training Epoch 4] Batch 171, Loss 0.25539302825927734\n",
      "[Training Epoch 4] Batch 172, Loss 0.2499837577342987\n",
      "[Training Epoch 4] Batch 173, Loss 0.26219940185546875\n",
      "[Training Epoch 4] Batch 174, Loss 0.2659352421760559\n",
      "[Training Epoch 4] Batch 175, Loss 0.2665928602218628\n",
      "[Training Epoch 4] Batch 176, Loss 0.25897860527038574\n",
      "[Training Epoch 4] Batch 177, Loss 0.25200337171554565\n",
      "[Training Epoch 4] Batch 178, Loss 0.27205345034599304\n",
      "[Training Epoch 4] Batch 179, Loss 0.27145346999168396\n",
      "[Training Epoch 4] Batch 180, Loss 0.24805589020252228\n",
      "[Training Epoch 4] Batch 181, Loss 0.2713756561279297\n",
      "[Training Epoch 4] Batch 182, Loss 0.28676706552505493\n",
      "[Training Epoch 4] Batch 183, Loss 0.27911704778671265\n",
      "[Training Epoch 4] Batch 184, Loss 0.27499061822891235\n",
      "[Training Epoch 4] Batch 185, Loss 0.2650597095489502\n",
      "[Training Epoch 4] Batch 186, Loss 0.2255754917860031\n",
      "[Training Epoch 4] Batch 187, Loss 0.2815670967102051\n",
      "[Training Epoch 4] Batch 188, Loss 0.2729978561401367\n",
      "[Training Epoch 4] Batch 189, Loss 0.28345757722854614\n",
      "[Training Epoch 4] Batch 190, Loss 0.2684382200241089\n",
      "[Training Epoch 4] Batch 191, Loss 0.25890403985977173\n",
      "[Training Epoch 4] Batch 192, Loss 0.2472788393497467\n",
      "[Training Epoch 4] Batch 193, Loss 0.24552379548549652\n",
      "[Training Epoch 4] Batch 194, Loss 0.2751913070678711\n",
      "[Training Epoch 4] Batch 195, Loss 0.2634550929069519\n",
      "[Training Epoch 4] Batch 196, Loss 0.237659752368927\n",
      "[Training Epoch 4] Batch 197, Loss 0.2532961964607239\n",
      "[Training Epoch 4] Batch 198, Loss 0.26778313517570496\n",
      "[Training Epoch 4] Batch 199, Loss 0.28430432081222534\n",
      "[Training Epoch 4] Batch 200, Loss 0.26116785407066345\n",
      "[Training Epoch 4] Batch 201, Loss 0.24261896312236786\n",
      "[Training Epoch 4] Batch 202, Loss 0.2455175817012787\n",
      "[Training Epoch 4] Batch 203, Loss 0.23418696224689484\n",
      "[Training Epoch 4] Batch 204, Loss 0.2488509714603424\n",
      "[Training Epoch 4] Batch 205, Loss 0.24689188599586487\n",
      "[Training Epoch 4] Batch 206, Loss 0.24292919039726257\n",
      "[Training Epoch 4] Batch 207, Loss 0.26903828978538513\n",
      "[Training Epoch 4] Batch 208, Loss 0.3006315529346466\n",
      "[Training Epoch 4] Batch 209, Loss 0.2525070011615753\n",
      "[Training Epoch 4] Batch 210, Loss 0.25888293981552124\n",
      "[Training Epoch 4] Batch 211, Loss 0.25864630937576294\n",
      "[Training Epoch 4] Batch 212, Loss 0.29509833455085754\n",
      "[Training Epoch 4] Batch 213, Loss 0.23323789238929749\n",
      "[Training Epoch 4] Batch 214, Loss 0.308369904756546\n",
      "[Training Epoch 4] Batch 215, Loss 0.2686256766319275\n",
      "[Training Epoch 4] Batch 216, Loss 0.2515908479690552\n",
      "[Training Epoch 4] Batch 217, Loss 0.26072537899017334\n",
      "[Training Epoch 4] Batch 218, Loss 0.25751379132270813\n",
      "[Training Epoch 4] Batch 219, Loss 0.2381177842617035\n",
      "[Training Epoch 4] Batch 220, Loss 0.261761873960495\n",
      "[Training Epoch 4] Batch 221, Loss 0.27750617265701294\n",
      "[Training Epoch 4] Batch 222, Loss 0.28255611658096313\n",
      "[Training Epoch 4] Batch 223, Loss 0.25045761466026306\n",
      "[Training Epoch 4] Batch 224, Loss 0.2534651458263397\n",
      "[Training Epoch 4] Batch 225, Loss 0.25056880712509155\n",
      "[Training Epoch 4] Batch 226, Loss 0.2746291756629944\n",
      "[Training Epoch 4] Batch 227, Loss 0.2469087839126587\n",
      "[Training Epoch 4] Batch 228, Loss 0.2645401358604431\n",
      "[Training Epoch 4] Batch 229, Loss 0.27531567215919495\n",
      "[Training Epoch 4] Batch 230, Loss 0.2532835602760315\n",
      "[Training Epoch 4] Batch 231, Loss 0.25446856021881104\n",
      "[Training Epoch 4] Batch 232, Loss 0.2595813274383545\n",
      "[Training Epoch 4] Batch 233, Loss 0.28860151767730713\n",
      "[Training Epoch 4] Batch 234, Loss 0.2798311114311218\n",
      "[Training Epoch 4] Batch 235, Loss 0.2717500627040863\n",
      "[Training Epoch 4] Batch 236, Loss 0.2666516900062561\n",
      "[Training Epoch 4] Batch 237, Loss 0.2592521905899048\n",
      "[Training Epoch 4] Batch 238, Loss 0.26376843452453613\n",
      "[Training Epoch 4] Batch 239, Loss 0.2785659730434418\n",
      "[Training Epoch 4] Batch 240, Loss 0.2620759606361389\n",
      "[Training Epoch 4] Batch 241, Loss 0.2575452923774719\n",
      "[Training Epoch 4] Batch 242, Loss 0.2552410960197449\n",
      "[Training Epoch 4] Batch 243, Loss 0.27829551696777344\n",
      "[Training Epoch 4] Batch 244, Loss 0.27585017681121826\n",
      "[Training Epoch 4] Batch 245, Loss 0.25303715467453003\n",
      "[Training Epoch 4] Batch 246, Loss 0.26910874247550964\n",
      "[Training Epoch 4] Batch 247, Loss 0.28195351362228394\n",
      "[Training Epoch 4] Batch 248, Loss 0.24867548048496246\n",
      "[Training Epoch 4] Batch 249, Loss 0.2580116391181946\n",
      "[Training Epoch 4] Batch 250, Loss 0.2515047788619995\n",
      "[Training Epoch 4] Batch 251, Loss 0.2958097457885742\n",
      "[Training Epoch 4] Batch 252, Loss 0.2491745948791504\n",
      "[Training Epoch 4] Batch 253, Loss 0.2516166567802429\n",
      "[Training Epoch 4] Batch 254, Loss 0.2638019919395447\n",
      "[Training Epoch 4] Batch 255, Loss 0.2565666139125824\n",
      "[Training Epoch 4] Batch 256, Loss 0.2677985429763794\n",
      "[Training Epoch 4] Batch 257, Loss 0.28822970390319824\n",
      "[Training Epoch 4] Batch 258, Loss 0.25897276401519775\n",
      "[Training Epoch 4] Batch 259, Loss 0.2556072175502777\n",
      "[Training Epoch 4] Batch 260, Loss 0.26160645484924316\n",
      "[Training Epoch 4] Batch 261, Loss 0.25880545377731323\n",
      "[Training Epoch 4] Batch 262, Loss 0.27504366636276245\n",
      "[Training Epoch 4] Batch 263, Loss 0.25609514117240906\n",
      "[Training Epoch 4] Batch 264, Loss 0.28392741084098816\n",
      "[Training Epoch 4] Batch 265, Loss 0.27032095193862915\n",
      "[Training Epoch 4] Batch 266, Loss 0.28151294589042664\n",
      "[Training Epoch 4] Batch 267, Loss 0.2815493941307068\n",
      "[Training Epoch 4] Batch 268, Loss 0.24552974104881287\n",
      "[Training Epoch 4] Batch 269, Loss 0.27239763736724854\n",
      "[Training Epoch 4] Batch 270, Loss 0.24798040091991425\n",
      "[Training Epoch 4] Batch 271, Loss 0.2460554540157318\n",
      "[Training Epoch 4] Batch 272, Loss 0.2467648833990097\n",
      "[Training Epoch 4] Batch 273, Loss 0.2578434348106384\n",
      "[Training Epoch 4] Batch 274, Loss 0.2682584822177887\n",
      "[Training Epoch 4] Batch 275, Loss 0.2671685814857483\n",
      "[Training Epoch 4] Batch 276, Loss 0.2751487195491791\n",
      "[Training Epoch 4] Batch 277, Loss 0.2763252854347229\n",
      "[Training Epoch 4] Batch 278, Loss 0.24201849102973938\n",
      "[Training Epoch 4] Batch 279, Loss 0.2595118284225464\n",
      "[Training Epoch 4] Batch 280, Loss 0.2934677004814148\n",
      "[Training Epoch 4] Batch 281, Loss 0.2584391236305237\n",
      "[Training Epoch 4] Batch 282, Loss 0.23453345894813538\n",
      "[Training Epoch 4] Batch 283, Loss 0.2659059762954712\n",
      "[Training Epoch 4] Batch 284, Loss 0.2632286846637726\n",
      "[Training Epoch 4] Batch 285, Loss 0.2660315930843353\n",
      "[Training Epoch 4] Batch 286, Loss 0.26419806480407715\n",
      "[Training Epoch 4] Batch 287, Loss 0.23355582356452942\n",
      "[Training Epoch 4] Batch 288, Loss 0.2816691994667053\n",
      "[Training Epoch 4] Batch 289, Loss 0.24686193466186523\n",
      "[Training Epoch 4] Batch 290, Loss 0.2577837109565735\n",
      "[Training Epoch 4] Batch 291, Loss 0.25537434220314026\n",
      "[Training Epoch 4] Batch 292, Loss 0.2778704762458801\n",
      "[Training Epoch 4] Batch 293, Loss 0.2528434991836548\n",
      "[Training Epoch 4] Batch 294, Loss 0.24968543648719788\n",
      "[Training Epoch 4] Batch 295, Loss 0.25954771041870117\n",
      "[Training Epoch 4] Batch 296, Loss 0.24962946772575378\n",
      "[Training Epoch 4] Batch 297, Loss 0.2633481025695801\n",
      "[Training Epoch 4] Batch 298, Loss 0.2827273905277252\n",
      "[Training Epoch 4] Batch 299, Loss 0.2802101969718933\n",
      "[Training Epoch 4] Batch 300, Loss 0.2640646696090698\n",
      "[Training Epoch 4] Batch 301, Loss 0.25432300567626953\n",
      "[Training Epoch 4] Batch 302, Loss 0.291977196931839\n",
      "[Training Epoch 4] Batch 303, Loss 0.24190616607666016\n",
      "[Training Epoch 4] Batch 304, Loss 0.27328693866729736\n",
      "[Training Epoch 4] Batch 305, Loss 0.2957305610179901\n",
      "[Training Epoch 4] Batch 306, Loss 0.2925345003604889\n",
      "[Training Epoch 4] Batch 307, Loss 0.27226710319519043\n",
      "[Training Epoch 4] Batch 308, Loss 0.26001113653182983\n",
      "[Training Epoch 4] Batch 309, Loss 0.28273633122444153\n",
      "[Training Epoch 4] Batch 310, Loss 0.26930755376815796\n",
      "[Training Epoch 4] Batch 311, Loss 0.23956646025180817\n",
      "[Training Epoch 4] Batch 312, Loss 0.23404526710510254\n",
      "[Training Epoch 4] Batch 313, Loss 0.2485060691833496\n",
      "[Training Epoch 4] Batch 314, Loss 0.2470356523990631\n",
      "[Training Epoch 4] Batch 315, Loss 0.23606862127780914\n",
      "[Training Epoch 4] Batch 316, Loss 0.27013716101646423\n",
      "[Training Epoch 4] Batch 317, Loss 0.27690958976745605\n",
      "[Training Epoch 4] Batch 318, Loss 0.2486916482448578\n",
      "[Training Epoch 4] Batch 319, Loss 0.23955824971199036\n",
      "[Training Epoch 4] Batch 320, Loss 0.2707965075969696\n",
      "[Training Epoch 4] Batch 321, Loss 0.25007838010787964\n",
      "[Training Epoch 4] Batch 322, Loss 0.2640666365623474\n",
      "[Training Epoch 4] Batch 323, Loss 0.28849345445632935\n",
      "[Training Epoch 4] Batch 324, Loss 0.27074551582336426\n",
      "[Training Epoch 4] Batch 325, Loss 0.2553463578224182\n",
      "[Training Epoch 4] Batch 326, Loss 0.2913784384727478\n",
      "[Training Epoch 4] Batch 327, Loss 0.27472010254859924\n",
      "[Training Epoch 4] Batch 328, Loss 0.2686503529548645\n",
      "[Training Epoch 4] Batch 329, Loss 0.24754564464092255\n",
      "[Training Epoch 4] Batch 330, Loss 0.277025043964386\n",
      "[Training Epoch 4] Batch 331, Loss 0.2612999677658081\n",
      "[Training Epoch 4] Batch 332, Loss 0.26008743047714233\n",
      "[Training Epoch 4] Batch 333, Loss 0.2272106111049652\n",
      "[Training Epoch 4] Batch 334, Loss 0.26914218068122864\n",
      "[Training Epoch 4] Batch 335, Loss 0.2622362971305847\n",
      "[Training Epoch 4] Batch 336, Loss 0.2738950848579407\n",
      "[Training Epoch 4] Batch 337, Loss 0.2688855528831482\n",
      "[Training Epoch 4] Batch 338, Loss 0.25070858001708984\n",
      "[Training Epoch 4] Batch 339, Loss 0.2644054889678955\n",
      "[Training Epoch 4] Batch 340, Loss 0.23480239510536194\n",
      "[Training Epoch 4] Batch 341, Loss 0.236718088388443\n",
      "[Training Epoch 4] Batch 342, Loss 0.2607377767562866\n",
      "[Training Epoch 4] Batch 343, Loss 0.2859921157360077\n",
      "[Training Epoch 4] Batch 344, Loss 0.2420584261417389\n",
      "[Training Epoch 4] Batch 345, Loss 0.25736987590789795\n",
      "[Training Epoch 4] Batch 346, Loss 0.2698889672756195\n",
      "[Training Epoch 4] Batch 347, Loss 0.248272106051445\n",
      "[Training Epoch 4] Batch 348, Loss 0.2444518804550171\n",
      "[Training Epoch 4] Batch 349, Loss 0.26416391134262085\n",
      "[Training Epoch 4] Batch 350, Loss 0.28903836011886597\n",
      "[Training Epoch 4] Batch 351, Loss 0.27837854623794556\n",
      "[Training Epoch 4] Batch 352, Loss 0.2569737434387207\n",
      "[Training Epoch 4] Batch 353, Loss 0.2393088936805725\n",
      "[Training Epoch 4] Batch 354, Loss 0.2860473394393921\n",
      "[Training Epoch 4] Batch 355, Loss 0.26952582597732544\n",
      "[Training Epoch 4] Batch 356, Loss 0.2654815912246704\n",
      "[Training Epoch 4] Batch 357, Loss 0.2599485516548157\n",
      "[Training Epoch 4] Batch 358, Loss 0.25939443707466125\n",
      "[Training Epoch 4] Batch 359, Loss 0.26807093620300293\n",
      "[Training Epoch 4] Batch 360, Loss 0.2614070177078247\n",
      "[Training Epoch 4] Batch 361, Loss 0.24109646677970886\n",
      "[Training Epoch 4] Batch 362, Loss 0.26218509674072266\n",
      "[Training Epoch 4] Batch 363, Loss 0.24091953039169312\n",
      "[Training Epoch 4] Batch 364, Loss 0.26425519585609436\n",
      "[Training Epoch 4] Batch 365, Loss 0.29614701867103577\n",
      "[Training Epoch 4] Batch 366, Loss 0.24748331308364868\n",
      "[Training Epoch 4] Batch 367, Loss 0.2948685884475708\n",
      "[Training Epoch 4] Batch 368, Loss 0.24290139973163605\n",
      "[Training Epoch 4] Batch 369, Loss 0.26507461071014404\n",
      "[Training Epoch 4] Batch 370, Loss 0.26588326692581177\n",
      "[Training Epoch 4] Batch 371, Loss 0.24655486643314362\n",
      "[Training Epoch 4] Batch 372, Loss 0.23006553947925568\n",
      "[Training Epoch 4] Batch 373, Loss 0.25379860401153564\n",
      "[Training Epoch 4] Batch 374, Loss 0.2637186050415039\n",
      "[Training Epoch 4] Batch 375, Loss 0.2670316994190216\n",
      "[Training Epoch 4] Batch 376, Loss 0.2517544627189636\n",
      "[Training Epoch 4] Batch 377, Loss 0.2686682641506195\n",
      "[Training Epoch 4] Batch 378, Loss 0.260404109954834\n",
      "[Training Epoch 4] Batch 379, Loss 0.2531754970550537\n",
      "[Training Epoch 4] Batch 380, Loss 0.22037723660469055\n",
      "[Training Epoch 4] Batch 381, Loss 0.25829988718032837\n",
      "[Training Epoch 4] Batch 382, Loss 0.2702484428882599\n",
      "[Training Epoch 4] Batch 383, Loss 0.2628784775733948\n",
      "[Training Epoch 4] Batch 384, Loss 0.26222148537635803\n",
      "[Training Epoch 4] Batch 385, Loss 0.2412905991077423\n",
      "[Training Epoch 4] Batch 386, Loss 0.27750706672668457\n",
      "[Training Epoch 4] Batch 387, Loss 0.2578062117099762\n",
      "[Training Epoch 4] Batch 388, Loss 0.2747500538825989\n",
      "[Training Epoch 4] Batch 389, Loss 0.28342729806900024\n",
      "[Training Epoch 4] Batch 390, Loss 0.2702029347419739\n",
      "[Training Epoch 4] Batch 391, Loss 0.28095343708992004\n",
      "[Training Epoch 4] Batch 392, Loss 0.2785213589668274\n",
      "[Training Epoch 4] Batch 393, Loss 0.29886776208877563\n",
      "[Training Epoch 4] Batch 394, Loss 0.2595869302749634\n",
      "[Training Epoch 4] Batch 395, Loss 0.25689393281936646\n",
      "[Training Epoch 4] Batch 396, Loss 0.25240886211395264\n",
      "[Training Epoch 4] Batch 397, Loss 0.2545364499092102\n",
      "[Training Epoch 4] Batch 398, Loss 0.25318142771720886\n",
      "[Training Epoch 4] Batch 399, Loss 0.2381562888622284\n",
      "[Training Epoch 4] Batch 400, Loss 0.2938086986541748\n",
      "[Training Epoch 4] Batch 401, Loss 0.2703067660331726\n",
      "[Training Epoch 4] Batch 402, Loss 0.2608402967453003\n",
      "[Training Epoch 4] Batch 403, Loss 0.2697935998439789\n",
      "[Training Epoch 4] Batch 404, Loss 0.26758602261543274\n",
      "[Training Epoch 4] Batch 405, Loss 0.2524288594722748\n",
      "[Training Epoch 4] Batch 406, Loss 0.29452335834503174\n",
      "[Training Epoch 4] Batch 407, Loss 0.27324149012565613\n",
      "[Training Epoch 4] Batch 408, Loss 0.24234098196029663\n",
      "[Training Epoch 4] Batch 409, Loss 0.2454289048910141\n",
      "[Training Epoch 4] Batch 410, Loss 0.2682672142982483\n",
      "[Training Epoch 4] Batch 411, Loss 0.2853456139564514\n",
      "[Training Epoch 4] Batch 412, Loss 0.25919121503829956\n",
      "[Training Epoch 4] Batch 413, Loss 0.2624782919883728\n",
      "[Training Epoch 4] Batch 414, Loss 0.26629704236984253\n",
      "[Training Epoch 4] Batch 415, Loss 0.2669944167137146\n",
      "[Training Epoch 4] Batch 416, Loss 0.23409993946552277\n",
      "[Training Epoch 4] Batch 417, Loss 0.2707329988479614\n",
      "[Training Epoch 4] Batch 418, Loss 0.26120930910110474\n",
      "[Training Epoch 4] Batch 419, Loss 0.2629280090332031\n",
      "[Training Epoch 4] Batch 420, Loss 0.263233482837677\n",
      "[Training Epoch 4] Batch 421, Loss 0.2510673403739929\n",
      "[Training Epoch 4] Batch 422, Loss 0.2792336940765381\n",
      "[Training Epoch 4] Batch 423, Loss 0.2609705328941345\n",
      "[Training Epoch 4] Batch 424, Loss 0.26485422253608704\n",
      "[Training Epoch 4] Batch 425, Loss 0.25299540162086487\n",
      "[Training Epoch 4] Batch 426, Loss 0.26196664571762085\n",
      "[Training Epoch 4] Batch 427, Loss 0.28000861406326294\n",
      "[Training Epoch 4] Batch 428, Loss 0.2830067574977875\n",
      "[Training Epoch 4] Batch 429, Loss 0.2918037176132202\n",
      "[Training Epoch 4] Batch 430, Loss 0.30829593539237976\n",
      "[Training Epoch 4] Batch 431, Loss 0.253328800201416\n",
      "[Training Epoch 4] Batch 432, Loss 0.2878987491130829\n",
      "[Training Epoch 4] Batch 433, Loss 0.27616333961486816\n",
      "[Training Epoch 4] Batch 434, Loss 0.26352551579475403\n",
      "[Training Epoch 4] Batch 435, Loss 0.27997899055480957\n",
      "[Training Epoch 4] Batch 436, Loss 0.30494630336761475\n",
      "[Training Epoch 4] Batch 437, Loss 0.27555400133132935\n",
      "[Training Epoch 4] Batch 438, Loss 0.26519984006881714\n",
      "[Training Epoch 4] Batch 439, Loss 0.27142763137817383\n",
      "[Training Epoch 4] Batch 440, Loss 0.26102641224861145\n",
      "[Training Epoch 4] Batch 441, Loss 0.24126029014587402\n",
      "[Training Epoch 4] Batch 442, Loss 0.25576815009117126\n",
      "[Training Epoch 4] Batch 443, Loss 0.28243130445480347\n",
      "[Training Epoch 4] Batch 444, Loss 0.2637108266353607\n",
      "[Training Epoch 4] Batch 445, Loss 0.24632740020751953\n",
      "[Training Epoch 4] Batch 446, Loss 0.26060885190963745\n",
      "[Training Epoch 4] Batch 447, Loss 0.26001137495040894\n",
      "[Training Epoch 4] Batch 448, Loss 0.2746835947036743\n",
      "[Training Epoch 4] Batch 449, Loss 0.2572256922721863\n",
      "[Training Epoch 4] Batch 450, Loss 0.2622889280319214\n",
      "[Training Epoch 4] Batch 451, Loss 0.2784527540206909\n",
      "[Training Epoch 4] Batch 452, Loss 0.24561166763305664\n",
      "[Training Epoch 4] Batch 453, Loss 0.2550697922706604\n",
      "[Training Epoch 4] Batch 454, Loss 0.29209020733833313\n",
      "[Training Epoch 4] Batch 455, Loss 0.2852024734020233\n",
      "[Training Epoch 4] Batch 456, Loss 0.2557579576969147\n",
      "[Training Epoch 4] Batch 457, Loss 0.2825479507446289\n",
      "[Training Epoch 4] Batch 458, Loss 0.2484062910079956\n",
      "[Training Epoch 4] Batch 459, Loss 0.2794049382209778\n",
      "[Training Epoch 4] Batch 460, Loss 0.2598533630371094\n",
      "[Training Epoch 4] Batch 461, Loss 0.2640325427055359\n",
      "[Training Epoch 4] Batch 462, Loss 0.26359742879867554\n",
      "[Training Epoch 4] Batch 463, Loss 0.25972428917884827\n",
      "[Training Epoch 4] Batch 464, Loss 0.24015606939792633\n",
      "[Training Epoch 4] Batch 465, Loss 0.26371172070503235\n",
      "[Training Epoch 4] Batch 466, Loss 0.25053584575653076\n",
      "[Training Epoch 4] Batch 467, Loss 0.27213093638420105\n",
      "[Training Epoch 4] Batch 468, Loss 0.2523748278617859\n",
      "[Training Epoch 4] Batch 469, Loss 0.2557976543903351\n",
      "[Training Epoch 4] Batch 470, Loss 0.29522478580474854\n",
      "[Training Epoch 4] Batch 471, Loss 0.2534055709838867\n",
      "[Training Epoch 4] Batch 472, Loss 0.2429019957780838\n",
      "[Training Epoch 4] Batch 473, Loss 0.27820834517478943\n",
      "[Training Epoch 4] Batch 474, Loss 0.26331377029418945\n",
      "[Training Epoch 4] Batch 475, Loss 0.27985066175460815\n",
      "[Training Epoch 4] Batch 476, Loss 0.2653766870498657\n",
      "[Training Epoch 4] Batch 477, Loss 0.25887206196784973\n",
      "[Training Epoch 4] Batch 478, Loss 0.24352099001407623\n",
      "[Training Epoch 4] Batch 479, Loss 0.25653740763664246\n",
      "[Training Epoch 4] Batch 480, Loss 0.26498448848724365\n",
      "[Training Epoch 4] Batch 481, Loss 0.2561170756816864\n",
      "[Training Epoch 4] Batch 482, Loss 0.28149327635765076\n",
      "[Training Epoch 4] Batch 483, Loss 0.24869783222675323\n",
      "[Training Epoch 4] Batch 484, Loss 0.26038533449172974\n",
      "[Training Epoch 4] Batch 485, Loss 0.2567000389099121\n",
      "[Training Epoch 4] Batch 486, Loss 0.25023412704467773\n",
      "[Training Epoch 4] Batch 487, Loss 0.2763558626174927\n",
      "[Training Epoch 4] Batch 488, Loss 0.2837013304233551\n",
      "[Training Epoch 4] Batch 489, Loss 0.3007095754146576\n",
      "[Training Epoch 4] Batch 490, Loss 0.28033751249313354\n",
      "[Training Epoch 4] Batch 491, Loss 0.2582648992538452\n",
      "[Training Epoch 4] Batch 492, Loss 0.26834315061569214\n",
      "[Training Epoch 4] Batch 493, Loss 0.2537868320941925\n",
      "[Training Epoch 4] Batch 494, Loss 0.27190667390823364\n",
      "[Training Epoch 4] Batch 495, Loss 0.24447283148765564\n",
      "[Training Epoch 4] Batch 496, Loss 0.28613367676734924\n",
      "[Training Epoch 4] Batch 497, Loss 0.2788484990596771\n",
      "[Training Epoch 4] Batch 498, Loss 0.2657928764820099\n",
      "[Training Epoch 4] Batch 499, Loss 0.303208589553833\n",
      "[Training Epoch 4] Batch 500, Loss 0.26528486609458923\n",
      "[Training Epoch 4] Batch 501, Loss 0.24859848618507385\n",
      "[Training Epoch 4] Batch 502, Loss 0.2303110659122467\n",
      "[Training Epoch 4] Batch 503, Loss 0.2716788649559021\n",
      "[Training Epoch 4] Batch 504, Loss 0.3081766366958618\n",
      "[Training Epoch 4] Batch 505, Loss 0.2696409225463867\n",
      "[Training Epoch 4] Batch 506, Loss 0.271689236164093\n",
      "[Training Epoch 4] Batch 507, Loss 0.29241907596588135\n",
      "[Training Epoch 4] Batch 508, Loss 0.24246014654636383\n",
      "[Training Epoch 4] Batch 509, Loss 0.28942471742630005\n",
      "[Training Epoch 4] Batch 510, Loss 0.26480114459991455\n",
      "[Training Epoch 4] Batch 511, Loss 0.27276355028152466\n",
      "[Training Epoch 4] Batch 512, Loss 0.2533642649650574\n",
      "[Training Epoch 4] Batch 513, Loss 0.26124733686447144\n",
      "[Training Epoch 4] Batch 514, Loss 0.25971949100494385\n",
      "[Training Epoch 4] Batch 515, Loss 0.244140625\n",
      "[Training Epoch 4] Batch 516, Loss 0.27624085545539856\n",
      "[Training Epoch 4] Batch 517, Loss 0.30123311281204224\n",
      "[Training Epoch 4] Batch 518, Loss 0.26810312271118164\n",
      "[Training Epoch 4] Batch 519, Loss 0.272738516330719\n",
      "[Training Epoch 4] Batch 520, Loss 0.2678559422492981\n",
      "[Training Epoch 4] Batch 521, Loss 0.25794368982315063\n",
      "[Training Epoch 4] Batch 522, Loss 0.23633435368537903\n",
      "[Training Epoch 4] Batch 523, Loss 0.25741612911224365\n",
      "[Training Epoch 4] Batch 524, Loss 0.2791874408721924\n",
      "[Training Epoch 4] Batch 525, Loss 0.27048540115356445\n",
      "[Training Epoch 4] Batch 526, Loss 0.24476368725299835\n",
      "[Training Epoch 4] Batch 527, Loss 0.25169479846954346\n",
      "[Training Epoch 4] Batch 528, Loss 0.2706156075000763\n",
      "[Training Epoch 4] Batch 529, Loss 0.2677931785583496\n",
      "[Training Epoch 4] Batch 530, Loss 0.26746928691864014\n",
      "[Training Epoch 4] Batch 531, Loss 0.2709392011165619\n",
      "[Training Epoch 4] Batch 532, Loss 0.2695266604423523\n",
      "[Training Epoch 4] Batch 533, Loss 0.27423784136772156\n",
      "[Training Epoch 4] Batch 534, Loss 0.2575225234031677\n",
      "[Training Epoch 4] Batch 535, Loss 0.2849613428115845\n",
      "[Training Epoch 4] Batch 536, Loss 0.26748496294021606\n",
      "[Training Epoch 4] Batch 537, Loss 0.2933158278465271\n",
      "[Training Epoch 4] Batch 538, Loss 0.24855352938175201\n",
      "[Training Epoch 4] Batch 539, Loss 0.2547880709171295\n",
      "[Training Epoch 4] Batch 540, Loss 0.23910093307495117\n",
      "[Training Epoch 4] Batch 541, Loss 0.24511538445949554\n",
      "[Training Epoch 4] Batch 542, Loss 0.2776515781879425\n",
      "[Training Epoch 4] Batch 543, Loss 0.26997053623199463\n",
      "[Training Epoch 4] Batch 544, Loss 0.21964097023010254\n",
      "[Training Epoch 4] Batch 545, Loss 0.2766304910182953\n",
      "[Training Epoch 4] Batch 546, Loss 0.2617254853248596\n",
      "[Training Epoch 4] Batch 547, Loss 0.2691799998283386\n",
      "[Training Epoch 4] Batch 548, Loss 0.23450346291065216\n",
      "[Training Epoch 4] Batch 549, Loss 0.23661834001541138\n",
      "[Training Epoch 4] Batch 550, Loss 0.24842259287834167\n",
      "[Training Epoch 4] Batch 551, Loss 0.28354984521865845\n",
      "[Training Epoch 4] Batch 552, Loss 0.2641254663467407\n",
      "[Training Epoch 4] Batch 553, Loss 0.24838823080062866\n",
      "[Training Epoch 4] Batch 554, Loss 0.2766079902648926\n",
      "[Training Epoch 4] Batch 555, Loss 0.28219395875930786\n",
      "[Training Epoch 4] Batch 556, Loss 0.26704373955726624\n",
      "[Training Epoch 4] Batch 557, Loss 0.26773297786712646\n",
      "[Training Epoch 4] Batch 558, Loss 0.2515222430229187\n",
      "[Training Epoch 4] Batch 559, Loss 0.2724308669567108\n",
      "[Training Epoch 4] Batch 560, Loss 0.27651265263557434\n",
      "[Training Epoch 4] Batch 561, Loss 0.2682728171348572\n",
      "[Training Epoch 4] Batch 562, Loss 0.2638189494609833\n",
      "[Training Epoch 4] Batch 563, Loss 0.24912004172801971\n",
      "[Training Epoch 4] Batch 564, Loss 0.2564984858036041\n",
      "[Training Epoch 4] Batch 565, Loss 0.2539011836051941\n",
      "[Training Epoch 4] Batch 566, Loss 0.2566159963607788\n",
      "[Training Epoch 4] Batch 567, Loss 0.27104905247688293\n",
      "[Training Epoch 4] Batch 568, Loss 0.2466876357793808\n",
      "[Training Epoch 4] Batch 569, Loss 0.2960575819015503\n",
      "[Training Epoch 4] Batch 570, Loss 0.2825925350189209\n",
      "[Training Epoch 4] Batch 571, Loss 0.24375277757644653\n",
      "[Training Epoch 4] Batch 572, Loss 0.2764620780944824\n",
      "[Training Epoch 4] Batch 573, Loss 0.2535073459148407\n",
      "[Training Epoch 4] Batch 574, Loss 0.2953515648841858\n",
      "[Training Epoch 4] Batch 575, Loss 0.3048965036869049\n",
      "[Training Epoch 4] Batch 576, Loss 0.26926517486572266\n",
      "[Training Epoch 4] Batch 577, Loss 0.27250051498413086\n",
      "[Training Epoch 4] Batch 578, Loss 0.2869172692298889\n",
      "[Training Epoch 4] Batch 579, Loss 0.24084807932376862\n",
      "[Training Epoch 4] Batch 580, Loss 0.2620614469051361\n",
      "[Training Epoch 4] Batch 581, Loss 0.2490219622850418\n",
      "[Training Epoch 4] Batch 582, Loss 0.2768976390361786\n",
      "[Training Epoch 4] Batch 583, Loss 0.23652119934558868\n",
      "[Training Epoch 4] Batch 584, Loss 0.25850826501846313\n",
      "[Training Epoch 4] Batch 585, Loss 0.2755153179168701\n",
      "[Training Epoch 4] Batch 586, Loss 0.27300748229026794\n",
      "[Training Epoch 4] Batch 587, Loss 0.26998013257980347\n",
      "[Training Epoch 4] Batch 588, Loss 0.23651155829429626\n",
      "[Training Epoch 4] Batch 589, Loss 0.23712894320487976\n",
      "[Training Epoch 4] Batch 590, Loss 0.2657594084739685\n",
      "[Training Epoch 4] Batch 591, Loss 0.24282294511795044\n",
      "[Training Epoch 4] Batch 592, Loss 0.27497681975364685\n",
      "[Training Epoch 4] Batch 593, Loss 0.2466774731874466\n",
      "[Training Epoch 4] Batch 594, Loss 0.23548170924186707\n",
      "[Training Epoch 4] Batch 595, Loss 0.26649412512779236\n",
      "[Training Epoch 4] Batch 596, Loss 0.2837596535682678\n",
      "[Training Epoch 4] Batch 597, Loss 0.2903445363044739\n",
      "[Training Epoch 4] Batch 598, Loss 0.2722131609916687\n",
      "[Training Epoch 4] Batch 599, Loss 0.2595454454421997\n",
      "[Training Epoch 4] Batch 600, Loss 0.2805376946926117\n",
      "[Training Epoch 4] Batch 601, Loss 0.28436410427093506\n",
      "[Training Epoch 4] Batch 602, Loss 0.22390210628509521\n",
      "[Training Epoch 4] Batch 603, Loss 0.2879026532173157\n",
      "[Training Epoch 4] Batch 604, Loss 0.24840347468852997\n",
      "[Training Epoch 4] Batch 605, Loss 0.24854639172554016\n",
      "[Training Epoch 4] Batch 606, Loss 0.23100914061069489\n",
      "[Training Epoch 4] Batch 607, Loss 0.287962943315506\n",
      "[Training Epoch 4] Batch 608, Loss 0.26977887749671936\n",
      "[Training Epoch 4] Batch 609, Loss 0.26172691583633423\n",
      "[Training Epoch 4] Batch 610, Loss 0.24344953894615173\n",
      "[Training Epoch 4] Batch 611, Loss 0.27982428669929504\n",
      "[Training Epoch 4] Batch 612, Loss 0.28179264068603516\n",
      "[Training Epoch 4] Batch 613, Loss 0.27349984645843506\n",
      "[Training Epoch 4] Batch 614, Loss 0.31566211581230164\n",
      "[Training Epoch 4] Batch 615, Loss 0.254999041557312\n",
      "[Training Epoch 4] Batch 616, Loss 0.2482285499572754\n",
      "[Training Epoch 4] Batch 617, Loss 0.240743026137352\n",
      "[Training Epoch 4] Batch 618, Loss 0.28871089220046997\n",
      "[Training Epoch 4] Batch 619, Loss 0.28058314323425293\n",
      "[Training Epoch 4] Batch 620, Loss 0.28697890043258667\n",
      "[Training Epoch 4] Batch 621, Loss 0.2645438015460968\n",
      "[Training Epoch 4] Batch 622, Loss 0.33559346199035645\n",
      "[Training Epoch 4] Batch 623, Loss 0.27247294783592224\n",
      "[Training Epoch 4] Batch 624, Loss 0.30000948905944824\n",
      "[Training Epoch 4] Batch 625, Loss 0.26591840386390686\n",
      "[Training Epoch 4] Batch 626, Loss 0.24936726689338684\n",
      "[Training Epoch 4] Batch 627, Loss 0.2587210536003113\n",
      "[Training Epoch 4] Batch 628, Loss 0.28987741470336914\n",
      "[Training Epoch 4] Batch 629, Loss 0.26704666018486023\n",
      "[Training Epoch 4] Batch 630, Loss 0.28239530324935913\n",
      "[Training Epoch 4] Batch 631, Loss 0.2554819881916046\n",
      "[Training Epoch 4] Batch 632, Loss 0.26492178440093994\n",
      "[Training Epoch 4] Batch 633, Loss 0.2721652686595917\n",
      "[Training Epoch 4] Batch 634, Loss 0.2333337664604187\n",
      "[Training Epoch 4] Batch 635, Loss 0.27274346351623535\n",
      "[Training Epoch 4] Batch 636, Loss 0.2775792181491852\n",
      "[Training Epoch 4] Batch 637, Loss 0.2517385482788086\n",
      "[Training Epoch 4] Batch 638, Loss 0.24878427386283875\n",
      "[Training Epoch 4] Batch 639, Loss 0.2597615122795105\n",
      "[Training Epoch 4] Batch 640, Loss 0.29003819823265076\n",
      "[Training Epoch 4] Batch 641, Loss 0.26160311698913574\n",
      "[Training Epoch 4] Batch 642, Loss 0.2744210660457611\n",
      "[Training Epoch 4] Batch 643, Loss 0.28597891330718994\n",
      "[Training Epoch 4] Batch 644, Loss 0.2588908076286316\n",
      "[Training Epoch 4] Batch 645, Loss 0.2546235918998718\n",
      "[Training Epoch 4] Batch 646, Loss 0.27385568618774414\n",
      "[Training Epoch 4] Batch 647, Loss 0.24099519848823547\n",
      "[Training Epoch 4] Batch 648, Loss 0.24487057328224182\n",
      "[Training Epoch 4] Batch 649, Loss 0.2683231234550476\n",
      "[Training Epoch 4] Batch 650, Loss 0.25413453578948975\n",
      "[Training Epoch 4] Batch 651, Loss 0.25768768787384033\n",
      "[Training Epoch 4] Batch 652, Loss 0.2741406559944153\n",
      "[Training Epoch 4] Batch 653, Loss 0.2519954442977905\n",
      "[Training Epoch 4] Batch 654, Loss 0.2649584412574768\n",
      "[Training Epoch 4] Batch 655, Loss 0.25000128149986267\n",
      "[Training Epoch 4] Batch 656, Loss 0.27609944343566895\n",
      "[Training Epoch 4] Batch 657, Loss 0.2508292496204376\n",
      "[Training Epoch 4] Batch 658, Loss 0.24147164821624756\n",
      "[Training Epoch 4] Batch 659, Loss 0.2726549506187439\n",
      "[Training Epoch 4] Batch 660, Loss 0.2564331293106079\n",
      "[Training Epoch 4] Batch 661, Loss 0.23554399609565735\n",
      "[Training Epoch 4] Batch 662, Loss 0.2684124708175659\n",
      "[Training Epoch 4] Batch 663, Loss 0.2773939371109009\n",
      "[Training Epoch 4] Batch 664, Loss 0.25367286801338196\n",
      "[Training Epoch 4] Batch 665, Loss 0.2668120265007019\n",
      "[Training Epoch 4] Batch 666, Loss 0.29041939973831177\n",
      "[Training Epoch 4] Batch 667, Loss 0.2675166130065918\n",
      "[Training Epoch 4] Batch 668, Loss 0.26633399724960327\n",
      "[Training Epoch 4] Batch 669, Loss 0.27425912022590637\n",
      "[Training Epoch 4] Batch 670, Loss 0.2565768361091614\n",
      "[Training Epoch 4] Batch 671, Loss 0.27087777853012085\n",
      "[Training Epoch 4] Batch 672, Loss 0.2723848223686218\n",
      "[Training Epoch 4] Batch 673, Loss 0.250951886177063\n",
      "[Training Epoch 4] Batch 674, Loss 0.2823948860168457\n",
      "[Training Epoch 4] Batch 675, Loss 0.2815666198730469\n",
      "[Training Epoch 4] Batch 676, Loss 0.26345011591911316\n",
      "[Training Epoch 4] Batch 677, Loss 0.2398250699043274\n",
      "[Training Epoch 4] Batch 678, Loss 0.2415958046913147\n",
      "[Training Epoch 4] Batch 679, Loss 0.27758944034576416\n",
      "[Training Epoch 4] Batch 680, Loss 0.24503284692764282\n",
      "[Training Epoch 4] Batch 681, Loss 0.29654890298843384\n",
      "[Training Epoch 4] Batch 682, Loss 0.2374759167432785\n",
      "[Training Epoch 4] Batch 683, Loss 0.2716965675354004\n",
      "[Training Epoch 4] Batch 684, Loss 0.2575283646583557\n",
      "[Training Epoch 4] Batch 685, Loss 0.2684495449066162\n",
      "[Training Epoch 4] Batch 686, Loss 0.28802525997161865\n",
      "[Training Epoch 4] Batch 687, Loss 0.22945725917816162\n",
      "[Training Epoch 4] Batch 688, Loss 0.28905221819877625\n",
      "[Training Epoch 4] Batch 689, Loss 0.23907238245010376\n",
      "[Training Epoch 4] Batch 690, Loss 0.26715904474258423\n",
      "[Training Epoch 4] Batch 691, Loss 0.26927921175956726\n",
      "[Training Epoch 4] Batch 692, Loss 0.2522784471511841\n",
      "[Training Epoch 4] Batch 693, Loss 0.2474856972694397\n",
      "[Training Epoch 4] Batch 694, Loss 0.2391197681427002\n",
      "[Training Epoch 4] Batch 695, Loss 0.2790173292160034\n",
      "[Training Epoch 4] Batch 696, Loss 0.28298020362854004\n",
      "[Training Epoch 4] Batch 697, Loss 0.27329516410827637\n",
      "[Training Epoch 4] Batch 698, Loss 0.239013209939003\n",
      "[Training Epoch 4] Batch 699, Loss 0.23499150574207306\n",
      "[Training Epoch 4] Batch 700, Loss 0.2612769603729248\n",
      "[Training Epoch 4] Batch 701, Loss 0.26322388648986816\n",
      "[Training Epoch 4] Batch 702, Loss 0.24320989847183228\n",
      "[Training Epoch 4] Batch 703, Loss 0.2689717411994934\n",
      "[Training Epoch 4] Batch 704, Loss 0.26735711097717285\n",
      "[Training Epoch 4] Batch 705, Loss 0.2610219419002533\n",
      "[Training Epoch 4] Batch 706, Loss 0.2527693510055542\n",
      "[Training Epoch 4] Batch 707, Loss 0.2705487310886383\n",
      "[Training Epoch 4] Batch 708, Loss 0.2424830198287964\n",
      "[Training Epoch 4] Batch 709, Loss 0.2832759916782379\n",
      "[Training Epoch 4] Batch 710, Loss 0.2549617290496826\n",
      "[Training Epoch 4] Batch 711, Loss 0.25063854455947876\n",
      "[Training Epoch 4] Batch 712, Loss 0.2500387728214264\n",
      "[Training Epoch 4] Batch 713, Loss 0.27781203389167786\n",
      "[Training Epoch 4] Batch 714, Loss 0.2559289038181305\n",
      "[Training Epoch 4] Batch 715, Loss 0.2859993278980255\n",
      "[Training Epoch 4] Batch 716, Loss 0.2737848162651062\n",
      "[Training Epoch 4] Batch 717, Loss 0.2558673918247223\n",
      "[Training Epoch 4] Batch 718, Loss 0.2849837839603424\n",
      "[Training Epoch 4] Batch 719, Loss 0.27423614263534546\n",
      "[Training Epoch 4] Batch 720, Loss 0.24714653193950653\n",
      "[Training Epoch 4] Batch 721, Loss 0.30849120020866394\n",
      "[Training Epoch 4] Batch 722, Loss 0.2251213788986206\n",
      "[Training Epoch 4] Batch 723, Loss 0.23190785944461823\n",
      "[Training Epoch 4] Batch 724, Loss 0.24202147126197815\n",
      "[Training Epoch 4] Batch 725, Loss 0.2585461139678955\n",
      "[Training Epoch 4] Batch 726, Loss 0.2672603130340576\n",
      "[Training Epoch 4] Batch 727, Loss 0.24256111681461334\n",
      "[Training Epoch 4] Batch 728, Loss 0.2625713646411896\n",
      "[Training Epoch 4] Batch 729, Loss 0.26452475786209106\n",
      "[Training Epoch 4] Batch 730, Loss 0.2652921974658966\n",
      "[Training Epoch 4] Batch 731, Loss 0.27338707447052\n",
      "[Training Epoch 4] Batch 732, Loss 0.26649582386016846\n",
      "[Training Epoch 4] Batch 733, Loss 0.251220703125\n",
      "[Training Epoch 4] Batch 734, Loss 0.2659126818180084\n",
      "[Training Epoch 4] Batch 735, Loss 0.24791841208934784\n",
      "[Training Epoch 4] Batch 736, Loss 0.26082009077072144\n",
      "[Training Epoch 4] Batch 737, Loss 0.28491535782814026\n",
      "[Training Epoch 4] Batch 738, Loss 0.27300065755844116\n",
      "[Training Epoch 4] Batch 739, Loss 0.27104637026786804\n",
      "[Training Epoch 4] Batch 740, Loss 0.26525282859802246\n",
      "[Training Epoch 4] Batch 741, Loss 0.24758826196193695\n",
      "[Training Epoch 4] Batch 742, Loss 0.2612287402153015\n",
      "[Training Epoch 4] Batch 743, Loss 0.24422353506088257\n",
      "[Training Epoch 4] Batch 744, Loss 0.2690485715866089\n",
      "[Training Epoch 4] Batch 745, Loss 0.25085005164146423\n",
      "[Training Epoch 4] Batch 746, Loss 0.2485901564359665\n",
      "[Training Epoch 4] Batch 747, Loss 0.28726959228515625\n",
      "[Training Epoch 4] Batch 748, Loss 0.24527060985565186\n",
      "[Training Epoch 4] Batch 749, Loss 0.25136569142341614\n",
      "[Training Epoch 4] Batch 750, Loss 0.24742300808429718\n",
      "[Training Epoch 4] Batch 751, Loss 0.25786077976226807\n",
      "[Training Epoch 4] Batch 752, Loss 0.25163400173187256\n",
      "[Training Epoch 4] Batch 753, Loss 0.26402005553245544\n",
      "[Training Epoch 4] Batch 754, Loss 0.2839614152908325\n",
      "[Training Epoch 4] Batch 755, Loss 0.2565324306488037\n",
      "[Training Epoch 4] Batch 756, Loss 0.2793075740337372\n",
      "[Training Epoch 4] Batch 757, Loss 0.24737651646137238\n",
      "[Training Epoch 4] Batch 758, Loss 0.2550300657749176\n",
      "[Training Epoch 4] Batch 759, Loss 0.24386373162269592\n",
      "[Training Epoch 4] Batch 760, Loss 0.25552603602409363\n",
      "[Training Epoch 4] Batch 761, Loss 0.2544999420642853\n",
      "[Training Epoch 4] Batch 762, Loss 0.25376439094543457\n",
      "[Training Epoch 4] Batch 763, Loss 0.26295486092567444\n",
      "[Training Epoch 4] Batch 764, Loss 0.24349701404571533\n",
      "[Training Epoch 4] Batch 765, Loss 0.2526060938835144\n",
      "[Training Epoch 4] Batch 766, Loss 0.2389470934867859\n",
      "[Training Epoch 4] Batch 767, Loss 0.26660478115081787\n",
      "[Training Epoch 4] Batch 768, Loss 0.276242733001709\n",
      "[Training Epoch 4] Batch 769, Loss 0.23971447348594666\n",
      "[Training Epoch 4] Batch 770, Loss 0.2784191966056824\n",
      "[Training Epoch 4] Batch 771, Loss 0.2727053463459015\n",
      "[Training Epoch 4] Batch 772, Loss 0.26002171635627747\n",
      "[Training Epoch 4] Batch 773, Loss 0.24793529510498047\n",
      "[Training Epoch 4] Batch 774, Loss 0.27901116013526917\n",
      "[Training Epoch 4] Batch 775, Loss 0.26647597551345825\n",
      "[Training Epoch 4] Batch 776, Loss 0.24748609960079193\n",
      "[Training Epoch 4] Batch 777, Loss 0.29262739419937134\n",
      "[Training Epoch 4] Batch 778, Loss 0.26615020632743835\n",
      "[Training Epoch 4] Batch 779, Loss 0.2688925266265869\n",
      "[Training Epoch 4] Batch 780, Loss 0.24268057942390442\n",
      "[Training Epoch 4] Batch 781, Loss 0.2586553394794464\n",
      "[Training Epoch 4] Batch 782, Loss 0.2648298740386963\n",
      "[Training Epoch 4] Batch 783, Loss 0.26150259375572205\n",
      "[Training Epoch 4] Batch 784, Loss 0.23414313793182373\n",
      "[Training Epoch 4] Batch 785, Loss 0.266360878944397\n",
      "[Training Epoch 4] Batch 786, Loss 0.2523423433303833\n",
      "[Training Epoch 4] Batch 787, Loss 0.2382585108280182\n",
      "[Training Epoch 4] Batch 788, Loss 0.27464133501052856\n",
      "[Training Epoch 4] Batch 789, Loss 0.27502748370170593\n",
      "[Training Epoch 4] Batch 790, Loss 0.2808804512023926\n",
      "[Training Epoch 4] Batch 791, Loss 0.2623511552810669\n",
      "[Training Epoch 4] Batch 792, Loss 0.2611151337623596\n",
      "[Training Epoch 4] Batch 793, Loss 0.27846580743789673\n",
      "[Training Epoch 4] Batch 794, Loss 0.25967320799827576\n",
      "[Training Epoch 4] Batch 795, Loss 0.25758498907089233\n",
      "[Training Epoch 4] Batch 796, Loss 0.23653468489646912\n",
      "[Training Epoch 4] Batch 797, Loss 0.2415340542793274\n",
      "[Training Epoch 4] Batch 798, Loss 0.24684831500053406\n",
      "[Training Epoch 4] Batch 799, Loss 0.28174978494644165\n",
      "[Training Epoch 4] Batch 800, Loss 0.2722219228744507\n",
      "[Training Epoch 4] Batch 801, Loss 0.2789979577064514\n",
      "[Training Epoch 4] Batch 802, Loss 0.2653718590736389\n",
      "[Training Epoch 4] Batch 803, Loss 0.2662678360939026\n",
      "[Training Epoch 4] Batch 804, Loss 0.2476106733083725\n",
      "[Training Epoch 4] Batch 805, Loss 0.25093668699264526\n",
      "[Training Epoch 4] Batch 806, Loss 0.24996910989284515\n",
      "[Training Epoch 4] Batch 807, Loss 0.25968247652053833\n",
      "[Training Epoch 4] Batch 808, Loss 0.2700490653514862\n",
      "[Training Epoch 4] Batch 809, Loss 0.2851890027523041\n",
      "[Training Epoch 4] Batch 810, Loss 0.26874545216560364\n",
      "[Training Epoch 4] Batch 811, Loss 0.27731239795684814\n",
      "[Training Epoch 4] Batch 812, Loss 0.2566086947917938\n",
      "[Training Epoch 4] Batch 813, Loss 0.276081383228302\n",
      "[Training Epoch 4] Batch 814, Loss 0.2470676600933075\n",
      "[Training Epoch 4] Batch 815, Loss 0.25566795468330383\n",
      "[Training Epoch 4] Batch 816, Loss 0.2630551755428314\n",
      "[Training Epoch 4] Batch 817, Loss 0.2778815031051636\n",
      "[Training Epoch 4] Batch 818, Loss 0.25354740023612976\n",
      "[Training Epoch 4] Batch 819, Loss 0.22957372665405273\n",
      "[Training Epoch 4] Batch 820, Loss 0.26101595163345337\n",
      "[Training Epoch 4] Batch 821, Loss 0.2721562385559082\n",
      "[Training Epoch 4] Batch 822, Loss 0.2568166255950928\n",
      "[Training Epoch 4] Batch 823, Loss 0.2705804407596588\n",
      "[Training Epoch 4] Batch 824, Loss 0.27659666538238525\n",
      "[Training Epoch 4] Batch 825, Loss 0.2719355821609497\n",
      "[Training Epoch 4] Batch 826, Loss 0.2625037133693695\n",
      "[Training Epoch 4] Batch 827, Loss 0.28522878885269165\n",
      "[Training Epoch 4] Batch 828, Loss 0.2714537978172302\n",
      "[Training Epoch 4] Batch 829, Loss 0.265532910823822\n",
      "[Training Epoch 4] Batch 830, Loss 0.24355772137641907\n",
      "[Training Epoch 4] Batch 831, Loss 0.2603134214878082\n",
      "[Training Epoch 4] Batch 832, Loss 0.26077017188072205\n",
      "[Training Epoch 4] Batch 833, Loss 0.24118822813034058\n",
      "[Training Epoch 4] Batch 834, Loss 0.26562145352363586\n",
      "[Training Epoch 4] Batch 835, Loss 0.2960946559906006\n",
      "[Training Epoch 4] Batch 836, Loss 0.25496935844421387\n",
      "[Training Epoch 4] Batch 837, Loss 0.27939921617507935\n",
      "[Training Epoch 4] Batch 838, Loss 0.2567157745361328\n",
      "[Training Epoch 4] Batch 839, Loss 0.22500185668468475\n",
      "[Training Epoch 4] Batch 840, Loss 0.259733647108078\n",
      "[Training Epoch 4] Batch 841, Loss 0.28235238790512085\n",
      "[Training Epoch 4] Batch 842, Loss 0.2617263197898865\n",
      "[Training Epoch 4] Batch 843, Loss 0.25342458486557007\n",
      "[Training Epoch 4] Batch 844, Loss 0.2449043244123459\n",
      "[Training Epoch 4] Batch 845, Loss 0.29302817583084106\n",
      "[Training Epoch 4] Batch 846, Loss 0.2681884765625\n",
      "[Training Epoch 4] Batch 847, Loss 0.2479148507118225\n",
      "[Training Epoch 4] Batch 848, Loss 0.27419257164001465\n",
      "[Training Epoch 4] Batch 849, Loss 0.25486600399017334\n",
      "[Training Epoch 4] Batch 850, Loss 0.26801633834838867\n",
      "[Training Epoch 4] Batch 851, Loss 0.28149908781051636\n",
      "[Training Epoch 4] Batch 852, Loss 0.2804129719734192\n",
      "[Training Epoch 4] Batch 853, Loss 0.24618783593177795\n",
      "[Training Epoch 4] Batch 854, Loss 0.2833746075630188\n",
      "[Training Epoch 4] Batch 855, Loss 0.2707875370979309\n",
      "[Training Epoch 4] Batch 856, Loss 0.25608769059181213\n",
      "[Training Epoch 4] Batch 857, Loss 0.26365727186203003\n",
      "[Training Epoch 4] Batch 858, Loss 0.27857285737991333\n",
      "[Training Epoch 4] Batch 859, Loss 0.26257652044296265\n",
      "[Training Epoch 4] Batch 860, Loss 0.26846420764923096\n",
      "[Training Epoch 4] Batch 861, Loss 0.27536070346832275\n",
      "[Training Epoch 4] Batch 862, Loss 0.2410801202058792\n",
      "[Training Epoch 4] Batch 863, Loss 0.27446451783180237\n",
      "[Training Epoch 4] Batch 864, Loss 0.2626533508300781\n",
      "[Training Epoch 4] Batch 865, Loss 0.24607151746749878\n",
      "[Training Epoch 4] Batch 866, Loss 0.2690873146057129\n",
      "[Training Epoch 4] Batch 867, Loss 0.26628589630126953\n",
      "[Training Epoch 4] Batch 868, Loss 0.28306636214256287\n",
      "[Training Epoch 4] Batch 869, Loss 0.26102638244628906\n",
      "[Training Epoch 4] Batch 870, Loss 0.2861025035381317\n",
      "[Training Epoch 4] Batch 871, Loss 0.25416338443756104\n",
      "[Training Epoch 4] Batch 872, Loss 0.22661541402339935\n",
      "[Training Epoch 4] Batch 873, Loss 0.28722429275512695\n",
      "[Training Epoch 4] Batch 874, Loss 0.2602720260620117\n",
      "[Training Epoch 4] Batch 875, Loss 0.24653741717338562\n",
      "[Training Epoch 4] Batch 876, Loss 0.2546271085739136\n",
      "[Training Epoch 4] Batch 877, Loss 0.2913852632045746\n",
      "[Training Epoch 4] Batch 878, Loss 0.2719384431838989\n",
      "[Training Epoch 4] Batch 879, Loss 0.2619350254535675\n",
      "[Training Epoch 4] Batch 880, Loss 0.27845001220703125\n",
      "[Training Epoch 4] Batch 881, Loss 0.28459441661834717\n",
      "[Training Epoch 4] Batch 882, Loss 0.2623477578163147\n",
      "[Training Epoch 4] Batch 883, Loss 0.23699021339416504\n",
      "[Training Epoch 4] Batch 884, Loss 0.2737255394458771\n",
      "[Training Epoch 4] Batch 885, Loss 0.23483216762542725\n",
      "[Training Epoch 4] Batch 886, Loss 0.26130980253219604\n",
      "[Training Epoch 4] Batch 887, Loss 0.2653903365135193\n",
      "[Training Epoch 4] Batch 888, Loss 0.26035404205322266\n",
      "[Training Epoch 4] Batch 889, Loss 0.23061195015907288\n",
      "[Training Epoch 4] Batch 890, Loss 0.2619350552558899\n",
      "[Training Epoch 4] Batch 891, Loss 0.25692257285118103\n",
      "[Training Epoch 4] Batch 892, Loss 0.2586592435836792\n",
      "[Training Epoch 4] Batch 893, Loss 0.2515747547149658\n",
      "[Training Epoch 4] Batch 894, Loss 0.27013278007507324\n",
      "[Training Epoch 4] Batch 895, Loss 0.25190168619155884\n",
      "[Training Epoch 4] Batch 896, Loss 0.2772080898284912\n",
      "[Training Epoch 4] Batch 897, Loss 0.24437953531742096\n",
      "[Training Epoch 4] Batch 898, Loss 0.26578813791275024\n",
      "[Training Epoch 4] Batch 899, Loss 0.2925175428390503\n",
      "[Training Epoch 4] Batch 900, Loss 0.26174023747444153\n",
      "[Training Epoch 4] Batch 901, Loss 0.2633697986602783\n",
      "[Training Epoch 4] Batch 902, Loss 0.2724917531013489\n",
      "[Training Epoch 4] Batch 903, Loss 0.24015213549137115\n",
      "[Training Epoch 4] Batch 904, Loss 0.29898858070373535\n",
      "[Training Epoch 4] Batch 905, Loss 0.28795552253723145\n",
      "[Training Epoch 4] Batch 906, Loss 0.2588062882423401\n",
      "[Training Epoch 4] Batch 907, Loss 0.24971216917037964\n",
      "[Training Epoch 4] Batch 908, Loss 0.24040552973747253\n",
      "[Training Epoch 4] Batch 909, Loss 0.2738734781742096\n",
      "[Training Epoch 4] Batch 910, Loss 0.2764597535133362\n",
      "[Training Epoch 4] Batch 911, Loss 0.26056650280952454\n",
      "[Training Epoch 4] Batch 912, Loss 0.24158576130867004\n",
      "[Training Epoch 4] Batch 913, Loss 0.2938176691532135\n",
      "[Training Epoch 4] Batch 914, Loss 0.26533013582229614\n",
      "[Training Epoch 4] Batch 915, Loss 0.25090181827545166\n",
      "[Training Epoch 4] Batch 916, Loss 0.28708237409591675\n",
      "[Training Epoch 4] Batch 917, Loss 0.2680286169052124\n",
      "[Training Epoch 4] Batch 918, Loss 0.2327078878879547\n",
      "[Training Epoch 4] Batch 919, Loss 0.2775641977787018\n",
      "[Training Epoch 4] Batch 920, Loss 0.22782002389431\n",
      "[Training Epoch 4] Batch 921, Loss 0.26767054200172424\n",
      "[Training Epoch 4] Batch 922, Loss 0.2719390392303467\n",
      "[Training Epoch 4] Batch 923, Loss 0.24798288941383362\n",
      "[Training Epoch 4] Batch 924, Loss 0.2466025948524475\n",
      "[Training Epoch 4] Batch 925, Loss 0.26473718881607056\n",
      "[Training Epoch 4] Batch 926, Loss 0.2566501796245575\n",
      "[Training Epoch 4] Batch 927, Loss 0.2622618079185486\n",
      "[Training Epoch 4] Batch 928, Loss 0.26824283599853516\n",
      "[Training Epoch 4] Batch 929, Loss 0.26361626386642456\n",
      "[Training Epoch 4] Batch 930, Loss 0.24097663164138794\n",
      "[Training Epoch 4] Batch 931, Loss 0.26610270142555237\n",
      "[Training Epoch 4] Batch 932, Loss 0.28802490234375\n",
      "[Training Epoch 4] Batch 933, Loss 0.2353973239660263\n",
      "[Training Epoch 4] Batch 934, Loss 0.2825586199760437\n",
      "[Training Epoch 4] Batch 935, Loss 0.2690412998199463\n",
      "[Training Epoch 4] Batch 936, Loss 0.22925621271133423\n",
      "[Training Epoch 4] Batch 937, Loss 0.2581283450126648\n",
      "[Training Epoch 4] Batch 938, Loss 0.2521835267543793\n",
      "[Training Epoch 4] Batch 939, Loss 0.25856348872184753\n",
      "[Training Epoch 4] Batch 940, Loss 0.2603667080402374\n",
      "[Training Epoch 4] Batch 941, Loss 0.26729172468185425\n",
      "[Training Epoch 4] Batch 942, Loss 0.26293817162513733\n",
      "[Training Epoch 4] Batch 943, Loss 0.2824682891368866\n",
      "[Training Epoch 4] Batch 944, Loss 0.24031242728233337\n",
      "[Training Epoch 4] Batch 945, Loss 0.277225136756897\n",
      "[Training Epoch 4] Batch 946, Loss 0.2711472511291504\n",
      "[Training Epoch 4] Batch 947, Loss 0.2506975531578064\n",
      "[Training Epoch 4] Batch 948, Loss 0.26874804496765137\n",
      "[Training Epoch 4] Batch 949, Loss 0.2726825475692749\n",
      "[Training Epoch 4] Batch 950, Loss 0.2506052851676941\n",
      "[Training Epoch 4] Batch 951, Loss 0.2617971897125244\n",
      "[Training Epoch 4] Batch 952, Loss 0.2602212429046631\n",
      "[Training Epoch 4] Batch 953, Loss 0.26096436381340027\n",
      "[Training Epoch 4] Batch 954, Loss 0.2370632141828537\n",
      "[Training Epoch 4] Batch 955, Loss 0.28673192858695984\n",
      "[Training Epoch 4] Batch 956, Loss 0.25258901715278625\n",
      "[Training Epoch 4] Batch 957, Loss 0.2570692300796509\n",
      "[Training Epoch 4] Batch 958, Loss 0.2713865637779236\n",
      "[Training Epoch 4] Batch 959, Loss 0.31095415353775024\n",
      "[Training Epoch 4] Batch 960, Loss 0.2512497305870056\n",
      "[Training Epoch 4] Batch 961, Loss 0.2618955969810486\n",
      "[Training Epoch 4] Batch 962, Loss 0.2698306143283844\n",
      "[Training Epoch 4] Batch 963, Loss 0.23953737318515778\n",
      "[Training Epoch 4] Batch 964, Loss 0.26952314376831055\n",
      "[Training Epoch 4] Batch 965, Loss 0.27236679196357727\n",
      "[Training Epoch 4] Batch 966, Loss 0.27011626958847046\n",
      "[Training Epoch 4] Batch 967, Loss 0.23323246836662292\n",
      "[Training Epoch 4] Batch 968, Loss 0.2696252465248108\n",
      "[Training Epoch 4] Batch 969, Loss 0.27887916564941406\n",
      "[Training Epoch 4] Batch 970, Loss 0.2589681148529053\n",
      "[Training Epoch 4] Batch 971, Loss 0.29555976390838623\n",
      "[Training Epoch 4] Batch 972, Loss 0.2889447808265686\n",
      "[Training Epoch 4] Batch 973, Loss 0.27624082565307617\n",
      "[Training Epoch 4] Batch 974, Loss 0.25821453332901\n",
      "[Training Epoch 4] Batch 975, Loss 0.2634502649307251\n",
      "[Training Epoch 4] Batch 976, Loss 0.2649880647659302\n",
      "[Training Epoch 4] Batch 977, Loss 0.28156471252441406\n",
      "[Training Epoch 4] Batch 978, Loss 0.23517051339149475\n",
      "[Training Epoch 4] Batch 979, Loss 0.24505615234375\n",
      "[Training Epoch 4] Batch 980, Loss 0.28754591941833496\n",
      "[Training Epoch 4] Batch 981, Loss 0.24093937873840332\n",
      "[Training Epoch 4] Batch 982, Loss 0.25981342792510986\n",
      "[Training Epoch 4] Batch 983, Loss 0.2650032043457031\n",
      "[Training Epoch 4] Batch 984, Loss 0.2524406313896179\n",
      "[Training Epoch 4] Batch 985, Loss 0.25403034687042236\n",
      "[Training Epoch 4] Batch 986, Loss 0.27424418926239014\n",
      "[Training Epoch 4] Batch 987, Loss 0.25603315234184265\n",
      "[Training Epoch 4] Batch 988, Loss 0.26222795248031616\n",
      "[Training Epoch 4] Batch 989, Loss 0.2457968145608902\n",
      "[Training Epoch 4] Batch 990, Loss 0.2465810477733612\n",
      "[Training Epoch 4] Batch 991, Loss 0.25140389800071716\n",
      "[Training Epoch 4] Batch 992, Loss 0.255309522151947\n",
      "[Training Epoch 4] Batch 993, Loss 0.25178539752960205\n",
      "[Training Epoch 4] Batch 994, Loss 0.2647254467010498\n",
      "[Training Epoch 4] Batch 995, Loss 0.26395973563194275\n",
      "[Training Epoch 4] Batch 996, Loss 0.2597898244857788\n",
      "[Training Epoch 4] Batch 997, Loss 0.2717086672782898\n",
      "[Training Epoch 4] Batch 998, Loss 0.24674011766910553\n",
      "[Training Epoch 4] Batch 999, Loss 0.25923967361450195\n",
      "[Training Epoch 4] Batch 1000, Loss 0.2557724714279175\n",
      "[Training Epoch 4] Batch 1001, Loss 0.2799205183982849\n",
      "[Training Epoch 4] Batch 1002, Loss 0.3030422329902649\n",
      "[Training Epoch 4] Batch 1003, Loss 0.26606711745262146\n",
      "[Training Epoch 4] Batch 1004, Loss 0.26211172342300415\n",
      "[Training Epoch 4] Batch 1005, Loss 0.2508719861507416\n",
      "[Training Epoch 4] Batch 1006, Loss 0.25077587366104126\n",
      "[Training Epoch 4] Batch 1007, Loss 0.27283036708831787\n",
      "[Training Epoch 4] Batch 1008, Loss 0.25408071279525757\n",
      "[Training Epoch 4] Batch 1009, Loss 0.2703830301761627\n",
      "[Training Epoch 4] Batch 1010, Loss 0.27872028946876526\n",
      "[Training Epoch 4] Batch 1011, Loss 0.25211605429649353\n",
      "[Training Epoch 4] Batch 1012, Loss 0.27386045455932617\n",
      "[Training Epoch 4] Batch 1013, Loss 0.2720502018928528\n",
      "[Training Epoch 4] Batch 1014, Loss 0.2404424399137497\n",
      "[Training Epoch 4] Batch 1015, Loss 0.25031518936157227\n",
      "[Training Epoch 4] Batch 1016, Loss 0.24740353226661682\n",
      "[Training Epoch 4] Batch 1017, Loss 0.257845938205719\n",
      "[Training Epoch 4] Batch 1018, Loss 0.25971096754074097\n",
      "[Training Epoch 4] Batch 1019, Loss 0.2523277997970581\n",
      "[Training Epoch 4] Batch 1020, Loss 0.24069783091545105\n",
      "[Training Epoch 4] Batch 1021, Loss 0.2631406784057617\n",
      "[Training Epoch 4] Batch 1022, Loss 0.2670905888080597\n",
      "[Training Epoch 4] Batch 1023, Loss 0.25405392050743103\n",
      "[Training Epoch 4] Batch 1024, Loss 0.2853667140007019\n",
      "[Training Epoch 4] Batch 1025, Loss 0.29600396752357483\n",
      "[Training Epoch 4] Batch 1026, Loss 0.2683892548084259\n",
      "[Training Epoch 4] Batch 1027, Loss 0.261407732963562\n",
      "[Training Epoch 4] Batch 1028, Loss 0.27722716331481934\n",
      "[Training Epoch 4] Batch 1029, Loss 0.25348204374313354\n",
      "[Training Epoch 4] Batch 1030, Loss 0.30648887157440186\n",
      "[Training Epoch 4] Batch 1031, Loss 0.26137375831604004\n",
      "[Training Epoch 4] Batch 1032, Loss 0.2578340172767639\n",
      "[Training Epoch 4] Batch 1033, Loss 0.2893809676170349\n",
      "[Training Epoch 4] Batch 1034, Loss 0.2922656834125519\n",
      "[Training Epoch 4] Batch 1035, Loss 0.23219522833824158\n",
      "[Training Epoch 4] Batch 1036, Loss 0.2755182981491089\n",
      "[Training Epoch 4] Batch 1037, Loss 0.27356046438217163\n",
      "[Training Epoch 4] Batch 1038, Loss 0.2644945979118347\n",
      "[Training Epoch 4] Batch 1039, Loss 0.2759954333305359\n",
      "[Training Epoch 4] Batch 1040, Loss 0.2738044857978821\n",
      "[Training Epoch 4] Batch 1041, Loss 0.2385040521621704\n",
      "[Training Epoch 4] Batch 1042, Loss 0.27939900755882263\n",
      "[Training Epoch 4] Batch 1043, Loss 0.27369779348373413\n",
      "[Training Epoch 4] Batch 1044, Loss 0.24765142798423767\n",
      "[Training Epoch 4] Batch 1045, Loss 0.27902892231941223\n",
      "[Training Epoch 4] Batch 1046, Loss 0.2646743059158325\n",
      "[Training Epoch 4] Batch 1047, Loss 0.2707880735397339\n",
      "[Training Epoch 4] Batch 1048, Loss 0.2474142611026764\n",
      "[Training Epoch 4] Batch 1049, Loss 0.2504686713218689\n",
      "[Training Epoch 4] Batch 1050, Loss 0.2729775011539459\n",
      "[Training Epoch 4] Batch 1051, Loss 0.26494282484054565\n",
      "[Training Epoch 4] Batch 1052, Loss 0.268466055393219\n",
      "[Training Epoch 4] Batch 1053, Loss 0.26484933495521545\n",
      "[Training Epoch 4] Batch 1054, Loss 0.27200767397880554\n",
      "[Training Epoch 4] Batch 1055, Loss 0.2804179787635803\n",
      "[Training Epoch 4] Batch 1056, Loss 0.28165093064308167\n",
      "[Training Epoch 4] Batch 1057, Loss 0.262981653213501\n",
      "[Training Epoch 4] Batch 1058, Loss 0.28460726141929626\n",
      "[Training Epoch 4] Batch 1059, Loss 0.23482605814933777\n",
      "[Training Epoch 4] Batch 1060, Loss 0.2837357223033905\n",
      "[Training Epoch 4] Batch 1061, Loss 0.2621724605560303\n",
      "[Training Epoch 4] Batch 1062, Loss 0.2572658061981201\n",
      "[Training Epoch 4] Batch 1063, Loss 0.2750011086463928\n",
      "[Training Epoch 4] Batch 1064, Loss 0.2759464979171753\n",
      "[Training Epoch 4] Batch 1065, Loss 0.2721472680568695\n",
      "[Training Epoch 4] Batch 1066, Loss 0.24821865558624268\n",
      "[Training Epoch 4] Batch 1067, Loss 0.24824309349060059\n",
      "[Training Epoch 4] Batch 1068, Loss 0.26858147978782654\n",
      "[Training Epoch 4] Batch 1069, Loss 0.2642894387245178\n",
      "[Training Epoch 4] Batch 1070, Loss 0.2692386507987976\n",
      "[Training Epoch 4] Batch 1071, Loss 0.2980077266693115\n",
      "[Training Epoch 4] Batch 1072, Loss 0.2521398663520813\n",
      "[Training Epoch 4] Batch 1073, Loss 0.248711958527565\n",
      "[Training Epoch 4] Batch 1074, Loss 0.23198050260543823\n",
      "[Training Epoch 4] Batch 1075, Loss 0.2760242819786072\n",
      "[Training Epoch 4] Batch 1076, Loss 0.23819872736930847\n",
      "[Training Epoch 4] Batch 1077, Loss 0.24254561960697174\n",
      "[Training Epoch 4] Batch 1078, Loss 0.274303138256073\n",
      "[Training Epoch 4] Batch 1079, Loss 0.28332626819610596\n",
      "[Training Epoch 4] Batch 1080, Loss 0.2722536027431488\n",
      "[Training Epoch 4] Batch 1081, Loss 0.248932346701622\n",
      "[Training Epoch 4] Batch 1082, Loss 0.2779276967048645\n",
      "[Training Epoch 4] Batch 1083, Loss 0.3069932460784912\n",
      "[Training Epoch 4] Batch 1084, Loss 0.2292492836713791\n",
      "[Training Epoch 4] Batch 1085, Loss 0.2692793905735016\n",
      "[Training Epoch 4] Batch 1086, Loss 0.25025755167007446\n",
      "[Training Epoch 4] Batch 1087, Loss 0.2754175364971161\n",
      "[Training Epoch 4] Batch 1088, Loss 0.2482769936323166\n",
      "[Training Epoch 4] Batch 1089, Loss 0.26892170310020447\n",
      "[Training Epoch 4] Batch 1090, Loss 0.24124310910701752\n",
      "[Training Epoch 4] Batch 1091, Loss 0.2464500367641449\n",
      "[Training Epoch 4] Batch 1092, Loss 0.2655041217803955\n",
      "[Training Epoch 4] Batch 1093, Loss 0.26920849084854126\n",
      "[Training Epoch 4] Batch 1094, Loss 0.2557837963104248\n",
      "[Training Epoch 4] Batch 1095, Loss 0.24650254845619202\n",
      "[Training Epoch 4] Batch 1096, Loss 0.2743748426437378\n",
      "[Training Epoch 4] Batch 1097, Loss 0.24310019612312317\n",
      "[Training Epoch 4] Batch 1098, Loss 0.23922356963157654\n",
      "[Training Epoch 4] Batch 1099, Loss 0.24084478616714478\n",
      "[Training Epoch 4] Batch 1100, Loss 0.2468959391117096\n",
      "[Training Epoch 4] Batch 1101, Loss 0.24689167737960815\n",
      "[Training Epoch 4] Batch 1102, Loss 0.2724301218986511\n",
      "[Training Epoch 4] Batch 1103, Loss 0.2705279588699341\n",
      "[Training Epoch 4] Batch 1104, Loss 0.2558310627937317\n",
      "[Training Epoch 4] Batch 1105, Loss 0.22826342284679413\n",
      "[Training Epoch 4] Batch 1106, Loss 0.2706058621406555\n",
      "[Training Epoch 4] Batch 1107, Loss 0.26281654834747314\n",
      "[Training Epoch 4] Batch 1108, Loss 0.2709410488605499\n",
      "[Training Epoch 4] Batch 1109, Loss 0.27645736932754517\n",
      "[Training Epoch 4] Batch 1110, Loss 0.25096845626831055\n",
      "[Training Epoch 4] Batch 1111, Loss 0.27799248695373535\n",
      "[Training Epoch 4] Batch 1112, Loss 0.2539818286895752\n",
      "[Training Epoch 4] Batch 1113, Loss 0.2495948076248169\n",
      "[Training Epoch 4] Batch 1114, Loss 0.22468388080596924\n",
      "[Training Epoch 4] Batch 1115, Loss 0.24853107333183289\n",
      "[Training Epoch 4] Batch 1116, Loss 0.2672807276248932\n",
      "[Training Epoch 4] Batch 1117, Loss 0.26526081562042236\n",
      "[Training Epoch 4] Batch 1118, Loss 0.27789971232414246\n",
      "[Training Epoch 4] Batch 1119, Loss 0.269906610250473\n",
      "[Training Epoch 4] Batch 1120, Loss 0.2735036611557007\n",
      "[Training Epoch 4] Batch 1121, Loss 0.25298750400543213\n",
      "[Training Epoch 4] Batch 1122, Loss 0.25089898705482483\n",
      "[Training Epoch 4] Batch 1123, Loss 0.26232051849365234\n",
      "[Training Epoch 4] Batch 1124, Loss 0.2694711685180664\n",
      "[Training Epoch 4] Batch 1125, Loss 0.26850616931915283\n",
      "[Training Epoch 4] Batch 1126, Loss 0.2610473036766052\n",
      "[Training Epoch 4] Batch 1127, Loss 0.2616299092769623\n",
      "[Training Epoch 4] Batch 1128, Loss 0.26261162757873535\n",
      "[Training Epoch 4] Batch 1129, Loss 0.28987088799476624\n",
      "[Training Epoch 4] Batch 1130, Loss 0.26976341009140015\n",
      "[Training Epoch 4] Batch 1131, Loss 0.28514859080314636\n",
      "[Training Epoch 4] Batch 1132, Loss 0.23163823783397675\n",
      "[Training Epoch 4] Batch 1133, Loss 0.277059942483902\n",
      "[Training Epoch 4] Batch 1134, Loss 0.27931952476501465\n",
      "[Training Epoch 4] Batch 1135, Loss 0.2637718617916107\n",
      "[Training Epoch 4] Batch 1136, Loss 0.24755221605300903\n",
      "[Training Epoch 4] Batch 1137, Loss 0.26186424493789673\n",
      "[Training Epoch 4] Batch 1138, Loss 0.2665471136569977\n",
      "[Training Epoch 4] Batch 1139, Loss 0.30991238355636597\n",
      "[Training Epoch 4] Batch 1140, Loss 0.24983544647693634\n",
      "[Training Epoch 4] Batch 1141, Loss 0.25827187299728394\n",
      "[Training Epoch 4] Batch 1142, Loss 0.25284987688064575\n",
      "[Training Epoch 4] Batch 1143, Loss 0.24899135529994965\n",
      "[Training Epoch 4] Batch 1144, Loss 0.26787975430488586\n",
      "[Training Epoch 4] Batch 1145, Loss 0.2352028787136078\n",
      "[Training Epoch 4] Batch 1146, Loss 0.254615843296051\n",
      "[Training Epoch 4] Batch 1147, Loss 0.28066486120224\n",
      "[Training Epoch 4] Batch 1148, Loss 0.2776826322078705\n",
      "[Training Epoch 4] Batch 1149, Loss 0.26582252979278564\n",
      "[Training Epoch 4] Batch 1150, Loss 0.25903719663619995\n",
      "[Training Epoch 4] Batch 1151, Loss 0.27939701080322266\n",
      "[Training Epoch 4] Batch 1152, Loss 0.259298175573349\n",
      "[Training Epoch 4] Batch 1153, Loss 0.2851998805999756\n",
      "[Training Epoch 4] Batch 1154, Loss 0.2509506046772003\n",
      "[Training Epoch 4] Batch 1155, Loss 0.2445090115070343\n",
      "[Training Epoch 4] Batch 1156, Loss 0.2629052996635437\n",
      "[Training Epoch 4] Batch 1157, Loss 0.24539101123809814\n",
      "[Training Epoch 4] Batch 1158, Loss 0.24589842557907104\n",
      "[Training Epoch 4] Batch 1159, Loss 0.2646278142929077\n",
      "[Training Epoch 4] Batch 1160, Loss 0.2555992007255554\n",
      "[Training Epoch 4] Batch 1161, Loss 0.269052654504776\n",
      "[Training Epoch 4] Batch 1162, Loss 0.2486763298511505\n",
      "[Training Epoch 4] Batch 1163, Loss 0.27667343616485596\n",
      "[Training Epoch 4] Batch 1164, Loss 0.26677316427230835\n",
      "[Training Epoch 4] Batch 1165, Loss 0.25830382108688354\n",
      "[Training Epoch 4] Batch 1166, Loss 0.2626765966415405\n",
      "[Training Epoch 4] Batch 1167, Loss 0.27812865376472473\n",
      "[Training Epoch 4] Batch 1168, Loss 0.2665768563747406\n",
      "[Training Epoch 4] Batch 1169, Loss 0.3082341253757477\n",
      "[Training Epoch 4] Batch 1170, Loss 0.2593906819820404\n",
      "[Training Epoch 4] Batch 1171, Loss 0.24731922149658203\n",
      "[Training Epoch 4] Batch 1172, Loss 0.25733619928359985\n",
      "[Training Epoch 4] Batch 1173, Loss 0.2932876944541931\n",
      "[Training Epoch 4] Batch 1174, Loss 0.2392413318157196\n",
      "[Training Epoch 4] Batch 1175, Loss 0.30365926027297974\n",
      "[Training Epoch 4] Batch 1176, Loss 0.2939416766166687\n",
      "[Training Epoch 4] Batch 1177, Loss 0.2725887596607208\n",
      "[Training Epoch 4] Batch 1178, Loss 0.25996890664100647\n",
      "[Training Epoch 4] Batch 1179, Loss 0.26624393463134766\n",
      "[Training Epoch 4] Batch 1180, Loss 0.2735206186771393\n",
      "[Training Epoch 4] Batch 1181, Loss 0.268200159072876\n",
      "[Training Epoch 4] Batch 1182, Loss 0.2510029077529907\n",
      "[Training Epoch 4] Batch 1183, Loss 0.2631511986255646\n",
      "[Training Epoch 4] Batch 1184, Loss 0.25325649976730347\n",
      "[Training Epoch 4] Batch 1185, Loss 0.24788720905780792\n",
      "[Training Epoch 4] Batch 1186, Loss 0.2788622975349426\n",
      "[Training Epoch 4] Batch 1187, Loss 0.2512064576148987\n",
      "[Training Epoch 4] Batch 1188, Loss 0.2544925808906555\n",
      "[Training Epoch 4] Batch 1189, Loss 0.2448056936264038\n",
      "[Training Epoch 4] Batch 1190, Loss 0.26432496309280396\n",
      "[Training Epoch 4] Batch 1191, Loss 0.26344889402389526\n",
      "[Training Epoch 4] Batch 1192, Loss 0.2510027587413788\n",
      "[Training Epoch 4] Batch 1193, Loss 0.2516706585884094\n",
      "[Training Epoch 4] Batch 1194, Loss 0.2557893991470337\n",
      "[Training Epoch 4] Batch 1195, Loss 0.2952861189842224\n",
      "[Training Epoch 4] Batch 1196, Loss 0.3036996126174927\n",
      "[Training Epoch 4] Batch 1197, Loss 0.2860833406448364\n",
      "[Training Epoch 4] Batch 1198, Loss 0.24859566986560822\n",
      "[Training Epoch 4] Batch 1199, Loss 0.30106085538864136\n",
      "[Training Epoch 4] Batch 1200, Loss 0.2796236574649811\n",
      "[Training Epoch 4] Batch 1201, Loss 0.25684624910354614\n",
      "[Training Epoch 4] Batch 1202, Loss 0.2685256898403168\n",
      "[Training Epoch 4] Batch 1203, Loss 0.2871939539909363\n",
      "[Training Epoch 4] Batch 1204, Loss 0.29477205872535706\n",
      "[Training Epoch 4] Batch 1205, Loss 0.25296664237976074\n",
      "[Training Epoch 4] Batch 1206, Loss 0.27237123250961304\n",
      "[Training Epoch 4] Batch 1207, Loss 0.2682275176048279\n",
      "[Training Epoch 4] Batch 1208, Loss 0.27656883001327515\n",
      "[Training Epoch 4] Batch 1209, Loss 0.23024198412895203\n",
      "[Training Epoch 4] Batch 1210, Loss 0.27010494470596313\n",
      "[Training Epoch 4] Batch 1211, Loss 0.29161277413368225\n",
      "[Training Epoch 4] Batch 1212, Loss 0.27137744426727295\n",
      "[Training Epoch 4] Batch 1213, Loss 0.253974050283432\n",
      "[Training Epoch 4] Batch 1214, Loss 0.25440412759780884\n",
      "[Training Epoch 4] Batch 1215, Loss 0.2708470821380615\n",
      "[Training Epoch 4] Batch 1216, Loss 0.27054348587989807\n",
      "[Training Epoch 4] Batch 1217, Loss 0.3034340739250183\n",
      "[Training Epoch 4] Batch 1218, Loss 0.25481879711151123\n",
      "[Training Epoch 4] Batch 1219, Loss 0.22641617059707642\n",
      "[Training Epoch 4] Batch 1220, Loss 0.29389238357543945\n",
      "[Training Epoch 4] Batch 1221, Loss 0.2836171090602875\n",
      "[Training Epoch 4] Batch 1222, Loss 0.25832509994506836\n",
      "[Training Epoch 4] Batch 1223, Loss 0.25271284580230713\n",
      "[Training Epoch 4] Batch 1224, Loss 0.27012571692466736\n",
      "[Training Epoch 4] Batch 1225, Loss 0.3329154849052429\n",
      "[Training Epoch 4] Batch 1226, Loss 0.2686263620853424\n",
      "[Training Epoch 4] Batch 1227, Loss 0.24541623890399933\n",
      "[Training Epoch 4] Batch 1228, Loss 0.2843097746372223\n",
      "[Training Epoch 4] Batch 1229, Loss 0.2719227075576782\n",
      "[Training Epoch 4] Batch 1230, Loss 0.2926506996154785\n",
      "[Training Epoch 4] Batch 1231, Loss 0.2864494025707245\n",
      "[Training Epoch 4] Batch 1232, Loss 0.2789214253425598\n",
      "[Training Epoch 4] Batch 1233, Loss 0.26337099075317383\n",
      "[Training Epoch 4] Batch 1234, Loss 0.2536618113517761\n",
      "[Training Epoch 4] Batch 1235, Loss 0.2824000120162964\n",
      "[Training Epoch 4] Batch 1236, Loss 0.30001312494277954\n",
      "[Training Epoch 4] Batch 1237, Loss 0.27070656418800354\n",
      "[Training Epoch 4] Batch 1238, Loss 0.25397127866744995\n",
      "[Training Epoch 4] Batch 1239, Loss 0.260183721780777\n",
      "[Training Epoch 4] Batch 1240, Loss 0.23918545246124268\n",
      "[Training Epoch 4] Batch 1241, Loss 0.24473810195922852\n",
      "[Training Epoch 4] Batch 1242, Loss 0.28985336422920227\n",
      "[Training Epoch 4] Batch 1243, Loss 0.25926896929740906\n",
      "[Training Epoch 4] Batch 1244, Loss 0.2546687126159668\n",
      "[Training Epoch 4] Batch 1245, Loss 0.26665806770324707\n",
      "[Training Epoch 4] Batch 1246, Loss 0.2518154978752136\n",
      "[Training Epoch 4] Batch 1247, Loss 0.2759896516799927\n",
      "[Training Epoch 4] Batch 1248, Loss 0.2601192593574524\n",
      "[Training Epoch 4] Batch 1249, Loss 0.2652881443500519\n",
      "[Training Epoch 4] Batch 1250, Loss 0.2740291357040405\n",
      "[Training Epoch 4] Batch 1251, Loss 0.24866412580013275\n",
      "[Training Epoch 4] Batch 1252, Loss 0.27461928129196167\n",
      "[Training Epoch 4] Batch 1253, Loss 0.2931370437145233\n",
      "[Training Epoch 4] Batch 1254, Loss 0.2823871076107025\n",
      "[Training Epoch 4] Batch 1255, Loss 0.2608490288257599\n",
      "[Training Epoch 4] Batch 1256, Loss 0.2535182237625122\n",
      "[Training Epoch 4] Batch 1257, Loss 0.25310054421424866\n",
      "[Training Epoch 4] Batch 1258, Loss 0.28908756375312805\n",
      "[Training Epoch 4] Batch 1259, Loss 0.2840634286403656\n",
      "[Training Epoch 4] Batch 1260, Loss 0.26777100563049316\n",
      "[Training Epoch 4] Batch 1261, Loss 0.3085174858570099\n",
      "[Training Epoch 4] Batch 1262, Loss 0.22737982869148254\n",
      "[Training Epoch 4] Batch 1263, Loss 0.2788521349430084\n",
      "[Training Epoch 4] Batch 1264, Loss 0.2728940546512604\n",
      "[Training Epoch 4] Batch 1265, Loss 0.2390872836112976\n",
      "[Training Epoch 4] Batch 1266, Loss 0.2748457193374634\n",
      "[Training Epoch 4] Batch 1267, Loss 0.26836225390434265\n",
      "[Training Epoch 4] Batch 1268, Loss 0.29179638624191284\n",
      "[Training Epoch 4] Batch 1269, Loss 0.26016366481781006\n",
      "[Training Epoch 4] Batch 1270, Loss 0.2563920319080353\n",
      "[Training Epoch 4] Batch 1271, Loss 0.27916598320007324\n",
      "[Training Epoch 4] Batch 1272, Loss 0.2731466293334961\n",
      "[Training Epoch 4] Batch 1273, Loss 0.2845434546470642\n",
      "[Training Epoch 4] Batch 1274, Loss 0.26508593559265137\n",
      "[Training Epoch 4] Batch 1275, Loss 0.2824528217315674\n",
      "[Training Epoch 4] Batch 1276, Loss 0.2662442922592163\n",
      "[Training Epoch 4] Batch 1277, Loss 0.2720024585723877\n",
      "[Training Epoch 4] Batch 1278, Loss 0.23904384672641754\n",
      "[Training Epoch 4] Batch 1279, Loss 0.2986636757850647\n",
      "[Training Epoch 4] Batch 1280, Loss 0.24348041415214539\n",
      "[Training Epoch 4] Batch 1281, Loss 0.26076841354370117\n",
      "[Training Epoch 4] Batch 1282, Loss 0.2908931076526642\n",
      "[Training Epoch 4] Batch 1283, Loss 0.24907682836055756\n",
      "[Training Epoch 4] Batch 1284, Loss 0.2457731068134308\n",
      "[Training Epoch 4] Batch 1285, Loss 0.24889600276947021\n",
      "[Training Epoch 4] Batch 1286, Loss 0.28730887174606323\n",
      "[Training Epoch 4] Batch 1287, Loss 0.25529733300209045\n",
      "[Training Epoch 4] Batch 1288, Loss 0.289512574672699\n",
      "[Training Epoch 4] Batch 1289, Loss 0.2767305374145508\n",
      "[Training Epoch 4] Batch 1290, Loss 0.2644621729850769\n",
      "[Training Epoch 4] Batch 1291, Loss 0.24800746142864227\n",
      "[Training Epoch 4] Batch 1292, Loss 0.2816372215747833\n",
      "[Training Epoch 4] Batch 1293, Loss 0.29071488976478577\n",
      "[Training Epoch 4] Batch 1294, Loss 0.24136804044246674\n",
      "[Training Epoch 4] Batch 1295, Loss 0.24481582641601562\n",
      "[Training Epoch 4] Batch 1296, Loss 0.2585797905921936\n",
      "[Training Epoch 4] Batch 1297, Loss 0.2605143189430237\n",
      "[Training Epoch 4] Batch 1298, Loss 0.2587815821170807\n",
      "[Training Epoch 4] Batch 1299, Loss 0.2451876848936081\n",
      "[Training Epoch 4] Batch 1300, Loss 0.23867136240005493\n",
      "[Training Epoch 4] Batch 1301, Loss 0.2803538739681244\n",
      "[Training Epoch 4] Batch 1302, Loss 0.289337694644928\n",
      "[Training Epoch 4] Batch 1303, Loss 0.3107449412345886\n",
      "[Training Epoch 4] Batch 1304, Loss 0.3020578622817993\n",
      "[Training Epoch 4] Batch 1305, Loss 0.2588833272457123\n",
      "[Training Epoch 4] Batch 1306, Loss 0.27575594186782837\n",
      "[Training Epoch 4] Batch 1307, Loss 0.27019739151000977\n",
      "[Training Epoch 4] Batch 1308, Loss 0.2644414007663727\n",
      "[Training Epoch 4] Batch 1309, Loss 0.26433753967285156\n",
      "[Training Epoch 4] Batch 1310, Loss 0.29023468494415283\n",
      "[Training Epoch 4] Batch 1311, Loss 0.2558203637599945\n",
      "[Training Epoch 4] Batch 1312, Loss 0.25268813967704773\n",
      "[Training Epoch 4] Batch 1313, Loss 0.2601950168609619\n",
      "[Training Epoch 4] Batch 1314, Loss 0.2572912275791168\n",
      "[Training Epoch 4] Batch 1315, Loss 0.2850373387336731\n",
      "[Training Epoch 4] Batch 1316, Loss 0.2518962025642395\n",
      "[Training Epoch 4] Batch 1317, Loss 0.26767975091934204\n",
      "[Training Epoch 4] Batch 1318, Loss 0.2433246672153473\n",
      "[Training Epoch 4] Batch 1319, Loss 0.25508421659469604\n",
      "[Training Epoch 4] Batch 1320, Loss 0.2778341770172119\n",
      "[Training Epoch 4] Batch 1321, Loss 0.2609268128871918\n",
      "[Training Epoch 4] Batch 1322, Loss 0.2283133566379547\n",
      "[Training Epoch 4] Batch 1323, Loss 0.2560714781284332\n",
      "[Training Epoch 4] Batch 1324, Loss 0.24776431918144226\n",
      "[Training Epoch 4] Batch 1325, Loss 0.2737206816673279\n",
      "[Training Epoch 4] Batch 1326, Loss 0.2850434184074402\n",
      "[Training Epoch 4] Batch 1327, Loss 0.2854105830192566\n",
      "[Training Epoch 4] Batch 1328, Loss 0.27698636054992676\n",
      "[Training Epoch 4] Batch 1329, Loss 0.28708332777023315\n",
      "[Training Epoch 4] Batch 1330, Loss 0.25689005851745605\n",
      "[Training Epoch 4] Batch 1331, Loss 0.2711523175239563\n",
      "[Training Epoch 4] Batch 1332, Loss 0.27360934019088745\n",
      "[Training Epoch 4] Batch 1333, Loss 0.28438106179237366\n",
      "[Training Epoch 4] Batch 1334, Loss 0.24342212080955505\n",
      "[Training Epoch 4] Batch 1335, Loss 0.25849664211273193\n",
      "[Training Epoch 4] Batch 1336, Loss 0.2574993968009949\n",
      "[Training Epoch 4] Batch 1337, Loss 0.2609533667564392\n",
      "[Training Epoch 4] Batch 1338, Loss 0.25380754470825195\n",
      "[Training Epoch 4] Batch 1339, Loss 0.2907448410987854\n",
      "[Training Epoch 4] Batch 1340, Loss 0.23688334226608276\n",
      "[Training Epoch 4] Batch 1341, Loss 0.28901034593582153\n",
      "[Training Epoch 4] Batch 1342, Loss 0.2668535113334656\n",
      "[Training Epoch 4] Batch 1343, Loss 0.2791174352169037\n",
      "[Training Epoch 4] Batch 1344, Loss 0.2578144967556\n",
      "[Training Epoch 4] Batch 1345, Loss 0.26416388154029846\n",
      "[Training Epoch 4] Batch 1346, Loss 0.29076504707336426\n",
      "[Training Epoch 4] Batch 1347, Loss 0.2732776403427124\n",
      "[Training Epoch 4] Batch 1348, Loss 0.2691127061843872\n",
      "[Training Epoch 4] Batch 1349, Loss 0.26936525106430054\n",
      "[Training Epoch 4] Batch 1350, Loss 0.27552568912506104\n",
      "[Training Epoch 4] Batch 1351, Loss 0.2614418864250183\n",
      "[Training Epoch 4] Batch 1352, Loss 0.2743135690689087\n",
      "[Training Epoch 4] Batch 1353, Loss 0.265318363904953\n",
      "[Training Epoch 4] Batch 1354, Loss 0.2911827564239502\n",
      "[Training Epoch 4] Batch 1355, Loss 0.2529398798942566\n",
      "[Training Epoch 4] Batch 1356, Loss 0.2696610689163208\n",
      "[Training Epoch 4] Batch 1357, Loss 0.24641606211662292\n",
      "[Training Epoch 4] Batch 1358, Loss 0.29080691933631897\n",
      "[Training Epoch 4] Batch 1359, Loss 0.24429957568645477\n",
      "[Training Epoch 4] Batch 1360, Loss 0.2564687132835388\n",
      "[Training Epoch 4] Batch 1361, Loss 0.258783757686615\n",
      "[Training Epoch 4] Batch 1362, Loss 0.2491397261619568\n",
      "[Training Epoch 4] Batch 1363, Loss 0.2850855886936188\n",
      "[Training Epoch 4] Batch 1364, Loss 0.2382282018661499\n",
      "[Training Epoch 4] Batch 1365, Loss 0.24990177154541016\n",
      "[Training Epoch 4] Batch 1366, Loss 0.2500878572463989\n",
      "[Training Epoch 4] Batch 1367, Loss 0.2794833779335022\n",
      "[Training Epoch 4] Batch 1368, Loss 0.2464464008808136\n",
      "[Training Epoch 4] Batch 1369, Loss 0.25162437558174133\n",
      "[Training Epoch 4] Batch 1370, Loss 0.26395702362060547\n",
      "[Training Epoch 4] Batch 1371, Loss 0.24569468200206757\n",
      "[Training Epoch 4] Batch 1372, Loss 0.2528317868709564\n",
      "[Training Epoch 4] Batch 1373, Loss 0.27659714221954346\n",
      "[Training Epoch 4] Batch 1374, Loss 0.24619564414024353\n",
      "[Training Epoch 4] Batch 1375, Loss 0.23771989345550537\n",
      "[Training Epoch 4] Batch 1376, Loss 0.277682900428772\n",
      "[Training Epoch 4] Batch 1377, Loss 0.25478991866111755\n",
      "[Training Epoch 4] Batch 1378, Loss 0.26318255066871643\n",
      "[Training Epoch 4] Batch 1379, Loss 0.27554765343666077\n",
      "[Training Epoch 4] Batch 1380, Loss 0.24735546112060547\n",
      "[Training Epoch 4] Batch 1381, Loss 0.2556173801422119\n",
      "[Training Epoch 4] Batch 1382, Loss 0.2795390486717224\n",
      "[Training Epoch 4] Batch 1383, Loss 0.2405245453119278\n",
      "[Training Epoch 4] Batch 1384, Loss 0.27244114875793457\n",
      "[Training Epoch 4] Batch 1385, Loss 0.26116839051246643\n",
      "[Training Epoch 4] Batch 1386, Loss 0.22264114022254944\n",
      "[Training Epoch 4] Batch 1387, Loss 0.26422595977783203\n",
      "[Training Epoch 4] Batch 1388, Loss 0.2684369683265686\n",
      "[Training Epoch 4] Batch 1389, Loss 0.25122910737991333\n",
      "[Training Epoch 4] Batch 1390, Loss 0.2711765766143799\n",
      "[Training Epoch 4] Batch 1391, Loss 0.29163873195648193\n",
      "[Training Epoch 4] Batch 1392, Loss 0.2859695255756378\n",
      "[Training Epoch 4] Batch 1393, Loss 0.2611340284347534\n",
      "[Training Epoch 4] Batch 1394, Loss 0.2638745605945587\n",
      "[Training Epoch 4] Batch 1395, Loss 0.29237186908721924\n",
      "[Training Epoch 4] Batch 1396, Loss 0.2618632912635803\n",
      "[Training Epoch 4] Batch 1397, Loss 0.2701657712459564\n",
      "[Training Epoch 4] Batch 1398, Loss 0.2307729721069336\n",
      "[Training Epoch 4] Batch 1399, Loss 0.25153303146362305\n",
      "[Training Epoch 4] Batch 1400, Loss 0.23817026615142822\n",
      "[Training Epoch 4] Batch 1401, Loss 0.24471133947372437\n",
      "[Training Epoch 4] Batch 1402, Loss 0.24386292695999146\n",
      "[Training Epoch 4] Batch 1403, Loss 0.24079157412052155\n",
      "[Training Epoch 4] Batch 1404, Loss 0.2644581198692322\n",
      "[Training Epoch 4] Batch 1405, Loss 0.25546082854270935\n",
      "[Training Epoch 4] Batch 1406, Loss 0.2708233594894409\n",
      "[Training Epoch 4] Batch 1407, Loss 0.2677994668483734\n",
      "[Training Epoch 4] Batch 1408, Loss 0.2479199469089508\n",
      "[Training Epoch 4] Batch 1409, Loss 0.23864614963531494\n",
      "[Training Epoch 4] Batch 1410, Loss 0.2843935489654541\n",
      "[Training Epoch 4] Batch 1411, Loss 0.2824999690055847\n",
      "[Training Epoch 4] Batch 1412, Loss 0.2665449380874634\n",
      "[Training Epoch 4] Batch 1413, Loss 0.23231244087219238\n",
      "[Training Epoch 4] Batch 1414, Loss 0.25106531381607056\n",
      "[Training Epoch 4] Batch 1415, Loss 0.26587775349617004\n",
      "[Training Epoch 4] Batch 1416, Loss 0.27141064405441284\n",
      "[Training Epoch 4] Batch 1417, Loss 0.26826202869415283\n",
      "[Training Epoch 4] Batch 1418, Loss 0.2727459967136383\n",
      "[Training Epoch 4] Batch 1419, Loss 0.26613593101501465\n",
      "[Training Epoch 4] Batch 1420, Loss 0.27650386095046997\n",
      "[Training Epoch 4] Batch 1421, Loss 0.2803407311439514\n",
      "[Training Epoch 4] Batch 1422, Loss 0.29862600564956665\n",
      "[Training Epoch 4] Batch 1423, Loss 0.2556387186050415\n",
      "[Training Epoch 4] Batch 1424, Loss 0.27009233832359314\n",
      "[Training Epoch 4] Batch 1425, Loss 0.2545914053916931\n",
      "[Training Epoch 4] Batch 1426, Loss 0.2744653522968292\n",
      "[Training Epoch 4] Batch 1427, Loss 0.2571457624435425\n",
      "[Training Epoch 4] Batch 1428, Loss 0.309251606464386\n",
      "[Training Epoch 4] Batch 1429, Loss 0.266023725271225\n",
      "[Training Epoch 4] Batch 1430, Loss 0.2692488431930542\n",
      "[Training Epoch 4] Batch 1431, Loss 0.2560880482196808\n",
      "[Training Epoch 4] Batch 1432, Loss 0.23731453716754913\n",
      "[Training Epoch 4] Batch 1433, Loss 0.28645947575569153\n",
      "[Training Epoch 4] Batch 1434, Loss 0.24065932631492615\n",
      "[Training Epoch 4] Batch 1435, Loss 0.2684342563152313\n",
      "[Training Epoch 4] Batch 1436, Loss 0.27794772386550903\n",
      "[Training Epoch 4] Batch 1437, Loss 0.24857503175735474\n",
      "[Training Epoch 4] Batch 1438, Loss 0.24529850482940674\n",
      "[Training Epoch 4] Batch 1439, Loss 0.3070560097694397\n",
      "[Training Epoch 4] Batch 1440, Loss 0.26666703820228577\n",
      "[Training Epoch 4] Batch 1441, Loss 0.26314085721969604\n",
      "[Training Epoch 4] Batch 1442, Loss 0.2662469148635864\n",
      "[Training Epoch 4] Batch 1443, Loss 0.2811127305030823\n",
      "[Training Epoch 4] Batch 1444, Loss 0.2766181230545044\n",
      "[Training Epoch 4] Batch 1445, Loss 0.2520532011985779\n",
      "[Training Epoch 4] Batch 1446, Loss 0.27175021171569824\n",
      "[Training Epoch 4] Batch 1447, Loss 0.2822960317134857\n",
      "[Training Epoch 4] Batch 1448, Loss 0.2535637617111206\n",
      "[Training Epoch 4] Batch 1449, Loss 0.24883949756622314\n",
      "[Training Epoch 4] Batch 1450, Loss 0.26235175132751465\n",
      "[Training Epoch 4] Batch 1451, Loss 0.2689940333366394\n",
      "[Training Epoch 4] Batch 1452, Loss 0.2330099493265152\n",
      "[Training Epoch 4] Batch 1453, Loss 0.2739220857620239\n",
      "[Training Epoch 4] Batch 1454, Loss 0.2759409546852112\n",
      "[Training Epoch 4] Batch 1455, Loss 0.26320844888687134\n",
      "[Training Epoch 4] Batch 1456, Loss 0.28211718797683716\n",
      "[Training Epoch 4] Batch 1457, Loss 0.2894858717918396\n",
      "[Training Epoch 4] Batch 1458, Loss 0.266845703125\n",
      "[Training Epoch 4] Batch 1459, Loss 0.2561988830566406\n",
      "[Training Epoch 4] Batch 1460, Loss 0.2779471278190613\n",
      "[Training Epoch 4] Batch 1461, Loss 0.26268935203552246\n",
      "[Training Epoch 4] Batch 1462, Loss 0.27272388339042664\n",
      "[Training Epoch 4] Batch 1463, Loss 0.2461194097995758\n",
      "[Training Epoch 4] Batch 1464, Loss 0.2676580548286438\n",
      "[Training Epoch 4] Batch 1465, Loss 0.28041571378707886\n",
      "[Training Epoch 4] Batch 1466, Loss 0.28187263011932373\n",
      "[Training Epoch 4] Batch 1467, Loss 0.25036630034446716\n",
      "[Training Epoch 4] Batch 1468, Loss 0.2793880105018616\n",
      "[Training Epoch 4] Batch 1469, Loss 0.25607818365097046\n",
      "[Training Epoch 4] Batch 1470, Loss 0.24592740833759308\n",
      "[Training Epoch 4] Batch 1471, Loss 0.27994483709335327\n",
      "[Training Epoch 4] Batch 1472, Loss 0.28004276752471924\n",
      "[Training Epoch 4] Batch 1473, Loss 0.2518152594566345\n",
      "[Training Epoch 4] Batch 1474, Loss 0.2639046311378479\n",
      "[Training Epoch 4] Batch 1475, Loss 0.28759127855300903\n",
      "[Training Epoch 4] Batch 1476, Loss 0.25314861536026\n",
      "[Training Epoch 4] Batch 1477, Loss 0.24622707068920135\n",
      "[Training Epoch 4] Batch 1478, Loss 0.27329587936401367\n",
      "[Training Epoch 4] Batch 1479, Loss 0.2548561096191406\n",
      "[Training Epoch 4] Batch 1480, Loss 0.26038920879364014\n",
      "[Training Epoch 4] Batch 1481, Loss 0.2689250111579895\n",
      "[Training Epoch 4] Batch 1482, Loss 0.28636738657951355\n",
      "[Training Epoch 4] Batch 1483, Loss 0.2604514956474304\n",
      "[Training Epoch 4] Batch 1484, Loss 0.2749655544757843\n",
      "[Training Epoch 4] Batch 1485, Loss 0.24022136628627777\n",
      "[Training Epoch 4] Batch 1486, Loss 0.2777923047542572\n",
      "[Training Epoch 4] Batch 1487, Loss 0.2871887683868408\n",
      "[Training Epoch 4] Batch 1488, Loss 0.2725600302219391\n",
      "[Training Epoch 4] Batch 1489, Loss 0.29556360840797424\n",
      "[Training Epoch 4] Batch 1490, Loss 0.2540580630302429\n",
      "[Training Epoch 4] Batch 1491, Loss 0.2654256522655487\n",
      "[Training Epoch 4] Batch 1492, Loss 0.2884727120399475\n",
      "[Training Epoch 4] Batch 1493, Loss 0.2671045958995819\n",
      "[Training Epoch 4] Batch 1494, Loss 0.27685046195983887\n",
      "[Training Epoch 4] Batch 1495, Loss 0.26712048053741455\n",
      "[Training Epoch 4] Batch 1496, Loss 0.2750356197357178\n",
      "[Training Epoch 4] Batch 1497, Loss 0.25781166553497314\n",
      "[Training Epoch 4] Batch 1498, Loss 0.25009921193122864\n",
      "[Training Epoch 4] Batch 1499, Loss 0.2635854482650757\n",
      "[Training Epoch 4] Batch 1500, Loss 0.2882723808288574\n",
      "[Training Epoch 4] Batch 1501, Loss 0.2967781126499176\n",
      "[Training Epoch 4] Batch 1502, Loss 0.2594689130783081\n",
      "[Training Epoch 4] Batch 1503, Loss 0.2746603488922119\n",
      "[Training Epoch 4] Batch 1504, Loss 0.2565130889415741\n",
      "[Training Epoch 4] Batch 1505, Loss 0.2873722314834595\n",
      "[Training Epoch 4] Batch 1506, Loss 0.2964428663253784\n",
      "[Training Epoch 4] Batch 1507, Loss 0.26604703068733215\n",
      "[Training Epoch 4] Batch 1508, Loss 0.26388829946517944\n",
      "[Training Epoch 4] Batch 1509, Loss 0.24888333678245544\n",
      "[Training Epoch 4] Batch 1510, Loss 0.2607647776603699\n",
      "[Training Epoch 4] Batch 1511, Loss 0.2811828851699829\n",
      "[Training Epoch 4] Batch 1512, Loss 0.27014440298080444\n",
      "[Training Epoch 4] Batch 1513, Loss 0.25545138120651245\n",
      "[Training Epoch 4] Batch 1514, Loss 0.2851303219795227\n",
      "[Training Epoch 4] Batch 1515, Loss 0.26422595977783203\n",
      "[Training Epoch 4] Batch 1516, Loss 0.26807644963264465\n",
      "[Training Epoch 4] Batch 1517, Loss 0.2739967703819275\n",
      "[Training Epoch 4] Batch 1518, Loss 0.2425747662782669\n",
      "[Training Epoch 4] Batch 1519, Loss 0.2449551224708557\n",
      "[Training Epoch 4] Batch 1520, Loss 0.26990702748298645\n",
      "[Training Epoch 4] Batch 1521, Loss 0.24043887853622437\n",
      "[Training Epoch 4] Batch 1522, Loss 0.26297152042388916\n",
      "[Training Epoch 4] Batch 1523, Loss 0.26890307664871216\n",
      "[Training Epoch 4] Batch 1524, Loss 0.2935168147087097\n",
      "[Training Epoch 4] Batch 1525, Loss 0.26242372393608093\n",
      "[Training Epoch 4] Batch 1526, Loss 0.2616000473499298\n",
      "[Training Epoch 4] Batch 1527, Loss 0.2745882272720337\n",
      "[Training Epoch 4] Batch 1528, Loss 0.25558191537857056\n",
      "[Training Epoch 4] Batch 1529, Loss 0.2611449360847473\n",
      "[Training Epoch 4] Batch 1530, Loss 0.2699885964393616\n",
      "[Training Epoch 4] Batch 1531, Loss 0.28203731775283813\n",
      "[Training Epoch 4] Batch 1532, Loss 0.24283018708229065\n",
      "[Training Epoch 4] Batch 1533, Loss 0.2656690180301666\n",
      "[Training Epoch 4] Batch 1534, Loss 0.30364716053009033\n",
      "[Training Epoch 4] Batch 1535, Loss 0.2962159812450409\n",
      "[Training Epoch 4] Batch 1536, Loss 0.282975435256958\n",
      "[Training Epoch 4] Batch 1537, Loss 0.25337734818458557\n",
      "[Training Epoch 4] Batch 1538, Loss 0.2512769401073456\n",
      "[Training Epoch 4] Batch 1539, Loss 0.2936372756958008\n",
      "[Training Epoch 4] Batch 1540, Loss 0.2412460595369339\n",
      "[Training Epoch 4] Batch 1541, Loss 0.2501624524593353\n",
      "[Training Epoch 4] Batch 1542, Loss 0.28212615847587585\n",
      "[Training Epoch 4] Batch 1543, Loss 0.26478806138038635\n",
      "[Training Epoch 4] Batch 1544, Loss 0.2628500759601593\n",
      "[Training Epoch 4] Batch 1545, Loss 0.2673548758029938\n",
      "[Training Epoch 4] Batch 1546, Loss 0.28254908323287964\n",
      "[Training Epoch 4] Batch 1547, Loss 0.2568093538284302\n",
      "[Training Epoch 4] Batch 1548, Loss 0.262187123298645\n",
      "[Training Epoch 4] Batch 1549, Loss 0.2619818150997162\n",
      "[Training Epoch 4] Batch 1550, Loss 0.2514750063419342\n",
      "[Training Epoch 4] Batch 1551, Loss 0.26796582341194153\n",
      "[Training Epoch 4] Batch 1552, Loss 0.26440131664276123\n",
      "[Training Epoch 4] Batch 1553, Loss 0.27174049615859985\n",
      "[Training Epoch 4] Batch 1554, Loss 0.23556333780288696\n",
      "[Training Epoch 4] Batch 1555, Loss 0.2619872987270355\n",
      "[Training Epoch 4] Batch 1556, Loss 0.2856905460357666\n",
      "[Training Epoch 4] Batch 1557, Loss 0.26755189895629883\n",
      "[Training Epoch 4] Batch 1558, Loss 0.23871907591819763\n",
      "[Training Epoch 4] Batch 1559, Loss 0.24630066752433777\n",
      "[Training Epoch 4] Batch 1560, Loss 0.28428006172180176\n",
      "[Training Epoch 4] Batch 1561, Loss 0.26581743359565735\n",
      "[Training Epoch 4] Batch 1562, Loss 0.22419139742851257\n",
      "[Training Epoch 4] Batch 1563, Loss 0.2583581507205963\n",
      "[Training Epoch 4] Batch 1564, Loss 0.25266823172569275\n",
      "[Training Epoch 4] Batch 1565, Loss 0.27857697010040283\n",
      "[Training Epoch 4] Batch 1566, Loss 0.26402777433395386\n",
      "[Training Epoch 4] Batch 1567, Loss 0.2565455734729767\n",
      "[Training Epoch 4] Batch 1568, Loss 0.2872374653816223\n",
      "[Training Epoch 4] Batch 1569, Loss 0.27716273069381714\n",
      "[Training Epoch 4] Batch 1570, Loss 0.2590254545211792\n",
      "[Training Epoch 4] Batch 1571, Loss 0.2714754641056061\n",
      "[Training Epoch 4] Batch 1572, Loss 0.25046712160110474\n",
      "[Training Epoch 4] Batch 1573, Loss 0.2483689785003662\n",
      "[Training Epoch 4] Batch 1574, Loss 0.262699693441391\n",
      "[Training Epoch 4] Batch 1575, Loss 0.2795009911060333\n",
      "[Training Epoch 4] Batch 1576, Loss 0.2678145170211792\n",
      "[Training Epoch 4] Batch 1577, Loss 0.24993321299552917\n",
      "[Training Epoch 4] Batch 1578, Loss 0.23529502749443054\n",
      "[Training Epoch 4] Batch 1579, Loss 0.2595512270927429\n",
      "[Training Epoch 4] Batch 1580, Loss 0.25459006428718567\n",
      "[Training Epoch 4] Batch 1581, Loss 0.292708158493042\n",
      "[Training Epoch 4] Batch 1582, Loss 0.27862706780433655\n",
      "[Training Epoch 4] Batch 1583, Loss 0.25994351506233215\n",
      "[Training Epoch 4] Batch 1584, Loss 0.2743320167064667\n",
      "[Training Epoch 4] Batch 1585, Loss 0.28248754143714905\n",
      "[Training Epoch 4] Batch 1586, Loss 0.26491081714630127\n",
      "[Training Epoch 4] Batch 1587, Loss 0.27067530155181885\n",
      "[Training Epoch 4] Batch 1588, Loss 0.23746401071548462\n",
      "[Training Epoch 4] Batch 1589, Loss 0.2635698914527893\n",
      "[Training Epoch 4] Batch 1590, Loss 0.26158976554870605\n",
      "[Training Epoch 4] Batch 1591, Loss 0.26066356897354126\n",
      "[Training Epoch 4] Batch 1592, Loss 0.24482810497283936\n",
      "[Training Epoch 4] Batch 1593, Loss 0.24064411222934723\n",
      "[Training Epoch 4] Batch 1594, Loss 0.2673044800758362\n",
      "[Training Epoch 4] Batch 1595, Loss 0.25720494985580444\n",
      "[Training Epoch 4] Batch 1596, Loss 0.279217004776001\n",
      "[Training Epoch 4] Batch 1597, Loss 0.28083622455596924\n",
      "[Training Epoch 4] Batch 1598, Loss 0.27660536766052246\n",
      "[Training Epoch 4] Batch 1599, Loss 0.26835256814956665\n",
      "[Training Epoch 4] Batch 1600, Loss 0.24476654827594757\n",
      "[Training Epoch 4] Batch 1601, Loss 0.2608179450035095\n",
      "[Training Epoch 4] Batch 1602, Loss 0.2784721851348877\n",
      "[Training Epoch 4] Batch 1603, Loss 0.24830207228660583\n",
      "[Training Epoch 4] Batch 1604, Loss 0.23555351793766022\n",
      "[Training Epoch 4] Batch 1605, Loss 0.26815932989120483\n",
      "[Training Epoch 4] Batch 1606, Loss 0.22872760891914368\n",
      "[Training Epoch 4] Batch 1607, Loss 0.22512997686862946\n",
      "[Training Epoch 4] Batch 1608, Loss 0.246072918176651\n",
      "[Training Epoch 4] Batch 1609, Loss 0.28660857677459717\n",
      "[Training Epoch 4] Batch 1610, Loss 0.28208810091018677\n",
      "[Training Epoch 4] Batch 1611, Loss 0.2660562992095947\n",
      "[Training Epoch 4] Batch 1612, Loss 0.2605854868888855\n",
      "[Training Epoch 4] Batch 1613, Loss 0.22932636737823486\n",
      "[Training Epoch 4] Batch 1614, Loss 0.30268871784210205\n",
      "[Training Epoch 4] Batch 1615, Loss 0.2622770667076111\n",
      "[Training Epoch 4] Batch 1616, Loss 0.2890859842300415\n",
      "[Training Epoch 4] Batch 1617, Loss 0.23318013548851013\n",
      "[Training Epoch 4] Batch 1618, Loss 0.26638859510421753\n",
      "[Training Epoch 4] Batch 1619, Loss 0.2796458899974823\n",
      "[Training Epoch 4] Batch 1620, Loss 0.25584524869918823\n",
      "[Training Epoch 4] Batch 1621, Loss 0.25666946172714233\n",
      "[Training Epoch 4] Batch 1622, Loss 0.24627861380577087\n",
      "[Training Epoch 4] Batch 1623, Loss 0.24906647205352783\n",
      "[Training Epoch 4] Batch 1624, Loss 0.28246334195137024\n",
      "[Training Epoch 4] Batch 1625, Loss 0.2690396010875702\n",
      "[Training Epoch 4] Batch 1626, Loss 0.2847006320953369\n",
      "[Training Epoch 4] Batch 1627, Loss 0.25987017154693604\n",
      "[Training Epoch 4] Batch 1628, Loss 0.27649223804473877\n",
      "[Training Epoch 4] Batch 1629, Loss 0.2556510269641876\n",
      "[Training Epoch 4] Batch 1630, Loss 0.26534026861190796\n",
      "[Training Epoch 4] Batch 1631, Loss 0.2908080816268921\n",
      "[Training Epoch 4] Batch 1632, Loss 0.25028571486473083\n",
      "[Training Epoch 4] Batch 1633, Loss 0.2890748381614685\n",
      "[Training Epoch 4] Batch 1634, Loss 0.29223620891571045\n",
      "[Training Epoch 4] Batch 1635, Loss 0.2528224587440491\n",
      "[Training Epoch 4] Batch 1636, Loss 0.24119865894317627\n",
      "[Training Epoch 4] Batch 1637, Loss 0.2745513916015625\n",
      "[Training Epoch 4] Batch 1638, Loss 0.25136274099349976\n",
      "[Training Epoch 4] Batch 1639, Loss 0.2741689682006836\n",
      "[Training Epoch 4] Batch 1640, Loss 0.23033393919467926\n",
      "[Training Epoch 4] Batch 1641, Loss 0.2665930390357971\n",
      "[Training Epoch 4] Batch 1642, Loss 0.275363028049469\n",
      "[Training Epoch 4] Batch 1643, Loss 0.27176523208618164\n",
      "[Training Epoch 4] Batch 1644, Loss 0.27476322650909424\n",
      "[Training Epoch 4] Batch 1645, Loss 0.2498610019683838\n",
      "[Training Epoch 4] Batch 1646, Loss 0.271389365196228\n",
      "[Training Epoch 4] Batch 1647, Loss 0.2819415330886841\n",
      "[Training Epoch 4] Batch 1648, Loss 0.2853710651397705\n",
      "[Training Epoch 4] Batch 1649, Loss 0.2767069339752197\n",
      "[Training Epoch 4] Batch 1650, Loss 0.27206340432167053\n",
      "[Training Epoch 4] Batch 1651, Loss 0.28146982192993164\n",
      "[Training Epoch 4] Batch 1652, Loss 0.3077583312988281\n",
      "[Training Epoch 4] Batch 1653, Loss 0.2373386174440384\n",
      "[Training Epoch 4] Batch 1654, Loss 0.26048779487609863\n",
      "[Training Epoch 4] Batch 1655, Loss 0.25375640392303467\n",
      "[Training Epoch 4] Batch 1656, Loss 0.278888463973999\n",
      "[Training Epoch 4] Batch 1657, Loss 0.25051069259643555\n",
      "[Training Epoch 4] Batch 1658, Loss 0.26489391922950745\n",
      "[Training Epoch 4] Batch 1659, Loss 0.27584901452064514\n",
      "[Training Epoch 4] Batch 1660, Loss 0.27379485964775085\n",
      "[Training Epoch 4] Batch 1661, Loss 0.2920699119567871\n",
      "[Training Epoch 4] Batch 1662, Loss 0.2664085030555725\n",
      "[Training Epoch 4] Batch 1663, Loss 0.2540067732334137\n",
      "[Training Epoch 4] Batch 1664, Loss 0.2571756839752197\n",
      "[Training Epoch 4] Batch 1665, Loss 0.2923440933227539\n",
      "[Training Epoch 4] Batch 1666, Loss 0.24483489990234375\n",
      "[Training Epoch 4] Batch 1667, Loss 0.2619348168373108\n",
      "[Training Epoch 4] Batch 1668, Loss 0.2420516312122345\n",
      "[Training Epoch 4] Batch 1669, Loss 0.27344071865081787\n",
      "[Training Epoch 4] Batch 1670, Loss 0.2569229006767273\n",
      "[Training Epoch 4] Batch 1671, Loss 0.25828635692596436\n",
      "[Training Epoch 4] Batch 1672, Loss 0.27450406551361084\n",
      "[Training Epoch 4] Batch 1673, Loss 0.2605712413787842\n",
      "[Training Epoch 4] Batch 1674, Loss 0.27504780888557434\n",
      "[Training Epoch 4] Batch 1675, Loss 0.2560821771621704\n",
      "[Training Epoch 4] Batch 1676, Loss 0.28864794969558716\n",
      "[Training Epoch 4] Batch 1677, Loss 0.29254263639450073\n",
      "[Training Epoch 4] Batch 1678, Loss 0.24639205634593964\n",
      "[Training Epoch 4] Batch 1679, Loss 0.27354103326797485\n",
      "[Training Epoch 4] Batch 1680, Loss 0.25805148482322693\n",
      "[Training Epoch 4] Batch 1681, Loss 0.2664504647254944\n",
      "[Training Epoch 4] Batch 1682, Loss 0.2661316394805908\n",
      "[Training Epoch 4] Batch 1683, Loss 0.2757173478603363\n",
      "[Training Epoch 4] Batch 1684, Loss 0.26190948486328125\n",
      "[Training Epoch 4] Batch 1685, Loss 0.28338003158569336\n",
      "[Training Epoch 4] Batch 1686, Loss 0.28028154373168945\n",
      "[Training Epoch 4] Batch 1687, Loss 0.2700233459472656\n",
      "[Training Epoch 4] Batch 1688, Loss 0.2831166982650757\n",
      "[Training Epoch 4] Batch 1689, Loss 0.2601367235183716\n",
      "[Training Epoch 4] Batch 1690, Loss 0.2443556785583496\n",
      "[Training Epoch 4] Batch 1691, Loss 0.2557358741760254\n",
      "[Training Epoch 4] Batch 1692, Loss 0.2761503756046295\n",
      "[Training Epoch 4] Batch 1693, Loss 0.23299455642700195\n",
      "[Training Epoch 4] Batch 1694, Loss 0.25723618268966675\n",
      "[Training Epoch 4] Batch 1695, Loss 0.26579540967941284\n",
      "[Training Epoch 4] Batch 1696, Loss 0.2755200266838074\n",
      "[Training Epoch 4] Batch 1697, Loss 0.26931196451187134\n",
      "[Training Epoch 4] Batch 1698, Loss 0.23356682062149048\n",
      "[Training Epoch 4] Batch 1699, Loss 0.25979751348495483\n",
      "[Training Epoch 4] Batch 1700, Loss 0.253750741481781\n",
      "[Training Epoch 4] Batch 1701, Loss 0.2608067989349365\n",
      "[Training Epoch 4] Batch 1702, Loss 0.2923150062561035\n",
      "[Training Epoch 4] Batch 1703, Loss 0.27410888671875\n",
      "[Training Epoch 4] Batch 1704, Loss 0.2651497721672058\n",
      "[Training Epoch 4] Batch 1705, Loss 0.24281790852546692\n",
      "[Training Epoch 4] Batch 1706, Loss 0.27123695611953735\n",
      "[Training Epoch 4] Batch 1707, Loss 0.2858334183692932\n",
      "[Training Epoch 4] Batch 1708, Loss 0.24477240443229675\n",
      "[Training Epoch 4] Batch 1709, Loss 0.2966671586036682\n",
      "[Training Epoch 4] Batch 1710, Loss 0.2581809163093567\n",
      "[Training Epoch 4] Batch 1711, Loss 0.26251137256622314\n",
      "[Training Epoch 4] Batch 1712, Loss 0.278817743062973\n",
      "[Training Epoch 4] Batch 1713, Loss 0.26804521679878235\n",
      "[Training Epoch 4] Batch 1714, Loss 0.26516976952552795\n",
      "[Training Epoch 4] Batch 1715, Loss 0.2494049370288849\n",
      "[Training Epoch 4] Batch 1716, Loss 0.24745409190654755\n",
      "[Training Epoch 4] Batch 1717, Loss 0.26805055141448975\n",
      "[Training Epoch 4] Batch 1718, Loss 0.28353556990623474\n",
      "[Training Epoch 4] Batch 1719, Loss 0.2858453691005707\n",
      "[Training Epoch 4] Batch 1720, Loss 0.24458244442939758\n",
      "[Training Epoch 4] Batch 1721, Loss 0.29211539030075073\n",
      "[Training Epoch 4] Batch 1722, Loss 0.281460702419281\n",
      "[Training Epoch 4] Batch 1723, Loss 0.25037533044815063\n",
      "[Training Epoch 4] Batch 1724, Loss 0.2664560079574585\n",
      "[Training Epoch 4] Batch 1725, Loss 0.2680222988128662\n",
      "[Training Epoch 4] Batch 1726, Loss 0.28285354375839233\n",
      "[Training Epoch 4] Batch 1727, Loss 0.26620471477508545\n",
      "[Training Epoch 4] Batch 1728, Loss 0.25824159383773804\n",
      "[Training Epoch 4] Batch 1729, Loss 0.2529216706752777\n",
      "[Training Epoch 4] Batch 1730, Loss 0.2771542966365814\n",
      "[Training Epoch 4] Batch 1731, Loss 0.25084197521209717\n",
      "[Training Epoch 4] Batch 1732, Loss 0.26784300804138184\n",
      "[Training Epoch 4] Batch 1733, Loss 0.29095399379730225\n",
      "[Training Epoch 4] Batch 1734, Loss 0.2768080532550812\n",
      "[Training Epoch 4] Batch 1735, Loss 0.29470810294151306\n",
      "[Training Epoch 4] Batch 1736, Loss 0.27674099802970886\n",
      "[Training Epoch 4] Batch 1737, Loss 0.2522630989551544\n",
      "[Training Epoch 4] Batch 1738, Loss 0.2519308030605316\n",
      "[Training Epoch 4] Batch 1739, Loss 0.2783530354499817\n",
      "[Training Epoch 4] Batch 1740, Loss 0.2775232195854187\n",
      "[Training Epoch 4] Batch 1741, Loss 0.2635442912578583\n",
      "[Training Epoch 4] Batch 1742, Loss 0.24963903427124023\n",
      "[Training Epoch 4] Batch 1743, Loss 0.2527928650379181\n",
      "[Training Epoch 4] Batch 1744, Loss 0.3071107566356659\n",
      "[Training Epoch 4] Batch 1745, Loss 0.28398633003234863\n",
      "[Training Epoch 4] Batch 1746, Loss 0.2627747654914856\n",
      "[Training Epoch 4] Batch 1747, Loss 0.24820107221603394\n",
      "[Training Epoch 4] Batch 1748, Loss 0.2306388020515442\n",
      "[Training Epoch 4] Batch 1749, Loss 0.2539093494415283\n",
      "[Training Epoch 4] Batch 1750, Loss 0.23950453102588654\n",
      "[Training Epoch 4] Batch 1751, Loss 0.2755383551120758\n",
      "[Training Epoch 4] Batch 1752, Loss 0.2687552571296692\n",
      "[Training Epoch 4] Batch 1753, Loss 0.23421725630760193\n",
      "[Training Epoch 4] Batch 1754, Loss 0.27259212732315063\n",
      "[Training Epoch 4] Batch 1755, Loss 0.25740012526512146\n",
      "[Training Epoch 4] Batch 1756, Loss 0.31252700090408325\n",
      "[Training Epoch 4] Batch 1757, Loss 0.25964435935020447\n",
      "[Training Epoch 4] Batch 1758, Loss 0.21768403053283691\n",
      "[Training Epoch 4] Batch 1759, Loss 0.2600354552268982\n",
      "[Training Epoch 4] Batch 1760, Loss 0.26792505383491516\n",
      "[Training Epoch 4] Batch 1761, Loss 0.26331251859664917\n",
      "[Training Epoch 4] Batch 1762, Loss 0.300110787153244\n",
      "[Training Epoch 4] Batch 1763, Loss 0.24759942293167114\n",
      "[Training Epoch 4] Batch 1764, Loss 0.2687264680862427\n",
      "[Training Epoch 4] Batch 1765, Loss 0.23448291420936584\n",
      "[Training Epoch 4] Batch 1766, Loss 0.26489514112472534\n",
      "[Training Epoch 4] Batch 1767, Loss 0.25110095739364624\n",
      "[Training Epoch 4] Batch 1768, Loss 0.26808395981788635\n",
      "[Training Epoch 4] Batch 1769, Loss 0.29475951194763184\n",
      "[Training Epoch 4] Batch 1770, Loss 0.2538299262523651\n",
      "[Training Epoch 4] Batch 1771, Loss 0.27870333194732666\n",
      "[Training Epoch 4] Batch 1772, Loss 0.27197176218032837\n",
      "[Training Epoch 4] Batch 1773, Loss 0.22820836305618286\n",
      "[Training Epoch 4] Batch 1774, Loss 0.2831000089645386\n",
      "[Training Epoch 4] Batch 1775, Loss 0.2527836561203003\n",
      "[Training Epoch 4] Batch 1776, Loss 0.24405458569526672\n",
      "[Training Epoch 4] Batch 1777, Loss 0.2724216878414154\n",
      "[Training Epoch 4] Batch 1778, Loss 0.25401467084884644\n",
      "[Training Epoch 4] Batch 1779, Loss 0.2340414822101593\n",
      "[Training Epoch 4] Batch 1780, Loss 0.28360018134117126\n",
      "[Training Epoch 4] Batch 1781, Loss 0.2648544907569885\n",
      "[Training Epoch 4] Batch 1782, Loss 0.25017571449279785\n",
      "[Training Epoch 4] Batch 1783, Loss 0.24896615743637085\n",
      "[Training Epoch 4] Batch 1784, Loss 0.2544962167739868\n",
      "[Training Epoch 4] Batch 1785, Loss 0.23840776085853577\n",
      "[Training Epoch 4] Batch 1786, Loss 0.249582439661026\n",
      "[Training Epoch 4] Batch 1787, Loss 0.2765989899635315\n",
      "[Training Epoch 4] Batch 1788, Loss 0.25987544655799866\n",
      "[Training Epoch 4] Batch 1789, Loss 0.28201204538345337\n",
      "[Training Epoch 4] Batch 1790, Loss 0.2755495309829712\n",
      "[Training Epoch 4] Batch 1791, Loss 0.2512111961841583\n",
      "[Training Epoch 4] Batch 1792, Loss 0.24903684854507446\n",
      "[Training Epoch 4] Batch 1793, Loss 0.2402343451976776\n",
      "[Training Epoch 4] Batch 1794, Loss 0.262445330619812\n",
      "[Training Epoch 4] Batch 1795, Loss 0.2685243785381317\n",
      "[Training Epoch 4] Batch 1796, Loss 0.2327490597963333\n",
      "[Training Epoch 4] Batch 1797, Loss 0.2740578353404999\n",
      "[Training Epoch 4] Batch 1798, Loss 0.23076894879341125\n",
      "[Training Epoch 4] Batch 1799, Loss 0.2457999736070633\n",
      "[Training Epoch 4] Batch 1800, Loss 0.2793196439743042\n",
      "[Training Epoch 4] Batch 1801, Loss 0.2501152753829956\n",
      "[Training Epoch 4] Batch 1802, Loss 0.255952388048172\n",
      "[Training Epoch 4] Batch 1803, Loss 0.2624008059501648\n",
      "[Training Epoch 4] Batch 1804, Loss 0.23432426154613495\n",
      "[Training Epoch 4] Batch 1805, Loss 0.24298059940338135\n",
      "[Training Epoch 4] Batch 1806, Loss 0.26820772886276245\n",
      "[Training Epoch 4] Batch 1807, Loss 0.2609596848487854\n",
      "[Training Epoch 4] Batch 1808, Loss 0.2528097629547119\n",
      "[Training Epoch 4] Batch 1809, Loss 0.27092045545578003\n",
      "[Training Epoch 4] Batch 1810, Loss 0.26544445753097534\n",
      "[Training Epoch 4] Batch 1811, Loss 0.2689732313156128\n",
      "[Training Epoch 4] Batch 1812, Loss 0.25278687477111816\n",
      "[Training Epoch 4] Batch 1813, Loss 0.2743656039237976\n",
      "[Training Epoch 4] Batch 1814, Loss 0.2786121070384979\n",
      "[Training Epoch 4] Batch 1815, Loss 0.2866359353065491\n",
      "[Training Epoch 4] Batch 1816, Loss 0.2590673565864563\n",
      "[Training Epoch 4] Batch 1817, Loss 0.24737150967121124\n",
      "[Training Epoch 4] Batch 1818, Loss 0.2528248429298401\n",
      "[Training Epoch 4] Batch 1819, Loss 0.25033313035964966\n",
      "[Training Epoch 4] Batch 1820, Loss 0.2809710204601288\n",
      "[Training Epoch 4] Batch 1821, Loss 0.28677791357040405\n",
      "[Training Epoch 4] Batch 1822, Loss 0.27657049894332886\n",
      "[Training Epoch 4] Batch 1823, Loss 0.2730773091316223\n",
      "[Training Epoch 4] Batch 1824, Loss 0.2833797037601471\n",
      "[Training Epoch 4] Batch 1825, Loss 0.27440255880355835\n",
      "[Training Epoch 4] Batch 1826, Loss 0.2860925495624542\n",
      "[Training Epoch 4] Batch 1827, Loss 0.2628825604915619\n",
      "[Training Epoch 4] Batch 1828, Loss 0.25446009635925293\n",
      "[Training Epoch 4] Batch 1829, Loss 0.2774342894554138\n",
      "[Training Epoch 4] Batch 1830, Loss 0.2704361081123352\n",
      "[Training Epoch 4] Batch 1831, Loss 0.261817991733551\n",
      "[Training Epoch 4] Batch 1832, Loss 0.26237553358078003\n",
      "[Training Epoch 4] Batch 1833, Loss 0.28001680970191956\n",
      "[Training Epoch 4] Batch 1834, Loss 0.25622159242630005\n",
      "[Training Epoch 4] Batch 1835, Loss 0.25454607605934143\n",
      "[Training Epoch 4] Batch 1836, Loss 0.2645837664604187\n",
      "[Training Epoch 4] Batch 1837, Loss 0.25871551036834717\n",
      "[Training Epoch 4] Batch 1838, Loss 0.28774434328079224\n",
      "[Training Epoch 4] Batch 1839, Loss 0.2710868716239929\n",
      "[Training Epoch 4] Batch 1840, Loss 0.26386749744415283\n",
      "[Training Epoch 4] Batch 1841, Loss 0.26203832030296326\n",
      "[Training Epoch 4] Batch 1842, Loss 0.2686682939529419\n",
      "[Training Epoch 4] Batch 1843, Loss 0.27373605966567993\n",
      "[Training Epoch 4] Batch 1844, Loss 0.2906354069709778\n",
      "[Training Epoch 4] Batch 1845, Loss 0.26595988869667053\n",
      "[Training Epoch 4] Batch 1846, Loss 0.2758823037147522\n",
      "[Training Epoch 4] Batch 1847, Loss 0.3046172857284546\n",
      "[Training Epoch 4] Batch 1848, Loss 0.2638557255268097\n",
      "[Training Epoch 4] Batch 1849, Loss 0.29244542121887207\n",
      "[Training Epoch 4] Batch 1850, Loss 0.2791377305984497\n",
      "[Training Epoch 4] Batch 1851, Loss 0.22001518309116364\n",
      "[Training Epoch 4] Batch 1852, Loss 0.25528842210769653\n",
      "[Training Epoch 4] Batch 1853, Loss 0.2717025578022003\n",
      "[Training Epoch 4] Batch 1854, Loss 0.29577794671058655\n",
      "[Training Epoch 4] Batch 1855, Loss 0.26610249280929565\n",
      "[Training Epoch 4] Batch 1856, Loss 0.26731836795806885\n",
      "[Training Epoch 4] Batch 1857, Loss 0.2526693046092987\n",
      "[Training Epoch 4] Batch 1858, Loss 0.2765457034111023\n",
      "[Training Epoch 4] Batch 1859, Loss 0.2497197389602661\n",
      "[Training Epoch 4] Batch 1860, Loss 0.26691728830337524\n",
      "[Training Epoch 4] Batch 1861, Loss 0.2464962601661682\n",
      "[Training Epoch 4] Batch 1862, Loss 0.264028936624527\n",
      "[Training Epoch 4] Batch 1863, Loss 0.24361184239387512\n",
      "[Training Epoch 4] Batch 1864, Loss 0.2562578320503235\n",
      "[Training Epoch 4] Batch 1865, Loss 0.26950085163116455\n",
      "[Training Epoch 4] Batch 1866, Loss 0.27349787950515747\n",
      "[Training Epoch 4] Batch 1867, Loss 0.25992557406425476\n",
      "[Training Epoch 4] Batch 1868, Loss 0.25439268350601196\n",
      "[Training Epoch 4] Batch 1869, Loss 0.26265332102775574\n",
      "[Training Epoch 4] Batch 1870, Loss 0.28684738278388977\n",
      "[Training Epoch 4] Batch 1871, Loss 0.24105125665664673\n",
      "[Training Epoch 4] Batch 1872, Loss 0.24512536823749542\n",
      "[Training Epoch 4] Batch 1873, Loss 0.29196277260780334\n",
      "[Training Epoch 4] Batch 1874, Loss 0.2904658913612366\n",
      "[Training Epoch 4] Batch 1875, Loss 0.23089562356472015\n",
      "[Training Epoch 4] Batch 1876, Loss 0.24015626311302185\n",
      "[Training Epoch 4] Batch 1877, Loss 0.28079158067703247\n",
      "[Training Epoch 4] Batch 1878, Loss 0.2871643900871277\n",
      "[Training Epoch 4] Batch 1879, Loss 0.27216827869415283\n",
      "[Training Epoch 4] Batch 1880, Loss 0.2831152081489563\n",
      "[Training Epoch 4] Batch 1881, Loss 0.23954010009765625\n",
      "[Training Epoch 4] Batch 1882, Loss 0.2506200969219208\n",
      "[Training Epoch 4] Batch 1883, Loss 0.25123634934425354\n",
      "[Training Epoch 4] Batch 1884, Loss 0.27063095569610596\n",
      "[Training Epoch 4] Batch 1885, Loss 0.24336564540863037\n",
      "[Training Epoch 4] Batch 1886, Loss 0.26365330815315247\n",
      "[Training Epoch 4] Batch 1887, Loss 0.2516438961029053\n",
      "[Training Epoch 4] Batch 1888, Loss 0.28600776195526123\n",
      "[Training Epoch 4] Batch 1889, Loss 0.2979740500450134\n",
      "[Training Epoch 4] Batch 1890, Loss 0.25365355610847473\n",
      "[Training Epoch 4] Batch 1891, Loss 0.2789263427257538\n",
      "[Training Epoch 4] Batch 1892, Loss 0.28071731328964233\n",
      "[Training Epoch 4] Batch 1893, Loss 0.27631378173828125\n",
      "[Training Epoch 4] Batch 1894, Loss 0.2978048324584961\n",
      "[Training Epoch 4] Batch 1895, Loss 0.2820553183555603\n",
      "[Training Epoch 4] Batch 1896, Loss 0.25699082016944885\n",
      "[Training Epoch 4] Batch 1897, Loss 0.26598256826400757\n",
      "[Training Epoch 4] Batch 1898, Loss 0.25818580389022827\n",
      "[Training Epoch 4] Batch 1899, Loss 0.2776227593421936\n",
      "[Training Epoch 4] Batch 1900, Loss 0.2757989168167114\n",
      "[Training Epoch 4] Batch 1901, Loss 0.27746498584747314\n",
      "[Training Epoch 4] Batch 1902, Loss 0.25573617219924927\n",
      "[Training Epoch 4] Batch 1903, Loss 0.24461984634399414\n",
      "[Training Epoch 4] Batch 1904, Loss 0.2693594694137573\n",
      "[Training Epoch 4] Batch 1905, Loss 0.28880906105041504\n",
      "[Training Epoch 4] Batch 1906, Loss 0.27562153339385986\n",
      "[Training Epoch 4] Batch 1907, Loss 0.24613460898399353\n",
      "[Training Epoch 4] Batch 1908, Loss 0.24793027341365814\n",
      "[Training Epoch 4] Batch 1909, Loss 0.25412049889564514\n",
      "[Training Epoch 4] Batch 1910, Loss 0.256173700094223\n",
      "[Training Epoch 4] Batch 1911, Loss 0.24747858941555023\n",
      "[Training Epoch 4] Batch 1912, Loss 0.26544082164764404\n",
      "[Training Epoch 4] Batch 1913, Loss 0.2676658034324646\n",
      "[Training Epoch 4] Batch 1914, Loss 0.2938879430294037\n",
      "[Training Epoch 4] Batch 1915, Loss 0.2819005250930786\n",
      "[Training Epoch 4] Batch 1916, Loss 0.27405476570129395\n",
      "[Training Epoch 4] Batch 1917, Loss 0.252625435590744\n",
      "[Training Epoch 4] Batch 1918, Loss 0.251609742641449\n",
      "[Training Epoch 4] Batch 1919, Loss 0.25415417551994324\n",
      "[Training Epoch 4] Batch 1920, Loss 0.26115262508392334\n",
      "[Training Epoch 4] Batch 1921, Loss 0.25677555799484253\n",
      "[Training Epoch 4] Batch 1922, Loss 0.29763078689575195\n",
      "[Training Epoch 4] Batch 1923, Loss 0.2561926245689392\n",
      "[Training Epoch 4] Batch 1924, Loss 0.2759919762611389\n",
      "[Training Epoch 4] Batch 1925, Loss 0.25427940487861633\n",
      "[Training Epoch 4] Batch 1926, Loss 0.24977324903011322\n",
      "[Training Epoch 4] Batch 1927, Loss 0.2506554126739502\n",
      "[Training Epoch 4] Batch 1928, Loss 0.2689473628997803\n",
      "[Training Epoch 4] Batch 1929, Loss 0.28051674365997314\n",
      "[Training Epoch 4] Batch 1930, Loss 0.25164657831192017\n",
      "[Training Epoch 4] Batch 1931, Loss 0.25310367345809937\n",
      "[Training Epoch 4] Batch 1932, Loss 0.24451115727424622\n",
      "[Training Epoch 4] Batch 1933, Loss 0.2647053003311157\n",
      "[Training Epoch 4] Batch 1934, Loss 0.25465214252471924\n",
      "[Training Epoch 4] Batch 1935, Loss 0.24181005358695984\n",
      "[Training Epoch 4] Batch 1936, Loss 0.27987855672836304\n",
      "[Training Epoch 4] Batch 1937, Loss 0.23275882005691528\n",
      "[Training Epoch 4] Batch 1938, Loss 0.2885932922363281\n",
      "[Training Epoch 4] Batch 1939, Loss 0.24995145201683044\n",
      "[Training Epoch 4] Batch 1940, Loss 0.24525108933448792\n",
      "[Training Epoch 4] Batch 1941, Loss 0.24482885003089905\n",
      "[Training Epoch 4] Batch 1942, Loss 0.24884271621704102\n",
      "[Training Epoch 4] Batch 1943, Loss 0.2746993899345398\n",
      "[Training Epoch 4] Batch 1944, Loss 0.26662787795066833\n",
      "[Training Epoch 4] Batch 1945, Loss 0.25346484780311584\n",
      "[Training Epoch 4] Batch 1946, Loss 0.28693336248397827\n",
      "[Training Epoch 4] Batch 1947, Loss 0.25365889072418213\n",
      "[Training Epoch 4] Batch 1948, Loss 0.2562253773212433\n",
      "[Training Epoch 4] Batch 1949, Loss 0.26328223943710327\n",
      "[Training Epoch 4] Batch 1950, Loss 0.269758939743042\n",
      "[Training Epoch 4] Batch 1951, Loss 0.26033884286880493\n",
      "[Training Epoch 4] Batch 1952, Loss 0.28819024562835693\n",
      "[Training Epoch 4] Batch 1953, Loss 0.254014253616333\n",
      "[Training Epoch 4] Batch 1954, Loss 0.24889013171195984\n",
      "[Training Epoch 4] Batch 1955, Loss 0.24478361010551453\n",
      "[Training Epoch 4] Batch 1956, Loss 0.25401389598846436\n",
      "[Training Epoch 4] Batch 1957, Loss 0.24253323674201965\n",
      "[Training Epoch 4] Batch 1958, Loss 0.27019381523132324\n",
      "[Training Epoch 4] Batch 1959, Loss 0.2367641180753708\n",
      "[Training Epoch 4] Batch 1960, Loss 0.2505820393562317\n",
      "[Training Epoch 4] Batch 1961, Loss 0.252418577671051\n",
      "[Training Epoch 4] Batch 1962, Loss 0.2748270034790039\n",
      "[Training Epoch 4] Batch 1963, Loss 0.24686627089977264\n",
      "[Training Epoch 4] Batch 1964, Loss 0.2853201627731323\n",
      "[Training Epoch 4] Batch 1965, Loss 0.2533454895019531\n",
      "[Training Epoch 4] Batch 1966, Loss 0.278075248003006\n",
      "[Training Epoch 4] Batch 1967, Loss 0.247765451669693\n",
      "[Training Epoch 4] Batch 1968, Loss 0.30092209577560425\n",
      "[Training Epoch 4] Batch 1969, Loss 0.27769941091537476\n",
      "[Training Epoch 4] Batch 1970, Loss 0.2626498341560364\n",
      "[Training Epoch 4] Batch 1971, Loss 0.2622200846672058\n",
      "[Training Epoch 4] Batch 1972, Loss 0.2950608730316162\n",
      "[Training Epoch 4] Batch 1973, Loss 0.25544819235801697\n",
      "[Training Epoch 4] Batch 1974, Loss 0.25400400161743164\n",
      "[Training Epoch 4] Batch 1975, Loss 0.2616014778614044\n",
      "[Training Epoch 4] Batch 1976, Loss 0.2591766119003296\n",
      "[Training Epoch 4] Batch 1977, Loss 0.2870604991912842\n",
      "[Training Epoch 4] Batch 1978, Loss 0.27822673320770264\n",
      "[Training Epoch 4] Batch 1979, Loss 0.2639244794845581\n",
      "[Training Epoch 4] Batch 1980, Loss 0.24483123421669006\n",
      "[Training Epoch 4] Batch 1981, Loss 0.26381656527519226\n",
      "[Training Epoch 4] Batch 1982, Loss 0.26142942905426025\n",
      "[Training Epoch 4] Batch 1983, Loss 0.2515166401863098\n",
      "[Training Epoch 4] Batch 1984, Loss 0.28153717517852783\n",
      "[Training Epoch 4] Batch 1985, Loss 0.28784292936325073\n",
      "[Training Epoch 4] Batch 1986, Loss 0.27753686904907227\n",
      "[Training Epoch 4] Batch 1987, Loss 0.30088531970977783\n",
      "[Training Epoch 4] Batch 1988, Loss 0.24171681702136993\n",
      "[Training Epoch 4] Batch 1989, Loss 0.26958397030830383\n",
      "[Training Epoch 4] Batch 1990, Loss 0.2687634825706482\n",
      "[Training Epoch 4] Batch 1991, Loss 0.26088249683380127\n",
      "[Training Epoch 4] Batch 1992, Loss 0.24108754098415375\n",
      "[Training Epoch 4] Batch 1993, Loss 0.2503622770309448\n",
      "[Training Epoch 4] Batch 1994, Loss 0.2575117349624634\n",
      "[Training Epoch 4] Batch 1995, Loss 0.27952536940574646\n",
      "[Training Epoch 4] Batch 1996, Loss 0.24572786688804626\n",
      "[Training Epoch 4] Batch 1997, Loss 0.28238534927368164\n",
      "[Training Epoch 4] Batch 1998, Loss 0.25027695298194885\n",
      "[Training Epoch 4] Batch 1999, Loss 0.23845484852790833\n",
      "[Training Epoch 4] Batch 2000, Loss 0.2804033160209656\n",
      "[Training Epoch 4] Batch 2001, Loss 0.2699955105781555\n",
      "[Training Epoch 4] Batch 2002, Loss 0.256952166557312\n",
      "[Training Epoch 4] Batch 2003, Loss 0.25600796937942505\n",
      "[Training Epoch 4] Batch 2004, Loss 0.27486452460289\n",
      "[Training Epoch 4] Batch 2005, Loss 0.2587232291698456\n",
      "[Training Epoch 4] Batch 2006, Loss 0.26705414056777954\n",
      "[Training Epoch 4] Batch 2007, Loss 0.28853052854537964\n",
      "[Training Epoch 4] Batch 2008, Loss 0.26216596364974976\n",
      "[Training Epoch 4] Batch 2009, Loss 0.2598009705543518\n",
      "[Training Epoch 4] Batch 2010, Loss 0.2720773220062256\n",
      "[Training Epoch 4] Batch 2011, Loss 0.24347491562366486\n",
      "[Training Epoch 4] Batch 2012, Loss 0.2823181450366974\n",
      "[Training Epoch 4] Batch 2013, Loss 0.25117558240890503\n",
      "[Training Epoch 4] Batch 2014, Loss 0.2446962296962738\n",
      "[Training Epoch 4] Batch 2015, Loss 0.2629225552082062\n",
      "[Training Epoch 4] Batch 2016, Loss 0.2618902921676636\n",
      "[Training Epoch 4] Batch 2017, Loss 0.26193881034851074\n",
      "[Training Epoch 4] Batch 2018, Loss 0.27675020694732666\n",
      "[Training Epoch 4] Batch 2019, Loss 0.24194876849651337\n",
      "[Training Epoch 4] Batch 2020, Loss 0.27637025713920593\n",
      "[Training Epoch 4] Batch 2021, Loss 0.27309054136276245\n",
      "[Training Epoch 4] Batch 2022, Loss 0.3067476153373718\n",
      "[Training Epoch 4] Batch 2023, Loss 0.2589340806007385\n",
      "[Training Epoch 4] Batch 2024, Loss 0.27889707684516907\n",
      "[Training Epoch 4] Batch 2025, Loss 0.24021197855472565\n",
      "[Training Epoch 4] Batch 2026, Loss 0.2845824956893921\n",
      "[Training Epoch 4] Batch 2027, Loss 0.2520774006843567\n",
      "[Training Epoch 4] Batch 2028, Loss 0.24652248620986938\n",
      "[Training Epoch 4] Batch 2029, Loss 0.27865374088287354\n",
      "[Training Epoch 4] Batch 2030, Loss 0.2759019136428833\n",
      "[Training Epoch 4] Batch 2031, Loss 0.2712586224079132\n",
      "[Training Epoch 4] Batch 2032, Loss 0.3008047640323639\n",
      "[Training Epoch 4] Batch 2033, Loss 0.2504250407218933\n",
      "[Training Epoch 4] Batch 2034, Loss 0.2873751223087311\n",
      "[Training Epoch 4] Batch 2035, Loss 0.2603387236595154\n",
      "[Training Epoch 4] Batch 2036, Loss 0.24290111660957336\n",
      "[Training Epoch 4] Batch 2037, Loss 0.24343866109848022\n",
      "[Training Epoch 4] Batch 2038, Loss 0.26478311419487\n",
      "[Training Epoch 4] Batch 2039, Loss 0.25630974769592285\n",
      "[Training Epoch 4] Batch 2040, Loss 0.270391047000885\n",
      "[Training Epoch 4] Batch 2041, Loss 0.24622201919555664\n",
      "[Training Epoch 4] Batch 2042, Loss 0.23949435353279114\n",
      "[Training Epoch 4] Batch 2043, Loss 0.25285598635673523\n",
      "[Training Epoch 4] Batch 2044, Loss 0.25035321712493896\n",
      "[Training Epoch 4] Batch 2045, Loss 0.26594239473342896\n",
      "[Training Epoch 4] Batch 2046, Loss 0.29832708835601807\n",
      "[Training Epoch 4] Batch 2047, Loss 0.2817081809043884\n",
      "[Training Epoch 4] Batch 2048, Loss 0.2541767954826355\n",
      "[Training Epoch 4] Batch 2049, Loss 0.27716004848480225\n",
      "[Training Epoch 4] Batch 2050, Loss 0.2677612006664276\n",
      "[Training Epoch 4] Batch 2051, Loss 0.2714928984642029\n",
      "[Training Epoch 4] Batch 2052, Loss 0.2664482593536377\n",
      "[Training Epoch 4] Batch 2053, Loss 0.26595503091812134\n",
      "[Training Epoch 4] Batch 2054, Loss 0.25493505597114563\n",
      "[Training Epoch 4] Batch 2055, Loss 0.26017454266548157\n",
      "[Training Epoch 4] Batch 2056, Loss 0.27828657627105713\n",
      "[Training Epoch 4] Batch 2057, Loss 0.2577645182609558\n",
      "[Training Epoch 4] Batch 2058, Loss 0.27301910519599915\n",
      "[Training Epoch 4] Batch 2059, Loss 0.2630753517150879\n",
      "[Training Epoch 4] Batch 2060, Loss 0.26137709617614746\n",
      "[Training Epoch 4] Batch 2061, Loss 0.24738752841949463\n",
      "[Training Epoch 4] Batch 2062, Loss 0.2461070865392685\n",
      "[Training Epoch 4] Batch 2063, Loss 0.2849377691745758\n",
      "[Training Epoch 4] Batch 2064, Loss 0.25266367197036743\n",
      "[Training Epoch 4] Batch 2065, Loss 0.23401987552642822\n",
      "[Training Epoch 4] Batch 2066, Loss 0.285237193107605\n",
      "[Training Epoch 4] Batch 2067, Loss 0.2596357762813568\n",
      "[Training Epoch 4] Batch 2068, Loss 0.27723968029022217\n",
      "[Training Epoch 4] Batch 2069, Loss 0.28466200828552246\n",
      "[Training Epoch 4] Batch 2070, Loss 0.25047874450683594\n",
      "[Training Epoch 4] Batch 2071, Loss 0.2753143906593323\n",
      "[Training Epoch 4] Batch 2072, Loss 0.27404356002807617\n",
      "[Training Epoch 4] Batch 2073, Loss 0.2889518141746521\n",
      "[Training Epoch 4] Batch 2074, Loss 0.2542969584465027\n",
      "[Training Epoch 4] Batch 2075, Loss 0.26524657011032104\n",
      "[Training Epoch 4] Batch 2076, Loss 0.2679629325866699\n",
      "[Training Epoch 4] Batch 2077, Loss 0.2528073787689209\n",
      "[Training Epoch 4] Batch 2078, Loss 0.26774996519088745\n",
      "[Training Epoch 4] Batch 2079, Loss 0.24768920242786407\n",
      "[Training Epoch 4] Batch 2080, Loss 0.27049720287323\n",
      "[Training Epoch 4] Batch 2081, Loss 0.29744112491607666\n",
      "[Training Epoch 4] Batch 2082, Loss 0.2821611762046814\n",
      "[Training Epoch 4] Batch 2083, Loss 0.25174564123153687\n",
      "[Training Epoch 4] Batch 2084, Loss 0.26120495796203613\n",
      "[Training Epoch 4] Batch 2085, Loss 0.2635209560394287\n",
      "[Training Epoch 4] Batch 2086, Loss 0.27689409255981445\n",
      "[Training Epoch 4] Batch 2087, Loss 0.25307697057724\n",
      "[Training Epoch 4] Batch 2088, Loss 0.27085691690444946\n",
      "[Training Epoch 4] Batch 2089, Loss 0.2661406993865967\n",
      "[Training Epoch 4] Batch 2090, Loss 0.27334168553352356\n",
      "[Training Epoch 4] Batch 2091, Loss 0.28720784187316895\n",
      "[Training Epoch 4] Batch 2092, Loss 0.25572603940963745\n",
      "[Training Epoch 4] Batch 2093, Loss 0.23992447555065155\n",
      "[Training Epoch 4] Batch 2094, Loss 0.29984527826309204\n",
      "[Training Epoch 4] Batch 2095, Loss 0.26830804347991943\n",
      "[Training Epoch 4] Batch 2096, Loss 0.263614684343338\n",
      "[Training Epoch 4] Batch 2097, Loss 0.2902643084526062\n",
      "[Training Epoch 4] Batch 2098, Loss 0.29393595457077026\n",
      "[Training Epoch 4] Batch 2099, Loss 0.2626359462738037\n",
      "[Training Epoch 4] Batch 2100, Loss 0.292880654335022\n",
      "[Training Epoch 4] Batch 2101, Loss 0.2713788151741028\n",
      "[Training Epoch 4] Batch 2102, Loss 0.2557993531227112\n",
      "[Training Epoch 4] Batch 2103, Loss 0.2889643907546997\n",
      "[Training Epoch 4] Batch 2104, Loss 0.2805708348751068\n",
      "[Training Epoch 4] Batch 2105, Loss 0.28524354100227356\n",
      "[Training Epoch 4] Batch 2106, Loss 0.25798299908638\n",
      "[Training Epoch 4] Batch 2107, Loss 0.261898398399353\n",
      "[Training Epoch 4] Batch 2108, Loss 0.26956871151924133\n",
      "[Training Epoch 4] Batch 2109, Loss 0.27002474665641785\n",
      "[Training Epoch 4] Batch 2110, Loss 0.2708115875720978\n",
      "[Training Epoch 4] Batch 2111, Loss 0.2712828516960144\n",
      "[Training Epoch 4] Batch 2112, Loss 0.2769266963005066\n",
      "[Training Epoch 4] Batch 2113, Loss 0.26456519961357117\n",
      "[Training Epoch 4] Batch 2114, Loss 0.2421170175075531\n",
      "[Training Epoch 4] Batch 2115, Loss 0.27369463443756104\n",
      "[Training Epoch 4] Batch 2116, Loss 0.2654153108596802\n",
      "[Training Epoch 4] Batch 2117, Loss 0.24449677765369415\n",
      "[Training Epoch 4] Batch 2118, Loss 0.2752818763256073\n",
      "[Training Epoch 4] Batch 2119, Loss 0.2841304838657379\n",
      "[Training Epoch 4] Batch 2120, Loss 0.24487930536270142\n",
      "[Training Epoch 4] Batch 2121, Loss 0.23208525776863098\n",
      "[Training Epoch 4] Batch 2122, Loss 0.2512282133102417\n",
      "[Training Epoch 4] Batch 2123, Loss 0.27570217847824097\n",
      "[Training Epoch 4] Batch 2124, Loss 0.2577071189880371\n",
      "[Training Epoch 4] Batch 2125, Loss 0.2700417637825012\n",
      "[Training Epoch 4] Batch 2126, Loss 0.2712517976760864\n",
      "[Training Epoch 4] Batch 2127, Loss 0.27134573459625244\n",
      "[Training Epoch 4] Batch 2128, Loss 0.2675129771232605\n",
      "[Training Epoch 4] Batch 2129, Loss 0.2885667383670807\n",
      "[Training Epoch 4] Batch 2130, Loss 0.26267290115356445\n",
      "[Training Epoch 4] Batch 2131, Loss 0.2575422525405884\n",
      "[Training Epoch 4] Batch 2132, Loss 0.2607915997505188\n",
      "[Training Epoch 4] Batch 2133, Loss 0.24214233458042145\n",
      "[Training Epoch 4] Batch 2134, Loss 0.2484663426876068\n",
      "[Training Epoch 4] Batch 2135, Loss 0.2529045343399048\n",
      "[Training Epoch 4] Batch 2136, Loss 0.24173504114151\n",
      "[Training Epoch 4] Batch 2137, Loss 0.2819921374320984\n",
      "[Training Epoch 4] Batch 2138, Loss 0.274946004152298\n",
      "[Training Epoch 4] Batch 2139, Loss 0.27534592151641846\n",
      "[Training Epoch 4] Batch 2140, Loss 0.2699114680290222\n",
      "[Training Epoch 4] Batch 2141, Loss 0.25200632214546204\n",
      "[Training Epoch 4] Batch 2142, Loss 0.26146548986434937\n",
      "[Training Epoch 4] Batch 2143, Loss 0.26535409688949585\n",
      "[Training Epoch 4] Batch 2144, Loss 0.26993557810783386\n",
      "[Training Epoch 4] Batch 2145, Loss 0.2538071274757385\n",
      "[Training Epoch 4] Batch 2146, Loss 0.2233351320028305\n",
      "[Training Epoch 4] Batch 2147, Loss 0.25781774520874023\n",
      "[Training Epoch 4] Batch 2148, Loss 0.29886114597320557\n",
      "[Training Epoch 4] Batch 2149, Loss 0.2772543132305145\n",
      "[Training Epoch 4] Batch 2150, Loss 0.2572017312049866\n",
      "[Training Epoch 4] Batch 2151, Loss 0.28734689950942993\n",
      "[Training Epoch 4] Batch 2152, Loss 0.2626572847366333\n",
      "[Training Epoch 4] Batch 2153, Loss 0.24882978200912476\n",
      "[Training Epoch 4] Batch 2154, Loss 0.24585960805416107\n",
      "[Training Epoch 4] Batch 2155, Loss 0.2746698260307312\n",
      "[Training Epoch 4] Batch 2156, Loss 0.2508878707885742\n",
      "[Training Epoch 4] Batch 2157, Loss 0.24988576769828796\n",
      "[Training Epoch 4] Batch 2158, Loss 0.26522618532180786\n",
      "[Training Epoch 4] Batch 2159, Loss 0.2889079451560974\n",
      "[Training Epoch 4] Batch 2160, Loss 0.2614923417568207\n",
      "[Training Epoch 4] Batch 2161, Loss 0.265440970659256\n",
      "[Training Epoch 4] Batch 2162, Loss 0.23761248588562012\n",
      "[Training Epoch 4] Batch 2163, Loss 0.278644859790802\n",
      "[Training Epoch 4] Batch 2164, Loss 0.2815237045288086\n",
      "[Training Epoch 4] Batch 2165, Loss 0.25282004475593567\n",
      "[Training Epoch 4] Batch 2166, Loss 0.269622266292572\n",
      "[Training Epoch 4] Batch 2167, Loss 0.2752447724342346\n",
      "[Training Epoch 4] Batch 2168, Loss 0.27241644263267517\n",
      "[Training Epoch 4] Batch 2169, Loss 0.23946622014045715\n",
      "[Training Epoch 4] Batch 2170, Loss 0.2547077536582947\n",
      "[Training Epoch 4] Batch 2171, Loss 0.28192901611328125\n",
      "[Training Epoch 4] Batch 2172, Loss 0.2714186906814575\n",
      "[Training Epoch 4] Batch 2173, Loss 0.2614642381668091\n",
      "[Training Epoch 4] Batch 2174, Loss 0.24956542253494263\n",
      "[Training Epoch 4] Batch 2175, Loss 0.2807723879814148\n",
      "[Training Epoch 4] Batch 2176, Loss 0.27667635679244995\n",
      "[Training Epoch 4] Batch 2177, Loss 0.26063665747642517\n",
      "[Training Epoch 4] Batch 2178, Loss 0.28146833181381226\n",
      "[Training Epoch 4] Batch 2179, Loss 0.2366740107536316\n",
      "[Training Epoch 4] Batch 2180, Loss 0.26474472880363464\n",
      "[Training Epoch 4] Batch 2181, Loss 0.2585732638835907\n",
      "[Training Epoch 4] Batch 2182, Loss 0.2677052617073059\n",
      "[Training Epoch 4] Batch 2183, Loss 0.2986829876899719\n",
      "[Training Epoch 4] Batch 2184, Loss 0.27662527561187744\n",
      "[Training Epoch 4] Batch 2185, Loss 0.2738998234272003\n",
      "[Training Epoch 4] Batch 2186, Loss 0.27450624108314514\n",
      "[Training Epoch 4] Batch 2187, Loss 0.26864486932754517\n",
      "[Training Epoch 4] Batch 2188, Loss 0.24002176523208618\n",
      "[Training Epoch 4] Batch 2189, Loss 0.2550085783004761\n",
      "[Training Epoch 4] Batch 2190, Loss 0.26405709981918335\n",
      "[Training Epoch 4] Batch 2191, Loss 0.2824689745903015\n",
      "[Training Epoch 4] Batch 2192, Loss 0.26021063327789307\n",
      "[Training Epoch 4] Batch 2193, Loss 0.2649518847465515\n",
      "[Training Epoch 4] Batch 2194, Loss 0.2748711109161377\n",
      "[Training Epoch 4] Batch 2195, Loss 0.30934929847717285\n",
      "[Training Epoch 4] Batch 2196, Loss 0.27134057879447937\n",
      "[Training Epoch 4] Batch 2197, Loss 0.2481466829776764\n",
      "[Training Epoch 4] Batch 2198, Loss 0.2748086750507355\n",
      "[Training Epoch 4] Batch 2199, Loss 0.25767335295677185\n",
      "[Training Epoch 4] Batch 2200, Loss 0.25372445583343506\n",
      "[Training Epoch 4] Batch 2201, Loss 0.27057066559791565\n",
      "[Training Epoch 4] Batch 2202, Loss 0.2661079168319702\n",
      "[Training Epoch 4] Batch 2203, Loss 0.2488490343093872\n",
      "[Training Epoch 4] Batch 2204, Loss 0.2792295217514038\n",
      "[Training Epoch 4] Batch 2205, Loss 0.2654058337211609\n",
      "[Training Epoch 4] Batch 2206, Loss 0.2878846824169159\n",
      "[Training Epoch 4] Batch 2207, Loss 0.2632082998752594\n",
      "[Training Epoch 4] Batch 2208, Loss 0.2582102417945862\n",
      "[Training Epoch 4] Batch 2209, Loss 0.2541539669036865\n",
      "[Training Epoch 4] Batch 2210, Loss 0.27538031339645386\n",
      "[Training Epoch 4] Batch 2211, Loss 0.2946936786174774\n",
      "[Training Epoch 4] Batch 2212, Loss 0.25734663009643555\n",
      "[Training Epoch 4] Batch 2213, Loss 0.2870088815689087\n",
      "[Training Epoch 4] Batch 2214, Loss 0.28144174814224243\n",
      "[Training Epoch 4] Batch 2215, Loss 0.2756405174732208\n",
      "[Training Epoch 4] Batch 2216, Loss 0.27827271819114685\n",
      "[Training Epoch 4] Batch 2217, Loss 0.2819063365459442\n",
      "[Training Epoch 4] Batch 2218, Loss 0.27260464429855347\n",
      "[Training Epoch 4] Batch 2219, Loss 0.2589823305606842\n",
      "[Training Epoch 4] Batch 2220, Loss 0.25335177779197693\n",
      "[Training Epoch 4] Batch 2221, Loss 0.2706938087940216\n",
      "[Training Epoch 4] Batch 2222, Loss 0.25806206464767456\n",
      "[Training Epoch 4] Batch 2223, Loss 0.23858240246772766\n",
      "[Training Epoch 4] Batch 2224, Loss 0.24641433358192444\n",
      "[Training Epoch 4] Batch 2225, Loss 0.2549625039100647\n",
      "[Training Epoch 4] Batch 2226, Loss 0.26542508602142334\n",
      "[Training Epoch 4] Batch 2227, Loss 0.2600291967391968\n",
      "[Training Epoch 4] Batch 2228, Loss 0.26400354504585266\n",
      "[Training Epoch 4] Batch 2229, Loss 0.24390661716461182\n",
      "[Training Epoch 4] Batch 2230, Loss 0.2778454124927521\n",
      "[Training Epoch 4] Batch 2231, Loss 0.26774418354034424\n",
      "[Training Epoch 4] Batch 2232, Loss 0.23503151535987854\n",
      "[Training Epoch 4] Batch 2233, Loss 0.25155723094940186\n",
      "[Training Epoch 4] Batch 2234, Loss 0.2666381299495697\n",
      "[Training Epoch 4] Batch 2235, Loss 0.2499932050704956\n",
      "[Training Epoch 4] Batch 2236, Loss 0.23690074682235718\n",
      "[Training Epoch 4] Batch 2237, Loss 0.2702024579048157\n",
      "[Training Epoch 4] Batch 2238, Loss 0.22979754209518433\n",
      "[Training Epoch 4] Batch 2239, Loss 0.2376067191362381\n",
      "[Training Epoch 4] Batch 2240, Loss 0.28381145000457764\n",
      "[Training Epoch 4] Batch 2241, Loss 0.27233201265335083\n",
      "[Training Epoch 4] Batch 2242, Loss 0.28743040561676025\n",
      "[Training Epoch 4] Batch 2243, Loss 0.3073657751083374\n",
      "[Training Epoch 4] Batch 2244, Loss 0.255490779876709\n",
      "[Training Epoch 4] Batch 2245, Loss 0.25086748600006104\n",
      "[Training Epoch 4] Batch 2246, Loss 0.2691006064414978\n",
      "[Training Epoch 4] Batch 2247, Loss 0.2836315333843231\n",
      "[Training Epoch 4] Batch 2248, Loss 0.26657789945602417\n",
      "[Training Epoch 4] Batch 2249, Loss 0.26131561398506165\n",
      "[Training Epoch 4] Batch 2250, Loss 0.2829875349998474\n",
      "[Training Epoch 4] Batch 2251, Loss 0.2653800845146179\n",
      "[Training Epoch 4] Batch 2252, Loss 0.26170748472213745\n",
      "[Training Epoch 4] Batch 2253, Loss 0.26756730675697327\n",
      "[Training Epoch 4] Batch 2254, Loss 0.28185075521469116\n",
      "[Training Epoch 4] Batch 2255, Loss 0.26344555616378784\n",
      "[Training Epoch 4] Batch 2256, Loss 0.2622065544128418\n",
      "[Training Epoch 4] Batch 2257, Loss 0.27811914682388306\n",
      "[Training Epoch 4] Batch 2258, Loss 0.27610987424850464\n",
      "[Training Epoch 4] Batch 2259, Loss 0.230831116437912\n",
      "[Training Epoch 4] Batch 2260, Loss 0.2701745331287384\n",
      "[Training Epoch 4] Batch 2261, Loss 0.255723237991333\n",
      "[Training Epoch 4] Batch 2262, Loss 0.26219892501831055\n",
      "[Training Epoch 4] Batch 2263, Loss 0.2851012945175171\n",
      "[Training Epoch 4] Batch 2264, Loss 0.2633618116378784\n",
      "[Training Epoch 4] Batch 2265, Loss 0.23964685201644897\n",
      "[Training Epoch 4] Batch 2266, Loss 0.2639003396034241\n",
      "[Training Epoch 4] Batch 2267, Loss 0.2811968922615051\n",
      "[Training Epoch 4] Batch 2268, Loss 0.25983667373657227\n",
      "[Training Epoch 4] Batch 2269, Loss 0.2779616117477417\n",
      "[Training Epoch 4] Batch 2270, Loss 0.24561963975429535\n",
      "[Training Epoch 4] Batch 2271, Loss 0.2662935256958008\n",
      "[Training Epoch 4] Batch 2272, Loss 0.24121227860450745\n",
      "[Training Epoch 4] Batch 2273, Loss 0.25090184807777405\n",
      "[Training Epoch 4] Batch 2274, Loss 0.2438054382801056\n",
      "[Training Epoch 4] Batch 2275, Loss 0.2661581039428711\n",
      "[Training Epoch 4] Batch 2276, Loss 0.2368066906929016\n",
      "[Training Epoch 4] Batch 2277, Loss 0.2597348093986511\n",
      "[Training Epoch 4] Batch 2278, Loss 0.27074116468429565\n",
      "[Training Epoch 4] Batch 2279, Loss 0.26015737652778625\n",
      "[Training Epoch 4] Batch 2280, Loss 0.26737767457962036\n",
      "[Training Epoch 4] Batch 2281, Loss 0.27759185433387756\n",
      "[Training Epoch 4] Batch 2282, Loss 0.2804914712905884\n",
      "[Training Epoch 4] Batch 2283, Loss 0.2665480971336365\n",
      "[Training Epoch 4] Batch 2284, Loss 0.26384127140045166\n",
      "[Training Epoch 4] Batch 2285, Loss 0.2570018172264099\n",
      "[Training Epoch 4] Batch 2286, Loss 0.2623533010482788\n",
      "[Training Epoch 4] Batch 2287, Loss 0.2569582164287567\n",
      "[Training Epoch 4] Batch 2288, Loss 0.2874446511268616\n",
      "[Training Epoch 4] Batch 2289, Loss 0.25989875197410583\n",
      "[Training Epoch 4] Batch 2290, Loss 0.2645305395126343\n",
      "[Training Epoch 4] Batch 2291, Loss 0.26437267661094666\n",
      "[Training Epoch 4] Batch 2292, Loss 0.24680909514427185\n",
      "[Training Epoch 4] Batch 2293, Loss 0.24237556755542755\n",
      "[Training Epoch 4] Batch 2294, Loss 0.2799574136734009\n",
      "[Training Epoch 4] Batch 2295, Loss 0.2593204379081726\n",
      "[Training Epoch 4] Batch 2296, Loss 0.27404534816741943\n",
      "[Training Epoch 4] Batch 2297, Loss 0.28057926893234253\n",
      "[Training Epoch 4] Batch 2298, Loss 0.2645483911037445\n",
      "[Training Epoch 4] Batch 2299, Loss 0.23652565479278564\n",
      "[Training Epoch 4] Batch 2300, Loss 0.2590453028678894\n",
      "[Training Epoch 4] Batch 2301, Loss 0.2829762101173401\n",
      "[Training Epoch 4] Batch 2302, Loss 0.22167932987213135\n",
      "[Training Epoch 4] Batch 2303, Loss 0.24674758315086365\n",
      "[Training Epoch 4] Batch 2304, Loss 0.2464124858379364\n",
      "[Training Epoch 4] Batch 2305, Loss 0.2678440511226654\n",
      "[Training Epoch 4] Batch 2306, Loss 0.29056745767593384\n",
      "[Training Epoch 4] Batch 2307, Loss 0.25450408458709717\n",
      "[Training Epoch 4] Batch 2308, Loss 0.2679266333580017\n",
      "[Training Epoch 4] Batch 2309, Loss 0.27120357751846313\n",
      "[Training Epoch 4] Batch 2310, Loss 0.22931711375713348\n",
      "[Training Epoch 4] Batch 2311, Loss 0.24571415781974792\n",
      "[Training Epoch 4] Batch 2312, Loss 0.2752154469490051\n",
      "[Training Epoch 4] Batch 2313, Loss 0.2604288160800934\n",
      "[Training Epoch 4] Batch 2314, Loss 0.23640498518943787\n",
      "[Training Epoch 4] Batch 2315, Loss 0.26030176877975464\n",
      "[Training Epoch 4] Batch 2316, Loss 0.26807689666748047\n",
      "[Training Epoch 4] Batch 2317, Loss 0.24162527918815613\n",
      "[Training Epoch 4] Batch 2318, Loss 0.2657725214958191\n",
      "[Training Epoch 4] Batch 2319, Loss 0.28910642862319946\n",
      "[Training Epoch 4] Batch 2320, Loss 0.2788335084915161\n",
      "[Training Epoch 4] Batch 2321, Loss 0.27387452125549316\n",
      "[Training Epoch 4] Batch 2322, Loss 0.21221986413002014\n",
      "[Training Epoch 4] Batch 2323, Loss 0.2532546818256378\n",
      "[Training Epoch 4] Batch 2324, Loss 0.2741108238697052\n",
      "[Training Epoch 4] Batch 2325, Loss 0.2521682381629944\n",
      "[Training Epoch 4] Batch 2326, Loss 0.25680607557296753\n",
      "[Training Epoch 4] Batch 2327, Loss 0.25916987657546997\n",
      "[Training Epoch 4] Batch 2328, Loss 0.26059669256210327\n",
      "[Training Epoch 4] Batch 2329, Loss 0.26909139752388\n",
      "[Training Epoch 4] Batch 2330, Loss 0.25257909297943115\n",
      "[Training Epoch 4] Batch 2331, Loss 0.2801458239555359\n",
      "[Training Epoch 4] Batch 2332, Loss 0.290067195892334\n",
      "[Training Epoch 4] Batch 2333, Loss 0.26053738594055176\n",
      "[Training Epoch 4] Batch 2334, Loss 0.2701015770435333\n",
      "[Training Epoch 4] Batch 2335, Loss 0.2592378258705139\n",
      "[Training Epoch 4] Batch 2336, Loss 0.24508100748062134\n",
      "[Training Epoch 4] Batch 2337, Loss 0.2697410583496094\n",
      "[Training Epoch 4] Batch 2338, Loss 0.2673490345478058\n",
      "[Training Epoch 4] Batch 2339, Loss 0.24439668655395508\n",
      "[Training Epoch 4] Batch 2340, Loss 0.22709324955940247\n",
      "[Training Epoch 4] Batch 2341, Loss 0.26677167415618896\n",
      "[Training Epoch 4] Batch 2342, Loss 0.2492000162601471\n",
      "[Training Epoch 4] Batch 2343, Loss 0.2607235908508301\n",
      "[Training Epoch 4] Batch 2344, Loss 0.3066938519477844\n",
      "[Training Epoch 4] Batch 2345, Loss 0.24862544238567352\n",
      "[Training Epoch 4] Batch 2346, Loss 0.2990677058696747\n",
      "[Training Epoch 4] Batch 2347, Loss 0.24206063151359558\n",
      "[Training Epoch 4] Batch 2348, Loss 0.27316585183143616\n",
      "[Training Epoch 4] Batch 2349, Loss 0.26551389694213867\n",
      "[Training Epoch 4] Batch 2350, Loss 0.28451743721961975\n",
      "[Training Epoch 4] Batch 2351, Loss 0.2432950735092163\n",
      "[Training Epoch 4] Batch 2352, Loss 0.2699574828147888\n",
      "[Training Epoch 4] Batch 2353, Loss 0.26409757137298584\n",
      "[Training Epoch 4] Batch 2354, Loss 0.24629279971122742\n",
      "[Training Epoch 4] Batch 2355, Loss 0.28873610496520996\n",
      "[Training Epoch 4] Batch 2356, Loss 0.25839436054229736\n",
      "[Training Epoch 4] Batch 2357, Loss 0.28086960315704346\n",
      "[Training Epoch 4] Batch 2358, Loss 0.24319395422935486\n",
      "[Training Epoch 4] Batch 2359, Loss 0.2655409276485443\n",
      "[Training Epoch 4] Batch 2360, Loss 0.26928094029426575\n",
      "[Training Epoch 4] Batch 2361, Loss 0.2578648328781128\n",
      "[Training Epoch 4] Batch 2362, Loss 0.29330572485923767\n",
      "[Training Epoch 4] Batch 2363, Loss 0.24149638414382935\n",
      "[Training Epoch 4] Batch 2364, Loss 0.24605236947536469\n",
      "[Training Epoch 4] Batch 2365, Loss 0.2599653899669647\n",
      "[Training Epoch 4] Batch 2366, Loss 0.25801122188568115\n",
      "[Training Epoch 4] Batch 2367, Loss 0.2597281336784363\n",
      "[Training Epoch 4] Batch 2368, Loss 0.30101919174194336\n",
      "[Training Epoch 4] Batch 2369, Loss 0.2871297001838684\n",
      "[Training Epoch 4] Batch 2370, Loss 0.2524346113204956\n",
      "[Training Epoch 4] Batch 2371, Loss 0.27337750792503357\n",
      "[Training Epoch 4] Batch 2372, Loss 0.30585145950317383\n",
      "[Training Epoch 4] Batch 2373, Loss 0.24376532435417175\n",
      "[Training Epoch 4] Batch 2374, Loss 0.26901036500930786\n",
      "[Training Epoch 4] Batch 2375, Loss 0.23503799736499786\n",
      "[Training Epoch 4] Batch 2376, Loss 0.2698405385017395\n",
      "[Training Epoch 4] Batch 2377, Loss 0.28032857179641724\n",
      "[Training Epoch 4] Batch 2378, Loss 0.2609836161136627\n",
      "[Training Epoch 4] Batch 2379, Loss 0.2673778235912323\n",
      "[Training Epoch 4] Batch 2380, Loss 0.24225156009197235\n",
      "[Training Epoch 4] Batch 2381, Loss 0.27791404724121094\n",
      "[Training Epoch 4] Batch 2382, Loss 0.283003568649292\n",
      "[Training Epoch 4] Batch 2383, Loss 0.23249861598014832\n",
      "[Training Epoch 4] Batch 2384, Loss 0.2676018476486206\n",
      "[Training Epoch 4] Batch 2385, Loss 0.2652377486228943\n",
      "[Training Epoch 4] Batch 2386, Loss 0.25263139605522156\n",
      "[Training Epoch 4] Batch 2387, Loss 0.2698667645454407\n",
      "[Training Epoch 4] Batch 2388, Loss 0.2786315083503723\n",
      "[Training Epoch 4] Batch 2389, Loss 0.28873366117477417\n",
      "[Training Epoch 4] Batch 2390, Loss 0.25954824686050415\n",
      "[Training Epoch 4] Batch 2391, Loss 0.23440931737422943\n",
      "[Training Epoch 4] Batch 2392, Loss 0.2656625211238861\n",
      "[Training Epoch 4] Batch 2393, Loss 0.26655489206314087\n",
      "[Training Epoch 4] Batch 2394, Loss 0.28795355558395386\n",
      "[Training Epoch 4] Batch 2395, Loss 0.27666160464286804\n",
      "[Training Epoch 4] Batch 2396, Loss 0.28190940618515015\n",
      "[Training Epoch 4] Batch 2397, Loss 0.24346433579921722\n",
      "[Training Epoch 4] Batch 2398, Loss 0.2761956453323364\n",
      "[Training Epoch 4] Batch 2399, Loss 0.26082590222358704\n",
      "[Training Epoch 4] Batch 2400, Loss 0.26351791620254517\n",
      "[Training Epoch 4] Batch 2401, Loss 0.25997084379196167\n",
      "[Training Epoch 4] Batch 2402, Loss 0.27092260122299194\n",
      "[Training Epoch 4] Batch 2403, Loss 0.2461402416229248\n",
      "[Training Epoch 4] Batch 2404, Loss 0.2705445885658264\n",
      "[Training Epoch 4] Batch 2405, Loss 0.22320927679538727\n",
      "[Training Epoch 4] Batch 2406, Loss 0.29092490673065186\n",
      "[Training Epoch 4] Batch 2407, Loss 0.25615420937538147\n",
      "[Training Epoch 4] Batch 2408, Loss 0.2527914047241211\n",
      "[Training Epoch 4] Batch 2409, Loss 0.2713044285774231\n",
      "[Training Epoch 4] Batch 2410, Loss 0.2971566915512085\n",
      "[Training Epoch 4] Batch 2411, Loss 0.2560874819755554\n",
      "[Training Epoch 4] Batch 2412, Loss 0.2552322447299957\n",
      "[Training Epoch 4] Batch 2413, Loss 0.24617114663124084\n",
      "[Training Epoch 4] Batch 2414, Loss 0.247134268283844\n",
      "[Training Epoch 4] Batch 2415, Loss 0.28215909004211426\n",
      "[Training Epoch 4] Batch 2416, Loss 0.255058616399765\n",
      "[Training Epoch 4] Batch 2417, Loss 0.2733842432498932\n",
      "[Training Epoch 4] Batch 2418, Loss 0.29298579692840576\n",
      "[Training Epoch 4] Batch 2419, Loss 0.28127989172935486\n",
      "[Training Epoch 4] Batch 2420, Loss 0.2670044004917145\n",
      "[Training Epoch 4] Batch 2421, Loss 0.27353399991989136\n",
      "[Training Epoch 4] Batch 2422, Loss 0.2546822130680084\n",
      "[Training Epoch 4] Batch 2423, Loss 0.2669641971588135\n",
      "[Training Epoch 4] Batch 2424, Loss 0.2561129331588745\n",
      "[Training Epoch 4] Batch 2425, Loss 0.27580690383911133\n",
      "[Training Epoch 4] Batch 2426, Loss 0.2737618088722229\n",
      "[Training Epoch 4] Batch 2427, Loss 0.2694739103317261\n",
      "[Training Epoch 4] Batch 2428, Loss 0.25354841351509094\n",
      "[Training Epoch 4] Batch 2429, Loss 0.28048238158226013\n",
      "[Training Epoch 4] Batch 2430, Loss 0.2695412337779999\n",
      "[Training Epoch 4] Batch 2431, Loss 0.2933952212333679\n",
      "[Training Epoch 4] Batch 2432, Loss 0.28041982650756836\n",
      "[Training Epoch 4] Batch 2433, Loss 0.26161491870880127\n",
      "[Training Epoch 4] Batch 2434, Loss 0.2749090790748596\n",
      "[Training Epoch 4] Batch 2435, Loss 0.27969273924827576\n",
      "[Training Epoch 4] Batch 2436, Loss 0.25376302003860474\n",
      "[Training Epoch 4] Batch 2437, Loss 0.27999818325042725\n",
      "[Training Epoch 4] Batch 2438, Loss 0.25992250442504883\n",
      "[Training Epoch 4] Batch 2439, Loss 0.27266958355903625\n",
      "[Training Epoch 4] Batch 2440, Loss 0.2288682758808136\n",
      "[Training Epoch 4] Batch 2441, Loss 0.2714698910713196\n",
      "[Training Epoch 4] Batch 2442, Loss 0.27559584379196167\n",
      "[Training Epoch 4] Batch 2443, Loss 0.2682296633720398\n",
      "[Training Epoch 4] Batch 2444, Loss 0.25027400255203247\n",
      "[Training Epoch 4] Batch 2445, Loss 0.25094330310821533\n",
      "[Training Epoch 4] Batch 2446, Loss 0.24836039543151855\n",
      "[Training Epoch 4] Batch 2447, Loss 0.2717583477497101\n",
      "[Training Epoch 4] Batch 2448, Loss 0.2697976231575012\n",
      "[Training Epoch 4] Batch 2449, Loss 0.2727309465408325\n",
      "[Training Epoch 4] Batch 2450, Loss 0.26278603076934814\n",
      "[Training Epoch 4] Batch 2451, Loss 0.24619080126285553\n",
      "[Training Epoch 4] Batch 2452, Loss 0.25372499227523804\n",
      "[Training Epoch 4] Batch 2453, Loss 0.2650330066680908\n",
      "[Training Epoch 4] Batch 2454, Loss 0.2603466510772705\n",
      "[Training Epoch 4] Batch 2455, Loss 0.23955363035202026\n",
      "[Training Epoch 4] Batch 2456, Loss 0.24687480926513672\n",
      "[Training Epoch 4] Batch 2457, Loss 0.2341594099998474\n",
      "[Training Epoch 4] Batch 2458, Loss 0.31724703311920166\n",
      "[Training Epoch 4] Batch 2459, Loss 0.2749128043651581\n",
      "[Training Epoch 4] Batch 2460, Loss 0.25532135367393494\n",
      "[Training Epoch 4] Batch 2461, Loss 0.26250797510147095\n",
      "[Training Epoch 4] Batch 2462, Loss 0.2821704149246216\n",
      "[Training Epoch 4] Batch 2463, Loss 0.26958519220352173\n",
      "[Training Epoch 4] Batch 2464, Loss 0.26871734857559204\n",
      "[Training Epoch 4] Batch 2465, Loss 0.2590617835521698\n",
      "[Training Epoch 4] Batch 2466, Loss 0.25969398021698\n",
      "[Training Epoch 4] Batch 2467, Loss 0.261080801486969\n",
      "[Training Epoch 4] Batch 2468, Loss 0.25492775440216064\n",
      "[Training Epoch 4] Batch 2469, Loss 0.26080960035324097\n",
      "[Training Epoch 4] Batch 2470, Loss 0.25865596532821655\n",
      "[Training Epoch 4] Batch 2471, Loss 0.2908499836921692\n",
      "[Training Epoch 4] Batch 2472, Loss 0.2529061436653137\n",
      "[Training Epoch 4] Batch 2473, Loss 0.2869330048561096\n",
      "[Training Epoch 4] Batch 2474, Loss 0.2683972716331482\n",
      "[Training Epoch 4] Batch 2475, Loss 0.26444876194000244\n",
      "[Training Epoch 4] Batch 2476, Loss 0.24137736856937408\n",
      "[Training Epoch 4] Batch 2477, Loss 0.2580258846282959\n",
      "[Training Epoch 4] Batch 2478, Loss 0.2852799892425537\n",
      "[Training Epoch 4] Batch 2479, Loss 0.28506141901016235\n",
      "[Training Epoch 4] Batch 2480, Loss 0.26337823271751404\n",
      "[Training Epoch 4] Batch 2481, Loss 0.25005611777305603\n",
      "[Training Epoch 4] Batch 2482, Loss 0.2534678280353546\n",
      "[Training Epoch 4] Batch 2483, Loss 0.25820955634117126\n",
      "[Training Epoch 4] Batch 2484, Loss 0.2646331787109375\n",
      "[Training Epoch 4] Batch 2485, Loss 0.2523277699947357\n",
      "[Training Epoch 4] Batch 2486, Loss 0.2420378029346466\n",
      "[Training Epoch 4] Batch 2487, Loss 0.2633398771286011\n",
      "[Training Epoch 4] Batch 2488, Loss 0.2901418209075928\n",
      "[Training Epoch 4] Batch 2489, Loss 0.25418001413345337\n",
      "[Training Epoch 4] Batch 2490, Loss 0.2636704742908478\n",
      "[Training Epoch 4] Batch 2491, Loss 0.29964274168014526\n",
      "[Training Epoch 4] Batch 2492, Loss 0.25275924801826477\n",
      "[Training Epoch 4] Batch 2493, Loss 0.2772443890571594\n",
      "[Training Epoch 4] Batch 2494, Loss 0.2548774480819702\n",
      "[Training Epoch 4] Batch 2495, Loss 0.2762562334537506\n",
      "[Training Epoch 4] Batch 2496, Loss 0.2884984314441681\n",
      "[Training Epoch 4] Batch 2497, Loss 0.2601906359195709\n",
      "[Training Epoch 4] Batch 2498, Loss 0.286805123090744\n",
      "[Training Epoch 4] Batch 2499, Loss 0.24451318383216858\n",
      "[Training Epoch 4] Batch 2500, Loss 0.2797335088253021\n",
      "[Training Epoch 4] Batch 2501, Loss 0.28610557317733765\n",
      "[Training Epoch 4] Batch 2502, Loss 0.2553197145462036\n",
      "[Training Epoch 4] Batch 2503, Loss 0.27476829290390015\n",
      "[Training Epoch 4] Batch 2504, Loss 0.2753680348396301\n",
      "[Training Epoch 4] Batch 2505, Loss 0.2884916663169861\n",
      "[Training Epoch 4] Batch 2506, Loss 0.2642015814781189\n",
      "[Training Epoch 4] Batch 2507, Loss 0.24771903455257416\n",
      "[Training Epoch 4] Batch 2508, Loss 0.28340843319892883\n",
      "[Training Epoch 4] Batch 2509, Loss 0.2632187008857727\n",
      "[Training Epoch 4] Batch 2510, Loss 0.25758078694343567\n",
      "[Training Epoch 4] Batch 2511, Loss 0.25584420561790466\n",
      "[Training Epoch 4] Batch 2512, Loss 0.24976804852485657\n",
      "[Training Epoch 4] Batch 2513, Loss 0.3174721598625183\n",
      "[Training Epoch 4] Batch 2514, Loss 0.27662211656570435\n",
      "[Training Epoch 4] Batch 2515, Loss 0.27672305703163147\n",
      "[Training Epoch 4] Batch 2516, Loss 0.2857549786567688\n",
      "[Training Epoch 4] Batch 2517, Loss 0.27343225479125977\n",
      "[Training Epoch 4] Batch 2518, Loss 0.26293307542800903\n",
      "[Training Epoch 4] Batch 2519, Loss 0.2665313482284546\n",
      "[Training Epoch 4] Batch 2520, Loss 0.2539807856082916\n",
      "[Training Epoch 4] Batch 2521, Loss 0.2510014474391937\n",
      "[Training Epoch 4] Batch 2522, Loss 0.24741728603839874\n",
      "[Training Epoch 4] Batch 2523, Loss 0.29612305760383606\n",
      "[Training Epoch 4] Batch 2524, Loss 0.2823558449745178\n",
      "[Training Epoch 4] Batch 2525, Loss 0.2750101685523987\n",
      "[Training Epoch 4] Batch 2526, Loss 0.25160518288612366\n",
      "[Training Epoch 4] Batch 2527, Loss 0.26231104135513306\n",
      "[Training Epoch 4] Batch 2528, Loss 0.255662202835083\n",
      "[Training Epoch 4] Batch 2529, Loss 0.25453969836235046\n",
      "[Training Epoch 4] Batch 2530, Loss 0.2610552906990051\n",
      "[Training Epoch 4] Batch 2531, Loss 0.263197660446167\n",
      "[Training Epoch 4] Batch 2532, Loss 0.2549114227294922\n",
      "[Training Epoch 4] Batch 2533, Loss 0.265586256980896\n",
      "[Training Epoch 4] Batch 2534, Loss 0.25075048208236694\n",
      "[Training Epoch 4] Batch 2535, Loss 0.25698503851890564\n",
      "[Training Epoch 4] Batch 2536, Loss 0.277010977268219\n",
      "[Training Epoch 4] Batch 2537, Loss 0.2637466788291931\n",
      "[Training Epoch 4] Batch 2538, Loss 0.2613554298877716\n",
      "[Training Epoch 4] Batch 2539, Loss 0.2481669932603836\n",
      "[Training Epoch 4] Batch 2540, Loss 0.24030795693397522\n",
      "[Training Epoch 4] Batch 2541, Loss 0.26513177156448364\n",
      "[Training Epoch 4] Batch 2542, Loss 0.2548726201057434\n",
      "[Training Epoch 4] Batch 2543, Loss 0.2682693898677826\n",
      "[Training Epoch 4] Batch 2544, Loss 0.23667557537555695\n",
      "[Training Epoch 4] Batch 2545, Loss 0.2622005045413971\n",
      "[Training Epoch 4] Batch 2546, Loss 0.2648196816444397\n",
      "[Training Epoch 4] Batch 2547, Loss 0.26513826847076416\n",
      "[Training Epoch 4] Batch 2548, Loss 0.24238193035125732\n",
      "[Training Epoch 4] Batch 2549, Loss 0.24146364629268646\n",
      "[Training Epoch 4] Batch 2550, Loss 0.24199821054935455\n",
      "[Training Epoch 4] Batch 2551, Loss 0.27507704496383667\n",
      "[Training Epoch 4] Batch 2552, Loss 0.2503415048122406\n",
      "[Training Epoch 4] Batch 2553, Loss 0.2911075949668884\n",
      "[Training Epoch 4] Batch 2554, Loss 0.2969871163368225\n",
      "[Training Epoch 4] Batch 2555, Loss 0.2494216114282608\n",
      "[Training Epoch 4] Batch 2556, Loss 0.2382168173789978\n",
      "[Training Epoch 4] Batch 2557, Loss 0.27282387018203735\n",
      "[Training Epoch 4] Batch 2558, Loss 0.27074798941612244\n",
      "[Training Epoch 4] Batch 2559, Loss 0.26233580708503723\n",
      "[Training Epoch 4] Batch 2560, Loss 0.30960968136787415\n",
      "[Training Epoch 4] Batch 2561, Loss 0.23897214233875275\n",
      "[Training Epoch 4] Batch 2562, Loss 0.2668339014053345\n",
      "[Training Epoch 4] Batch 2563, Loss 0.27856624126434326\n",
      "[Training Epoch 4] Batch 2564, Loss 0.26600492000579834\n",
      "[Training Epoch 4] Batch 2565, Loss 0.2798391580581665\n",
      "[Training Epoch 4] Batch 2566, Loss 0.25502243638038635\n",
      "[Training Epoch 4] Batch 2567, Loss 0.30031120777130127\n",
      "[Training Epoch 4] Batch 2568, Loss 0.27163612842559814\n",
      "[Training Epoch 4] Batch 2569, Loss 0.28799885511398315\n",
      "[Training Epoch 4] Batch 2570, Loss 0.2867484986782074\n",
      "[Training Epoch 4] Batch 2571, Loss 0.27076011896133423\n",
      "[Training Epoch 4] Batch 2572, Loss 0.2687377333641052\n",
      "[Training Epoch 4] Batch 2573, Loss 0.2651674151420593\n",
      "[Training Epoch 4] Batch 2574, Loss 0.2513219118118286\n",
      "[Training Epoch 4] Batch 2575, Loss 0.23760655522346497\n",
      "[Training Epoch 4] Batch 2576, Loss 0.27746516466140747\n",
      "[Training Epoch 4] Batch 2577, Loss 0.2786750793457031\n",
      "[Training Epoch 4] Batch 2578, Loss 0.26104024052619934\n",
      "[Training Epoch 4] Batch 2579, Loss 0.2876942753791809\n",
      "[Training Epoch 4] Batch 2580, Loss 0.26941409707069397\n",
      "[Training Epoch 4] Batch 2581, Loss 0.2702573537826538\n",
      "[Training Epoch 4] Batch 2582, Loss 0.27387088537216187\n",
      "[Training Epoch 4] Batch 2583, Loss 0.25695139169692993\n",
      "[Training Epoch 4] Batch 2584, Loss 0.2794186472892761\n",
      "[Training Epoch 4] Batch 2585, Loss 0.2501955032348633\n",
      "[Training Epoch 4] Batch 2586, Loss 0.2484082579612732\n",
      "[Training Epoch 4] Batch 2587, Loss 0.2619228959083557\n",
      "[Training Epoch 4] Batch 2588, Loss 0.2761165201663971\n",
      "[Training Epoch 4] Batch 2589, Loss 0.2733978033065796\n",
      "[Training Epoch 4] Batch 2590, Loss 0.2539833188056946\n",
      "[Training Epoch 4] Batch 2591, Loss 0.267305463552475\n",
      "[Training Epoch 4] Batch 2592, Loss 0.26164716482162476\n",
      "[Training Epoch 4] Batch 2593, Loss 0.2586078643798828\n",
      "[Training Epoch 4] Batch 2594, Loss 0.25118404626846313\n",
      "[Training Epoch 4] Batch 2595, Loss 0.23297348618507385\n",
      "[Training Epoch 4] Batch 2596, Loss 0.2850060760974884\n",
      "[Training Epoch 4] Batch 2597, Loss 0.27589675784111023\n",
      "[Training Epoch 4] Batch 2598, Loss 0.2546233534812927\n",
      "[Training Epoch 4] Batch 2599, Loss 0.27351123094558716\n",
      "[Training Epoch 4] Batch 2600, Loss 0.27126049995422363\n",
      "[Training Epoch 4] Batch 2601, Loss 0.25386902689933777\n",
      "[Training Epoch 4] Batch 2602, Loss 0.2867017388343811\n",
      "[Training Epoch 4] Batch 2603, Loss 0.28550273180007935\n",
      "[Training Epoch 4] Batch 2604, Loss 0.28220027685165405\n",
      "[Training Epoch 4] Batch 2605, Loss 0.28231745958328247\n",
      "[Training Epoch 4] Batch 2606, Loss 0.2650984525680542\n",
      "[Training Epoch 4] Batch 2607, Loss 0.24117667973041534\n",
      "[Training Epoch 4] Batch 2608, Loss 0.2578297257423401\n",
      "[Training Epoch 4] Batch 2609, Loss 0.24837768077850342\n",
      "[Training Epoch 4] Batch 2610, Loss 0.25608041882514954\n",
      "[Training Epoch 4] Batch 2611, Loss 0.25173330307006836\n",
      "[Training Epoch 4] Batch 2612, Loss 0.31643906235694885\n",
      "[Training Epoch 4] Batch 2613, Loss 0.2789871096611023\n",
      "[Training Epoch 4] Batch 2614, Loss 0.27044594287872314\n",
      "[Training Epoch 4] Batch 2615, Loss 0.24039870500564575\n",
      "[Training Epoch 4] Batch 2616, Loss 0.2802615761756897\n",
      "[Training Epoch 4] Batch 2617, Loss 0.26061761379241943\n",
      "[Training Epoch 4] Batch 2618, Loss 0.2633541226387024\n",
      "[Training Epoch 4] Batch 2619, Loss 0.2736121416091919\n",
      "[Training Epoch 4] Batch 2620, Loss 0.2857934534549713\n",
      "[Training Epoch 4] Batch 2621, Loss 0.2747986912727356\n",
      "[Training Epoch 4] Batch 2622, Loss 0.2675744891166687\n",
      "[Training Epoch 4] Batch 2623, Loss 0.25596749782562256\n",
      "[Training Epoch 4] Batch 2624, Loss 0.2740473747253418\n",
      "[Training Epoch 4] Batch 2625, Loss 0.24862244725227356\n",
      "[Training Epoch 4] Batch 2626, Loss 0.2505553960800171\n",
      "[Training Epoch 4] Batch 2627, Loss 0.255695104598999\n",
      "[Training Epoch 4] Batch 2628, Loss 0.2563561201095581\n",
      "[Training Epoch 4] Batch 2629, Loss 0.2901742458343506\n",
      "[Training Epoch 4] Batch 2630, Loss 0.2533895969390869\n",
      "[Training Epoch 4] Batch 2631, Loss 0.25534915924072266\n",
      "[Training Epoch 4] Batch 2632, Loss 0.24288144707679749\n",
      "[Training Epoch 4] Batch 2633, Loss 0.29217302799224854\n",
      "[Training Epoch 4] Batch 2634, Loss 0.26517125964164734\n",
      "[Training Epoch 4] Batch 2635, Loss 0.24852631986141205\n",
      "[Training Epoch 4] Batch 2636, Loss 0.240068718791008\n",
      "[Training Epoch 4] Batch 2637, Loss 0.2529638409614563\n",
      "[Training Epoch 4] Batch 2638, Loss 0.26211851835250854\n",
      "[Training Epoch 4] Batch 2639, Loss 0.28344517946243286\n",
      "[Training Epoch 4] Batch 2640, Loss 0.2626933455467224\n",
      "[Training Epoch 4] Batch 2641, Loss 0.2558656930923462\n",
      "[Training Epoch 4] Batch 2642, Loss 0.2838117778301239\n",
      "[Training Epoch 4] Batch 2643, Loss 0.26897454261779785\n",
      "[Training Epoch 4] Batch 2644, Loss 0.2358936369419098\n",
      "[Training Epoch 4] Batch 2645, Loss 0.26361948251724243\n",
      "[Training Epoch 4] Batch 2646, Loss 0.25941845774650574\n",
      "[Training Epoch 4] Batch 2647, Loss 0.2765679359436035\n",
      "[Training Epoch 4] Batch 2648, Loss 0.26734912395477295\n",
      "[Training Epoch 4] Batch 2649, Loss 0.2593615651130676\n",
      "[Training Epoch 4] Batch 2650, Loss 0.2704430818557739\n",
      "[Training Epoch 4] Batch 2651, Loss 0.2508813440799713\n",
      "[Training Epoch 4] Batch 2652, Loss 0.23171129822731018\n",
      "[Training Epoch 4] Batch 2653, Loss 0.2760467827320099\n",
      "[Training Epoch 4] Batch 2654, Loss 0.2602730393409729\n",
      "[Training Epoch 4] Batch 2655, Loss 0.29769253730773926\n",
      "[Training Epoch 4] Batch 2656, Loss 0.27802467346191406\n",
      "[Training Epoch 4] Batch 2657, Loss 0.2675536274909973\n",
      "[Training Epoch 4] Batch 2658, Loss 0.2852873206138611\n",
      "[Training Epoch 4] Batch 2659, Loss 0.2916598320007324\n",
      "[Training Epoch 4] Batch 2660, Loss 0.2681379020214081\n",
      "[Training Epoch 4] Batch 2661, Loss 0.25740915536880493\n",
      "[Training Epoch 4] Batch 2662, Loss 0.2557598352432251\n",
      "[Training Epoch 4] Batch 2663, Loss 0.26435911655426025\n",
      "[Training Epoch 4] Batch 2664, Loss 0.27635064721107483\n",
      "[Training Epoch 4] Batch 2665, Loss 0.2627813220024109\n",
      "[Training Epoch 4] Batch 2666, Loss 0.2919936776161194\n",
      "[Training Epoch 4] Batch 2667, Loss 0.27563193440437317\n",
      "[Training Epoch 4] Batch 2668, Loss 0.2225734442472458\n",
      "[Training Epoch 4] Batch 2669, Loss 0.29648450016975403\n",
      "[Training Epoch 4] Batch 2670, Loss 0.26223769783973694\n",
      "[Training Epoch 4] Batch 2671, Loss 0.2485818862915039\n",
      "[Training Epoch 4] Batch 2672, Loss 0.27023714780807495\n",
      "[Training Epoch 4] Batch 2673, Loss 0.28015124797821045\n",
      "[Training Epoch 4] Batch 2674, Loss 0.2583577632904053\n",
      "[Training Epoch 4] Batch 2675, Loss 0.2505052387714386\n",
      "[Training Epoch 4] Batch 2676, Loss 0.2766578793525696\n",
      "[Training Epoch 4] Batch 2677, Loss 0.26554644107818604\n",
      "[Training Epoch 4] Batch 2678, Loss 0.2660379409790039\n",
      "[Training Epoch 4] Batch 2679, Loss 0.270725280046463\n",
      "[Training Epoch 4] Batch 2680, Loss 0.29510271549224854\n",
      "[Training Epoch 4] Batch 2681, Loss 0.2805544137954712\n",
      "[Training Epoch 4] Batch 2682, Loss 0.2843065857887268\n",
      "[Training Epoch 4] Batch 2683, Loss 0.2648029625415802\n",
      "[Training Epoch 4] Batch 2684, Loss 0.2829367518424988\n",
      "[Training Epoch 4] Batch 2685, Loss 0.23556829988956451\n",
      "[Training Epoch 4] Batch 2686, Loss 0.23898130655288696\n",
      "[Training Epoch 4] Batch 2687, Loss 0.26936814188957214\n",
      "[Training Epoch 4] Batch 2688, Loss 0.28914201259613037\n",
      "[Training Epoch 4] Batch 2689, Loss 0.26266634464263916\n",
      "[Training Epoch 4] Batch 2690, Loss 0.2620897591114044\n",
      "[Training Epoch 4] Batch 2691, Loss 0.25441741943359375\n",
      "[Training Epoch 4] Batch 2692, Loss 0.25266408920288086\n",
      "[Training Epoch 4] Batch 2693, Loss 0.2941749393939972\n",
      "[Training Epoch 4] Batch 2694, Loss 0.25040942430496216\n",
      "[Training Epoch 4] Batch 2695, Loss 0.24369198083877563\n",
      "[Training Epoch 4] Batch 2696, Loss 0.28071632981300354\n",
      "[Training Epoch 4] Batch 2697, Loss 0.28330475091934204\n",
      "[Training Epoch 4] Batch 2698, Loss 0.28422796726226807\n",
      "[Training Epoch 4] Batch 2699, Loss 0.25943148136138916\n",
      "[Training Epoch 4] Batch 2700, Loss 0.2865124046802521\n",
      "[Training Epoch 4] Batch 2701, Loss 0.2811514735221863\n",
      "[Training Epoch 4] Batch 2702, Loss 0.2510024905204773\n",
      "[Training Epoch 4] Batch 2703, Loss 0.2622584104537964\n",
      "[Training Epoch 4] Batch 2704, Loss 0.2600348889827728\n",
      "[Training Epoch 4] Batch 2705, Loss 0.2714473307132721\n",
      "[Training Epoch 4] Batch 2706, Loss 0.2618575394153595\n",
      "[Training Epoch 4] Batch 2707, Loss 0.25581973791122437\n",
      "[Training Epoch 4] Batch 2708, Loss 0.27876028418540955\n",
      "[Training Epoch 4] Batch 2709, Loss 0.28884732723236084\n",
      "[Training Epoch 4] Batch 2710, Loss 0.26836585998535156\n",
      "[Training Epoch 4] Batch 2711, Loss 0.2809736728668213\n",
      "[Training Epoch 4] Batch 2712, Loss 0.270151823759079\n",
      "[Training Epoch 4] Batch 2713, Loss 0.2801300883293152\n",
      "[Training Epoch 4] Batch 2714, Loss 0.264093816280365\n",
      "[Training Epoch 4] Batch 2715, Loss 0.2484603226184845\n",
      "[Training Epoch 4] Batch 2716, Loss 0.262857586145401\n",
      "[Training Epoch 4] Batch 2717, Loss 0.26642346382141113\n",
      "[Training Epoch 4] Batch 2718, Loss 0.2630435824394226\n",
      "[Training Epoch 4] Batch 2719, Loss 0.2218359112739563\n",
      "[Training Epoch 4] Batch 2720, Loss 0.2777846157550812\n",
      "[Training Epoch 4] Batch 2721, Loss 0.2791934907436371\n",
      "[Training Epoch 4] Batch 2722, Loss 0.25576096773147583\n",
      "[Training Epoch 4] Batch 2723, Loss 0.28865790367126465\n",
      "[Training Epoch 4] Batch 2724, Loss 0.25192946195602417\n",
      "[Training Epoch 4] Batch 2725, Loss 0.2509496510028839\n",
      "[Training Epoch 4] Batch 2726, Loss 0.2503975033760071\n",
      "[Training Epoch 4] Batch 2727, Loss 0.2422580122947693\n",
      "[Training Epoch 4] Batch 2728, Loss 0.3012816607952118\n",
      "[Training Epoch 4] Batch 2729, Loss 0.2624385356903076\n",
      "[Training Epoch 4] Batch 2730, Loss 0.2509632110595703\n",
      "[Training Epoch 4] Batch 2731, Loss 0.2608632445335388\n",
      "[Training Epoch 4] Batch 2732, Loss 0.28787147998809814\n",
      "[Training Epoch 4] Batch 2733, Loss 0.26058298349380493\n",
      "[Training Epoch 4] Batch 2734, Loss 0.25796952843666077\n",
      "[Training Epoch 4] Batch 2735, Loss 0.26003241539001465\n",
      "[Training Epoch 4] Batch 2736, Loss 0.2762241065502167\n",
      "[Training Epoch 4] Batch 2737, Loss 0.2864989638328552\n",
      "[Training Epoch 4] Batch 2738, Loss 0.25210684537887573\n",
      "[Training Epoch 4] Batch 2739, Loss 0.28115952014923096\n",
      "[Training Epoch 4] Batch 2740, Loss 0.243614062666893\n",
      "[Training Epoch 4] Batch 2741, Loss 0.25509700179100037\n",
      "[Training Epoch 4] Batch 2742, Loss 0.2588978409767151\n",
      "[Training Epoch 4] Batch 2743, Loss 0.2606785297393799\n",
      "[Training Epoch 4] Batch 2744, Loss 0.25318312644958496\n",
      "[Training Epoch 4] Batch 2745, Loss 0.25579866766929626\n",
      "[Training Epoch 4] Batch 2746, Loss 0.2589217722415924\n",
      "[Training Epoch 4] Batch 2747, Loss 0.28522953391075134\n",
      "[Training Epoch 4] Batch 2748, Loss 0.253912091255188\n",
      "[Training Epoch 4] Batch 2749, Loss 0.22462251782417297\n",
      "[Training Epoch 4] Batch 2750, Loss 0.2728496193885803\n",
      "[Training Epoch 4] Batch 2751, Loss 0.25507500767707825\n",
      "[Training Epoch 4] Batch 2752, Loss 0.2655829191207886\n",
      "[Training Epoch 4] Batch 2753, Loss 0.24289897084236145\n",
      "[Training Epoch 4] Batch 2754, Loss 0.26597028970718384\n",
      "[Training Epoch 4] Batch 2755, Loss 0.23373711109161377\n",
      "[Training Epoch 4] Batch 2756, Loss 0.31178373098373413\n",
      "[Training Epoch 4] Batch 2757, Loss 0.2763056457042694\n",
      "[Training Epoch 4] Batch 2758, Loss 0.29024070501327515\n",
      "[Training Epoch 4] Batch 2759, Loss 0.24840830266475677\n",
      "[Training Epoch 4] Batch 2760, Loss 0.26620352268218994\n",
      "[Training Epoch 4] Batch 2761, Loss 0.26645809412002563\n",
      "[Training Epoch 4] Batch 2762, Loss 0.24819612503051758\n",
      "[Training Epoch 4] Batch 2763, Loss 0.26953065395355225\n",
      "[Training Epoch 4] Batch 2764, Loss 0.28383249044418335\n",
      "[Training Epoch 4] Batch 2765, Loss 0.24549444019794464\n",
      "[Training Epoch 4] Batch 2766, Loss 0.26247602701187134\n",
      "[Training Epoch 4] Batch 2767, Loss 0.2836124002933502\n",
      "[Training Epoch 4] Batch 2768, Loss 0.2526273727416992\n",
      "[Training Epoch 4] Batch 2769, Loss 0.24858078360557556\n",
      "[Training Epoch 4] Batch 2770, Loss 0.27494925260543823\n",
      "[Training Epoch 4] Batch 2771, Loss 0.26065975427627563\n",
      "[Training Epoch 4] Batch 2772, Loss 0.2753073275089264\n",
      "[Training Epoch 4] Batch 2773, Loss 0.24951936304569244\n",
      "[Training Epoch 4] Batch 2774, Loss 0.26320159435272217\n",
      "[Training Epoch 4] Batch 2775, Loss 0.25516054034233093\n",
      "[Training Epoch 4] Batch 2776, Loss 0.2942339777946472\n",
      "[Training Epoch 4] Batch 2777, Loss 0.3053525984287262\n",
      "[Training Epoch 4] Batch 2778, Loss 0.24564117193222046\n",
      "[Training Epoch 4] Batch 2779, Loss 0.26849591732025146\n",
      "[Training Epoch 4] Batch 2780, Loss 0.30434709787368774\n",
      "[Training Epoch 4] Batch 2781, Loss 0.25477027893066406\n",
      "[Training Epoch 4] Batch 2782, Loss 0.27477550506591797\n",
      "[Training Epoch 4] Batch 2783, Loss 0.27559423446655273\n",
      "[Training Epoch 4] Batch 2784, Loss 0.2573431730270386\n",
      "[Training Epoch 4] Batch 2785, Loss 0.26678985357284546\n",
      "[Training Epoch 4] Batch 2786, Loss 0.24107787013053894\n",
      "[Training Epoch 4] Batch 2787, Loss 0.27375850081443787\n",
      "[Training Epoch 4] Batch 2788, Loss 0.24624758958816528\n",
      "[Training Epoch 4] Batch 2789, Loss 0.2557903528213501\n",
      "[Training Epoch 4] Batch 2790, Loss 0.23666179180145264\n",
      "[Training Epoch 4] Batch 2791, Loss 0.26167917251586914\n",
      "[Training Epoch 4] Batch 2792, Loss 0.2772219181060791\n",
      "[Training Epoch 4] Batch 2793, Loss 0.27422982454299927\n",
      "[Training Epoch 4] Batch 2794, Loss 0.2666577994823456\n",
      "[Training Epoch 4] Batch 2795, Loss 0.23661614954471588\n",
      "[Training Epoch 4] Batch 2796, Loss 0.2521945834159851\n",
      "[Training Epoch 4] Batch 2797, Loss 0.24019789695739746\n",
      "[Training Epoch 4] Batch 2798, Loss 0.20019377768039703\n",
      "[Training Epoch 4] Batch 2799, Loss 0.30545225739479065\n",
      "[Training Epoch 4] Batch 2800, Loss 0.25322526693344116\n",
      "[Training Epoch 4] Batch 2801, Loss 0.2740890681743622\n",
      "[Training Epoch 4] Batch 2802, Loss 0.26334649324417114\n",
      "[Training Epoch 4] Batch 2803, Loss 0.23422229290008545\n",
      "[Training Epoch 4] Batch 2804, Loss 0.25268036127090454\n",
      "[Training Epoch 4] Batch 2805, Loss 0.27146780490875244\n",
      "[Training Epoch 4] Batch 2806, Loss 0.28922101855278015\n",
      "[Training Epoch 4] Batch 2807, Loss 0.23918516933918\n",
      "[Training Epoch 4] Batch 2808, Loss 0.29406484961509705\n",
      "[Training Epoch 4] Batch 2809, Loss 0.25681787729263306\n",
      "[Training Epoch 4] Batch 2810, Loss 0.2695232331752777\n",
      "[Training Epoch 4] Batch 2811, Loss 0.2595636546611786\n",
      "[Training Epoch 4] Batch 2812, Loss 0.24596725404262543\n",
      "[Training Epoch 4] Batch 2813, Loss 0.2387746125459671\n",
      "[Training Epoch 4] Batch 2814, Loss 0.2430383265018463\n",
      "[Training Epoch 4] Batch 2815, Loss 0.2729845643043518\n",
      "[Training Epoch 4] Batch 2816, Loss 0.2771439850330353\n",
      "[Training Epoch 4] Batch 2817, Loss 0.2667875587940216\n",
      "[Training Epoch 4] Batch 2818, Loss 0.2551164925098419\n",
      "[Training Epoch 4] Batch 2819, Loss 0.29280203580856323\n",
      "[Training Epoch 4] Batch 2820, Loss 0.2856539189815521\n",
      "[Training Epoch 4] Batch 2821, Loss 0.2472515106201172\n",
      "[Training Epoch 4] Batch 2822, Loss 0.25164365768432617\n",
      "[Training Epoch 4] Batch 2823, Loss 0.23662684857845306\n",
      "[Training Epoch 4] Batch 2824, Loss 0.26992881298065186\n",
      "[Training Epoch 4] Batch 2825, Loss 0.27114003896713257\n",
      "[Training Epoch 4] Batch 2826, Loss 0.23959514498710632\n",
      "[Training Epoch 4] Batch 2827, Loss 0.26424816250801086\n",
      "[Training Epoch 4] Batch 2828, Loss 0.23137161135673523\n",
      "[Training Epoch 4] Batch 2829, Loss 0.2976739704608917\n",
      "[Training Epoch 4] Batch 2830, Loss 0.2563611567020416\n",
      "[Training Epoch 4] Batch 2831, Loss 0.273457795381546\n",
      "[Training Epoch 4] Batch 2832, Loss 0.2579710781574249\n",
      "[Training Epoch 4] Batch 2833, Loss 0.2676064968109131\n",
      "[Training Epoch 4] Batch 2834, Loss 0.24027302861213684\n",
      "[Training Epoch 4] Batch 2835, Loss 0.2695165276527405\n",
      "[Training Epoch 4] Batch 2836, Loss 0.26325708627700806\n",
      "[Training Epoch 4] Batch 2837, Loss 0.28457075357437134\n",
      "[Training Epoch 4] Batch 2838, Loss 0.25625118613243103\n",
      "[Training Epoch 4] Batch 2839, Loss 0.28564974665641785\n",
      "[Training Epoch 4] Batch 2840, Loss 0.27616769075393677\n",
      "[Training Epoch 4] Batch 2841, Loss 0.2526358366012573\n",
      "[Training Epoch 4] Batch 2842, Loss 0.27813586592674255\n",
      "[Training Epoch 4] Batch 2843, Loss 0.2678535282611847\n",
      "[Training Epoch 4] Batch 2844, Loss 0.24380254745483398\n",
      "[Training Epoch 4] Batch 2845, Loss 0.2755773961544037\n",
      "[Training Epoch 4] Batch 2846, Loss 0.2927582859992981\n",
      "[Training Epoch 4] Batch 2847, Loss 0.27667558193206787\n",
      "[Training Epoch 4] Batch 2848, Loss 0.28977975249290466\n",
      "[Training Epoch 4] Batch 2849, Loss 0.24690735340118408\n",
      "[Training Epoch 4] Batch 2850, Loss 0.2507627606391907\n",
      "[Training Epoch 4] Batch 2851, Loss 0.25646060705184937\n",
      "[Training Epoch 4] Batch 2852, Loss 0.23896095156669617\n",
      "[Training Epoch 4] Batch 2853, Loss 0.2504827380180359\n",
      "[Training Epoch 4] Batch 2854, Loss 0.24462367594242096\n",
      "[Training Epoch 4] Batch 2855, Loss 0.2545933723449707\n",
      "[Training Epoch 4] Batch 2856, Loss 0.24117505550384521\n",
      "[Training Epoch 4] Batch 2857, Loss 0.2509106397628784\n",
      "[Training Epoch 4] Batch 2858, Loss 0.24447345733642578\n",
      "[Training Epoch 4] Batch 2859, Loss 0.27341821789741516\n",
      "[Training Epoch 4] Batch 2860, Loss 0.27126365900039673\n",
      "[Training Epoch 4] Batch 2861, Loss 0.265045166015625\n",
      "[Training Epoch 4] Batch 2862, Loss 0.3019254803657532\n",
      "[Training Epoch 4] Batch 2863, Loss 0.2742478549480438\n",
      "[Training Epoch 4] Batch 2864, Loss 0.25752583146095276\n",
      "[Training Epoch 4] Batch 2865, Loss 0.27376842498779297\n",
      "[Training Epoch 4] Batch 2866, Loss 0.2575471103191376\n",
      "[Training Epoch 4] Batch 2867, Loss 0.25308334827423096\n",
      "[Training Epoch 4] Batch 2868, Loss 0.2536732852458954\n",
      "[Training Epoch 4] Batch 2869, Loss 0.25040119886398315\n",
      "[Training Epoch 4] Batch 2870, Loss 0.26393020153045654\n",
      "[Training Epoch 4] Batch 2871, Loss 0.278399795293808\n",
      "[Training Epoch 4] Batch 2872, Loss 0.26992490887641907\n",
      "[Training Epoch 4] Batch 2873, Loss 0.2545841336250305\n",
      "[Training Epoch 4] Batch 2874, Loss 0.27293580770492554\n",
      "[Training Epoch 4] Batch 2875, Loss 0.2912521958351135\n",
      "[Training Epoch 4] Batch 2876, Loss 0.24263998866081238\n",
      "[Training Epoch 4] Batch 2877, Loss 0.2415221929550171\n",
      "[Training Epoch 4] Batch 2878, Loss 0.2556726336479187\n",
      "[Training Epoch 4] Batch 2879, Loss 0.26288554072380066\n",
      "[Training Epoch 4] Batch 2880, Loss 0.2875581383705139\n",
      "[Training Epoch 4] Batch 2881, Loss 0.2857608199119568\n",
      "[Training Epoch 4] Batch 2882, Loss 0.27149704098701477\n",
      "[Training Epoch 4] Batch 2883, Loss 0.24782319366931915\n",
      "[Training Epoch 4] Batch 2884, Loss 0.27729958295822144\n",
      "[Training Epoch 4] Batch 2885, Loss 0.28538134694099426\n",
      "[Training Epoch 4] Batch 2886, Loss 0.3021159768104553\n",
      "[Training Epoch 4] Batch 2887, Loss 0.22442440688610077\n",
      "[Training Epoch 4] Batch 2888, Loss 0.28228309750556946\n",
      "[Training Epoch 4] Batch 2889, Loss 0.2495993673801422\n",
      "[Training Epoch 4] Batch 2890, Loss 0.25781694054603577\n",
      "[Training Epoch 4] Batch 2891, Loss 0.25275230407714844\n",
      "[Training Epoch 4] Batch 2892, Loss 0.2388656735420227\n",
      "[Training Epoch 4] Batch 2893, Loss 0.23686371743679047\n",
      "[Training Epoch 4] Batch 2894, Loss 0.26148179173469543\n",
      "[Training Epoch 4] Batch 2895, Loss 0.25202256441116333\n",
      "[Training Epoch 4] Batch 2896, Loss 0.27042636275291443\n",
      "[Training Epoch 4] Batch 2897, Loss 0.2492690086364746\n",
      "[Training Epoch 4] Batch 2898, Loss 0.2658887505531311\n",
      "[Training Epoch 4] Batch 2899, Loss 0.23903068900108337\n",
      "[Training Epoch 4] Batch 2900, Loss 0.2589501738548279\n",
      "[Training Epoch 4] Batch 2901, Loss 0.2580280005931854\n",
      "[Training Epoch 4] Batch 2902, Loss 0.25268837809562683\n",
      "[Training Epoch 4] Batch 2903, Loss 0.22850723564624786\n",
      "[Training Epoch 4] Batch 2904, Loss 0.2738993465900421\n",
      "[Training Epoch 4] Batch 2905, Loss 0.26219192147254944\n",
      "[Training Epoch 4] Batch 2906, Loss 0.23821574449539185\n",
      "[Training Epoch 4] Batch 2907, Loss 0.266244113445282\n",
      "[Training Epoch 4] Batch 2908, Loss 0.2500635087490082\n",
      "[Training Epoch 4] Batch 2909, Loss 0.29492175579071045\n",
      "[Training Epoch 4] Batch 2910, Loss 0.26803645491600037\n",
      "[Training Epoch 4] Batch 2911, Loss 0.2602185010910034\n",
      "[Training Epoch 4] Batch 2912, Loss 0.25910699367523193\n",
      "[Training Epoch 4] Batch 2913, Loss 0.2577974200248718\n",
      "[Training Epoch 4] Batch 2914, Loss 0.2747586965560913\n",
      "[Training Epoch 4] Batch 2915, Loss 0.2537676692008972\n",
      "[Training Epoch 4] Batch 2916, Loss 0.29131174087524414\n",
      "[Training Epoch 4] Batch 2917, Loss 0.2668535113334656\n",
      "[Training Epoch 4] Batch 2918, Loss 0.29819613695144653\n",
      "[Training Epoch 4] Batch 2919, Loss 0.3039686679840088\n",
      "[Training Epoch 4] Batch 2920, Loss 0.2527227997779846\n",
      "[Training Epoch 4] Batch 2921, Loss 0.2649993896484375\n",
      "[Training Epoch 4] Batch 2922, Loss 0.24972188472747803\n",
      "[Training Epoch 4] Batch 2923, Loss 0.24674390256404877\n",
      "[Training Epoch 4] Batch 2924, Loss 0.2610923945903778\n",
      "[Training Epoch 4] Batch 2925, Loss 0.26412251591682434\n",
      "[Training Epoch 4] Batch 2926, Loss 0.2583599090576172\n",
      "[Training Epoch 4] Batch 2927, Loss 0.2911895513534546\n",
      "[Training Epoch 4] Batch 2928, Loss 0.23131495714187622\n",
      "[Training Epoch 4] Batch 2929, Loss 0.2725958228111267\n",
      "[Training Epoch 4] Batch 2930, Loss 0.2496607005596161\n",
      "[Training Epoch 4] Batch 2931, Loss 0.24291589856147766\n",
      "[Training Epoch 4] Batch 2932, Loss 0.2920072078704834\n",
      "[Training Epoch 4] Batch 2933, Loss 0.23884299397468567\n",
      "[Training Epoch 4] Batch 2934, Loss 0.27563101053237915\n",
      "[Training Epoch 4] Batch 2935, Loss 0.24843701720237732\n",
      "[Training Epoch 4] Batch 2936, Loss 0.3059762120246887\n",
      "[Training Epoch 4] Batch 2937, Loss 0.24180437624454498\n",
      "[Training Epoch 4] Batch 2938, Loss 0.25377264618873596\n",
      "[Training Epoch 4] Batch 2939, Loss 0.2644549012184143\n",
      "[Training Epoch 4] Batch 2940, Loss 0.2529798150062561\n",
      "[Training Epoch 4] Batch 2941, Loss 0.2392338216304779\n",
      "[Training Epoch 4] Batch 2942, Loss 0.25765299797058105\n",
      "[Training Epoch 4] Batch 2943, Loss 0.2511407434940338\n",
      "[Training Epoch 4] Batch 2944, Loss 0.2650369107723236\n",
      "[Training Epoch 4] Batch 2945, Loss 0.25007620453834534\n",
      "[Training Epoch 4] Batch 2946, Loss 0.26309099793434143\n",
      "[Training Epoch 4] Batch 2947, Loss 0.26509079337120056\n",
      "[Training Epoch 4] Batch 2948, Loss 0.2807939350605011\n",
      "[Training Epoch 4] Batch 2949, Loss 0.27306777238845825\n",
      "[Training Epoch 4] Batch 2950, Loss 0.23011364042758942\n",
      "[Training Epoch 4] Batch 2951, Loss 0.25843435525894165\n",
      "[Training Epoch 4] Batch 2952, Loss 0.23390313982963562\n",
      "[Training Epoch 4] Batch 2953, Loss 0.2366456687450409\n",
      "[Training Epoch 4] Batch 2954, Loss 0.2766101658344269\n",
      "[Training Epoch 4] Batch 2955, Loss 0.27777624130249023\n",
      "[Training Epoch 4] Batch 2956, Loss 0.2798367142677307\n",
      "[Training Epoch 4] Batch 2957, Loss 0.2609602212905884\n",
      "[Training Epoch 4] Batch 2958, Loss 0.27586227655410767\n",
      "[Training Epoch 4] Batch 2959, Loss 0.2876608967781067\n",
      "[Training Epoch 4] Batch 2960, Loss 0.26467961072921753\n",
      "[Training Epoch 4] Batch 2961, Loss 0.2821907699108124\n",
      "[Training Epoch 4] Batch 2962, Loss 0.289345920085907\n",
      "[Training Epoch 4] Batch 2963, Loss 0.267203688621521\n",
      "[Training Epoch 4] Batch 2964, Loss 0.25817060470581055\n",
      "[Training Epoch 4] Batch 2965, Loss 0.25169312953948975\n",
      "[Training Epoch 4] Batch 2966, Loss 0.26734375953674316\n",
      "[Training Epoch 4] Batch 2967, Loss 0.2798781991004944\n",
      "[Training Epoch 4] Batch 2968, Loss 0.2731674015522003\n",
      "[Training Epoch 4] Batch 2969, Loss 0.2801779508590698\n",
      "[Training Epoch 4] Batch 2970, Loss 0.2885335087776184\n",
      "[Training Epoch 4] Batch 2971, Loss 0.266524076461792\n",
      "[Training Epoch 4] Batch 2972, Loss 0.26988154649734497\n",
      "[Training Epoch 4] Batch 2973, Loss 0.2720382809638977\n",
      "[Training Epoch 4] Batch 2974, Loss 0.27713656425476074\n",
      "[Training Epoch 4] Batch 2975, Loss 0.26172715425491333\n",
      "[Training Epoch 4] Batch 2976, Loss 0.2775996923446655\n",
      "[Training Epoch 4] Batch 2977, Loss 0.2501267194747925\n",
      "[Training Epoch 4] Batch 2978, Loss 0.22937867045402527\n",
      "[Training Epoch 4] Batch 2979, Loss 0.25027796626091003\n",
      "[Training Epoch 4] Batch 2980, Loss 0.2656406760215759\n",
      "[Training Epoch 4] Batch 2981, Loss 0.24837368726730347\n",
      "[Training Epoch 4] Batch 2982, Loss 0.24892953038215637\n",
      "[Training Epoch 4] Batch 2983, Loss 0.27453190088272095\n",
      "[Training Epoch 4] Batch 2984, Loss 0.2677692770957947\n",
      "[Training Epoch 4] Batch 2985, Loss 0.26141709089279175\n",
      "[Training Epoch 4] Batch 2986, Loss 0.29702162742614746\n",
      "[Training Epoch 4] Batch 2987, Loss 0.2984769940376282\n",
      "[Training Epoch 4] Batch 2988, Loss 0.24833500385284424\n",
      "[Training Epoch 4] Batch 2989, Loss 0.27038854360580444\n",
      "[Training Epoch 4] Batch 2990, Loss 0.2786332368850708\n",
      "[Training Epoch 4] Batch 2991, Loss 0.24639281630516052\n",
      "[Training Epoch 4] Batch 2992, Loss 0.29687464237213135\n",
      "[Training Epoch 4] Batch 2993, Loss 0.2615148425102234\n",
      "[Training Epoch 4] Batch 2994, Loss 0.2654074430465698\n",
      "[Training Epoch 4] Batch 2995, Loss 0.2573104500770569\n",
      "[Training Epoch 4] Batch 2996, Loss 0.2590256929397583\n",
      "[Training Epoch 4] Batch 2997, Loss 0.23165450990200043\n",
      "[Training Epoch 4] Batch 2998, Loss 0.2751816511154175\n",
      "[Training Epoch 4] Batch 2999, Loss 0.24087224900722504\n",
      "[Training Epoch 4] Batch 3000, Loss 0.2698366045951843\n",
      "[Training Epoch 4] Batch 3001, Loss 0.2937353253364563\n",
      "[Training Epoch 4] Batch 3002, Loss 0.2671630084514618\n",
      "[Training Epoch 4] Batch 3003, Loss 0.2685973048210144\n",
      "[Training Epoch 4] Batch 3004, Loss 0.2800270915031433\n",
      "[Training Epoch 4] Batch 3005, Loss 0.2935245931148529\n",
      "[Training Epoch 4] Batch 3006, Loss 0.23566797375679016\n",
      "[Training Epoch 4] Batch 3007, Loss 0.289005845785141\n",
      "[Training Epoch 4] Batch 3008, Loss 0.26478564739227295\n",
      "[Training Epoch 4] Batch 3009, Loss 0.24338173866271973\n",
      "[Training Epoch 4] Batch 3010, Loss 0.26581525802612305\n",
      "[Training Epoch 4] Batch 3011, Loss 0.2644893527030945\n",
      "[Training Epoch 4] Batch 3012, Loss 0.2534887194633484\n",
      "[Training Epoch 4] Batch 3013, Loss 0.26107871532440186\n",
      "[Training Epoch 4] Batch 3014, Loss 0.2585161328315735\n",
      "[Training Epoch 4] Batch 3015, Loss 0.26336371898651123\n",
      "[Training Epoch 4] Batch 3016, Loss 0.2817506790161133\n",
      "[Training Epoch 4] Batch 3017, Loss 0.2511519491672516\n",
      "[Training Epoch 4] Batch 3018, Loss 0.25057274103164673\n",
      "[Training Epoch 4] Batch 3019, Loss 0.2837696075439453\n",
      "[Training Epoch 4] Batch 3020, Loss 0.2551978528499603\n",
      "[Training Epoch 4] Batch 3021, Loss 0.2552182376384735\n",
      "[Training Epoch 4] Batch 3022, Loss 0.2612271308898926\n",
      "[Training Epoch 4] Batch 3023, Loss 0.2456144541501999\n",
      "[Training Epoch 4] Batch 3024, Loss 0.28215932846069336\n",
      "[Training Epoch 4] Batch 3025, Loss 0.26830583810806274\n",
      "[Training Epoch 4] Batch 3026, Loss 0.2683871388435364\n",
      "[Training Epoch 4] Batch 3027, Loss 0.2581455111503601\n",
      "[Training Epoch 4] Batch 3028, Loss 0.2350994199514389\n",
      "[Training Epoch 4] Batch 3029, Loss 0.2881345748901367\n",
      "[Training Epoch 4] Batch 3030, Loss 0.2663500905036926\n",
      "[Training Epoch 4] Batch 3031, Loss 0.30273836851119995\n",
      "[Training Epoch 4] Batch 3032, Loss 0.24712838232517242\n",
      "[Training Epoch 4] Batch 3033, Loss 0.2639479339122772\n",
      "[Training Epoch 4] Batch 3034, Loss 0.25855955481529236\n",
      "[Training Epoch 4] Batch 3035, Loss 0.26463910937309265\n",
      "[Training Epoch 4] Batch 3036, Loss 0.26771748065948486\n",
      "[Training Epoch 4] Batch 3037, Loss 0.25235289335250854\n",
      "[Training Epoch 4] Batch 3038, Loss 0.2821882367134094\n",
      "[Training Epoch 4] Batch 3039, Loss 0.24123167991638184\n",
      "[Training Epoch 4] Batch 3040, Loss 0.2801375091075897\n",
      "[Training Epoch 4] Batch 3041, Loss 0.2523288428783417\n",
      "[Training Epoch 4] Batch 3042, Loss 0.2520396113395691\n",
      "[Training Epoch 4] Batch 3043, Loss 0.26492977142333984\n",
      "[Training Epoch 4] Batch 3044, Loss 0.2559111416339874\n",
      "[Training Epoch 4] Batch 3045, Loss 0.2756829261779785\n",
      "[Training Epoch 4] Batch 3046, Loss 0.26540786027908325\n",
      "[Training Epoch 4] Batch 3047, Loss 0.2535724639892578\n",
      "[Training Epoch 4] Batch 3048, Loss 0.25025537610054016\n",
      "[Training Epoch 4] Batch 3049, Loss 0.29420286417007446\n",
      "[Training Epoch 4] Batch 3050, Loss 0.2668744921684265\n",
      "[Training Epoch 4] Batch 3051, Loss 0.26929283142089844\n",
      "[Training Epoch 4] Batch 3052, Loss 0.3016217350959778\n",
      "[Training Epoch 4] Batch 3053, Loss 0.2535702884197235\n",
      "[Training Epoch 4] Batch 3054, Loss 0.25129568576812744\n",
      "[Training Epoch 4] Batch 3055, Loss 0.2645059823989868\n",
      "[Training Epoch 4] Batch 3056, Loss 0.25990593433380127\n",
      "[Training Epoch 4] Batch 3057, Loss 0.24421697854995728\n",
      "[Training Epoch 4] Batch 3058, Loss 0.27356186509132385\n",
      "[Training Epoch 4] Batch 3059, Loss 0.2701702117919922\n",
      "[Training Epoch 4] Batch 3060, Loss 0.28992632031440735\n",
      "[Training Epoch 4] Batch 3061, Loss 0.22092294692993164\n",
      "[Training Epoch 4] Batch 3062, Loss 0.2623232305049896\n",
      "[Training Epoch 4] Batch 3063, Loss 0.275333046913147\n",
      "[Training Epoch 4] Batch 3064, Loss 0.2465989887714386\n",
      "[Training Epoch 4] Batch 3065, Loss 0.2740703225135803\n",
      "[Training Epoch 4] Batch 3066, Loss 0.2550753355026245\n",
      "[Training Epoch 4] Batch 3067, Loss 0.23027843236923218\n",
      "[Training Epoch 4] Batch 3068, Loss 0.2639589309692383\n",
      "[Training Epoch 4] Batch 3069, Loss 0.27626556158065796\n",
      "[Training Epoch 4] Batch 3070, Loss 0.23519788682460785\n",
      "[Training Epoch 4] Batch 3071, Loss 0.2629537582397461\n",
      "[Training Epoch 4] Batch 3072, Loss 0.24877256155014038\n",
      "[Training Epoch 4] Batch 3073, Loss 0.26888707280158997\n",
      "[Training Epoch 4] Batch 3074, Loss 0.26728975772857666\n",
      "[Training Epoch 4] Batch 3075, Loss 0.23392605781555176\n",
      "[Training Epoch 4] Batch 3076, Loss 0.3041384220123291\n",
      "[Training Epoch 4] Batch 3077, Loss 0.27141711115837097\n",
      "[Training Epoch 4] Batch 3078, Loss 0.26240274310112\n",
      "[Training Epoch 4] Batch 3079, Loss 0.2473108470439911\n",
      "[Training Epoch 4] Batch 3080, Loss 0.2753247022628784\n",
      "[Training Epoch 4] Batch 3081, Loss 0.23996059596538544\n",
      "[Training Epoch 4] Batch 3082, Loss 0.2539364993572235\n",
      "[Training Epoch 4] Batch 3083, Loss 0.280070424079895\n",
      "[Training Epoch 4] Batch 3084, Loss 0.28206831216812134\n",
      "[Training Epoch 4] Batch 3085, Loss 0.2640801966190338\n",
      "[Training Epoch 4] Batch 3086, Loss 0.2689098119735718\n",
      "[Training Epoch 4] Batch 3087, Loss 0.25831252336502075\n",
      "[Training Epoch 4] Batch 3088, Loss 0.26976901292800903\n",
      "[Training Epoch 4] Batch 3089, Loss 0.2597029507160187\n",
      "[Training Epoch 4] Batch 3090, Loss 0.2722128629684448\n",
      "[Training Epoch 4] Batch 3091, Loss 0.26779988408088684\n",
      "[Training Epoch 4] Batch 3092, Loss 0.2779542803764343\n",
      "[Training Epoch 4] Batch 3093, Loss 0.2327844649553299\n",
      "[Training Epoch 4] Batch 3094, Loss 0.28065377473831177\n",
      "[Training Epoch 4] Batch 3095, Loss 0.27277910709381104\n",
      "[Training Epoch 4] Batch 3096, Loss 0.28348976373672485\n",
      "[Training Epoch 4] Batch 3097, Loss 0.2582758069038391\n",
      "[Training Epoch 4] Batch 3098, Loss 0.244159996509552\n",
      "[Training Epoch 4] Batch 3099, Loss 0.3030247092247009\n",
      "[Training Epoch 4] Batch 3100, Loss 0.26936331391334534\n",
      "[Training Epoch 4] Batch 3101, Loss 0.2522199749946594\n",
      "[Training Epoch 4] Batch 3102, Loss 0.2689054012298584\n",
      "[Training Epoch 4] Batch 3103, Loss 0.27220863103866577\n",
      "[Training Epoch 4] Batch 3104, Loss 0.27018266916275024\n",
      "[Training Epoch 4] Batch 3105, Loss 0.2822840213775635\n",
      "[Training Epoch 4] Batch 3106, Loss 0.2577414810657501\n",
      "[Training Epoch 4] Batch 3107, Loss 0.2715035378932953\n",
      "[Training Epoch 4] Batch 3108, Loss 0.2847747802734375\n",
      "[Training Epoch 4] Batch 3109, Loss 0.24052777886390686\n",
      "[Training Epoch 4] Batch 3110, Loss 0.2440347671508789\n",
      "[Training Epoch 4] Batch 3111, Loss 0.2758414149284363\n",
      "[Training Epoch 4] Batch 3112, Loss 0.2682974636554718\n",
      "[Training Epoch 4] Batch 3113, Loss 0.27187973260879517\n",
      "[Training Epoch 4] Batch 3114, Loss 0.27047520875930786\n",
      "[Training Epoch 4] Batch 3115, Loss 0.28866028785705566\n",
      "[Training Epoch 4] Batch 3116, Loss 0.2452848255634308\n",
      "[Training Epoch 4] Batch 3117, Loss 0.2596225142478943\n",
      "[Training Epoch 4] Batch 3118, Loss 0.2951657772064209\n",
      "[Training Epoch 4] Batch 3119, Loss 0.2720012068748474\n",
      "[Training Epoch 4] Batch 3120, Loss 0.24220481514930725\n",
      "[Training Epoch 4] Batch 3121, Loss 0.2866430878639221\n",
      "[Training Epoch 4] Batch 3122, Loss 0.2897593677043915\n",
      "[Training Epoch 4] Batch 3123, Loss 0.27886396646499634\n",
      "[Training Epoch 4] Batch 3124, Loss 0.24142798781394958\n",
      "[Training Epoch 4] Batch 3125, Loss 0.26305943727493286\n",
      "[Training Epoch 4] Batch 3126, Loss 0.23481041193008423\n",
      "[Training Epoch 4] Batch 3127, Loss 0.2598666250705719\n",
      "[Training Epoch 4] Batch 3128, Loss 0.25873005390167236\n",
      "[Training Epoch 4] Batch 3129, Loss 0.279086172580719\n",
      "[Training Epoch 4] Batch 3130, Loss 0.2599272131919861\n",
      "[Training Epoch 4] Batch 3131, Loss 0.27510055899620056\n",
      "[Training Epoch 4] Batch 3132, Loss 0.26139360666275024\n",
      "[Training Epoch 4] Batch 3133, Loss 0.23451103270053864\n",
      "[Training Epoch 4] Batch 3134, Loss 0.27368396520614624\n",
      "[Training Epoch 4] Batch 3135, Loss 0.2514779567718506\n",
      "[Training Epoch 4] Batch 3136, Loss 0.24776987731456757\n",
      "[Training Epoch 4] Batch 3137, Loss 0.24182751774787903\n",
      "[Training Epoch 4] Batch 3138, Loss 0.24270769953727722\n",
      "[Training Epoch 4] Batch 3139, Loss 0.27063995599746704\n",
      "[Training Epoch 4] Batch 3140, Loss 0.25307369232177734\n",
      "[Training Epoch 4] Batch 3141, Loss 0.2679889500141144\n",
      "[Training Epoch 4] Batch 3142, Loss 0.24424540996551514\n",
      "[Training Epoch 4] Batch 3143, Loss 0.30066442489624023\n",
      "[Training Epoch 4] Batch 3144, Loss 0.28184783458709717\n",
      "[Training Epoch 4] Batch 3145, Loss 0.25389721989631653\n",
      "[Training Epoch 4] Batch 3146, Loss 0.2699834108352661\n",
      "[Training Epoch 4] Batch 3147, Loss 0.2510136365890503\n",
      "[Training Epoch 4] Batch 3148, Loss 0.2739790678024292\n",
      "[Training Epoch 4] Batch 3149, Loss 0.2572886347770691\n",
      "[Training Epoch 4] Batch 3150, Loss 0.2354123592376709\n",
      "[Training Epoch 4] Batch 3151, Loss 0.25935453176498413\n",
      "[Training Epoch 4] Batch 3152, Loss 0.2617156505584717\n",
      "[Training Epoch 4] Batch 3153, Loss 0.24307045340538025\n",
      "[Training Epoch 4] Batch 3154, Loss 0.2899705767631531\n",
      "[Training Epoch 4] Batch 3155, Loss 0.2740214765071869\n",
      "[Training Epoch 4] Batch 3156, Loss 0.25957268476486206\n",
      "[Training Epoch 4] Batch 3157, Loss 0.27303004264831543\n",
      "[Training Epoch 4] Batch 3158, Loss 0.275940477848053\n",
      "[Training Epoch 4] Batch 3159, Loss 0.22365280985832214\n",
      "[Training Epoch 4] Batch 3160, Loss 0.24478504061698914\n",
      "[Training Epoch 4] Batch 3161, Loss 0.257951945066452\n",
      "[Training Epoch 4] Batch 3162, Loss 0.2904092073440552\n",
      "[Training Epoch 4] Batch 3163, Loss 0.2604484558105469\n",
      "[Training Epoch 4] Batch 3164, Loss 0.2537170648574829\n",
      "[Training Epoch 4] Batch 3165, Loss 0.26152917742729187\n",
      "[Training Epoch 4] Batch 3166, Loss 0.26167088747024536\n",
      "[Training Epoch 4] Batch 3167, Loss 0.26237571239471436\n",
      "[Training Epoch 4] Batch 3168, Loss 0.2494322955608368\n",
      "[Training Epoch 4] Batch 3169, Loss 0.24725238978862762\n",
      "[Training Epoch 4] Batch 3170, Loss 0.2702633738517761\n",
      "[Training Epoch 4] Batch 3171, Loss 0.28031957149505615\n",
      "[Training Epoch 4] Batch 3172, Loss 0.2519364655017853\n",
      "[Training Epoch 4] Batch 3173, Loss 0.27340707182884216\n",
      "[Training Epoch 4] Batch 3174, Loss 0.2702457904815674\n",
      "[Training Epoch 4] Batch 3175, Loss 0.2808569371700287\n",
      "[Training Epoch 4] Batch 3176, Loss 0.254200279712677\n",
      "[Training Epoch 4] Batch 3177, Loss 0.28044143319129944\n",
      "[Training Epoch 4] Batch 3178, Loss 0.2626338005065918\n",
      "[Training Epoch 4] Batch 3179, Loss 0.2398364543914795\n",
      "[Training Epoch 4] Batch 3180, Loss 0.2535465359687805\n",
      "[Training Epoch 4] Batch 3181, Loss 0.23029771447181702\n",
      "[Training Epoch 4] Batch 3182, Loss 0.27118223905563354\n",
      "[Training Epoch 4] Batch 3183, Loss 0.278364360332489\n",
      "[Training Epoch 4] Batch 3184, Loss 0.22791796922683716\n",
      "[Training Epoch 4] Batch 3185, Loss 0.2620422840118408\n",
      "[Training Epoch 4] Batch 3186, Loss 0.25869208574295044\n",
      "[Training Epoch 4] Batch 3187, Loss 0.2541741132736206\n",
      "[Training Epoch 4] Batch 3188, Loss 0.2699393033981323\n",
      "[Training Epoch 4] Batch 3189, Loss 0.2608134150505066\n",
      "[Training Epoch 4] Batch 3190, Loss 0.2924756705760956\n",
      "[Training Epoch 4] Batch 3191, Loss 0.31139111518859863\n",
      "[Training Epoch 4] Batch 3192, Loss 0.2634422779083252\n",
      "[Training Epoch 4] Batch 3193, Loss 0.26156771183013916\n",
      "[Training Epoch 4] Batch 3194, Loss 0.26941314339637756\n",
      "[Training Epoch 4] Batch 3195, Loss 0.24978575110435486\n",
      "[Training Epoch 4] Batch 3196, Loss 0.2660079002380371\n",
      "[Training Epoch 4] Batch 3197, Loss 0.2817584276199341\n",
      "[Training Epoch 4] Batch 3198, Loss 0.28245216608047485\n",
      "[Training Epoch 4] Batch 3199, Loss 0.24612930417060852\n",
      "[Training Epoch 4] Batch 3200, Loss 0.28070539236068726\n",
      "[Training Epoch 4] Batch 3201, Loss 0.26143068075180054\n",
      "[Training Epoch 4] Batch 3202, Loss 0.2546567916870117\n",
      "[Training Epoch 4] Batch 3203, Loss 0.2664470970630646\n",
      "[Training Epoch 4] Batch 3204, Loss 0.266588032245636\n",
      "[Training Epoch 4] Batch 3205, Loss 0.2769082486629486\n",
      "[Training Epoch 4] Batch 3206, Loss 0.23903626203536987\n",
      "[Training Epoch 4] Batch 3207, Loss 0.27528494596481323\n",
      "[Training Epoch 4] Batch 3208, Loss 0.28052398562431335\n",
      "[Training Epoch 4] Batch 3209, Loss 0.2619916796684265\n",
      "[Training Epoch 4] Batch 3210, Loss 0.2686029076576233\n",
      "[Training Epoch 4] Batch 3211, Loss 0.28766006231307983\n",
      "[Training Epoch 4] Batch 3212, Loss 0.2639297842979431\n",
      "[Training Epoch 4] Batch 3213, Loss 0.2555816173553467\n",
      "[Training Epoch 4] Batch 3214, Loss 0.23700115084648132\n",
      "[Training Epoch 4] Batch 3215, Loss 0.24479791522026062\n",
      "[Training Epoch 4] Batch 3216, Loss 0.25591254234313965\n",
      "[Training Epoch 4] Batch 3217, Loss 0.2609001398086548\n",
      "[Training Epoch 4] Batch 3218, Loss 0.26810723543167114\n",
      "[Training Epoch 4] Batch 3219, Loss 0.24946144223213196\n",
      "[Training Epoch 4] Batch 3220, Loss 0.2642393708229065\n",
      "[Training Epoch 4] Batch 3221, Loss 0.21786656975746155\n",
      "[Training Epoch 4] Batch 3222, Loss 0.2700526714324951\n",
      "[Training Epoch 4] Batch 3223, Loss 0.24239803850650787\n",
      "[Training Epoch 4] Batch 3224, Loss 0.2481488585472107\n",
      "[Training Epoch 4] Batch 3225, Loss 0.26474177837371826\n",
      "[Training Epoch 4] Batch 3226, Loss 0.272527277469635\n",
      "[Training Epoch 4] Batch 3227, Loss 0.28423622250556946\n",
      "[Training Epoch 4] Batch 3228, Loss 0.24336574971675873\n",
      "[Training Epoch 4] Batch 3229, Loss 0.2737542986869812\n",
      "[Training Epoch 4] Batch 3230, Loss 0.24352949857711792\n",
      "[Training Epoch 4] Batch 3231, Loss 0.2541124224662781\n",
      "[Training Epoch 4] Batch 3232, Loss 0.2427002638578415\n",
      "[Training Epoch 4] Batch 3233, Loss 0.2914816737174988\n",
      "[Training Epoch 4] Batch 3234, Loss 0.28969937562942505\n",
      "[Training Epoch 4] Batch 3235, Loss 0.27768903970718384\n",
      "[Training Epoch 4] Batch 3236, Loss 0.2965819239616394\n",
      "[Training Epoch 4] Batch 3237, Loss 0.2976377308368683\n",
      "[Training Epoch 4] Batch 3238, Loss 0.2603335678577423\n",
      "[Training Epoch 4] Batch 3239, Loss 0.29379940032958984\n",
      "[Training Epoch 4] Batch 3240, Loss 0.24847210943698883\n",
      "[Training Epoch 4] Batch 3241, Loss 0.2731638550758362\n",
      "[Training Epoch 4] Batch 3242, Loss 0.23453915119171143\n",
      "[Training Epoch 4] Batch 3243, Loss 0.27057167887687683\n",
      "[Training Epoch 4] Batch 3244, Loss 0.2574414014816284\n",
      "[Training Epoch 4] Batch 3245, Loss 0.2618626058101654\n",
      "[Training Epoch 4] Batch 3246, Loss 0.2709614932537079\n",
      "[Training Epoch 4] Batch 3247, Loss 0.27482157945632935\n",
      "[Training Epoch 4] Batch 3248, Loss 0.2661440670490265\n",
      "[Training Epoch 4] Batch 3249, Loss 0.23553794622421265\n",
      "[Training Epoch 4] Batch 3250, Loss 0.3015292286872864\n",
      "[Training Epoch 4] Batch 3251, Loss 0.24862529337406158\n",
      "[Training Epoch 4] Batch 3252, Loss 0.24700459837913513\n",
      "[Training Epoch 4] Batch 3253, Loss 0.3096437454223633\n",
      "[Training Epoch 4] Batch 3254, Loss 0.2670033574104309\n",
      "[Training Epoch 4] Batch 3255, Loss 0.295193076133728\n",
      "[Training Epoch 4] Batch 3256, Loss 0.27382341027259827\n",
      "[Training Epoch 4] Batch 3257, Loss 0.26546812057495117\n",
      "[Training Epoch 4] Batch 3258, Loss 0.26207247376441956\n",
      "[Training Epoch 4] Batch 3259, Loss 0.23640763759613037\n",
      "[Training Epoch 4] Batch 3260, Loss 0.255073219537735\n",
      "[Training Epoch 4] Batch 3261, Loss 0.24949778616428375\n",
      "[Training Epoch 4] Batch 3262, Loss 0.28668779134750366\n",
      "[Training Epoch 4] Batch 3263, Loss 0.22839917242527008\n",
      "[Training Epoch 4] Batch 3264, Loss 0.29111242294311523\n",
      "[Training Epoch 4] Batch 3265, Loss 0.26655879616737366\n",
      "[Training Epoch 4] Batch 3266, Loss 0.25062140822410583\n",
      "[Training Epoch 4] Batch 3267, Loss 0.2768622040748596\n",
      "[Training Epoch 4] Batch 3268, Loss 0.251523494720459\n",
      "[Training Epoch 4] Batch 3269, Loss 0.25202488899230957\n",
      "[Training Epoch 4] Batch 3270, Loss 0.24889159202575684\n",
      "[Training Epoch 4] Batch 3271, Loss 0.26114803552627563\n",
      "[Training Epoch 4] Batch 3272, Loss 0.251074880361557\n",
      "[Training Epoch 4] Batch 3273, Loss 0.2573625147342682\n",
      "[Training Epoch 4] Batch 3274, Loss 0.25527435541152954\n",
      "[Training Epoch 4] Batch 3275, Loss 0.2619880437850952\n",
      "[Training Epoch 4] Batch 3276, Loss 0.2841554284095764\n",
      "[Training Epoch 4] Batch 3277, Loss 0.2630888819694519\n",
      "[Training Epoch 4] Batch 3278, Loss 0.2609635293483734\n",
      "[Training Epoch 4] Batch 3279, Loss 0.2679256200790405\n",
      "[Training Epoch 4] Batch 3280, Loss 0.2691479027271271\n",
      "[Training Epoch 4] Batch 3281, Loss 0.2527036964893341\n",
      "[Training Epoch 4] Batch 3282, Loss 0.2651497721672058\n",
      "[Training Epoch 4] Batch 3283, Loss 0.2580500543117523\n",
      "[Training Epoch 4] Batch 3284, Loss 0.2615118622779846\n",
      "[Training Epoch 4] Batch 3285, Loss 0.22103707492351532\n",
      "[Training Epoch 4] Batch 3286, Loss 0.2717997431755066\n",
      "[Training Epoch 4] Batch 3287, Loss 0.25708985328674316\n",
      "[Training Epoch 4] Batch 3288, Loss 0.2598317861557007\n",
      "[Training Epoch 4] Batch 3289, Loss 0.27316781878471375\n",
      "[Training Epoch 4] Batch 3290, Loss 0.2556038796901703\n",
      "[Training Epoch 4] Batch 3291, Loss 0.23486065864562988\n",
      "[Training Epoch 4] Batch 3292, Loss 0.24662388861179352\n",
      "[Training Epoch 4] Batch 3293, Loss 0.25469735264778137\n",
      "[Training Epoch 4] Batch 3294, Loss 0.25711148977279663\n",
      "[Training Epoch 4] Batch 3295, Loss 0.2552567422389984\n",
      "[Training Epoch 4] Batch 3296, Loss 0.25619685649871826\n",
      "[Training Epoch 4] Batch 3297, Loss 0.2694937288761139\n",
      "[Training Epoch 4] Batch 3298, Loss 0.2319650948047638\n",
      "[Training Epoch 4] Batch 3299, Loss 0.2573874592781067\n",
      "[Training Epoch 4] Batch 3300, Loss 0.2580057382583618\n",
      "[Training Epoch 4] Batch 3301, Loss 0.26558637619018555\n",
      "[Training Epoch 4] Batch 3302, Loss 0.2768825888633728\n",
      "[Training Epoch 4] Batch 3303, Loss 0.2748185992240906\n",
      "[Training Epoch 4] Batch 3304, Loss 0.2707556486129761\n",
      "[Training Epoch 4] Batch 3305, Loss 0.25091493129730225\n",
      "[Training Epoch 4] Batch 3306, Loss 0.24962544441223145\n",
      "[Training Epoch 4] Batch 3307, Loss 0.27812355756759644\n",
      "[Training Epoch 4] Batch 3308, Loss 0.25777292251586914\n",
      "[Training Epoch 4] Batch 3309, Loss 0.2456156313419342\n",
      "[Training Epoch 4] Batch 3310, Loss 0.2504095435142517\n",
      "[Training Epoch 4] Batch 3311, Loss 0.2582992613315582\n",
      "[Training Epoch 4] Batch 3312, Loss 0.254869282245636\n",
      "[Training Epoch 4] Batch 3313, Loss 0.26316654682159424\n",
      "[Training Epoch 4] Batch 3314, Loss 0.28273701667785645\n",
      "[Training Epoch 4] Batch 3315, Loss 0.2584148049354553\n",
      "[Training Epoch 4] Batch 3316, Loss 0.24901461601257324\n",
      "[Training Epoch 4] Batch 3317, Loss 0.26697713136672974\n",
      "[Training Epoch 4] Batch 3318, Loss 0.2952529788017273\n",
      "[Training Epoch 4] Batch 3319, Loss 0.26226091384887695\n",
      "[Training Epoch 4] Batch 3320, Loss 0.2622692584991455\n",
      "[Training Epoch 4] Batch 3321, Loss 0.27074626088142395\n",
      "[Training Epoch 4] Batch 3322, Loss 0.2580048739910126\n",
      "[Training Epoch 4] Batch 3323, Loss 0.25488775968551636\n",
      "[Training Epoch 4] Batch 3324, Loss 0.28570228815078735\n",
      "[Training Epoch 4] Batch 3325, Loss 0.28342655301094055\n",
      "[Training Epoch 4] Batch 3326, Loss 0.27160751819610596\n",
      "[Training Epoch 4] Batch 3327, Loss 0.2914998531341553\n",
      "[Training Epoch 4] Batch 3328, Loss 0.2668752074241638\n",
      "[Training Epoch 4] Batch 3329, Loss 0.2744103968143463\n",
      "[Training Epoch 4] Batch 3330, Loss 0.2593804895877838\n",
      "[Training Epoch 4] Batch 3331, Loss 0.2741699516773224\n",
      "[Training Epoch 4] Batch 3332, Loss 0.29353904724121094\n",
      "[Training Epoch 4] Batch 3333, Loss 0.2813355326652527\n",
      "[Training Epoch 4] Batch 3334, Loss 0.2740178108215332\n",
      "[Training Epoch 4] Batch 3335, Loss 0.27123352885246277\n",
      "[Training Epoch 4] Batch 3336, Loss 0.25941431522369385\n",
      "[Training Epoch 4] Batch 3337, Loss 0.2542581260204315\n",
      "[Training Epoch 4] Batch 3338, Loss 0.2537544369697571\n",
      "[Training Epoch 4] Batch 3339, Loss 0.2535581588745117\n",
      "[Training Epoch 4] Batch 3340, Loss 0.2593461871147156\n",
      "[Training Epoch 4] Batch 3341, Loss 0.25192856788635254\n",
      "[Training Epoch 4] Batch 3342, Loss 0.25927990674972534\n",
      "[Training Epoch 4] Batch 3343, Loss 0.28058966994285583\n",
      "[Training Epoch 4] Batch 3344, Loss 0.253947377204895\n",
      "[Training Epoch 4] Batch 3345, Loss 0.23945002257823944\n",
      "[Training Epoch 4] Batch 3346, Loss 0.25325679779052734\n",
      "[Training Epoch 4] Batch 3347, Loss 0.28696131706237793\n",
      "[Training Epoch 4] Batch 3348, Loss 0.2541274130344391\n",
      "[Training Epoch 4] Batch 3349, Loss 0.27101486921310425\n",
      "[Training Epoch 4] Batch 3350, Loss 0.28418049216270447\n",
      "[Training Epoch 4] Batch 3351, Loss 0.2552427053451538\n",
      "[Training Epoch 4] Batch 3352, Loss 0.2843169569969177\n",
      "[Training Epoch 4] Batch 3353, Loss 0.2530149221420288\n",
      "[Training Epoch 4] Batch 3354, Loss 0.2598038613796234\n",
      "[Training Epoch 4] Batch 3355, Loss 0.25490814447402954\n",
      "[Training Epoch 4] Batch 3356, Loss 0.26917973160743713\n",
      "[Training Epoch 4] Batch 3357, Loss 0.2530285120010376\n",
      "[Training Epoch 4] Batch 3358, Loss 0.24827659130096436\n",
      "[Training Epoch 4] Batch 3359, Loss 0.23568671941757202\n",
      "[Training Epoch 4] Batch 3360, Loss 0.2562220096588135\n",
      "[Training Epoch 4] Batch 3361, Loss 0.265657901763916\n",
      "[Training Epoch 4] Batch 3362, Loss 0.27507370710372925\n",
      "[Training Epoch 4] Batch 3363, Loss 0.25993049144744873\n",
      "[Training Epoch 4] Batch 3364, Loss 0.24659398198127747\n",
      "[Training Epoch 4] Batch 3365, Loss 0.25274744629859924\n",
      "[Training Epoch 4] Batch 3366, Loss 0.26174217462539673\n",
      "[Training Epoch 4] Batch 3367, Loss 0.26311367750167847\n",
      "[Training Epoch 4] Batch 3368, Loss 0.2712968587875366\n",
      "[Training Epoch 4] Batch 3369, Loss 0.2501447796821594\n",
      "[Training Epoch 4] Batch 3370, Loss 0.2404472529888153\n",
      "[Training Epoch 4] Batch 3371, Loss 0.263949453830719\n",
      "[Training Epoch 4] Batch 3372, Loss 0.2656644582748413\n",
      "[Training Epoch 4] Batch 3373, Loss 0.2360643446445465\n",
      "[Training Epoch 4] Batch 3374, Loss 0.26636460423469543\n",
      "[Training Epoch 4] Batch 3375, Loss 0.25474950671195984\n",
      "[Training Epoch 4] Batch 3376, Loss 0.2615020275115967\n",
      "[Training Epoch 4] Batch 3377, Loss 0.24245694279670715\n",
      "[Training Epoch 4] Batch 3378, Loss 0.2569333612918854\n",
      "[Training Epoch 4] Batch 3379, Loss 0.261560320854187\n",
      "[Training Epoch 4] Batch 3380, Loss 0.25977444648742676\n",
      "[Training Epoch 4] Batch 3381, Loss 0.27020925283432007\n",
      "[Training Epoch 4] Batch 3382, Loss 0.263671875\n",
      "[Training Epoch 4] Batch 3383, Loss 0.30259180068969727\n",
      "[Training Epoch 4] Batch 3384, Loss 0.24537226557731628\n",
      "[Training Epoch 4] Batch 3385, Loss 0.23520657420158386\n",
      "[Training Epoch 4] Batch 3386, Loss 0.26208871603012085\n",
      "[Training Epoch 4] Batch 3387, Loss 0.23284131288528442\n",
      "[Training Epoch 4] Batch 3388, Loss 0.22485673427581787\n",
      "[Training Epoch 4] Batch 3389, Loss 0.22152365744113922\n",
      "[Training Epoch 4] Batch 3390, Loss 0.2702539265155792\n",
      "[Training Epoch 4] Batch 3391, Loss 0.25681477785110474\n",
      "[Training Epoch 4] Batch 3392, Loss 0.2644183039665222\n",
      "[Training Epoch 4] Batch 3393, Loss 0.2511780261993408\n",
      "[Training Epoch 4] Batch 3394, Loss 0.2900434136390686\n",
      "[Training Epoch 4] Batch 3395, Loss 0.2545933127403259\n",
      "[Training Epoch 4] Batch 3396, Loss 0.2835830748081207\n",
      "[Training Epoch 4] Batch 3397, Loss 0.25386160612106323\n",
      "[Training Epoch 4] Batch 3398, Loss 0.2601510286331177\n",
      "[Training Epoch 4] Batch 3399, Loss 0.2611617147922516\n",
      "[Training Epoch 4] Batch 3400, Loss 0.2419300377368927\n",
      "[Training Epoch 4] Batch 3401, Loss 0.2326536625623703\n",
      "[Training Epoch 4] Batch 3402, Loss 0.2490067481994629\n",
      "[Training Epoch 4] Batch 3403, Loss 0.2599313259124756\n",
      "[Training Epoch 4] Batch 3404, Loss 0.26884931325912476\n",
      "[Training Epoch 4] Batch 3405, Loss 0.2739347517490387\n",
      "[Training Epoch 4] Batch 3406, Loss 0.27100804448127747\n",
      "[Training Epoch 4] Batch 3407, Loss 0.26328372955322266\n",
      "[Training Epoch 4] Batch 3408, Loss 0.2598891854286194\n",
      "[Training Epoch 4] Batch 3409, Loss 0.2539803385734558\n",
      "[Training Epoch 4] Batch 3410, Loss 0.2790874242782593\n",
      "[Training Epoch 4] Batch 3411, Loss 0.2714863419532776\n",
      "[Training Epoch 4] Batch 3412, Loss 0.2661108374595642\n",
      "[Training Epoch 4] Batch 3413, Loss 0.2517729699611664\n",
      "[Training Epoch 4] Batch 3414, Loss 0.2625397741794586\n",
      "[Training Epoch 4] Batch 3415, Loss 0.2578901946544647\n",
      "[Training Epoch 4] Batch 3416, Loss 0.2543319761753082\n",
      "[Training Epoch 4] Batch 3417, Loss 0.2896115183830261\n",
      "[Training Epoch 4] Batch 3418, Loss 0.2719731032848358\n",
      "[Training Epoch 4] Batch 3419, Loss 0.23975211381912231\n",
      "[Training Epoch 4] Batch 3420, Loss 0.2581726014614105\n",
      "[Training Epoch 4] Batch 3421, Loss 0.26854103803634644\n",
      "[Training Epoch 4] Batch 3422, Loss 0.26512736082077026\n",
      "[Training Epoch 4] Batch 3423, Loss 0.23521141707897186\n",
      "[Training Epoch 4] Batch 3424, Loss 0.2801189422607422\n",
      "[Training Epoch 4] Batch 3425, Loss 0.24319615960121155\n",
      "[Training Epoch 4] Batch 3426, Loss 0.2751445770263672\n",
      "[Training Epoch 4] Batch 3427, Loss 0.27427253127098083\n",
      "[Training Epoch 4] Batch 3428, Loss 0.26404517889022827\n",
      "[Training Epoch 4] Batch 3429, Loss 0.23799365758895874\n",
      "[Training Epoch 4] Batch 3430, Loss 0.2930571436882019\n",
      "[Training Epoch 4] Batch 3431, Loss 0.2360585331916809\n",
      "[Training Epoch 4] Batch 3432, Loss 0.2412586212158203\n",
      "[Training Epoch 4] Batch 3433, Loss 0.3013842701911926\n",
      "[Training Epoch 4] Batch 3434, Loss 0.27948445081710815\n",
      "[Training Epoch 4] Batch 3435, Loss 0.23956534266471863\n",
      "[Training Epoch 4] Batch 3436, Loss 0.25150302052497864\n",
      "[Training Epoch 4] Batch 3437, Loss 0.2498617321252823\n",
      "[Training Epoch 4] Batch 3438, Loss 0.25067296624183655\n",
      "[Training Epoch 4] Batch 3439, Loss 0.2711436450481415\n",
      "[Training Epoch 4] Batch 3440, Loss 0.2730990946292877\n",
      "[Training Epoch 4] Batch 3441, Loss 0.28522399067878723\n",
      "[Training Epoch 4] Batch 3442, Loss 0.26834461092948914\n",
      "[Training Epoch 4] Batch 3443, Loss 0.2583249807357788\n",
      "[Training Epoch 4] Batch 3444, Loss 0.2693386673927307\n",
      "[Training Epoch 4] Batch 3445, Loss 0.2589368224143982\n",
      "[Training Epoch 4] Batch 3446, Loss 0.24950087070465088\n",
      "[Training Epoch 4] Batch 3447, Loss 0.26647529006004333\n",
      "[Training Epoch 4] Batch 3448, Loss 0.27064424753189087\n",
      "[Training Epoch 4] Batch 3449, Loss 0.24570739269256592\n",
      "[Training Epoch 4] Batch 3450, Loss 0.2654856741428375\n",
      "[Training Epoch 4] Batch 3451, Loss 0.28105539083480835\n",
      "[Training Epoch 4] Batch 3452, Loss 0.24634861946105957\n",
      "[Training Epoch 4] Batch 3453, Loss 0.2523655593395233\n",
      "[Training Epoch 4] Batch 3454, Loss 0.2428823709487915\n",
      "[Training Epoch 4] Batch 3455, Loss 0.28716158866882324\n",
      "[Training Epoch 4] Batch 3456, Loss 0.2849823236465454\n",
      "[Training Epoch 4] Batch 3457, Loss 0.26870810985565186\n",
      "[Training Epoch 4] Batch 3458, Loss 0.2585085332393646\n",
      "[Training Epoch 4] Batch 3459, Loss 0.2839469909667969\n",
      "[Training Epoch 4] Batch 3460, Loss 0.2750861644744873\n",
      "[Training Epoch 4] Batch 3461, Loss 0.2911325991153717\n",
      "[Training Epoch 4] Batch 3462, Loss 0.28214508295059204\n",
      "[Training Epoch 4] Batch 3463, Loss 0.26367294788360596\n",
      "[Training Epoch 4] Batch 3464, Loss 0.26732999086380005\n",
      "[Training Epoch 4] Batch 3465, Loss 0.26139017939567566\n",
      "[Training Epoch 4] Batch 3466, Loss 0.2795138955116272\n",
      "[Training Epoch 4] Batch 3467, Loss 0.28461477160453796\n",
      "[Training Epoch 4] Batch 3468, Loss 0.23081040382385254\n",
      "[Training Epoch 4] Batch 3469, Loss 0.25750496983528137\n",
      "[Training Epoch 4] Batch 3470, Loss 0.2511897385120392\n",
      "[Training Epoch 4] Batch 3471, Loss 0.26125428080558777\n",
      "[Training Epoch 4] Batch 3472, Loss 0.2735741138458252\n",
      "[Training Epoch 4] Batch 3473, Loss 0.25487715005874634\n",
      "[Training Epoch 4] Batch 3474, Loss 0.27151021361351013\n",
      "[Training Epoch 4] Batch 3475, Loss 0.270332932472229\n",
      "[Training Epoch 4] Batch 3476, Loss 0.28377068042755127\n",
      "[Training Epoch 4] Batch 3477, Loss 0.3018905520439148\n",
      "[Training Epoch 4] Batch 3478, Loss 0.26594066619873047\n",
      "[Training Epoch 4] Batch 3479, Loss 0.27046966552734375\n",
      "[Training Epoch 4] Batch 3480, Loss 0.252915620803833\n",
      "[Training Epoch 4] Batch 3481, Loss 0.2665427327156067\n",
      "[Training Epoch 4] Batch 3482, Loss 0.25468623638153076\n",
      "[Training Epoch 4] Batch 3483, Loss 0.2828464210033417\n",
      "[Training Epoch 4] Batch 3484, Loss 0.24341994524002075\n",
      "[Training Epoch 4] Batch 3485, Loss 0.27952539920806885\n",
      "[Training Epoch 4] Batch 3486, Loss 0.3100886940956116\n",
      "[Training Epoch 4] Batch 3487, Loss 0.2706473767757416\n",
      "[Training Epoch 4] Batch 3488, Loss 0.25526297092437744\n",
      "[Training Epoch 4] Batch 3489, Loss 0.2578880190849304\n",
      "[Training Epoch 4] Batch 3490, Loss 0.27324509620666504\n",
      "[Training Epoch 4] Batch 3491, Loss 0.28303465247154236\n",
      "[Training Epoch 4] Batch 3492, Loss 0.271313339471817\n",
      "[Training Epoch 4] Batch 3493, Loss 0.26862138509750366\n",
      "[Training Epoch 4] Batch 3494, Loss 0.24635736644268036\n",
      "[Training Epoch 4] Batch 3495, Loss 0.26081663370132446\n",
      "[Training Epoch 4] Batch 3496, Loss 0.26513516902923584\n",
      "[Training Epoch 4] Batch 3497, Loss 0.29698365926742554\n",
      "[Training Epoch 4] Batch 3498, Loss 0.2585565447807312\n",
      "[Training Epoch 4] Batch 3499, Loss 0.2811547517776489\n",
      "[Training Epoch 4] Batch 3500, Loss 0.285338819026947\n",
      "[Training Epoch 4] Batch 3501, Loss 0.26015400886535645\n",
      "[Training Epoch 4] Batch 3502, Loss 0.2767176628112793\n",
      "[Training Epoch 4] Batch 3503, Loss 0.26927638053894043\n",
      "[Training Epoch 4] Batch 3504, Loss 0.24833285808563232\n",
      "[Training Epoch 4] Batch 3505, Loss 0.2703794538974762\n",
      "[Training Epoch 4] Batch 3506, Loss 0.2521047592163086\n",
      "[Training Epoch 4] Batch 3507, Loss 0.28132420778274536\n",
      "[Training Epoch 4] Batch 3508, Loss 0.26389068365097046\n",
      "[Training Epoch 4] Batch 3509, Loss 0.24369178712368011\n",
      "[Training Epoch 4] Batch 3510, Loss 0.26507943868637085\n",
      "[Training Epoch 4] Batch 3511, Loss 0.2760513126850128\n",
      "[Training Epoch 4] Batch 3512, Loss 0.24718214571475983\n",
      "[Training Epoch 4] Batch 3513, Loss 0.24684016406536102\n",
      "[Training Epoch 4] Batch 3514, Loss 0.264757364988327\n",
      "[Training Epoch 4] Batch 3515, Loss 0.2954001724720001\n",
      "[Training Epoch 4] Batch 3516, Loss 0.29636961221694946\n",
      "[Training Epoch 4] Batch 3517, Loss 0.280063271522522\n",
      "[Training Epoch 4] Batch 3518, Loss 0.24322548508644104\n",
      "[Training Epoch 4] Batch 3519, Loss 0.2567063868045807\n",
      "[Training Epoch 4] Batch 3520, Loss 0.2913323640823364\n",
      "[Training Epoch 4] Batch 3521, Loss 0.26913267374038696\n",
      "[Training Epoch 4] Batch 3522, Loss 0.2727457284927368\n",
      "[Training Epoch 4] Batch 3523, Loss 0.2827660143375397\n",
      "[Training Epoch 4] Batch 3524, Loss 0.26665985584259033\n",
      "[Training Epoch 4] Batch 3525, Loss 0.27790242433547974\n",
      "[Training Epoch 4] Batch 3526, Loss 0.26414763927459717\n",
      "[Training Epoch 4] Batch 3527, Loss 0.2866663932800293\n",
      "[Training Epoch 4] Batch 3528, Loss 0.23291277885437012\n",
      "[Training Epoch 4] Batch 3529, Loss 0.2650810182094574\n",
      "[Training Epoch 4] Batch 3530, Loss 0.2520006597042084\n",
      "[Training Epoch 4] Batch 3531, Loss 0.23269352316856384\n",
      "[Training Epoch 4] Batch 3532, Loss 0.2661764323711395\n",
      "[Training Epoch 4] Batch 3533, Loss 0.2505055069923401\n",
      "[Training Epoch 4] Batch 3534, Loss 0.2757786214351654\n",
      "[Training Epoch 4] Batch 3535, Loss 0.25595539808273315\n",
      "[Training Epoch 4] Batch 3536, Loss 0.2483721673488617\n",
      "[Training Epoch 4] Batch 3537, Loss 0.260528564453125\n",
      "[Training Epoch 4] Batch 3538, Loss 0.2799833118915558\n",
      "[Training Epoch 4] Batch 3539, Loss 0.26857417821884155\n",
      "[Training Epoch 4] Batch 3540, Loss 0.2719596326351166\n",
      "[Training Epoch 4] Batch 3541, Loss 0.23511263728141785\n",
      "[Training Epoch 4] Batch 3542, Loss 0.2611205279827118\n",
      "[Training Epoch 4] Batch 3543, Loss 0.26299241185188293\n",
      "[Training Epoch 4] Batch 3544, Loss 0.26492148637771606\n",
      "[Training Epoch 4] Batch 3545, Loss 0.2623974680900574\n",
      "[Training Epoch 4] Batch 3546, Loss 0.28796929121017456\n",
      "[Training Epoch 4] Batch 3547, Loss 0.25265640020370483\n",
      "[Training Epoch 4] Batch 3548, Loss 0.26435643434524536\n",
      "[Training Epoch 4] Batch 3549, Loss 0.2771291136741638\n",
      "[Training Epoch 4] Batch 3550, Loss 0.26951855421066284\n",
      "[Training Epoch 4] Batch 3551, Loss 0.25535598397254944\n",
      "[Training Epoch 4] Batch 3552, Loss 0.2474430948495865\n",
      "[Training Epoch 4] Batch 3553, Loss 0.2664838433265686\n",
      "[Training Epoch 4] Batch 3554, Loss 0.25299763679504395\n",
      "[Training Epoch 4] Batch 3555, Loss 0.27017444372177124\n",
      "[Training Epoch 4] Batch 3556, Loss 0.24648962914943695\n",
      "[Training Epoch 4] Batch 3557, Loss 0.24904796481132507\n",
      "[Training Epoch 4] Batch 3558, Loss 0.22941294312477112\n",
      "[Training Epoch 4] Batch 3559, Loss 0.27332642674446106\n",
      "[Training Epoch 4] Batch 3560, Loss 0.29803332686424255\n",
      "[Training Epoch 4] Batch 3561, Loss 0.2572590708732605\n",
      "[Training Epoch 4] Batch 3562, Loss 0.30116623640060425\n",
      "[Training Epoch 4] Batch 3563, Loss 0.2472032606601715\n",
      "[Training Epoch 4] Batch 3564, Loss 0.2833804190158844\n",
      "[Training Epoch 4] Batch 3565, Loss 0.2666422426700592\n",
      "[Training Epoch 4] Batch 3566, Loss 0.26294994354248047\n",
      "[Training Epoch 4] Batch 3567, Loss 0.2594105005264282\n",
      "[Training Epoch 4] Batch 3568, Loss 0.2683447003364563\n",
      "[Training Epoch 4] Batch 3569, Loss 0.2932565212249756\n",
      "[Training Epoch 4] Batch 3570, Loss 0.2745056748390198\n",
      "[Training Epoch 4] Batch 3571, Loss 0.2529833912849426\n",
      "[Training Epoch 4] Batch 3572, Loss 0.26184195280075073\n",
      "[Training Epoch 4] Batch 3573, Loss 0.2441258430480957\n",
      "[Training Epoch 4] Batch 3574, Loss 0.25131529569625854\n",
      "[Training Epoch 4] Batch 3575, Loss 0.252460777759552\n",
      "[Training Epoch 4] Batch 3576, Loss 0.2663786709308624\n",
      "[Training Epoch 4] Batch 3577, Loss 0.26420801877975464\n",
      "[Training Epoch 4] Batch 3578, Loss 0.27380695939064026\n",
      "[Training Epoch 4] Batch 3579, Loss 0.2647412419319153\n",
      "[Training Epoch 4] Batch 3580, Loss 0.27551212906837463\n",
      "[Training Epoch 4] Batch 3581, Loss 0.25298354029655457\n",
      "[Training Epoch 4] Batch 3582, Loss 0.2602100670337677\n",
      "[Training Epoch 4] Batch 3583, Loss 0.2548756003379822\n",
      "[Training Epoch 4] Batch 3584, Loss 0.24429935216903687\n",
      "[Training Epoch 4] Batch 3585, Loss 0.2895604074001312\n",
      "[Training Epoch 4] Batch 3586, Loss 0.28804832696914673\n",
      "[Training Epoch 4] Batch 3587, Loss 0.2821219563484192\n",
      "[Training Epoch 4] Batch 3588, Loss 0.26365774869918823\n",
      "[Training Epoch 4] Batch 3589, Loss 0.2523233890533447\n",
      "[Training Epoch 4] Batch 3590, Loss 0.25710529088974\n",
      "[Training Epoch 4] Batch 3591, Loss 0.2390834391117096\n",
      "[Training Epoch 4] Batch 3592, Loss 0.26966363191604614\n",
      "[Training Epoch 4] Batch 3593, Loss 0.2584620714187622\n",
      "[Training Epoch 4] Batch 3594, Loss 0.2612372636795044\n",
      "[Training Epoch 4] Batch 3595, Loss 0.26599717140197754\n",
      "[Training Epoch 4] Batch 3596, Loss 0.25855928659439087\n",
      "[Training Epoch 4] Batch 3597, Loss 0.26394420862197876\n",
      "[Training Epoch 4] Batch 3598, Loss 0.2689303159713745\n",
      "[Training Epoch 4] Batch 3599, Loss 0.3020747900009155\n",
      "[Training Epoch 4] Batch 3600, Loss 0.25522905588150024\n",
      "[Training Epoch 4] Batch 3601, Loss 0.28536397218704224\n",
      "[Training Epoch 4] Batch 3602, Loss 0.27890270948410034\n",
      "[Training Epoch 4] Batch 3603, Loss 0.2577582597732544\n",
      "[Training Epoch 4] Batch 3604, Loss 0.24957144260406494\n",
      "[Training Epoch 4] Batch 3605, Loss 0.25575515627861023\n",
      "[Training Epoch 4] Batch 3606, Loss 0.24063368141651154\n",
      "[Training Epoch 4] Batch 3607, Loss 0.260759174823761\n",
      "[Training Epoch 4] Batch 3608, Loss 0.27499768137931824\n",
      "[Training Epoch 4] Batch 3609, Loss 0.2795691192150116\n",
      "[Training Epoch 4] Batch 3610, Loss 0.2706586718559265\n",
      "[Training Epoch 4] Batch 3611, Loss 0.24446626007556915\n",
      "[Training Epoch 4] Batch 3612, Loss 0.2516036033630371\n",
      "[Training Epoch 4] Batch 3613, Loss 0.26867035031318665\n",
      "[Training Epoch 4] Batch 3614, Loss 0.24785687029361725\n",
      "[Training Epoch 4] Batch 3615, Loss 0.2667917013168335\n",
      "[Training Epoch 4] Batch 3616, Loss 0.24467478692531586\n",
      "[Training Epoch 4] Batch 3617, Loss 0.26335108280181885\n",
      "[Training Epoch 4] Batch 3618, Loss 0.2334657907485962\n",
      "[Training Epoch 4] Batch 3619, Loss 0.278217077255249\n",
      "[Training Epoch 4] Batch 3620, Loss 0.25807154178619385\n",
      "[Training Epoch 4] Batch 3621, Loss 0.27864909172058105\n",
      "[Training Epoch 4] Batch 3622, Loss 0.2566262483596802\n",
      "[Training Epoch 4] Batch 3623, Loss 0.272089421749115\n",
      "[Training Epoch 4] Batch 3624, Loss 0.2933032810688019\n",
      "[Training Epoch 4] Batch 3625, Loss 0.2503984570503235\n",
      "[Training Epoch 4] Batch 3626, Loss 0.257888525724411\n",
      "[Training Epoch 4] Batch 3627, Loss 0.2629472017288208\n",
      "[Training Epoch 4] Batch 3628, Loss 0.2746936082839966\n",
      "[Training Epoch 4] Batch 3629, Loss 0.2449764460325241\n",
      "[Training Epoch 4] Batch 3630, Loss 0.29028576612472534\n",
      "[Training Epoch 4] Batch 3631, Loss 0.2930179834365845\n",
      "[Training Epoch 4] Batch 3632, Loss 0.2261466085910797\n",
      "[Training Epoch 4] Batch 3633, Loss 0.24045486748218536\n",
      "[Training Epoch 4] Batch 3634, Loss 0.2391120195388794\n",
      "[Training Epoch 4] Batch 3635, Loss 0.27278932929039\n",
      "[Training Epoch 4] Batch 3636, Loss 0.2712287902832031\n",
      "[Training Epoch 4] Batch 3637, Loss 0.2651616334915161\n",
      "[Training Epoch 4] Batch 3638, Loss 0.27219656109809875\n",
      "[Training Epoch 4] Batch 3639, Loss 0.2719586491584778\n",
      "[Training Epoch 4] Batch 3640, Loss 0.26864686608314514\n",
      "[Training Epoch 4] Batch 3641, Loss 0.2462402582168579\n",
      "[Training Epoch 4] Batch 3642, Loss 0.24960575997829437\n",
      "[Training Epoch 4] Batch 3643, Loss 0.26082828640937805\n",
      "[Training Epoch 4] Batch 3644, Loss 0.271560937166214\n",
      "[Training Epoch 4] Batch 3645, Loss 0.26554006338119507\n",
      "[Training Epoch 4] Batch 3646, Loss 0.2751433253288269\n",
      "[Training Epoch 4] Batch 3647, Loss 0.2574484646320343\n",
      "[Training Epoch 4] Batch 3648, Loss 0.24164852499961853\n",
      "[Training Epoch 4] Batch 3649, Loss 0.25719884037971497\n",
      "[Training Epoch 4] Batch 3650, Loss 0.2884429097175598\n",
      "[Training Epoch 4] Batch 3651, Loss 0.24650943279266357\n",
      "[Training Epoch 4] Batch 3652, Loss 0.290080189704895\n",
      "[Training Epoch 4] Batch 3653, Loss 0.29248422384262085\n",
      "[Training Epoch 4] Batch 3654, Loss 0.25139257311820984\n",
      "[Training Epoch 4] Batch 3655, Loss 0.25245869159698486\n",
      "[Training Epoch 4] Batch 3656, Loss 0.2620669901371002\n",
      "[Training Epoch 4] Batch 3657, Loss 0.2206207513809204\n",
      "[Training Epoch 4] Batch 3658, Loss 0.2762843370437622\n",
      "[Training Epoch 4] Batch 3659, Loss 0.2679929733276367\n",
      "[Training Epoch 4] Batch 3660, Loss 0.24854543805122375\n",
      "[Training Epoch 4] Batch 3661, Loss 0.25919830799102783\n",
      "[Training Epoch 4] Batch 3662, Loss 0.27904582023620605\n",
      "[Training Epoch 4] Batch 3663, Loss 0.28138718008995056\n",
      "[Training Epoch 4] Batch 3664, Loss 0.2604175806045532\n",
      "[Training Epoch 4] Batch 3665, Loss 0.2767568826675415\n",
      "[Training Epoch 4] Batch 3666, Loss 0.23154814541339874\n",
      "[Training Epoch 4] Batch 3667, Loss 0.2476784586906433\n",
      "[Training Epoch 4] Batch 3668, Loss 0.2642936110496521\n",
      "[Training Epoch 4] Batch 3669, Loss 0.28337281942367554\n",
      "[Training Epoch 4] Batch 3670, Loss 0.25868523120880127\n",
      "[Training Epoch 4] Batch 3671, Loss 0.29687169194221497\n",
      "[Training Epoch 4] Batch 3672, Loss 0.2764033377170563\n",
      "[Training Epoch 4] Batch 3673, Loss 0.27660393714904785\n",
      "[Training Epoch 4] Batch 3674, Loss 0.2732917070388794\n",
      "[Training Epoch 4] Batch 3675, Loss 0.2613316476345062\n",
      "[Training Epoch 4] Batch 3676, Loss 0.2527637481689453\n",
      "[Training Epoch 4] Batch 3677, Loss 0.26509636640548706\n",
      "[Training Epoch 4] Batch 3678, Loss 0.2583100199699402\n",
      "[Training Epoch 4] Batch 3679, Loss 0.24763336777687073\n",
      "[Training Epoch 4] Batch 3680, Loss 0.252302348613739\n",
      "[Training Epoch 4] Batch 3681, Loss 0.22725586593151093\n",
      "[Training Epoch 4] Batch 3682, Loss 0.27734315395355225\n",
      "[Training Epoch 4] Batch 3683, Loss 0.2595100998878479\n",
      "[Training Epoch 4] Batch 3684, Loss 0.25896650552749634\n",
      "[Training Epoch 4] Batch 3685, Loss 0.22791758179664612\n",
      "[Training Epoch 4] Batch 3686, Loss 0.2565511167049408\n",
      "[Training Epoch 4] Batch 3687, Loss 0.28822436928749084\n",
      "[Training Epoch 4] Batch 3688, Loss 0.2976077198982239\n",
      "[Training Epoch 4] Batch 3689, Loss 0.25226813554763794\n",
      "[Training Epoch 4] Batch 3690, Loss 0.2939887046813965\n",
      "[Training Epoch 4] Batch 3691, Loss 0.2657950818538666\n",
      "[Training Epoch 4] Batch 3692, Loss 0.2770092487335205\n",
      "[Training Epoch 4] Batch 3693, Loss 0.26198261976242065\n",
      "[Training Epoch 4] Batch 3694, Loss 0.2651740312576294\n",
      "[Training Epoch 4] Batch 3695, Loss 0.2868330478668213\n",
      "[Training Epoch 4] Batch 3696, Loss 0.2646908462047577\n",
      "[Training Epoch 4] Batch 3697, Loss 0.26233410835266113\n",
      "[Training Epoch 4] Batch 3698, Loss 0.2547338604927063\n",
      "[Training Epoch 4] Batch 3699, Loss 0.24896563589572906\n",
      "[Training Epoch 4] Batch 3700, Loss 0.27627167105674744\n",
      "[Training Epoch 4] Batch 3701, Loss 0.23763637244701385\n",
      "[Training Epoch 4] Batch 3702, Loss 0.2988283038139343\n",
      "[Training Epoch 4] Batch 3703, Loss 0.29379934072494507\n",
      "[Training Epoch 4] Batch 3704, Loss 0.2543626129627228\n",
      "[Training Epoch 4] Batch 3705, Loss 0.25674691796302795\n",
      "[Training Epoch 4] Batch 3706, Loss 0.24092510342597961\n",
      "[Training Epoch 4] Batch 3707, Loss 0.26670560240745544\n",
      "[Training Epoch 4] Batch 3708, Loss 0.23377645015716553\n",
      "[Training Epoch 4] Batch 3709, Loss 0.2362898588180542\n",
      "[Training Epoch 4] Batch 3710, Loss 0.2663441300392151\n",
      "[Training Epoch 4] Batch 3711, Loss 0.23803913593292236\n",
      "[Training Epoch 4] Batch 3712, Loss 0.25787824392318726\n",
      "[Training Epoch 4] Batch 3713, Loss 0.26911184191703796\n",
      "[Training Epoch 4] Batch 3714, Loss 0.27096545696258545\n",
      "[Training Epoch 4] Batch 3715, Loss 0.2832626700401306\n",
      "[Training Epoch 4] Batch 3716, Loss 0.30288630723953247\n",
      "[Training Epoch 4] Batch 3717, Loss 0.285436749458313\n",
      "[Training Epoch 4] Batch 3718, Loss 0.2707482576370239\n",
      "[Training Epoch 4] Batch 3719, Loss 0.21764618158340454\n",
      "[Training Epoch 4] Batch 3720, Loss 0.262062132358551\n",
      "[Training Epoch 4] Batch 3721, Loss 0.23578858375549316\n",
      "[Training Epoch 4] Batch 3722, Loss 0.2878182530403137\n",
      "[Training Epoch 4] Batch 3723, Loss 0.25908613204956055\n",
      "[Training Epoch 4] Batch 3724, Loss 0.2572193145751953\n",
      "[Training Epoch 4] Batch 3725, Loss 0.2708270251750946\n",
      "[Training Epoch 4] Batch 3726, Loss 0.24931970238685608\n",
      "[Training Epoch 4] Batch 3727, Loss 0.251245379447937\n",
      "[Training Epoch 4] Batch 3728, Loss 0.2580329179763794\n",
      "[Training Epoch 4] Batch 3729, Loss 0.25426924228668213\n",
      "[Training Epoch 4] Batch 3730, Loss 0.2752454876899719\n",
      "[Training Epoch 4] Batch 3731, Loss 0.28433525562286377\n",
      "[Training Epoch 4] Batch 3732, Loss 0.24633875489234924\n",
      "[Training Epoch 4] Batch 3733, Loss 0.2962690591812134\n",
      "[Training Epoch 4] Batch 3734, Loss 0.25266033411026\n",
      "[Training Epoch 4] Batch 3735, Loss 0.2794264853000641\n",
      "[Training Epoch 4] Batch 3736, Loss 0.25902289152145386\n",
      "[Training Epoch 4] Batch 3737, Loss 0.2802824378013611\n",
      "[Training Epoch 4] Batch 3738, Loss 0.2598755955696106\n",
      "[Training Epoch 4] Batch 3739, Loss 0.27099835872650146\n",
      "[Training Epoch 4] Batch 3740, Loss 0.22374466061592102\n",
      "[Training Epoch 4] Batch 3741, Loss 0.2524707019329071\n",
      "[Training Epoch 4] Batch 3742, Loss 0.2564627230167389\n",
      "[Training Epoch 4] Batch 3743, Loss 0.2758694291114807\n",
      "[Training Epoch 4] Batch 3744, Loss 0.26721394062042236\n",
      "[Training Epoch 4] Batch 3745, Loss 0.24841146171092987\n",
      "[Training Epoch 4] Batch 3746, Loss 0.2648128271102905\n",
      "[Training Epoch 4] Batch 3747, Loss 0.26045262813568115\n",
      "[Training Epoch 4] Batch 3748, Loss 0.28505778312683105\n",
      "[Training Epoch 4] Batch 3749, Loss 0.2267400026321411\n",
      "[Training Epoch 4] Batch 3750, Loss 0.26223331689834595\n",
      "[Training Epoch 4] Batch 3751, Loss 0.2425137460231781\n",
      "[Training Epoch 4] Batch 3752, Loss 0.24638158082962036\n",
      "[Training Epoch 4] Batch 3753, Loss 0.268205463886261\n",
      "[Training Epoch 4] Batch 3754, Loss 0.24550215899944305\n",
      "[Training Epoch 4] Batch 3755, Loss 0.28088486194610596\n",
      "[Training Epoch 4] Batch 3756, Loss 0.27204424142837524\n",
      "[Training Epoch 4] Batch 3757, Loss 0.26701080799102783\n",
      "[Training Epoch 4] Batch 3758, Loss 0.25544416904449463\n",
      "[Training Epoch 4] Batch 3759, Loss 0.2701641917228699\n",
      "[Training Epoch 4] Batch 3760, Loss 0.27374541759490967\n",
      "[Training Epoch 4] Batch 3761, Loss 0.26890242099761963\n",
      "[Training Epoch 4] Batch 3762, Loss 0.24685832858085632\n",
      "[Training Epoch 4] Batch 3763, Loss 0.28293362259864807\n",
      "[Training Epoch 4] Batch 3764, Loss 0.24117103219032288\n",
      "[Training Epoch 4] Batch 3765, Loss 0.2584049105644226\n",
      "[Training Epoch 4] Batch 3766, Loss 0.26884594559669495\n",
      "[Training Epoch 4] Batch 3767, Loss 0.2620505094528198\n",
      "[Training Epoch 4] Batch 3768, Loss 0.24826984107494354\n",
      "[Training Epoch 4] Batch 3769, Loss 0.2564423680305481\n",
      "[Training Epoch 4] Batch 3770, Loss 0.2589972913265228\n",
      "[Training Epoch 4] Batch 3771, Loss 0.25113779306411743\n",
      "[Training Epoch 4] Batch 3772, Loss 0.26214319467544556\n",
      "[Training Epoch 4] Batch 3773, Loss 0.26348191499710083\n",
      "[Training Epoch 4] Batch 3774, Loss 0.25137948989868164\n",
      "[Training Epoch 4] Batch 3775, Loss 0.245173841714859\n",
      "[Training Epoch 4] Batch 3776, Loss 0.2655205726623535\n",
      "[Training Epoch 4] Batch 3777, Loss 0.3039385676383972\n",
      "[Training Epoch 4] Batch 3778, Loss 0.24312236905097961\n",
      "[Training Epoch 4] Batch 3779, Loss 0.2369963526725769\n",
      "[Training Epoch 4] Batch 3780, Loss 0.263289213180542\n",
      "[Training Epoch 4] Batch 3781, Loss 0.2988269329071045\n",
      "[Training Epoch 4] Batch 3782, Loss 0.2444104701280594\n",
      "[Training Epoch 4] Batch 3783, Loss 0.2664392292499542\n",
      "[Training Epoch 4] Batch 3784, Loss 0.27615922689437866\n",
      "[Training Epoch 4] Batch 3785, Loss 0.24127787351608276\n",
      "[Training Epoch 4] Batch 3786, Loss 0.2721969485282898\n",
      "[Training Epoch 4] Batch 3787, Loss 0.28857117891311646\n",
      "[Training Epoch 4] Batch 3788, Loss 0.26995909214019775\n",
      "[Training Epoch 4] Batch 3789, Loss 0.27024778723716736\n",
      "[Training Epoch 4] Batch 3790, Loss 0.26006263494491577\n",
      "[Training Epoch 4] Batch 3791, Loss 0.2555767893791199\n",
      "[Training Epoch 4] Batch 3792, Loss 0.2724319100379944\n",
      "[Training Epoch 4] Batch 3793, Loss 0.2590293288230896\n",
      "[Training Epoch 4] Batch 3794, Loss 0.2512001395225525\n",
      "[Training Epoch 4] Batch 3795, Loss 0.3107512593269348\n",
      "[Training Epoch 4] Batch 3796, Loss 0.25702032446861267\n",
      "[Training Epoch 4] Batch 3797, Loss 0.2988376021385193\n",
      "[Training Epoch 4] Batch 3798, Loss 0.24996261298656464\n",
      "[Training Epoch 4] Batch 3799, Loss 0.2682257890701294\n",
      "[Training Epoch 4] Batch 3800, Loss 0.2806830108165741\n",
      "[Training Epoch 4] Batch 3801, Loss 0.25559931993484497\n",
      "[Training Epoch 4] Batch 3802, Loss 0.2629944682121277\n",
      "[Training Epoch 4] Batch 3803, Loss 0.2799968719482422\n",
      "[Training Epoch 4] Batch 3804, Loss 0.2520972490310669\n",
      "[Training Epoch 4] Batch 3805, Loss 0.24864010512828827\n",
      "[Training Epoch 4] Batch 3806, Loss 0.2688522934913635\n",
      "[Training Epoch 4] Batch 3807, Loss 0.26919692754745483\n",
      "[Training Epoch 4] Batch 3808, Loss 0.2760360836982727\n",
      "[Training Epoch 4] Batch 3809, Loss 0.24123451113700867\n",
      "[Training Epoch 4] Batch 3810, Loss 0.25202661752700806\n",
      "[Training Epoch 4] Batch 3811, Loss 0.2739562690258026\n",
      "[Training Epoch 4] Batch 3812, Loss 0.28744208812713623\n",
      "[Training Epoch 4] Batch 3813, Loss 0.26040467619895935\n",
      "[Training Epoch 4] Batch 3814, Loss 0.24557507038116455\n",
      "[Training Epoch 4] Batch 3815, Loss 0.26206886768341064\n",
      "[Training Epoch 4] Batch 3816, Loss 0.27856260538101196\n",
      "[Training Epoch 4] Batch 3817, Loss 0.2730328142642975\n",
      "[Training Epoch 4] Batch 3818, Loss 0.25199705362319946\n",
      "[Training Epoch 4] Batch 3819, Loss 0.2663581073284149\n",
      "[Training Epoch 4] Batch 3820, Loss 0.23535630106925964\n",
      "[Training Epoch 4] Batch 3821, Loss 0.27188169956207275\n",
      "[Training Epoch 4] Batch 3822, Loss 0.24389979243278503\n",
      "[Training Epoch 4] Batch 3823, Loss 0.2734347879886627\n",
      "[Training Epoch 4] Batch 3824, Loss 0.27097123861312866\n",
      "[Training Epoch 4] Batch 3825, Loss 0.28192001581192017\n",
      "[Training Epoch 4] Batch 3826, Loss 0.27369922399520874\n",
      "[Training Epoch 4] Batch 3827, Loss 0.25705599784851074\n",
      "[Training Epoch 4] Batch 3828, Loss 0.2608325481414795\n",
      "[Training Epoch 4] Batch 3829, Loss 0.26065492630004883\n",
      "[Training Epoch 4] Batch 3830, Loss 0.24320988357067108\n",
      "[Training Epoch 4] Batch 3831, Loss 0.2503361403942108\n",
      "[Training Epoch 4] Batch 3832, Loss 0.26048997044563293\n",
      "[Training Epoch 4] Batch 3833, Loss 0.2666378915309906\n",
      "[Training Epoch 4] Batch 3834, Loss 0.24704262614250183\n",
      "[Training Epoch 4] Batch 3835, Loss 0.2559303641319275\n",
      "[Training Epoch 4] Batch 3836, Loss 0.28631433844566345\n",
      "[Training Epoch 4] Batch 3837, Loss 0.28787466883659363\n",
      "[Training Epoch 4] Batch 3838, Loss 0.263179749250412\n",
      "[Training Epoch 4] Batch 3839, Loss 0.26685631275177\n",
      "[Training Epoch 4] Batch 3840, Loss 0.2969915270805359\n",
      "[Training Epoch 4] Batch 3841, Loss 0.27811747789382935\n",
      "[Training Epoch 4] Batch 3842, Loss 0.28721633553504944\n",
      "[Training Epoch 4] Batch 3843, Loss 0.2609115540981293\n",
      "[Training Epoch 4] Batch 3844, Loss 0.2648530900478363\n",
      "[Training Epoch 4] Batch 3845, Loss 0.2751554250717163\n",
      "[Training Epoch 4] Batch 3846, Loss 0.2622433304786682\n",
      "[Training Epoch 4] Batch 3847, Loss 0.27586299180984497\n",
      "[Training Epoch 4] Batch 3848, Loss 0.2616986036300659\n",
      "[Training Epoch 4] Batch 3849, Loss 0.25845062732696533\n",
      "[Training Epoch 4] Batch 3850, Loss 0.266095370054245\n",
      "[Training Epoch 4] Batch 3851, Loss 0.2728491425514221\n",
      "[Training Epoch 4] Batch 3852, Loss 0.27087751030921936\n",
      "[Training Epoch 4] Batch 3853, Loss 0.2712484300136566\n",
      "[Training Epoch 4] Batch 3854, Loss 0.2526189684867859\n",
      "[Training Epoch 4] Batch 3855, Loss 0.23548871278762817\n",
      "[Training Epoch 4] Batch 3856, Loss 0.2507004141807556\n",
      "[Training Epoch 4] Batch 3857, Loss 0.2523663640022278\n",
      "[Training Epoch 4] Batch 3858, Loss 0.2639458477497101\n",
      "[Training Epoch 4] Batch 3859, Loss 0.2575731873512268\n",
      "[Training Epoch 4] Batch 3860, Loss 0.22032499313354492\n",
      "[Training Epoch 4] Batch 3861, Loss 0.2773856222629547\n",
      "[Training Epoch 4] Batch 3862, Loss 0.26411014795303345\n",
      "[Training Epoch 4] Batch 3863, Loss 0.25794485211372375\n",
      "[Training Epoch 4] Batch 3864, Loss 0.26037919521331787\n",
      "[Training Epoch 4] Batch 3865, Loss 0.2539355754852295\n",
      "[Training Epoch 4] Batch 3866, Loss 0.2712015211582184\n",
      "[Training Epoch 4] Batch 3867, Loss 0.26853859424591064\n",
      "[Training Epoch 4] Batch 3868, Loss 0.2811511158943176\n",
      "[Training Epoch 4] Batch 3869, Loss 0.24694639444351196\n",
      "[Training Epoch 4] Batch 3870, Loss 0.26602083444595337\n",
      "[Training Epoch 4] Batch 3871, Loss 0.25817880034446716\n",
      "[Training Epoch 4] Batch 3872, Loss 0.2773256301879883\n",
      "[Training Epoch 4] Batch 3873, Loss 0.27054816484451294\n",
      "[Training Epoch 4] Batch 3874, Loss 0.27438217401504517\n",
      "[Training Epoch 4] Batch 3875, Loss 0.23076754808425903\n",
      "[Training Epoch 4] Batch 3876, Loss 0.30998533964157104\n",
      "[Training Epoch 4] Batch 3877, Loss 0.2614528238773346\n",
      "[Training Epoch 4] Batch 3878, Loss 0.25527942180633545\n",
      "[Training Epoch 4] Batch 3879, Loss 0.2878270149230957\n",
      "[Training Epoch 4] Batch 3880, Loss 0.2536435127258301\n",
      "[Training Epoch 4] Batch 3881, Loss 0.2648341953754425\n",
      "[Training Epoch 4] Batch 3882, Loss 0.26869845390319824\n",
      "[Training Epoch 4] Batch 3883, Loss 0.27043530344963074\n",
      "[Training Epoch 4] Batch 3884, Loss 0.2545982003211975\n",
      "[Training Epoch 4] Batch 3885, Loss 0.25889134407043457\n",
      "[Training Epoch 4] Batch 3886, Loss 0.22874870896339417\n",
      "[Training Epoch 4] Batch 3887, Loss 0.27208173274993896\n",
      "[Training Epoch 4] Batch 3888, Loss 0.2603089213371277\n",
      "[Training Epoch 4] Batch 3889, Loss 0.2677510380744934\n",
      "[Training Epoch 4] Batch 3890, Loss 0.23852413892745972\n",
      "[Training Epoch 4] Batch 3891, Loss 0.26306647062301636\n",
      "[Training Epoch 4] Batch 3892, Loss 0.26522573828697205\n",
      "[Training Epoch 4] Batch 3893, Loss 0.2694148123264313\n",
      "[Training Epoch 4] Batch 3894, Loss 0.2557786703109741\n",
      "[Training Epoch 4] Batch 3895, Loss 0.24274113774299622\n",
      "[Training Epoch 4] Batch 3896, Loss 0.2526996433734894\n",
      "[Training Epoch 4] Batch 3897, Loss 0.2724772095680237\n",
      "[Training Epoch 4] Batch 3898, Loss 0.26365160942077637\n",
      "[Training Epoch 4] Batch 3899, Loss 0.2562314569950104\n",
      "[Training Epoch 4] Batch 3900, Loss 0.25598829984664917\n",
      "[Training Epoch 4] Batch 3901, Loss 0.2605412006378174\n",
      "[Training Epoch 4] Batch 3902, Loss 0.25752073526382446\n",
      "[Training Epoch 4] Batch 3903, Loss 0.29770419001579285\n",
      "[Training Epoch 4] Batch 3904, Loss 0.2556670308113098\n",
      "[Training Epoch 4] Batch 3905, Loss 0.2869129180908203\n",
      "[Training Epoch 4] Batch 3906, Loss 0.27857550978660583\n",
      "[Training Epoch 4] Batch 3907, Loss 0.2714991271495819\n",
      "[Training Epoch 4] Batch 3908, Loss 0.26926299929618835\n",
      "[Training Epoch 4] Batch 3909, Loss 0.29547545313835144\n",
      "[Training Epoch 4] Batch 3910, Loss 0.2552233934402466\n",
      "[Training Epoch 4] Batch 3911, Loss 0.2621077597141266\n",
      "[Training Epoch 4] Batch 3912, Loss 0.2674345076084137\n",
      "[Training Epoch 4] Batch 3913, Loss 0.28100940585136414\n",
      "[Training Epoch 4] Batch 3914, Loss 0.26199275255203247\n",
      "[Training Epoch 4] Batch 3915, Loss 0.23923465609550476\n",
      "[Training Epoch 4] Batch 3916, Loss 0.26531997323036194\n",
      "[Training Epoch 4] Batch 3917, Loss 0.2734687924385071\n",
      "[Training Epoch 4] Batch 3918, Loss 0.2630729079246521\n",
      "[Training Epoch 4] Batch 3919, Loss 0.2643130421638489\n",
      "[Training Epoch 4] Batch 3920, Loss 0.26404619216918945\n",
      "[Training Epoch 4] Batch 3921, Loss 0.2555463910102844\n",
      "[Training Epoch 4] Batch 3922, Loss 0.28008610010147095\n",
      "[Training Epoch 4] Batch 3923, Loss 0.2387968897819519\n",
      "[Training Epoch 4] Batch 3924, Loss 0.282365083694458\n",
      "[Training Epoch 4] Batch 3925, Loss 0.2586956024169922\n",
      "[Training Epoch 4] Batch 3926, Loss 0.25918281078338623\n",
      "[Training Epoch 4] Batch 3927, Loss 0.2540256679058075\n",
      "[Training Epoch 4] Batch 3928, Loss 0.25969576835632324\n",
      "[Training Epoch 4] Batch 3929, Loss 0.2591610550880432\n",
      "[Training Epoch 4] Batch 3930, Loss 0.24842676520347595\n",
      "[Training Epoch 4] Batch 3931, Loss 0.257770299911499\n",
      "[Training Epoch 4] Batch 3932, Loss 0.28057152032852173\n",
      "[Training Epoch 4] Batch 3933, Loss 0.25249484181404114\n",
      "[Training Epoch 4] Batch 3934, Loss 0.2583967447280884\n",
      "[Training Epoch 4] Batch 3935, Loss 0.26138734817504883\n",
      "[Training Epoch 4] Batch 3936, Loss 0.2622734308242798\n",
      "[Training Epoch 4] Batch 3937, Loss 0.25446534156799316\n",
      "[Training Epoch 4] Batch 3938, Loss 0.25327447056770325\n",
      "[Training Epoch 4] Batch 3939, Loss 0.2524380683898926\n",
      "[Training Epoch 4] Batch 3940, Loss 0.27494102716445923\n",
      "[Training Epoch 4] Batch 3941, Loss 0.2852577865123749\n",
      "[Training Epoch 4] Batch 3942, Loss 0.26752251386642456\n",
      "[Training Epoch 4] Batch 3943, Loss 0.2446092814207077\n",
      "[Training Epoch 4] Batch 3944, Loss 0.2762305736541748\n",
      "[Training Epoch 4] Batch 3945, Loss 0.2373257577419281\n",
      "[Training Epoch 4] Batch 3946, Loss 0.2608710527420044\n",
      "[Training Epoch 4] Batch 3947, Loss 0.24634391069412231\n",
      "[Training Epoch 4] Batch 3948, Loss 0.2900879979133606\n",
      "[Training Epoch 4] Batch 3949, Loss 0.27286767959594727\n",
      "[Training Epoch 4] Batch 3950, Loss 0.2611638009548187\n",
      "[Training Epoch 4] Batch 3951, Loss 0.25122252106666565\n",
      "[Training Epoch 4] Batch 3952, Loss 0.28316205739974976\n",
      "[Training Epoch 4] Batch 3953, Loss 0.2931396961212158\n",
      "[Training Epoch 4] Batch 3954, Loss 0.27312353253364563\n",
      "[Training Epoch 4] Batch 3955, Loss 0.2483891248703003\n",
      "[Training Epoch 4] Batch 3956, Loss 0.26085805892944336\n",
      "[Training Epoch 4] Batch 3957, Loss 0.26585865020751953\n",
      "[Training Epoch 4] Batch 3958, Loss 0.2837642431259155\n",
      "[Training Epoch 4] Batch 3959, Loss 0.30971813201904297\n",
      "[Training Epoch 4] Batch 3960, Loss 0.24433748424053192\n",
      "[Training Epoch 4] Batch 3961, Loss 0.24262486398220062\n",
      "[Training Epoch 4] Batch 3962, Loss 0.26781418919563293\n",
      "[Training Epoch 4] Batch 3963, Loss 0.23726189136505127\n",
      "[Training Epoch 4] Batch 3964, Loss 0.263884037733078\n",
      "[Training Epoch 4] Batch 3965, Loss 0.2512122094631195\n",
      "[Training Epoch 4] Batch 3966, Loss 0.2720111608505249\n",
      "[Training Epoch 4] Batch 3967, Loss 0.28705358505249023\n",
      "[Training Epoch 4] Batch 3968, Loss 0.23780304193496704\n",
      "[Training Epoch 4] Batch 3969, Loss 0.2512243986129761\n",
      "[Training Epoch 4] Batch 3970, Loss 0.26228994131088257\n",
      "[Training Epoch 4] Batch 3971, Loss 0.30134326219558716\n",
      "[Training Epoch 4] Batch 3972, Loss 0.2724202275276184\n",
      "[Training Epoch 4] Batch 3973, Loss 0.2617764174938202\n",
      "[Training Epoch 4] Batch 3974, Loss 0.26920953392982483\n",
      "[Training Epoch 4] Batch 3975, Loss 0.30299150943756104\n",
      "[Training Epoch 4] Batch 3976, Loss 0.2503516972064972\n",
      "[Training Epoch 4] Batch 3977, Loss 0.2605997323989868\n",
      "[Training Epoch 4] Batch 3978, Loss 0.26149845123291016\n",
      "[Training Epoch 4] Batch 3979, Loss 0.2801732122898102\n",
      "[Training Epoch 4] Batch 3980, Loss 0.27431073784828186\n",
      "[Training Epoch 4] Batch 3981, Loss 0.26590898633003235\n",
      "[Training Epoch 4] Batch 3982, Loss 0.24263422191143036\n",
      "[Training Epoch 4] Batch 3983, Loss 0.25661543011665344\n",
      "[Training Epoch 4] Batch 3984, Loss 0.2610045075416565\n",
      "[Training Epoch 4] Batch 3985, Loss 0.29830846190452576\n",
      "[Training Epoch 4] Batch 3986, Loss 0.2868952453136444\n",
      "[Training Epoch 4] Batch 3987, Loss 0.2193630039691925\n",
      "[Training Epoch 4] Batch 3988, Loss 0.27173393964767456\n",
      "[Training Epoch 4] Batch 3989, Loss 0.27299270033836365\n",
      "[Training Epoch 4] Batch 3990, Loss 0.24097345769405365\n",
      "[Training Epoch 4] Batch 3991, Loss 0.27471721172332764\n",
      "[Training Epoch 4] Batch 3992, Loss 0.2663203477859497\n",
      "[Training Epoch 4] Batch 3993, Loss 0.24539697170257568\n",
      "[Training Epoch 4] Batch 3994, Loss 0.2683250904083252\n",
      "[Training Epoch 4] Batch 3995, Loss 0.24077683687210083\n",
      "[Training Epoch 4] Batch 3996, Loss 0.263774037361145\n",
      "[Training Epoch 4] Batch 3997, Loss 0.2500297427177429\n",
      "[Training Epoch 4] Batch 3998, Loss 0.2375592142343521\n",
      "[Training Epoch 4] Batch 3999, Loss 0.25915297865867615\n",
      "[Training Epoch 4] Batch 4000, Loss 0.2724102735519409\n",
      "[Training Epoch 4] Batch 4001, Loss 0.2707252502441406\n",
      "[Training Epoch 4] Batch 4002, Loss 0.27144691348075867\n",
      "[Training Epoch 4] Batch 4003, Loss 0.25422924757003784\n",
      "[Training Epoch 4] Batch 4004, Loss 0.29131585359573364\n",
      "[Training Epoch 4] Batch 4005, Loss 0.24465259909629822\n",
      "[Training Epoch 4] Batch 4006, Loss 0.266543447971344\n",
      "[Training Epoch 4] Batch 4007, Loss 0.2590206265449524\n",
      "[Training Epoch 4] Batch 4008, Loss 0.2659456133842468\n",
      "[Training Epoch 4] Batch 4009, Loss 0.26836591958999634\n",
      "[Training Epoch 4] Batch 4010, Loss 0.24456116557121277\n",
      "[Training Epoch 4] Batch 4011, Loss 0.25906091928482056\n",
      "[Training Epoch 4] Batch 4012, Loss 0.2864547371864319\n",
      "[Training Epoch 4] Batch 4013, Loss 0.24814699590206146\n",
      "[Training Epoch 4] Batch 4014, Loss 0.24793988466262817\n",
      "[Training Epoch 4] Batch 4015, Loss 0.25737255811691284\n",
      "[Training Epoch 4] Batch 4016, Loss 0.2701069116592407\n",
      "[Training Epoch 4] Batch 4017, Loss 0.2436848282814026\n",
      "[Training Epoch 4] Batch 4018, Loss 0.24922969937324524\n",
      "[Training Epoch 4] Batch 4019, Loss 0.26878583431243896\n",
      "[Training Epoch 4] Batch 4020, Loss 0.2772258520126343\n",
      "[Training Epoch 4] Batch 4021, Loss 0.2600603699684143\n",
      "[Training Epoch 4] Batch 4022, Loss 0.23117679357528687\n",
      "[Training Epoch 4] Batch 4023, Loss 0.27429091930389404\n",
      "[Training Epoch 4] Batch 4024, Loss 0.25994765758514404\n",
      "[Training Epoch 4] Batch 4025, Loss 0.27687719464302063\n",
      "[Training Epoch 4] Batch 4026, Loss 0.261849045753479\n",
      "[Training Epoch 4] Batch 4027, Loss 0.28367334604263306\n",
      "[Training Epoch 4] Batch 4028, Loss 0.27048394083976746\n",
      "[Training Epoch 4] Batch 4029, Loss 0.2692566514015198\n",
      "[Training Epoch 4] Batch 4030, Loss 0.2673792243003845\n",
      "[Training Epoch 4] Batch 4031, Loss 0.2771163284778595\n",
      "[Training Epoch 4] Batch 4032, Loss 0.2527919113636017\n",
      "[Training Epoch 4] Batch 4033, Loss 0.2663479149341583\n",
      "[Training Epoch 4] Batch 4034, Loss 0.263627290725708\n",
      "[Training Epoch 4] Batch 4035, Loss 0.2750868797302246\n",
      "[Training Epoch 4] Batch 4036, Loss 0.25544920563697815\n",
      "[Training Epoch 4] Batch 4037, Loss 0.25446808338165283\n",
      "[Training Epoch 4] Batch 4038, Loss 0.2929125130176544\n",
      "[Training Epoch 4] Batch 4039, Loss 0.2652691900730133\n",
      "[Training Epoch 4] Batch 4040, Loss 0.24050387740135193\n",
      "[Training Epoch 4] Batch 4041, Loss 0.2661546468734741\n",
      "[Training Epoch 4] Batch 4042, Loss 0.22040536999702454\n",
      "[Training Epoch 4] Batch 4043, Loss 0.23499643802642822\n",
      "[Training Epoch 4] Batch 4044, Loss 0.2667580842971802\n",
      "[Training Epoch 4] Batch 4045, Loss 0.2772170901298523\n",
      "[Training Epoch 4] Batch 4046, Loss 0.26770153641700745\n",
      "[Training Epoch 4] Batch 4047, Loss 0.2527351677417755\n",
      "[Training Epoch 4] Batch 4048, Loss 0.28379082679748535\n",
      "[Training Epoch 4] Batch 4049, Loss 0.2799021601676941\n",
      "[Training Epoch 4] Batch 4050, Loss 0.2754606604576111\n",
      "[Training Epoch 4] Batch 4051, Loss 0.2801341414451599\n",
      "[Training Epoch 4] Batch 4052, Loss 0.27088361978530884\n",
      "[Training Epoch 4] Batch 4053, Loss 0.25353169441223145\n",
      "[Training Epoch 4] Batch 4054, Loss 0.2646009922027588\n",
      "[Training Epoch 4] Batch 4055, Loss 0.27315282821655273\n",
      "[Training Epoch 4] Batch 4056, Loss 0.24471627175807953\n",
      "[Training Epoch 4] Batch 4057, Loss 0.28277599811553955\n",
      "[Training Epoch 4] Batch 4058, Loss 0.2518438398838043\n",
      "[Training Epoch 4] Batch 4059, Loss 0.2896534204483032\n",
      "[Training Epoch 4] Batch 4060, Loss 0.291745126247406\n",
      "[Training Epoch 4] Batch 4061, Loss 0.2413506805896759\n",
      "[Training Epoch 4] Batch 4062, Loss 0.2588886618614197\n",
      "[Training Epoch 4] Batch 4063, Loss 0.27294936776161194\n",
      "[Training Epoch 4] Batch 4064, Loss 0.27218395471572876\n",
      "[Training Epoch 4] Batch 4065, Loss 0.24566274881362915\n",
      "[Training Epoch 4] Batch 4066, Loss 0.23486897349357605\n",
      "[Training Epoch 4] Batch 4067, Loss 0.25572487711906433\n",
      "[Training Epoch 4] Batch 4068, Loss 0.26583582162857056\n",
      "[Training Epoch 4] Batch 4069, Loss 0.2518231272697449\n",
      "[Training Epoch 4] Batch 4070, Loss 0.25219130516052246\n",
      "[Training Epoch 4] Batch 4071, Loss 0.2617139220237732\n",
      "[Training Epoch 4] Batch 4072, Loss 0.24728816747665405\n",
      "[Training Epoch 4] Batch 4073, Loss 0.2911284565925598\n",
      "[Training Epoch 4] Batch 4074, Loss 0.272707462310791\n",
      "[Training Epoch 4] Batch 4075, Loss 0.23030003905296326\n",
      "[Training Epoch 4] Batch 4076, Loss 0.2855869233608246\n",
      "[Training Epoch 4] Batch 4077, Loss 0.28026339411735535\n",
      "[Training Epoch 4] Batch 4078, Loss 0.24942636489868164\n",
      "[Training Epoch 4] Batch 4079, Loss 0.26117590069770813\n",
      "[Training Epoch 4] Batch 4080, Loss 0.26297298073768616\n",
      "[Training Epoch 4] Batch 4081, Loss 0.27974963188171387\n",
      "[Training Epoch 4] Batch 4082, Loss 0.3046303391456604\n",
      "[Training Epoch 4] Batch 4083, Loss 0.24613280594348907\n",
      "[Training Epoch 4] Batch 4084, Loss 0.261706680059433\n",
      "[Training Epoch 4] Batch 4085, Loss 0.2519597113132477\n",
      "[Training Epoch 4] Batch 4086, Loss 0.2783651351928711\n",
      "[Training Epoch 4] Batch 4087, Loss 0.2617693543434143\n",
      "[Training Epoch 4] Batch 4088, Loss 0.2600492835044861\n",
      "[Training Epoch 4] Batch 4089, Loss 0.2777135372161865\n",
      "[Training Epoch 4] Batch 4090, Loss 0.24827967584133148\n",
      "[Training Epoch 4] Batch 4091, Loss 0.2676783502101898\n",
      "[Training Epoch 4] Batch 4092, Loss 0.28388547897338867\n",
      "[Training Epoch 4] Batch 4093, Loss 0.27371200919151306\n",
      "[Training Epoch 4] Batch 4094, Loss 0.24746692180633545\n",
      "[Training Epoch 4] Batch 4095, Loss 0.2680136561393738\n",
      "[Training Epoch 4] Batch 4096, Loss 0.24627569317817688\n",
      "[Training Epoch 4] Batch 4097, Loss 0.253517746925354\n",
      "[Training Epoch 4] Batch 4098, Loss 0.2497023046016693\n",
      "[Training Epoch 4] Batch 4099, Loss 0.26631975173950195\n",
      "[Training Epoch 4] Batch 4100, Loss 0.30036067962646484\n",
      "[Training Epoch 4] Batch 4101, Loss 0.2643949091434479\n",
      "[Training Epoch 4] Batch 4102, Loss 0.25866666436195374\n",
      "[Training Epoch 4] Batch 4103, Loss 0.2750162184238434\n",
      "[Training Epoch 4] Batch 4104, Loss 0.2706536650657654\n",
      "[Training Epoch 4] Batch 4105, Loss 0.262915700674057\n",
      "[Training Epoch 4] Batch 4106, Loss 0.276735782623291\n",
      "[Training Epoch 4] Batch 4107, Loss 0.2736831605434418\n",
      "[Training Epoch 4] Batch 4108, Loss 0.2688683569431305\n",
      "[Training Epoch 4] Batch 4109, Loss 0.3112311363220215\n",
      "[Training Epoch 4] Batch 4110, Loss 0.2511606514453888\n",
      "[Training Epoch 4] Batch 4111, Loss 0.25950154662132263\n",
      "[Training Epoch 4] Batch 4112, Loss 0.27058690786361694\n",
      "[Training Epoch 4] Batch 4113, Loss 0.2558800280094147\n",
      "[Training Epoch 4] Batch 4114, Loss 0.29887086153030396\n",
      "[Training Epoch 4] Batch 4115, Loss 0.24335122108459473\n",
      "[Training Epoch 4] Batch 4116, Loss 0.269755095243454\n",
      "[Training Epoch 4] Batch 4117, Loss 0.26963889598846436\n",
      "[Training Epoch 4] Batch 4118, Loss 0.2788591980934143\n",
      "[Training Epoch 4] Batch 4119, Loss 0.2712395489215851\n",
      "[Training Epoch 4] Batch 4120, Loss 0.2469298243522644\n",
      "[Training Epoch 4] Batch 4121, Loss 0.2608458399772644\n",
      "[Training Epoch 4] Batch 4122, Loss 0.28808262944221497\n",
      "[Training Epoch 4] Batch 4123, Loss 0.2594476640224457\n",
      "[Training Epoch 4] Batch 4124, Loss 0.25779786705970764\n",
      "[Training Epoch 4] Batch 4125, Loss 0.29838085174560547\n",
      "[Training Epoch 4] Batch 4126, Loss 0.2696571350097656\n",
      "[Training Epoch 4] Batch 4127, Loss 0.25857269763946533\n",
      "[Training Epoch 4] Batch 4128, Loss 0.24119658768177032\n",
      "[Training Epoch 4] Batch 4129, Loss 0.27740928530693054\n",
      "[Training Epoch 4] Batch 4130, Loss 0.24899420142173767\n",
      "[Training Epoch 4] Batch 4131, Loss 0.26005369424819946\n",
      "[Training Epoch 4] Batch 4132, Loss 0.2776315212249756\n",
      "[Training Epoch 4] Batch 4133, Loss 0.24675053358078003\n",
      "[Training Epoch 4] Batch 4134, Loss 0.2358986735343933\n",
      "[Training Epoch 4] Batch 4135, Loss 0.25841861963272095\n",
      "[Training Epoch 4] Batch 4136, Loss 0.2473589926958084\n",
      "[Training Epoch 4] Batch 4137, Loss 0.3000282645225525\n",
      "[Training Epoch 4] Batch 4138, Loss 0.264059454202652\n",
      "[Training Epoch 4] Batch 4139, Loss 0.25973308086395264\n",
      "[Training Epoch 4] Batch 4140, Loss 0.2486499547958374\n",
      "[Training Epoch 4] Batch 4141, Loss 0.27182328701019287\n",
      "[Training Epoch 4] Batch 4142, Loss 0.2619006335735321\n",
      "[Training Epoch 4] Batch 4143, Loss 0.25238651037216187\n",
      "[Training Epoch 4] Batch 4144, Loss 0.2853715121746063\n",
      "[Training Epoch 4] Batch 4145, Loss 0.26243647933006287\n",
      "[Training Epoch 4] Batch 4146, Loss 0.2544277310371399\n",
      "[Training Epoch 4] Batch 4147, Loss 0.24310147762298584\n",
      "[Training Epoch 4] Batch 4148, Loss 0.2382119745016098\n",
      "[Training Epoch 4] Batch 4149, Loss 0.24945396184921265\n",
      "[Training Epoch 4] Batch 4150, Loss 0.251181423664093\n",
      "[Training Epoch 4] Batch 4151, Loss 0.2838557958602905\n",
      "[Training Epoch 4] Batch 4152, Loss 0.26676684617996216\n",
      "[Training Epoch 4] Batch 4153, Loss 0.24201354384422302\n",
      "[Training Epoch 4] Batch 4154, Loss 0.2865602374076843\n",
      "[Training Epoch 4] Batch 4155, Loss 0.2374819815158844\n",
      "[Training Epoch 4] Batch 4156, Loss 0.25996917486190796\n",
      "[Training Epoch 4] Batch 4157, Loss 0.2641698718070984\n",
      "[Training Epoch 4] Batch 4158, Loss 0.26881808042526245\n",
      "[Training Epoch 4] Batch 4159, Loss 0.25494253635406494\n",
      "[Training Epoch 4] Batch 4160, Loss 0.2681010961532593\n",
      "[Training Epoch 4] Batch 4161, Loss 0.2827683687210083\n",
      "[Training Epoch 4] Batch 4162, Loss 0.2801627516746521\n",
      "[Training Epoch 4] Batch 4163, Loss 0.27188533544540405\n",
      "[Training Epoch 4] Batch 4164, Loss 0.2764872610569\n",
      "[Training Epoch 4] Batch 4165, Loss 0.29140812158584595\n",
      "[Training Epoch 4] Batch 4166, Loss 0.26975739002227783\n",
      "[Training Epoch 4] Batch 4167, Loss 0.2299811840057373\n",
      "[Training Epoch 4] Batch 4168, Loss 0.2701706290245056\n",
      "[Training Epoch 4] Batch 4169, Loss 0.27004140615463257\n",
      "[Training Epoch 4] Batch 4170, Loss 0.2831198573112488\n",
      "[Training Epoch 4] Batch 4171, Loss 0.2500704526901245\n",
      "[Training Epoch 4] Batch 4172, Loss 0.33335864543914795\n",
      "[Training Epoch 4] Batch 4173, Loss 0.2397315353155136\n",
      "[Training Epoch 4] Batch 4174, Loss 0.26518332958221436\n",
      "[Training Epoch 4] Batch 4175, Loss 0.28834500908851624\n",
      "[Training Epoch 4] Batch 4176, Loss 0.2982451021671295\n",
      "[Training Epoch 4] Batch 4177, Loss 0.27994340658187866\n",
      "[Training Epoch 4] Batch 4178, Loss 0.23425498604774475\n",
      "[Training Epoch 4] Batch 4179, Loss 0.2773711681365967\n",
      "[Training Epoch 4] Batch 4180, Loss 0.2464226484298706\n",
      "[Training Epoch 4] Batch 4181, Loss 0.25183194875717163\n",
      "[Training Epoch 4] Batch 4182, Loss 0.27994227409362793\n",
      "[Training Epoch 4] Batch 4183, Loss 0.2623169422149658\n",
      "[Training Epoch 4] Batch 4184, Loss 0.26750725507736206\n",
      "[Training Epoch 4] Batch 4185, Loss 0.242253378033638\n",
      "[Training Epoch 4] Batch 4186, Loss 0.26380133628845215\n",
      "[Training Epoch 4] Batch 4187, Loss 0.2810325622558594\n",
      "[Training Epoch 4] Batch 4188, Loss 0.2711449861526489\n",
      "[Training Epoch 4] Batch 4189, Loss 0.27023863792419434\n",
      "[Training Epoch 4] Batch 4190, Loss 0.2991628646850586\n",
      "[Training Epoch 4] Batch 4191, Loss 0.25682464241981506\n",
      "[Training Epoch 4] Batch 4192, Loss 0.25300300121307373\n",
      "[Training Epoch 4] Batch 4193, Loss 0.2536372244358063\n",
      "[Training Epoch 4] Batch 4194, Loss 0.2617691159248352\n",
      "[Training Epoch 4] Batch 4195, Loss 0.25402048230171204\n",
      "[Training Epoch 4] Batch 4196, Loss 0.27386343479156494\n",
      "[Training Epoch 4] Batch 4197, Loss 0.25289565324783325\n",
      "[Training Epoch 4] Batch 4198, Loss 0.25691381096839905\n",
      "[Training Epoch 4] Batch 4199, Loss 0.25267139077186584\n",
      "[Training Epoch 4] Batch 4200, Loss 0.2766798436641693\n",
      "[Training Epoch 4] Batch 4201, Loss 0.28616827726364136\n",
      "[Training Epoch 4] Batch 4202, Loss 0.26807701587677\n",
      "[Training Epoch 4] Batch 4203, Loss 0.2846008241176605\n",
      "[Training Epoch 4] Batch 4204, Loss 0.29370954632759094\n",
      "[Training Epoch 4] Batch 4205, Loss 0.2710558772087097\n",
      "[Training Epoch 4] Batch 4206, Loss 0.2653544545173645\n",
      "[Training Epoch 4] Batch 4207, Loss 0.2633218765258789\n",
      "[Training Epoch 4] Batch 4208, Loss 0.2787571847438812\n",
      "[Training Epoch 4] Batch 4209, Loss 0.25972723960876465\n",
      "[Training Epoch 4] Batch 4210, Loss 0.26496621966362\n",
      "[Training Epoch 4] Batch 4211, Loss 0.2599242925643921\n",
      "[Training Epoch 4] Batch 4212, Loss 0.24817608296871185\n",
      "[Training Epoch 4] Batch 4213, Loss 0.2667100429534912\n",
      "[Training Epoch 4] Batch 4214, Loss 0.24463976919651031\n",
      "[Training Epoch 4] Batch 4215, Loss 0.25875598192214966\n",
      "[Training Epoch 4] Batch 4216, Loss 0.27067485451698303\n",
      "[Training Epoch 4] Batch 4217, Loss 0.2470177561044693\n",
      "[Training Epoch 4] Batch 4218, Loss 0.2487952560186386\n",
      "[Training Epoch 4] Batch 4219, Loss 0.25370585918426514\n",
      "[Training Epoch 4] Batch 4220, Loss 0.26040905714035034\n",
      "[Training Epoch 4] Batch 4221, Loss 0.27255815267562866\n",
      "[Training Epoch 4] Batch 4222, Loss 0.25224769115448\n",
      "[Training Epoch 4] Batch 4223, Loss 0.25025033950805664\n",
      "[Training Epoch 4] Batch 4224, Loss 0.2680196762084961\n",
      "[Training Epoch 4] Batch 4225, Loss 0.24719804525375366\n",
      "[Training Epoch 4] Batch 4226, Loss 0.25693070888519287\n",
      "[Training Epoch 4] Batch 4227, Loss 0.24927616119384766\n",
      "[Training Epoch 4] Batch 4228, Loss 0.24879157543182373\n",
      "[Training Epoch 4] Batch 4229, Loss 0.25753846764564514\n",
      "[Training Epoch 4] Batch 4230, Loss 0.24139465391635895\n",
      "[Training Epoch 4] Batch 4231, Loss 0.2465871274471283\n",
      "[Training Epoch 4] Batch 4232, Loss 0.2788947820663452\n",
      "[Training Epoch 4] Batch 4233, Loss 0.24772168695926666\n",
      "[Training Epoch 4] Batch 4234, Loss 0.2900797724723816\n",
      "[Training Epoch 4] Batch 4235, Loss 0.2834210991859436\n",
      "[Training Epoch 4] Batch 4236, Loss 0.28324759006500244\n",
      "[Training Epoch 4] Batch 4237, Loss 0.25542306900024414\n",
      "[Training Epoch 4] Batch 4238, Loss 0.2578217387199402\n",
      "[Training Epoch 4] Batch 4239, Loss 0.24319413304328918\n",
      "[Training Epoch 4] Batch 4240, Loss 0.23497821390628815\n",
      "[Training Epoch 4] Batch 4241, Loss 0.2483091801404953\n",
      "[Training Epoch 4] Batch 4242, Loss 0.26913151144981384\n",
      "[Training Epoch 4] Batch 4243, Loss 0.2522916793823242\n",
      "[Training Epoch 4] Batch 4244, Loss 0.2586190700531006\n",
      "[Training Epoch 4] Batch 4245, Loss 0.28215235471725464\n",
      "[Training Epoch 4] Batch 4246, Loss 0.2610225975513458\n",
      "[Training Epoch 4] Batch 4247, Loss 0.2640749216079712\n",
      "[Training Epoch 4] Batch 4248, Loss 0.25950872898101807\n",
      "[Training Epoch 4] Batch 4249, Loss 0.2792038917541504\n",
      "[Training Epoch 4] Batch 4250, Loss 0.2549629807472229\n",
      "[Training Epoch 4] Batch 4251, Loss 0.2516721487045288\n",
      "[Training Epoch 4] Batch 4252, Loss 0.24825167655944824\n",
      "[Training Epoch 4] Batch 4253, Loss 0.2672606110572815\n",
      "[Training Epoch 4] Batch 4254, Loss 0.24280677735805511\n",
      "[Training Epoch 4] Batch 4255, Loss 0.2629396319389343\n",
      "[Training Epoch 4] Batch 4256, Loss 0.2908250093460083\n",
      "[Training Epoch 4] Batch 4257, Loss 0.2722468972206116\n",
      "[Training Epoch 4] Batch 4258, Loss 0.2568446397781372\n",
      "[Training Epoch 4] Batch 4259, Loss 0.2645246386528015\n",
      "[Training Epoch 4] Batch 4260, Loss 0.29957816004753113\n",
      "[Training Epoch 4] Batch 4261, Loss 0.27349814772605896\n",
      "[Training Epoch 4] Batch 4262, Loss 0.26259031891822815\n",
      "[Training Epoch 4] Batch 4263, Loss 0.2896352708339691\n",
      "[Training Epoch 4] Batch 4264, Loss 0.24605923891067505\n",
      "[Training Epoch 4] Batch 4265, Loss 0.2824922204017639\n",
      "[Training Epoch 4] Batch 4266, Loss 0.25387758016586304\n",
      "[Training Epoch 4] Batch 4267, Loss 0.272907555103302\n",
      "[Training Epoch 4] Batch 4268, Loss 0.2567427158355713\n",
      "[Training Epoch 4] Batch 4269, Loss 0.26376819610595703\n",
      "[Training Epoch 4] Batch 4270, Loss 0.29547202587127686\n",
      "[Training Epoch 4] Batch 4271, Loss 0.2819603681564331\n",
      "[Training Epoch 4] Batch 4272, Loss 0.2662052512168884\n",
      "[Training Epoch 4] Batch 4273, Loss 0.2821459174156189\n",
      "[Training Epoch 4] Batch 4274, Loss 0.274386465549469\n",
      "[Training Epoch 4] Batch 4275, Loss 0.2595812678337097\n",
      "[Training Epoch 4] Batch 4276, Loss 0.254111647605896\n",
      "[Training Epoch 4] Batch 4277, Loss 0.24394473433494568\n",
      "[Training Epoch 4] Batch 4278, Loss 0.27036502957344055\n",
      "[Training Epoch 4] Batch 4279, Loss 0.2890074849128723\n",
      "[Training Epoch 4] Batch 4280, Loss 0.25966885685920715\n",
      "[Training Epoch 4] Batch 4281, Loss 0.2614646553993225\n",
      "[Training Epoch 4] Batch 4282, Loss 0.2818310856819153\n",
      "[Training Epoch 4] Batch 4283, Loss 0.24121695756912231\n",
      "[Training Epoch 4] Batch 4284, Loss 0.25217118859291077\n",
      "[Training Epoch 4] Batch 4285, Loss 0.27538883686065674\n",
      "[Training Epoch 4] Batch 4286, Loss 0.27272117137908936\n",
      "[Training Epoch 4] Batch 4287, Loss 0.26724210381507874\n",
      "[Training Epoch 4] Batch 4288, Loss 0.2727469801902771\n",
      "[Training Epoch 4] Batch 4289, Loss 0.2641865015029907\n",
      "[Training Epoch 4] Batch 4290, Loss 0.2601647973060608\n",
      "[Training Epoch 4] Batch 4291, Loss 0.2656073570251465\n",
      "[Training Epoch 4] Batch 4292, Loss 0.2950134575366974\n",
      "[Training Epoch 4] Batch 4293, Loss 0.244826078414917\n",
      "[Training Epoch 4] Batch 4294, Loss 0.2762908935546875\n",
      "[Training Epoch 4] Batch 4295, Loss 0.27371495962142944\n",
      "[Training Epoch 4] Batch 4296, Loss 0.3012315034866333\n",
      "[Training Epoch 4] Batch 4297, Loss 0.2663966715335846\n",
      "[Training Epoch 4] Batch 4298, Loss 0.2500467300415039\n",
      "[Training Epoch 4] Batch 4299, Loss 0.2871900200843811\n",
      "[Training Epoch 4] Batch 4300, Loss 0.24895638227462769\n",
      "[Training Epoch 4] Batch 4301, Loss 0.23712164163589478\n",
      "[Training Epoch 4] Batch 4302, Loss 0.2687496840953827\n",
      "[Training Epoch 4] Batch 4303, Loss 0.23955611884593964\n",
      "[Training Epoch 4] Batch 4304, Loss 0.28708648681640625\n",
      "[Training Epoch 4] Batch 4305, Loss 0.2784131169319153\n",
      "[Training Epoch 4] Batch 4306, Loss 0.25338301062583923\n",
      "[Training Epoch 4] Batch 4307, Loss 0.2894405126571655\n",
      "[Training Epoch 4] Batch 4308, Loss 0.29003840684890747\n",
      "[Training Epoch 4] Batch 4309, Loss 0.25904276967048645\n",
      "[Training Epoch 4] Batch 4310, Loss 0.2606564462184906\n",
      "[Training Epoch 4] Batch 4311, Loss 0.2705633044242859\n",
      "[Training Epoch 4] Batch 4312, Loss 0.22250139713287354\n",
      "[Training Epoch 4] Batch 4313, Loss 0.24749696254730225\n",
      "[Training Epoch 4] Batch 4314, Loss 0.2660423517227173\n",
      "[Training Epoch 4] Batch 4315, Loss 0.27619004249572754\n",
      "[Training Epoch 4] Batch 4316, Loss 0.270025372505188\n",
      "[Training Epoch 4] Batch 4317, Loss 0.2528933882713318\n",
      "[Training Epoch 4] Batch 4318, Loss 0.246779203414917\n",
      "[Training Epoch 4] Batch 4319, Loss 0.27042853832244873\n",
      "[Training Epoch 4] Batch 4320, Loss 0.2634592354297638\n",
      "[Training Epoch 4] Batch 4321, Loss 0.2432764619588852\n",
      "[Training Epoch 4] Batch 4322, Loss 0.2965162396430969\n",
      "[Training Epoch 4] Batch 4323, Loss 0.2597784996032715\n",
      "[Training Epoch 4] Batch 4324, Loss 0.269555002450943\n",
      "[Training Epoch 4] Batch 4325, Loss 0.26107800006866455\n",
      "[Training Epoch 4] Batch 4326, Loss 0.25820186734199524\n",
      "[Training Epoch 4] Batch 4327, Loss 0.2652800381183624\n",
      "[Training Epoch 4] Batch 4328, Loss 0.26181676983833313\n",
      "[Training Epoch 4] Batch 4329, Loss 0.2503170073032379\n",
      "[Training Epoch 4] Batch 4330, Loss 0.2551882863044739\n",
      "[Training Epoch 4] Batch 4331, Loss 0.27733805775642395\n",
      "[Training Epoch 4] Batch 4332, Loss 0.2338375449180603\n",
      "[Training Epoch 4] Batch 4333, Loss 0.2713950276374817\n",
      "[Training Epoch 4] Batch 4334, Loss 0.27762651443481445\n",
      "[Training Epoch 4] Batch 4335, Loss 0.2595275938510895\n",
      "[Training Epoch 4] Batch 4336, Loss 0.25302833318710327\n",
      "[Training Epoch 4] Batch 4337, Loss 0.2937051057815552\n",
      "[Training Epoch 4] Batch 4338, Loss 0.229749858379364\n",
      "[Training Epoch 4] Batch 4339, Loss 0.2632444500923157\n",
      "[Training Epoch 4] Batch 4340, Loss 0.24150148034095764\n",
      "[Training Epoch 4] Batch 4341, Loss 0.2442299723625183\n",
      "[Training Epoch 4] Batch 4342, Loss 0.30594301223754883\n",
      "[Training Epoch 4] Batch 4343, Loss 0.2469225823879242\n",
      "[Training Epoch 4] Batch 4344, Loss 0.24155619740486145\n",
      "[Training Epoch 4] Batch 4345, Loss 0.26187407970428467\n",
      "[Training Epoch 4] Batch 4346, Loss 0.2513464689254761\n",
      "[Training Epoch 4] Batch 4347, Loss 0.2915225625038147\n",
      "[Training Epoch 4] Batch 4348, Loss 0.2767040729522705\n",
      "[Training Epoch 4] Batch 4349, Loss 0.2636505365371704\n",
      "[Training Epoch 4] Batch 4350, Loss 0.2722908854484558\n",
      "[Training Epoch 4] Batch 4351, Loss 0.23433685302734375\n",
      "[Training Epoch 4] Batch 4352, Loss 0.2546853721141815\n",
      "[Training Epoch 4] Batch 4353, Loss 0.25436538457870483\n",
      "[Training Epoch 4] Batch 4354, Loss 0.2676945924758911\n",
      "[Training Epoch 4] Batch 4355, Loss 0.23231783509254456\n",
      "[Training Epoch 4] Batch 4356, Loss 0.28310543298721313\n",
      "[Training Epoch 4] Batch 4357, Loss 0.23811471462249756\n",
      "[Training Epoch 4] Batch 4358, Loss 0.2590596079826355\n",
      "[Training Epoch 4] Batch 4359, Loss 0.24094277620315552\n",
      "[Training Epoch 4] Batch 4360, Loss 0.2722194790840149\n",
      "[Training Epoch 4] Batch 4361, Loss 0.25448012351989746\n",
      "[Training Epoch 4] Batch 4362, Loss 0.25336363911628723\n",
      "[Training Epoch 4] Batch 4363, Loss 0.2588340640068054\n",
      "[Training Epoch 4] Batch 4364, Loss 0.29165688157081604\n",
      "[Training Epoch 4] Batch 4365, Loss 0.29730308055877686\n",
      "[Training Epoch 4] Batch 4366, Loss 0.2529417872428894\n",
      "[Training Epoch 4] Batch 4367, Loss 0.2612207233905792\n",
      "[Training Epoch 4] Batch 4368, Loss 0.2570189833641052\n",
      "[Training Epoch 4] Batch 4369, Loss 0.2566705346107483\n",
      "[Training Epoch 4] Batch 4370, Loss 0.2595195770263672\n",
      "[Training Epoch 4] Batch 4371, Loss 0.2634771466255188\n",
      "[Training Epoch 4] Batch 4372, Loss 0.2660268545150757\n",
      "[Training Epoch 4] Batch 4373, Loss 0.2615095376968384\n",
      "[Training Epoch 4] Batch 4374, Loss 0.2633891701698303\n",
      "[Training Epoch 4] Batch 4375, Loss 0.24055707454681396\n",
      "[Training Epoch 4] Batch 4376, Loss 0.2772282660007477\n",
      "[Training Epoch 4] Batch 4377, Loss 0.29027289152145386\n",
      "[Training Epoch 4] Batch 4378, Loss 0.32258275151252747\n",
      "[Training Epoch 4] Batch 4379, Loss 0.267900288105011\n",
      "[Training Epoch 4] Batch 4380, Loss 0.2646214962005615\n",
      "[Training Epoch 4] Batch 4381, Loss 0.2794911563396454\n",
      "[Training Epoch 4] Batch 4382, Loss 0.31795328855514526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2236.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 4] Precision = 0.2615, Recall = 0.7742\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.28261321783065796\n",
      "[Training Epoch 5] Batch 1, Loss 0.2614433169364929\n",
      "[Training Epoch 5] Batch 2, Loss 0.2504774034023285\n",
      "[Training Epoch 5] Batch 3, Loss 0.2848097085952759\n",
      "[Training Epoch 5] Batch 4, Loss 0.27237385511398315\n",
      "[Training Epoch 5] Batch 5, Loss 0.25333815813064575\n",
      "[Training Epoch 5] Batch 6, Loss 0.2596832513809204\n",
      "[Training Epoch 5] Batch 7, Loss 0.2382521778345108\n",
      "[Training Epoch 5] Batch 8, Loss 0.2619789242744446\n",
      "[Training Epoch 5] Batch 9, Loss 0.26289230585098267\n",
      "[Training Epoch 5] Batch 10, Loss 0.24932755529880524\n",
      "[Training Epoch 5] Batch 11, Loss 0.25418782234191895\n",
      "[Training Epoch 5] Batch 12, Loss 0.22430717945098877\n",
      "[Training Epoch 5] Batch 13, Loss 0.2638402581214905\n",
      "[Training Epoch 5] Batch 14, Loss 0.25772157311439514\n",
      "[Training Epoch 5] Batch 15, Loss 0.25339770317077637\n",
      "[Training Epoch 5] Batch 16, Loss 0.2612823247909546\n",
      "[Training Epoch 5] Batch 17, Loss 0.309858500957489\n",
      "[Training Epoch 5] Batch 18, Loss 0.2327030897140503\n",
      "[Training Epoch 5] Batch 19, Loss 0.2802266776561737\n",
      "[Training Epoch 5] Batch 20, Loss 0.2755424976348877\n",
      "[Training Epoch 5] Batch 21, Loss 0.2508961260318756\n",
      "[Training Epoch 5] Batch 22, Loss 0.23994365334510803\n",
      "[Training Epoch 5] Batch 23, Loss 0.2171117067337036\n",
      "[Training Epoch 5] Batch 24, Loss 0.2801728844642639\n",
      "[Training Epoch 5] Batch 25, Loss 0.2555162310600281\n",
      "[Training Epoch 5] Batch 26, Loss 0.2533611059188843\n",
      "[Training Epoch 5] Batch 27, Loss 0.24941647052764893\n",
      "[Training Epoch 5] Batch 28, Loss 0.24992701411247253\n",
      "[Training Epoch 5] Batch 29, Loss 0.22790658473968506\n",
      "[Training Epoch 5] Batch 30, Loss 0.25728899240493774\n",
      "[Training Epoch 5] Batch 31, Loss 0.2843219041824341\n",
      "[Training Epoch 5] Batch 32, Loss 0.23506252467632294\n",
      "[Training Epoch 5] Batch 33, Loss 0.23254859447479248\n",
      "[Training Epoch 5] Batch 34, Loss 0.2633455693721771\n",
      "[Training Epoch 5] Batch 35, Loss 0.24302853643894196\n",
      "[Training Epoch 5] Batch 36, Loss 0.24384373426437378\n",
      "[Training Epoch 5] Batch 37, Loss 0.26140230894088745\n",
      "[Training Epoch 5] Batch 38, Loss 0.24448275566101074\n",
      "[Training Epoch 5] Batch 39, Loss 0.2693674564361572\n",
      "[Training Epoch 5] Batch 40, Loss 0.263093501329422\n",
      "[Training Epoch 5] Batch 41, Loss 0.24160072207450867\n",
      "[Training Epoch 5] Batch 42, Loss 0.2646016776561737\n",
      "[Training Epoch 5] Batch 43, Loss 0.25055062770843506\n",
      "[Training Epoch 5] Batch 44, Loss 0.25291794538497925\n",
      "[Training Epoch 5] Batch 45, Loss 0.2734560966491699\n",
      "[Training Epoch 5] Batch 46, Loss 0.2665085196495056\n",
      "[Training Epoch 5] Batch 47, Loss 0.25075721740722656\n",
      "[Training Epoch 5] Batch 48, Loss 0.25613462924957275\n",
      "[Training Epoch 5] Batch 49, Loss 0.278939425945282\n",
      "[Training Epoch 5] Batch 50, Loss 0.2460654377937317\n",
      "[Training Epoch 5] Batch 51, Loss 0.2694976329803467\n",
      "[Training Epoch 5] Batch 52, Loss 0.2624049186706543\n",
      "[Training Epoch 5] Batch 53, Loss 0.2699941396713257\n",
      "[Training Epoch 5] Batch 54, Loss 0.2567811906337738\n",
      "[Training Epoch 5] Batch 55, Loss 0.24981734156608582\n",
      "[Training Epoch 5] Batch 56, Loss 0.23382653295993805\n",
      "[Training Epoch 5] Batch 57, Loss 0.24906407296657562\n",
      "[Training Epoch 5] Batch 58, Loss 0.28145408630371094\n",
      "[Training Epoch 5] Batch 59, Loss 0.2704843282699585\n",
      "[Training Epoch 5] Batch 60, Loss 0.24089445173740387\n",
      "[Training Epoch 5] Batch 61, Loss 0.2788718342781067\n",
      "[Training Epoch 5] Batch 62, Loss 0.2538238763809204\n",
      "[Training Epoch 5] Batch 63, Loss 0.27608728408813477\n",
      "[Training Epoch 5] Batch 64, Loss 0.2710922062397003\n",
      "[Training Epoch 5] Batch 65, Loss 0.26493099331855774\n",
      "[Training Epoch 5] Batch 66, Loss 0.24147433042526245\n",
      "[Training Epoch 5] Batch 67, Loss 0.26505357027053833\n",
      "[Training Epoch 5] Batch 68, Loss 0.27193737030029297\n",
      "[Training Epoch 5] Batch 69, Loss 0.27453377842903137\n",
      "[Training Epoch 5] Batch 70, Loss 0.26861345767974854\n",
      "[Training Epoch 5] Batch 71, Loss 0.25389528274536133\n",
      "[Training Epoch 5] Batch 72, Loss 0.2837127447128296\n",
      "[Training Epoch 5] Batch 73, Loss 0.2687574625015259\n",
      "[Training Epoch 5] Batch 74, Loss 0.2866496741771698\n",
      "[Training Epoch 5] Batch 75, Loss 0.26393407583236694\n",
      "[Training Epoch 5] Batch 76, Loss 0.29297786951065063\n",
      "[Training Epoch 5] Batch 77, Loss 0.2677093744277954\n",
      "[Training Epoch 5] Batch 78, Loss 0.237547367811203\n",
      "[Training Epoch 5] Batch 79, Loss 0.26092836260795593\n",
      "[Training Epoch 5] Batch 80, Loss 0.27199089527130127\n",
      "[Training Epoch 5] Batch 81, Loss 0.23208916187286377\n",
      "[Training Epoch 5] Batch 82, Loss 0.24360975623130798\n",
      "[Training Epoch 5] Batch 83, Loss 0.2701343894004822\n",
      "[Training Epoch 5] Batch 84, Loss 0.24043801426887512\n",
      "[Training Epoch 5] Batch 85, Loss 0.23691734671592712\n",
      "[Training Epoch 5] Batch 86, Loss 0.24576805531978607\n",
      "[Training Epoch 5] Batch 87, Loss 0.2499249279499054\n",
      "[Training Epoch 5] Batch 88, Loss 0.25636914372444153\n",
      "[Training Epoch 5] Batch 89, Loss 0.2482035756111145\n",
      "[Training Epoch 5] Batch 90, Loss 0.23477444052696228\n",
      "[Training Epoch 5] Batch 91, Loss 0.2615918517112732\n",
      "[Training Epoch 5] Batch 92, Loss 0.24653831124305725\n",
      "[Training Epoch 5] Batch 93, Loss 0.26179662346839905\n",
      "[Training Epoch 5] Batch 94, Loss 0.24850529432296753\n",
      "[Training Epoch 5] Batch 95, Loss 0.261375367641449\n",
      "[Training Epoch 5] Batch 96, Loss 0.2567336857318878\n",
      "[Training Epoch 5] Batch 97, Loss 0.26518529653549194\n",
      "[Training Epoch 5] Batch 98, Loss 0.2645745873451233\n",
      "[Training Epoch 5] Batch 99, Loss 0.2617912292480469\n",
      "[Training Epoch 5] Batch 100, Loss 0.23988157510757446\n",
      "[Training Epoch 5] Batch 101, Loss 0.24419352412223816\n",
      "[Training Epoch 5] Batch 102, Loss 0.23816154897212982\n",
      "[Training Epoch 5] Batch 103, Loss 0.2583701014518738\n",
      "[Training Epoch 5] Batch 104, Loss 0.26281046867370605\n",
      "[Training Epoch 5] Batch 105, Loss 0.25035426020622253\n",
      "[Training Epoch 5] Batch 106, Loss 0.24555793404579163\n",
      "[Training Epoch 5] Batch 107, Loss 0.2594037652015686\n",
      "[Training Epoch 5] Batch 108, Loss 0.25462740659713745\n",
      "[Training Epoch 5] Batch 109, Loss 0.2514103353023529\n",
      "[Training Epoch 5] Batch 110, Loss 0.2526742219924927\n",
      "[Training Epoch 5] Batch 111, Loss 0.25546973943710327\n",
      "[Training Epoch 5] Batch 112, Loss 0.26000791788101196\n",
      "[Training Epoch 5] Batch 113, Loss 0.26075634360313416\n",
      "[Training Epoch 5] Batch 114, Loss 0.271918922662735\n",
      "[Training Epoch 5] Batch 115, Loss 0.23330485820770264\n",
      "[Training Epoch 5] Batch 116, Loss 0.23908889293670654\n",
      "[Training Epoch 5] Batch 117, Loss 0.2449013888835907\n",
      "[Training Epoch 5] Batch 118, Loss 0.24046441912651062\n",
      "[Training Epoch 5] Batch 119, Loss 0.260390043258667\n",
      "[Training Epoch 5] Batch 120, Loss 0.27759942412376404\n",
      "[Training Epoch 5] Batch 121, Loss 0.2613760828971863\n",
      "[Training Epoch 5] Batch 122, Loss 0.25891563296318054\n",
      "[Training Epoch 5] Batch 123, Loss 0.26066160202026367\n",
      "[Training Epoch 5] Batch 124, Loss 0.24695762991905212\n",
      "[Training Epoch 5] Batch 125, Loss 0.2569591999053955\n",
      "[Training Epoch 5] Batch 126, Loss 0.256176233291626\n",
      "[Training Epoch 5] Batch 127, Loss 0.2698119580745697\n",
      "[Training Epoch 5] Batch 128, Loss 0.26196199655532837\n",
      "[Training Epoch 5] Batch 129, Loss 0.2366836667060852\n",
      "[Training Epoch 5] Batch 130, Loss 0.2671242952346802\n",
      "[Training Epoch 5] Batch 131, Loss 0.23838411271572113\n",
      "[Training Epoch 5] Batch 132, Loss 0.2523556351661682\n",
      "[Training Epoch 5] Batch 133, Loss 0.27861806750297546\n",
      "[Training Epoch 5] Batch 134, Loss 0.25506308674812317\n",
      "[Training Epoch 5] Batch 135, Loss 0.2125781774520874\n",
      "[Training Epoch 5] Batch 136, Loss 0.2517876625061035\n",
      "[Training Epoch 5] Batch 137, Loss 0.2675204873085022\n",
      "[Training Epoch 5] Batch 138, Loss 0.23402047157287598\n",
      "[Training Epoch 5] Batch 139, Loss 0.2636074423789978\n",
      "[Training Epoch 5] Batch 140, Loss 0.2697756290435791\n",
      "[Training Epoch 5] Batch 141, Loss 0.23949900269508362\n",
      "[Training Epoch 5] Batch 142, Loss 0.2634408473968506\n",
      "[Training Epoch 5] Batch 143, Loss 0.2770881950855255\n",
      "[Training Epoch 5] Batch 144, Loss 0.26346391439437866\n",
      "[Training Epoch 5] Batch 145, Loss 0.268718957901001\n",
      "[Training Epoch 5] Batch 146, Loss 0.2629460096359253\n",
      "[Training Epoch 5] Batch 147, Loss 0.26610618829727173\n",
      "[Training Epoch 5] Batch 148, Loss 0.2517474889755249\n",
      "[Training Epoch 5] Batch 149, Loss 0.24628625810146332\n",
      "[Training Epoch 5] Batch 150, Loss 0.24210181832313538\n",
      "[Training Epoch 5] Batch 151, Loss 0.2351641058921814\n",
      "[Training Epoch 5] Batch 152, Loss 0.23121504485607147\n",
      "[Training Epoch 5] Batch 153, Loss 0.26096686720848083\n",
      "[Training Epoch 5] Batch 154, Loss 0.22664007544517517\n",
      "[Training Epoch 5] Batch 155, Loss 0.24635210633277893\n",
      "[Training Epoch 5] Batch 156, Loss 0.26370739936828613\n",
      "[Training Epoch 5] Batch 157, Loss 0.24461612105369568\n",
      "[Training Epoch 5] Batch 158, Loss 0.2752518653869629\n",
      "[Training Epoch 5] Batch 159, Loss 0.25564008951187134\n",
      "[Training Epoch 5] Batch 160, Loss 0.26143085956573486\n",
      "[Training Epoch 5] Batch 161, Loss 0.2707750201225281\n",
      "[Training Epoch 5] Batch 162, Loss 0.2561413645744324\n",
      "[Training Epoch 5] Batch 163, Loss 0.2577912509441376\n",
      "[Training Epoch 5] Batch 164, Loss 0.2451181411743164\n",
      "[Training Epoch 5] Batch 165, Loss 0.23835664987564087\n",
      "[Training Epoch 5] Batch 166, Loss 0.2615653872489929\n",
      "[Training Epoch 5] Batch 167, Loss 0.2610849142074585\n",
      "[Training Epoch 5] Batch 168, Loss 0.24891096353530884\n",
      "[Training Epoch 5] Batch 169, Loss 0.25406554341316223\n",
      "[Training Epoch 5] Batch 170, Loss 0.25546491146087646\n",
      "[Training Epoch 5] Batch 171, Loss 0.2598216235637665\n",
      "[Training Epoch 5] Batch 172, Loss 0.26224902272224426\n",
      "[Training Epoch 5] Batch 173, Loss 0.272725373506546\n",
      "[Training Epoch 5] Batch 174, Loss 0.2712134122848511\n",
      "[Training Epoch 5] Batch 175, Loss 0.2618160843849182\n",
      "[Training Epoch 5] Batch 176, Loss 0.2600701153278351\n",
      "[Training Epoch 5] Batch 177, Loss 0.25580742955207825\n",
      "[Training Epoch 5] Batch 178, Loss 0.26426705718040466\n",
      "[Training Epoch 5] Batch 179, Loss 0.2820313572883606\n",
      "[Training Epoch 5] Batch 180, Loss 0.25456491112709045\n",
      "[Training Epoch 5] Batch 181, Loss 0.27034059166908264\n",
      "[Training Epoch 5] Batch 182, Loss 0.2624403238296509\n",
      "[Training Epoch 5] Batch 183, Loss 0.26725322008132935\n",
      "[Training Epoch 5] Batch 184, Loss 0.2489403784275055\n",
      "[Training Epoch 5] Batch 185, Loss 0.2718617916107178\n",
      "[Training Epoch 5] Batch 186, Loss 0.26869040727615356\n",
      "[Training Epoch 5] Batch 187, Loss 0.2570597529411316\n",
      "[Training Epoch 5] Batch 188, Loss 0.24321143329143524\n",
      "[Training Epoch 5] Batch 189, Loss 0.2586566209793091\n",
      "[Training Epoch 5] Batch 190, Loss 0.26563093066215515\n",
      "[Training Epoch 5] Batch 191, Loss 0.2348594218492508\n",
      "[Training Epoch 5] Batch 192, Loss 0.23673728108406067\n",
      "[Training Epoch 5] Batch 193, Loss 0.2211836874485016\n",
      "[Training Epoch 5] Batch 194, Loss 0.25432634353637695\n",
      "[Training Epoch 5] Batch 195, Loss 0.25773537158966064\n",
      "[Training Epoch 5] Batch 196, Loss 0.24293094873428345\n",
      "[Training Epoch 5] Batch 197, Loss 0.25754669308662415\n",
      "[Training Epoch 5] Batch 198, Loss 0.25476834177970886\n",
      "[Training Epoch 5] Batch 199, Loss 0.2590399384498596\n",
      "[Training Epoch 5] Batch 200, Loss 0.27206990122795105\n",
      "[Training Epoch 5] Batch 201, Loss 0.27333176136016846\n",
      "[Training Epoch 5] Batch 202, Loss 0.2529457211494446\n",
      "[Training Epoch 5] Batch 203, Loss 0.2586742043495178\n",
      "[Training Epoch 5] Batch 204, Loss 0.2530665695667267\n",
      "[Training Epoch 5] Batch 205, Loss 0.2699674963951111\n",
      "[Training Epoch 5] Batch 206, Loss 0.26161956787109375\n",
      "[Training Epoch 5] Batch 207, Loss 0.2535455822944641\n",
      "[Training Epoch 5] Batch 208, Loss 0.28860965371131897\n",
      "[Training Epoch 5] Batch 209, Loss 0.2626289129257202\n",
      "[Training Epoch 5] Batch 210, Loss 0.25620871782302856\n",
      "[Training Epoch 5] Batch 211, Loss 0.2719067633152008\n",
      "[Training Epoch 5] Batch 212, Loss 0.24988476932048798\n",
      "[Training Epoch 5] Batch 213, Loss 0.2577023208141327\n",
      "[Training Epoch 5] Batch 214, Loss 0.2641279697418213\n",
      "[Training Epoch 5] Batch 215, Loss 0.2462003082036972\n",
      "[Training Epoch 5] Batch 216, Loss 0.23358787596225739\n",
      "[Training Epoch 5] Batch 217, Loss 0.24491667747497559\n",
      "[Training Epoch 5] Batch 218, Loss 0.2541043758392334\n",
      "[Training Epoch 5] Batch 219, Loss 0.2581859529018402\n",
      "[Training Epoch 5] Batch 220, Loss 0.26876088976860046\n",
      "[Training Epoch 5] Batch 221, Loss 0.21276095509529114\n",
      "[Training Epoch 5] Batch 222, Loss 0.272701233625412\n",
      "[Training Epoch 5] Batch 223, Loss 0.27291974425315857\n",
      "[Training Epoch 5] Batch 224, Loss 0.25140202045440674\n",
      "[Training Epoch 5] Batch 225, Loss 0.25369831919670105\n",
      "[Training Epoch 5] Batch 226, Loss 0.23506209254264832\n",
      "[Training Epoch 5] Batch 227, Loss 0.253054141998291\n",
      "[Training Epoch 5] Batch 228, Loss 0.251467227935791\n",
      "[Training Epoch 5] Batch 229, Loss 0.24187207221984863\n",
      "[Training Epoch 5] Batch 230, Loss 0.26681721210479736\n",
      "[Training Epoch 5] Batch 231, Loss 0.261935293674469\n",
      "[Training Epoch 5] Batch 232, Loss 0.23609542846679688\n",
      "[Training Epoch 5] Batch 233, Loss 0.26694780588150024\n",
      "[Training Epoch 5] Batch 234, Loss 0.24767853319644928\n",
      "[Training Epoch 5] Batch 235, Loss 0.27740606665611267\n",
      "[Training Epoch 5] Batch 236, Loss 0.2460973560810089\n",
      "[Training Epoch 5] Batch 237, Loss 0.2644648253917694\n",
      "[Training Epoch 5] Batch 238, Loss 0.2591297924518585\n",
      "[Training Epoch 5] Batch 239, Loss 0.2796412706375122\n",
      "[Training Epoch 5] Batch 240, Loss 0.2555857300758362\n",
      "[Training Epoch 5] Batch 241, Loss 0.2608259320259094\n",
      "[Training Epoch 5] Batch 242, Loss 0.24388661980628967\n",
      "[Training Epoch 5] Batch 243, Loss 0.2294285148382187\n",
      "[Training Epoch 5] Batch 244, Loss 0.27038371562957764\n",
      "[Training Epoch 5] Batch 245, Loss 0.24878059327602386\n",
      "[Training Epoch 5] Batch 246, Loss 0.2419293224811554\n",
      "[Training Epoch 5] Batch 247, Loss 0.2590806484222412\n",
      "[Training Epoch 5] Batch 248, Loss 0.24921244382858276\n",
      "[Training Epoch 5] Batch 249, Loss 0.24802710115909576\n",
      "[Training Epoch 5] Batch 250, Loss 0.25717639923095703\n",
      "[Training Epoch 5] Batch 251, Loss 0.26460400223731995\n",
      "[Training Epoch 5] Batch 252, Loss 0.23649728298187256\n",
      "[Training Epoch 5] Batch 253, Loss 0.20872780680656433\n",
      "[Training Epoch 5] Batch 254, Loss 0.23298868536949158\n",
      "[Training Epoch 5] Batch 255, Loss 0.26138341426849365\n",
      "[Training Epoch 5] Batch 256, Loss 0.2631199061870575\n",
      "[Training Epoch 5] Batch 257, Loss 0.24139824509620667\n",
      "[Training Epoch 5] Batch 258, Loss 0.24646374583244324\n",
      "[Training Epoch 5] Batch 259, Loss 0.23558196425437927\n",
      "[Training Epoch 5] Batch 260, Loss 0.2862972915172577\n",
      "[Training Epoch 5] Batch 261, Loss 0.24405495822429657\n",
      "[Training Epoch 5] Batch 262, Loss 0.24189342558383942\n",
      "[Training Epoch 5] Batch 263, Loss 0.25422403216362\n",
      "[Training Epoch 5] Batch 264, Loss 0.2409244179725647\n",
      "[Training Epoch 5] Batch 265, Loss 0.26343509554862976\n",
      "[Training Epoch 5] Batch 266, Loss 0.24509000778198242\n",
      "[Training Epoch 5] Batch 267, Loss 0.2694081664085388\n",
      "[Training Epoch 5] Batch 268, Loss 0.23376426100730896\n",
      "[Training Epoch 5] Batch 269, Loss 0.23851469159126282\n",
      "[Training Epoch 5] Batch 270, Loss 0.2415284365415573\n",
      "[Training Epoch 5] Batch 271, Loss 0.23584118485450745\n",
      "[Training Epoch 5] Batch 272, Loss 0.25969040393829346\n",
      "[Training Epoch 5] Batch 273, Loss 0.23384760320186615\n",
      "[Training Epoch 5] Batch 274, Loss 0.26711225509643555\n",
      "[Training Epoch 5] Batch 275, Loss 0.26290011405944824\n",
      "[Training Epoch 5] Batch 276, Loss 0.2342880368232727\n",
      "[Training Epoch 5] Batch 277, Loss 0.25615406036376953\n",
      "[Training Epoch 5] Batch 278, Loss 0.2985627055168152\n",
      "[Training Epoch 5] Batch 279, Loss 0.26354166865348816\n",
      "[Training Epoch 5] Batch 280, Loss 0.27010035514831543\n",
      "[Training Epoch 5] Batch 281, Loss 0.26010316610336304\n",
      "[Training Epoch 5] Batch 282, Loss 0.24421636760234833\n",
      "[Training Epoch 5] Batch 283, Loss 0.24490082263946533\n",
      "[Training Epoch 5] Batch 284, Loss 0.20811080932617188\n",
      "[Training Epoch 5] Batch 285, Loss 0.23510220646858215\n",
      "[Training Epoch 5] Batch 286, Loss 0.25842317938804626\n",
      "[Training Epoch 5] Batch 287, Loss 0.26276910305023193\n",
      "[Training Epoch 5] Batch 288, Loss 0.27591371536254883\n",
      "[Training Epoch 5] Batch 289, Loss 0.2644893527030945\n",
      "[Training Epoch 5] Batch 290, Loss 0.23027797043323517\n",
      "[Training Epoch 5] Batch 291, Loss 0.2638426423072815\n",
      "[Training Epoch 5] Batch 292, Loss 0.27011704444885254\n",
      "[Training Epoch 5] Batch 293, Loss 0.26672351360321045\n",
      "[Training Epoch 5] Batch 294, Loss 0.2647261619567871\n",
      "[Training Epoch 5] Batch 295, Loss 0.2520741820335388\n",
      "[Training Epoch 5] Batch 296, Loss 0.26920807361602783\n",
      "[Training Epoch 5] Batch 297, Loss 0.24883492290973663\n",
      "[Training Epoch 5] Batch 298, Loss 0.27266013622283936\n",
      "[Training Epoch 5] Batch 299, Loss 0.2525217533111572\n",
      "[Training Epoch 5] Batch 300, Loss 0.2322683036327362\n",
      "[Training Epoch 5] Batch 301, Loss 0.3002697825431824\n",
      "[Training Epoch 5] Batch 302, Loss 0.23310859501361847\n",
      "[Training Epoch 5] Batch 303, Loss 0.2735058069229126\n",
      "[Training Epoch 5] Batch 304, Loss 0.2583848834037781\n",
      "[Training Epoch 5] Batch 305, Loss 0.2568051815032959\n",
      "[Training Epoch 5] Batch 306, Loss 0.2522817850112915\n",
      "[Training Epoch 5] Batch 307, Loss 0.2538353204727173\n",
      "[Training Epoch 5] Batch 308, Loss 0.23761402070522308\n",
      "[Training Epoch 5] Batch 309, Loss 0.25545018911361694\n",
      "[Training Epoch 5] Batch 310, Loss 0.2228681445121765\n",
      "[Training Epoch 5] Batch 311, Loss 0.2358773648738861\n",
      "[Training Epoch 5] Batch 312, Loss 0.27060258388519287\n",
      "[Training Epoch 5] Batch 313, Loss 0.24432042241096497\n",
      "[Training Epoch 5] Batch 314, Loss 0.28641366958618164\n",
      "[Training Epoch 5] Batch 315, Loss 0.25952422618865967\n",
      "[Training Epoch 5] Batch 316, Loss 0.2714741826057434\n",
      "[Training Epoch 5] Batch 317, Loss 0.2662336826324463\n",
      "[Training Epoch 5] Batch 318, Loss 0.24671754240989685\n",
      "[Training Epoch 5] Batch 319, Loss 0.2430947721004486\n",
      "[Training Epoch 5] Batch 320, Loss 0.2509382963180542\n",
      "[Training Epoch 5] Batch 321, Loss 0.29215070605278015\n",
      "[Training Epoch 5] Batch 322, Loss 0.24925661087036133\n",
      "[Training Epoch 5] Batch 323, Loss 0.22055387496948242\n",
      "[Training Epoch 5] Batch 324, Loss 0.27552667260169983\n",
      "[Training Epoch 5] Batch 325, Loss 0.2596485912799835\n",
      "[Training Epoch 5] Batch 326, Loss 0.2727336287498474\n",
      "[Training Epoch 5] Batch 327, Loss 0.2813062071800232\n",
      "[Training Epoch 5] Batch 328, Loss 0.2310473918914795\n",
      "[Training Epoch 5] Batch 329, Loss 0.3019276261329651\n",
      "[Training Epoch 5] Batch 330, Loss 0.26162731647491455\n",
      "[Training Epoch 5] Batch 331, Loss 0.25627654790878296\n",
      "[Training Epoch 5] Batch 332, Loss 0.2954828441143036\n",
      "[Training Epoch 5] Batch 333, Loss 0.2755996584892273\n",
      "[Training Epoch 5] Batch 334, Loss 0.2684117257595062\n",
      "[Training Epoch 5] Batch 335, Loss 0.28448399901390076\n",
      "[Training Epoch 5] Batch 336, Loss 0.2624065577983856\n",
      "[Training Epoch 5] Batch 337, Loss 0.29199808835983276\n",
      "[Training Epoch 5] Batch 338, Loss 0.25314846634864807\n",
      "[Training Epoch 5] Batch 339, Loss 0.2764832079410553\n",
      "[Training Epoch 5] Batch 340, Loss 0.2565702199935913\n",
      "[Training Epoch 5] Batch 341, Loss 0.27223092317581177\n",
      "[Training Epoch 5] Batch 342, Loss 0.26936718821525574\n",
      "[Training Epoch 5] Batch 343, Loss 0.26641446352005005\n",
      "[Training Epoch 5] Batch 344, Loss 0.2718902826309204\n",
      "[Training Epoch 5] Batch 345, Loss 0.22795559465885162\n",
      "[Training Epoch 5] Batch 346, Loss 0.22082918882369995\n",
      "[Training Epoch 5] Batch 347, Loss 0.2722572684288025\n",
      "[Training Epoch 5] Batch 348, Loss 0.27314281463623047\n",
      "[Training Epoch 5] Batch 349, Loss 0.25992608070373535\n",
      "[Training Epoch 5] Batch 350, Loss 0.2596752941608429\n",
      "[Training Epoch 5] Batch 351, Loss 0.25520411133766174\n",
      "[Training Epoch 5] Batch 352, Loss 0.24518705904483795\n",
      "[Training Epoch 5] Batch 353, Loss 0.30024898052215576\n",
      "[Training Epoch 5] Batch 354, Loss 0.274016797542572\n",
      "[Training Epoch 5] Batch 355, Loss 0.2738451063632965\n",
      "[Training Epoch 5] Batch 356, Loss 0.2602214217185974\n",
      "[Training Epoch 5] Batch 357, Loss 0.26315921545028687\n",
      "[Training Epoch 5] Batch 358, Loss 0.25281745195388794\n",
      "[Training Epoch 5] Batch 359, Loss 0.25772666931152344\n",
      "[Training Epoch 5] Batch 360, Loss 0.27257996797561646\n",
      "[Training Epoch 5] Batch 361, Loss 0.2785075902938843\n",
      "[Training Epoch 5] Batch 362, Loss 0.26634666323661804\n",
      "[Training Epoch 5] Batch 363, Loss 0.23759624361991882\n",
      "[Training Epoch 5] Batch 364, Loss 0.24307256937026978\n",
      "[Training Epoch 5] Batch 365, Loss 0.2672995626926422\n",
      "[Training Epoch 5] Batch 366, Loss 0.24736753106117249\n",
      "[Training Epoch 5] Batch 367, Loss 0.2319890260696411\n",
      "[Training Epoch 5] Batch 368, Loss 0.2919655442237854\n",
      "[Training Epoch 5] Batch 369, Loss 0.2446661740541458\n",
      "[Training Epoch 5] Batch 370, Loss 0.2604738771915436\n",
      "[Training Epoch 5] Batch 371, Loss 0.24940602481365204\n",
      "[Training Epoch 5] Batch 372, Loss 0.2677702307701111\n",
      "[Training Epoch 5] Batch 373, Loss 0.2811047434806824\n",
      "[Training Epoch 5] Batch 374, Loss 0.28156810998916626\n",
      "[Training Epoch 5] Batch 375, Loss 0.26443010568618774\n",
      "[Training Epoch 5] Batch 376, Loss 0.2591136693954468\n",
      "[Training Epoch 5] Batch 377, Loss 0.2512286305427551\n",
      "[Training Epoch 5] Batch 378, Loss 0.26878735423088074\n",
      "[Training Epoch 5] Batch 379, Loss 0.2533683776855469\n",
      "[Training Epoch 5] Batch 380, Loss 0.26722007989883423\n",
      "[Training Epoch 5] Batch 381, Loss 0.260175496339798\n",
      "[Training Epoch 5] Batch 382, Loss 0.24625425040721893\n",
      "[Training Epoch 5] Batch 383, Loss 0.25553134083747864\n",
      "[Training Epoch 5] Batch 384, Loss 0.2591093182563782\n",
      "[Training Epoch 5] Batch 385, Loss 0.24661999940872192\n",
      "[Training Epoch 5] Batch 386, Loss 0.25374555587768555\n",
      "[Training Epoch 5] Batch 387, Loss 0.25221139192581177\n",
      "[Training Epoch 5] Batch 388, Loss 0.2727428674697876\n",
      "[Training Epoch 5] Batch 389, Loss 0.2532181143760681\n",
      "[Training Epoch 5] Batch 390, Loss 0.25267407298088074\n",
      "[Training Epoch 5] Batch 391, Loss 0.2628474533557892\n",
      "[Training Epoch 5] Batch 392, Loss 0.2833326458930969\n",
      "[Training Epoch 5] Batch 393, Loss 0.2618906795978546\n",
      "[Training Epoch 5] Batch 394, Loss 0.2583165168762207\n",
      "[Training Epoch 5] Batch 395, Loss 0.2632104754447937\n",
      "[Training Epoch 5] Batch 396, Loss 0.2768133878707886\n",
      "[Training Epoch 5] Batch 397, Loss 0.25571712851524353\n",
      "[Training Epoch 5] Batch 398, Loss 0.2580776512622833\n",
      "[Training Epoch 5] Batch 399, Loss 0.27040529251098633\n",
      "[Training Epoch 5] Batch 400, Loss 0.2355390191078186\n",
      "[Training Epoch 5] Batch 401, Loss 0.27849990129470825\n",
      "[Training Epoch 5] Batch 402, Loss 0.24937060475349426\n",
      "[Training Epoch 5] Batch 403, Loss 0.24169155955314636\n",
      "[Training Epoch 5] Batch 404, Loss 0.24882330000400543\n",
      "[Training Epoch 5] Batch 405, Loss 0.2721809446811676\n",
      "[Training Epoch 5] Batch 406, Loss 0.23324835300445557\n",
      "[Training Epoch 5] Batch 407, Loss 0.2324666678905487\n",
      "[Training Epoch 5] Batch 408, Loss 0.2734302282333374\n",
      "[Training Epoch 5] Batch 409, Loss 0.2658386826515198\n",
      "[Training Epoch 5] Batch 410, Loss 0.27628380060195923\n",
      "[Training Epoch 5] Batch 411, Loss 0.25504231452941895\n",
      "[Training Epoch 5] Batch 412, Loss 0.25462234020233154\n",
      "[Training Epoch 5] Batch 413, Loss 0.22498859465122223\n",
      "[Training Epoch 5] Batch 414, Loss 0.29952022433280945\n",
      "[Training Epoch 5] Batch 415, Loss 0.24843604862689972\n",
      "[Training Epoch 5] Batch 416, Loss 0.239554300904274\n",
      "[Training Epoch 5] Batch 417, Loss 0.2890093922615051\n",
      "[Training Epoch 5] Batch 418, Loss 0.2585528492927551\n",
      "[Training Epoch 5] Batch 419, Loss 0.27807947993278503\n",
      "[Training Epoch 5] Batch 420, Loss 0.24970535933971405\n",
      "[Training Epoch 5] Batch 421, Loss 0.2598699629306793\n",
      "[Training Epoch 5] Batch 422, Loss 0.2632693648338318\n",
      "[Training Epoch 5] Batch 423, Loss 0.288553386926651\n",
      "[Training Epoch 5] Batch 424, Loss 0.2653989791870117\n",
      "[Training Epoch 5] Batch 425, Loss 0.24026133120059967\n",
      "[Training Epoch 5] Batch 426, Loss 0.25191813707351685\n",
      "[Training Epoch 5] Batch 427, Loss 0.2400207817554474\n",
      "[Training Epoch 5] Batch 428, Loss 0.2497585415840149\n",
      "[Training Epoch 5] Batch 429, Loss 0.24253281950950623\n",
      "[Training Epoch 5] Batch 430, Loss 0.2618241310119629\n",
      "[Training Epoch 5] Batch 431, Loss 0.2553008198738098\n",
      "[Training Epoch 5] Batch 432, Loss 0.24913887679576874\n",
      "[Training Epoch 5] Batch 433, Loss 0.2834579348564148\n",
      "[Training Epoch 5] Batch 434, Loss 0.2583162188529968\n",
      "[Training Epoch 5] Batch 435, Loss 0.28741830587387085\n",
      "[Training Epoch 5] Batch 436, Loss 0.27365466952323914\n",
      "[Training Epoch 5] Batch 437, Loss 0.23121237754821777\n",
      "[Training Epoch 5] Batch 438, Loss 0.25124359130859375\n",
      "[Training Epoch 5] Batch 439, Loss 0.26839298009872437\n",
      "[Training Epoch 5] Batch 440, Loss 0.26466482877731323\n",
      "[Training Epoch 5] Batch 441, Loss 0.24599066376686096\n",
      "[Training Epoch 5] Batch 442, Loss 0.25280818343162537\n",
      "[Training Epoch 5] Batch 443, Loss 0.25022751092910767\n",
      "[Training Epoch 5] Batch 444, Loss 0.26538553833961487\n",
      "[Training Epoch 5] Batch 445, Loss 0.2336115837097168\n",
      "[Training Epoch 5] Batch 446, Loss 0.2392364889383316\n",
      "[Training Epoch 5] Batch 447, Loss 0.25483596324920654\n",
      "[Training Epoch 5] Batch 448, Loss 0.2698572278022766\n",
      "[Training Epoch 5] Batch 449, Loss 0.2568311393260956\n",
      "[Training Epoch 5] Batch 450, Loss 0.24997013807296753\n",
      "[Training Epoch 5] Batch 451, Loss 0.26531875133514404\n",
      "[Training Epoch 5] Batch 452, Loss 0.2840151786804199\n",
      "[Training Epoch 5] Batch 453, Loss 0.2618491053581238\n",
      "[Training Epoch 5] Batch 454, Loss 0.26424306631088257\n",
      "[Training Epoch 5] Batch 455, Loss 0.24698784947395325\n",
      "[Training Epoch 5] Batch 456, Loss 0.26498162746429443\n",
      "[Training Epoch 5] Batch 457, Loss 0.2711604833602905\n",
      "[Training Epoch 5] Batch 458, Loss 0.2496105432510376\n",
      "[Training Epoch 5] Batch 459, Loss 0.2813626527786255\n",
      "[Training Epoch 5] Batch 460, Loss 0.2440933883190155\n",
      "[Training Epoch 5] Batch 461, Loss 0.2617308497428894\n",
      "[Training Epoch 5] Batch 462, Loss 0.2704943120479584\n",
      "[Training Epoch 5] Batch 463, Loss 0.23268422484397888\n",
      "[Training Epoch 5] Batch 464, Loss 0.23189549148082733\n",
      "[Training Epoch 5] Batch 465, Loss 0.252103328704834\n",
      "[Training Epoch 5] Batch 466, Loss 0.29784807562828064\n",
      "[Training Epoch 5] Batch 467, Loss 0.24318338930606842\n",
      "[Training Epoch 5] Batch 468, Loss 0.26704251766204834\n",
      "[Training Epoch 5] Batch 469, Loss 0.22999317944049835\n",
      "[Training Epoch 5] Batch 470, Loss 0.26204007863998413\n",
      "[Training Epoch 5] Batch 471, Loss 0.284941166639328\n",
      "[Training Epoch 5] Batch 472, Loss 0.22472316026687622\n",
      "[Training Epoch 5] Batch 473, Loss 0.2717745900154114\n",
      "[Training Epoch 5] Batch 474, Loss 0.2626226544380188\n",
      "[Training Epoch 5] Batch 475, Loss 0.2491348534822464\n",
      "[Training Epoch 5] Batch 476, Loss 0.2818140387535095\n",
      "[Training Epoch 5] Batch 477, Loss 0.24496735632419586\n",
      "[Training Epoch 5] Batch 478, Loss 0.28902292251586914\n",
      "[Training Epoch 5] Batch 479, Loss 0.26272737979888916\n",
      "[Training Epoch 5] Batch 480, Loss 0.25370073318481445\n",
      "[Training Epoch 5] Batch 481, Loss 0.2687497138977051\n",
      "[Training Epoch 5] Batch 482, Loss 0.2761181592941284\n",
      "[Training Epoch 5] Batch 483, Loss 0.25086861848831177\n",
      "[Training Epoch 5] Batch 484, Loss 0.2455299198627472\n",
      "[Training Epoch 5] Batch 485, Loss 0.23576216399669647\n",
      "[Training Epoch 5] Batch 486, Loss 0.2542906105518341\n",
      "[Training Epoch 5] Batch 487, Loss 0.258807510137558\n",
      "[Training Epoch 5] Batch 488, Loss 0.24580132961273193\n",
      "[Training Epoch 5] Batch 489, Loss 0.2704923152923584\n",
      "[Training Epoch 5] Batch 490, Loss 0.2500811517238617\n",
      "[Training Epoch 5] Batch 491, Loss 0.25428807735443115\n",
      "[Training Epoch 5] Batch 492, Loss 0.2530169188976288\n",
      "[Training Epoch 5] Batch 493, Loss 0.2610572278499603\n",
      "[Training Epoch 5] Batch 494, Loss 0.24437588453292847\n",
      "[Training Epoch 5] Batch 495, Loss 0.2619631588459015\n",
      "[Training Epoch 5] Batch 496, Loss 0.2491830289363861\n",
      "[Training Epoch 5] Batch 497, Loss 0.2651771306991577\n",
      "[Training Epoch 5] Batch 498, Loss 0.24658611416816711\n",
      "[Training Epoch 5] Batch 499, Loss 0.26473820209503174\n",
      "[Training Epoch 5] Batch 500, Loss 0.2596275806427002\n",
      "[Training Epoch 5] Batch 501, Loss 0.23352937400341034\n",
      "[Training Epoch 5] Batch 502, Loss 0.2584574222564697\n",
      "[Training Epoch 5] Batch 503, Loss 0.28391706943511963\n",
      "[Training Epoch 5] Batch 504, Loss 0.24403493106365204\n",
      "[Training Epoch 5] Batch 505, Loss 0.2570812404155731\n",
      "[Training Epoch 5] Batch 506, Loss 0.2783110737800598\n",
      "[Training Epoch 5] Batch 507, Loss 0.27417218685150146\n",
      "[Training Epoch 5] Batch 508, Loss 0.23821789026260376\n",
      "[Training Epoch 5] Batch 509, Loss 0.2828352451324463\n",
      "[Training Epoch 5] Batch 510, Loss 0.27213215827941895\n",
      "[Training Epoch 5] Batch 511, Loss 0.2544298768043518\n",
      "[Training Epoch 5] Batch 512, Loss 0.2804783880710602\n",
      "[Training Epoch 5] Batch 513, Loss 0.2503628432750702\n",
      "[Training Epoch 5] Batch 514, Loss 0.2906246483325958\n",
      "[Training Epoch 5] Batch 515, Loss 0.22932907938957214\n",
      "[Training Epoch 5] Batch 516, Loss 0.24290446937084198\n",
      "[Training Epoch 5] Batch 517, Loss 0.2545754909515381\n",
      "[Training Epoch 5] Batch 518, Loss 0.24777399003505707\n",
      "[Training Epoch 5] Batch 519, Loss 0.25418615341186523\n",
      "[Training Epoch 5] Batch 520, Loss 0.2563931345939636\n",
      "[Training Epoch 5] Batch 521, Loss 0.26349812746047974\n",
      "[Training Epoch 5] Batch 522, Loss 0.251594603061676\n",
      "[Training Epoch 5] Batch 523, Loss 0.2810474634170532\n",
      "[Training Epoch 5] Batch 524, Loss 0.24532558023929596\n",
      "[Training Epoch 5] Batch 525, Loss 0.2954515218734741\n",
      "[Training Epoch 5] Batch 526, Loss 0.2729722261428833\n",
      "[Training Epoch 5] Batch 527, Loss 0.2777610719203949\n",
      "[Training Epoch 5] Batch 528, Loss 0.2620566487312317\n",
      "[Training Epoch 5] Batch 529, Loss 0.29004037380218506\n",
      "[Training Epoch 5] Batch 530, Loss 0.2295932024717331\n",
      "[Training Epoch 5] Batch 531, Loss 0.2525242567062378\n",
      "[Training Epoch 5] Batch 532, Loss 0.2138168215751648\n",
      "[Training Epoch 5] Batch 533, Loss 0.2690523564815521\n",
      "[Training Epoch 5] Batch 534, Loss 0.27461540699005127\n",
      "[Training Epoch 5] Batch 535, Loss 0.25921115279197693\n",
      "[Training Epoch 5] Batch 536, Loss 0.25499528646469116\n",
      "[Training Epoch 5] Batch 537, Loss 0.24210861325263977\n",
      "[Training Epoch 5] Batch 538, Loss 0.23268640041351318\n",
      "[Training Epoch 5] Batch 539, Loss 0.24675683677196503\n",
      "[Training Epoch 5] Batch 540, Loss 0.26714712381362915\n",
      "[Training Epoch 5] Batch 541, Loss 0.2565953731536865\n",
      "[Training Epoch 5] Batch 542, Loss 0.2882440686225891\n",
      "[Training Epoch 5] Batch 543, Loss 0.24351245164871216\n",
      "[Training Epoch 5] Batch 544, Loss 0.27568942308425903\n",
      "[Training Epoch 5] Batch 545, Loss 0.26021015644073486\n",
      "[Training Epoch 5] Batch 546, Loss 0.2562144696712494\n",
      "[Training Epoch 5] Batch 547, Loss 0.2798619270324707\n",
      "[Training Epoch 5] Batch 548, Loss 0.2382095754146576\n",
      "[Training Epoch 5] Batch 549, Loss 0.28440624475479126\n",
      "[Training Epoch 5] Batch 550, Loss 0.2633170783519745\n",
      "[Training Epoch 5] Batch 551, Loss 0.27393269538879395\n",
      "[Training Epoch 5] Batch 552, Loss 0.2520105540752411\n",
      "[Training Epoch 5] Batch 553, Loss 0.27892887592315674\n",
      "[Training Epoch 5] Batch 554, Loss 0.2820580005645752\n",
      "[Training Epoch 5] Batch 555, Loss 0.2786880433559418\n",
      "[Training Epoch 5] Batch 556, Loss 0.2632617950439453\n",
      "[Training Epoch 5] Batch 557, Loss 0.29020780324935913\n",
      "[Training Epoch 5] Batch 558, Loss 0.271610826253891\n",
      "[Training Epoch 5] Batch 559, Loss 0.2594647705554962\n",
      "[Training Epoch 5] Batch 560, Loss 0.2699434459209442\n",
      "[Training Epoch 5] Batch 561, Loss 0.26760485768318176\n",
      "[Training Epoch 5] Batch 562, Loss 0.25525641441345215\n",
      "[Training Epoch 5] Batch 563, Loss 0.24254131317138672\n",
      "[Training Epoch 5] Batch 564, Loss 0.28740930557250977\n",
      "[Training Epoch 5] Batch 565, Loss 0.26628392934799194\n",
      "[Training Epoch 5] Batch 566, Loss 0.2668501138687134\n",
      "[Training Epoch 5] Batch 567, Loss 0.2627115845680237\n",
      "[Training Epoch 5] Batch 568, Loss 0.28500646352767944\n",
      "[Training Epoch 5] Batch 569, Loss 0.26803672313690186\n",
      "[Training Epoch 5] Batch 570, Loss 0.2612021565437317\n",
      "[Training Epoch 5] Batch 571, Loss 0.2626190185546875\n",
      "[Training Epoch 5] Batch 572, Loss 0.2671681344509125\n",
      "[Training Epoch 5] Batch 573, Loss 0.2614586353302002\n",
      "[Training Epoch 5] Batch 574, Loss 0.26822805404663086\n",
      "[Training Epoch 5] Batch 575, Loss 0.2889043092727661\n",
      "[Training Epoch 5] Batch 576, Loss 0.2804410457611084\n",
      "[Training Epoch 5] Batch 577, Loss 0.27930647134780884\n",
      "[Training Epoch 5] Batch 578, Loss 0.23493431508541107\n",
      "[Training Epoch 5] Batch 579, Loss 0.27949264645576477\n",
      "[Training Epoch 5] Batch 580, Loss 0.25647664070129395\n",
      "[Training Epoch 5] Batch 581, Loss 0.24596571922302246\n",
      "[Training Epoch 5] Batch 582, Loss 0.2692981958389282\n",
      "[Training Epoch 5] Batch 583, Loss 0.23290221393108368\n",
      "[Training Epoch 5] Batch 584, Loss 0.21566054224967957\n",
      "[Training Epoch 5] Batch 585, Loss 0.2533874213695526\n",
      "[Training Epoch 5] Batch 586, Loss 0.2381816804409027\n",
      "[Training Epoch 5] Batch 587, Loss 0.2703169584274292\n",
      "[Training Epoch 5] Batch 588, Loss 0.24822638928890228\n",
      "[Training Epoch 5] Batch 589, Loss 0.27003344893455505\n",
      "[Training Epoch 5] Batch 590, Loss 0.24236831068992615\n",
      "[Training Epoch 5] Batch 591, Loss 0.24568909406661987\n",
      "[Training Epoch 5] Batch 592, Loss 0.2515050172805786\n",
      "[Training Epoch 5] Batch 593, Loss 0.27013036608695984\n",
      "[Training Epoch 5] Batch 594, Loss 0.260296106338501\n",
      "[Training Epoch 5] Batch 595, Loss 0.26308268308639526\n",
      "[Training Epoch 5] Batch 596, Loss 0.2617871165275574\n",
      "[Training Epoch 5] Batch 597, Loss 0.2492786943912506\n",
      "[Training Epoch 5] Batch 598, Loss 0.27870234847068787\n",
      "[Training Epoch 5] Batch 599, Loss 0.27191850543022156\n",
      "[Training Epoch 5] Batch 600, Loss 0.255126416683197\n",
      "[Training Epoch 5] Batch 601, Loss 0.31165754795074463\n",
      "[Training Epoch 5] Batch 602, Loss 0.27449047565460205\n",
      "[Training Epoch 5] Batch 603, Loss 0.2514311671257019\n",
      "[Training Epoch 5] Batch 604, Loss 0.2663751542568207\n",
      "[Training Epoch 5] Batch 605, Loss 0.2922087609767914\n",
      "[Training Epoch 5] Batch 606, Loss 0.2615174949169159\n",
      "[Training Epoch 5] Batch 607, Loss 0.2584613263607025\n",
      "[Training Epoch 5] Batch 608, Loss 0.2429201304912567\n",
      "[Training Epoch 5] Batch 609, Loss 0.22871902585029602\n",
      "[Training Epoch 5] Batch 610, Loss 0.2699436545372009\n",
      "[Training Epoch 5] Batch 611, Loss 0.27192413806915283\n",
      "[Training Epoch 5] Batch 612, Loss 0.2783321738243103\n",
      "[Training Epoch 5] Batch 613, Loss 0.2862800359725952\n",
      "[Training Epoch 5] Batch 614, Loss 0.25641924142837524\n",
      "[Training Epoch 5] Batch 615, Loss 0.2779995799064636\n",
      "[Training Epoch 5] Batch 616, Loss 0.27656328678131104\n",
      "[Training Epoch 5] Batch 617, Loss 0.25162822008132935\n",
      "[Training Epoch 5] Batch 618, Loss 0.2603759765625\n",
      "[Training Epoch 5] Batch 619, Loss 0.27159392833709717\n",
      "[Training Epoch 5] Batch 620, Loss 0.28338685631752014\n",
      "[Training Epoch 5] Batch 621, Loss 0.2840910851955414\n",
      "[Training Epoch 5] Batch 622, Loss 0.25390625\n",
      "[Training Epoch 5] Batch 623, Loss 0.25224077701568604\n",
      "[Training Epoch 5] Batch 624, Loss 0.2765469551086426\n",
      "[Training Epoch 5] Batch 625, Loss 0.24649713933467865\n",
      "[Training Epoch 5] Batch 626, Loss 0.26808834075927734\n",
      "[Training Epoch 5] Batch 627, Loss 0.2803029417991638\n",
      "[Training Epoch 5] Batch 628, Loss 0.23301514983177185\n",
      "[Training Epoch 5] Batch 629, Loss 0.2413122057914734\n",
      "[Training Epoch 5] Batch 630, Loss 0.29370903968811035\n",
      "[Training Epoch 5] Batch 631, Loss 0.2844480276107788\n",
      "[Training Epoch 5] Batch 632, Loss 0.26396945118904114\n",
      "[Training Epoch 5] Batch 633, Loss 0.2584705352783203\n",
      "[Training Epoch 5] Batch 634, Loss 0.24944902956485748\n",
      "[Training Epoch 5] Batch 635, Loss 0.25255119800567627\n",
      "[Training Epoch 5] Batch 636, Loss 0.28549158573150635\n",
      "[Training Epoch 5] Batch 637, Loss 0.26715460419654846\n",
      "[Training Epoch 5] Batch 638, Loss 0.2673375606536865\n",
      "[Training Epoch 5] Batch 639, Loss 0.25185626745224\n",
      "[Training Epoch 5] Batch 640, Loss 0.2495383620262146\n",
      "[Training Epoch 5] Batch 641, Loss 0.268627405166626\n",
      "[Training Epoch 5] Batch 642, Loss 0.2836300730705261\n",
      "[Training Epoch 5] Batch 643, Loss 0.26542168855667114\n",
      "[Training Epoch 5] Batch 644, Loss 0.23966023325920105\n",
      "[Training Epoch 5] Batch 645, Loss 0.2353544980287552\n",
      "[Training Epoch 5] Batch 646, Loss 0.2477525770664215\n",
      "[Training Epoch 5] Batch 647, Loss 0.24963970482349396\n",
      "[Training Epoch 5] Batch 648, Loss 0.26515674591064453\n",
      "[Training Epoch 5] Batch 649, Loss 0.26108866930007935\n",
      "[Training Epoch 5] Batch 650, Loss 0.23679926991462708\n",
      "[Training Epoch 5] Batch 651, Loss 0.26196959614753723\n",
      "[Training Epoch 5] Batch 652, Loss 0.2576053738594055\n",
      "[Training Epoch 5] Batch 653, Loss 0.2788398563861847\n",
      "[Training Epoch 5] Batch 654, Loss 0.2350849062204361\n",
      "[Training Epoch 5] Batch 655, Loss 0.2703329920768738\n",
      "[Training Epoch 5] Batch 656, Loss 0.2642734944820404\n",
      "[Training Epoch 5] Batch 657, Loss 0.2554333508014679\n",
      "[Training Epoch 5] Batch 658, Loss 0.2704704999923706\n",
      "[Training Epoch 5] Batch 659, Loss 0.29332244396209717\n",
      "[Training Epoch 5] Batch 660, Loss 0.2667194604873657\n",
      "[Training Epoch 5] Batch 661, Loss 0.24623572826385498\n",
      "[Training Epoch 5] Batch 662, Loss 0.25529545545578003\n",
      "[Training Epoch 5] Batch 663, Loss 0.28173622488975525\n",
      "[Training Epoch 5] Batch 664, Loss 0.25686168670654297\n",
      "[Training Epoch 5] Batch 665, Loss 0.23914651572704315\n",
      "[Training Epoch 5] Batch 666, Loss 0.2490171194076538\n",
      "[Training Epoch 5] Batch 667, Loss 0.2534099817276001\n",
      "[Training Epoch 5] Batch 668, Loss 0.25705811381340027\n",
      "[Training Epoch 5] Batch 669, Loss 0.2668275833129883\n",
      "[Training Epoch 5] Batch 670, Loss 0.25029781460762024\n",
      "[Training Epoch 5] Batch 671, Loss 0.2826736867427826\n",
      "[Training Epoch 5] Batch 672, Loss 0.2670721113681793\n",
      "[Training Epoch 5] Batch 673, Loss 0.2547304034233093\n",
      "[Training Epoch 5] Batch 674, Loss 0.2656368315219879\n",
      "[Training Epoch 5] Batch 675, Loss 0.28814512491226196\n",
      "[Training Epoch 5] Batch 676, Loss 0.2629737854003906\n",
      "[Training Epoch 5] Batch 677, Loss 0.23463045060634613\n",
      "[Training Epoch 5] Batch 678, Loss 0.2458251416683197\n",
      "[Training Epoch 5] Batch 679, Loss 0.28738465905189514\n",
      "[Training Epoch 5] Batch 680, Loss 0.2473977655172348\n",
      "[Training Epoch 5] Batch 681, Loss 0.25352078676223755\n",
      "[Training Epoch 5] Batch 682, Loss 0.2797635793685913\n",
      "[Training Epoch 5] Batch 683, Loss 0.2511230707168579\n",
      "[Training Epoch 5] Batch 684, Loss 0.241726815700531\n",
      "[Training Epoch 5] Batch 685, Loss 0.2602939009666443\n",
      "[Training Epoch 5] Batch 686, Loss 0.2707369327545166\n",
      "[Training Epoch 5] Batch 687, Loss 0.24074363708496094\n",
      "[Training Epoch 5] Batch 688, Loss 0.24535894393920898\n",
      "[Training Epoch 5] Batch 689, Loss 0.24306678771972656\n",
      "[Training Epoch 5] Batch 690, Loss 0.25350603461265564\n",
      "[Training Epoch 5] Batch 691, Loss 0.3078474998474121\n",
      "[Training Epoch 5] Batch 692, Loss 0.2597350478172302\n",
      "[Training Epoch 5] Batch 693, Loss 0.2702375054359436\n",
      "[Training Epoch 5] Batch 694, Loss 0.23078903555870056\n",
      "[Training Epoch 5] Batch 695, Loss 0.2870263457298279\n",
      "[Training Epoch 5] Batch 696, Loss 0.2602500319480896\n",
      "[Training Epoch 5] Batch 697, Loss 0.2917819619178772\n",
      "[Training Epoch 5] Batch 698, Loss 0.26263904571533203\n",
      "[Training Epoch 5] Batch 699, Loss 0.2430899441242218\n",
      "[Training Epoch 5] Batch 700, Loss 0.24752122163772583\n",
      "[Training Epoch 5] Batch 701, Loss 0.28768786787986755\n",
      "[Training Epoch 5] Batch 702, Loss 0.27676159143447876\n",
      "[Training Epoch 5] Batch 703, Loss 0.26138055324554443\n",
      "[Training Epoch 5] Batch 704, Loss 0.26883673667907715\n",
      "[Training Epoch 5] Batch 705, Loss 0.2448534220457077\n",
      "[Training Epoch 5] Batch 706, Loss 0.2768487334251404\n",
      "[Training Epoch 5] Batch 707, Loss 0.28632932901382446\n",
      "[Training Epoch 5] Batch 708, Loss 0.24412058293819427\n",
      "[Training Epoch 5] Batch 709, Loss 0.25106745958328247\n",
      "[Training Epoch 5] Batch 710, Loss 0.22761094570159912\n",
      "[Training Epoch 5] Batch 711, Loss 0.2639017105102539\n",
      "[Training Epoch 5] Batch 712, Loss 0.2619859576225281\n",
      "[Training Epoch 5] Batch 713, Loss 0.2749784588813782\n",
      "[Training Epoch 5] Batch 714, Loss 0.2779591381549835\n",
      "[Training Epoch 5] Batch 715, Loss 0.2512226700782776\n",
      "[Training Epoch 5] Batch 716, Loss 0.3032991886138916\n",
      "[Training Epoch 5] Batch 717, Loss 0.2484605461359024\n",
      "[Training Epoch 5] Batch 718, Loss 0.24150671064853668\n",
      "[Training Epoch 5] Batch 719, Loss 0.23531237244606018\n",
      "[Training Epoch 5] Batch 720, Loss 0.23647315800189972\n",
      "[Training Epoch 5] Batch 721, Loss 0.2430213987827301\n",
      "[Training Epoch 5] Batch 722, Loss 0.2626921534538269\n",
      "[Training Epoch 5] Batch 723, Loss 0.24779167771339417\n",
      "[Training Epoch 5] Batch 724, Loss 0.2773575782775879\n",
      "[Training Epoch 5] Batch 725, Loss 0.2728109359741211\n",
      "[Training Epoch 5] Batch 726, Loss 0.22895899415016174\n",
      "[Training Epoch 5] Batch 727, Loss 0.24611057341098785\n",
      "[Training Epoch 5] Batch 728, Loss 0.24058613181114197\n",
      "[Training Epoch 5] Batch 729, Loss 0.24511826038360596\n",
      "[Training Epoch 5] Batch 730, Loss 0.2740788757801056\n",
      "[Training Epoch 5] Batch 731, Loss 0.28066548705101013\n",
      "[Training Epoch 5] Batch 732, Loss 0.279890775680542\n",
      "[Training Epoch 5] Batch 733, Loss 0.2529221177101135\n",
      "[Training Epoch 5] Batch 734, Loss 0.24464836716651917\n",
      "[Training Epoch 5] Batch 735, Loss 0.2338150441646576\n",
      "[Training Epoch 5] Batch 736, Loss 0.262107253074646\n",
      "[Training Epoch 5] Batch 737, Loss 0.2762082517147064\n",
      "[Training Epoch 5] Batch 738, Loss 0.27862194180488586\n",
      "[Training Epoch 5] Batch 739, Loss 0.2633480131626129\n",
      "[Training Epoch 5] Batch 740, Loss 0.24538090825080872\n",
      "[Training Epoch 5] Batch 741, Loss 0.26231855154037476\n",
      "[Training Epoch 5] Batch 742, Loss 0.2597617506980896\n",
      "[Training Epoch 5] Batch 743, Loss 0.2589799165725708\n",
      "[Training Epoch 5] Batch 744, Loss 0.243507519364357\n",
      "[Training Epoch 5] Batch 745, Loss 0.27985286712646484\n",
      "[Training Epoch 5] Batch 746, Loss 0.2586839497089386\n",
      "[Training Epoch 5] Batch 747, Loss 0.25714075565338135\n",
      "[Training Epoch 5] Batch 748, Loss 0.26934486627578735\n",
      "[Training Epoch 5] Batch 749, Loss 0.2680438756942749\n",
      "[Training Epoch 5] Batch 750, Loss 0.25924500823020935\n",
      "[Training Epoch 5] Batch 751, Loss 0.23355746269226074\n",
      "[Training Epoch 5] Batch 752, Loss 0.2752210795879364\n",
      "[Training Epoch 5] Batch 753, Loss 0.26153337955474854\n",
      "[Training Epoch 5] Batch 754, Loss 0.2792516052722931\n",
      "[Training Epoch 5] Batch 755, Loss 0.26372769474983215\n",
      "[Training Epoch 5] Batch 756, Loss 0.2627079486846924\n",
      "[Training Epoch 5] Batch 757, Loss 0.2627195119857788\n",
      "[Training Epoch 5] Batch 758, Loss 0.2669414281845093\n",
      "[Training Epoch 5] Batch 759, Loss 0.26043403148651123\n",
      "[Training Epoch 5] Batch 760, Loss 0.2763981819152832\n",
      "[Training Epoch 5] Batch 761, Loss 0.24967603385448456\n",
      "[Training Epoch 5] Batch 762, Loss 0.25653570890426636\n",
      "[Training Epoch 5] Batch 763, Loss 0.2734857201576233\n",
      "[Training Epoch 5] Batch 764, Loss 0.2566602826118469\n",
      "[Training Epoch 5] Batch 765, Loss 0.26883959770202637\n",
      "[Training Epoch 5] Batch 766, Loss 0.2504582107067108\n",
      "[Training Epoch 5] Batch 767, Loss 0.2584325671195984\n",
      "[Training Epoch 5] Batch 768, Loss 0.2710123062133789\n",
      "[Training Epoch 5] Batch 769, Loss 0.2845332622528076\n",
      "[Training Epoch 5] Batch 770, Loss 0.2783258557319641\n",
      "[Training Epoch 5] Batch 771, Loss 0.26461929082870483\n",
      "[Training Epoch 5] Batch 772, Loss 0.24127037823200226\n",
      "[Training Epoch 5] Batch 773, Loss 0.22763016819953918\n",
      "[Training Epoch 5] Batch 774, Loss 0.2757401764392853\n",
      "[Training Epoch 5] Batch 775, Loss 0.27037250995635986\n",
      "[Training Epoch 5] Batch 776, Loss 0.2728928327560425\n",
      "[Training Epoch 5] Batch 777, Loss 0.28125476837158203\n",
      "[Training Epoch 5] Batch 778, Loss 0.2719048857688904\n",
      "[Training Epoch 5] Batch 779, Loss 0.2527529001235962\n",
      "[Training Epoch 5] Batch 780, Loss 0.2413499653339386\n",
      "[Training Epoch 5] Batch 781, Loss 0.2580462694168091\n",
      "[Training Epoch 5] Batch 782, Loss 0.2628280520439148\n",
      "[Training Epoch 5] Batch 783, Loss 0.2523730397224426\n",
      "[Training Epoch 5] Batch 784, Loss 0.24562804400920868\n",
      "[Training Epoch 5] Batch 785, Loss 0.2514386773109436\n",
      "[Training Epoch 5] Batch 786, Loss 0.24775949120521545\n",
      "[Training Epoch 5] Batch 787, Loss 0.2721664607524872\n",
      "[Training Epoch 5] Batch 788, Loss 0.26307153701782227\n",
      "[Training Epoch 5] Batch 789, Loss 0.23089256882667542\n",
      "[Training Epoch 5] Batch 790, Loss 0.2619088888168335\n",
      "[Training Epoch 5] Batch 791, Loss 0.2785663604736328\n",
      "[Training Epoch 5] Batch 792, Loss 0.2894696891307831\n",
      "[Training Epoch 5] Batch 793, Loss 0.25274568796157837\n",
      "[Training Epoch 5] Batch 794, Loss 0.23809687793254852\n",
      "[Training Epoch 5] Batch 795, Loss 0.3013031482696533\n",
      "[Training Epoch 5] Batch 796, Loss 0.28424760699272156\n",
      "[Training Epoch 5] Batch 797, Loss 0.2656504213809967\n",
      "[Training Epoch 5] Batch 798, Loss 0.2577283978462219\n",
      "[Training Epoch 5] Batch 799, Loss 0.27266526222229004\n",
      "[Training Epoch 5] Batch 800, Loss 0.29297417402267456\n",
      "[Training Epoch 5] Batch 801, Loss 0.29290902614593506\n",
      "[Training Epoch 5] Batch 802, Loss 0.26410388946533203\n",
      "[Training Epoch 5] Batch 803, Loss 0.28707295656204224\n",
      "[Training Epoch 5] Batch 804, Loss 0.2574347257614136\n",
      "[Training Epoch 5] Batch 805, Loss 0.2597780227661133\n",
      "[Training Epoch 5] Batch 806, Loss 0.24880576133728027\n",
      "[Training Epoch 5] Batch 807, Loss 0.2595170736312866\n",
      "[Training Epoch 5] Batch 808, Loss 0.29521000385284424\n",
      "[Training Epoch 5] Batch 809, Loss 0.25467872619628906\n",
      "[Training Epoch 5] Batch 810, Loss 0.2438584864139557\n",
      "[Training Epoch 5] Batch 811, Loss 0.2646365165710449\n",
      "[Training Epoch 5] Batch 812, Loss 0.2520044147968292\n",
      "[Training Epoch 5] Batch 813, Loss 0.25727158784866333\n",
      "[Training Epoch 5] Batch 814, Loss 0.25152939558029175\n",
      "[Training Epoch 5] Batch 815, Loss 0.2708088457584381\n",
      "[Training Epoch 5] Batch 816, Loss 0.27991026639938354\n",
      "[Training Epoch 5] Batch 817, Loss 0.2508324384689331\n",
      "[Training Epoch 5] Batch 818, Loss 0.25529545545578003\n",
      "[Training Epoch 5] Batch 819, Loss 0.24961328506469727\n",
      "[Training Epoch 5] Batch 820, Loss 0.25452762842178345\n",
      "[Training Epoch 5] Batch 821, Loss 0.22789491713047028\n",
      "[Training Epoch 5] Batch 822, Loss 0.2596499025821686\n",
      "[Training Epoch 5] Batch 823, Loss 0.2754979431629181\n",
      "[Training Epoch 5] Batch 824, Loss 0.26501229405403137\n",
      "[Training Epoch 5] Batch 825, Loss 0.26920920610427856\n",
      "[Training Epoch 5] Batch 826, Loss 0.28778374195098877\n",
      "[Training Epoch 5] Batch 827, Loss 0.2904514968395233\n",
      "[Training Epoch 5] Batch 828, Loss 0.24167966842651367\n",
      "[Training Epoch 5] Batch 829, Loss 0.24224945902824402\n",
      "[Training Epoch 5] Batch 830, Loss 0.2673878073692322\n",
      "[Training Epoch 5] Batch 831, Loss 0.240439772605896\n",
      "[Training Epoch 5] Batch 832, Loss 0.2580251395702362\n",
      "[Training Epoch 5] Batch 833, Loss 0.25039082765579224\n",
      "[Training Epoch 5] Batch 834, Loss 0.23819780349731445\n",
      "[Training Epoch 5] Batch 835, Loss 0.27364397048950195\n",
      "[Training Epoch 5] Batch 836, Loss 0.2556169033050537\n",
      "[Training Epoch 5] Batch 837, Loss 0.25321951508522034\n",
      "[Training Epoch 5] Batch 838, Loss 0.2521132826805115\n",
      "[Training Epoch 5] Batch 839, Loss 0.24638041853904724\n",
      "[Training Epoch 5] Batch 840, Loss 0.271557092666626\n",
      "[Training Epoch 5] Batch 841, Loss 0.2502676844596863\n",
      "[Training Epoch 5] Batch 842, Loss 0.24101145565509796\n",
      "[Training Epoch 5] Batch 843, Loss 0.26587027311325073\n",
      "[Training Epoch 5] Batch 844, Loss 0.2652474641799927\n",
      "[Training Epoch 5] Batch 845, Loss 0.2641584575176239\n",
      "[Training Epoch 5] Batch 846, Loss 0.2864420711994171\n",
      "[Training Epoch 5] Batch 847, Loss 0.24430757761001587\n",
      "[Training Epoch 5] Batch 848, Loss 0.26792845129966736\n",
      "[Training Epoch 5] Batch 849, Loss 0.26116693019866943\n",
      "[Training Epoch 5] Batch 850, Loss 0.25639480352401733\n",
      "[Training Epoch 5] Batch 851, Loss 0.26170942187309265\n",
      "[Training Epoch 5] Batch 852, Loss 0.28522634506225586\n",
      "[Training Epoch 5] Batch 853, Loss 0.25207439064979553\n",
      "[Training Epoch 5] Batch 854, Loss 0.2875748872756958\n",
      "[Training Epoch 5] Batch 855, Loss 0.29377955198287964\n",
      "[Training Epoch 5] Batch 856, Loss 0.2774299383163452\n",
      "[Training Epoch 5] Batch 857, Loss 0.24627646803855896\n",
      "[Training Epoch 5] Batch 858, Loss 0.2828110456466675\n",
      "[Training Epoch 5] Batch 859, Loss 0.27706465125083923\n",
      "[Training Epoch 5] Batch 860, Loss 0.26146620512008667\n",
      "[Training Epoch 5] Batch 861, Loss 0.25869685411453247\n",
      "[Training Epoch 5] Batch 862, Loss 0.2691643238067627\n",
      "[Training Epoch 5] Batch 863, Loss 0.2773793637752533\n",
      "[Training Epoch 5] Batch 864, Loss 0.26441818475723267\n",
      "[Training Epoch 5] Batch 865, Loss 0.2951032817363739\n",
      "[Training Epoch 5] Batch 866, Loss 0.26104968786239624\n",
      "[Training Epoch 5] Batch 867, Loss 0.2729074954986572\n",
      "[Training Epoch 5] Batch 868, Loss 0.2679130434989929\n",
      "[Training Epoch 5] Batch 869, Loss 0.26780620217323303\n",
      "[Training Epoch 5] Batch 870, Loss 0.2696063220500946\n",
      "[Training Epoch 5] Batch 871, Loss 0.2501411437988281\n",
      "[Training Epoch 5] Batch 872, Loss 0.24364779889583588\n",
      "[Training Epoch 5] Batch 873, Loss 0.2746361494064331\n",
      "[Training Epoch 5] Batch 874, Loss 0.24062582850456238\n",
      "[Training Epoch 5] Batch 875, Loss 0.2580551505088806\n",
      "[Training Epoch 5] Batch 876, Loss 0.261810839176178\n",
      "[Training Epoch 5] Batch 877, Loss 0.23769629001617432\n",
      "[Training Epoch 5] Batch 878, Loss 0.2655852735042572\n",
      "[Training Epoch 5] Batch 879, Loss 0.26788055896759033\n",
      "[Training Epoch 5] Batch 880, Loss 0.2373240888118744\n",
      "[Training Epoch 5] Batch 881, Loss 0.2987654209136963\n",
      "[Training Epoch 5] Batch 882, Loss 0.28277158737182617\n",
      "[Training Epoch 5] Batch 883, Loss 0.2472268044948578\n",
      "[Training Epoch 5] Batch 884, Loss 0.2352321445941925\n",
      "[Training Epoch 5] Batch 885, Loss 0.3107355833053589\n",
      "[Training Epoch 5] Batch 886, Loss 0.2572835087776184\n",
      "[Training Epoch 5] Batch 887, Loss 0.2582181394100189\n",
      "[Training Epoch 5] Batch 888, Loss 0.2747308015823364\n",
      "[Training Epoch 5] Batch 889, Loss 0.26626884937286377\n",
      "[Training Epoch 5] Batch 890, Loss 0.21414797008037567\n",
      "[Training Epoch 5] Batch 891, Loss 0.2662857472896576\n",
      "[Training Epoch 5] Batch 892, Loss 0.28088411688804626\n",
      "[Training Epoch 5] Batch 893, Loss 0.26324695348739624\n",
      "[Training Epoch 5] Batch 894, Loss 0.2424948662519455\n",
      "[Training Epoch 5] Batch 895, Loss 0.25082433223724365\n",
      "[Training Epoch 5] Batch 896, Loss 0.26078176498413086\n",
      "[Training Epoch 5] Batch 897, Loss 0.2749519944190979\n",
      "[Training Epoch 5] Batch 898, Loss 0.26169514656066895\n",
      "[Training Epoch 5] Batch 899, Loss 0.2602880001068115\n",
      "[Training Epoch 5] Batch 900, Loss 0.2723422050476074\n",
      "[Training Epoch 5] Batch 901, Loss 0.2855064272880554\n",
      "[Training Epoch 5] Batch 902, Loss 0.2584644854068756\n",
      "[Training Epoch 5] Batch 903, Loss 0.264285683631897\n",
      "[Training Epoch 5] Batch 904, Loss 0.27394378185272217\n",
      "[Training Epoch 5] Batch 905, Loss 0.28679075837135315\n",
      "[Training Epoch 5] Batch 906, Loss 0.27403029799461365\n",
      "[Training Epoch 5] Batch 907, Loss 0.2480853796005249\n",
      "[Training Epoch 5] Batch 908, Loss 0.23990511894226074\n",
      "[Training Epoch 5] Batch 909, Loss 0.25950562953948975\n",
      "[Training Epoch 5] Batch 910, Loss 0.25933101773262024\n",
      "[Training Epoch 5] Batch 911, Loss 0.2608158588409424\n",
      "[Training Epoch 5] Batch 912, Loss 0.24041400849819183\n",
      "[Training Epoch 5] Batch 913, Loss 0.2599513530731201\n",
      "[Training Epoch 5] Batch 914, Loss 0.23861368000507355\n",
      "[Training Epoch 5] Batch 915, Loss 0.2820279598236084\n",
      "[Training Epoch 5] Batch 916, Loss 0.2555004358291626\n",
      "[Training Epoch 5] Batch 917, Loss 0.2581346333026886\n",
      "[Training Epoch 5] Batch 918, Loss 0.2635733485221863\n",
      "[Training Epoch 5] Batch 919, Loss 0.2837887406349182\n",
      "[Training Epoch 5] Batch 920, Loss 0.25977545976638794\n",
      "[Training Epoch 5] Batch 921, Loss 0.23337234556674957\n",
      "[Training Epoch 5] Batch 922, Loss 0.2695954740047455\n",
      "[Training Epoch 5] Batch 923, Loss 0.24945852160453796\n",
      "[Training Epoch 5] Batch 924, Loss 0.2369687855243683\n",
      "[Training Epoch 5] Batch 925, Loss 0.25560885667800903\n",
      "[Training Epoch 5] Batch 926, Loss 0.2594372630119324\n",
      "[Training Epoch 5] Batch 927, Loss 0.23860976099967957\n",
      "[Training Epoch 5] Batch 928, Loss 0.2722158432006836\n",
      "[Training Epoch 5] Batch 929, Loss 0.2563910484313965\n",
      "[Training Epoch 5] Batch 930, Loss 0.2563760280609131\n",
      "[Training Epoch 5] Batch 931, Loss 0.2568327784538269\n",
      "[Training Epoch 5] Batch 932, Loss 0.27417922019958496\n",
      "[Training Epoch 5] Batch 933, Loss 0.26704907417297363\n",
      "[Training Epoch 5] Batch 934, Loss 0.23436975479125977\n",
      "[Training Epoch 5] Batch 935, Loss 0.25180599093437195\n",
      "[Training Epoch 5] Batch 936, Loss 0.2517560124397278\n",
      "[Training Epoch 5] Batch 937, Loss 0.25599053502082825\n",
      "[Training Epoch 5] Batch 938, Loss 0.25150132179260254\n",
      "[Training Epoch 5] Batch 939, Loss 0.24073797464370728\n",
      "[Training Epoch 5] Batch 940, Loss 0.22806775569915771\n",
      "[Training Epoch 5] Batch 941, Loss 0.2697962522506714\n",
      "[Training Epoch 5] Batch 942, Loss 0.2678455412387848\n",
      "[Training Epoch 5] Batch 943, Loss 0.2651854455471039\n",
      "[Training Epoch 5] Batch 944, Loss 0.2312333583831787\n",
      "[Training Epoch 5] Batch 945, Loss 0.25762099027633667\n",
      "[Training Epoch 5] Batch 946, Loss 0.2642785906791687\n",
      "[Training Epoch 5] Batch 947, Loss 0.23225054144859314\n",
      "[Training Epoch 5] Batch 948, Loss 0.22852203249931335\n",
      "[Training Epoch 5] Batch 949, Loss 0.23241852223873138\n",
      "[Training Epoch 5] Batch 950, Loss 0.2505139112472534\n",
      "[Training Epoch 5] Batch 951, Loss 0.26271313428878784\n",
      "[Training Epoch 5] Batch 952, Loss 0.25409024953842163\n",
      "[Training Epoch 5] Batch 953, Loss 0.2834760546684265\n",
      "[Training Epoch 5] Batch 954, Loss 0.30077117681503296\n",
      "[Training Epoch 5] Batch 955, Loss 0.258603572845459\n",
      "[Training Epoch 5] Batch 956, Loss 0.2443009912967682\n",
      "[Training Epoch 5] Batch 957, Loss 0.27392423152923584\n",
      "[Training Epoch 5] Batch 958, Loss 0.2640899419784546\n",
      "[Training Epoch 5] Batch 959, Loss 0.23016226291656494\n",
      "[Training Epoch 5] Batch 960, Loss 0.2778315544128418\n",
      "[Training Epoch 5] Batch 961, Loss 0.24100364744663239\n",
      "[Training Epoch 5] Batch 962, Loss 0.26870113611221313\n",
      "[Training Epoch 5] Batch 963, Loss 0.28338587284088135\n",
      "[Training Epoch 5] Batch 964, Loss 0.253734827041626\n",
      "[Training Epoch 5] Batch 965, Loss 0.2638469338417053\n",
      "[Training Epoch 5] Batch 966, Loss 0.2864445447921753\n",
      "[Training Epoch 5] Batch 967, Loss 0.260747492313385\n",
      "[Training Epoch 5] Batch 968, Loss 0.2409682422876358\n",
      "[Training Epoch 5] Batch 969, Loss 0.2522926926612854\n",
      "[Training Epoch 5] Batch 970, Loss 0.2665679454803467\n",
      "[Training Epoch 5] Batch 971, Loss 0.2555830776691437\n",
      "[Training Epoch 5] Batch 972, Loss 0.24572722613811493\n",
      "[Training Epoch 5] Batch 973, Loss 0.28709444403648376\n",
      "[Training Epoch 5] Batch 974, Loss 0.23767517507076263\n",
      "[Training Epoch 5] Batch 975, Loss 0.2503349184989929\n",
      "[Training Epoch 5] Batch 976, Loss 0.27032002806663513\n",
      "[Training Epoch 5] Batch 977, Loss 0.2524978518486023\n",
      "[Training Epoch 5] Batch 978, Loss 0.2734602689743042\n",
      "[Training Epoch 5] Batch 979, Loss 0.26438137888908386\n",
      "[Training Epoch 5] Batch 980, Loss 0.25733205676078796\n",
      "[Training Epoch 5] Batch 981, Loss 0.2701435983181\n",
      "[Training Epoch 5] Batch 982, Loss 0.250046968460083\n",
      "[Training Epoch 5] Batch 983, Loss 0.25347232818603516\n",
      "[Training Epoch 5] Batch 984, Loss 0.2467506229877472\n",
      "[Training Epoch 5] Batch 985, Loss 0.2547343969345093\n",
      "[Training Epoch 5] Batch 986, Loss 0.2787685990333557\n",
      "[Training Epoch 5] Batch 987, Loss 0.2576979398727417\n",
      "[Training Epoch 5] Batch 988, Loss 0.26557016372680664\n",
      "[Training Epoch 5] Batch 989, Loss 0.27321815490722656\n",
      "[Training Epoch 5] Batch 990, Loss 0.2754570245742798\n",
      "[Training Epoch 5] Batch 991, Loss 0.265994131565094\n",
      "[Training Epoch 5] Batch 992, Loss 0.275936484336853\n",
      "[Training Epoch 5] Batch 993, Loss 0.25142332911491394\n",
      "[Training Epoch 5] Batch 994, Loss 0.25239065289497375\n",
      "[Training Epoch 5] Batch 995, Loss 0.2461581826210022\n",
      "[Training Epoch 5] Batch 996, Loss 0.2682628631591797\n",
      "[Training Epoch 5] Batch 997, Loss 0.31392139196395874\n",
      "[Training Epoch 5] Batch 998, Loss 0.2647886276245117\n",
      "[Training Epoch 5] Batch 999, Loss 0.2551056742668152\n",
      "[Training Epoch 5] Batch 1000, Loss 0.22652262449264526\n",
      "[Training Epoch 5] Batch 1001, Loss 0.2673455476760864\n",
      "[Training Epoch 5] Batch 1002, Loss 0.2683568000793457\n",
      "[Training Epoch 5] Batch 1003, Loss 0.24703621864318848\n",
      "[Training Epoch 5] Batch 1004, Loss 0.2664144039154053\n",
      "[Training Epoch 5] Batch 1005, Loss 0.25340044498443604\n",
      "[Training Epoch 5] Batch 1006, Loss 0.24469727277755737\n",
      "[Training Epoch 5] Batch 1007, Loss 0.23536920547485352\n",
      "[Training Epoch 5] Batch 1008, Loss 0.2698087990283966\n",
      "[Training Epoch 5] Batch 1009, Loss 0.2518444061279297\n",
      "[Training Epoch 5] Batch 1010, Loss 0.23176908493041992\n",
      "[Training Epoch 5] Batch 1011, Loss 0.24625101685523987\n",
      "[Training Epoch 5] Batch 1012, Loss 0.2749814987182617\n",
      "[Training Epoch 5] Batch 1013, Loss 0.2608305811882019\n",
      "[Training Epoch 5] Batch 1014, Loss 0.24470214545726776\n",
      "[Training Epoch 5] Batch 1015, Loss 0.2628737688064575\n",
      "[Training Epoch 5] Batch 1016, Loss 0.2739410400390625\n",
      "[Training Epoch 5] Batch 1017, Loss 0.26774054765701294\n",
      "[Training Epoch 5] Batch 1018, Loss 0.2834506630897522\n",
      "[Training Epoch 5] Batch 1019, Loss 0.25259801745414734\n",
      "[Training Epoch 5] Batch 1020, Loss 0.27689123153686523\n",
      "[Training Epoch 5] Batch 1021, Loss 0.2748902440071106\n",
      "[Training Epoch 5] Batch 1022, Loss 0.25909480452537537\n",
      "[Training Epoch 5] Batch 1023, Loss 0.26370686292648315\n",
      "[Training Epoch 5] Batch 1024, Loss 0.25785648822784424\n",
      "[Training Epoch 5] Batch 1025, Loss 0.26719892024993896\n",
      "[Training Epoch 5] Batch 1026, Loss 0.25948241353034973\n",
      "[Training Epoch 5] Batch 1027, Loss 0.25325635075569153\n",
      "[Training Epoch 5] Batch 1028, Loss 0.24976769089698792\n",
      "[Training Epoch 5] Batch 1029, Loss 0.290141224861145\n",
      "[Training Epoch 5] Batch 1030, Loss 0.2351396232843399\n",
      "[Training Epoch 5] Batch 1031, Loss 0.2566053569316864\n",
      "[Training Epoch 5] Batch 1032, Loss 0.28521811962127686\n",
      "[Training Epoch 5] Batch 1033, Loss 0.2498631775379181\n",
      "[Training Epoch 5] Batch 1034, Loss 0.2439993917942047\n",
      "[Training Epoch 5] Batch 1035, Loss 0.2543746531009674\n",
      "[Training Epoch 5] Batch 1036, Loss 0.2905914783477783\n",
      "[Training Epoch 5] Batch 1037, Loss 0.24084095656871796\n",
      "[Training Epoch 5] Batch 1038, Loss 0.2653559446334839\n",
      "[Training Epoch 5] Batch 1039, Loss 0.26454564929008484\n",
      "[Training Epoch 5] Batch 1040, Loss 0.27499112486839294\n",
      "[Training Epoch 5] Batch 1041, Loss 0.23692306876182556\n",
      "[Training Epoch 5] Batch 1042, Loss 0.2498728483915329\n",
      "[Training Epoch 5] Batch 1043, Loss 0.27763986587524414\n",
      "[Training Epoch 5] Batch 1044, Loss 0.26209449768066406\n",
      "[Training Epoch 5] Batch 1045, Loss 0.2785572409629822\n",
      "[Training Epoch 5] Batch 1046, Loss 0.2899805009365082\n",
      "[Training Epoch 5] Batch 1047, Loss 0.28389114141464233\n",
      "[Training Epoch 5] Batch 1048, Loss 0.25721967220306396\n",
      "[Training Epoch 5] Batch 1049, Loss 0.24953335523605347\n",
      "[Training Epoch 5] Batch 1050, Loss 0.2566182613372803\n",
      "[Training Epoch 5] Batch 1051, Loss 0.27002960443496704\n",
      "[Training Epoch 5] Batch 1052, Loss 0.2684778571128845\n",
      "[Training Epoch 5] Batch 1053, Loss 0.2565349340438843\n",
      "[Training Epoch 5] Batch 1054, Loss 0.24458357691764832\n",
      "[Training Epoch 5] Batch 1055, Loss 0.260755717754364\n",
      "[Training Epoch 5] Batch 1056, Loss 0.2609926760196686\n",
      "[Training Epoch 5] Batch 1057, Loss 0.24584993720054626\n",
      "[Training Epoch 5] Batch 1058, Loss 0.2505066990852356\n",
      "[Training Epoch 5] Batch 1059, Loss 0.26386088132858276\n",
      "[Training Epoch 5] Batch 1060, Loss 0.24834531545639038\n",
      "[Training Epoch 5] Batch 1061, Loss 0.2882232069969177\n",
      "[Training Epoch 5] Batch 1062, Loss 0.2569243609905243\n",
      "[Training Epoch 5] Batch 1063, Loss 0.23765556514263153\n",
      "[Training Epoch 5] Batch 1064, Loss 0.2594507932662964\n",
      "[Training Epoch 5] Batch 1065, Loss 0.2968286871910095\n",
      "[Training Epoch 5] Batch 1066, Loss 0.28413891792297363\n",
      "[Training Epoch 5] Batch 1067, Loss 0.2944691777229309\n",
      "[Training Epoch 5] Batch 1068, Loss 0.27589014172554016\n",
      "[Training Epoch 5] Batch 1069, Loss 0.2318897396326065\n",
      "[Training Epoch 5] Batch 1070, Loss 0.2456023395061493\n",
      "[Training Epoch 5] Batch 1071, Loss 0.23547303676605225\n",
      "[Training Epoch 5] Batch 1072, Loss 0.25394177436828613\n",
      "[Training Epoch 5] Batch 1073, Loss 0.2814967632293701\n",
      "[Training Epoch 5] Batch 1074, Loss 0.26461169123649597\n",
      "[Training Epoch 5] Batch 1075, Loss 0.2789629399776459\n",
      "[Training Epoch 5] Batch 1076, Loss 0.2596389651298523\n",
      "[Training Epoch 5] Batch 1077, Loss 0.2761414349079132\n",
      "[Training Epoch 5] Batch 1078, Loss 0.22772513329982758\n",
      "[Training Epoch 5] Batch 1079, Loss 0.2776840925216675\n",
      "[Training Epoch 5] Batch 1080, Loss 0.2729131877422333\n",
      "[Training Epoch 5] Batch 1081, Loss 0.24674133956432343\n",
      "[Training Epoch 5] Batch 1082, Loss 0.24921156466007233\n",
      "[Training Epoch 5] Batch 1083, Loss 0.2574082612991333\n",
      "[Training Epoch 5] Batch 1084, Loss 0.2563449740409851\n",
      "[Training Epoch 5] Batch 1085, Loss 0.2345091551542282\n",
      "[Training Epoch 5] Batch 1086, Loss 0.25133413076400757\n",
      "[Training Epoch 5] Batch 1087, Loss 0.27530765533447266\n",
      "[Training Epoch 5] Batch 1088, Loss 0.265072762966156\n",
      "[Training Epoch 5] Batch 1089, Loss 0.2408769279718399\n",
      "[Training Epoch 5] Batch 1090, Loss 0.26654505729675293\n",
      "[Training Epoch 5] Batch 1091, Loss 0.2740093171596527\n",
      "[Training Epoch 5] Batch 1092, Loss 0.2777010202407837\n",
      "[Training Epoch 5] Batch 1093, Loss 0.23848757147789001\n",
      "[Training Epoch 5] Batch 1094, Loss 0.259040892124176\n",
      "[Training Epoch 5] Batch 1095, Loss 0.24047960340976715\n",
      "[Training Epoch 5] Batch 1096, Loss 0.2519790232181549\n",
      "[Training Epoch 5] Batch 1097, Loss 0.28428733348846436\n",
      "[Training Epoch 5] Batch 1098, Loss 0.2419360876083374\n",
      "[Training Epoch 5] Batch 1099, Loss 0.23627790808677673\n",
      "[Training Epoch 5] Batch 1100, Loss 0.25588545203208923\n",
      "[Training Epoch 5] Batch 1101, Loss 0.2717931866645813\n",
      "[Training Epoch 5] Batch 1102, Loss 0.29618823528289795\n",
      "[Training Epoch 5] Batch 1103, Loss 0.26627394556999207\n",
      "[Training Epoch 5] Batch 1104, Loss 0.26105207204818726\n",
      "[Training Epoch 5] Batch 1105, Loss 0.2875005304813385\n",
      "[Training Epoch 5] Batch 1106, Loss 0.2729542851448059\n",
      "[Training Epoch 5] Batch 1107, Loss 0.2561143636703491\n",
      "[Training Epoch 5] Batch 1108, Loss 0.23844751715660095\n",
      "[Training Epoch 5] Batch 1109, Loss 0.26793646812438965\n",
      "[Training Epoch 5] Batch 1110, Loss 0.24257028102874756\n",
      "[Training Epoch 5] Batch 1111, Loss 0.2687035799026489\n",
      "[Training Epoch 5] Batch 1112, Loss 0.2649376094341278\n",
      "[Training Epoch 5] Batch 1113, Loss 0.23788458108901978\n",
      "[Training Epoch 5] Batch 1114, Loss 0.2420474886894226\n",
      "[Training Epoch 5] Batch 1115, Loss 0.2620643079280853\n",
      "[Training Epoch 5] Batch 1116, Loss 0.2562409043312073\n",
      "[Training Epoch 5] Batch 1117, Loss 0.2577087879180908\n",
      "[Training Epoch 5] Batch 1118, Loss 0.2650151550769806\n",
      "[Training Epoch 5] Batch 1119, Loss 0.24120664596557617\n",
      "[Training Epoch 5] Batch 1120, Loss 0.24597102403640747\n",
      "[Training Epoch 5] Batch 1121, Loss 0.24895089864730835\n",
      "[Training Epoch 5] Batch 1122, Loss 0.27271872758865356\n",
      "[Training Epoch 5] Batch 1123, Loss 0.24646791815757751\n",
      "[Training Epoch 5] Batch 1124, Loss 0.27646929025650024\n",
      "[Training Epoch 5] Batch 1125, Loss 0.2740474045276642\n",
      "[Training Epoch 5] Batch 1126, Loss 0.26103949546813965\n",
      "[Training Epoch 5] Batch 1127, Loss 0.2673143744468689\n",
      "[Training Epoch 5] Batch 1128, Loss 0.2654900550842285\n",
      "[Training Epoch 5] Batch 1129, Loss 0.25129109621047974\n",
      "[Training Epoch 5] Batch 1130, Loss 0.2696729898452759\n",
      "[Training Epoch 5] Batch 1131, Loss 0.25404661893844604\n",
      "[Training Epoch 5] Batch 1132, Loss 0.24329730868339539\n",
      "[Training Epoch 5] Batch 1133, Loss 0.26923805475234985\n",
      "[Training Epoch 5] Batch 1134, Loss 0.2321818470954895\n",
      "[Training Epoch 5] Batch 1135, Loss 0.26486191153526306\n",
      "[Training Epoch 5] Batch 1136, Loss 0.26077136397361755\n",
      "[Training Epoch 5] Batch 1137, Loss 0.2405945360660553\n",
      "[Training Epoch 5] Batch 1138, Loss 0.25846776366233826\n",
      "[Training Epoch 5] Batch 1139, Loss 0.2685337960720062\n",
      "[Training Epoch 5] Batch 1140, Loss 0.2431250363588333\n",
      "[Training Epoch 5] Batch 1141, Loss 0.2727646231651306\n",
      "[Training Epoch 5] Batch 1142, Loss 0.24776053428649902\n",
      "[Training Epoch 5] Batch 1143, Loss 0.27088019251823425\n",
      "[Training Epoch 5] Batch 1144, Loss 0.27014368772506714\n",
      "[Training Epoch 5] Batch 1145, Loss 0.2481061816215515\n",
      "[Training Epoch 5] Batch 1146, Loss 0.26203617453575134\n",
      "[Training Epoch 5] Batch 1147, Loss 0.2687027156352997\n",
      "[Training Epoch 5] Batch 1148, Loss 0.2769126892089844\n",
      "[Training Epoch 5] Batch 1149, Loss 0.24321512877941132\n",
      "[Training Epoch 5] Batch 1150, Loss 0.2844037115573883\n",
      "[Training Epoch 5] Batch 1151, Loss 0.2880866527557373\n",
      "[Training Epoch 5] Batch 1152, Loss 0.25421208143234253\n",
      "[Training Epoch 5] Batch 1153, Loss 0.28710120916366577\n",
      "[Training Epoch 5] Batch 1154, Loss 0.26007702946662903\n",
      "[Training Epoch 5] Batch 1155, Loss 0.23638223111629486\n",
      "[Training Epoch 5] Batch 1156, Loss 0.24986740946769714\n",
      "[Training Epoch 5] Batch 1157, Loss 0.25176966190338135\n",
      "[Training Epoch 5] Batch 1158, Loss 0.2601814270019531\n",
      "[Training Epoch 5] Batch 1159, Loss 0.2711772918701172\n",
      "[Training Epoch 5] Batch 1160, Loss 0.2794645428657532\n",
      "[Training Epoch 5] Batch 1161, Loss 0.24767999351024628\n",
      "[Training Epoch 5] Batch 1162, Loss 0.2443903386592865\n",
      "[Training Epoch 5] Batch 1163, Loss 0.25507375597953796\n",
      "[Training Epoch 5] Batch 1164, Loss 0.2700344920158386\n",
      "[Training Epoch 5] Batch 1165, Loss 0.2679256200790405\n",
      "[Training Epoch 5] Batch 1166, Loss 0.24112755060195923\n",
      "[Training Epoch 5] Batch 1167, Loss 0.25097572803497314\n",
      "[Training Epoch 5] Batch 1168, Loss 0.26192158460617065\n",
      "[Training Epoch 5] Batch 1169, Loss 0.2495023012161255\n",
      "[Training Epoch 5] Batch 1170, Loss 0.2515869736671448\n",
      "[Training Epoch 5] Batch 1171, Loss 0.257926344871521\n",
      "[Training Epoch 5] Batch 1172, Loss 0.27425819635391235\n",
      "[Training Epoch 5] Batch 1173, Loss 0.26772773265838623\n",
      "[Training Epoch 5] Batch 1174, Loss 0.28371143341064453\n",
      "[Training Epoch 5] Batch 1175, Loss 0.25529128313064575\n",
      "[Training Epoch 5] Batch 1176, Loss 0.24952827394008636\n",
      "[Training Epoch 5] Batch 1177, Loss 0.2453116476535797\n",
      "[Training Epoch 5] Batch 1178, Loss 0.2615644335746765\n",
      "[Training Epoch 5] Batch 1179, Loss 0.26093918085098267\n",
      "[Training Epoch 5] Batch 1180, Loss 0.24423953890800476\n",
      "[Training Epoch 5] Batch 1181, Loss 0.2478831708431244\n",
      "[Training Epoch 5] Batch 1182, Loss 0.2502346634864807\n",
      "[Training Epoch 5] Batch 1183, Loss 0.2287304848432541\n",
      "[Training Epoch 5] Batch 1184, Loss 0.2501256465911865\n",
      "[Training Epoch 5] Batch 1185, Loss 0.27725017070770264\n",
      "[Training Epoch 5] Batch 1186, Loss 0.26532015204429626\n",
      "[Training Epoch 5] Batch 1187, Loss 0.24949336051940918\n",
      "[Training Epoch 5] Batch 1188, Loss 0.27350616455078125\n",
      "[Training Epoch 5] Batch 1189, Loss 0.2596110701560974\n",
      "[Training Epoch 5] Batch 1190, Loss 0.25407975912094116\n",
      "[Training Epoch 5] Batch 1191, Loss 0.27770549058914185\n",
      "[Training Epoch 5] Batch 1192, Loss 0.265322744846344\n",
      "[Training Epoch 5] Batch 1193, Loss 0.25161364674568176\n",
      "[Training Epoch 5] Batch 1194, Loss 0.25042152404785156\n",
      "[Training Epoch 5] Batch 1195, Loss 0.2598390579223633\n",
      "[Training Epoch 5] Batch 1196, Loss 0.2564679980278015\n",
      "[Training Epoch 5] Batch 1197, Loss 0.2780524492263794\n",
      "[Training Epoch 5] Batch 1198, Loss 0.2542455196380615\n",
      "[Training Epoch 5] Batch 1199, Loss 0.24425595998764038\n",
      "[Training Epoch 5] Batch 1200, Loss 0.2757246792316437\n",
      "[Training Epoch 5] Batch 1201, Loss 0.2580161392688751\n",
      "[Training Epoch 5] Batch 1202, Loss 0.28968846797943115\n",
      "[Training Epoch 5] Batch 1203, Loss 0.257381409406662\n",
      "[Training Epoch 5] Batch 1204, Loss 0.2759980261325836\n",
      "[Training Epoch 5] Batch 1205, Loss 0.2703772187232971\n",
      "[Training Epoch 5] Batch 1206, Loss 0.270876407623291\n",
      "[Training Epoch 5] Batch 1207, Loss 0.2342277467250824\n",
      "[Training Epoch 5] Batch 1208, Loss 0.2622588872909546\n",
      "[Training Epoch 5] Batch 1209, Loss 0.2881004214286804\n",
      "[Training Epoch 5] Batch 1210, Loss 0.2430473268032074\n",
      "[Training Epoch 5] Batch 1211, Loss 0.25526827573776245\n",
      "[Training Epoch 5] Batch 1212, Loss 0.2519412636756897\n",
      "[Training Epoch 5] Batch 1213, Loss 0.24275726079940796\n",
      "[Training Epoch 5] Batch 1214, Loss 0.25599873065948486\n",
      "[Training Epoch 5] Batch 1215, Loss 0.22735396027565002\n",
      "[Training Epoch 5] Batch 1216, Loss 0.2711557149887085\n",
      "[Training Epoch 5] Batch 1217, Loss 0.26608705520629883\n",
      "[Training Epoch 5] Batch 1218, Loss 0.25533944368362427\n",
      "[Training Epoch 5] Batch 1219, Loss 0.23354752361774445\n",
      "[Training Epoch 5] Batch 1220, Loss 0.23950383067131042\n",
      "[Training Epoch 5] Batch 1221, Loss 0.29806309938430786\n",
      "[Training Epoch 5] Batch 1222, Loss 0.2889133095741272\n",
      "[Training Epoch 5] Batch 1223, Loss 0.2935304045677185\n",
      "[Training Epoch 5] Batch 1224, Loss 0.25219178199768066\n",
      "[Training Epoch 5] Batch 1225, Loss 0.2542712092399597\n",
      "[Training Epoch 5] Batch 1226, Loss 0.2453310787677765\n",
      "[Training Epoch 5] Batch 1227, Loss 0.27087849378585815\n",
      "[Training Epoch 5] Batch 1228, Loss 0.2702521085739136\n",
      "[Training Epoch 5] Batch 1229, Loss 0.2614552676677704\n",
      "[Training Epoch 5] Batch 1230, Loss 0.23727236688137054\n",
      "[Training Epoch 5] Batch 1231, Loss 0.2754519283771515\n",
      "[Training Epoch 5] Batch 1232, Loss 0.2595762610435486\n",
      "[Training Epoch 5] Batch 1233, Loss 0.2662868797779083\n",
      "[Training Epoch 5] Batch 1234, Loss 0.2680874466896057\n",
      "[Training Epoch 5] Batch 1235, Loss 0.2552701234817505\n",
      "[Training Epoch 5] Batch 1236, Loss 0.2667081356048584\n",
      "[Training Epoch 5] Batch 1237, Loss 0.274109810590744\n",
      "[Training Epoch 5] Batch 1238, Loss 0.26670730113983154\n",
      "[Training Epoch 5] Batch 1239, Loss 0.2704941928386688\n",
      "[Training Epoch 5] Batch 1240, Loss 0.27784478664398193\n",
      "[Training Epoch 5] Batch 1241, Loss 0.2637348771095276\n",
      "[Training Epoch 5] Batch 1242, Loss 0.2533433437347412\n",
      "[Training Epoch 5] Batch 1243, Loss 0.24876482784748077\n",
      "[Training Epoch 5] Batch 1244, Loss 0.26166167855262756\n",
      "[Training Epoch 5] Batch 1245, Loss 0.2688893973827362\n",
      "[Training Epoch 5] Batch 1246, Loss 0.2790302038192749\n",
      "[Training Epoch 5] Batch 1247, Loss 0.26847153902053833\n",
      "[Training Epoch 5] Batch 1248, Loss 0.2462500035762787\n",
      "[Training Epoch 5] Batch 1249, Loss 0.25915491580963135\n",
      "[Training Epoch 5] Batch 1250, Loss 0.27308207750320435\n",
      "[Training Epoch 5] Batch 1251, Loss 0.23255643248558044\n",
      "[Training Epoch 5] Batch 1252, Loss 0.2678598165512085\n",
      "[Training Epoch 5] Batch 1253, Loss 0.26459550857543945\n",
      "[Training Epoch 5] Batch 1254, Loss 0.24607251584529877\n",
      "[Training Epoch 5] Batch 1255, Loss 0.25659868121147156\n",
      "[Training Epoch 5] Batch 1256, Loss 0.26124632358551025\n",
      "[Training Epoch 5] Batch 1257, Loss 0.2672932744026184\n",
      "[Training Epoch 5] Batch 1258, Loss 0.2627025842666626\n",
      "[Training Epoch 5] Batch 1259, Loss 0.2832452356815338\n",
      "[Training Epoch 5] Batch 1260, Loss 0.2521507740020752\n",
      "[Training Epoch 5] Batch 1261, Loss 0.26543474197387695\n",
      "[Training Epoch 5] Batch 1262, Loss 0.25021183490753174\n",
      "[Training Epoch 5] Batch 1263, Loss 0.2730041742324829\n",
      "[Training Epoch 5] Batch 1264, Loss 0.2718938887119293\n",
      "[Training Epoch 5] Batch 1265, Loss 0.23928071558475494\n",
      "[Training Epoch 5] Batch 1266, Loss 0.24565601348876953\n",
      "[Training Epoch 5] Batch 1267, Loss 0.2506820857524872\n",
      "[Training Epoch 5] Batch 1268, Loss 0.2631155252456665\n",
      "[Training Epoch 5] Batch 1269, Loss 0.22084613144397736\n",
      "[Training Epoch 5] Batch 1270, Loss 0.226765438914299\n",
      "[Training Epoch 5] Batch 1271, Loss 0.2684287428855896\n",
      "[Training Epoch 5] Batch 1272, Loss 0.2610320448875427\n",
      "[Training Epoch 5] Batch 1273, Loss 0.26832377910614014\n",
      "[Training Epoch 5] Batch 1274, Loss 0.2517520785331726\n",
      "[Training Epoch 5] Batch 1275, Loss 0.26080122590065\n",
      "[Training Epoch 5] Batch 1276, Loss 0.2571161389350891\n",
      "[Training Epoch 5] Batch 1277, Loss 0.28994977474212646\n",
      "[Training Epoch 5] Batch 1278, Loss 0.24522282183170319\n",
      "[Training Epoch 5] Batch 1279, Loss 0.2725342810153961\n",
      "[Training Epoch 5] Batch 1280, Loss 0.2668478488922119\n",
      "[Training Epoch 5] Batch 1281, Loss 0.2835361361503601\n",
      "[Training Epoch 5] Batch 1282, Loss 0.23237068951129913\n",
      "[Training Epoch 5] Batch 1283, Loss 0.28371119499206543\n",
      "[Training Epoch 5] Batch 1284, Loss 0.28092435002326965\n",
      "[Training Epoch 5] Batch 1285, Loss 0.2671588957309723\n",
      "[Training Epoch 5] Batch 1286, Loss 0.2416817843914032\n",
      "[Training Epoch 5] Batch 1287, Loss 0.25322622060775757\n",
      "[Training Epoch 5] Batch 1288, Loss 0.27881234884262085\n",
      "[Training Epoch 5] Batch 1289, Loss 0.24519075453281403\n",
      "[Training Epoch 5] Batch 1290, Loss 0.25121819972991943\n",
      "[Training Epoch 5] Batch 1291, Loss 0.27966105937957764\n",
      "[Training Epoch 5] Batch 1292, Loss 0.2870330810546875\n",
      "[Training Epoch 5] Batch 1293, Loss 0.22933602333068848\n",
      "[Training Epoch 5] Batch 1294, Loss 0.27637410163879395\n",
      "[Training Epoch 5] Batch 1295, Loss 0.2458285391330719\n",
      "[Training Epoch 5] Batch 1296, Loss 0.2571629285812378\n",
      "[Training Epoch 5] Batch 1297, Loss 0.24223193526268005\n",
      "[Training Epoch 5] Batch 1298, Loss 0.270880788564682\n",
      "[Training Epoch 5] Batch 1299, Loss 0.2888701558113098\n",
      "[Training Epoch 5] Batch 1300, Loss 0.239900603890419\n",
      "[Training Epoch 5] Batch 1301, Loss 0.24448585510253906\n",
      "[Training Epoch 5] Batch 1302, Loss 0.23323354125022888\n",
      "[Training Epoch 5] Batch 1303, Loss 0.29415762424468994\n",
      "[Training Epoch 5] Batch 1304, Loss 0.2392110973596573\n",
      "[Training Epoch 5] Batch 1305, Loss 0.2851025462150574\n",
      "[Training Epoch 5] Batch 1306, Loss 0.2676568031311035\n",
      "[Training Epoch 5] Batch 1307, Loss 0.247508704662323\n",
      "[Training Epoch 5] Batch 1308, Loss 0.26158177852630615\n",
      "[Training Epoch 5] Batch 1309, Loss 0.22053708136081696\n",
      "[Training Epoch 5] Batch 1310, Loss 0.2592034637928009\n",
      "[Training Epoch 5] Batch 1311, Loss 0.26843172311782837\n",
      "[Training Epoch 5] Batch 1312, Loss 0.263995498418808\n",
      "[Training Epoch 5] Batch 1313, Loss 0.2393665313720703\n",
      "[Training Epoch 5] Batch 1314, Loss 0.24868783354759216\n",
      "[Training Epoch 5] Batch 1315, Loss 0.27955353260040283\n",
      "[Training Epoch 5] Batch 1316, Loss 0.2543776333332062\n",
      "[Training Epoch 5] Batch 1317, Loss 0.22810709476470947\n",
      "[Training Epoch 5] Batch 1318, Loss 0.26981255412101746\n",
      "[Training Epoch 5] Batch 1319, Loss 0.29269930720329285\n",
      "[Training Epoch 5] Batch 1320, Loss 0.2692764699459076\n",
      "[Training Epoch 5] Batch 1321, Loss 0.2995471954345703\n",
      "[Training Epoch 5] Batch 1322, Loss 0.2757582366466522\n",
      "[Training Epoch 5] Batch 1323, Loss 0.25202232599258423\n",
      "[Training Epoch 5] Batch 1324, Loss 0.26042747497558594\n",
      "[Training Epoch 5] Batch 1325, Loss 0.2639826536178589\n",
      "[Training Epoch 5] Batch 1326, Loss 0.27714061737060547\n",
      "[Training Epoch 5] Batch 1327, Loss 0.2891855537891388\n",
      "[Training Epoch 5] Batch 1328, Loss 0.25895339250564575\n",
      "[Training Epoch 5] Batch 1329, Loss 0.2983298897743225\n",
      "[Training Epoch 5] Batch 1330, Loss 0.2866615056991577\n",
      "[Training Epoch 5] Batch 1331, Loss 0.2665710747241974\n",
      "[Training Epoch 5] Batch 1332, Loss 0.25726428627967834\n",
      "[Training Epoch 5] Batch 1333, Loss 0.30020731687545776\n",
      "[Training Epoch 5] Batch 1334, Loss 0.25810739398002625\n",
      "[Training Epoch 5] Batch 1335, Loss 0.28019481897354126\n",
      "[Training Epoch 5] Batch 1336, Loss 0.2709519863128662\n",
      "[Training Epoch 5] Batch 1337, Loss 0.24515733122825623\n",
      "[Training Epoch 5] Batch 1338, Loss 0.25566980242729187\n",
      "[Training Epoch 5] Batch 1339, Loss 0.24665427207946777\n",
      "[Training Epoch 5] Batch 1340, Loss 0.26984158158302307\n",
      "[Training Epoch 5] Batch 1341, Loss 0.2670135796070099\n",
      "[Training Epoch 5] Batch 1342, Loss 0.2685956358909607\n",
      "[Training Epoch 5] Batch 1343, Loss 0.25840824842453003\n",
      "[Training Epoch 5] Batch 1344, Loss 0.2518044412136078\n",
      "[Training Epoch 5] Batch 1345, Loss 0.251729279756546\n",
      "[Training Epoch 5] Batch 1346, Loss 0.2407032698392868\n",
      "[Training Epoch 5] Batch 1347, Loss 0.2632179856300354\n",
      "[Training Epoch 5] Batch 1348, Loss 0.2567574381828308\n",
      "[Training Epoch 5] Batch 1349, Loss 0.29799148440361023\n",
      "[Training Epoch 5] Batch 1350, Loss 0.2264437973499298\n",
      "[Training Epoch 5] Batch 1351, Loss 0.2849254012107849\n",
      "[Training Epoch 5] Batch 1352, Loss 0.2667718231678009\n",
      "[Training Epoch 5] Batch 1353, Loss 0.2627585530281067\n",
      "[Training Epoch 5] Batch 1354, Loss 0.2806716561317444\n",
      "[Training Epoch 5] Batch 1355, Loss 0.282593697309494\n",
      "[Training Epoch 5] Batch 1356, Loss 0.27331864833831787\n",
      "[Training Epoch 5] Batch 1357, Loss 0.26102742552757263\n",
      "[Training Epoch 5] Batch 1358, Loss 0.23935022950172424\n",
      "[Training Epoch 5] Batch 1359, Loss 0.2576996088027954\n",
      "[Training Epoch 5] Batch 1360, Loss 0.2786768078804016\n",
      "[Training Epoch 5] Batch 1361, Loss 0.2520124614238739\n",
      "[Training Epoch 5] Batch 1362, Loss 0.24719612300395966\n",
      "[Training Epoch 5] Batch 1363, Loss 0.2569676637649536\n",
      "[Training Epoch 5] Batch 1364, Loss 0.27074864506721497\n",
      "[Training Epoch 5] Batch 1365, Loss 0.28286224603652954\n",
      "[Training Epoch 5] Batch 1366, Loss 0.2748074531555176\n",
      "[Training Epoch 5] Batch 1367, Loss 0.29337024688720703\n",
      "[Training Epoch 5] Batch 1368, Loss 0.2591707408428192\n",
      "[Training Epoch 5] Batch 1369, Loss 0.2541101276874542\n",
      "[Training Epoch 5] Batch 1370, Loss 0.2299351841211319\n",
      "[Training Epoch 5] Batch 1371, Loss 0.23154118657112122\n",
      "[Training Epoch 5] Batch 1372, Loss 0.27763622999191284\n",
      "[Training Epoch 5] Batch 1373, Loss 0.28544801473617554\n",
      "[Training Epoch 5] Batch 1374, Loss 0.2855037450790405\n",
      "[Training Epoch 5] Batch 1375, Loss 0.27438977360725403\n",
      "[Training Epoch 5] Batch 1376, Loss 0.2566814124584198\n",
      "[Training Epoch 5] Batch 1377, Loss 0.23987305164337158\n",
      "[Training Epoch 5] Batch 1378, Loss 0.2396649569272995\n",
      "[Training Epoch 5] Batch 1379, Loss 0.26464927196502686\n",
      "[Training Epoch 5] Batch 1380, Loss 0.22524575889110565\n",
      "[Training Epoch 5] Batch 1381, Loss 0.23410268127918243\n",
      "[Training Epoch 5] Batch 1382, Loss 0.25229132175445557\n",
      "[Training Epoch 5] Batch 1383, Loss 0.25071394443511963\n",
      "[Training Epoch 5] Batch 1384, Loss 0.2501508891582489\n",
      "[Training Epoch 5] Batch 1385, Loss 0.26306700706481934\n",
      "[Training Epoch 5] Batch 1386, Loss 0.2676458954811096\n",
      "[Training Epoch 5] Batch 1387, Loss 0.2550047039985657\n",
      "[Training Epoch 5] Batch 1388, Loss 0.26355284452438354\n",
      "[Training Epoch 5] Batch 1389, Loss 0.2494129240512848\n",
      "[Training Epoch 5] Batch 1390, Loss 0.26150834560394287\n",
      "[Training Epoch 5] Batch 1391, Loss 0.2522258460521698\n",
      "[Training Epoch 5] Batch 1392, Loss 0.27599239349365234\n",
      "[Training Epoch 5] Batch 1393, Loss 0.24681635200977325\n",
      "[Training Epoch 5] Batch 1394, Loss 0.28083640336990356\n",
      "[Training Epoch 5] Batch 1395, Loss 0.2418418824672699\n",
      "[Training Epoch 5] Batch 1396, Loss 0.25902479887008667\n",
      "[Training Epoch 5] Batch 1397, Loss 0.28637781739234924\n",
      "[Training Epoch 5] Batch 1398, Loss 0.26582181453704834\n",
      "[Training Epoch 5] Batch 1399, Loss 0.2557528018951416\n",
      "[Training Epoch 5] Batch 1400, Loss 0.22526785731315613\n",
      "[Training Epoch 5] Batch 1401, Loss 0.26996225118637085\n",
      "[Training Epoch 5] Batch 1402, Loss 0.22944672405719757\n",
      "[Training Epoch 5] Batch 1403, Loss 0.289689302444458\n",
      "[Training Epoch 5] Batch 1404, Loss 0.2494061291217804\n",
      "[Training Epoch 5] Batch 1405, Loss 0.22470209002494812\n",
      "[Training Epoch 5] Batch 1406, Loss 0.28595876693725586\n",
      "[Training Epoch 5] Batch 1407, Loss 0.2796570658683777\n",
      "[Training Epoch 5] Batch 1408, Loss 0.252941370010376\n",
      "[Training Epoch 5] Batch 1409, Loss 0.26801854372024536\n",
      "[Training Epoch 5] Batch 1410, Loss 0.26241788268089294\n",
      "[Training Epoch 5] Batch 1411, Loss 0.2703218460083008\n",
      "[Training Epoch 5] Batch 1412, Loss 0.2543184757232666\n",
      "[Training Epoch 5] Batch 1413, Loss 0.2549314796924591\n",
      "[Training Epoch 5] Batch 1414, Loss 0.2414160519838333\n",
      "[Training Epoch 5] Batch 1415, Loss 0.27244332432746887\n",
      "[Training Epoch 5] Batch 1416, Loss 0.2460460066795349\n",
      "[Training Epoch 5] Batch 1417, Loss 0.2638113498687744\n",
      "[Training Epoch 5] Batch 1418, Loss 0.21821144223213196\n",
      "[Training Epoch 5] Batch 1419, Loss 0.2756091058254242\n",
      "[Training Epoch 5] Batch 1420, Loss 0.2618107497692108\n",
      "[Training Epoch 5] Batch 1421, Loss 0.24728256464004517\n",
      "[Training Epoch 5] Batch 1422, Loss 0.26079225540161133\n",
      "[Training Epoch 5] Batch 1423, Loss 0.2403087615966797\n",
      "[Training Epoch 5] Batch 1424, Loss 0.2622724771499634\n",
      "[Training Epoch 5] Batch 1425, Loss 0.28978461027145386\n",
      "[Training Epoch 5] Batch 1426, Loss 0.24811673164367676\n",
      "[Training Epoch 5] Batch 1427, Loss 0.2670799493789673\n",
      "[Training Epoch 5] Batch 1428, Loss 0.2652431130409241\n",
      "[Training Epoch 5] Batch 1429, Loss 0.2710452675819397\n",
      "[Training Epoch 5] Batch 1430, Loss 0.28216829895973206\n",
      "[Training Epoch 5] Batch 1431, Loss 0.2636883854866028\n",
      "[Training Epoch 5] Batch 1432, Loss 0.25123369693756104\n",
      "[Training Epoch 5] Batch 1433, Loss 0.23709575831890106\n",
      "[Training Epoch 5] Batch 1434, Loss 0.2661650478839874\n",
      "[Training Epoch 5] Batch 1435, Loss 0.2719894051551819\n",
      "[Training Epoch 5] Batch 1436, Loss 0.283306360244751\n",
      "[Training Epoch 5] Batch 1437, Loss 0.2400462031364441\n",
      "[Training Epoch 5] Batch 1438, Loss 0.2629741430282593\n",
      "[Training Epoch 5] Batch 1439, Loss 0.270527184009552\n",
      "[Training Epoch 5] Batch 1440, Loss 0.28118348121643066\n",
      "[Training Epoch 5] Batch 1441, Loss 0.25871115922927856\n",
      "[Training Epoch 5] Batch 1442, Loss 0.24801760911941528\n",
      "[Training Epoch 5] Batch 1443, Loss 0.25588715076446533\n",
      "[Training Epoch 5] Batch 1444, Loss 0.244502454996109\n",
      "[Training Epoch 5] Batch 1445, Loss 0.2617063820362091\n",
      "[Training Epoch 5] Batch 1446, Loss 0.25951898097991943\n",
      "[Training Epoch 5] Batch 1447, Loss 0.2905295491218567\n",
      "[Training Epoch 5] Batch 1448, Loss 0.25270336866378784\n",
      "[Training Epoch 5] Batch 1449, Loss 0.2152816504240036\n",
      "[Training Epoch 5] Batch 1450, Loss 0.25756001472473145\n",
      "[Training Epoch 5] Batch 1451, Loss 0.2434367537498474\n",
      "[Training Epoch 5] Batch 1452, Loss 0.23635876178741455\n",
      "[Training Epoch 5] Batch 1453, Loss 0.2342347502708435\n",
      "[Training Epoch 5] Batch 1454, Loss 0.28826138377189636\n",
      "[Training Epoch 5] Batch 1455, Loss 0.24447767436504364\n",
      "[Training Epoch 5] Batch 1456, Loss 0.26100242137908936\n",
      "[Training Epoch 5] Batch 1457, Loss 0.246792733669281\n",
      "[Training Epoch 5] Batch 1458, Loss 0.3085600733757019\n",
      "[Training Epoch 5] Batch 1459, Loss 0.27439725399017334\n",
      "[Training Epoch 5] Batch 1460, Loss 0.26241838932037354\n",
      "[Training Epoch 5] Batch 1461, Loss 0.2742529511451721\n",
      "[Training Epoch 5] Batch 1462, Loss 0.25571614503860474\n",
      "[Training Epoch 5] Batch 1463, Loss 0.26812314987182617\n",
      "[Training Epoch 5] Batch 1464, Loss 0.23843669891357422\n",
      "[Training Epoch 5] Batch 1465, Loss 0.25977063179016113\n",
      "[Training Epoch 5] Batch 1466, Loss 0.25898680090904236\n",
      "[Training Epoch 5] Batch 1467, Loss 0.22791697084903717\n",
      "[Training Epoch 5] Batch 1468, Loss 0.2338126301765442\n",
      "[Training Epoch 5] Batch 1469, Loss 0.26142850518226624\n",
      "[Training Epoch 5] Batch 1470, Loss 0.27326351404190063\n",
      "[Training Epoch 5] Batch 1471, Loss 0.28487029671669006\n",
      "[Training Epoch 5] Batch 1472, Loss 0.2724904716014862\n",
      "[Training Epoch 5] Batch 1473, Loss 0.29008015990257263\n",
      "[Training Epoch 5] Batch 1474, Loss 0.25057098269462585\n",
      "[Training Epoch 5] Batch 1475, Loss 0.2563869059085846\n",
      "[Training Epoch 5] Batch 1476, Loss 0.2508131265640259\n",
      "[Training Epoch 5] Batch 1477, Loss 0.24816033244132996\n",
      "[Training Epoch 5] Batch 1478, Loss 0.28568074107170105\n",
      "[Training Epoch 5] Batch 1479, Loss 0.28052493929862976\n",
      "[Training Epoch 5] Batch 1480, Loss 0.26106780767440796\n",
      "[Training Epoch 5] Batch 1481, Loss 0.2539992928504944\n",
      "[Training Epoch 5] Batch 1482, Loss 0.2867293953895569\n",
      "[Training Epoch 5] Batch 1483, Loss 0.268486350774765\n",
      "[Training Epoch 5] Batch 1484, Loss 0.25644102692604065\n",
      "[Training Epoch 5] Batch 1485, Loss 0.29140597581863403\n",
      "[Training Epoch 5] Batch 1486, Loss 0.2570863664150238\n",
      "[Training Epoch 5] Batch 1487, Loss 0.2518777549266815\n",
      "[Training Epoch 5] Batch 1488, Loss 0.2415734827518463\n",
      "[Training Epoch 5] Batch 1489, Loss 0.28551048040390015\n",
      "[Training Epoch 5] Batch 1490, Loss 0.27457720041275024\n",
      "[Training Epoch 5] Batch 1491, Loss 0.22696644067764282\n",
      "[Training Epoch 5] Batch 1492, Loss 0.2581216096878052\n",
      "[Training Epoch 5] Batch 1493, Loss 0.2696324586868286\n",
      "[Training Epoch 5] Batch 1494, Loss 0.3072262704372406\n",
      "[Training Epoch 5] Batch 1495, Loss 0.2823752462863922\n",
      "[Training Epoch 5] Batch 1496, Loss 0.23608583211898804\n",
      "[Training Epoch 5] Batch 1497, Loss 0.25095635652542114\n",
      "[Training Epoch 5] Batch 1498, Loss 0.25847142934799194\n",
      "[Training Epoch 5] Batch 1499, Loss 0.28249555826187134\n",
      "[Training Epoch 5] Batch 1500, Loss 0.2766111493110657\n",
      "[Training Epoch 5] Batch 1501, Loss 0.2793814539909363\n",
      "[Training Epoch 5] Batch 1502, Loss 0.23693063855171204\n",
      "[Training Epoch 5] Batch 1503, Loss 0.2979814112186432\n",
      "[Training Epoch 5] Batch 1504, Loss 0.2630329728126526\n",
      "[Training Epoch 5] Batch 1505, Loss 0.2581605315208435\n",
      "[Training Epoch 5] Batch 1506, Loss 0.26078373193740845\n",
      "[Training Epoch 5] Batch 1507, Loss 0.25225260853767395\n",
      "[Training Epoch 5] Batch 1508, Loss 0.26574552059173584\n",
      "[Training Epoch 5] Batch 1509, Loss 0.2570338845252991\n",
      "[Training Epoch 5] Batch 1510, Loss 0.2706972062587738\n",
      "[Training Epoch 5] Batch 1511, Loss 0.26125574111938477\n",
      "[Training Epoch 5] Batch 1512, Loss 0.2502899169921875\n",
      "[Training Epoch 5] Batch 1513, Loss 0.2785681486129761\n",
      "[Training Epoch 5] Batch 1514, Loss 0.2744125723838806\n",
      "[Training Epoch 5] Batch 1515, Loss 0.2501952052116394\n",
      "[Training Epoch 5] Batch 1516, Loss 0.2738203704357147\n",
      "[Training Epoch 5] Batch 1517, Loss 0.27998781204223633\n",
      "[Training Epoch 5] Batch 1518, Loss 0.26362812519073486\n",
      "[Training Epoch 5] Batch 1519, Loss 0.2977033257484436\n",
      "[Training Epoch 5] Batch 1520, Loss 0.25349336862564087\n",
      "[Training Epoch 5] Batch 1521, Loss 0.24575775861740112\n",
      "[Training Epoch 5] Batch 1522, Loss 0.2507496178150177\n",
      "[Training Epoch 5] Batch 1523, Loss 0.26363274455070496\n",
      "[Training Epoch 5] Batch 1524, Loss 0.25292831659317017\n",
      "[Training Epoch 5] Batch 1525, Loss 0.2879723608493805\n",
      "[Training Epoch 5] Batch 1526, Loss 0.2846987247467041\n",
      "[Training Epoch 5] Batch 1527, Loss 0.24661017954349518\n",
      "[Training Epoch 5] Batch 1528, Loss 0.27256646752357483\n",
      "[Training Epoch 5] Batch 1529, Loss 0.25342631340026855\n",
      "[Training Epoch 5] Batch 1530, Loss 0.2513238191604614\n",
      "[Training Epoch 5] Batch 1531, Loss 0.24979840219020844\n",
      "[Training Epoch 5] Batch 1532, Loss 0.2651498317718506\n",
      "[Training Epoch 5] Batch 1533, Loss 0.25958800315856934\n",
      "[Training Epoch 5] Batch 1534, Loss 0.26802685856819153\n",
      "[Training Epoch 5] Batch 1535, Loss 0.25300222635269165\n",
      "[Training Epoch 5] Batch 1536, Loss 0.2789604663848877\n",
      "[Training Epoch 5] Batch 1537, Loss 0.24703514575958252\n",
      "[Training Epoch 5] Batch 1538, Loss 0.25556495785713196\n",
      "[Training Epoch 5] Batch 1539, Loss 0.26008015871047974\n",
      "[Training Epoch 5] Batch 1540, Loss 0.2917989492416382\n",
      "[Training Epoch 5] Batch 1541, Loss 0.2375674992799759\n",
      "[Training Epoch 5] Batch 1542, Loss 0.26351168751716614\n",
      "[Training Epoch 5] Batch 1543, Loss 0.2691206932067871\n",
      "[Training Epoch 5] Batch 1544, Loss 0.2503824234008789\n",
      "[Training Epoch 5] Batch 1545, Loss 0.26611602306365967\n",
      "[Training Epoch 5] Batch 1546, Loss 0.2723260521888733\n",
      "[Training Epoch 5] Batch 1547, Loss 0.2874622344970703\n",
      "[Training Epoch 5] Batch 1548, Loss 0.2566853165626526\n",
      "[Training Epoch 5] Batch 1549, Loss 0.25992196798324585\n",
      "[Training Epoch 5] Batch 1550, Loss 0.2554281949996948\n",
      "[Training Epoch 5] Batch 1551, Loss 0.2836030423641205\n",
      "[Training Epoch 5] Batch 1552, Loss 0.23761089146137238\n",
      "[Training Epoch 5] Batch 1553, Loss 0.26219281554222107\n",
      "[Training Epoch 5] Batch 1554, Loss 0.25914472341537476\n",
      "[Training Epoch 5] Batch 1555, Loss 0.2697811722755432\n",
      "[Training Epoch 5] Batch 1556, Loss 0.2697083055973053\n",
      "[Training Epoch 5] Batch 1557, Loss 0.2621171176433563\n",
      "[Training Epoch 5] Batch 1558, Loss 0.25686338543891907\n",
      "[Training Epoch 5] Batch 1559, Loss 0.2677469253540039\n",
      "[Training Epoch 5] Batch 1560, Loss 0.2669810652732849\n",
      "[Training Epoch 5] Batch 1561, Loss 0.2723844349384308\n",
      "[Training Epoch 5] Batch 1562, Loss 0.25271356105804443\n",
      "[Training Epoch 5] Batch 1563, Loss 0.2537660002708435\n",
      "[Training Epoch 5] Batch 1564, Loss 0.26443374156951904\n",
      "[Training Epoch 5] Batch 1565, Loss 0.23663078248500824\n",
      "[Training Epoch 5] Batch 1566, Loss 0.26593881845474243\n",
      "[Training Epoch 5] Batch 1567, Loss 0.253323495388031\n",
      "[Training Epoch 5] Batch 1568, Loss 0.23654109239578247\n",
      "[Training Epoch 5] Batch 1569, Loss 0.2625948488712311\n",
      "[Training Epoch 5] Batch 1570, Loss 0.2408752143383026\n",
      "[Training Epoch 5] Batch 1571, Loss 0.2656837999820709\n",
      "[Training Epoch 5] Batch 1572, Loss 0.25553643703460693\n",
      "[Training Epoch 5] Batch 1573, Loss 0.2570189833641052\n",
      "[Training Epoch 5] Batch 1574, Loss 0.2499406486749649\n",
      "[Training Epoch 5] Batch 1575, Loss 0.28873589634895325\n",
      "[Training Epoch 5] Batch 1576, Loss 0.24252095818519592\n",
      "[Training Epoch 5] Batch 1577, Loss 0.2383182942867279\n",
      "[Training Epoch 5] Batch 1578, Loss 0.277538537979126\n",
      "[Training Epoch 5] Batch 1579, Loss 0.27321845293045044\n",
      "[Training Epoch 5] Batch 1580, Loss 0.2700018286705017\n",
      "[Training Epoch 5] Batch 1581, Loss 0.27057528495788574\n",
      "[Training Epoch 5] Batch 1582, Loss 0.2740754187107086\n",
      "[Training Epoch 5] Batch 1583, Loss 0.2429841160774231\n",
      "[Training Epoch 5] Batch 1584, Loss 0.26737475395202637\n",
      "[Training Epoch 5] Batch 1585, Loss 0.25601643323898315\n",
      "[Training Epoch 5] Batch 1586, Loss 0.23057329654693604\n",
      "[Training Epoch 5] Batch 1587, Loss 0.2586473226547241\n",
      "[Training Epoch 5] Batch 1588, Loss 0.25158727169036865\n",
      "[Training Epoch 5] Batch 1589, Loss 0.2890833020210266\n",
      "[Training Epoch 5] Batch 1590, Loss 0.28339633345603943\n",
      "[Training Epoch 5] Batch 1591, Loss 0.2568815350532532\n",
      "[Training Epoch 5] Batch 1592, Loss 0.24951106309890747\n",
      "[Training Epoch 5] Batch 1593, Loss 0.25429072976112366\n",
      "[Training Epoch 5] Batch 1594, Loss 0.25724515318870544\n",
      "[Training Epoch 5] Batch 1595, Loss 0.235799640417099\n",
      "[Training Epoch 5] Batch 1596, Loss 0.2429753690958023\n",
      "[Training Epoch 5] Batch 1597, Loss 0.2541392743587494\n",
      "[Training Epoch 5] Batch 1598, Loss 0.26991939544677734\n",
      "[Training Epoch 5] Batch 1599, Loss 0.2630614936351776\n",
      "[Training Epoch 5] Batch 1600, Loss 0.278024822473526\n",
      "[Training Epoch 5] Batch 1601, Loss 0.2641109228134155\n",
      "[Training Epoch 5] Batch 1602, Loss 0.27109190821647644\n",
      "[Training Epoch 5] Batch 1603, Loss 0.2527204155921936\n",
      "[Training Epoch 5] Batch 1604, Loss 0.26833295822143555\n",
      "[Training Epoch 5] Batch 1605, Loss 0.2730959355831146\n",
      "[Training Epoch 5] Batch 1606, Loss 0.2738761901855469\n",
      "[Training Epoch 5] Batch 1607, Loss 0.262972354888916\n",
      "[Training Epoch 5] Batch 1608, Loss 0.25980621576309204\n",
      "[Training Epoch 5] Batch 1609, Loss 0.2628100514411926\n",
      "[Training Epoch 5] Batch 1610, Loss 0.28050777316093445\n",
      "[Training Epoch 5] Batch 1611, Loss 0.2529013752937317\n",
      "[Training Epoch 5] Batch 1612, Loss 0.284138023853302\n",
      "[Training Epoch 5] Batch 1613, Loss 0.25158971548080444\n",
      "[Training Epoch 5] Batch 1614, Loss 0.2816089391708374\n",
      "[Training Epoch 5] Batch 1615, Loss 0.24214112758636475\n",
      "[Training Epoch 5] Batch 1616, Loss 0.24414032697677612\n",
      "[Training Epoch 5] Batch 1617, Loss 0.25487104058265686\n",
      "[Training Epoch 5] Batch 1618, Loss 0.2386297732591629\n",
      "[Training Epoch 5] Batch 1619, Loss 0.27204638719558716\n",
      "[Training Epoch 5] Batch 1620, Loss 0.28283384442329407\n",
      "[Training Epoch 5] Batch 1621, Loss 0.26431137323379517\n",
      "[Training Epoch 5] Batch 1622, Loss 0.25562262535095215\n",
      "[Training Epoch 5] Batch 1623, Loss 0.27101826667785645\n",
      "[Training Epoch 5] Batch 1624, Loss 0.24840642511844635\n",
      "[Training Epoch 5] Batch 1625, Loss 0.292508065700531\n",
      "[Training Epoch 5] Batch 1626, Loss 0.2694544196128845\n",
      "[Training Epoch 5] Batch 1627, Loss 0.24999688565731049\n",
      "[Training Epoch 5] Batch 1628, Loss 0.24017450213432312\n",
      "[Training Epoch 5] Batch 1629, Loss 0.2833341956138611\n",
      "[Training Epoch 5] Batch 1630, Loss 0.27793169021606445\n",
      "[Training Epoch 5] Batch 1631, Loss 0.25682002305984497\n",
      "[Training Epoch 5] Batch 1632, Loss 0.2747551202774048\n",
      "[Training Epoch 5] Batch 1633, Loss 0.28917789459228516\n",
      "[Training Epoch 5] Batch 1634, Loss 0.2603316307067871\n",
      "[Training Epoch 5] Batch 1635, Loss 0.24004840850830078\n",
      "[Training Epoch 5] Batch 1636, Loss 0.25225287675857544\n",
      "[Training Epoch 5] Batch 1637, Loss 0.25856083631515503\n",
      "[Training Epoch 5] Batch 1638, Loss 0.2707265317440033\n",
      "[Training Epoch 5] Batch 1639, Loss 0.264860600233078\n",
      "[Training Epoch 5] Batch 1640, Loss 0.24200893938541412\n",
      "[Training Epoch 5] Batch 1641, Loss 0.24302658438682556\n",
      "[Training Epoch 5] Batch 1642, Loss 0.2650989294052124\n",
      "[Training Epoch 5] Batch 1643, Loss 0.266190767288208\n",
      "[Training Epoch 5] Batch 1644, Loss 0.2746985852718353\n",
      "[Training Epoch 5] Batch 1645, Loss 0.25376954674720764\n",
      "[Training Epoch 5] Batch 1646, Loss 0.2311507761478424\n",
      "[Training Epoch 5] Batch 1647, Loss 0.2847101390361786\n",
      "[Training Epoch 5] Batch 1648, Loss 0.25363975763320923\n",
      "[Training Epoch 5] Batch 1649, Loss 0.2865085005760193\n",
      "[Training Epoch 5] Batch 1650, Loss 0.24067799746990204\n",
      "[Training Epoch 5] Batch 1651, Loss 0.2581297755241394\n",
      "[Training Epoch 5] Batch 1652, Loss 0.2469748854637146\n",
      "[Training Epoch 5] Batch 1653, Loss 0.28128087520599365\n",
      "[Training Epoch 5] Batch 1654, Loss 0.2888179123401642\n",
      "[Training Epoch 5] Batch 1655, Loss 0.24609822034835815\n",
      "[Training Epoch 5] Batch 1656, Loss 0.26388081908226013\n",
      "[Training Epoch 5] Batch 1657, Loss 0.2721903324127197\n",
      "[Training Epoch 5] Batch 1658, Loss 0.2674241065979004\n",
      "[Training Epoch 5] Batch 1659, Loss 0.2148876190185547\n",
      "[Training Epoch 5] Batch 1660, Loss 0.2660908102989197\n",
      "[Training Epoch 5] Batch 1661, Loss 0.20777788758277893\n",
      "[Training Epoch 5] Batch 1662, Loss 0.27459678053855896\n",
      "[Training Epoch 5] Batch 1663, Loss 0.28283655643463135\n",
      "[Training Epoch 5] Batch 1664, Loss 0.24997219443321228\n",
      "[Training Epoch 5] Batch 1665, Loss 0.23722140491008759\n",
      "[Training Epoch 5] Batch 1666, Loss 0.30088627338409424\n",
      "[Training Epoch 5] Batch 1667, Loss 0.24544024467468262\n",
      "[Training Epoch 5] Batch 1668, Loss 0.26627492904663086\n",
      "[Training Epoch 5] Batch 1669, Loss 0.2665884494781494\n",
      "[Training Epoch 5] Batch 1670, Loss 0.2665109634399414\n",
      "[Training Epoch 5] Batch 1671, Loss 0.2676340341567993\n",
      "[Training Epoch 5] Batch 1672, Loss 0.2640644311904907\n",
      "[Training Epoch 5] Batch 1673, Loss 0.23508019745349884\n",
      "[Training Epoch 5] Batch 1674, Loss 0.2788237929344177\n",
      "[Training Epoch 5] Batch 1675, Loss 0.2688325345516205\n",
      "[Training Epoch 5] Batch 1676, Loss 0.27439621090888977\n",
      "[Training Epoch 5] Batch 1677, Loss 0.24371474981307983\n",
      "[Training Epoch 5] Batch 1678, Loss 0.2580925226211548\n",
      "[Training Epoch 5] Batch 1679, Loss 0.2674786448478699\n",
      "[Training Epoch 5] Batch 1680, Loss 0.2765195369720459\n",
      "[Training Epoch 5] Batch 1681, Loss 0.26959919929504395\n",
      "[Training Epoch 5] Batch 1682, Loss 0.26496049761772156\n",
      "[Training Epoch 5] Batch 1683, Loss 0.24228723347187042\n",
      "[Training Epoch 5] Batch 1684, Loss 0.27696216106414795\n",
      "[Training Epoch 5] Batch 1685, Loss 0.26440495252609253\n",
      "[Training Epoch 5] Batch 1686, Loss 0.2642900049686432\n",
      "[Training Epoch 5] Batch 1687, Loss 0.2877923846244812\n",
      "[Training Epoch 5] Batch 1688, Loss 0.24870866537094116\n",
      "[Training Epoch 5] Batch 1689, Loss 0.25778287649154663\n",
      "[Training Epoch 5] Batch 1690, Loss 0.2472095489501953\n",
      "[Training Epoch 5] Batch 1691, Loss 0.2588467597961426\n",
      "[Training Epoch 5] Batch 1692, Loss 0.24436374008655548\n",
      "[Training Epoch 5] Batch 1693, Loss 0.2617063820362091\n",
      "[Training Epoch 5] Batch 1694, Loss 0.23484180867671967\n",
      "[Training Epoch 5] Batch 1695, Loss 0.2457365244626999\n",
      "[Training Epoch 5] Batch 1696, Loss 0.24159124493598938\n",
      "[Training Epoch 5] Batch 1697, Loss 0.26882994174957275\n",
      "[Training Epoch 5] Batch 1698, Loss 0.2781309485435486\n",
      "[Training Epoch 5] Batch 1699, Loss 0.256797730922699\n",
      "[Training Epoch 5] Batch 1700, Loss 0.2707917094230652\n",
      "[Training Epoch 5] Batch 1701, Loss 0.289730429649353\n",
      "[Training Epoch 5] Batch 1702, Loss 0.2546728849411011\n",
      "[Training Epoch 5] Batch 1703, Loss 0.27136993408203125\n",
      "[Training Epoch 5] Batch 1704, Loss 0.2687526345252991\n",
      "[Training Epoch 5] Batch 1705, Loss 0.24044236540794373\n",
      "[Training Epoch 5] Batch 1706, Loss 0.2558475136756897\n",
      "[Training Epoch 5] Batch 1707, Loss 0.2698965072631836\n",
      "[Training Epoch 5] Batch 1708, Loss 0.2769901752471924\n",
      "[Training Epoch 5] Batch 1709, Loss 0.2624191343784332\n",
      "[Training Epoch 5] Batch 1710, Loss 0.27886778116226196\n",
      "[Training Epoch 5] Batch 1711, Loss 0.24776288866996765\n",
      "[Training Epoch 5] Batch 1712, Loss 0.2897394895553589\n",
      "[Training Epoch 5] Batch 1713, Loss 0.26259514689445496\n",
      "[Training Epoch 5] Batch 1714, Loss 0.26742082834243774\n",
      "[Training Epoch 5] Batch 1715, Loss 0.2636713981628418\n",
      "[Training Epoch 5] Batch 1716, Loss 0.2691405117511749\n",
      "[Training Epoch 5] Batch 1717, Loss 0.259450763463974\n",
      "[Training Epoch 5] Batch 1718, Loss 0.23993930220603943\n",
      "[Training Epoch 5] Batch 1719, Loss 0.28513675928115845\n",
      "[Training Epoch 5] Batch 1720, Loss 0.2652961313724518\n",
      "[Training Epoch 5] Batch 1721, Loss 0.2622429132461548\n",
      "[Training Epoch 5] Batch 1722, Loss 0.23556636273860931\n",
      "[Training Epoch 5] Batch 1723, Loss 0.24447335302829742\n",
      "[Training Epoch 5] Batch 1724, Loss 0.27959999442100525\n",
      "[Training Epoch 5] Batch 1725, Loss 0.26460403203964233\n",
      "[Training Epoch 5] Batch 1726, Loss 0.25394952297210693\n",
      "[Training Epoch 5] Batch 1727, Loss 0.27793437242507935\n",
      "[Training Epoch 5] Batch 1728, Loss 0.25283747911453247\n",
      "[Training Epoch 5] Batch 1729, Loss 0.26910096406936646\n",
      "[Training Epoch 5] Batch 1730, Loss 0.2591206431388855\n",
      "[Training Epoch 5] Batch 1731, Loss 0.2617824375629425\n",
      "[Training Epoch 5] Batch 1732, Loss 0.265170693397522\n",
      "[Training Epoch 5] Batch 1733, Loss 0.25586575269699097\n",
      "[Training Epoch 5] Batch 1734, Loss 0.26173511147499084\n",
      "[Training Epoch 5] Batch 1735, Loss 0.2504482865333557\n",
      "[Training Epoch 5] Batch 1736, Loss 0.2700209617614746\n",
      "[Training Epoch 5] Batch 1737, Loss 0.2528420686721802\n",
      "[Training Epoch 5] Batch 1738, Loss 0.26140308380126953\n",
      "[Training Epoch 5] Batch 1739, Loss 0.2661552429199219\n",
      "[Training Epoch 5] Batch 1740, Loss 0.2864154577255249\n",
      "[Training Epoch 5] Batch 1741, Loss 0.2297687828540802\n",
      "[Training Epoch 5] Batch 1742, Loss 0.2294759601354599\n",
      "[Training Epoch 5] Batch 1743, Loss 0.2563142478466034\n",
      "[Training Epoch 5] Batch 1744, Loss 0.2845146358013153\n",
      "[Training Epoch 5] Batch 1745, Loss 0.23663301765918732\n",
      "[Training Epoch 5] Batch 1746, Loss 0.2773221731185913\n",
      "[Training Epoch 5] Batch 1747, Loss 0.2739591598510742\n",
      "[Training Epoch 5] Batch 1748, Loss 0.22687432169914246\n",
      "[Training Epoch 5] Batch 1749, Loss 0.2742037773132324\n",
      "[Training Epoch 5] Batch 1750, Loss 0.264740914106369\n",
      "[Training Epoch 5] Batch 1751, Loss 0.24148719012737274\n",
      "[Training Epoch 5] Batch 1752, Loss 0.25552937388420105\n",
      "[Training Epoch 5] Batch 1753, Loss 0.2622106671333313\n",
      "[Training Epoch 5] Batch 1754, Loss 0.24752625823020935\n",
      "[Training Epoch 5] Batch 1755, Loss 0.2558683156967163\n",
      "[Training Epoch 5] Batch 1756, Loss 0.25478026270866394\n",
      "[Training Epoch 5] Batch 1757, Loss 0.2822176218032837\n",
      "[Training Epoch 5] Batch 1758, Loss 0.2764614224433899\n",
      "[Training Epoch 5] Batch 1759, Loss 0.248787984251976\n",
      "[Training Epoch 5] Batch 1760, Loss 0.2671661674976349\n",
      "[Training Epoch 5] Batch 1761, Loss 0.2599065899848938\n",
      "[Training Epoch 5] Batch 1762, Loss 0.2803407609462738\n",
      "[Training Epoch 5] Batch 1763, Loss 0.24837589263916016\n",
      "[Training Epoch 5] Batch 1764, Loss 0.28891611099243164\n",
      "[Training Epoch 5] Batch 1765, Loss 0.3081209659576416\n",
      "[Training Epoch 5] Batch 1766, Loss 0.23987099528312683\n",
      "[Training Epoch 5] Batch 1767, Loss 0.28279274702072144\n",
      "[Training Epoch 5] Batch 1768, Loss 0.27294307947158813\n",
      "[Training Epoch 5] Batch 1769, Loss 0.2522880434989929\n",
      "[Training Epoch 5] Batch 1770, Loss 0.25393766164779663\n",
      "[Training Epoch 5] Batch 1771, Loss 0.24498066306114197\n",
      "[Training Epoch 5] Batch 1772, Loss 0.2495442032814026\n",
      "[Training Epoch 5] Batch 1773, Loss 0.24112311005592346\n",
      "[Training Epoch 5] Batch 1774, Loss 0.25409436225891113\n",
      "[Training Epoch 5] Batch 1775, Loss 0.2873128056526184\n",
      "[Training Epoch 5] Batch 1776, Loss 0.2584823668003082\n",
      "[Training Epoch 5] Batch 1777, Loss 0.24530412256717682\n",
      "[Training Epoch 5] Batch 1778, Loss 0.27466583251953125\n",
      "[Training Epoch 5] Batch 1779, Loss 0.2681157886981964\n",
      "[Training Epoch 5] Batch 1780, Loss 0.2467125654220581\n",
      "[Training Epoch 5] Batch 1781, Loss 0.25353291630744934\n",
      "[Training Epoch 5] Batch 1782, Loss 0.27186572551727295\n",
      "[Training Epoch 5] Batch 1783, Loss 0.27249836921691895\n",
      "[Training Epoch 5] Batch 1784, Loss 0.26103267073631287\n",
      "[Training Epoch 5] Batch 1785, Loss 0.2688365578651428\n",
      "[Training Epoch 5] Batch 1786, Loss 0.2544231712818146\n",
      "[Training Epoch 5] Batch 1787, Loss 0.28633517026901245\n",
      "[Training Epoch 5] Batch 1788, Loss 0.25416794419288635\n",
      "[Training Epoch 5] Batch 1789, Loss 0.22418586909770966\n",
      "[Training Epoch 5] Batch 1790, Loss 0.2752794623374939\n",
      "[Training Epoch 5] Batch 1791, Loss 0.2522048354148865\n",
      "[Training Epoch 5] Batch 1792, Loss 0.25472792983055115\n",
      "[Training Epoch 5] Batch 1793, Loss 0.24430842697620392\n",
      "[Training Epoch 5] Batch 1794, Loss 0.2609960734844208\n",
      "[Training Epoch 5] Batch 1795, Loss 0.24334901571273804\n",
      "[Training Epoch 5] Batch 1796, Loss 0.24819952249526978\n",
      "[Training Epoch 5] Batch 1797, Loss 0.2580570578575134\n",
      "[Training Epoch 5] Batch 1798, Loss 0.2534635663032532\n",
      "[Training Epoch 5] Batch 1799, Loss 0.2605780363082886\n",
      "[Training Epoch 5] Batch 1800, Loss 0.2497159093618393\n",
      "[Training Epoch 5] Batch 1801, Loss 0.2839685082435608\n",
      "[Training Epoch 5] Batch 1802, Loss 0.224810391664505\n",
      "[Training Epoch 5] Batch 1803, Loss 0.2627537250518799\n",
      "[Training Epoch 5] Batch 1804, Loss 0.25005996227264404\n",
      "[Training Epoch 5] Batch 1805, Loss 0.2556401491165161\n",
      "[Training Epoch 5] Batch 1806, Loss 0.245996356010437\n",
      "[Training Epoch 5] Batch 1807, Loss 0.25846245884895325\n",
      "[Training Epoch 5] Batch 1808, Loss 0.22269627451896667\n",
      "[Training Epoch 5] Batch 1809, Loss 0.2381259948015213\n",
      "[Training Epoch 5] Batch 1810, Loss 0.23944836854934692\n",
      "[Training Epoch 5] Batch 1811, Loss 0.29420411586761475\n",
      "[Training Epoch 5] Batch 1812, Loss 0.2620515525341034\n",
      "[Training Epoch 5] Batch 1813, Loss 0.25259101390838623\n",
      "[Training Epoch 5] Batch 1814, Loss 0.25524166226387024\n",
      "[Training Epoch 5] Batch 1815, Loss 0.2511898875236511\n",
      "[Training Epoch 5] Batch 1816, Loss 0.2500222325325012\n",
      "[Training Epoch 5] Batch 1817, Loss 0.29469034075737\n",
      "[Training Epoch 5] Batch 1818, Loss 0.27928054332733154\n",
      "[Training Epoch 5] Batch 1819, Loss 0.21084770560264587\n",
      "[Training Epoch 5] Batch 1820, Loss 0.2511504888534546\n",
      "[Training Epoch 5] Batch 1821, Loss 0.28571417927742004\n",
      "[Training Epoch 5] Batch 1822, Loss 0.2448108196258545\n",
      "[Training Epoch 5] Batch 1823, Loss 0.25937944650650024\n",
      "[Training Epoch 5] Batch 1824, Loss 0.23098397254943848\n",
      "[Training Epoch 5] Batch 1825, Loss 0.25041663646698\n",
      "[Training Epoch 5] Batch 1826, Loss 0.2447618842124939\n",
      "[Training Epoch 5] Batch 1827, Loss 0.27877289056777954\n",
      "[Training Epoch 5] Batch 1828, Loss 0.2515895366668701\n",
      "[Training Epoch 5] Batch 1829, Loss 0.2569453716278076\n",
      "[Training Epoch 5] Batch 1830, Loss 0.24549786746501923\n",
      "[Training Epoch 5] Batch 1831, Loss 0.2640588879585266\n",
      "[Training Epoch 5] Batch 1832, Loss 0.24651286005973816\n",
      "[Training Epoch 5] Batch 1833, Loss 0.27731698751449585\n",
      "[Training Epoch 5] Batch 1834, Loss 0.2509133815765381\n",
      "[Training Epoch 5] Batch 1835, Loss 0.2672463655471802\n",
      "[Training Epoch 5] Batch 1836, Loss 0.2334154099225998\n",
      "[Training Epoch 5] Batch 1837, Loss 0.2418849766254425\n",
      "[Training Epoch 5] Batch 1838, Loss 0.2754102945327759\n",
      "[Training Epoch 5] Batch 1839, Loss 0.2676725387573242\n",
      "[Training Epoch 5] Batch 1840, Loss 0.2855028510093689\n",
      "[Training Epoch 5] Batch 1841, Loss 0.26804959774017334\n",
      "[Training Epoch 5] Batch 1842, Loss 0.25880467891693115\n",
      "[Training Epoch 5] Batch 1843, Loss 0.2348741739988327\n",
      "[Training Epoch 5] Batch 1844, Loss 0.2480066418647766\n",
      "[Training Epoch 5] Batch 1845, Loss 0.2536933422088623\n",
      "[Training Epoch 5] Batch 1846, Loss 0.303413450717926\n",
      "[Training Epoch 5] Batch 1847, Loss 0.28797778487205505\n",
      "[Training Epoch 5] Batch 1848, Loss 0.2812952995300293\n",
      "[Training Epoch 5] Batch 1849, Loss 0.2686039209365845\n",
      "[Training Epoch 5] Batch 1850, Loss 0.27459898591041565\n",
      "[Training Epoch 5] Batch 1851, Loss 0.2618376314640045\n",
      "[Training Epoch 5] Batch 1852, Loss 0.24596910178661346\n",
      "[Training Epoch 5] Batch 1853, Loss 0.25500962138175964\n",
      "[Training Epoch 5] Batch 1854, Loss 0.2488071769475937\n",
      "[Training Epoch 5] Batch 1855, Loss 0.24810829758644104\n",
      "[Training Epoch 5] Batch 1856, Loss 0.26494184136390686\n",
      "[Training Epoch 5] Batch 1857, Loss 0.24774779379367828\n",
      "[Training Epoch 5] Batch 1858, Loss 0.24437198042869568\n",
      "[Training Epoch 5] Batch 1859, Loss 0.259757936000824\n",
      "[Training Epoch 5] Batch 1860, Loss 0.2719874083995819\n",
      "[Training Epoch 5] Batch 1861, Loss 0.25319093465805054\n",
      "[Training Epoch 5] Batch 1862, Loss 0.2573074698448181\n",
      "[Training Epoch 5] Batch 1863, Loss 0.2712423801422119\n",
      "[Training Epoch 5] Batch 1864, Loss 0.27169355750083923\n",
      "[Training Epoch 5] Batch 1865, Loss 0.23671701550483704\n",
      "[Training Epoch 5] Batch 1866, Loss 0.24952924251556396\n",
      "[Training Epoch 5] Batch 1867, Loss 0.23957672715187073\n",
      "[Training Epoch 5] Batch 1868, Loss 0.2473180890083313\n",
      "[Training Epoch 5] Batch 1869, Loss 0.23353634774684906\n",
      "[Training Epoch 5] Batch 1870, Loss 0.28403279185295105\n",
      "[Training Epoch 5] Batch 1871, Loss 0.28147536516189575\n",
      "[Training Epoch 5] Batch 1872, Loss 0.2509855628013611\n",
      "[Training Epoch 5] Batch 1873, Loss 0.2715664505958557\n",
      "[Training Epoch 5] Batch 1874, Loss 0.25162196159362793\n",
      "[Training Epoch 5] Batch 1875, Loss 0.23652034997940063\n",
      "[Training Epoch 5] Batch 1876, Loss 0.25365930795669556\n",
      "[Training Epoch 5] Batch 1877, Loss 0.23618367314338684\n",
      "[Training Epoch 5] Batch 1878, Loss 0.23506149649620056\n",
      "[Training Epoch 5] Batch 1879, Loss 0.26423293352127075\n",
      "[Training Epoch 5] Batch 1880, Loss 0.27521413564682007\n",
      "[Training Epoch 5] Batch 1881, Loss 0.2356664538383484\n",
      "[Training Epoch 5] Batch 1882, Loss 0.2251531481742859\n",
      "[Training Epoch 5] Batch 1883, Loss 0.2568426728248596\n",
      "[Training Epoch 5] Batch 1884, Loss 0.2639080882072449\n",
      "[Training Epoch 5] Batch 1885, Loss 0.2644919157028198\n",
      "[Training Epoch 5] Batch 1886, Loss 0.2534691393375397\n",
      "[Training Epoch 5] Batch 1887, Loss 0.25411373376846313\n",
      "[Training Epoch 5] Batch 1888, Loss 0.2726956009864807\n",
      "[Training Epoch 5] Batch 1889, Loss 0.27343958616256714\n",
      "[Training Epoch 5] Batch 1890, Loss 0.29025399684906006\n",
      "[Training Epoch 5] Batch 1891, Loss 0.2513200640678406\n",
      "[Training Epoch 5] Batch 1892, Loss 0.2595537304878235\n",
      "[Training Epoch 5] Batch 1893, Loss 0.24829161167144775\n",
      "[Training Epoch 5] Batch 1894, Loss 0.2778322398662567\n",
      "[Training Epoch 5] Batch 1895, Loss 0.274824857711792\n",
      "[Training Epoch 5] Batch 1896, Loss 0.2537451386451721\n",
      "[Training Epoch 5] Batch 1897, Loss 0.28844746947288513\n",
      "[Training Epoch 5] Batch 1898, Loss 0.2485605627298355\n",
      "[Training Epoch 5] Batch 1899, Loss 0.2570856809616089\n",
      "[Training Epoch 5] Batch 1900, Loss 0.2521272301673889\n",
      "[Training Epoch 5] Batch 1901, Loss 0.2536214590072632\n",
      "[Training Epoch 5] Batch 1902, Loss 0.27060016989707947\n",
      "[Training Epoch 5] Batch 1903, Loss 0.25002771615982056\n",
      "[Training Epoch 5] Batch 1904, Loss 0.23871120810508728\n",
      "[Training Epoch 5] Batch 1905, Loss 0.25316670536994934\n",
      "[Training Epoch 5] Batch 1906, Loss 0.2625274658203125\n",
      "[Training Epoch 5] Batch 1907, Loss 0.26766812801361084\n",
      "[Training Epoch 5] Batch 1908, Loss 0.25161486864089966\n",
      "[Training Epoch 5] Batch 1909, Loss 0.2534141540527344\n",
      "[Training Epoch 5] Batch 1910, Loss 0.29162049293518066\n",
      "[Training Epoch 5] Batch 1911, Loss 0.28033447265625\n",
      "[Training Epoch 5] Batch 1912, Loss 0.27800047397613525\n",
      "[Training Epoch 5] Batch 1913, Loss 0.29235023260116577\n",
      "[Training Epoch 5] Batch 1914, Loss 0.2775360941886902\n",
      "[Training Epoch 5] Batch 1915, Loss 0.24351102113723755\n",
      "[Training Epoch 5] Batch 1916, Loss 0.2374449521303177\n",
      "[Training Epoch 5] Batch 1917, Loss 0.2597745656967163\n",
      "[Training Epoch 5] Batch 1918, Loss 0.23124685883522034\n",
      "[Training Epoch 5] Batch 1919, Loss 0.2567558288574219\n",
      "[Training Epoch 5] Batch 1920, Loss 0.2543542981147766\n",
      "[Training Epoch 5] Batch 1921, Loss 0.2741781175136566\n",
      "[Training Epoch 5] Batch 1922, Loss 0.26062285900115967\n",
      "[Training Epoch 5] Batch 1923, Loss 0.25337886810302734\n",
      "[Training Epoch 5] Batch 1924, Loss 0.238820880651474\n",
      "[Training Epoch 5] Batch 1925, Loss 0.25464215874671936\n",
      "[Training Epoch 5] Batch 1926, Loss 0.2903139889240265\n",
      "[Training Epoch 5] Batch 1927, Loss 0.2811228036880493\n",
      "[Training Epoch 5] Batch 1928, Loss 0.2542714476585388\n",
      "[Training Epoch 5] Batch 1929, Loss 0.25136542320251465\n",
      "[Training Epoch 5] Batch 1930, Loss 0.24645650386810303\n",
      "[Training Epoch 5] Batch 1931, Loss 0.28900033235549927\n",
      "[Training Epoch 5] Batch 1932, Loss 0.23715616762638092\n",
      "[Training Epoch 5] Batch 1933, Loss 0.27082228660583496\n",
      "[Training Epoch 5] Batch 1934, Loss 0.27535322308540344\n",
      "[Training Epoch 5] Batch 1935, Loss 0.2723750174045563\n",
      "[Training Epoch 5] Batch 1936, Loss 0.23688094317913055\n",
      "[Training Epoch 5] Batch 1937, Loss 0.2643257975578308\n",
      "[Training Epoch 5] Batch 1938, Loss 0.25771182775497437\n",
      "[Training Epoch 5] Batch 1939, Loss 0.27321815490722656\n",
      "[Training Epoch 5] Batch 1940, Loss 0.2637740671634674\n",
      "[Training Epoch 5] Batch 1941, Loss 0.26795440912246704\n",
      "[Training Epoch 5] Batch 1942, Loss 0.27867862582206726\n",
      "[Training Epoch 5] Batch 1943, Loss 0.2761978209018707\n",
      "[Training Epoch 5] Batch 1944, Loss 0.29458242654800415\n",
      "[Training Epoch 5] Batch 1945, Loss 0.2618236541748047\n",
      "[Training Epoch 5] Batch 1946, Loss 0.24841421842575073\n",
      "[Training Epoch 5] Batch 1947, Loss 0.24674828350543976\n",
      "[Training Epoch 5] Batch 1948, Loss 0.2630891799926758\n",
      "[Training Epoch 5] Batch 1949, Loss 0.27203264832496643\n",
      "[Training Epoch 5] Batch 1950, Loss 0.2595028877258301\n",
      "[Training Epoch 5] Batch 1951, Loss 0.2909456491470337\n",
      "[Training Epoch 5] Batch 1952, Loss 0.2617036998271942\n",
      "[Training Epoch 5] Batch 1953, Loss 0.23349082469940186\n",
      "[Training Epoch 5] Batch 1954, Loss 0.2765769958496094\n",
      "[Training Epoch 5] Batch 1955, Loss 0.24889647960662842\n",
      "[Training Epoch 5] Batch 1956, Loss 0.26246264576911926\n",
      "[Training Epoch 5] Batch 1957, Loss 0.24741636216640472\n",
      "[Training Epoch 5] Batch 1958, Loss 0.2887754440307617\n",
      "[Training Epoch 5] Batch 1959, Loss 0.28352636098861694\n",
      "[Training Epoch 5] Batch 1960, Loss 0.258806437253952\n",
      "[Training Epoch 5] Batch 1961, Loss 0.2695925533771515\n",
      "[Training Epoch 5] Batch 1962, Loss 0.25904378294944763\n",
      "[Training Epoch 5] Batch 1963, Loss 0.2423396110534668\n",
      "[Training Epoch 5] Batch 1964, Loss 0.2565014064311981\n",
      "[Training Epoch 5] Batch 1965, Loss 0.25349515676498413\n",
      "[Training Epoch 5] Batch 1966, Loss 0.2520807385444641\n",
      "[Training Epoch 5] Batch 1967, Loss 0.28222745656967163\n",
      "[Training Epoch 5] Batch 1968, Loss 0.2604657709598541\n",
      "[Training Epoch 5] Batch 1969, Loss 0.24243177473545074\n",
      "[Training Epoch 5] Batch 1970, Loss 0.27931272983551025\n",
      "[Training Epoch 5] Batch 1971, Loss 0.23864389955997467\n",
      "[Training Epoch 5] Batch 1972, Loss 0.2561810314655304\n",
      "[Training Epoch 5] Batch 1973, Loss 0.255965918302536\n",
      "[Training Epoch 5] Batch 1974, Loss 0.2660917043685913\n",
      "[Training Epoch 5] Batch 1975, Loss 0.2879071831703186\n",
      "[Training Epoch 5] Batch 1976, Loss 0.2728639245033264\n",
      "[Training Epoch 5] Batch 1977, Loss 0.25923001766204834\n",
      "[Training Epoch 5] Batch 1978, Loss 0.2449653446674347\n",
      "[Training Epoch 5] Batch 1979, Loss 0.27060240507125854\n",
      "[Training Epoch 5] Batch 1980, Loss 0.2571731209754944\n",
      "[Training Epoch 5] Batch 1981, Loss 0.26567429304122925\n",
      "[Training Epoch 5] Batch 1982, Loss 0.27840012311935425\n",
      "[Training Epoch 5] Batch 1983, Loss 0.23957888782024384\n",
      "[Training Epoch 5] Batch 1984, Loss 0.25728604197502136\n",
      "[Training Epoch 5] Batch 1985, Loss 0.25426363945007324\n",
      "[Training Epoch 5] Batch 1986, Loss 0.26293283700942993\n",
      "[Training Epoch 5] Batch 1987, Loss 0.28722405433654785\n",
      "[Training Epoch 5] Batch 1988, Loss 0.28343915939331055\n",
      "[Training Epoch 5] Batch 1989, Loss 0.26368725299835205\n",
      "[Training Epoch 5] Batch 1990, Loss 0.2623611390590668\n",
      "[Training Epoch 5] Batch 1991, Loss 0.26565805077552795\n",
      "[Training Epoch 5] Batch 1992, Loss 0.2567194700241089\n",
      "[Training Epoch 5] Batch 1993, Loss 0.25853562355041504\n",
      "[Training Epoch 5] Batch 1994, Loss 0.2817593514919281\n",
      "[Training Epoch 5] Batch 1995, Loss 0.22475263476371765\n",
      "[Training Epoch 5] Batch 1996, Loss 0.25450801849365234\n",
      "[Training Epoch 5] Batch 1997, Loss 0.23907674849033356\n",
      "[Training Epoch 5] Batch 1998, Loss 0.2620210647583008\n",
      "[Training Epoch 5] Batch 1999, Loss 0.24023082852363586\n",
      "[Training Epoch 5] Batch 2000, Loss 0.25035643577575684\n",
      "[Training Epoch 5] Batch 2001, Loss 0.262172669172287\n",
      "[Training Epoch 5] Batch 2002, Loss 0.2672345042228699\n",
      "[Training Epoch 5] Batch 2003, Loss 0.2584055960178375\n",
      "[Training Epoch 5] Batch 2004, Loss 0.24618783593177795\n",
      "[Training Epoch 5] Batch 2005, Loss 0.23286670446395874\n",
      "[Training Epoch 5] Batch 2006, Loss 0.24607905745506287\n",
      "[Training Epoch 5] Batch 2007, Loss 0.2895764708518982\n",
      "[Training Epoch 5] Batch 2008, Loss 0.2583919167518616\n",
      "[Training Epoch 5] Batch 2009, Loss 0.25727489590644836\n",
      "[Training Epoch 5] Batch 2010, Loss 0.2345842570066452\n",
      "[Training Epoch 5] Batch 2011, Loss 0.26553940773010254\n",
      "[Training Epoch 5] Batch 2012, Loss 0.2520216703414917\n",
      "[Training Epoch 5] Batch 2013, Loss 0.25262075662612915\n",
      "[Training Epoch 5] Batch 2014, Loss 0.2914876937866211\n",
      "[Training Epoch 5] Batch 2015, Loss 0.2753676176071167\n",
      "[Training Epoch 5] Batch 2016, Loss 0.2669583559036255\n",
      "[Training Epoch 5] Batch 2017, Loss 0.25160378217697144\n",
      "[Training Epoch 5] Batch 2018, Loss 0.26703140139579773\n",
      "[Training Epoch 5] Batch 2019, Loss 0.25829005241394043\n",
      "[Training Epoch 5] Batch 2020, Loss 0.23574106395244598\n",
      "[Training Epoch 5] Batch 2021, Loss 0.226570263504982\n",
      "[Training Epoch 5] Batch 2022, Loss 0.2648829221725464\n",
      "[Training Epoch 5] Batch 2023, Loss 0.2704920768737793\n",
      "[Training Epoch 5] Batch 2024, Loss 0.24707084894180298\n",
      "[Training Epoch 5] Batch 2025, Loss 0.28162798285484314\n",
      "[Training Epoch 5] Batch 2026, Loss 0.2443726509809494\n",
      "[Training Epoch 5] Batch 2027, Loss 0.2640988826751709\n",
      "[Training Epoch 5] Batch 2028, Loss 0.29165011644363403\n",
      "[Training Epoch 5] Batch 2029, Loss 0.24025630950927734\n",
      "[Training Epoch 5] Batch 2030, Loss 0.2797658443450928\n",
      "[Training Epoch 5] Batch 2031, Loss 0.29513925313949585\n",
      "[Training Epoch 5] Batch 2032, Loss 0.26024726033210754\n",
      "[Training Epoch 5] Batch 2033, Loss 0.259623646736145\n",
      "[Training Epoch 5] Batch 2034, Loss 0.252130389213562\n",
      "[Training Epoch 5] Batch 2035, Loss 0.29355213046073914\n",
      "[Training Epoch 5] Batch 2036, Loss 0.2625628113746643\n",
      "[Training Epoch 5] Batch 2037, Loss 0.27130216360092163\n",
      "[Training Epoch 5] Batch 2038, Loss 0.2480280101299286\n",
      "[Training Epoch 5] Batch 2039, Loss 0.2586359977722168\n",
      "[Training Epoch 5] Batch 2040, Loss 0.24940428137779236\n",
      "[Training Epoch 5] Batch 2041, Loss 0.2447793036699295\n",
      "[Training Epoch 5] Batch 2042, Loss 0.2702506184577942\n",
      "[Training Epoch 5] Batch 2043, Loss 0.2432098686695099\n",
      "[Training Epoch 5] Batch 2044, Loss 0.25029027462005615\n",
      "[Training Epoch 5] Batch 2045, Loss 0.2578010559082031\n",
      "[Training Epoch 5] Batch 2046, Loss 0.2564932703971863\n",
      "[Training Epoch 5] Batch 2047, Loss 0.24810543656349182\n",
      "[Training Epoch 5] Batch 2048, Loss 0.28134608268737793\n",
      "[Training Epoch 5] Batch 2049, Loss 0.28632408380508423\n",
      "[Training Epoch 5] Batch 2050, Loss 0.26351267099380493\n",
      "[Training Epoch 5] Batch 2051, Loss 0.2583291828632355\n",
      "[Training Epoch 5] Batch 2052, Loss 0.26488637924194336\n",
      "[Training Epoch 5] Batch 2053, Loss 0.2604871392250061\n",
      "[Training Epoch 5] Batch 2054, Loss 0.28097134828567505\n",
      "[Training Epoch 5] Batch 2055, Loss 0.27271315455436707\n",
      "[Training Epoch 5] Batch 2056, Loss 0.2660280466079712\n",
      "[Training Epoch 5] Batch 2057, Loss 0.23845772445201874\n",
      "[Training Epoch 5] Batch 2058, Loss 0.28512412309646606\n",
      "[Training Epoch 5] Batch 2059, Loss 0.2411210536956787\n",
      "[Training Epoch 5] Batch 2060, Loss 0.2776021957397461\n",
      "[Training Epoch 5] Batch 2061, Loss 0.28476858139038086\n",
      "[Training Epoch 5] Batch 2062, Loss 0.2672162652015686\n",
      "[Training Epoch 5] Batch 2063, Loss 0.2509732246398926\n",
      "[Training Epoch 5] Batch 2064, Loss 0.27555620670318604\n",
      "[Training Epoch 5] Batch 2065, Loss 0.26077741384506226\n",
      "[Training Epoch 5] Batch 2066, Loss 0.2568872570991516\n",
      "[Training Epoch 5] Batch 2067, Loss 0.24259474873542786\n",
      "[Training Epoch 5] Batch 2068, Loss 0.2491641342639923\n",
      "[Training Epoch 5] Batch 2069, Loss 0.2952006757259369\n",
      "[Training Epoch 5] Batch 2070, Loss 0.2825581729412079\n",
      "[Training Epoch 5] Batch 2071, Loss 0.24473805725574493\n",
      "[Training Epoch 5] Batch 2072, Loss 0.2713116407394409\n",
      "[Training Epoch 5] Batch 2073, Loss 0.2577073574066162\n",
      "[Training Epoch 5] Batch 2074, Loss 0.2622690200805664\n",
      "[Training Epoch 5] Batch 2075, Loss 0.2843239903450012\n",
      "[Training Epoch 5] Batch 2076, Loss 0.22051486372947693\n",
      "[Training Epoch 5] Batch 2077, Loss 0.2542843818664551\n",
      "[Training Epoch 5] Batch 2078, Loss 0.24910777807235718\n",
      "[Training Epoch 5] Batch 2079, Loss 0.28754979372024536\n",
      "[Training Epoch 5] Batch 2080, Loss 0.25992846488952637\n",
      "[Training Epoch 5] Batch 2081, Loss 0.24191559851169586\n",
      "[Training Epoch 5] Batch 2082, Loss 0.26764023303985596\n",
      "[Training Epoch 5] Batch 2083, Loss 0.24958476424217224\n",
      "[Training Epoch 5] Batch 2084, Loss 0.25963670015335083\n",
      "[Training Epoch 5] Batch 2085, Loss 0.23567934334278107\n",
      "[Training Epoch 5] Batch 2086, Loss 0.2723892033100128\n",
      "[Training Epoch 5] Batch 2087, Loss 0.2685476541519165\n",
      "[Training Epoch 5] Batch 2088, Loss 0.23921902477741241\n",
      "[Training Epoch 5] Batch 2089, Loss 0.27406632900238037\n",
      "[Training Epoch 5] Batch 2090, Loss 0.25196123123168945\n",
      "[Training Epoch 5] Batch 2091, Loss 0.28677910566329956\n",
      "[Training Epoch 5] Batch 2092, Loss 0.24170133471488953\n",
      "[Training Epoch 5] Batch 2093, Loss 0.24481400847434998\n",
      "[Training Epoch 5] Batch 2094, Loss 0.2534758448600769\n",
      "[Training Epoch 5] Batch 2095, Loss 0.23767228424549103\n",
      "[Training Epoch 5] Batch 2096, Loss 0.28426462411880493\n",
      "[Training Epoch 5] Batch 2097, Loss 0.2486741542816162\n",
      "[Training Epoch 5] Batch 2098, Loss 0.24221652746200562\n",
      "[Training Epoch 5] Batch 2099, Loss 0.23849768936634064\n",
      "[Training Epoch 5] Batch 2100, Loss 0.2822362184524536\n",
      "[Training Epoch 5] Batch 2101, Loss 0.28398391604423523\n",
      "[Training Epoch 5] Batch 2102, Loss 0.264326274394989\n",
      "[Training Epoch 5] Batch 2103, Loss 0.27981770038604736\n",
      "[Training Epoch 5] Batch 2104, Loss 0.28207820653915405\n",
      "[Training Epoch 5] Batch 2105, Loss 0.26462915539741516\n",
      "[Training Epoch 5] Batch 2106, Loss 0.24819964170455933\n",
      "[Training Epoch 5] Batch 2107, Loss 0.2547004818916321\n",
      "[Training Epoch 5] Batch 2108, Loss 0.24831831455230713\n",
      "[Training Epoch 5] Batch 2109, Loss 0.26735520362854004\n",
      "[Training Epoch 5] Batch 2110, Loss 0.2724918723106384\n",
      "[Training Epoch 5] Batch 2111, Loss 0.2573741674423218\n",
      "[Training Epoch 5] Batch 2112, Loss 0.26820117235183716\n",
      "[Training Epoch 5] Batch 2113, Loss 0.24942827224731445\n",
      "[Training Epoch 5] Batch 2114, Loss 0.27392837405204773\n",
      "[Training Epoch 5] Batch 2115, Loss 0.2660152018070221\n",
      "[Training Epoch 5] Batch 2116, Loss 0.2398478239774704\n",
      "[Training Epoch 5] Batch 2117, Loss 0.25926631689071655\n",
      "[Training Epoch 5] Batch 2118, Loss 0.25944432616233826\n",
      "[Training Epoch 5] Batch 2119, Loss 0.2816644310951233\n",
      "[Training Epoch 5] Batch 2120, Loss 0.25580886006355286\n",
      "[Training Epoch 5] Batch 2121, Loss 0.24191434681415558\n",
      "[Training Epoch 5] Batch 2122, Loss 0.24330256879329681\n",
      "[Training Epoch 5] Batch 2123, Loss 0.27576467394828796\n",
      "[Training Epoch 5] Batch 2124, Loss 0.2765117287635803\n",
      "[Training Epoch 5] Batch 2125, Loss 0.26760515570640564\n",
      "[Training Epoch 5] Batch 2126, Loss 0.254163920879364\n",
      "[Training Epoch 5] Batch 2127, Loss 0.23467259109020233\n",
      "[Training Epoch 5] Batch 2128, Loss 0.2714836895465851\n",
      "[Training Epoch 5] Batch 2129, Loss 0.24890407919883728\n",
      "[Training Epoch 5] Batch 2130, Loss 0.2638304829597473\n",
      "[Training Epoch 5] Batch 2131, Loss 0.24935367703437805\n",
      "[Training Epoch 5] Batch 2132, Loss 0.2461145669221878\n",
      "[Training Epoch 5] Batch 2133, Loss 0.24324586987495422\n",
      "[Training Epoch 5] Batch 2134, Loss 0.3007141351699829\n",
      "[Training Epoch 5] Batch 2135, Loss 0.24232228100299835\n",
      "[Training Epoch 5] Batch 2136, Loss 0.25047844648361206\n",
      "[Training Epoch 5] Batch 2137, Loss 0.26998549699783325\n",
      "[Training Epoch 5] Batch 2138, Loss 0.2601807713508606\n",
      "[Training Epoch 5] Batch 2139, Loss 0.23418216407299042\n",
      "[Training Epoch 5] Batch 2140, Loss 0.26277869939804077\n",
      "[Training Epoch 5] Batch 2141, Loss 0.26076483726501465\n",
      "[Training Epoch 5] Batch 2142, Loss 0.2508004903793335\n",
      "[Training Epoch 5] Batch 2143, Loss 0.2527640461921692\n",
      "[Training Epoch 5] Batch 2144, Loss 0.2611693739891052\n",
      "[Training Epoch 5] Batch 2145, Loss 0.2633182406425476\n",
      "[Training Epoch 5] Batch 2146, Loss 0.2571619153022766\n",
      "[Training Epoch 5] Batch 2147, Loss 0.24246525764465332\n",
      "[Training Epoch 5] Batch 2148, Loss 0.24317577481269836\n",
      "[Training Epoch 5] Batch 2149, Loss 0.2491411417722702\n",
      "[Training Epoch 5] Batch 2150, Loss 0.24971334636211395\n",
      "[Training Epoch 5] Batch 2151, Loss 0.25816744565963745\n",
      "[Training Epoch 5] Batch 2152, Loss 0.2475486844778061\n",
      "[Training Epoch 5] Batch 2153, Loss 0.2674817740917206\n",
      "[Training Epoch 5] Batch 2154, Loss 0.24787163734436035\n",
      "[Training Epoch 5] Batch 2155, Loss 0.24736973643302917\n",
      "[Training Epoch 5] Batch 2156, Loss 0.2624593675136566\n",
      "[Training Epoch 5] Batch 2157, Loss 0.27197709679603577\n",
      "[Training Epoch 5] Batch 2158, Loss 0.2534816265106201\n",
      "[Training Epoch 5] Batch 2159, Loss 0.2709028422832489\n",
      "[Training Epoch 5] Batch 2160, Loss 0.2406144142150879\n",
      "[Training Epoch 5] Batch 2161, Loss 0.2694920599460602\n",
      "[Training Epoch 5] Batch 2162, Loss 0.28368788957595825\n",
      "[Training Epoch 5] Batch 2163, Loss 0.2764371335506439\n",
      "[Training Epoch 5] Batch 2164, Loss 0.24496152997016907\n",
      "[Training Epoch 5] Batch 2165, Loss 0.2722209095954895\n",
      "[Training Epoch 5] Batch 2166, Loss 0.2843198776245117\n",
      "[Training Epoch 5] Batch 2167, Loss 0.25888314843177795\n",
      "[Training Epoch 5] Batch 2168, Loss 0.261578232049942\n",
      "[Training Epoch 5] Batch 2169, Loss 0.24250119924545288\n",
      "[Training Epoch 5] Batch 2170, Loss 0.24121606349945068\n",
      "[Training Epoch 5] Batch 2171, Loss 0.22785276174545288\n",
      "[Training Epoch 5] Batch 2172, Loss 0.26551052927970886\n",
      "[Training Epoch 5] Batch 2173, Loss 0.256814181804657\n",
      "[Training Epoch 5] Batch 2174, Loss 0.27297264337539673\n",
      "[Training Epoch 5] Batch 2175, Loss 0.2848612666130066\n",
      "[Training Epoch 5] Batch 2176, Loss 0.23208889365196228\n",
      "[Training Epoch 5] Batch 2177, Loss 0.29469406604766846\n",
      "[Training Epoch 5] Batch 2178, Loss 0.2667108178138733\n",
      "[Training Epoch 5] Batch 2179, Loss 0.28393810987472534\n",
      "[Training Epoch 5] Batch 2180, Loss 0.2814730405807495\n",
      "[Training Epoch 5] Batch 2181, Loss 0.2562916874885559\n",
      "[Training Epoch 5] Batch 2182, Loss 0.2475593239068985\n",
      "[Training Epoch 5] Batch 2183, Loss 0.2897023558616638\n",
      "[Training Epoch 5] Batch 2184, Loss 0.28272053599357605\n",
      "[Training Epoch 5] Batch 2185, Loss 0.23653729259967804\n",
      "[Training Epoch 5] Batch 2186, Loss 0.28232961893081665\n",
      "[Training Epoch 5] Batch 2187, Loss 0.24355585873126984\n",
      "[Training Epoch 5] Batch 2188, Loss 0.2378847450017929\n",
      "[Training Epoch 5] Batch 2189, Loss 0.26284661889076233\n",
      "[Training Epoch 5] Batch 2190, Loss 0.23791126906871796\n",
      "[Training Epoch 5] Batch 2191, Loss 0.25373879075050354\n",
      "[Training Epoch 5] Batch 2192, Loss 0.266793429851532\n",
      "[Training Epoch 5] Batch 2193, Loss 0.2638104259967804\n",
      "[Training Epoch 5] Batch 2194, Loss 0.30375590920448303\n",
      "[Training Epoch 5] Batch 2195, Loss 0.2674115300178528\n",
      "[Training Epoch 5] Batch 2196, Loss 0.2767268717288971\n",
      "[Training Epoch 5] Batch 2197, Loss 0.2633991837501526\n",
      "[Training Epoch 5] Batch 2198, Loss 0.2572616934776306\n",
      "[Training Epoch 5] Batch 2199, Loss 0.2654738426208496\n",
      "[Training Epoch 5] Batch 2200, Loss 0.2506771683692932\n",
      "[Training Epoch 5] Batch 2201, Loss 0.2628248929977417\n",
      "[Training Epoch 5] Batch 2202, Loss 0.2799510955810547\n",
      "[Training Epoch 5] Batch 2203, Loss 0.3074987530708313\n",
      "[Training Epoch 5] Batch 2204, Loss 0.27259671688079834\n",
      "[Training Epoch 5] Batch 2205, Loss 0.23213554918766022\n",
      "[Training Epoch 5] Batch 2206, Loss 0.23711654543876648\n",
      "[Training Epoch 5] Batch 2207, Loss 0.2580665946006775\n",
      "[Training Epoch 5] Batch 2208, Loss 0.2394583523273468\n",
      "[Training Epoch 5] Batch 2209, Loss 0.2808894217014313\n",
      "[Training Epoch 5] Batch 2210, Loss 0.2475775182247162\n",
      "[Training Epoch 5] Batch 2211, Loss 0.25383174419403076\n",
      "[Training Epoch 5] Batch 2212, Loss 0.2808418273925781\n",
      "[Training Epoch 5] Batch 2213, Loss 0.28915494680404663\n",
      "[Training Epoch 5] Batch 2214, Loss 0.24008558690547943\n",
      "[Training Epoch 5] Batch 2215, Loss 0.2626146376132965\n",
      "[Training Epoch 5] Batch 2216, Loss 0.2567257285118103\n",
      "[Training Epoch 5] Batch 2217, Loss 0.2628338038921356\n",
      "[Training Epoch 5] Batch 2218, Loss 0.24191325902938843\n",
      "[Training Epoch 5] Batch 2219, Loss 0.2802782654762268\n",
      "[Training Epoch 5] Batch 2220, Loss 0.27449560165405273\n",
      "[Training Epoch 5] Batch 2221, Loss 0.2562185525894165\n",
      "[Training Epoch 5] Batch 2222, Loss 0.25920239090919495\n",
      "[Training Epoch 5] Batch 2223, Loss 0.25884169340133667\n",
      "[Training Epoch 5] Batch 2224, Loss 0.22559112310409546\n",
      "[Training Epoch 5] Batch 2225, Loss 0.2733231782913208\n",
      "[Training Epoch 5] Batch 2226, Loss 0.2473161518573761\n",
      "[Training Epoch 5] Batch 2227, Loss 0.2482750415802002\n",
      "[Training Epoch 5] Batch 2228, Loss 0.24413925409317017\n",
      "[Training Epoch 5] Batch 2229, Loss 0.25600793957710266\n",
      "[Training Epoch 5] Batch 2230, Loss 0.26882025599479675\n",
      "[Training Epoch 5] Batch 2231, Loss 0.2610039710998535\n",
      "[Training Epoch 5] Batch 2232, Loss 0.2599973678588867\n",
      "[Training Epoch 5] Batch 2233, Loss 0.2336178421974182\n",
      "[Training Epoch 5] Batch 2234, Loss 0.26057642698287964\n",
      "[Training Epoch 5] Batch 2235, Loss 0.25790560245513916\n",
      "[Training Epoch 5] Batch 2236, Loss 0.24369476735591888\n",
      "[Training Epoch 5] Batch 2237, Loss 0.2327946126461029\n",
      "[Training Epoch 5] Batch 2238, Loss 0.26049306988716125\n",
      "[Training Epoch 5] Batch 2239, Loss 0.299353688955307\n",
      "[Training Epoch 5] Batch 2240, Loss 0.25115641951560974\n",
      "[Training Epoch 5] Batch 2241, Loss 0.28095704317092896\n",
      "[Training Epoch 5] Batch 2242, Loss 0.2829627990722656\n",
      "[Training Epoch 5] Batch 2243, Loss 0.299440860748291\n",
      "[Training Epoch 5] Batch 2244, Loss 0.27294740080833435\n",
      "[Training Epoch 5] Batch 2245, Loss 0.24652956426143646\n",
      "[Training Epoch 5] Batch 2246, Loss 0.27182817459106445\n",
      "[Training Epoch 5] Batch 2247, Loss 0.27205270528793335\n",
      "[Training Epoch 5] Batch 2248, Loss 0.2796236276626587\n",
      "[Training Epoch 5] Batch 2249, Loss 0.2609429657459259\n",
      "[Training Epoch 5] Batch 2250, Loss 0.27039429545402527\n",
      "[Training Epoch 5] Batch 2251, Loss 0.2789740562438965\n",
      "[Training Epoch 5] Batch 2252, Loss 0.2868899703025818\n",
      "[Training Epoch 5] Batch 2253, Loss 0.24478521943092346\n",
      "[Training Epoch 5] Batch 2254, Loss 0.2510014772415161\n",
      "[Training Epoch 5] Batch 2255, Loss 0.263044148683548\n",
      "[Training Epoch 5] Batch 2256, Loss 0.26500004529953003\n",
      "[Training Epoch 5] Batch 2257, Loss 0.25346946716308594\n",
      "[Training Epoch 5] Batch 2258, Loss 0.2611185312271118\n",
      "[Training Epoch 5] Batch 2259, Loss 0.2375059723854065\n",
      "[Training Epoch 5] Batch 2260, Loss 0.27311694622039795\n",
      "[Training Epoch 5] Batch 2261, Loss 0.27814942598342896\n",
      "[Training Epoch 5] Batch 2262, Loss 0.2534635066986084\n",
      "[Training Epoch 5] Batch 2263, Loss 0.25399646162986755\n",
      "[Training Epoch 5] Batch 2264, Loss 0.2920771837234497\n",
      "[Training Epoch 5] Batch 2265, Loss 0.2423509657382965\n",
      "[Training Epoch 5] Batch 2266, Loss 0.2473350465297699\n",
      "[Training Epoch 5] Batch 2267, Loss 0.26713165640830994\n",
      "[Training Epoch 5] Batch 2268, Loss 0.2541912794113159\n",
      "[Training Epoch 5] Batch 2269, Loss 0.24694161117076874\n",
      "[Training Epoch 5] Batch 2270, Loss 0.25851699709892273\n",
      "[Training Epoch 5] Batch 2271, Loss 0.2657293677330017\n",
      "[Training Epoch 5] Batch 2272, Loss 0.2741405665874481\n",
      "[Training Epoch 5] Batch 2273, Loss 0.27452951669692993\n",
      "[Training Epoch 5] Batch 2274, Loss 0.28511953353881836\n",
      "[Training Epoch 5] Batch 2275, Loss 0.27150410413742065\n",
      "[Training Epoch 5] Batch 2276, Loss 0.22768628597259521\n",
      "[Training Epoch 5] Batch 2277, Loss 0.22597022354602814\n",
      "[Training Epoch 5] Batch 2278, Loss 0.24443817138671875\n",
      "[Training Epoch 5] Batch 2279, Loss 0.29269492626190186\n",
      "[Training Epoch 5] Batch 2280, Loss 0.2417043149471283\n",
      "[Training Epoch 5] Batch 2281, Loss 0.2595210075378418\n",
      "[Training Epoch 5] Batch 2282, Loss 0.2562561631202698\n",
      "[Training Epoch 5] Batch 2283, Loss 0.27002301812171936\n",
      "[Training Epoch 5] Batch 2284, Loss 0.26548224687576294\n",
      "[Training Epoch 5] Batch 2285, Loss 0.25149503350257874\n",
      "[Training Epoch 5] Batch 2286, Loss 0.299191951751709\n",
      "[Training Epoch 5] Batch 2287, Loss 0.2608969807624817\n",
      "[Training Epoch 5] Batch 2288, Loss 0.2601354122161865\n",
      "[Training Epoch 5] Batch 2289, Loss 0.2442958652973175\n",
      "[Training Epoch 5] Batch 2290, Loss 0.2677241563796997\n",
      "[Training Epoch 5] Batch 2291, Loss 0.2554154396057129\n",
      "[Training Epoch 5] Batch 2292, Loss 0.30005961656570435\n",
      "[Training Epoch 5] Batch 2293, Loss 0.25177091360092163\n",
      "[Training Epoch 5] Batch 2294, Loss 0.27117079496383667\n",
      "[Training Epoch 5] Batch 2295, Loss 0.27751126885414124\n",
      "[Training Epoch 5] Batch 2296, Loss 0.24944409728050232\n",
      "[Training Epoch 5] Batch 2297, Loss 0.28219282627105713\n",
      "[Training Epoch 5] Batch 2298, Loss 0.2681996822357178\n",
      "[Training Epoch 5] Batch 2299, Loss 0.28119152784347534\n",
      "[Training Epoch 5] Batch 2300, Loss 0.245585098862648\n",
      "[Training Epoch 5] Batch 2301, Loss 0.25861048698425293\n",
      "[Training Epoch 5] Batch 2302, Loss 0.23669500648975372\n",
      "[Training Epoch 5] Batch 2303, Loss 0.2864009737968445\n",
      "[Training Epoch 5] Batch 2304, Loss 0.24846135079860687\n",
      "[Training Epoch 5] Batch 2305, Loss 0.2653086185455322\n",
      "[Training Epoch 5] Batch 2306, Loss 0.25192344188690186\n",
      "[Training Epoch 5] Batch 2307, Loss 0.29394638538360596\n",
      "[Training Epoch 5] Batch 2308, Loss 0.2547670304775238\n",
      "[Training Epoch 5] Batch 2309, Loss 0.24680498242378235\n",
      "[Training Epoch 5] Batch 2310, Loss 0.2770860493183136\n",
      "[Training Epoch 5] Batch 2311, Loss 0.266190767288208\n",
      "[Training Epoch 5] Batch 2312, Loss 0.26176491379737854\n",
      "[Training Epoch 5] Batch 2313, Loss 0.24804073572158813\n",
      "[Training Epoch 5] Batch 2314, Loss 0.2635117769241333\n",
      "[Training Epoch 5] Batch 2315, Loss 0.2548542618751526\n",
      "[Training Epoch 5] Batch 2316, Loss 0.2570505738258362\n",
      "[Training Epoch 5] Batch 2317, Loss 0.24981746077537537\n",
      "[Training Epoch 5] Batch 2318, Loss 0.23527364432811737\n",
      "[Training Epoch 5] Batch 2319, Loss 0.2648307681083679\n",
      "[Training Epoch 5] Batch 2320, Loss 0.295654833316803\n",
      "[Training Epoch 5] Batch 2321, Loss 0.2478981614112854\n",
      "[Training Epoch 5] Batch 2322, Loss 0.26880016922950745\n",
      "[Training Epoch 5] Batch 2323, Loss 0.298123836517334\n",
      "[Training Epoch 5] Batch 2324, Loss 0.24359683692455292\n",
      "[Training Epoch 5] Batch 2325, Loss 0.2714223265647888\n",
      "[Training Epoch 5] Batch 2326, Loss 0.27814149856567383\n",
      "[Training Epoch 5] Batch 2327, Loss 0.2864418029785156\n",
      "[Training Epoch 5] Batch 2328, Loss 0.2588937282562256\n",
      "[Training Epoch 5] Batch 2329, Loss 0.26520562171936035\n",
      "[Training Epoch 5] Batch 2330, Loss 0.2799474596977234\n",
      "[Training Epoch 5] Batch 2331, Loss 0.2479960024356842\n",
      "[Training Epoch 5] Batch 2332, Loss 0.27426934242248535\n",
      "[Training Epoch 5] Batch 2333, Loss 0.2583039402961731\n",
      "[Training Epoch 5] Batch 2334, Loss 0.2612488269805908\n",
      "[Training Epoch 5] Batch 2335, Loss 0.2685166299343109\n",
      "[Training Epoch 5] Batch 2336, Loss 0.27183979749679565\n",
      "[Training Epoch 5] Batch 2337, Loss 0.2354636788368225\n",
      "[Training Epoch 5] Batch 2338, Loss 0.25294336676597595\n",
      "[Training Epoch 5] Batch 2339, Loss 0.27104318141937256\n",
      "[Training Epoch 5] Batch 2340, Loss 0.2798008620738983\n",
      "[Training Epoch 5] Batch 2341, Loss 0.24387314915657043\n",
      "[Training Epoch 5] Batch 2342, Loss 0.2606304883956909\n",
      "[Training Epoch 5] Batch 2343, Loss 0.24593952298164368\n",
      "[Training Epoch 5] Batch 2344, Loss 0.274913489818573\n",
      "[Training Epoch 5] Batch 2345, Loss 0.27683529257774353\n",
      "[Training Epoch 5] Batch 2346, Loss 0.25300559401512146\n",
      "[Training Epoch 5] Batch 2347, Loss 0.2643400728702545\n",
      "[Training Epoch 5] Batch 2348, Loss 0.24801263213157654\n",
      "[Training Epoch 5] Batch 2349, Loss 0.24789837002754211\n",
      "[Training Epoch 5] Batch 2350, Loss 0.2509523034095764\n",
      "[Training Epoch 5] Batch 2351, Loss 0.24834713339805603\n",
      "[Training Epoch 5] Batch 2352, Loss 0.2621915340423584\n",
      "[Training Epoch 5] Batch 2353, Loss 0.24455994367599487\n",
      "[Training Epoch 5] Batch 2354, Loss 0.25756192207336426\n",
      "[Training Epoch 5] Batch 2355, Loss 0.25866568088531494\n",
      "[Training Epoch 5] Batch 2356, Loss 0.29064324498176575\n",
      "[Training Epoch 5] Batch 2357, Loss 0.2542232275009155\n",
      "[Training Epoch 5] Batch 2358, Loss 0.2728217840194702\n",
      "[Training Epoch 5] Batch 2359, Loss 0.2534855902194977\n",
      "[Training Epoch 5] Batch 2360, Loss 0.25606054067611694\n",
      "[Training Epoch 5] Batch 2361, Loss 0.2530903220176697\n",
      "[Training Epoch 5] Batch 2362, Loss 0.2575509250164032\n",
      "[Training Epoch 5] Batch 2363, Loss 0.2664971947669983\n",
      "[Training Epoch 5] Batch 2364, Loss 0.2483448088169098\n",
      "[Training Epoch 5] Batch 2365, Loss 0.2769753336906433\n",
      "[Training Epoch 5] Batch 2366, Loss 0.2529456615447998\n",
      "[Training Epoch 5] Batch 2367, Loss 0.23992592096328735\n",
      "[Training Epoch 5] Batch 2368, Loss 0.276668518781662\n",
      "[Training Epoch 5] Batch 2369, Loss 0.2692677676677704\n",
      "[Training Epoch 5] Batch 2370, Loss 0.27834925055503845\n",
      "[Training Epoch 5] Batch 2371, Loss 0.24834440648555756\n",
      "[Training Epoch 5] Batch 2372, Loss 0.2728056311607361\n",
      "[Training Epoch 5] Batch 2373, Loss 0.27391916513442993\n",
      "[Training Epoch 5] Batch 2374, Loss 0.2552701532840729\n",
      "[Training Epoch 5] Batch 2375, Loss 0.2928326427936554\n",
      "[Training Epoch 5] Batch 2376, Loss 0.26881521940231323\n",
      "[Training Epoch 5] Batch 2377, Loss 0.26004189252853394\n",
      "[Training Epoch 5] Batch 2378, Loss 0.26273152232170105\n",
      "[Training Epoch 5] Batch 2379, Loss 0.25900256633758545\n",
      "[Training Epoch 5] Batch 2380, Loss 0.2496834546327591\n",
      "[Training Epoch 5] Batch 2381, Loss 0.25236451625823975\n",
      "[Training Epoch 5] Batch 2382, Loss 0.27052944898605347\n",
      "[Training Epoch 5] Batch 2383, Loss 0.2610812783241272\n",
      "[Training Epoch 5] Batch 2384, Loss 0.25209319591522217\n",
      "[Training Epoch 5] Batch 2385, Loss 0.27615243196487427\n",
      "[Training Epoch 5] Batch 2386, Loss 0.2470993995666504\n",
      "[Training Epoch 5] Batch 2387, Loss 0.26660358905792236\n",
      "[Training Epoch 5] Batch 2388, Loss 0.23663002252578735\n",
      "[Training Epoch 5] Batch 2389, Loss 0.24825605750083923\n",
      "[Training Epoch 5] Batch 2390, Loss 0.2960713505744934\n",
      "[Training Epoch 5] Batch 2391, Loss 0.2542838454246521\n",
      "[Training Epoch 5] Batch 2392, Loss 0.25634557008743286\n",
      "[Training Epoch 5] Batch 2393, Loss 0.26516568660736084\n",
      "[Training Epoch 5] Batch 2394, Loss 0.2868741750717163\n",
      "[Training Epoch 5] Batch 2395, Loss 0.2440938651561737\n",
      "[Training Epoch 5] Batch 2396, Loss 0.27811646461486816\n",
      "[Training Epoch 5] Batch 2397, Loss 0.2502884268760681\n",
      "[Training Epoch 5] Batch 2398, Loss 0.26145029067993164\n",
      "[Training Epoch 5] Batch 2399, Loss 0.2533336877822876\n",
      "[Training Epoch 5] Batch 2400, Loss 0.2859954833984375\n",
      "[Training Epoch 5] Batch 2401, Loss 0.25295063853263855\n",
      "[Training Epoch 5] Batch 2402, Loss 0.2666366696357727\n",
      "[Training Epoch 5] Batch 2403, Loss 0.2603214979171753\n",
      "[Training Epoch 5] Batch 2404, Loss 0.2766749858856201\n",
      "[Training Epoch 5] Batch 2405, Loss 0.2690381407737732\n",
      "[Training Epoch 5] Batch 2406, Loss 0.2638246417045593\n",
      "[Training Epoch 5] Batch 2407, Loss 0.2462407648563385\n",
      "[Training Epoch 5] Batch 2408, Loss 0.26515915989875793\n",
      "[Training Epoch 5] Batch 2409, Loss 0.2723729610443115\n",
      "[Training Epoch 5] Batch 2410, Loss 0.27145490050315857\n",
      "[Training Epoch 5] Batch 2411, Loss 0.28702735900878906\n",
      "[Training Epoch 5] Batch 2412, Loss 0.24305690824985504\n",
      "[Training Epoch 5] Batch 2413, Loss 0.2647727131843567\n",
      "[Training Epoch 5] Batch 2414, Loss 0.29590994119644165\n",
      "[Training Epoch 5] Batch 2415, Loss 0.2531306743621826\n",
      "[Training Epoch 5] Batch 2416, Loss 0.2511260509490967\n",
      "[Training Epoch 5] Batch 2417, Loss 0.28297188878059387\n",
      "[Training Epoch 5] Batch 2418, Loss 0.282274454832077\n",
      "[Training Epoch 5] Batch 2419, Loss 0.24214045703411102\n",
      "[Training Epoch 5] Batch 2420, Loss 0.2619044780731201\n",
      "[Training Epoch 5] Batch 2421, Loss 0.2603731155395508\n",
      "[Training Epoch 5] Batch 2422, Loss 0.24362961947917938\n",
      "[Training Epoch 5] Batch 2423, Loss 0.27310705184936523\n",
      "[Training Epoch 5] Batch 2424, Loss 0.2592938244342804\n",
      "[Training Epoch 5] Batch 2425, Loss 0.28680601716041565\n",
      "[Training Epoch 5] Batch 2426, Loss 0.25084054470062256\n",
      "[Training Epoch 5] Batch 2427, Loss 0.2649947702884674\n",
      "[Training Epoch 5] Batch 2428, Loss 0.2591492533683777\n",
      "[Training Epoch 5] Batch 2429, Loss 0.3042711317539215\n",
      "[Training Epoch 5] Batch 2430, Loss 0.26140111684799194\n",
      "[Training Epoch 5] Batch 2431, Loss 0.2610305845737457\n",
      "[Training Epoch 5] Batch 2432, Loss 0.26080822944641113\n",
      "[Training Epoch 5] Batch 2433, Loss 0.24765029549598694\n",
      "[Training Epoch 5] Batch 2434, Loss 0.26834601163864136\n",
      "[Training Epoch 5] Batch 2435, Loss 0.24199579656124115\n",
      "[Training Epoch 5] Batch 2436, Loss 0.2544771432876587\n",
      "[Training Epoch 5] Batch 2437, Loss 0.26156944036483765\n",
      "[Training Epoch 5] Batch 2438, Loss 0.2722262740135193\n",
      "[Training Epoch 5] Batch 2439, Loss 0.25671809911727905\n",
      "[Training Epoch 5] Batch 2440, Loss 0.2498677670955658\n",
      "[Training Epoch 5] Batch 2441, Loss 0.27057206630706787\n",
      "[Training Epoch 5] Batch 2442, Loss 0.25663092732429504\n",
      "[Training Epoch 5] Batch 2443, Loss 0.2688336968421936\n",
      "[Training Epoch 5] Batch 2444, Loss 0.27525967359542847\n",
      "[Training Epoch 5] Batch 2445, Loss 0.24098749458789825\n",
      "[Training Epoch 5] Batch 2446, Loss 0.2852722406387329\n",
      "[Training Epoch 5] Batch 2447, Loss 0.25383591651916504\n",
      "[Training Epoch 5] Batch 2448, Loss 0.2646292448043823\n",
      "[Training Epoch 5] Batch 2449, Loss 0.2617154121398926\n",
      "[Training Epoch 5] Batch 2450, Loss 0.2459992915391922\n",
      "[Training Epoch 5] Batch 2451, Loss 0.2576597034931183\n",
      "[Training Epoch 5] Batch 2452, Loss 0.25953978300094604\n",
      "[Training Epoch 5] Batch 2453, Loss 0.2511337399482727\n",
      "[Training Epoch 5] Batch 2454, Loss 0.2749313414096832\n",
      "[Training Epoch 5] Batch 2455, Loss 0.24478541314601898\n",
      "[Training Epoch 5] Batch 2456, Loss 0.2834627628326416\n",
      "[Training Epoch 5] Batch 2457, Loss 0.284203439950943\n",
      "[Training Epoch 5] Batch 2458, Loss 0.2752473056316376\n",
      "[Training Epoch 5] Batch 2459, Loss 0.27200937271118164\n",
      "[Training Epoch 5] Batch 2460, Loss 0.24404597282409668\n",
      "[Training Epoch 5] Batch 2461, Loss 0.25735336542129517\n",
      "[Training Epoch 5] Batch 2462, Loss 0.29876115918159485\n",
      "[Training Epoch 5] Batch 2463, Loss 0.26807132363319397\n",
      "[Training Epoch 5] Batch 2464, Loss 0.2803034782409668\n",
      "[Training Epoch 5] Batch 2465, Loss 0.25517624616622925\n",
      "[Training Epoch 5] Batch 2466, Loss 0.27217087149620056\n",
      "[Training Epoch 5] Batch 2467, Loss 0.2885324954986572\n",
      "[Training Epoch 5] Batch 2468, Loss 0.2780992388725281\n",
      "[Training Epoch 5] Batch 2469, Loss 0.2649623155593872\n",
      "[Training Epoch 5] Batch 2470, Loss 0.24815629422664642\n",
      "[Training Epoch 5] Batch 2471, Loss 0.2840242385864258\n",
      "[Training Epoch 5] Batch 2472, Loss 0.24020913243293762\n",
      "[Training Epoch 5] Batch 2473, Loss 0.29572317004203796\n",
      "[Training Epoch 5] Batch 2474, Loss 0.252156525850296\n",
      "[Training Epoch 5] Batch 2475, Loss 0.2541937828063965\n",
      "[Training Epoch 5] Batch 2476, Loss 0.26158609986305237\n",
      "[Training Epoch 5] Batch 2477, Loss 0.26154056191444397\n",
      "[Training Epoch 5] Batch 2478, Loss 0.29210537672042847\n",
      "[Training Epoch 5] Batch 2479, Loss 0.27066078782081604\n",
      "[Training Epoch 5] Batch 2480, Loss 0.2544935345649719\n",
      "[Training Epoch 5] Batch 2481, Loss 0.24761630594730377\n",
      "[Training Epoch 5] Batch 2482, Loss 0.2513003945350647\n",
      "[Training Epoch 5] Batch 2483, Loss 0.2638906240463257\n",
      "[Training Epoch 5] Batch 2484, Loss 0.2838912904262543\n",
      "[Training Epoch 5] Batch 2485, Loss 0.2509726285934448\n",
      "[Training Epoch 5] Batch 2486, Loss 0.2800791561603546\n",
      "[Training Epoch 5] Batch 2487, Loss 0.24766477942466736\n",
      "[Training Epoch 5] Batch 2488, Loss 0.2733660638332367\n",
      "[Training Epoch 5] Batch 2489, Loss 0.24630214273929596\n",
      "[Training Epoch 5] Batch 2490, Loss 0.24601507186889648\n",
      "[Training Epoch 5] Batch 2491, Loss 0.26473358273506165\n",
      "[Training Epoch 5] Batch 2492, Loss 0.2708369791507721\n",
      "[Training Epoch 5] Batch 2493, Loss 0.23686553537845612\n",
      "[Training Epoch 5] Batch 2494, Loss 0.24292516708374023\n",
      "[Training Epoch 5] Batch 2495, Loss 0.3005913496017456\n",
      "[Training Epoch 5] Batch 2496, Loss 0.2555737793445587\n",
      "[Training Epoch 5] Batch 2497, Loss 0.2738686203956604\n",
      "[Training Epoch 5] Batch 2498, Loss 0.24585950374603271\n",
      "[Training Epoch 5] Batch 2499, Loss 0.23400402069091797\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2679271101951599\n",
      "[Training Epoch 5] Batch 2501, Loss 0.25721973180770874\n",
      "[Training Epoch 5] Batch 2502, Loss 0.27404287457466125\n",
      "[Training Epoch 5] Batch 2503, Loss 0.2646790146827698\n",
      "[Training Epoch 5] Batch 2504, Loss 0.2788436710834503\n",
      "[Training Epoch 5] Batch 2505, Loss 0.2663685083389282\n",
      "[Training Epoch 5] Batch 2506, Loss 0.2445400506258011\n",
      "[Training Epoch 5] Batch 2507, Loss 0.254239022731781\n",
      "[Training Epoch 5] Batch 2508, Loss 0.2744107246398926\n",
      "[Training Epoch 5] Batch 2509, Loss 0.26573580503463745\n",
      "[Training Epoch 5] Batch 2510, Loss 0.2620522975921631\n",
      "[Training Epoch 5] Batch 2511, Loss 0.2628430426120758\n",
      "[Training Epoch 5] Batch 2512, Loss 0.23449672758579254\n",
      "[Training Epoch 5] Batch 2513, Loss 0.2937433421611786\n",
      "[Training Epoch 5] Batch 2514, Loss 0.2650839388370514\n",
      "[Training Epoch 5] Batch 2515, Loss 0.24036356806755066\n",
      "[Training Epoch 5] Batch 2516, Loss 0.24619832634925842\n",
      "[Training Epoch 5] Batch 2517, Loss 0.2524644136428833\n",
      "[Training Epoch 5] Batch 2518, Loss 0.2833592891693115\n",
      "[Training Epoch 5] Batch 2519, Loss 0.2519647479057312\n",
      "[Training Epoch 5] Batch 2520, Loss 0.2513371706008911\n",
      "[Training Epoch 5] Batch 2521, Loss 0.2574717700481415\n",
      "[Training Epoch 5] Batch 2522, Loss 0.2399858832359314\n",
      "[Training Epoch 5] Batch 2523, Loss 0.2363145351409912\n",
      "[Training Epoch 5] Batch 2524, Loss 0.24681927263736725\n",
      "[Training Epoch 5] Batch 2525, Loss 0.2921432554721832\n",
      "[Training Epoch 5] Batch 2526, Loss 0.23736047744750977\n",
      "[Training Epoch 5] Batch 2527, Loss 0.27199533581733704\n",
      "[Training Epoch 5] Batch 2528, Loss 0.2791135311126709\n",
      "[Training Epoch 5] Batch 2529, Loss 0.2891882061958313\n",
      "[Training Epoch 5] Batch 2530, Loss 0.2534170150756836\n",
      "[Training Epoch 5] Batch 2531, Loss 0.2516343593597412\n",
      "[Training Epoch 5] Batch 2532, Loss 0.2611476182937622\n",
      "[Training Epoch 5] Batch 2533, Loss 0.26881641149520874\n",
      "[Training Epoch 5] Batch 2534, Loss 0.2369874119758606\n",
      "[Training Epoch 5] Batch 2535, Loss 0.27660536766052246\n",
      "[Training Epoch 5] Batch 2536, Loss 0.24403102695941925\n",
      "[Training Epoch 5] Batch 2537, Loss 0.25177931785583496\n",
      "[Training Epoch 5] Batch 2538, Loss 0.2879065275192261\n",
      "[Training Epoch 5] Batch 2539, Loss 0.24885699152946472\n",
      "[Training Epoch 5] Batch 2540, Loss 0.26455390453338623\n",
      "[Training Epoch 5] Batch 2541, Loss 0.24819648265838623\n",
      "[Training Epoch 5] Batch 2542, Loss 0.2802123427391052\n",
      "[Training Epoch 5] Batch 2543, Loss 0.2678202986717224\n",
      "[Training Epoch 5] Batch 2544, Loss 0.2810988426208496\n",
      "[Training Epoch 5] Batch 2545, Loss 0.2502061724662781\n",
      "[Training Epoch 5] Batch 2546, Loss 0.27886080741882324\n",
      "[Training Epoch 5] Batch 2547, Loss 0.2499156892299652\n",
      "[Training Epoch 5] Batch 2548, Loss 0.26612770557403564\n",
      "[Training Epoch 5] Batch 2549, Loss 0.3010812997817993\n",
      "[Training Epoch 5] Batch 2550, Loss 0.25368455052375793\n",
      "[Training Epoch 5] Batch 2551, Loss 0.2524743974208832\n",
      "[Training Epoch 5] Batch 2552, Loss 0.2411344349384308\n",
      "[Training Epoch 5] Batch 2553, Loss 0.2509949803352356\n",
      "[Training Epoch 5] Batch 2554, Loss 0.2635805010795593\n",
      "[Training Epoch 5] Batch 2555, Loss 0.2853684425354004\n",
      "[Training Epoch 5] Batch 2556, Loss 0.25890052318573\n",
      "[Training Epoch 5] Batch 2557, Loss 0.2347458302974701\n",
      "[Training Epoch 5] Batch 2558, Loss 0.2557768225669861\n",
      "[Training Epoch 5] Batch 2559, Loss 0.25001949071884155\n",
      "[Training Epoch 5] Batch 2560, Loss 0.28489071130752563\n",
      "[Training Epoch 5] Batch 2561, Loss 0.2653268873691559\n",
      "[Training Epoch 5] Batch 2562, Loss 0.263245165348053\n",
      "[Training Epoch 5] Batch 2563, Loss 0.2841023802757263\n",
      "[Training Epoch 5] Batch 2564, Loss 0.24649494886398315\n",
      "[Training Epoch 5] Batch 2565, Loss 0.22783902287483215\n",
      "[Training Epoch 5] Batch 2566, Loss 0.2554488182067871\n",
      "[Training Epoch 5] Batch 2567, Loss 0.250667929649353\n",
      "[Training Epoch 5] Batch 2568, Loss 0.23892399668693542\n",
      "[Training Epoch 5] Batch 2569, Loss 0.2704196870326996\n",
      "[Training Epoch 5] Batch 2570, Loss 0.2536200284957886\n",
      "[Training Epoch 5] Batch 2571, Loss 0.2844686508178711\n",
      "[Training Epoch 5] Batch 2572, Loss 0.2712261974811554\n",
      "[Training Epoch 5] Batch 2573, Loss 0.25438663363456726\n",
      "[Training Epoch 5] Batch 2574, Loss 0.26645147800445557\n",
      "[Training Epoch 5] Batch 2575, Loss 0.278598815202713\n",
      "[Training Epoch 5] Batch 2576, Loss 0.2676616311073303\n",
      "[Training Epoch 5] Batch 2577, Loss 0.2740451693534851\n",
      "[Training Epoch 5] Batch 2578, Loss 0.26938748359680176\n",
      "[Training Epoch 5] Batch 2579, Loss 0.26994842290878296\n",
      "[Training Epoch 5] Batch 2580, Loss 0.2536275386810303\n",
      "[Training Epoch 5] Batch 2581, Loss 0.252474844455719\n",
      "[Training Epoch 5] Batch 2582, Loss 0.2543691396713257\n",
      "[Training Epoch 5] Batch 2583, Loss 0.2440299689769745\n",
      "[Training Epoch 5] Batch 2584, Loss 0.2615584433078766\n",
      "[Training Epoch 5] Batch 2585, Loss 0.27369725704193115\n",
      "[Training Epoch 5] Batch 2586, Loss 0.2615516781806946\n",
      "[Training Epoch 5] Batch 2587, Loss 0.26375049352645874\n",
      "[Training Epoch 5] Batch 2588, Loss 0.2500861585140228\n",
      "[Training Epoch 5] Batch 2589, Loss 0.25561848282814026\n",
      "[Training Epoch 5] Batch 2590, Loss 0.26492172479629517\n",
      "[Training Epoch 5] Batch 2591, Loss 0.24955269694328308\n",
      "[Training Epoch 5] Batch 2592, Loss 0.24761664867401123\n",
      "[Training Epoch 5] Batch 2593, Loss 0.23457562923431396\n",
      "[Training Epoch 5] Batch 2594, Loss 0.28168269991874695\n",
      "[Training Epoch 5] Batch 2595, Loss 0.27394071221351624\n",
      "[Training Epoch 5] Batch 2596, Loss 0.2749435305595398\n",
      "[Training Epoch 5] Batch 2597, Loss 0.26375824213027954\n",
      "[Training Epoch 5] Batch 2598, Loss 0.2444303333759308\n",
      "[Training Epoch 5] Batch 2599, Loss 0.2985113859176636\n",
      "[Training Epoch 5] Batch 2600, Loss 0.2800964117050171\n",
      "[Training Epoch 5] Batch 2601, Loss 0.23155146837234497\n",
      "[Training Epoch 5] Batch 2602, Loss 0.24621574580669403\n",
      "[Training Epoch 5] Batch 2603, Loss 0.27483806014060974\n",
      "[Training Epoch 5] Batch 2604, Loss 0.25759005546569824\n",
      "[Training Epoch 5] Batch 2605, Loss 0.28594446182250977\n",
      "[Training Epoch 5] Batch 2606, Loss 0.23928797245025635\n",
      "[Training Epoch 5] Batch 2607, Loss 0.2595616281032562\n",
      "[Training Epoch 5] Batch 2608, Loss 0.24640558660030365\n",
      "[Training Epoch 5] Batch 2609, Loss 0.25422313809394836\n",
      "[Training Epoch 5] Batch 2610, Loss 0.26371318101882935\n",
      "[Training Epoch 5] Batch 2611, Loss 0.23287734389305115\n",
      "[Training Epoch 5] Batch 2612, Loss 0.25707775354385376\n",
      "[Training Epoch 5] Batch 2613, Loss 0.27043426036834717\n",
      "[Training Epoch 5] Batch 2614, Loss 0.2698720395565033\n",
      "[Training Epoch 5] Batch 2615, Loss 0.26046329736709595\n",
      "[Training Epoch 5] Batch 2616, Loss 0.2822544574737549\n",
      "[Training Epoch 5] Batch 2617, Loss 0.235894113779068\n",
      "[Training Epoch 5] Batch 2618, Loss 0.24015192687511444\n",
      "[Training Epoch 5] Batch 2619, Loss 0.2563258409500122\n",
      "[Training Epoch 5] Batch 2620, Loss 0.2583591341972351\n",
      "[Training Epoch 5] Batch 2621, Loss 0.28407931327819824\n",
      "[Training Epoch 5] Batch 2622, Loss 0.2578648626804352\n",
      "[Training Epoch 5] Batch 2623, Loss 0.26819145679473877\n",
      "[Training Epoch 5] Batch 2624, Loss 0.28069090843200684\n",
      "[Training Epoch 5] Batch 2625, Loss 0.24703918397426605\n",
      "[Training Epoch 5] Batch 2626, Loss 0.2668081521987915\n",
      "[Training Epoch 5] Batch 2627, Loss 0.2331784963607788\n",
      "[Training Epoch 5] Batch 2628, Loss 0.24509300291538239\n",
      "[Training Epoch 5] Batch 2629, Loss 0.23409944772720337\n",
      "[Training Epoch 5] Batch 2630, Loss 0.2766241729259491\n",
      "[Training Epoch 5] Batch 2631, Loss 0.2637521028518677\n",
      "[Training Epoch 5] Batch 2632, Loss 0.25908464193344116\n",
      "[Training Epoch 5] Batch 2633, Loss 0.24108003079891205\n",
      "[Training Epoch 5] Batch 2634, Loss 0.2550885081291199\n",
      "[Training Epoch 5] Batch 2635, Loss 0.232634499669075\n",
      "[Training Epoch 5] Batch 2636, Loss 0.2519087791442871\n",
      "[Training Epoch 5] Batch 2637, Loss 0.24256621301174164\n",
      "[Training Epoch 5] Batch 2638, Loss 0.23039627075195312\n",
      "[Training Epoch 5] Batch 2639, Loss 0.27083373069763184\n",
      "[Training Epoch 5] Batch 2640, Loss 0.25019004940986633\n",
      "[Training Epoch 5] Batch 2641, Loss 0.26243337988853455\n",
      "[Training Epoch 5] Batch 2642, Loss 0.24870553612709045\n",
      "[Training Epoch 5] Batch 2643, Loss 0.2866102457046509\n",
      "[Training Epoch 5] Batch 2644, Loss 0.2609213590621948\n",
      "[Training Epoch 5] Batch 2645, Loss 0.23353245854377747\n",
      "[Training Epoch 5] Batch 2646, Loss 0.28860583901405334\n",
      "[Training Epoch 5] Batch 2647, Loss 0.2564046084880829\n",
      "[Training Epoch 5] Batch 2648, Loss 0.24690456688404083\n",
      "[Training Epoch 5] Batch 2649, Loss 0.2672158479690552\n",
      "[Training Epoch 5] Batch 2650, Loss 0.26348501443862915\n",
      "[Training Epoch 5] Batch 2651, Loss 0.2537451982498169\n",
      "[Training Epoch 5] Batch 2652, Loss 0.2552417516708374\n",
      "[Training Epoch 5] Batch 2653, Loss 0.27245983481407166\n",
      "[Training Epoch 5] Batch 2654, Loss 0.23898454010486603\n",
      "[Training Epoch 5] Batch 2655, Loss 0.24642117321491241\n",
      "[Training Epoch 5] Batch 2656, Loss 0.2680623531341553\n",
      "[Training Epoch 5] Batch 2657, Loss 0.304412305355072\n",
      "[Training Epoch 5] Batch 2658, Loss 0.28085386753082275\n",
      "[Training Epoch 5] Batch 2659, Loss 0.26122432947158813\n",
      "[Training Epoch 5] Batch 2660, Loss 0.2524157166481018\n",
      "[Training Epoch 5] Batch 2661, Loss 0.24920901656150818\n",
      "[Training Epoch 5] Batch 2662, Loss 0.26476582884788513\n",
      "[Training Epoch 5] Batch 2663, Loss 0.251306414604187\n",
      "[Training Epoch 5] Batch 2664, Loss 0.22118330001831055\n",
      "[Training Epoch 5] Batch 2665, Loss 0.2718919813632965\n",
      "[Training Epoch 5] Batch 2666, Loss 0.29187434911727905\n",
      "[Training Epoch 5] Batch 2667, Loss 0.23914840817451477\n",
      "[Training Epoch 5] Batch 2668, Loss 0.25444507598876953\n",
      "[Training Epoch 5] Batch 2669, Loss 0.2800353169441223\n",
      "[Training Epoch 5] Batch 2670, Loss 0.28132128715515137\n",
      "[Training Epoch 5] Batch 2671, Loss 0.2817666530609131\n",
      "[Training Epoch 5] Batch 2672, Loss 0.26698800921440125\n",
      "[Training Epoch 5] Batch 2673, Loss 0.3058947026729584\n",
      "[Training Epoch 5] Batch 2674, Loss 0.26386386156082153\n",
      "[Training Epoch 5] Batch 2675, Loss 0.2741422951221466\n",
      "[Training Epoch 5] Batch 2676, Loss 0.2725232243537903\n",
      "[Training Epoch 5] Batch 2677, Loss 0.25927603244781494\n",
      "[Training Epoch 5] Batch 2678, Loss 0.2614113688468933\n",
      "[Training Epoch 5] Batch 2679, Loss 0.2570480704307556\n",
      "[Training Epoch 5] Batch 2680, Loss 0.2434166669845581\n",
      "[Training Epoch 5] Batch 2681, Loss 0.25742924213409424\n",
      "[Training Epoch 5] Batch 2682, Loss 0.2547195255756378\n",
      "[Training Epoch 5] Batch 2683, Loss 0.26056304574012756\n",
      "[Training Epoch 5] Batch 2684, Loss 0.24298205971717834\n",
      "[Training Epoch 5] Batch 2685, Loss 0.23560036718845367\n",
      "[Training Epoch 5] Batch 2686, Loss 0.25463882088661194\n",
      "[Training Epoch 5] Batch 2687, Loss 0.25117751955986023\n",
      "[Training Epoch 5] Batch 2688, Loss 0.262509822845459\n",
      "[Training Epoch 5] Batch 2689, Loss 0.27447062730789185\n",
      "[Training Epoch 5] Batch 2690, Loss 0.2535845637321472\n",
      "[Training Epoch 5] Batch 2691, Loss 0.2840901017189026\n",
      "[Training Epoch 5] Batch 2692, Loss 0.2651258409023285\n",
      "[Training Epoch 5] Batch 2693, Loss 0.23546868562698364\n",
      "[Training Epoch 5] Batch 2694, Loss 0.267788827419281\n",
      "[Training Epoch 5] Batch 2695, Loss 0.29252058267593384\n",
      "[Training Epoch 5] Batch 2696, Loss 0.26697808504104614\n",
      "[Training Epoch 5] Batch 2697, Loss 0.23472166061401367\n",
      "[Training Epoch 5] Batch 2698, Loss 0.27677178382873535\n",
      "[Training Epoch 5] Batch 2699, Loss 0.26116323471069336\n",
      "[Training Epoch 5] Batch 2700, Loss 0.2770116329193115\n",
      "[Training Epoch 5] Batch 2701, Loss 0.2609633505344391\n",
      "[Training Epoch 5] Batch 2702, Loss 0.25964635610580444\n",
      "[Training Epoch 5] Batch 2703, Loss 0.23821264505386353\n",
      "[Training Epoch 5] Batch 2704, Loss 0.27443727850914\n",
      "[Training Epoch 5] Batch 2705, Loss 0.24450638890266418\n",
      "[Training Epoch 5] Batch 2706, Loss 0.25607869029045105\n",
      "[Training Epoch 5] Batch 2707, Loss 0.2560955286026001\n",
      "[Training Epoch 5] Batch 2708, Loss 0.24891209602355957\n",
      "[Training Epoch 5] Batch 2709, Loss 0.2504298985004425\n",
      "[Training Epoch 5] Batch 2710, Loss 0.2383619248867035\n",
      "[Training Epoch 5] Batch 2711, Loss 0.2723504304885864\n",
      "[Training Epoch 5] Batch 2712, Loss 0.25258782505989075\n",
      "[Training Epoch 5] Batch 2713, Loss 0.2577609717845917\n",
      "[Training Epoch 5] Batch 2714, Loss 0.2668899893760681\n",
      "[Training Epoch 5] Batch 2715, Loss 0.23156411945819855\n",
      "[Training Epoch 5] Batch 2716, Loss 0.2469983696937561\n",
      "[Training Epoch 5] Batch 2717, Loss 0.2627393305301666\n",
      "[Training Epoch 5] Batch 2718, Loss 0.23899947106838226\n",
      "[Training Epoch 5] Batch 2719, Loss 0.24838699400424957\n",
      "[Training Epoch 5] Batch 2720, Loss 0.2673552632331848\n",
      "[Training Epoch 5] Batch 2721, Loss 0.2899741232395172\n",
      "[Training Epoch 5] Batch 2722, Loss 0.2486010193824768\n",
      "[Training Epoch 5] Batch 2723, Loss 0.27373313903808594\n",
      "[Training Epoch 5] Batch 2724, Loss 0.2604086399078369\n",
      "[Training Epoch 5] Batch 2725, Loss 0.2674095332622528\n",
      "[Training Epoch 5] Batch 2726, Loss 0.26681017875671387\n",
      "[Training Epoch 5] Batch 2727, Loss 0.2700892984867096\n",
      "[Training Epoch 5] Batch 2728, Loss 0.26311439275741577\n",
      "[Training Epoch 5] Batch 2729, Loss 0.2588101625442505\n",
      "[Training Epoch 5] Batch 2730, Loss 0.25405269861221313\n",
      "[Training Epoch 5] Batch 2731, Loss 0.26982957124710083\n",
      "[Training Epoch 5] Batch 2732, Loss 0.24582752585411072\n",
      "[Training Epoch 5] Batch 2733, Loss 0.29098302125930786\n",
      "[Training Epoch 5] Batch 2734, Loss 0.2737705111503601\n",
      "[Training Epoch 5] Batch 2735, Loss 0.2617058753967285\n",
      "[Training Epoch 5] Batch 2736, Loss 0.2503964304924011\n",
      "[Training Epoch 5] Batch 2737, Loss 0.2589350640773773\n",
      "[Training Epoch 5] Batch 2738, Loss 0.26952964067459106\n",
      "[Training Epoch 5] Batch 2739, Loss 0.27255821228027344\n",
      "[Training Epoch 5] Batch 2740, Loss 0.297036737203598\n",
      "[Training Epoch 5] Batch 2741, Loss 0.2657923996448517\n",
      "[Training Epoch 5] Batch 2742, Loss 0.25761765241622925\n",
      "[Training Epoch 5] Batch 2743, Loss 0.27939414978027344\n",
      "[Training Epoch 5] Batch 2744, Loss 0.24706053733825684\n",
      "[Training Epoch 5] Batch 2745, Loss 0.26174601912498474\n",
      "[Training Epoch 5] Batch 2746, Loss 0.24992483854293823\n",
      "[Training Epoch 5] Batch 2747, Loss 0.265996515750885\n",
      "[Training Epoch 5] Batch 2748, Loss 0.25582021474838257\n",
      "[Training Epoch 5] Batch 2749, Loss 0.2612353563308716\n",
      "[Training Epoch 5] Batch 2750, Loss 0.23840191960334778\n",
      "[Training Epoch 5] Batch 2751, Loss 0.24887138605117798\n",
      "[Training Epoch 5] Batch 2752, Loss 0.25169575214385986\n",
      "[Training Epoch 5] Batch 2753, Loss 0.2661353349685669\n",
      "[Training Epoch 5] Batch 2754, Loss 0.2860667109489441\n",
      "[Training Epoch 5] Batch 2755, Loss 0.2656414210796356\n",
      "[Training Epoch 5] Batch 2756, Loss 0.24464741349220276\n",
      "[Training Epoch 5] Batch 2757, Loss 0.2539556622505188\n",
      "[Training Epoch 5] Batch 2758, Loss 0.2476024329662323\n",
      "[Training Epoch 5] Batch 2759, Loss 0.2714705169200897\n",
      "[Training Epoch 5] Batch 2760, Loss 0.24397748708724976\n",
      "[Training Epoch 5] Batch 2761, Loss 0.2529735863208771\n",
      "[Training Epoch 5] Batch 2762, Loss 0.2599566876888275\n",
      "[Training Epoch 5] Batch 2763, Loss 0.26675570011138916\n",
      "[Training Epoch 5] Batch 2764, Loss 0.2439003437757492\n",
      "[Training Epoch 5] Batch 2765, Loss 0.268604576587677\n",
      "[Training Epoch 5] Batch 2766, Loss 0.22614030539989471\n",
      "[Training Epoch 5] Batch 2767, Loss 0.26277461647987366\n",
      "[Training Epoch 5] Batch 2768, Loss 0.23719580471515656\n",
      "[Training Epoch 5] Batch 2769, Loss 0.25301647186279297\n",
      "[Training Epoch 5] Batch 2770, Loss 0.2583427131175995\n",
      "[Training Epoch 5] Batch 2771, Loss 0.29285919666290283\n",
      "[Training Epoch 5] Batch 2772, Loss 0.2874641418457031\n",
      "[Training Epoch 5] Batch 2773, Loss 0.25824859738349915\n",
      "[Training Epoch 5] Batch 2774, Loss 0.2383342683315277\n",
      "[Training Epoch 5] Batch 2775, Loss 0.2757241427898407\n",
      "[Training Epoch 5] Batch 2776, Loss 0.24058008193969727\n",
      "[Training Epoch 5] Batch 2777, Loss 0.25247079133987427\n",
      "[Training Epoch 5] Batch 2778, Loss 0.2843284010887146\n",
      "[Training Epoch 5] Batch 2779, Loss 0.2479032427072525\n",
      "[Training Epoch 5] Batch 2780, Loss 0.24438506364822388\n",
      "[Training Epoch 5] Batch 2781, Loss 0.26177775859832764\n",
      "[Training Epoch 5] Batch 2782, Loss 0.2646837830543518\n",
      "[Training Epoch 5] Batch 2783, Loss 0.2532632350921631\n",
      "[Training Epoch 5] Batch 2784, Loss 0.24878962337970734\n",
      "[Training Epoch 5] Batch 2785, Loss 0.23445172607898712\n",
      "[Training Epoch 5] Batch 2786, Loss 0.2395733892917633\n",
      "[Training Epoch 5] Batch 2787, Loss 0.23194041848182678\n",
      "[Training Epoch 5] Batch 2788, Loss 0.2804078459739685\n",
      "[Training Epoch 5] Batch 2789, Loss 0.26283228397369385\n",
      "[Training Epoch 5] Batch 2790, Loss 0.2708871364593506\n",
      "[Training Epoch 5] Batch 2791, Loss 0.22191749513149261\n",
      "[Training Epoch 5] Batch 2792, Loss 0.24872511625289917\n",
      "[Training Epoch 5] Batch 2793, Loss 0.2730180323123932\n",
      "[Training Epoch 5] Batch 2794, Loss 0.2567565441131592\n",
      "[Training Epoch 5] Batch 2795, Loss 0.24288202822208405\n",
      "[Training Epoch 5] Batch 2796, Loss 0.2620573043823242\n",
      "[Training Epoch 5] Batch 2797, Loss 0.27252545952796936\n",
      "[Training Epoch 5] Batch 2798, Loss 0.24652448296546936\n",
      "[Training Epoch 5] Batch 2799, Loss 0.2532074749469757\n",
      "[Training Epoch 5] Batch 2800, Loss 0.25413113832473755\n",
      "[Training Epoch 5] Batch 2801, Loss 0.2541624903678894\n",
      "[Training Epoch 5] Batch 2802, Loss 0.24850253760814667\n",
      "[Training Epoch 5] Batch 2803, Loss 0.25879520177841187\n",
      "[Training Epoch 5] Batch 2804, Loss 0.2507202625274658\n",
      "[Training Epoch 5] Batch 2805, Loss 0.25470370054244995\n",
      "[Training Epoch 5] Batch 2806, Loss 0.2645396590232849\n",
      "[Training Epoch 5] Batch 2807, Loss 0.24622373282909393\n",
      "[Training Epoch 5] Batch 2808, Loss 0.24124641716480255\n",
      "[Training Epoch 5] Batch 2809, Loss 0.22634565830230713\n",
      "[Training Epoch 5] Batch 2810, Loss 0.25030556321144104\n",
      "[Training Epoch 5] Batch 2811, Loss 0.2652893662452698\n",
      "[Training Epoch 5] Batch 2812, Loss 0.21810945868492126\n",
      "[Training Epoch 5] Batch 2813, Loss 0.2510254979133606\n",
      "[Training Epoch 5] Batch 2814, Loss 0.24956804513931274\n",
      "[Training Epoch 5] Batch 2815, Loss 0.25564509630203247\n",
      "[Training Epoch 5] Batch 2816, Loss 0.2884994149208069\n",
      "[Training Epoch 5] Batch 2817, Loss 0.24327725172042847\n",
      "[Training Epoch 5] Batch 2818, Loss 0.2664724886417389\n",
      "[Training Epoch 5] Batch 2819, Loss 0.2641656994819641\n",
      "[Training Epoch 5] Batch 2820, Loss 0.2860010862350464\n",
      "[Training Epoch 5] Batch 2821, Loss 0.3010735809803009\n",
      "[Training Epoch 5] Batch 2822, Loss 0.24882754683494568\n",
      "[Training Epoch 5] Batch 2823, Loss 0.25039058923721313\n",
      "[Training Epoch 5] Batch 2824, Loss 0.2971799671649933\n",
      "[Training Epoch 5] Batch 2825, Loss 0.26910436153411865\n",
      "[Training Epoch 5] Batch 2826, Loss 0.25037407875061035\n",
      "[Training Epoch 5] Batch 2827, Loss 0.25161245465278625\n",
      "[Training Epoch 5] Batch 2828, Loss 0.2600991725921631\n",
      "[Training Epoch 5] Batch 2829, Loss 0.2438981831073761\n",
      "[Training Epoch 5] Batch 2830, Loss 0.27212828397750854\n",
      "[Training Epoch 5] Batch 2831, Loss 0.2500656545162201\n",
      "[Training Epoch 5] Batch 2832, Loss 0.26993054151535034\n",
      "[Training Epoch 5] Batch 2833, Loss 0.2917925715446472\n",
      "[Training Epoch 5] Batch 2834, Loss 0.2941465377807617\n",
      "[Training Epoch 5] Batch 2835, Loss 0.2626373767852783\n",
      "[Training Epoch 5] Batch 2836, Loss 0.25325265526771545\n",
      "[Training Epoch 5] Batch 2837, Loss 0.2472132444381714\n",
      "[Training Epoch 5] Batch 2838, Loss 0.2518489956855774\n",
      "[Training Epoch 5] Batch 2839, Loss 0.25081121921539307\n",
      "[Training Epoch 5] Batch 2840, Loss 0.2812771797180176\n",
      "[Training Epoch 5] Batch 2841, Loss 0.27095162868499756\n",
      "[Training Epoch 5] Batch 2842, Loss 0.2564879059791565\n",
      "[Training Epoch 5] Batch 2843, Loss 0.2557374835014343\n",
      "[Training Epoch 5] Batch 2844, Loss 0.2713943123817444\n",
      "[Training Epoch 5] Batch 2845, Loss 0.24129149317741394\n",
      "[Training Epoch 5] Batch 2846, Loss 0.24631138145923615\n",
      "[Training Epoch 5] Batch 2847, Loss 0.2690894305706024\n",
      "[Training Epoch 5] Batch 2848, Loss 0.27010685205459595\n",
      "[Training Epoch 5] Batch 2849, Loss 0.2298530638217926\n",
      "[Training Epoch 5] Batch 2850, Loss 0.28777146339416504\n",
      "[Training Epoch 5] Batch 2851, Loss 0.233979269862175\n",
      "[Training Epoch 5] Batch 2852, Loss 0.26668089628219604\n",
      "[Training Epoch 5] Batch 2853, Loss 0.2533024549484253\n",
      "[Training Epoch 5] Batch 2854, Loss 0.22771534323692322\n",
      "[Training Epoch 5] Batch 2855, Loss 0.24102763831615448\n",
      "[Training Epoch 5] Batch 2856, Loss 0.2676701843738556\n",
      "[Training Epoch 5] Batch 2857, Loss 0.25655344128608704\n",
      "[Training Epoch 5] Batch 2858, Loss 0.26404690742492676\n",
      "[Training Epoch 5] Batch 2859, Loss 0.27700120210647583\n",
      "[Training Epoch 5] Batch 2860, Loss 0.25914591550827026\n",
      "[Training Epoch 5] Batch 2861, Loss 0.2640244662761688\n",
      "[Training Epoch 5] Batch 2862, Loss 0.25756406784057617\n",
      "[Training Epoch 5] Batch 2863, Loss 0.25772732496261597\n",
      "[Training Epoch 5] Batch 2864, Loss 0.25423577427864075\n",
      "[Training Epoch 5] Batch 2865, Loss 0.26139962673187256\n",
      "[Training Epoch 5] Batch 2866, Loss 0.24868282675743103\n",
      "[Training Epoch 5] Batch 2867, Loss 0.2490067481994629\n",
      "[Training Epoch 5] Batch 2868, Loss 0.25653907656669617\n",
      "[Training Epoch 5] Batch 2869, Loss 0.2827114164829254\n",
      "[Training Epoch 5] Batch 2870, Loss 0.2524341344833374\n",
      "[Training Epoch 5] Batch 2871, Loss 0.24157856404781342\n",
      "[Training Epoch 5] Batch 2872, Loss 0.26555579900741577\n",
      "[Training Epoch 5] Batch 2873, Loss 0.24932819604873657\n",
      "[Training Epoch 5] Batch 2874, Loss 0.2693377137184143\n",
      "[Training Epoch 5] Batch 2875, Loss 0.24412575364112854\n",
      "[Training Epoch 5] Batch 2876, Loss 0.2368532121181488\n",
      "[Training Epoch 5] Batch 2877, Loss 0.26482993364334106\n",
      "[Training Epoch 5] Batch 2878, Loss 0.28008151054382324\n",
      "[Training Epoch 5] Batch 2879, Loss 0.2601085603237152\n",
      "[Training Epoch 5] Batch 2880, Loss 0.27840346097946167\n",
      "[Training Epoch 5] Batch 2881, Loss 0.265887588262558\n",
      "[Training Epoch 5] Batch 2882, Loss 0.28426650166511536\n",
      "[Training Epoch 5] Batch 2883, Loss 0.23993343114852905\n",
      "[Training Epoch 5] Batch 2884, Loss 0.2743534445762634\n",
      "[Training Epoch 5] Batch 2885, Loss 0.2784765958786011\n",
      "[Training Epoch 5] Batch 2886, Loss 0.2733500003814697\n",
      "[Training Epoch 5] Batch 2887, Loss 0.25844547152519226\n",
      "[Training Epoch 5] Batch 2888, Loss 0.26677024364471436\n",
      "[Training Epoch 5] Batch 2889, Loss 0.25963127613067627\n",
      "[Training Epoch 5] Batch 2890, Loss 0.2732856273651123\n",
      "[Training Epoch 5] Batch 2891, Loss 0.239629328250885\n",
      "[Training Epoch 5] Batch 2892, Loss 0.2615904211997986\n",
      "[Training Epoch 5] Batch 2893, Loss 0.2570103704929352\n",
      "[Training Epoch 5] Batch 2894, Loss 0.23026198148727417\n",
      "[Training Epoch 5] Batch 2895, Loss 0.27358102798461914\n",
      "[Training Epoch 5] Batch 2896, Loss 0.265042781829834\n",
      "[Training Epoch 5] Batch 2897, Loss 0.24156004190444946\n",
      "[Training Epoch 5] Batch 2898, Loss 0.23923322558403015\n",
      "[Training Epoch 5] Batch 2899, Loss 0.25408104062080383\n",
      "[Training Epoch 5] Batch 2900, Loss 0.25943607091903687\n",
      "[Training Epoch 5] Batch 2901, Loss 0.2716831564903259\n",
      "[Training Epoch 5] Batch 2902, Loss 0.276289165019989\n",
      "[Training Epoch 5] Batch 2903, Loss 0.27225685119628906\n",
      "[Training Epoch 5] Batch 2904, Loss 0.2462557554244995\n",
      "[Training Epoch 5] Batch 2905, Loss 0.24093352258205414\n",
      "[Training Epoch 5] Batch 2906, Loss 0.24593620002269745\n",
      "[Training Epoch 5] Batch 2907, Loss 0.2633270025253296\n",
      "[Training Epoch 5] Batch 2908, Loss 0.2473469078540802\n",
      "[Training Epoch 5] Batch 2909, Loss 0.26805368065834045\n",
      "[Training Epoch 5] Batch 2910, Loss 0.26520588994026184\n",
      "[Training Epoch 5] Batch 2911, Loss 0.23961669206619263\n",
      "[Training Epoch 5] Batch 2912, Loss 0.281114399433136\n",
      "[Training Epoch 5] Batch 2913, Loss 0.2618791460990906\n",
      "[Training Epoch 5] Batch 2914, Loss 0.2563612759113312\n",
      "[Training Epoch 5] Batch 2915, Loss 0.2532976269721985\n",
      "[Training Epoch 5] Batch 2916, Loss 0.2560349702835083\n",
      "[Training Epoch 5] Batch 2917, Loss 0.24056115746498108\n",
      "[Training Epoch 5] Batch 2918, Loss 0.2573045492172241\n",
      "[Training Epoch 5] Batch 2919, Loss 0.2484433948993683\n",
      "[Training Epoch 5] Batch 2920, Loss 0.2597303092479706\n",
      "[Training Epoch 5] Batch 2921, Loss 0.2553507387638092\n",
      "[Training Epoch 5] Batch 2922, Loss 0.23083233833312988\n",
      "[Training Epoch 5] Batch 2923, Loss 0.2681366503238678\n",
      "[Training Epoch 5] Batch 2924, Loss 0.24964651465415955\n",
      "[Training Epoch 5] Batch 2925, Loss 0.26319557428359985\n",
      "[Training Epoch 5] Batch 2926, Loss 0.22797313332557678\n",
      "[Training Epoch 5] Batch 2927, Loss 0.2545181214809418\n",
      "[Training Epoch 5] Batch 2928, Loss 0.22150450944900513\n",
      "[Training Epoch 5] Batch 2929, Loss 0.2610698938369751\n",
      "[Training Epoch 5] Batch 2930, Loss 0.23760908842086792\n",
      "[Training Epoch 5] Batch 2931, Loss 0.25073742866516113\n",
      "[Training Epoch 5] Batch 2932, Loss 0.24817398190498352\n",
      "[Training Epoch 5] Batch 2933, Loss 0.29756999015808105\n",
      "[Training Epoch 5] Batch 2934, Loss 0.22808174788951874\n",
      "[Training Epoch 5] Batch 2935, Loss 0.276808500289917\n",
      "[Training Epoch 5] Batch 2936, Loss 0.2757870852947235\n",
      "[Training Epoch 5] Batch 2937, Loss 0.2677810490131378\n",
      "[Training Epoch 5] Batch 2938, Loss 0.24725164473056793\n",
      "[Training Epoch 5] Batch 2939, Loss 0.2465449869632721\n",
      "[Training Epoch 5] Batch 2940, Loss 0.2822874188423157\n",
      "[Training Epoch 5] Batch 2941, Loss 0.2886278033256531\n",
      "[Training Epoch 5] Batch 2942, Loss 0.28271761536598206\n",
      "[Training Epoch 5] Batch 2943, Loss 0.270956426858902\n",
      "[Training Epoch 5] Batch 2944, Loss 0.21951638162136078\n",
      "[Training Epoch 5] Batch 2945, Loss 0.24344459176063538\n",
      "[Training Epoch 5] Batch 2946, Loss 0.2814496159553528\n",
      "[Training Epoch 5] Batch 2947, Loss 0.2592506408691406\n",
      "[Training Epoch 5] Batch 2948, Loss 0.2504481375217438\n",
      "[Training Epoch 5] Batch 2949, Loss 0.2853683829307556\n",
      "[Training Epoch 5] Batch 2950, Loss 0.25473201274871826\n",
      "[Training Epoch 5] Batch 2951, Loss 0.26346272230148315\n",
      "[Training Epoch 5] Batch 2952, Loss 0.29273727536201477\n",
      "[Training Epoch 5] Batch 2953, Loss 0.30251824855804443\n",
      "[Training Epoch 5] Batch 2954, Loss 0.24078194797039032\n",
      "[Training Epoch 5] Batch 2955, Loss 0.28541332483291626\n",
      "[Training Epoch 5] Batch 2956, Loss 0.26756879687309265\n",
      "[Training Epoch 5] Batch 2957, Loss 0.24445270001888275\n",
      "[Training Epoch 5] Batch 2958, Loss 0.2655123472213745\n",
      "[Training Epoch 5] Batch 2959, Loss 0.2560182809829712\n",
      "[Training Epoch 5] Batch 2960, Loss 0.2973724603652954\n",
      "[Training Epoch 5] Batch 2961, Loss 0.2570416331291199\n",
      "[Training Epoch 5] Batch 2962, Loss 0.254448801279068\n",
      "[Training Epoch 5] Batch 2963, Loss 0.25715023279190063\n",
      "[Training Epoch 5] Batch 2964, Loss 0.24734781682491302\n",
      "[Training Epoch 5] Batch 2965, Loss 0.25220242142677307\n",
      "[Training Epoch 5] Batch 2966, Loss 0.2632756233215332\n",
      "[Training Epoch 5] Batch 2967, Loss 0.23102980852127075\n",
      "[Training Epoch 5] Batch 2968, Loss 0.23225966095924377\n",
      "[Training Epoch 5] Batch 2969, Loss 0.2520521283149719\n",
      "[Training Epoch 5] Batch 2970, Loss 0.2745651602745056\n",
      "[Training Epoch 5] Batch 2971, Loss 0.27227312326431274\n",
      "[Training Epoch 5] Batch 2972, Loss 0.22954490780830383\n",
      "[Training Epoch 5] Batch 2973, Loss 0.26589643955230713\n",
      "[Training Epoch 5] Batch 2974, Loss 0.274793803691864\n",
      "[Training Epoch 5] Batch 2975, Loss 0.2726752758026123\n",
      "[Training Epoch 5] Batch 2976, Loss 0.2567429542541504\n",
      "[Training Epoch 5] Batch 2977, Loss 0.2581895589828491\n",
      "[Training Epoch 5] Batch 2978, Loss 0.2586994171142578\n",
      "[Training Epoch 5] Batch 2979, Loss 0.26005420088768005\n",
      "[Training Epoch 5] Batch 2980, Loss 0.2570611536502838\n",
      "[Training Epoch 5] Batch 2981, Loss 0.2606342136859894\n",
      "[Training Epoch 5] Batch 2982, Loss 0.25124239921569824\n",
      "[Training Epoch 5] Batch 2983, Loss 0.26125264167785645\n",
      "[Training Epoch 5] Batch 2984, Loss 0.24554742872714996\n",
      "[Training Epoch 5] Batch 2985, Loss 0.24999095499515533\n",
      "[Training Epoch 5] Batch 2986, Loss 0.24908649921417236\n",
      "[Training Epoch 5] Batch 2987, Loss 0.23315578699111938\n",
      "[Training Epoch 5] Batch 2988, Loss 0.22749833762645721\n",
      "[Training Epoch 5] Batch 2989, Loss 0.29263627529144287\n",
      "[Training Epoch 5] Batch 2990, Loss 0.2604737877845764\n",
      "[Training Epoch 5] Batch 2991, Loss 0.2494640350341797\n",
      "[Training Epoch 5] Batch 2992, Loss 0.26889169216156006\n",
      "[Training Epoch 5] Batch 2993, Loss 0.2430785894393921\n",
      "[Training Epoch 5] Batch 2994, Loss 0.2774457633495331\n",
      "[Training Epoch 5] Batch 2995, Loss 0.24787241220474243\n",
      "[Training Epoch 5] Batch 2996, Loss 0.2722046375274658\n",
      "[Training Epoch 5] Batch 2997, Loss 0.2572474181652069\n",
      "[Training Epoch 5] Batch 2998, Loss 0.24564343690872192\n",
      "[Training Epoch 5] Batch 2999, Loss 0.24222803115844727\n",
      "[Training Epoch 5] Batch 3000, Loss 0.24306635558605194\n",
      "[Training Epoch 5] Batch 3001, Loss 0.24772392213344574\n",
      "[Training Epoch 5] Batch 3002, Loss 0.2690827250480652\n",
      "[Training Epoch 5] Batch 3003, Loss 0.2564079761505127\n",
      "[Training Epoch 5] Batch 3004, Loss 0.276969313621521\n",
      "[Training Epoch 5] Batch 3005, Loss 0.2546994686126709\n",
      "[Training Epoch 5] Batch 3006, Loss 0.2797260284423828\n",
      "[Training Epoch 5] Batch 3007, Loss 0.26261863112449646\n",
      "[Training Epoch 5] Batch 3008, Loss 0.255931556224823\n",
      "[Training Epoch 5] Batch 3009, Loss 0.24100139737129211\n",
      "[Training Epoch 5] Batch 3010, Loss 0.27608126401901245\n",
      "[Training Epoch 5] Batch 3011, Loss 0.2542209029197693\n",
      "[Training Epoch 5] Batch 3012, Loss 0.24915018677711487\n",
      "[Training Epoch 5] Batch 3013, Loss 0.2679113745689392\n",
      "[Training Epoch 5] Batch 3014, Loss 0.24306806921958923\n",
      "[Training Epoch 5] Batch 3015, Loss 0.2504163384437561\n",
      "[Training Epoch 5] Batch 3016, Loss 0.2778506278991699\n",
      "[Training Epoch 5] Batch 3017, Loss 0.2659868597984314\n",
      "[Training Epoch 5] Batch 3018, Loss 0.27656805515289307\n",
      "[Training Epoch 5] Batch 3019, Loss 0.22859953343868256\n",
      "[Training Epoch 5] Batch 3020, Loss 0.27406978607177734\n",
      "[Training Epoch 5] Batch 3021, Loss 0.26295918226242065\n",
      "[Training Epoch 5] Batch 3022, Loss 0.2655721604824066\n",
      "[Training Epoch 5] Batch 3023, Loss 0.2348429560661316\n",
      "[Training Epoch 5] Batch 3024, Loss 0.28537750244140625\n",
      "[Training Epoch 5] Batch 3025, Loss 0.2505265176296234\n",
      "[Training Epoch 5] Batch 3026, Loss 0.251271516084671\n",
      "[Training Epoch 5] Batch 3027, Loss 0.23368574678897858\n",
      "[Training Epoch 5] Batch 3028, Loss 0.2909106910228729\n",
      "[Training Epoch 5] Batch 3029, Loss 0.24578657746315002\n",
      "[Training Epoch 5] Batch 3030, Loss 0.24110588431358337\n",
      "[Training Epoch 5] Batch 3031, Loss 0.27699410915374756\n",
      "[Training Epoch 5] Batch 3032, Loss 0.2747800350189209\n",
      "[Training Epoch 5] Batch 3033, Loss 0.28059273958206177\n",
      "[Training Epoch 5] Batch 3034, Loss 0.23977631330490112\n",
      "[Training Epoch 5] Batch 3035, Loss 0.2585929036140442\n",
      "[Training Epoch 5] Batch 3036, Loss 0.23181366920471191\n",
      "[Training Epoch 5] Batch 3037, Loss 0.2856464087963104\n",
      "[Training Epoch 5] Batch 3038, Loss 0.2778322696685791\n",
      "[Training Epoch 5] Batch 3039, Loss 0.27687472105026245\n",
      "[Training Epoch 5] Batch 3040, Loss 0.2985847294330597\n",
      "[Training Epoch 5] Batch 3041, Loss 0.2876160144805908\n",
      "[Training Epoch 5] Batch 3042, Loss 0.2789703607559204\n",
      "[Training Epoch 5] Batch 3043, Loss 0.27180445194244385\n",
      "[Training Epoch 5] Batch 3044, Loss 0.2408408522605896\n",
      "[Training Epoch 5] Batch 3045, Loss 0.273787260055542\n",
      "[Training Epoch 5] Batch 3046, Loss 0.2583986222743988\n",
      "[Training Epoch 5] Batch 3047, Loss 0.262348473072052\n",
      "[Training Epoch 5] Batch 3048, Loss 0.2717139422893524\n",
      "[Training Epoch 5] Batch 3049, Loss 0.2463366836309433\n",
      "[Training Epoch 5] Batch 3050, Loss 0.2757471203804016\n",
      "[Training Epoch 5] Batch 3051, Loss 0.2670896649360657\n",
      "[Training Epoch 5] Batch 3052, Loss 0.2877388000488281\n",
      "[Training Epoch 5] Batch 3053, Loss 0.27566638588905334\n",
      "[Training Epoch 5] Batch 3054, Loss 0.2420184165239334\n",
      "[Training Epoch 5] Batch 3055, Loss 0.25634443759918213\n",
      "[Training Epoch 5] Batch 3056, Loss 0.2513611912727356\n",
      "[Training Epoch 5] Batch 3057, Loss 0.25179624557495117\n",
      "[Training Epoch 5] Batch 3058, Loss 0.2746882438659668\n",
      "[Training Epoch 5] Batch 3059, Loss 0.2712330222129822\n",
      "[Training Epoch 5] Batch 3060, Loss 0.2632709741592407\n",
      "[Training Epoch 5] Batch 3061, Loss 0.26726216077804565\n",
      "[Training Epoch 5] Batch 3062, Loss 0.24841748178005219\n",
      "[Training Epoch 5] Batch 3063, Loss 0.2335936576128006\n",
      "[Training Epoch 5] Batch 3064, Loss 0.2492859661579132\n",
      "[Training Epoch 5] Batch 3065, Loss 0.28150469064712524\n",
      "[Training Epoch 5] Batch 3066, Loss 0.24733595550060272\n",
      "[Training Epoch 5] Batch 3067, Loss 0.27646604180336\n",
      "[Training Epoch 5] Batch 3068, Loss 0.25388258695602417\n",
      "[Training Epoch 5] Batch 3069, Loss 0.2645282447338104\n",
      "[Training Epoch 5] Batch 3070, Loss 0.26661068201065063\n",
      "[Training Epoch 5] Batch 3071, Loss 0.2720862627029419\n",
      "[Training Epoch 5] Batch 3072, Loss 0.2492726743221283\n",
      "[Training Epoch 5] Batch 3073, Loss 0.2711023688316345\n",
      "[Training Epoch 5] Batch 3074, Loss 0.28056418895721436\n",
      "[Training Epoch 5] Batch 3075, Loss 0.2317122519016266\n",
      "[Training Epoch 5] Batch 3076, Loss 0.2851055860519409\n",
      "[Training Epoch 5] Batch 3077, Loss 0.25077614188194275\n",
      "[Training Epoch 5] Batch 3078, Loss 0.25525426864624023\n",
      "[Training Epoch 5] Batch 3079, Loss 0.2511694133281708\n",
      "[Training Epoch 5] Batch 3080, Loss 0.2713276743888855\n",
      "[Training Epoch 5] Batch 3081, Loss 0.2504526972770691\n",
      "[Training Epoch 5] Batch 3082, Loss 0.2856624722480774\n",
      "[Training Epoch 5] Batch 3083, Loss 0.29241490364074707\n",
      "[Training Epoch 5] Batch 3084, Loss 0.2633633613586426\n",
      "[Training Epoch 5] Batch 3085, Loss 0.26612961292266846\n",
      "[Training Epoch 5] Batch 3086, Loss 0.2515394687652588\n",
      "[Training Epoch 5] Batch 3087, Loss 0.2706114649772644\n",
      "[Training Epoch 5] Batch 3088, Loss 0.24761007726192474\n",
      "[Training Epoch 5] Batch 3089, Loss 0.26282650232315063\n",
      "[Training Epoch 5] Batch 3090, Loss 0.24014252424240112\n",
      "[Training Epoch 5] Batch 3091, Loss 0.2612784504890442\n",
      "[Training Epoch 5] Batch 3092, Loss 0.28401249647140503\n",
      "[Training Epoch 5] Batch 3093, Loss 0.25781288743019104\n",
      "[Training Epoch 5] Batch 3094, Loss 0.25386735796928406\n",
      "[Training Epoch 5] Batch 3095, Loss 0.2506043314933777\n",
      "[Training Epoch 5] Batch 3096, Loss 0.28361183404922485\n",
      "[Training Epoch 5] Batch 3097, Loss 0.25745365023612976\n",
      "[Training Epoch 5] Batch 3098, Loss 0.28644800186157227\n",
      "[Training Epoch 5] Batch 3099, Loss 0.25620460510253906\n",
      "[Training Epoch 5] Batch 3100, Loss 0.244961678981781\n",
      "[Training Epoch 5] Batch 3101, Loss 0.26861217617988586\n",
      "[Training Epoch 5] Batch 3102, Loss 0.27686163783073425\n",
      "[Training Epoch 5] Batch 3103, Loss 0.26739367842674255\n",
      "[Training Epoch 5] Batch 3104, Loss 0.25506526231765747\n",
      "[Training Epoch 5] Batch 3105, Loss 0.2797999382019043\n",
      "[Training Epoch 5] Batch 3106, Loss 0.3025243580341339\n",
      "[Training Epoch 5] Batch 3107, Loss 0.26043131947517395\n",
      "[Training Epoch 5] Batch 3108, Loss 0.2684840261936188\n",
      "[Training Epoch 5] Batch 3109, Loss 0.26186802983283997\n",
      "[Training Epoch 5] Batch 3110, Loss 0.2913494110107422\n",
      "[Training Epoch 5] Batch 3111, Loss 0.2712630033493042\n",
      "[Training Epoch 5] Batch 3112, Loss 0.2364501655101776\n",
      "[Training Epoch 5] Batch 3113, Loss 0.2633369565010071\n",
      "[Training Epoch 5] Batch 3114, Loss 0.2719186842441559\n",
      "[Training Epoch 5] Batch 3115, Loss 0.258322536945343\n",
      "[Training Epoch 5] Batch 3116, Loss 0.24722111225128174\n",
      "[Training Epoch 5] Batch 3117, Loss 0.266586571931839\n",
      "[Training Epoch 5] Batch 3118, Loss 0.24575889110565186\n",
      "[Training Epoch 5] Batch 3119, Loss 0.27651602029800415\n",
      "[Training Epoch 5] Batch 3120, Loss 0.2704305052757263\n",
      "[Training Epoch 5] Batch 3121, Loss 0.24300163984298706\n",
      "[Training Epoch 5] Batch 3122, Loss 0.24911794066429138\n",
      "[Training Epoch 5] Batch 3123, Loss 0.29940518736839294\n",
      "[Training Epoch 5] Batch 3124, Loss 0.2660433053970337\n",
      "[Training Epoch 5] Batch 3125, Loss 0.25156158208847046\n",
      "[Training Epoch 5] Batch 3126, Loss 0.25407180190086365\n",
      "[Training Epoch 5] Batch 3127, Loss 0.254780650138855\n",
      "[Training Epoch 5] Batch 3128, Loss 0.26448261737823486\n",
      "[Training Epoch 5] Batch 3129, Loss 0.24674320220947266\n",
      "[Training Epoch 5] Batch 3130, Loss 0.2669380307197571\n",
      "[Training Epoch 5] Batch 3131, Loss 0.23893064260482788\n",
      "[Training Epoch 5] Batch 3132, Loss 0.24946218729019165\n",
      "[Training Epoch 5] Batch 3133, Loss 0.28045278787612915\n",
      "[Training Epoch 5] Batch 3134, Loss 0.24002112448215485\n",
      "[Training Epoch 5] Batch 3135, Loss 0.26843637228012085\n",
      "[Training Epoch 5] Batch 3136, Loss 0.23627406358718872\n",
      "[Training Epoch 5] Batch 3137, Loss 0.2398439347743988\n",
      "[Training Epoch 5] Batch 3138, Loss 0.2629966735839844\n",
      "[Training Epoch 5] Batch 3139, Loss 0.26154130697250366\n",
      "[Training Epoch 5] Batch 3140, Loss 0.25207000970840454\n",
      "[Training Epoch 5] Batch 3141, Loss 0.24133817851543427\n",
      "[Training Epoch 5] Batch 3142, Loss 0.27294790744781494\n",
      "[Training Epoch 5] Batch 3143, Loss 0.23724031448364258\n",
      "[Training Epoch 5] Batch 3144, Loss 0.2589303255081177\n",
      "[Training Epoch 5] Batch 3145, Loss 0.29495570063591003\n",
      "[Training Epoch 5] Batch 3146, Loss 0.26931774616241455\n",
      "[Training Epoch 5] Batch 3147, Loss 0.2561785578727722\n",
      "[Training Epoch 5] Batch 3148, Loss 0.24825513362884521\n",
      "[Training Epoch 5] Batch 3149, Loss 0.23978394269943237\n",
      "[Training Epoch 5] Batch 3150, Loss 0.26607373356819153\n",
      "[Training Epoch 5] Batch 3151, Loss 0.2518848478794098\n",
      "[Training Epoch 5] Batch 3152, Loss 0.2621299624443054\n",
      "[Training Epoch 5] Batch 3153, Loss 0.24110238254070282\n",
      "[Training Epoch 5] Batch 3154, Loss 0.2574213743209839\n",
      "[Training Epoch 5] Batch 3155, Loss 0.26082539558410645\n",
      "[Training Epoch 5] Batch 3156, Loss 0.26771602034568787\n",
      "[Training Epoch 5] Batch 3157, Loss 0.2524791955947876\n",
      "[Training Epoch 5] Batch 3158, Loss 0.2609359920024872\n",
      "[Training Epoch 5] Batch 3159, Loss 0.23765306174755096\n",
      "[Training Epoch 5] Batch 3160, Loss 0.28464287519454956\n",
      "[Training Epoch 5] Batch 3161, Loss 0.2531434893608093\n",
      "[Training Epoch 5] Batch 3162, Loss 0.24717676639556885\n",
      "[Training Epoch 5] Batch 3163, Loss 0.2528155446052551\n",
      "[Training Epoch 5] Batch 3164, Loss 0.26838356256484985\n",
      "[Training Epoch 5] Batch 3165, Loss 0.2605186998844147\n",
      "[Training Epoch 5] Batch 3166, Loss 0.2739964723587036\n",
      "[Training Epoch 5] Batch 3167, Loss 0.26431143283843994\n",
      "[Training Epoch 5] Batch 3168, Loss 0.23789183795452118\n",
      "[Training Epoch 5] Batch 3169, Loss 0.22367557883262634\n",
      "[Training Epoch 5] Batch 3170, Loss 0.27497512102127075\n",
      "[Training Epoch 5] Batch 3171, Loss 0.25367307662963867\n",
      "[Training Epoch 5] Batch 3172, Loss 0.2676108479499817\n",
      "[Training Epoch 5] Batch 3173, Loss 0.2578977346420288\n",
      "[Training Epoch 5] Batch 3174, Loss 0.24866655468940735\n",
      "[Training Epoch 5] Batch 3175, Loss 0.2367844581604004\n",
      "[Training Epoch 5] Batch 3176, Loss 0.2559269368648529\n",
      "[Training Epoch 5] Batch 3177, Loss 0.27970442175865173\n",
      "[Training Epoch 5] Batch 3178, Loss 0.2708340287208557\n",
      "[Training Epoch 5] Batch 3179, Loss 0.26463592052459717\n",
      "[Training Epoch 5] Batch 3180, Loss 0.2718900442123413\n",
      "[Training Epoch 5] Batch 3181, Loss 0.25147679448127747\n",
      "[Training Epoch 5] Batch 3182, Loss 0.2490295171737671\n",
      "[Training Epoch 5] Batch 3183, Loss 0.27609914541244507\n",
      "[Training Epoch 5] Batch 3184, Loss 0.2593889534473419\n",
      "[Training Epoch 5] Batch 3185, Loss 0.2629111409187317\n",
      "[Training Epoch 5] Batch 3186, Loss 0.26611563563346863\n",
      "[Training Epoch 5] Batch 3187, Loss 0.2394150197505951\n",
      "[Training Epoch 5] Batch 3188, Loss 0.26403719186782837\n",
      "[Training Epoch 5] Batch 3189, Loss 0.27419203519821167\n",
      "[Training Epoch 5] Batch 3190, Loss 0.28100302815437317\n",
      "[Training Epoch 5] Batch 3191, Loss 0.28323063254356384\n",
      "[Training Epoch 5] Batch 3192, Loss 0.2651488780975342\n",
      "[Training Epoch 5] Batch 3193, Loss 0.2704674005508423\n",
      "[Training Epoch 5] Batch 3194, Loss 0.25565797090530396\n",
      "[Training Epoch 5] Batch 3195, Loss 0.24384570121765137\n",
      "[Training Epoch 5] Batch 3196, Loss 0.2812897562980652\n",
      "[Training Epoch 5] Batch 3197, Loss 0.27221304178237915\n",
      "[Training Epoch 5] Batch 3198, Loss 0.2664421796798706\n",
      "[Training Epoch 5] Batch 3199, Loss 0.2554149925708771\n",
      "[Training Epoch 5] Batch 3200, Loss 0.26695793867111206\n",
      "[Training Epoch 5] Batch 3201, Loss 0.26358142495155334\n",
      "[Training Epoch 5] Batch 3202, Loss 0.26451051235198975\n",
      "[Training Epoch 5] Batch 3203, Loss 0.28005659580230713\n",
      "[Training Epoch 5] Batch 3204, Loss 0.2548524737358093\n",
      "[Training Epoch 5] Batch 3205, Loss 0.2549225986003876\n",
      "[Training Epoch 5] Batch 3206, Loss 0.23615336418151855\n",
      "[Training Epoch 5] Batch 3207, Loss 0.2802944779396057\n",
      "[Training Epoch 5] Batch 3208, Loss 0.28374946117401123\n",
      "[Training Epoch 5] Batch 3209, Loss 0.2633121907711029\n",
      "[Training Epoch 5] Batch 3210, Loss 0.29417040944099426\n",
      "[Training Epoch 5] Batch 3211, Loss 0.2558024823665619\n",
      "[Training Epoch 5] Batch 3212, Loss 0.2545131742954254\n",
      "[Training Epoch 5] Batch 3213, Loss 0.2568162679672241\n",
      "[Training Epoch 5] Batch 3214, Loss 0.26173198223114014\n",
      "[Training Epoch 5] Batch 3215, Loss 0.2587912678718567\n",
      "[Training Epoch 5] Batch 3216, Loss 0.2664470076560974\n",
      "[Training Epoch 5] Batch 3217, Loss 0.23547619581222534\n",
      "[Training Epoch 5] Batch 3218, Loss 0.26344311237335205\n",
      "[Training Epoch 5] Batch 3219, Loss 0.24407805502414703\n",
      "[Training Epoch 5] Batch 3220, Loss 0.2448827475309372\n",
      "[Training Epoch 5] Batch 3221, Loss 0.2932701110839844\n",
      "[Training Epoch 5] Batch 3222, Loss 0.27757400274276733\n",
      "[Training Epoch 5] Batch 3223, Loss 0.2633342444896698\n",
      "[Training Epoch 5] Batch 3224, Loss 0.26910650730133057\n",
      "[Training Epoch 5] Batch 3225, Loss 0.2506135404109955\n",
      "[Training Epoch 5] Batch 3226, Loss 0.26198869943618774\n",
      "[Training Epoch 5] Batch 3227, Loss 0.24735572934150696\n",
      "[Training Epoch 5] Batch 3228, Loss 0.2344919741153717\n",
      "[Training Epoch 5] Batch 3229, Loss 0.2830042243003845\n",
      "[Training Epoch 5] Batch 3230, Loss 0.272406667470932\n",
      "[Training Epoch 5] Batch 3231, Loss 0.23846334218978882\n",
      "[Training Epoch 5] Batch 3232, Loss 0.2630748748779297\n",
      "[Training Epoch 5] Batch 3233, Loss 0.22984561324119568\n",
      "[Training Epoch 5] Batch 3234, Loss 0.2683733105659485\n",
      "[Training Epoch 5] Batch 3235, Loss 0.24785321950912476\n",
      "[Training Epoch 5] Batch 3236, Loss 0.2588493227958679\n",
      "[Training Epoch 5] Batch 3237, Loss 0.2660568952560425\n",
      "[Training Epoch 5] Batch 3238, Loss 0.22973725199699402\n",
      "[Training Epoch 5] Batch 3239, Loss 0.2523031234741211\n",
      "[Training Epoch 5] Batch 3240, Loss 0.2403174638748169\n",
      "[Training Epoch 5] Batch 3241, Loss 0.20396652817726135\n",
      "[Training Epoch 5] Batch 3242, Loss 0.2589416801929474\n",
      "[Training Epoch 5] Batch 3243, Loss 0.2769860625267029\n",
      "[Training Epoch 5] Batch 3244, Loss 0.27437350153923035\n",
      "[Training Epoch 5] Batch 3245, Loss 0.24344606697559357\n",
      "[Training Epoch 5] Batch 3246, Loss 0.27458399534225464\n",
      "[Training Epoch 5] Batch 3247, Loss 0.2592993974685669\n",
      "[Training Epoch 5] Batch 3248, Loss 0.2703578472137451\n",
      "[Training Epoch 5] Batch 3249, Loss 0.2643842399120331\n",
      "[Training Epoch 5] Batch 3250, Loss 0.2613212466239929\n",
      "[Training Epoch 5] Batch 3251, Loss 0.26713135838508606\n",
      "[Training Epoch 5] Batch 3252, Loss 0.2840002775192261\n",
      "[Training Epoch 5] Batch 3253, Loss 0.2597230076789856\n",
      "[Training Epoch 5] Batch 3254, Loss 0.23892545700073242\n",
      "[Training Epoch 5] Batch 3255, Loss 0.2636025547981262\n",
      "[Training Epoch 5] Batch 3256, Loss 0.2530509829521179\n",
      "[Training Epoch 5] Batch 3257, Loss 0.28842610120773315\n",
      "[Training Epoch 5] Batch 3258, Loss 0.25754469633102417\n",
      "[Training Epoch 5] Batch 3259, Loss 0.29119613766670227\n",
      "[Training Epoch 5] Batch 3260, Loss 0.2683095335960388\n",
      "[Training Epoch 5] Batch 3261, Loss 0.2683030664920807\n",
      "[Training Epoch 5] Batch 3262, Loss 0.2541055381298065\n",
      "[Training Epoch 5] Batch 3263, Loss 0.2600241005420685\n",
      "[Training Epoch 5] Batch 3264, Loss 0.23651370406150818\n",
      "[Training Epoch 5] Batch 3265, Loss 0.25500816106796265\n",
      "[Training Epoch 5] Batch 3266, Loss 0.24795323610305786\n",
      "[Training Epoch 5] Batch 3267, Loss 0.26862668991088867\n",
      "[Training Epoch 5] Batch 3268, Loss 0.2750866711139679\n",
      "[Training Epoch 5] Batch 3269, Loss 0.2628180682659149\n",
      "[Training Epoch 5] Batch 3270, Loss 0.23626044392585754\n",
      "[Training Epoch 5] Batch 3271, Loss 0.2725539803504944\n",
      "[Training Epoch 5] Batch 3272, Loss 0.26954737305641174\n",
      "[Training Epoch 5] Batch 3273, Loss 0.24644127488136292\n",
      "[Training Epoch 5] Batch 3274, Loss 0.25904691219329834\n",
      "[Training Epoch 5] Batch 3275, Loss 0.2676098942756653\n",
      "[Training Epoch 5] Batch 3276, Loss 0.2696327567100525\n",
      "[Training Epoch 5] Batch 3277, Loss 0.29794421792030334\n",
      "[Training Epoch 5] Batch 3278, Loss 0.2532901465892792\n",
      "[Training Epoch 5] Batch 3279, Loss 0.2506534457206726\n",
      "[Training Epoch 5] Batch 3280, Loss 0.27453500032424927\n",
      "[Training Epoch 5] Batch 3281, Loss 0.2772365212440491\n",
      "[Training Epoch 5] Batch 3282, Loss 0.28038540482521057\n",
      "[Training Epoch 5] Batch 3283, Loss 0.24576647579669952\n",
      "[Training Epoch 5] Batch 3284, Loss 0.2729095220565796\n",
      "[Training Epoch 5] Batch 3285, Loss 0.2738439440727234\n",
      "[Training Epoch 5] Batch 3286, Loss 0.26297372579574585\n",
      "[Training Epoch 5] Batch 3287, Loss 0.26275622844696045\n",
      "[Training Epoch 5] Batch 3288, Loss 0.2411373406648636\n",
      "[Training Epoch 5] Batch 3289, Loss 0.2539044916629791\n",
      "[Training Epoch 5] Batch 3290, Loss 0.2619744539260864\n",
      "[Training Epoch 5] Batch 3291, Loss 0.2655704617500305\n",
      "[Training Epoch 5] Batch 3292, Loss 0.2664090692996979\n",
      "[Training Epoch 5] Batch 3293, Loss 0.2760007679462433\n",
      "[Training Epoch 5] Batch 3294, Loss 0.2716611921787262\n",
      "[Training Epoch 5] Batch 3295, Loss 0.2910875082015991\n",
      "[Training Epoch 5] Batch 3296, Loss 0.2714688181877136\n",
      "[Training Epoch 5] Batch 3297, Loss 0.2511163353919983\n",
      "[Training Epoch 5] Batch 3298, Loss 0.28956174850463867\n",
      "[Training Epoch 5] Batch 3299, Loss 0.24650506675243378\n",
      "[Training Epoch 5] Batch 3300, Loss 0.23324188590049744\n",
      "[Training Epoch 5] Batch 3301, Loss 0.2756967842578888\n",
      "[Training Epoch 5] Batch 3302, Loss 0.27261435985565186\n",
      "[Training Epoch 5] Batch 3303, Loss 0.2711380124092102\n",
      "[Training Epoch 5] Batch 3304, Loss 0.29579296708106995\n",
      "[Training Epoch 5] Batch 3305, Loss 0.2568587064743042\n",
      "[Training Epoch 5] Batch 3306, Loss 0.3033906817436218\n",
      "[Training Epoch 5] Batch 3307, Loss 0.272577702999115\n",
      "[Training Epoch 5] Batch 3308, Loss 0.2536872327327728\n",
      "[Training Epoch 5] Batch 3309, Loss 0.2423609048128128\n",
      "[Training Epoch 5] Batch 3310, Loss 0.25347021222114563\n",
      "[Training Epoch 5] Batch 3311, Loss 0.23866277933120728\n",
      "[Training Epoch 5] Batch 3312, Loss 0.28670138120651245\n",
      "[Training Epoch 5] Batch 3313, Loss 0.2539225220680237\n",
      "[Training Epoch 5] Batch 3314, Loss 0.2583290636539459\n",
      "[Training Epoch 5] Batch 3315, Loss 0.23629337549209595\n",
      "[Training Epoch 5] Batch 3316, Loss 0.257499098777771\n",
      "[Training Epoch 5] Batch 3317, Loss 0.24996604025363922\n",
      "[Training Epoch 5] Batch 3318, Loss 0.2862412929534912\n",
      "[Training Epoch 5] Batch 3319, Loss 0.2510553300380707\n",
      "[Training Epoch 5] Batch 3320, Loss 0.26190385222435\n",
      "[Training Epoch 5] Batch 3321, Loss 0.24008476734161377\n",
      "[Training Epoch 5] Batch 3322, Loss 0.2854424715042114\n",
      "[Training Epoch 5] Batch 3323, Loss 0.2655927240848541\n",
      "[Training Epoch 5] Batch 3324, Loss 0.2635864019393921\n",
      "[Training Epoch 5] Batch 3325, Loss 0.2705625593662262\n",
      "[Training Epoch 5] Batch 3326, Loss 0.2828952968120575\n",
      "[Training Epoch 5] Batch 3327, Loss 0.25959521532058716\n",
      "[Training Epoch 5] Batch 3328, Loss 0.2448297142982483\n",
      "[Training Epoch 5] Batch 3329, Loss 0.25148433446884155\n",
      "[Training Epoch 5] Batch 3330, Loss 0.2656470537185669\n",
      "[Training Epoch 5] Batch 3331, Loss 0.24068501591682434\n",
      "[Training Epoch 5] Batch 3332, Loss 0.2576696574687958\n",
      "[Training Epoch 5] Batch 3333, Loss 0.2374492585659027\n",
      "[Training Epoch 5] Batch 3334, Loss 0.2549173831939697\n",
      "[Training Epoch 5] Batch 3335, Loss 0.2607256770133972\n",
      "[Training Epoch 5] Batch 3336, Loss 0.27392321825027466\n",
      "[Training Epoch 5] Batch 3337, Loss 0.28797483444213867\n",
      "[Training Epoch 5] Batch 3338, Loss 0.26237958669662476\n",
      "[Training Epoch 5] Batch 3339, Loss 0.2492944300174713\n",
      "[Training Epoch 5] Batch 3340, Loss 0.2711551785469055\n",
      "[Training Epoch 5] Batch 3341, Loss 0.23028704524040222\n",
      "[Training Epoch 5] Batch 3342, Loss 0.24557684361934662\n",
      "[Training Epoch 5] Batch 3343, Loss 0.23981507122516632\n",
      "[Training Epoch 5] Batch 3344, Loss 0.25597870349884033\n",
      "[Training Epoch 5] Batch 3345, Loss 0.25908204913139343\n",
      "[Training Epoch 5] Batch 3346, Loss 0.2725422978401184\n",
      "[Training Epoch 5] Batch 3347, Loss 0.2628944218158722\n",
      "[Training Epoch 5] Batch 3348, Loss 0.29314470291137695\n",
      "[Training Epoch 5] Batch 3349, Loss 0.2589137554168701\n",
      "[Training Epoch 5] Batch 3350, Loss 0.28198081254959106\n",
      "[Training Epoch 5] Batch 3351, Loss 0.2617962956428528\n",
      "[Training Epoch 5] Batch 3352, Loss 0.26375311613082886\n",
      "[Training Epoch 5] Batch 3353, Loss 0.2719688415527344\n",
      "[Training Epoch 5] Batch 3354, Loss 0.25808584690093994\n",
      "[Training Epoch 5] Batch 3355, Loss 0.246436208486557\n",
      "[Training Epoch 5] Batch 3356, Loss 0.25825294852256775\n",
      "[Training Epoch 5] Batch 3357, Loss 0.2774572968482971\n",
      "[Training Epoch 5] Batch 3358, Loss 0.23873063921928406\n",
      "[Training Epoch 5] Batch 3359, Loss 0.26611557602882385\n",
      "[Training Epoch 5] Batch 3360, Loss 0.2299417108297348\n",
      "[Training Epoch 5] Batch 3361, Loss 0.2722550630569458\n",
      "[Training Epoch 5] Batch 3362, Loss 0.2540033459663391\n",
      "[Training Epoch 5] Batch 3363, Loss 0.2529618740081787\n",
      "[Training Epoch 5] Batch 3364, Loss 0.2596219778060913\n",
      "[Training Epoch 5] Batch 3365, Loss 0.2761068642139435\n",
      "[Training Epoch 5] Batch 3366, Loss 0.2716837525367737\n",
      "[Training Epoch 5] Batch 3367, Loss 0.2607085108757019\n",
      "[Training Epoch 5] Batch 3368, Loss 0.2776075005531311\n",
      "[Training Epoch 5] Batch 3369, Loss 0.2631312906742096\n",
      "[Training Epoch 5] Batch 3370, Loss 0.24789534509181976\n",
      "[Training Epoch 5] Batch 3371, Loss 0.24372316896915436\n",
      "[Training Epoch 5] Batch 3372, Loss 0.27255165576934814\n",
      "[Training Epoch 5] Batch 3373, Loss 0.24802298843860626\n",
      "[Training Epoch 5] Batch 3374, Loss 0.2624155879020691\n",
      "[Training Epoch 5] Batch 3375, Loss 0.2591473162174225\n",
      "[Training Epoch 5] Batch 3376, Loss 0.2790612578392029\n",
      "[Training Epoch 5] Batch 3377, Loss 0.2722378969192505\n",
      "[Training Epoch 5] Batch 3378, Loss 0.25632110238075256\n",
      "[Training Epoch 5] Batch 3379, Loss 0.27483922243118286\n",
      "[Training Epoch 5] Batch 3380, Loss 0.24857112765312195\n",
      "[Training Epoch 5] Batch 3381, Loss 0.24901926517486572\n",
      "[Training Epoch 5] Batch 3382, Loss 0.2407592236995697\n",
      "[Training Epoch 5] Batch 3383, Loss 0.2584552764892578\n",
      "[Training Epoch 5] Batch 3384, Loss 0.25113850831985474\n",
      "[Training Epoch 5] Batch 3385, Loss 0.27376753091812134\n",
      "[Training Epoch 5] Batch 3386, Loss 0.27843523025512695\n",
      "[Training Epoch 5] Batch 3387, Loss 0.2637467682361603\n",
      "[Training Epoch 5] Batch 3388, Loss 0.24999022483825684\n",
      "[Training Epoch 5] Batch 3389, Loss 0.26911747455596924\n",
      "[Training Epoch 5] Batch 3390, Loss 0.2715960443019867\n",
      "[Training Epoch 5] Batch 3391, Loss 0.26702338457107544\n",
      "[Training Epoch 5] Batch 3392, Loss 0.2782042920589447\n",
      "[Training Epoch 5] Batch 3393, Loss 0.27440887689590454\n",
      "[Training Epoch 5] Batch 3394, Loss 0.20508083701133728\n",
      "[Training Epoch 5] Batch 3395, Loss 0.24883994460105896\n",
      "[Training Epoch 5] Batch 3396, Loss 0.2628447115421295\n",
      "[Training Epoch 5] Batch 3397, Loss 0.27813655138015747\n",
      "[Training Epoch 5] Batch 3398, Loss 0.23523765802383423\n",
      "[Training Epoch 5] Batch 3399, Loss 0.2370530664920807\n",
      "[Training Epoch 5] Batch 3400, Loss 0.2786446809768677\n",
      "[Training Epoch 5] Batch 3401, Loss 0.2785132825374603\n",
      "[Training Epoch 5] Batch 3402, Loss 0.26515892148017883\n",
      "[Training Epoch 5] Batch 3403, Loss 0.27086520195007324\n",
      "[Training Epoch 5] Batch 3404, Loss 0.260934054851532\n",
      "[Training Epoch 5] Batch 3405, Loss 0.25857555866241455\n",
      "[Training Epoch 5] Batch 3406, Loss 0.2765807509422302\n",
      "[Training Epoch 5] Batch 3407, Loss 0.26628223061561584\n",
      "[Training Epoch 5] Batch 3408, Loss 0.27016299962997437\n",
      "[Training Epoch 5] Batch 3409, Loss 0.26085934042930603\n",
      "[Training Epoch 5] Batch 3410, Loss 0.25062793493270874\n",
      "[Training Epoch 5] Batch 3411, Loss 0.24790799617767334\n",
      "[Training Epoch 5] Batch 3412, Loss 0.27797168493270874\n",
      "[Training Epoch 5] Batch 3413, Loss 0.3039061427116394\n",
      "[Training Epoch 5] Batch 3414, Loss 0.26777565479278564\n",
      "[Training Epoch 5] Batch 3415, Loss 0.26299524307250977\n",
      "[Training Epoch 5] Batch 3416, Loss 0.2912621796131134\n",
      "[Training Epoch 5] Batch 3417, Loss 0.2653212547302246\n",
      "[Training Epoch 5] Batch 3418, Loss 0.2415883094072342\n",
      "[Training Epoch 5] Batch 3419, Loss 0.26072239875793457\n",
      "[Training Epoch 5] Batch 3420, Loss 0.28375428915023804\n",
      "[Training Epoch 5] Batch 3421, Loss 0.25608572363853455\n",
      "[Training Epoch 5] Batch 3422, Loss 0.26867228746414185\n",
      "[Training Epoch 5] Batch 3423, Loss 0.2781079411506653\n",
      "[Training Epoch 5] Batch 3424, Loss 0.2868174612522125\n",
      "[Training Epoch 5] Batch 3425, Loss 0.2582990825176239\n",
      "[Training Epoch 5] Batch 3426, Loss 0.24518577754497528\n",
      "[Training Epoch 5] Batch 3427, Loss 0.2484130859375\n",
      "[Training Epoch 5] Batch 3428, Loss 0.23298704624176025\n",
      "[Training Epoch 5] Batch 3429, Loss 0.2788192629814148\n",
      "[Training Epoch 5] Batch 3430, Loss 0.2619408071041107\n",
      "[Training Epoch 5] Batch 3431, Loss 0.2732194662094116\n",
      "[Training Epoch 5] Batch 3432, Loss 0.2597332298755646\n",
      "[Training Epoch 5] Batch 3433, Loss 0.2861378788948059\n",
      "[Training Epoch 5] Batch 3434, Loss 0.27353063225746155\n",
      "[Training Epoch 5] Batch 3435, Loss 0.2580745816230774\n",
      "[Training Epoch 5] Batch 3436, Loss 0.2654041647911072\n",
      "[Training Epoch 5] Batch 3437, Loss 0.27957049012184143\n",
      "[Training Epoch 5] Batch 3438, Loss 0.273632675409317\n",
      "[Training Epoch 5] Batch 3439, Loss 0.290635347366333\n",
      "[Training Epoch 5] Batch 3440, Loss 0.2528696060180664\n",
      "[Training Epoch 5] Batch 3441, Loss 0.2429848611354828\n",
      "[Training Epoch 5] Batch 3442, Loss 0.258304238319397\n",
      "[Training Epoch 5] Batch 3443, Loss 0.2812749147415161\n",
      "[Training Epoch 5] Batch 3444, Loss 0.24564722180366516\n",
      "[Training Epoch 5] Batch 3445, Loss 0.26264292001724243\n",
      "[Training Epoch 5] Batch 3446, Loss 0.26366114616394043\n",
      "[Training Epoch 5] Batch 3447, Loss 0.28118813037872314\n",
      "[Training Epoch 5] Batch 3448, Loss 0.26438096165657043\n",
      "[Training Epoch 5] Batch 3449, Loss 0.23952598869800568\n",
      "[Training Epoch 5] Batch 3450, Loss 0.27887314558029175\n",
      "[Training Epoch 5] Batch 3451, Loss 0.273667573928833\n",
      "[Training Epoch 5] Batch 3452, Loss 0.2572212219238281\n",
      "[Training Epoch 5] Batch 3453, Loss 0.2697001099586487\n",
      "[Training Epoch 5] Batch 3454, Loss 0.2818945050239563\n",
      "[Training Epoch 5] Batch 3455, Loss 0.2404690533876419\n",
      "[Training Epoch 5] Batch 3456, Loss 0.2435234934091568\n",
      "[Training Epoch 5] Batch 3457, Loss 0.2733325660228729\n",
      "[Training Epoch 5] Batch 3458, Loss 0.2560637593269348\n",
      "[Training Epoch 5] Batch 3459, Loss 0.2625507414340973\n",
      "[Training Epoch 5] Batch 3460, Loss 0.25732386112213135\n",
      "[Training Epoch 5] Batch 3461, Loss 0.2586122751235962\n",
      "[Training Epoch 5] Batch 3462, Loss 0.2528538703918457\n",
      "[Training Epoch 5] Batch 3463, Loss 0.25577250123023987\n",
      "[Training Epoch 5] Batch 3464, Loss 0.23440858721733093\n",
      "[Training Epoch 5] Batch 3465, Loss 0.2903531789779663\n",
      "[Training Epoch 5] Batch 3466, Loss 0.2861960232257843\n",
      "[Training Epoch 5] Batch 3467, Loss 0.24554161727428436\n",
      "[Training Epoch 5] Batch 3468, Loss 0.2640979290008545\n",
      "[Training Epoch 5] Batch 3469, Loss 0.2730748653411865\n",
      "[Training Epoch 5] Batch 3470, Loss 0.2663387060165405\n",
      "[Training Epoch 5] Batch 3471, Loss 0.263706237077713\n",
      "[Training Epoch 5] Batch 3472, Loss 0.25848525762557983\n",
      "[Training Epoch 5] Batch 3473, Loss 0.24950599670410156\n",
      "[Training Epoch 5] Batch 3474, Loss 0.2623048722743988\n",
      "[Training Epoch 5] Batch 3475, Loss 0.24226067960262299\n",
      "[Training Epoch 5] Batch 3476, Loss 0.272742360830307\n",
      "[Training Epoch 5] Batch 3477, Loss 0.2360178828239441\n",
      "[Training Epoch 5] Batch 3478, Loss 0.2687409222126007\n",
      "[Training Epoch 5] Batch 3479, Loss 0.2507839798927307\n",
      "[Training Epoch 5] Batch 3480, Loss 0.2516937553882599\n",
      "[Training Epoch 5] Batch 3481, Loss 0.2792295217514038\n",
      "[Training Epoch 5] Batch 3482, Loss 0.2787937819957733\n",
      "[Training Epoch 5] Batch 3483, Loss 0.24402876198291779\n",
      "[Training Epoch 5] Batch 3484, Loss 0.24584443867206573\n",
      "[Training Epoch 5] Batch 3485, Loss 0.28456979990005493\n",
      "[Training Epoch 5] Batch 3486, Loss 0.2418479472398758\n",
      "[Training Epoch 5] Batch 3487, Loss 0.2548757791519165\n",
      "[Training Epoch 5] Batch 3488, Loss 0.25735437870025635\n",
      "[Training Epoch 5] Batch 3489, Loss 0.24665236473083496\n",
      "[Training Epoch 5] Batch 3490, Loss 0.2542140781879425\n",
      "[Training Epoch 5] Batch 3491, Loss 0.24578045308589935\n",
      "[Training Epoch 5] Batch 3492, Loss 0.2888011932373047\n",
      "[Training Epoch 5] Batch 3493, Loss 0.28353965282440186\n",
      "[Training Epoch 5] Batch 3494, Loss 0.2348097562789917\n",
      "[Training Epoch 5] Batch 3495, Loss 0.25041109323501587\n",
      "[Training Epoch 5] Batch 3496, Loss 0.25429198145866394\n",
      "[Training Epoch 5] Batch 3497, Loss 0.263666033744812\n",
      "[Training Epoch 5] Batch 3498, Loss 0.2663739025592804\n",
      "[Training Epoch 5] Batch 3499, Loss 0.24425926804542542\n",
      "[Training Epoch 5] Batch 3500, Loss 0.3037247657775879\n",
      "[Training Epoch 5] Batch 3501, Loss 0.2604144811630249\n",
      "[Training Epoch 5] Batch 3502, Loss 0.28857171535491943\n",
      "[Training Epoch 5] Batch 3503, Loss 0.2542731463909149\n",
      "[Training Epoch 5] Batch 3504, Loss 0.27819013595581055\n",
      "[Training Epoch 5] Batch 3505, Loss 0.2878769338130951\n",
      "[Training Epoch 5] Batch 3506, Loss 0.2322077453136444\n",
      "[Training Epoch 5] Batch 3507, Loss 0.2486369013786316\n",
      "[Training Epoch 5] Batch 3508, Loss 0.24062363803386688\n",
      "[Training Epoch 5] Batch 3509, Loss 0.2484891414642334\n",
      "[Training Epoch 5] Batch 3510, Loss 0.2625032663345337\n",
      "[Training Epoch 5] Batch 3511, Loss 0.2441892921924591\n",
      "[Training Epoch 5] Batch 3512, Loss 0.2398296296596527\n",
      "[Training Epoch 5] Batch 3513, Loss 0.2555570602416992\n",
      "[Training Epoch 5] Batch 3514, Loss 0.2378944456577301\n",
      "[Training Epoch 5] Batch 3515, Loss 0.23956520855426788\n",
      "[Training Epoch 5] Batch 3516, Loss 0.2727735638618469\n",
      "[Training Epoch 5] Batch 3517, Loss 0.2575172483921051\n",
      "[Training Epoch 5] Batch 3518, Loss 0.24715903401374817\n",
      "[Training Epoch 5] Batch 3519, Loss 0.2819923162460327\n",
      "[Training Epoch 5] Batch 3520, Loss 0.24423986673355103\n",
      "[Training Epoch 5] Batch 3521, Loss 0.27018898725509644\n",
      "[Training Epoch 5] Batch 3522, Loss 0.22813747823238373\n",
      "[Training Epoch 5] Batch 3523, Loss 0.2684204578399658\n",
      "[Training Epoch 5] Batch 3524, Loss 0.2807094156742096\n",
      "[Training Epoch 5] Batch 3525, Loss 0.254549115896225\n",
      "[Training Epoch 5] Batch 3526, Loss 0.2645556628704071\n",
      "[Training Epoch 5] Batch 3527, Loss 0.26917415857315063\n",
      "[Training Epoch 5] Batch 3528, Loss 0.27221930027008057\n",
      "[Training Epoch 5] Batch 3529, Loss 0.2370489537715912\n",
      "[Training Epoch 5] Batch 3530, Loss 0.28431573510169983\n",
      "[Training Epoch 5] Batch 3531, Loss 0.26655155420303345\n",
      "[Training Epoch 5] Batch 3532, Loss 0.26268094778060913\n",
      "[Training Epoch 5] Batch 3533, Loss 0.27499520778656006\n",
      "[Training Epoch 5] Batch 3534, Loss 0.2561802566051483\n",
      "[Training Epoch 5] Batch 3535, Loss 0.2728657126426697\n",
      "[Training Epoch 5] Batch 3536, Loss 0.2165561318397522\n",
      "[Training Epoch 5] Batch 3537, Loss 0.23613563179969788\n",
      "[Training Epoch 5] Batch 3538, Loss 0.25813207030296326\n",
      "[Training Epoch 5] Batch 3539, Loss 0.251779705286026\n",
      "[Training Epoch 5] Batch 3540, Loss 0.2759982943534851\n",
      "[Training Epoch 5] Batch 3541, Loss 0.28894686698913574\n",
      "[Training Epoch 5] Batch 3542, Loss 0.25033092498779297\n",
      "[Training Epoch 5] Batch 3543, Loss 0.26127147674560547\n",
      "[Training Epoch 5] Batch 3544, Loss 0.26368340849876404\n",
      "[Training Epoch 5] Batch 3545, Loss 0.2583109438419342\n",
      "[Training Epoch 5] Batch 3546, Loss 0.25744226574897766\n",
      "[Training Epoch 5] Batch 3547, Loss 0.24876531958580017\n",
      "[Training Epoch 5] Batch 3548, Loss 0.23344513773918152\n",
      "[Training Epoch 5] Batch 3549, Loss 0.2648921012878418\n",
      "[Training Epoch 5] Batch 3550, Loss 0.24834918975830078\n",
      "[Training Epoch 5] Batch 3551, Loss 0.28272077441215515\n",
      "[Training Epoch 5] Batch 3552, Loss 0.26146119832992554\n",
      "[Training Epoch 5] Batch 3553, Loss 0.29147303104400635\n",
      "[Training Epoch 5] Batch 3554, Loss 0.268684446811676\n",
      "[Training Epoch 5] Batch 3555, Loss 0.25243836641311646\n",
      "[Training Epoch 5] Batch 3556, Loss 0.26259124279022217\n",
      "[Training Epoch 5] Batch 3557, Loss 0.2515529990196228\n",
      "[Training Epoch 5] Batch 3558, Loss 0.2517535388469696\n",
      "[Training Epoch 5] Batch 3559, Loss 0.2617039978504181\n",
      "[Training Epoch 5] Batch 3560, Loss 0.24939295649528503\n",
      "[Training Epoch 5] Batch 3561, Loss 0.2677159309387207\n",
      "[Training Epoch 5] Batch 3562, Loss 0.2640814781188965\n",
      "[Training Epoch 5] Batch 3563, Loss 0.2825741767883301\n",
      "[Training Epoch 5] Batch 3564, Loss 0.24840688705444336\n",
      "[Training Epoch 5] Batch 3565, Loss 0.24642623960971832\n",
      "[Training Epoch 5] Batch 3566, Loss 0.26025453209877014\n",
      "[Training Epoch 5] Batch 3567, Loss 0.23965388536453247\n",
      "[Training Epoch 5] Batch 3568, Loss 0.2951558828353882\n",
      "[Training Epoch 5] Batch 3569, Loss 0.27914273738861084\n",
      "[Training Epoch 5] Batch 3570, Loss 0.2714713513851166\n",
      "[Training Epoch 5] Batch 3571, Loss 0.23388350009918213\n",
      "[Training Epoch 5] Batch 3572, Loss 0.2793484628200531\n",
      "[Training Epoch 5] Batch 3573, Loss 0.26808297634124756\n",
      "[Training Epoch 5] Batch 3574, Loss 0.2749403715133667\n",
      "[Training Epoch 5] Batch 3575, Loss 0.25174057483673096\n",
      "[Training Epoch 5] Batch 3576, Loss 0.2480669915676117\n",
      "[Training Epoch 5] Batch 3577, Loss 0.22534829378128052\n",
      "[Training Epoch 5] Batch 3578, Loss 0.259579062461853\n",
      "[Training Epoch 5] Batch 3579, Loss 0.24734550714492798\n",
      "[Training Epoch 5] Batch 3580, Loss 0.27195030450820923\n",
      "[Training Epoch 5] Batch 3581, Loss 0.2694076895713806\n",
      "[Training Epoch 5] Batch 3582, Loss 0.2802968919277191\n",
      "[Training Epoch 5] Batch 3583, Loss 0.23057778179645538\n",
      "[Training Epoch 5] Batch 3584, Loss 0.25886374711990356\n",
      "[Training Epoch 5] Batch 3585, Loss 0.24035298824310303\n",
      "[Training Epoch 5] Batch 3586, Loss 0.2655973434448242\n",
      "[Training Epoch 5] Batch 3587, Loss 0.2632591426372528\n",
      "[Training Epoch 5] Batch 3588, Loss 0.28032582998275757\n",
      "[Training Epoch 5] Batch 3589, Loss 0.26919013261795044\n",
      "[Training Epoch 5] Batch 3590, Loss 0.23043666779994965\n",
      "[Training Epoch 5] Batch 3591, Loss 0.25494569540023804\n",
      "[Training Epoch 5] Batch 3592, Loss 0.2590351998806\n",
      "[Training Epoch 5] Batch 3593, Loss 0.24397340416908264\n",
      "[Training Epoch 5] Batch 3594, Loss 0.2708175480365753\n",
      "[Training Epoch 5] Batch 3595, Loss 0.25571656227111816\n",
      "[Training Epoch 5] Batch 3596, Loss 0.23321989178657532\n",
      "[Training Epoch 5] Batch 3597, Loss 0.23405316472053528\n",
      "[Training Epoch 5] Batch 3598, Loss 0.2777639627456665\n",
      "[Training Epoch 5] Batch 3599, Loss 0.23846706748008728\n",
      "[Training Epoch 5] Batch 3600, Loss 0.25934845209121704\n",
      "[Training Epoch 5] Batch 3601, Loss 0.24853768944740295\n",
      "[Training Epoch 5] Batch 3602, Loss 0.25451427698135376\n",
      "[Training Epoch 5] Batch 3603, Loss 0.25085026025772095\n",
      "[Training Epoch 5] Batch 3604, Loss 0.2357461005449295\n",
      "[Training Epoch 5] Batch 3605, Loss 0.24162468314170837\n",
      "[Training Epoch 5] Batch 3606, Loss 0.2484084814786911\n",
      "[Training Epoch 5] Batch 3607, Loss 0.2651658058166504\n",
      "[Training Epoch 5] Batch 3608, Loss 0.2587873935699463\n",
      "[Training Epoch 5] Batch 3609, Loss 0.2746483385562897\n",
      "[Training Epoch 5] Batch 3610, Loss 0.26358139514923096\n",
      "[Training Epoch 5] Batch 3611, Loss 0.21342945098876953\n",
      "[Training Epoch 5] Batch 3612, Loss 0.2519474923610687\n",
      "[Training Epoch 5] Batch 3613, Loss 0.2217697650194168\n",
      "[Training Epoch 5] Batch 3614, Loss 0.2755482494831085\n",
      "[Training Epoch 5] Batch 3615, Loss 0.21808704733848572\n",
      "[Training Epoch 5] Batch 3616, Loss 0.2699697017669678\n",
      "[Training Epoch 5] Batch 3617, Loss 0.2865586280822754\n",
      "[Training Epoch 5] Batch 3618, Loss 0.26206284761428833\n",
      "[Training Epoch 5] Batch 3619, Loss 0.2536433935165405\n",
      "[Training Epoch 5] Batch 3620, Loss 0.27356478571891785\n",
      "[Training Epoch 5] Batch 3621, Loss 0.24567994475364685\n",
      "[Training Epoch 5] Batch 3622, Loss 0.22896148264408112\n",
      "[Training Epoch 5] Batch 3623, Loss 0.26410555839538574\n",
      "[Training Epoch 5] Batch 3624, Loss 0.2514658570289612\n",
      "[Training Epoch 5] Batch 3625, Loss 0.2771262526512146\n",
      "[Training Epoch 5] Batch 3626, Loss 0.2600018382072449\n",
      "[Training Epoch 5] Batch 3627, Loss 0.2679066061973572\n",
      "[Training Epoch 5] Batch 3628, Loss 0.287379652261734\n",
      "[Training Epoch 5] Batch 3629, Loss 0.2658371031284332\n",
      "[Training Epoch 5] Batch 3630, Loss 0.24352090060710907\n",
      "[Training Epoch 5] Batch 3631, Loss 0.26343029737472534\n",
      "[Training Epoch 5] Batch 3632, Loss 0.2550446391105652\n",
      "[Training Epoch 5] Batch 3633, Loss 0.28709450364112854\n",
      "[Training Epoch 5] Batch 3634, Loss 0.2380426824092865\n",
      "[Training Epoch 5] Batch 3635, Loss 0.2502889633178711\n",
      "[Training Epoch 5] Batch 3636, Loss 0.2527887225151062\n",
      "[Training Epoch 5] Batch 3637, Loss 0.27154216170310974\n",
      "[Training Epoch 5] Batch 3638, Loss 0.2988723814487457\n",
      "[Training Epoch 5] Batch 3639, Loss 0.24754559993743896\n",
      "[Training Epoch 5] Batch 3640, Loss 0.2523520588874817\n",
      "[Training Epoch 5] Batch 3641, Loss 0.2683115601539612\n",
      "[Training Epoch 5] Batch 3642, Loss 0.2406911998987198\n",
      "[Training Epoch 5] Batch 3643, Loss 0.2667394280433655\n",
      "[Training Epoch 5] Batch 3644, Loss 0.25341448187828064\n",
      "[Training Epoch 5] Batch 3645, Loss 0.2640798091888428\n",
      "[Training Epoch 5] Batch 3646, Loss 0.2578999698162079\n",
      "[Training Epoch 5] Batch 3647, Loss 0.26332780718803406\n",
      "[Training Epoch 5] Batch 3648, Loss 0.24793538451194763\n",
      "[Training Epoch 5] Batch 3649, Loss 0.2484031617641449\n",
      "[Training Epoch 5] Batch 3650, Loss 0.27415764331817627\n",
      "[Training Epoch 5] Batch 3651, Loss 0.25646716356277466\n",
      "[Training Epoch 5] Batch 3652, Loss 0.22562679648399353\n",
      "[Training Epoch 5] Batch 3653, Loss 0.254067599773407\n",
      "[Training Epoch 5] Batch 3654, Loss 0.25689777731895447\n",
      "[Training Epoch 5] Batch 3655, Loss 0.25318557024002075\n",
      "[Training Epoch 5] Batch 3656, Loss 0.2631874084472656\n",
      "[Training Epoch 5] Batch 3657, Loss 0.2413579225540161\n",
      "[Training Epoch 5] Batch 3658, Loss 0.2786179184913635\n",
      "[Training Epoch 5] Batch 3659, Loss 0.28238508105278015\n",
      "[Training Epoch 5] Batch 3660, Loss 0.27673763036727905\n",
      "[Training Epoch 5] Batch 3661, Loss 0.26314860582351685\n",
      "[Training Epoch 5] Batch 3662, Loss 0.26992225646972656\n",
      "[Training Epoch 5] Batch 3663, Loss 0.24619437754154205\n",
      "[Training Epoch 5] Batch 3664, Loss 0.2671378254890442\n",
      "[Training Epoch 5] Batch 3665, Loss 0.2636793255805969\n",
      "[Training Epoch 5] Batch 3666, Loss 0.22028863430023193\n",
      "[Training Epoch 5] Batch 3667, Loss 0.2810039222240448\n",
      "[Training Epoch 5] Batch 3668, Loss 0.23384204506874084\n",
      "[Training Epoch 5] Batch 3669, Loss 0.2679034471511841\n",
      "[Training Epoch 5] Batch 3670, Loss 0.2620694041252136\n",
      "[Training Epoch 5] Batch 3671, Loss 0.2509966194629669\n",
      "[Training Epoch 5] Batch 3672, Loss 0.24908946454524994\n",
      "[Training Epoch 5] Batch 3673, Loss 0.26815375685691833\n",
      "[Training Epoch 5] Batch 3674, Loss 0.25858691334724426\n",
      "[Training Epoch 5] Batch 3675, Loss 0.26767539978027344\n",
      "[Training Epoch 5] Batch 3676, Loss 0.26429128646850586\n",
      "[Training Epoch 5] Batch 3677, Loss 0.29199063777923584\n",
      "[Training Epoch 5] Batch 3678, Loss 0.23511618375778198\n",
      "[Training Epoch 5] Batch 3679, Loss 0.23792991042137146\n",
      "[Training Epoch 5] Batch 3680, Loss 0.26590174436569214\n",
      "[Training Epoch 5] Batch 3681, Loss 0.26179349422454834\n",
      "[Training Epoch 5] Batch 3682, Loss 0.24160349369049072\n",
      "[Training Epoch 5] Batch 3683, Loss 0.24385741353034973\n",
      "[Training Epoch 5] Batch 3684, Loss 0.2732403874397278\n",
      "[Training Epoch 5] Batch 3685, Loss 0.23039355874061584\n",
      "[Training Epoch 5] Batch 3686, Loss 0.26642024517059326\n",
      "[Training Epoch 5] Batch 3687, Loss 0.2622765898704529\n",
      "[Training Epoch 5] Batch 3688, Loss 0.2700517773628235\n",
      "[Training Epoch 5] Batch 3689, Loss 0.26904791593551636\n",
      "[Training Epoch 5] Batch 3690, Loss 0.26560211181640625\n",
      "[Training Epoch 5] Batch 3691, Loss 0.2738819718360901\n",
      "[Training Epoch 5] Batch 3692, Loss 0.27877041697502136\n",
      "[Training Epoch 5] Batch 3693, Loss 0.24990102648735046\n",
      "[Training Epoch 5] Batch 3694, Loss 0.2456091046333313\n",
      "[Training Epoch 5] Batch 3695, Loss 0.25083765387535095\n",
      "[Training Epoch 5] Batch 3696, Loss 0.24565744400024414\n",
      "[Training Epoch 5] Batch 3697, Loss 0.29155033826828003\n",
      "[Training Epoch 5] Batch 3698, Loss 0.27507299184799194\n",
      "[Training Epoch 5] Batch 3699, Loss 0.26373690366744995\n",
      "[Training Epoch 5] Batch 3700, Loss 0.26582908630371094\n",
      "[Training Epoch 5] Batch 3701, Loss 0.24829509854316711\n",
      "[Training Epoch 5] Batch 3702, Loss 0.2765651345252991\n",
      "[Training Epoch 5] Batch 3703, Loss 0.2921643555164337\n",
      "[Training Epoch 5] Batch 3704, Loss 0.24000060558319092\n",
      "[Training Epoch 5] Batch 3705, Loss 0.2671191394329071\n",
      "[Training Epoch 5] Batch 3706, Loss 0.23149235546588898\n",
      "[Training Epoch 5] Batch 3707, Loss 0.25008663535118103\n",
      "[Training Epoch 5] Batch 3708, Loss 0.2534148097038269\n",
      "[Training Epoch 5] Batch 3709, Loss 0.25723838806152344\n",
      "[Training Epoch 5] Batch 3710, Loss 0.27621597051620483\n",
      "[Training Epoch 5] Batch 3711, Loss 0.26417696475982666\n",
      "[Training Epoch 5] Batch 3712, Loss 0.28725671768188477\n",
      "[Training Epoch 5] Batch 3713, Loss 0.2762996554374695\n",
      "[Training Epoch 5] Batch 3714, Loss 0.26511311531066895\n",
      "[Training Epoch 5] Batch 3715, Loss 0.26829156279563904\n",
      "[Training Epoch 5] Batch 3716, Loss 0.23013530671596527\n",
      "[Training Epoch 5] Batch 3717, Loss 0.26310455799102783\n",
      "[Training Epoch 5] Batch 3718, Loss 0.2750377655029297\n",
      "[Training Epoch 5] Batch 3719, Loss 0.24067014455795288\n",
      "[Training Epoch 5] Batch 3720, Loss 0.25442397594451904\n",
      "[Training Epoch 5] Batch 3721, Loss 0.26898735761642456\n",
      "[Training Epoch 5] Batch 3722, Loss 0.2562601566314697\n",
      "[Training Epoch 5] Batch 3723, Loss 0.24713319540023804\n",
      "[Training Epoch 5] Batch 3724, Loss 0.30942487716674805\n",
      "[Training Epoch 5] Batch 3725, Loss 0.28707438707351685\n",
      "[Training Epoch 5] Batch 3726, Loss 0.2517291307449341\n",
      "[Training Epoch 5] Batch 3727, Loss 0.26312559843063354\n",
      "[Training Epoch 5] Batch 3728, Loss 0.27411723136901855\n",
      "[Training Epoch 5] Batch 3729, Loss 0.2543514370918274\n",
      "[Training Epoch 5] Batch 3730, Loss 0.27899619936943054\n",
      "[Training Epoch 5] Batch 3731, Loss 0.24594783782958984\n",
      "[Training Epoch 5] Batch 3732, Loss 0.26522937417030334\n",
      "[Training Epoch 5] Batch 3733, Loss 0.2796059846878052\n",
      "[Training Epoch 5] Batch 3734, Loss 0.25888538360595703\n",
      "[Training Epoch 5] Batch 3735, Loss 0.26406407356262207\n",
      "[Training Epoch 5] Batch 3736, Loss 0.2828788757324219\n",
      "[Training Epoch 5] Batch 3737, Loss 0.2923123240470886\n",
      "[Training Epoch 5] Batch 3738, Loss 0.25073257088661194\n",
      "[Training Epoch 5] Batch 3739, Loss 0.2504419684410095\n",
      "[Training Epoch 5] Batch 3740, Loss 0.25549641251564026\n",
      "[Training Epoch 5] Batch 3741, Loss 0.27299171686172485\n",
      "[Training Epoch 5] Batch 3742, Loss 0.27564868330955505\n",
      "[Training Epoch 5] Batch 3743, Loss 0.2397536188364029\n",
      "[Training Epoch 5] Batch 3744, Loss 0.2888367176055908\n",
      "[Training Epoch 5] Batch 3745, Loss 0.26935797929763794\n",
      "[Training Epoch 5] Batch 3746, Loss 0.2516576051712036\n",
      "[Training Epoch 5] Batch 3747, Loss 0.2611474394798279\n",
      "[Training Epoch 5] Batch 3748, Loss 0.250257283449173\n",
      "[Training Epoch 5] Batch 3749, Loss 0.24995076656341553\n",
      "[Training Epoch 5] Batch 3750, Loss 0.24296537041664124\n",
      "[Training Epoch 5] Batch 3751, Loss 0.28341490030288696\n",
      "[Training Epoch 5] Batch 3752, Loss 0.2839335501194\n",
      "[Training Epoch 5] Batch 3753, Loss 0.2629416584968567\n",
      "[Training Epoch 5] Batch 3754, Loss 0.2645680606365204\n",
      "[Training Epoch 5] Batch 3755, Loss 0.2967199683189392\n",
      "[Training Epoch 5] Batch 3756, Loss 0.2622177004814148\n",
      "[Training Epoch 5] Batch 3757, Loss 0.2700296640396118\n",
      "[Training Epoch 5] Batch 3758, Loss 0.28056395053863525\n",
      "[Training Epoch 5] Batch 3759, Loss 0.24225616455078125\n",
      "[Training Epoch 5] Batch 3760, Loss 0.2555490732192993\n",
      "[Training Epoch 5] Batch 3761, Loss 0.2839791178703308\n",
      "[Training Epoch 5] Batch 3762, Loss 0.22676044702529907\n",
      "[Training Epoch 5] Batch 3763, Loss 0.24773018062114716\n",
      "[Training Epoch 5] Batch 3764, Loss 0.2845967411994934\n",
      "[Training Epoch 5] Batch 3765, Loss 0.251626193523407\n",
      "[Training Epoch 5] Batch 3766, Loss 0.2559240460395813\n",
      "[Training Epoch 5] Batch 3767, Loss 0.2573809325695038\n",
      "[Training Epoch 5] Batch 3768, Loss 0.2562985420227051\n",
      "[Training Epoch 5] Batch 3769, Loss 0.2851133346557617\n",
      "[Training Epoch 5] Batch 3770, Loss 0.2735767066478729\n",
      "[Training Epoch 5] Batch 3771, Loss 0.24730345606803894\n",
      "[Training Epoch 5] Batch 3772, Loss 0.28240787982940674\n",
      "[Training Epoch 5] Batch 3773, Loss 0.24735350906848907\n",
      "[Training Epoch 5] Batch 3774, Loss 0.24889114499092102\n",
      "[Training Epoch 5] Batch 3775, Loss 0.25744110345840454\n",
      "[Training Epoch 5] Batch 3776, Loss 0.27266740798950195\n",
      "[Training Epoch 5] Batch 3777, Loss 0.2469702959060669\n",
      "[Training Epoch 5] Batch 3778, Loss 0.26485371589660645\n",
      "[Training Epoch 5] Batch 3779, Loss 0.28197330236434937\n",
      "[Training Epoch 5] Batch 3780, Loss 0.280262291431427\n",
      "[Training Epoch 5] Batch 3781, Loss 0.2751840353012085\n",
      "[Training Epoch 5] Batch 3782, Loss 0.27108272910118103\n",
      "[Training Epoch 5] Batch 3783, Loss 0.26667606830596924\n",
      "[Training Epoch 5] Batch 3784, Loss 0.25956419110298157\n",
      "[Training Epoch 5] Batch 3785, Loss 0.2804098129272461\n",
      "[Training Epoch 5] Batch 3786, Loss 0.27718132734298706\n",
      "[Training Epoch 5] Batch 3787, Loss 0.2665570378303528\n",
      "[Training Epoch 5] Batch 3788, Loss 0.24505461752414703\n",
      "[Training Epoch 5] Batch 3789, Loss 0.2988686263561249\n",
      "[Training Epoch 5] Batch 3790, Loss 0.24936902523040771\n",
      "[Training Epoch 5] Batch 3791, Loss 0.25852876901626587\n",
      "[Training Epoch 5] Batch 3792, Loss 0.2681681513786316\n",
      "[Training Epoch 5] Batch 3793, Loss 0.23766230046749115\n",
      "[Training Epoch 5] Batch 3794, Loss 0.26020556688308716\n",
      "[Training Epoch 5] Batch 3795, Loss 0.24831198155879974\n",
      "[Training Epoch 5] Batch 3796, Loss 0.27013909816741943\n",
      "[Training Epoch 5] Batch 3797, Loss 0.25240835547447205\n",
      "[Training Epoch 5] Batch 3798, Loss 0.2463531643152237\n",
      "[Training Epoch 5] Batch 3799, Loss 0.26319876313209534\n",
      "[Training Epoch 5] Batch 3800, Loss 0.24641917645931244\n",
      "[Training Epoch 5] Batch 3801, Loss 0.26304328441619873\n",
      "[Training Epoch 5] Batch 3802, Loss 0.24876204133033752\n",
      "[Training Epoch 5] Batch 3803, Loss 0.2558217942714691\n",
      "[Training Epoch 5] Batch 3804, Loss 0.2817528247833252\n",
      "[Training Epoch 5] Batch 3805, Loss 0.23874354362487793\n",
      "[Training Epoch 5] Batch 3806, Loss 0.2887847423553467\n",
      "[Training Epoch 5] Batch 3807, Loss 0.2528303861618042\n",
      "[Training Epoch 5] Batch 3808, Loss 0.26103898882865906\n",
      "[Training Epoch 5] Batch 3809, Loss 0.264299213886261\n",
      "[Training Epoch 5] Batch 3810, Loss 0.2652278542518616\n",
      "[Training Epoch 5] Batch 3811, Loss 0.2771885097026825\n",
      "[Training Epoch 5] Batch 3812, Loss 0.2728525400161743\n",
      "[Training Epoch 5] Batch 3813, Loss 0.24979014694690704\n",
      "[Training Epoch 5] Batch 3814, Loss 0.24879516661167145\n",
      "[Training Epoch 5] Batch 3815, Loss 0.2540600001811981\n",
      "[Training Epoch 5] Batch 3816, Loss 0.23606941103935242\n",
      "[Training Epoch 5] Batch 3817, Loss 0.30318933725357056\n",
      "[Training Epoch 5] Batch 3818, Loss 0.3005877733230591\n",
      "[Training Epoch 5] Batch 3819, Loss 0.2871699929237366\n",
      "[Training Epoch 5] Batch 3820, Loss 0.2637311518192291\n",
      "[Training Epoch 5] Batch 3821, Loss 0.24921569228172302\n",
      "[Training Epoch 5] Batch 3822, Loss 0.27764418721199036\n",
      "[Training Epoch 5] Batch 3823, Loss 0.24767673015594482\n",
      "[Training Epoch 5] Batch 3824, Loss 0.28322291374206543\n",
      "[Training Epoch 5] Batch 3825, Loss 0.2766175866127014\n",
      "[Training Epoch 5] Batch 3826, Loss 0.2486976981163025\n",
      "[Training Epoch 5] Batch 3827, Loss 0.2697259485721588\n",
      "[Training Epoch 5] Batch 3828, Loss 0.26117992401123047\n",
      "[Training Epoch 5] Batch 3829, Loss 0.27676916122436523\n",
      "[Training Epoch 5] Batch 3830, Loss 0.24979190528392792\n",
      "[Training Epoch 5] Batch 3831, Loss 0.25236305594444275\n",
      "[Training Epoch 5] Batch 3832, Loss 0.22720959782600403\n",
      "[Training Epoch 5] Batch 3833, Loss 0.27497050166130066\n",
      "[Training Epoch 5] Batch 3834, Loss 0.25382059812545776\n",
      "[Training Epoch 5] Batch 3835, Loss 0.2658361792564392\n",
      "[Training Epoch 5] Batch 3836, Loss 0.24373197555541992\n",
      "[Training Epoch 5] Batch 3837, Loss 0.252474308013916\n",
      "[Training Epoch 5] Batch 3838, Loss 0.26595383882522583\n",
      "[Training Epoch 5] Batch 3839, Loss 0.23576383292675018\n",
      "[Training Epoch 5] Batch 3840, Loss 0.2506519556045532\n",
      "[Training Epoch 5] Batch 3841, Loss 0.27480971813201904\n",
      "[Training Epoch 5] Batch 3842, Loss 0.24784362316131592\n",
      "[Training Epoch 5] Batch 3843, Loss 0.28168416023254395\n",
      "[Training Epoch 5] Batch 3844, Loss 0.26938360929489136\n",
      "[Training Epoch 5] Batch 3845, Loss 0.24654150009155273\n",
      "[Training Epoch 5] Batch 3846, Loss 0.2771655321121216\n",
      "[Training Epoch 5] Batch 3847, Loss 0.2485186904668808\n",
      "[Training Epoch 5] Batch 3848, Loss 0.23631948232650757\n",
      "[Training Epoch 5] Batch 3849, Loss 0.27832064032554626\n",
      "[Training Epoch 5] Batch 3850, Loss 0.24413906037807465\n",
      "[Training Epoch 5] Batch 3851, Loss 0.2819788455963135\n",
      "[Training Epoch 5] Batch 3852, Loss 0.2842898368835449\n",
      "[Training Epoch 5] Batch 3853, Loss 0.2836392819881439\n",
      "[Training Epoch 5] Batch 3854, Loss 0.2668135166168213\n",
      "[Training Epoch 5] Batch 3855, Loss 0.28592127561569214\n",
      "[Training Epoch 5] Batch 3856, Loss 0.281030535697937\n",
      "[Training Epoch 5] Batch 3857, Loss 0.24633680284023285\n",
      "[Training Epoch 5] Batch 3858, Loss 0.2521894574165344\n",
      "[Training Epoch 5] Batch 3859, Loss 0.28820136189460754\n",
      "[Training Epoch 5] Batch 3860, Loss 0.2755560874938965\n",
      "[Training Epoch 5] Batch 3861, Loss 0.2650144696235657\n",
      "[Training Epoch 5] Batch 3862, Loss 0.2548929452896118\n",
      "[Training Epoch 5] Batch 3863, Loss 0.2492978274822235\n",
      "[Training Epoch 5] Batch 3864, Loss 0.305059015750885\n",
      "[Training Epoch 5] Batch 3865, Loss 0.2512791156768799\n",
      "[Training Epoch 5] Batch 3866, Loss 0.23595227301120758\n",
      "[Training Epoch 5] Batch 3867, Loss 0.26655542850494385\n",
      "[Training Epoch 5] Batch 3868, Loss 0.2732856869697571\n",
      "[Training Epoch 5] Batch 3869, Loss 0.25305816531181335\n",
      "[Training Epoch 5] Batch 3870, Loss 0.2922847867012024\n",
      "[Training Epoch 5] Batch 3871, Loss 0.28695082664489746\n",
      "[Training Epoch 5] Batch 3872, Loss 0.2818739414215088\n",
      "[Training Epoch 5] Batch 3873, Loss 0.26204168796539307\n",
      "[Training Epoch 5] Batch 3874, Loss 0.2599661350250244\n",
      "[Training Epoch 5] Batch 3875, Loss 0.23762208223342896\n",
      "[Training Epoch 5] Batch 3876, Loss 0.272227942943573\n",
      "[Training Epoch 5] Batch 3877, Loss 0.2582259178161621\n",
      "[Training Epoch 5] Batch 3878, Loss 0.3059639036655426\n",
      "[Training Epoch 5] Batch 3879, Loss 0.23502960801124573\n",
      "[Training Epoch 5] Batch 3880, Loss 0.24519973993301392\n",
      "[Training Epoch 5] Batch 3881, Loss 0.23753611743450165\n",
      "[Training Epoch 5] Batch 3882, Loss 0.2767582833766937\n",
      "[Training Epoch 5] Batch 3883, Loss 0.2762320935726166\n",
      "[Training Epoch 5] Batch 3884, Loss 0.2660733163356781\n",
      "[Training Epoch 5] Batch 3885, Loss 0.26052093505859375\n",
      "[Training Epoch 5] Batch 3886, Loss 0.24458593130111694\n",
      "[Training Epoch 5] Batch 3887, Loss 0.2522836923599243\n",
      "[Training Epoch 5] Batch 3888, Loss 0.28302091360092163\n",
      "[Training Epoch 5] Batch 3889, Loss 0.27404773235321045\n",
      "[Training Epoch 5] Batch 3890, Loss 0.26483768224716187\n",
      "[Training Epoch 5] Batch 3891, Loss 0.26113635301589966\n",
      "[Training Epoch 5] Batch 3892, Loss 0.27272653579711914\n",
      "[Training Epoch 5] Batch 3893, Loss 0.28879573941230774\n",
      "[Training Epoch 5] Batch 3894, Loss 0.2574913799762726\n",
      "[Training Epoch 5] Batch 3895, Loss 0.27959656715393066\n",
      "[Training Epoch 5] Batch 3896, Loss 0.2720519006252289\n",
      "[Training Epoch 5] Batch 3897, Loss 0.29289501905441284\n",
      "[Training Epoch 5] Batch 3898, Loss 0.2920560836791992\n",
      "[Training Epoch 5] Batch 3899, Loss 0.2824150323867798\n",
      "[Training Epoch 5] Batch 3900, Loss 0.24886669218540192\n",
      "[Training Epoch 5] Batch 3901, Loss 0.2514917552471161\n",
      "[Training Epoch 5] Batch 3902, Loss 0.2540389895439148\n",
      "[Training Epoch 5] Batch 3903, Loss 0.23279736936092377\n",
      "[Training Epoch 5] Batch 3904, Loss 0.2579520046710968\n",
      "[Training Epoch 5] Batch 3905, Loss 0.2742937207221985\n",
      "[Training Epoch 5] Batch 3906, Loss 0.28747883439064026\n",
      "[Training Epoch 5] Batch 3907, Loss 0.24085113406181335\n",
      "[Training Epoch 5] Batch 3908, Loss 0.28723227977752686\n",
      "[Training Epoch 5] Batch 3909, Loss 0.27270829677581787\n",
      "[Training Epoch 5] Batch 3910, Loss 0.2452690452337265\n",
      "[Training Epoch 5] Batch 3911, Loss 0.23776531219482422\n",
      "[Training Epoch 5] Batch 3912, Loss 0.2638642191886902\n",
      "[Training Epoch 5] Batch 3913, Loss 0.2556624114513397\n",
      "[Training Epoch 5] Batch 3914, Loss 0.28389492630958557\n",
      "[Training Epoch 5] Batch 3915, Loss 0.27138131856918335\n",
      "[Training Epoch 5] Batch 3916, Loss 0.23596051335334778\n",
      "[Training Epoch 5] Batch 3917, Loss 0.2751769721508026\n",
      "[Training Epoch 5] Batch 3918, Loss 0.24540108442306519\n",
      "[Training Epoch 5] Batch 3919, Loss 0.24698194861412048\n",
      "[Training Epoch 5] Batch 3920, Loss 0.2280683070421219\n",
      "[Training Epoch 5] Batch 3921, Loss 0.2569253444671631\n",
      "[Training Epoch 5] Batch 3922, Loss 0.27366262674331665\n",
      "[Training Epoch 5] Batch 3923, Loss 0.25212857127189636\n",
      "[Training Epoch 5] Batch 3924, Loss 0.26943737268447876\n",
      "[Training Epoch 5] Batch 3925, Loss 0.24252864718437195\n",
      "[Training Epoch 5] Batch 3926, Loss 0.2821351885795593\n",
      "[Training Epoch 5] Batch 3927, Loss 0.27407264709472656\n",
      "[Training Epoch 5] Batch 3928, Loss 0.25390753149986267\n",
      "[Training Epoch 5] Batch 3929, Loss 0.2672865390777588\n",
      "[Training Epoch 5] Batch 3930, Loss 0.2663501799106598\n",
      "[Training Epoch 5] Batch 3931, Loss 0.2544337809085846\n",
      "[Training Epoch 5] Batch 3932, Loss 0.2407912313938141\n",
      "[Training Epoch 5] Batch 3933, Loss 0.27088481187820435\n",
      "[Training Epoch 5] Batch 3934, Loss 0.2638177275657654\n",
      "[Training Epoch 5] Batch 3935, Loss 0.2693721055984497\n",
      "[Training Epoch 5] Batch 3936, Loss 0.2561790645122528\n",
      "[Training Epoch 5] Batch 3937, Loss 0.2595464289188385\n",
      "[Training Epoch 5] Batch 3938, Loss 0.2599404454231262\n",
      "[Training Epoch 5] Batch 3939, Loss 0.24087578058242798\n",
      "[Training Epoch 5] Batch 3940, Loss 0.2543184161186218\n",
      "[Training Epoch 5] Batch 3941, Loss 0.25988125801086426\n",
      "[Training Epoch 5] Batch 3942, Loss 0.2577627897262573\n",
      "[Training Epoch 5] Batch 3943, Loss 0.23701119422912598\n",
      "[Training Epoch 5] Batch 3944, Loss 0.2755320370197296\n",
      "[Training Epoch 5] Batch 3945, Loss 0.23250889778137207\n",
      "[Training Epoch 5] Batch 3946, Loss 0.2558988332748413\n",
      "[Training Epoch 5] Batch 3947, Loss 0.2565998435020447\n",
      "[Training Epoch 5] Batch 3948, Loss 0.24739772081375122\n",
      "[Training Epoch 5] Batch 3949, Loss 0.23910415172576904\n",
      "[Training Epoch 5] Batch 3950, Loss 0.2701115608215332\n",
      "[Training Epoch 5] Batch 3951, Loss 0.25666552782058716\n",
      "[Training Epoch 5] Batch 3952, Loss 0.26113224029541016\n",
      "[Training Epoch 5] Batch 3953, Loss 0.2730335295200348\n",
      "[Training Epoch 5] Batch 3954, Loss 0.2968263030052185\n",
      "[Training Epoch 5] Batch 3955, Loss 0.22467681765556335\n",
      "[Training Epoch 5] Batch 3956, Loss 0.2994954586029053\n",
      "[Training Epoch 5] Batch 3957, Loss 0.2815781235694885\n",
      "[Training Epoch 5] Batch 3958, Loss 0.25071027874946594\n",
      "[Training Epoch 5] Batch 3959, Loss 0.2374960333108902\n",
      "[Training Epoch 5] Batch 3960, Loss 0.23782214522361755\n",
      "[Training Epoch 5] Batch 3961, Loss 0.276892751455307\n",
      "[Training Epoch 5] Batch 3962, Loss 0.24522942304611206\n",
      "[Training Epoch 5] Batch 3963, Loss 0.2831049859523773\n",
      "[Training Epoch 5] Batch 3964, Loss 0.2556826174259186\n",
      "[Training Epoch 5] Batch 3965, Loss 0.2539118528366089\n",
      "[Training Epoch 5] Batch 3966, Loss 0.25148093700408936\n",
      "[Training Epoch 5] Batch 3967, Loss 0.2443520426750183\n",
      "[Training Epoch 5] Batch 3968, Loss 0.2740579843521118\n",
      "[Training Epoch 5] Batch 3969, Loss 0.2610028386116028\n",
      "[Training Epoch 5] Batch 3970, Loss 0.25878986716270447\n",
      "[Training Epoch 5] Batch 3971, Loss 0.22199949622154236\n",
      "[Training Epoch 5] Batch 3972, Loss 0.25889742374420166\n",
      "[Training Epoch 5] Batch 3973, Loss 0.27294737100601196\n",
      "[Training Epoch 5] Batch 3974, Loss 0.24100852012634277\n",
      "[Training Epoch 5] Batch 3975, Loss 0.262567400932312\n",
      "[Training Epoch 5] Batch 3976, Loss 0.2656364440917969\n",
      "[Training Epoch 5] Batch 3977, Loss 0.27647948265075684\n",
      "[Training Epoch 5] Batch 3978, Loss 0.2683364152908325\n",
      "[Training Epoch 5] Batch 3979, Loss 0.25050443410873413\n",
      "[Training Epoch 5] Batch 3980, Loss 0.28092774748802185\n",
      "[Training Epoch 5] Batch 3981, Loss 0.24812406301498413\n",
      "[Training Epoch 5] Batch 3982, Loss 0.2860814929008484\n",
      "[Training Epoch 5] Batch 3983, Loss 0.25193336606025696\n",
      "[Training Epoch 5] Batch 3984, Loss 0.2734924554824829\n",
      "[Training Epoch 5] Batch 3985, Loss 0.2435014396905899\n",
      "[Training Epoch 5] Batch 3986, Loss 0.2643965780735016\n",
      "[Training Epoch 5] Batch 3987, Loss 0.26690953969955444\n",
      "[Training Epoch 5] Batch 3988, Loss 0.24289460480213165\n",
      "[Training Epoch 5] Batch 3989, Loss 0.25490802526474\n",
      "[Training Epoch 5] Batch 3990, Loss 0.28189730644226074\n",
      "[Training Epoch 5] Batch 3991, Loss 0.25104713439941406\n",
      "[Training Epoch 5] Batch 3992, Loss 0.24550454318523407\n",
      "[Training Epoch 5] Batch 3993, Loss 0.25272130966186523\n",
      "[Training Epoch 5] Batch 3994, Loss 0.24595527350902557\n",
      "[Training Epoch 5] Batch 3995, Loss 0.2504344582557678\n",
      "[Training Epoch 5] Batch 3996, Loss 0.2826758921146393\n",
      "[Training Epoch 5] Batch 3997, Loss 0.2725926339626312\n",
      "[Training Epoch 5] Batch 3998, Loss 0.2703823149204254\n",
      "[Training Epoch 5] Batch 3999, Loss 0.24619819223880768\n",
      "[Training Epoch 5] Batch 4000, Loss 0.24685561656951904\n",
      "[Training Epoch 5] Batch 4001, Loss 0.25411921739578247\n",
      "[Training Epoch 5] Batch 4002, Loss 0.2598083019256592\n",
      "[Training Epoch 5] Batch 4003, Loss 0.24908290803432465\n",
      "[Training Epoch 5] Batch 4004, Loss 0.2662872076034546\n",
      "[Training Epoch 5] Batch 4005, Loss 0.2856025695800781\n",
      "[Training Epoch 5] Batch 4006, Loss 0.2197183072566986\n",
      "[Training Epoch 5] Batch 4007, Loss 0.23966263234615326\n",
      "[Training Epoch 5] Batch 4008, Loss 0.25479525327682495\n",
      "[Training Epoch 5] Batch 4009, Loss 0.24368607997894287\n",
      "[Training Epoch 5] Batch 4010, Loss 0.2573814392089844\n",
      "[Training Epoch 5] Batch 4011, Loss 0.2532610297203064\n",
      "[Training Epoch 5] Batch 4012, Loss 0.269206702709198\n",
      "[Training Epoch 5] Batch 4013, Loss 0.25735005736351013\n",
      "[Training Epoch 5] Batch 4014, Loss 0.23130878806114197\n",
      "[Training Epoch 5] Batch 4015, Loss 0.243489071726799\n",
      "[Training Epoch 5] Batch 4016, Loss 0.27885299921035767\n",
      "[Training Epoch 5] Batch 4017, Loss 0.25038981437683105\n",
      "[Training Epoch 5] Batch 4018, Loss 0.2444840371608734\n",
      "[Training Epoch 5] Batch 4019, Loss 0.27246010303497314\n",
      "[Training Epoch 5] Batch 4020, Loss 0.25751250982284546\n",
      "[Training Epoch 5] Batch 4021, Loss 0.2938748002052307\n",
      "[Training Epoch 5] Batch 4022, Loss 0.27207234501838684\n",
      "[Training Epoch 5] Batch 4023, Loss 0.25103306770324707\n",
      "[Training Epoch 5] Batch 4024, Loss 0.23656438291072845\n",
      "[Training Epoch 5] Batch 4025, Loss 0.27730363607406616\n",
      "[Training Epoch 5] Batch 4026, Loss 0.2477147877216339\n",
      "[Training Epoch 5] Batch 4027, Loss 0.2571411430835724\n",
      "[Training Epoch 5] Batch 4028, Loss 0.2687583863735199\n",
      "[Training Epoch 5] Batch 4029, Loss 0.23100875318050385\n",
      "[Training Epoch 5] Batch 4030, Loss 0.23496949672698975\n",
      "[Training Epoch 5] Batch 4031, Loss 0.28212040662765503\n",
      "[Training Epoch 5] Batch 4032, Loss 0.24732156097888947\n",
      "[Training Epoch 5] Batch 4033, Loss 0.2521664500236511\n",
      "[Training Epoch 5] Batch 4034, Loss 0.25600528717041016\n",
      "[Training Epoch 5] Batch 4035, Loss 0.2629598081111908\n",
      "[Training Epoch 5] Batch 4036, Loss 0.26682883501052856\n",
      "[Training Epoch 5] Batch 4037, Loss 0.2598113417625427\n",
      "[Training Epoch 5] Batch 4038, Loss 0.2782331109046936\n",
      "[Training Epoch 5] Batch 4039, Loss 0.24912235140800476\n",
      "[Training Epoch 5] Batch 4040, Loss 0.25722068548202515\n",
      "[Training Epoch 5] Batch 4041, Loss 0.24471578001976013\n",
      "[Training Epoch 5] Batch 4042, Loss 0.2718569040298462\n",
      "[Training Epoch 5] Batch 4043, Loss 0.255254328250885\n",
      "[Training Epoch 5] Batch 4044, Loss 0.2704465687274933\n",
      "[Training Epoch 5] Batch 4045, Loss 0.2754620611667633\n",
      "[Training Epoch 5] Batch 4046, Loss 0.27194127440452576\n",
      "[Training Epoch 5] Batch 4047, Loss 0.27819812297821045\n",
      "[Training Epoch 5] Batch 4048, Loss 0.24266447126865387\n",
      "[Training Epoch 5] Batch 4049, Loss 0.2479497194290161\n",
      "[Training Epoch 5] Batch 4050, Loss 0.28343409299850464\n",
      "[Training Epoch 5] Batch 4051, Loss 0.23089802265167236\n",
      "[Training Epoch 5] Batch 4052, Loss 0.264797568321228\n",
      "[Training Epoch 5] Batch 4053, Loss 0.2403612732887268\n",
      "[Training Epoch 5] Batch 4054, Loss 0.26240986585617065\n",
      "[Training Epoch 5] Batch 4055, Loss 0.2749491035938263\n",
      "[Training Epoch 5] Batch 4056, Loss 0.2389017641544342\n",
      "[Training Epoch 5] Batch 4057, Loss 0.2669031620025635\n",
      "[Training Epoch 5] Batch 4058, Loss 0.25824230909347534\n",
      "[Training Epoch 5] Batch 4059, Loss 0.272355318069458\n",
      "[Training Epoch 5] Batch 4060, Loss 0.2810910642147064\n",
      "[Training Epoch 5] Batch 4061, Loss 0.2728237509727478\n",
      "[Training Epoch 5] Batch 4062, Loss 0.2629922032356262\n",
      "[Training Epoch 5] Batch 4063, Loss 0.2718431353569031\n",
      "[Training Epoch 5] Batch 4064, Loss 0.2692072093486786\n",
      "[Training Epoch 5] Batch 4065, Loss 0.2628931999206543\n",
      "[Training Epoch 5] Batch 4066, Loss 0.2376461923122406\n",
      "[Training Epoch 5] Batch 4067, Loss 0.2629568576812744\n",
      "[Training Epoch 5] Batch 4068, Loss 0.25525060296058655\n",
      "[Training Epoch 5] Batch 4069, Loss 0.2817263603210449\n",
      "[Training Epoch 5] Batch 4070, Loss 0.2660800516605377\n",
      "[Training Epoch 5] Batch 4071, Loss 0.2502903342247009\n",
      "[Training Epoch 5] Batch 4072, Loss 0.2549513876438141\n",
      "[Training Epoch 5] Batch 4073, Loss 0.2653067409992218\n",
      "[Training Epoch 5] Batch 4074, Loss 0.27140581607818604\n",
      "[Training Epoch 5] Batch 4075, Loss 0.2741450071334839\n",
      "[Training Epoch 5] Batch 4076, Loss 0.2763330340385437\n",
      "[Training Epoch 5] Batch 4077, Loss 0.25867217779159546\n",
      "[Training Epoch 5] Batch 4078, Loss 0.26453542709350586\n",
      "[Training Epoch 5] Batch 4079, Loss 0.26425379514694214\n",
      "[Training Epoch 5] Batch 4080, Loss 0.2610991299152374\n",
      "[Training Epoch 5] Batch 4081, Loss 0.2447807341814041\n",
      "[Training Epoch 5] Batch 4082, Loss 0.22404247522354126\n",
      "[Training Epoch 5] Batch 4083, Loss 0.26388248801231384\n",
      "[Training Epoch 5] Batch 4084, Loss 0.26532548666000366\n",
      "[Training Epoch 5] Batch 4085, Loss 0.27163976430892944\n",
      "[Training Epoch 5] Batch 4086, Loss 0.24198691546916962\n",
      "[Training Epoch 5] Batch 4087, Loss 0.2409982830286026\n",
      "[Training Epoch 5] Batch 4088, Loss 0.2880234718322754\n",
      "[Training Epoch 5] Batch 4089, Loss 0.23613861203193665\n",
      "[Training Epoch 5] Batch 4090, Loss 0.2643049657344818\n",
      "[Training Epoch 5] Batch 4091, Loss 0.24304190278053284\n",
      "[Training Epoch 5] Batch 4092, Loss 0.28520065546035767\n",
      "[Training Epoch 5] Batch 4093, Loss 0.23242491483688354\n",
      "[Training Epoch 5] Batch 4094, Loss 0.21994423866271973\n",
      "[Training Epoch 5] Batch 4095, Loss 0.2529827952384949\n",
      "[Training Epoch 5] Batch 4096, Loss 0.23753419518470764\n",
      "[Training Epoch 5] Batch 4097, Loss 0.22282128036022186\n",
      "[Training Epoch 5] Batch 4098, Loss 0.2711021602153778\n",
      "[Training Epoch 5] Batch 4099, Loss 0.2714429795742035\n",
      "[Training Epoch 5] Batch 4100, Loss 0.24971625208854675\n",
      "[Training Epoch 5] Batch 4101, Loss 0.25582969188690186\n",
      "[Training Epoch 5] Batch 4102, Loss 0.27830594778060913\n",
      "[Training Epoch 5] Batch 4103, Loss 0.263316810131073\n",
      "[Training Epoch 5] Batch 4104, Loss 0.2730156183242798\n",
      "[Training Epoch 5] Batch 4105, Loss 0.2808701992034912\n",
      "[Training Epoch 5] Batch 4106, Loss 0.26874807476997375\n",
      "[Training Epoch 5] Batch 4107, Loss 0.28413939476013184\n",
      "[Training Epoch 5] Batch 4108, Loss 0.24242755770683289\n",
      "[Training Epoch 5] Batch 4109, Loss 0.25573796033859253\n",
      "[Training Epoch 5] Batch 4110, Loss 0.28368279337882996\n",
      "[Training Epoch 5] Batch 4111, Loss 0.2605747580528259\n",
      "[Training Epoch 5] Batch 4112, Loss 0.2545459568500519\n",
      "[Training Epoch 5] Batch 4113, Loss 0.2421729564666748\n",
      "[Training Epoch 5] Batch 4114, Loss 0.26736801862716675\n",
      "[Training Epoch 5] Batch 4115, Loss 0.24160706996917725\n",
      "[Training Epoch 5] Batch 4116, Loss 0.25120556354522705\n",
      "[Training Epoch 5] Batch 4117, Loss 0.25891873240470886\n",
      "[Training Epoch 5] Batch 4118, Loss 0.244379922747612\n",
      "[Training Epoch 5] Batch 4119, Loss 0.29526978731155396\n",
      "[Training Epoch 5] Batch 4120, Loss 0.23606185615062714\n",
      "[Training Epoch 5] Batch 4121, Loss 0.2796114683151245\n",
      "[Training Epoch 5] Batch 4122, Loss 0.2549651563167572\n",
      "[Training Epoch 5] Batch 4123, Loss 0.2796560227870941\n",
      "[Training Epoch 5] Batch 4124, Loss 0.29242652654647827\n",
      "[Training Epoch 5] Batch 4125, Loss 0.25872281193733215\n",
      "[Training Epoch 5] Batch 4126, Loss 0.3023275136947632\n",
      "[Training Epoch 5] Batch 4127, Loss 0.279732346534729\n",
      "[Training Epoch 5] Batch 4128, Loss 0.26698923110961914\n",
      "[Training Epoch 5] Batch 4129, Loss 0.2513228952884674\n",
      "[Training Epoch 5] Batch 4130, Loss 0.2724078595638275\n",
      "[Training Epoch 5] Batch 4131, Loss 0.2685566246509552\n",
      "[Training Epoch 5] Batch 4132, Loss 0.2643243670463562\n",
      "[Training Epoch 5] Batch 4133, Loss 0.26752936840057373\n",
      "[Training Epoch 5] Batch 4134, Loss 0.2626791000366211\n",
      "[Training Epoch 5] Batch 4135, Loss 0.2808709740638733\n",
      "[Training Epoch 5] Batch 4136, Loss 0.27676913142204285\n",
      "[Training Epoch 5] Batch 4137, Loss 0.2366248071193695\n",
      "[Training Epoch 5] Batch 4138, Loss 0.2483511120080948\n",
      "[Training Epoch 5] Batch 4139, Loss 0.2641782760620117\n",
      "[Training Epoch 5] Batch 4140, Loss 0.27584630250930786\n",
      "[Training Epoch 5] Batch 4141, Loss 0.23058201372623444\n",
      "[Training Epoch 5] Batch 4142, Loss 0.2627294063568115\n",
      "[Training Epoch 5] Batch 4143, Loss 0.296212375164032\n",
      "[Training Epoch 5] Batch 4144, Loss 0.2273041307926178\n",
      "[Training Epoch 5] Batch 4145, Loss 0.25455379486083984\n",
      "[Training Epoch 5] Batch 4146, Loss 0.26269057393074036\n",
      "[Training Epoch 5] Batch 4147, Loss 0.2736354172229767\n",
      "[Training Epoch 5] Batch 4148, Loss 0.2522873282432556\n",
      "[Training Epoch 5] Batch 4149, Loss 0.2614503800868988\n",
      "[Training Epoch 5] Batch 4150, Loss 0.2595187723636627\n",
      "[Training Epoch 5] Batch 4151, Loss 0.25424736738204956\n",
      "[Training Epoch 5] Batch 4152, Loss 0.2424885332584381\n",
      "[Training Epoch 5] Batch 4153, Loss 0.21819737553596497\n",
      "[Training Epoch 5] Batch 4154, Loss 0.239181786775589\n",
      "[Training Epoch 5] Batch 4155, Loss 0.23745006322860718\n",
      "[Training Epoch 5] Batch 4156, Loss 0.2611237168312073\n",
      "[Training Epoch 5] Batch 4157, Loss 0.27598464488983154\n",
      "[Training Epoch 5] Batch 4158, Loss 0.23577247560024261\n",
      "[Training Epoch 5] Batch 4159, Loss 0.24613535404205322\n",
      "[Training Epoch 5] Batch 4160, Loss 0.26130473613739014\n",
      "[Training Epoch 5] Batch 4161, Loss 0.2765880525112152\n",
      "[Training Epoch 5] Batch 4162, Loss 0.24626676738262177\n",
      "[Training Epoch 5] Batch 4163, Loss 0.2734073996543884\n",
      "[Training Epoch 5] Batch 4164, Loss 0.28935787081718445\n",
      "[Training Epoch 5] Batch 4165, Loss 0.2698337137699127\n",
      "[Training Epoch 5] Batch 4166, Loss 0.25525879859924316\n",
      "[Training Epoch 5] Batch 4167, Loss 0.2545812726020813\n",
      "[Training Epoch 5] Batch 4168, Loss 0.2441045045852661\n",
      "[Training Epoch 5] Batch 4169, Loss 0.27921342849731445\n",
      "[Training Epoch 5] Batch 4170, Loss 0.27448534965515137\n",
      "[Training Epoch 5] Batch 4171, Loss 0.27998992800712585\n",
      "[Training Epoch 5] Batch 4172, Loss 0.23778679966926575\n",
      "[Training Epoch 5] Batch 4173, Loss 0.26732102036476135\n",
      "[Training Epoch 5] Batch 4174, Loss 0.2755637466907501\n",
      "[Training Epoch 5] Batch 4175, Loss 0.23926928639411926\n",
      "[Training Epoch 5] Batch 4176, Loss 0.2602161765098572\n",
      "[Training Epoch 5] Batch 4177, Loss 0.24517112970352173\n",
      "[Training Epoch 5] Batch 4178, Loss 0.2664829194545746\n",
      "[Training Epoch 5] Batch 4179, Loss 0.24490708112716675\n",
      "[Training Epoch 5] Batch 4180, Loss 0.30462709069252014\n",
      "[Training Epoch 5] Batch 4181, Loss 0.2549302577972412\n",
      "[Training Epoch 5] Batch 4182, Loss 0.2595728635787964\n",
      "[Training Epoch 5] Batch 4183, Loss 0.22357016801834106\n",
      "[Training Epoch 5] Batch 4184, Loss 0.2289951741695404\n",
      "[Training Epoch 5] Batch 4185, Loss 0.26184940338134766\n",
      "[Training Epoch 5] Batch 4186, Loss 0.251460999250412\n",
      "[Training Epoch 5] Batch 4187, Loss 0.2662507891654968\n",
      "[Training Epoch 5] Batch 4188, Loss 0.2783868908882141\n",
      "[Training Epoch 5] Batch 4189, Loss 0.2314647138118744\n",
      "[Training Epoch 5] Batch 4190, Loss 0.2355409562587738\n",
      "[Training Epoch 5] Batch 4191, Loss 0.2162080854177475\n",
      "[Training Epoch 5] Batch 4192, Loss 0.2247292548418045\n",
      "[Training Epoch 5] Batch 4193, Loss 0.2881976366043091\n",
      "[Training Epoch 5] Batch 4194, Loss 0.24744254350662231\n",
      "[Training Epoch 5] Batch 4195, Loss 0.2426304817199707\n",
      "[Training Epoch 5] Batch 4196, Loss 0.2646961808204651\n",
      "[Training Epoch 5] Batch 4197, Loss 0.268435001373291\n",
      "[Training Epoch 5] Batch 4198, Loss 0.2678185701370239\n",
      "[Training Epoch 5] Batch 4199, Loss 0.2933465540409088\n",
      "[Training Epoch 5] Batch 4200, Loss 0.2634391784667969\n",
      "[Training Epoch 5] Batch 4201, Loss 0.2700522243976593\n",
      "[Training Epoch 5] Batch 4202, Loss 0.25585147738456726\n",
      "[Training Epoch 5] Batch 4203, Loss 0.2741134464740753\n",
      "[Training Epoch 5] Batch 4204, Loss 0.2419102042913437\n",
      "[Training Epoch 5] Batch 4205, Loss 0.284823477268219\n",
      "[Training Epoch 5] Batch 4206, Loss 0.2805498540401459\n",
      "[Training Epoch 5] Batch 4207, Loss 0.2701917290687561\n",
      "[Training Epoch 5] Batch 4208, Loss 0.26707786321640015\n",
      "[Training Epoch 5] Batch 4209, Loss 0.25062572956085205\n",
      "[Training Epoch 5] Batch 4210, Loss 0.2659892439842224\n",
      "[Training Epoch 5] Batch 4211, Loss 0.24658861756324768\n",
      "[Training Epoch 5] Batch 4212, Loss 0.2645496428012848\n",
      "[Training Epoch 5] Batch 4213, Loss 0.24704372882843018\n",
      "[Training Epoch 5] Batch 4214, Loss 0.2795124053955078\n",
      "[Training Epoch 5] Batch 4215, Loss 0.27292734384536743\n",
      "[Training Epoch 5] Batch 4216, Loss 0.2702268362045288\n",
      "[Training Epoch 5] Batch 4217, Loss 0.2469608336687088\n",
      "[Training Epoch 5] Batch 4218, Loss 0.28450554609298706\n",
      "[Training Epoch 5] Batch 4219, Loss 0.25510174036026\n",
      "[Training Epoch 5] Batch 4220, Loss 0.2772868871688843\n",
      "[Training Epoch 5] Batch 4221, Loss 0.26019757986068726\n",
      "[Training Epoch 5] Batch 4222, Loss 0.2686825394630432\n",
      "[Training Epoch 5] Batch 4223, Loss 0.2703031897544861\n",
      "[Training Epoch 5] Batch 4224, Loss 0.24539390206336975\n",
      "[Training Epoch 5] Batch 4225, Loss 0.28559643030166626\n",
      "[Training Epoch 5] Batch 4226, Loss 0.24464848637580872\n",
      "[Training Epoch 5] Batch 4227, Loss 0.29930394887924194\n",
      "[Training Epoch 5] Batch 4228, Loss 0.23977062106132507\n",
      "[Training Epoch 5] Batch 4229, Loss 0.2583833932876587\n",
      "[Training Epoch 5] Batch 4230, Loss 0.24512234330177307\n",
      "[Training Epoch 5] Batch 4231, Loss 0.2625824511051178\n",
      "[Training Epoch 5] Batch 4232, Loss 0.2863193154335022\n",
      "[Training Epoch 5] Batch 4233, Loss 0.26593464612960815\n",
      "[Training Epoch 5] Batch 4234, Loss 0.25658220052719116\n",
      "[Training Epoch 5] Batch 4235, Loss 0.27120715379714966\n",
      "[Training Epoch 5] Batch 4236, Loss 0.26117444038391113\n",
      "[Training Epoch 5] Batch 4237, Loss 0.25933000445365906\n",
      "[Training Epoch 5] Batch 4238, Loss 0.2718104124069214\n",
      "[Training Epoch 5] Batch 4239, Loss 0.26536476612091064\n",
      "[Training Epoch 5] Batch 4240, Loss 0.2491767853498459\n",
      "[Training Epoch 5] Batch 4241, Loss 0.27050304412841797\n",
      "[Training Epoch 5] Batch 4242, Loss 0.2531554102897644\n",
      "[Training Epoch 5] Batch 4243, Loss 0.26303327083587646\n",
      "[Training Epoch 5] Batch 4244, Loss 0.2740253210067749\n",
      "[Training Epoch 5] Batch 4245, Loss 0.2570652961730957\n",
      "[Training Epoch 5] Batch 4246, Loss 0.27915698289871216\n",
      "[Training Epoch 5] Batch 4247, Loss 0.2622033953666687\n",
      "[Training Epoch 5] Batch 4248, Loss 0.28111472725868225\n",
      "[Training Epoch 5] Batch 4249, Loss 0.2801573872566223\n",
      "[Training Epoch 5] Batch 4250, Loss 0.27943575382232666\n",
      "[Training Epoch 5] Batch 4251, Loss 0.2769029438495636\n",
      "[Training Epoch 5] Batch 4252, Loss 0.22150608897209167\n",
      "[Training Epoch 5] Batch 4253, Loss 0.2529980540275574\n",
      "[Training Epoch 5] Batch 4254, Loss 0.253818154335022\n",
      "[Training Epoch 5] Batch 4255, Loss 0.27332252264022827\n",
      "[Training Epoch 5] Batch 4256, Loss 0.28809279203414917\n",
      "[Training Epoch 5] Batch 4257, Loss 0.2742317020893097\n",
      "[Training Epoch 5] Batch 4258, Loss 0.25559002161026\n",
      "[Training Epoch 5] Batch 4259, Loss 0.266817569732666\n",
      "[Training Epoch 5] Batch 4260, Loss 0.27255940437316895\n",
      "[Training Epoch 5] Batch 4261, Loss 0.2558177709579468\n",
      "[Training Epoch 5] Batch 4262, Loss 0.25404661893844604\n",
      "[Training Epoch 5] Batch 4263, Loss 0.27912870049476624\n",
      "[Training Epoch 5] Batch 4264, Loss 0.24897412955760956\n",
      "[Training Epoch 5] Batch 4265, Loss 0.26449471712112427\n",
      "[Training Epoch 5] Batch 4266, Loss 0.251062273979187\n",
      "[Training Epoch 5] Batch 4267, Loss 0.2478596419095993\n",
      "[Training Epoch 5] Batch 4268, Loss 0.30684202909469604\n",
      "[Training Epoch 5] Batch 4269, Loss 0.2847004532814026\n",
      "[Training Epoch 5] Batch 4270, Loss 0.2740454375743866\n",
      "[Training Epoch 5] Batch 4271, Loss 0.23537878692150116\n",
      "[Training Epoch 5] Batch 4272, Loss 0.2842845320701599\n",
      "[Training Epoch 5] Batch 4273, Loss 0.2663990259170532\n",
      "[Training Epoch 5] Batch 4274, Loss 0.26667460799217224\n",
      "[Training Epoch 5] Batch 4275, Loss 0.2640109360218048\n",
      "[Training Epoch 5] Batch 4276, Loss 0.2559796869754791\n",
      "[Training Epoch 5] Batch 4277, Loss 0.2743379473686218\n",
      "[Training Epoch 5] Batch 4278, Loss 0.2627986967563629\n",
      "[Training Epoch 5] Batch 4279, Loss 0.2673707902431488\n",
      "[Training Epoch 5] Batch 4280, Loss 0.24359816312789917\n",
      "[Training Epoch 5] Batch 4281, Loss 0.25894567370414734\n",
      "[Training Epoch 5] Batch 4282, Loss 0.25913354754447937\n",
      "[Training Epoch 5] Batch 4283, Loss 0.28933218121528625\n",
      "[Training Epoch 5] Batch 4284, Loss 0.25885337591171265\n",
      "[Training Epoch 5] Batch 4285, Loss 0.2524466812610626\n",
      "[Training Epoch 5] Batch 4286, Loss 0.2792564034461975\n",
      "[Training Epoch 5] Batch 4287, Loss 0.27909815311431885\n",
      "[Training Epoch 5] Batch 4288, Loss 0.26484328508377075\n",
      "[Training Epoch 5] Batch 4289, Loss 0.2304353415966034\n",
      "[Training Epoch 5] Batch 4290, Loss 0.24126771092414856\n",
      "[Training Epoch 5] Batch 4291, Loss 0.2729547619819641\n",
      "[Training Epoch 5] Batch 4292, Loss 0.27871572971343994\n",
      "[Training Epoch 5] Batch 4293, Loss 0.2653384208679199\n",
      "[Training Epoch 5] Batch 4294, Loss 0.2564693093299866\n",
      "[Training Epoch 5] Batch 4295, Loss 0.2511388659477234\n",
      "[Training Epoch 5] Batch 4296, Loss 0.2320956289768219\n",
      "[Training Epoch 5] Batch 4297, Loss 0.27231845259666443\n",
      "[Training Epoch 5] Batch 4298, Loss 0.25495877861976624\n",
      "[Training Epoch 5] Batch 4299, Loss 0.28484565019607544\n",
      "[Training Epoch 5] Batch 4300, Loss 0.23567454516887665\n",
      "[Training Epoch 5] Batch 4301, Loss 0.2844577729701996\n",
      "[Training Epoch 5] Batch 4302, Loss 0.2618216872215271\n",
      "[Training Epoch 5] Batch 4303, Loss 0.24086116254329681\n",
      "[Training Epoch 5] Batch 4304, Loss 0.2586435377597809\n",
      "[Training Epoch 5] Batch 4305, Loss 0.26902562379837036\n",
      "[Training Epoch 5] Batch 4306, Loss 0.260847270488739\n",
      "[Training Epoch 5] Batch 4307, Loss 0.254644513130188\n",
      "[Training Epoch 5] Batch 4308, Loss 0.2677001357078552\n",
      "[Training Epoch 5] Batch 4309, Loss 0.2644069194793701\n",
      "[Training Epoch 5] Batch 4310, Loss 0.24312153458595276\n",
      "[Training Epoch 5] Batch 4311, Loss 0.25417906045913696\n",
      "[Training Epoch 5] Batch 4312, Loss 0.24939508736133575\n",
      "[Training Epoch 5] Batch 4313, Loss 0.2745678722858429\n",
      "[Training Epoch 5] Batch 4314, Loss 0.2505446672439575\n",
      "[Training Epoch 5] Batch 4315, Loss 0.25673478841781616\n",
      "[Training Epoch 5] Batch 4316, Loss 0.2487122118473053\n",
      "[Training Epoch 5] Batch 4317, Loss 0.2715194821357727\n",
      "[Training Epoch 5] Batch 4318, Loss 0.27393051981925964\n",
      "[Training Epoch 5] Batch 4319, Loss 0.2665187120437622\n",
      "[Training Epoch 5] Batch 4320, Loss 0.2538280487060547\n",
      "[Training Epoch 5] Batch 4321, Loss 0.2651921510696411\n",
      "[Training Epoch 5] Batch 4322, Loss 0.276191771030426\n",
      "[Training Epoch 5] Batch 4323, Loss 0.2949853539466858\n",
      "[Training Epoch 5] Batch 4324, Loss 0.24989932775497437\n",
      "[Training Epoch 5] Batch 4325, Loss 0.25745415687561035\n",
      "[Training Epoch 5] Batch 4326, Loss 0.260286420583725\n",
      "[Training Epoch 5] Batch 4327, Loss 0.2781815528869629\n",
      "[Training Epoch 5] Batch 4328, Loss 0.2538336217403412\n",
      "[Training Epoch 5] Batch 4329, Loss 0.26114922761917114\n",
      "[Training Epoch 5] Batch 4330, Loss 0.2424445003271103\n",
      "[Training Epoch 5] Batch 4331, Loss 0.2749926447868347\n",
      "[Training Epoch 5] Batch 4332, Loss 0.2629759609699249\n",
      "[Training Epoch 5] Batch 4333, Loss 0.25437456369400024\n",
      "[Training Epoch 5] Batch 4334, Loss 0.2664130628108978\n",
      "[Training Epoch 5] Batch 4335, Loss 0.25190240144729614\n",
      "[Training Epoch 5] Batch 4336, Loss 0.24308735132217407\n",
      "[Training Epoch 5] Batch 4337, Loss 0.27363765239715576\n",
      "[Training Epoch 5] Batch 4338, Loss 0.24903947114944458\n",
      "[Training Epoch 5] Batch 4339, Loss 0.26141494512557983\n",
      "[Training Epoch 5] Batch 4340, Loss 0.2691103219985962\n",
      "[Training Epoch 5] Batch 4341, Loss 0.25285810232162476\n",
      "[Training Epoch 5] Batch 4342, Loss 0.26635393500328064\n",
      "[Training Epoch 5] Batch 4343, Loss 0.2386392205953598\n",
      "[Training Epoch 5] Batch 4344, Loss 0.27296942472457886\n",
      "[Training Epoch 5] Batch 4345, Loss 0.2655947208404541\n",
      "[Training Epoch 5] Batch 4346, Loss 0.24584195017814636\n",
      "[Training Epoch 5] Batch 4347, Loss 0.24730122089385986\n",
      "[Training Epoch 5] Batch 4348, Loss 0.2626663148403168\n",
      "[Training Epoch 5] Batch 4349, Loss 0.24836328625679016\n",
      "[Training Epoch 5] Batch 4350, Loss 0.26179635524749756\n",
      "[Training Epoch 5] Batch 4351, Loss 0.25728702545166016\n",
      "[Training Epoch 5] Batch 4352, Loss 0.252738356590271\n",
      "[Training Epoch 5] Batch 4353, Loss 0.2680419087409973\n",
      "[Training Epoch 5] Batch 4354, Loss 0.2666151821613312\n",
      "[Training Epoch 5] Batch 4355, Loss 0.2529663145542145\n",
      "[Training Epoch 5] Batch 4356, Loss 0.27593401074409485\n",
      "[Training Epoch 5] Batch 4357, Loss 0.2677590548992157\n",
      "[Training Epoch 5] Batch 4358, Loss 0.25190407037734985\n",
      "[Training Epoch 5] Batch 4359, Loss 0.23782141506671906\n",
      "[Training Epoch 5] Batch 4360, Loss 0.3040498197078705\n",
      "[Training Epoch 5] Batch 4361, Loss 0.26933369040489197\n",
      "[Training Epoch 5] Batch 4362, Loss 0.28073808550834656\n",
      "[Training Epoch 5] Batch 4363, Loss 0.2656898498535156\n",
      "[Training Epoch 5] Batch 4364, Loss 0.2895737886428833\n",
      "[Training Epoch 5] Batch 4365, Loss 0.22083643078804016\n",
      "[Training Epoch 5] Batch 4366, Loss 0.26436322927474976\n",
      "[Training Epoch 5] Batch 4367, Loss 0.3032726049423218\n",
      "[Training Epoch 5] Batch 4368, Loss 0.2809992730617523\n",
      "[Training Epoch 5] Batch 4369, Loss 0.2556156516075134\n",
      "[Training Epoch 5] Batch 4370, Loss 0.24781817197799683\n",
      "[Training Epoch 5] Batch 4371, Loss 0.23038628697395325\n",
      "[Training Epoch 5] Batch 4372, Loss 0.2659861445426941\n",
      "[Training Epoch 5] Batch 4373, Loss 0.2645474672317505\n",
      "[Training Epoch 5] Batch 4374, Loss 0.2600577175617218\n",
      "[Training Epoch 5] Batch 4375, Loss 0.24909767508506775\n",
      "[Training Epoch 5] Batch 4376, Loss 0.27619031071662903\n",
      "[Training Epoch 5] Batch 4377, Loss 0.26041945815086365\n",
      "[Training Epoch 5] Batch 4378, Loss 0.23308277130126953\n",
      "[Training Epoch 5] Batch 4379, Loss 0.24759703874588013\n",
      "[Training Epoch 5] Batch 4380, Loss 0.2476787567138672\n",
      "[Training Epoch 5] Batch 4381, Loss 0.2743271291255951\n",
      "[Training Epoch 5] Batch 4382, Loss 0.1744556874036789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2252.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 5] Precision = 0.2617, Recall = 0.7735\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.25592997670173645\n",
      "[Training Epoch 6] Batch 1, Loss 0.282188355922699\n",
      "[Training Epoch 6] Batch 2, Loss 0.23869386315345764\n",
      "[Training Epoch 6] Batch 3, Loss 0.2686041593551636\n",
      "[Training Epoch 6] Batch 4, Loss 0.23837636411190033\n",
      "[Training Epoch 6] Batch 5, Loss 0.27058589458465576\n",
      "[Training Epoch 6] Batch 6, Loss 0.2397046536207199\n",
      "[Training Epoch 6] Batch 7, Loss 0.27085453271865845\n",
      "[Training Epoch 6] Batch 8, Loss 0.2577418386936188\n",
      "[Training Epoch 6] Batch 9, Loss 0.25823163986206055\n",
      "[Training Epoch 6] Batch 10, Loss 0.24811668694019318\n",
      "[Training Epoch 6] Batch 11, Loss 0.2471630573272705\n",
      "[Training Epoch 6] Batch 12, Loss 0.2279222160577774\n",
      "[Training Epoch 6] Batch 13, Loss 0.23365555703639984\n",
      "[Training Epoch 6] Batch 14, Loss 0.25727027654647827\n",
      "[Training Epoch 6] Batch 15, Loss 0.24965953826904297\n",
      "[Training Epoch 6] Batch 16, Loss 0.22354696691036224\n",
      "[Training Epoch 6] Batch 17, Loss 0.2752825617790222\n",
      "[Training Epoch 6] Batch 18, Loss 0.2321244776248932\n",
      "[Training Epoch 6] Batch 19, Loss 0.2634497880935669\n",
      "[Training Epoch 6] Batch 20, Loss 0.2422451674938202\n",
      "[Training Epoch 6] Batch 21, Loss 0.2530273497104645\n",
      "[Training Epoch 6] Batch 22, Loss 0.25041335821151733\n",
      "[Training Epoch 6] Batch 23, Loss 0.25477832555770874\n",
      "[Training Epoch 6] Batch 24, Loss 0.24693144857883453\n",
      "[Training Epoch 6] Batch 25, Loss 0.24929499626159668\n",
      "[Training Epoch 6] Batch 26, Loss 0.27817797660827637\n",
      "[Training Epoch 6] Batch 27, Loss 0.26958388090133667\n",
      "[Training Epoch 6] Batch 28, Loss 0.2508615255355835\n",
      "[Training Epoch 6] Batch 29, Loss 0.2280331701040268\n",
      "[Training Epoch 6] Batch 30, Loss 0.28060880303382874\n",
      "[Training Epoch 6] Batch 31, Loss 0.26472362875938416\n",
      "[Training Epoch 6] Batch 32, Loss 0.2817845344543457\n",
      "[Training Epoch 6] Batch 33, Loss 0.26597875356674194\n",
      "[Training Epoch 6] Batch 34, Loss 0.24077916145324707\n",
      "[Training Epoch 6] Batch 35, Loss 0.258916974067688\n",
      "[Training Epoch 6] Batch 36, Loss 0.2456119954586029\n",
      "[Training Epoch 6] Batch 37, Loss 0.24872323870658875\n",
      "[Training Epoch 6] Batch 38, Loss 0.23998452723026276\n",
      "[Training Epoch 6] Batch 39, Loss 0.24882256984710693\n",
      "[Training Epoch 6] Batch 40, Loss 0.2576479911804199\n",
      "[Training Epoch 6] Batch 41, Loss 0.2574513554573059\n",
      "[Training Epoch 6] Batch 42, Loss 0.2489115297794342\n",
      "[Training Epoch 6] Batch 43, Loss 0.24245142936706543\n",
      "[Training Epoch 6] Batch 44, Loss 0.22272542119026184\n",
      "[Training Epoch 6] Batch 45, Loss 0.2482389211654663\n",
      "[Training Epoch 6] Batch 46, Loss 0.2263856828212738\n",
      "[Training Epoch 6] Batch 47, Loss 0.2697865962982178\n",
      "[Training Epoch 6] Batch 48, Loss 0.25967222452163696\n",
      "[Training Epoch 6] Batch 49, Loss 0.27437087893486023\n",
      "[Training Epoch 6] Batch 50, Loss 0.3036755919456482\n",
      "[Training Epoch 6] Batch 51, Loss 0.24210326373577118\n",
      "[Training Epoch 6] Batch 52, Loss 0.29042351245880127\n",
      "[Training Epoch 6] Batch 53, Loss 0.2619038224220276\n",
      "[Training Epoch 6] Batch 54, Loss 0.27567529678344727\n",
      "[Training Epoch 6] Batch 55, Loss 0.2464759349822998\n",
      "[Training Epoch 6] Batch 56, Loss 0.24427470564842224\n",
      "[Training Epoch 6] Batch 57, Loss 0.234256774187088\n",
      "[Training Epoch 6] Batch 58, Loss 0.2587370276451111\n",
      "[Training Epoch 6] Batch 59, Loss 0.2621857523918152\n",
      "[Training Epoch 6] Batch 60, Loss 0.27268195152282715\n",
      "[Training Epoch 6] Batch 61, Loss 0.22725901007652283\n",
      "[Training Epoch 6] Batch 62, Loss 0.24994859099388123\n",
      "[Training Epoch 6] Batch 63, Loss 0.24711401760578156\n",
      "[Training Epoch 6] Batch 64, Loss 0.2490105926990509\n",
      "[Training Epoch 6] Batch 65, Loss 0.2626451551914215\n",
      "[Training Epoch 6] Batch 66, Loss 0.2619835436344147\n",
      "[Training Epoch 6] Batch 67, Loss 0.25332945585250854\n",
      "[Training Epoch 6] Batch 68, Loss 0.25634413957595825\n",
      "[Training Epoch 6] Batch 69, Loss 0.2496265172958374\n",
      "[Training Epoch 6] Batch 70, Loss 0.26488110423088074\n",
      "[Training Epoch 6] Batch 71, Loss 0.24781587719917297\n",
      "[Training Epoch 6] Batch 72, Loss 0.2807720899581909\n",
      "[Training Epoch 6] Batch 73, Loss 0.27486753463745117\n",
      "[Training Epoch 6] Batch 74, Loss 0.2317202240228653\n",
      "[Training Epoch 6] Batch 75, Loss 0.250796377658844\n",
      "[Training Epoch 6] Batch 76, Loss 0.24190664291381836\n",
      "[Training Epoch 6] Batch 77, Loss 0.25307485461235046\n",
      "[Training Epoch 6] Batch 78, Loss 0.276117742061615\n",
      "[Training Epoch 6] Batch 79, Loss 0.25645703077316284\n",
      "[Training Epoch 6] Batch 80, Loss 0.2568018436431885\n",
      "[Training Epoch 6] Batch 81, Loss 0.24900297820568085\n",
      "[Training Epoch 6] Batch 82, Loss 0.2646821141242981\n",
      "[Training Epoch 6] Batch 83, Loss 0.2759290337562561\n",
      "[Training Epoch 6] Batch 84, Loss 0.26348596811294556\n",
      "[Training Epoch 6] Batch 85, Loss 0.260529100894928\n",
      "[Training Epoch 6] Batch 86, Loss 0.22330139577388763\n",
      "[Training Epoch 6] Batch 87, Loss 0.2516878843307495\n",
      "[Training Epoch 6] Batch 88, Loss 0.2447376549243927\n",
      "[Training Epoch 6] Batch 89, Loss 0.2689531147480011\n",
      "[Training Epoch 6] Batch 90, Loss 0.24724872410297394\n",
      "[Training Epoch 6] Batch 91, Loss 0.2644388973712921\n",
      "[Training Epoch 6] Batch 92, Loss 0.2539491653442383\n",
      "[Training Epoch 6] Batch 93, Loss 0.2424200475215912\n",
      "[Training Epoch 6] Batch 94, Loss 0.2433159500360489\n",
      "[Training Epoch 6] Batch 95, Loss 0.25275206565856934\n",
      "[Training Epoch 6] Batch 96, Loss 0.25127482414245605\n",
      "[Training Epoch 6] Batch 97, Loss 0.27950912714004517\n",
      "[Training Epoch 6] Batch 98, Loss 0.257668673992157\n",
      "[Training Epoch 6] Batch 99, Loss 0.2500872313976288\n",
      "[Training Epoch 6] Batch 100, Loss 0.2589753270149231\n",
      "[Training Epoch 6] Batch 101, Loss 0.2919507622718811\n",
      "[Training Epoch 6] Batch 102, Loss 0.2336137890815735\n",
      "[Training Epoch 6] Batch 103, Loss 0.24456313252449036\n",
      "[Training Epoch 6] Batch 104, Loss 0.22845464944839478\n",
      "[Training Epoch 6] Batch 105, Loss 0.24469882249832153\n",
      "[Training Epoch 6] Batch 106, Loss 0.2742401957511902\n",
      "[Training Epoch 6] Batch 107, Loss 0.25329774618148804\n",
      "[Training Epoch 6] Batch 108, Loss 0.2703026831150055\n",
      "[Training Epoch 6] Batch 109, Loss 0.25442785024642944\n",
      "[Training Epoch 6] Batch 110, Loss 0.2427007406949997\n",
      "[Training Epoch 6] Batch 111, Loss 0.25470930337905884\n",
      "[Training Epoch 6] Batch 112, Loss 0.24183207750320435\n",
      "[Training Epoch 6] Batch 113, Loss 0.2407429814338684\n",
      "[Training Epoch 6] Batch 114, Loss 0.293830931186676\n",
      "[Training Epoch 6] Batch 115, Loss 0.263017475605011\n",
      "[Training Epoch 6] Batch 116, Loss 0.24724456667900085\n",
      "[Training Epoch 6] Batch 117, Loss 0.27333417534828186\n",
      "[Training Epoch 6] Batch 118, Loss 0.21827638149261475\n",
      "[Training Epoch 6] Batch 119, Loss 0.22909408807754517\n",
      "[Training Epoch 6] Batch 120, Loss 0.2540449798107147\n",
      "[Training Epoch 6] Batch 121, Loss 0.2699246406555176\n",
      "[Training Epoch 6] Batch 122, Loss 0.24674232304096222\n",
      "[Training Epoch 6] Batch 123, Loss 0.24911761283874512\n",
      "[Training Epoch 6] Batch 124, Loss 0.27228018641471863\n",
      "[Training Epoch 6] Batch 125, Loss 0.2660162150859833\n",
      "[Training Epoch 6] Batch 126, Loss 0.256600022315979\n",
      "[Training Epoch 6] Batch 127, Loss 0.23205041885375977\n",
      "[Training Epoch 6] Batch 128, Loss 0.2801138162612915\n",
      "[Training Epoch 6] Batch 129, Loss 0.24349570274353027\n",
      "[Training Epoch 6] Batch 130, Loss 0.23907403647899628\n",
      "[Training Epoch 6] Batch 131, Loss 0.23560303449630737\n",
      "[Training Epoch 6] Batch 132, Loss 0.24743404984474182\n",
      "[Training Epoch 6] Batch 133, Loss 0.2551705241203308\n",
      "[Training Epoch 6] Batch 134, Loss 0.24731141328811646\n",
      "[Training Epoch 6] Batch 135, Loss 0.26850783824920654\n",
      "[Training Epoch 6] Batch 136, Loss 0.26460713148117065\n",
      "[Training Epoch 6] Batch 137, Loss 0.25578573346138\n",
      "[Training Epoch 6] Batch 138, Loss 0.2732546329498291\n",
      "[Training Epoch 6] Batch 139, Loss 0.2574195861816406\n",
      "[Training Epoch 6] Batch 140, Loss 0.24913132190704346\n",
      "[Training Epoch 6] Batch 141, Loss 0.2572755217552185\n",
      "[Training Epoch 6] Batch 142, Loss 0.2394227683544159\n",
      "[Training Epoch 6] Batch 143, Loss 0.2704313397407532\n",
      "[Training Epoch 6] Batch 144, Loss 0.23685535788536072\n",
      "[Training Epoch 6] Batch 145, Loss 0.23029036819934845\n",
      "[Training Epoch 6] Batch 146, Loss 0.2376764565706253\n",
      "[Training Epoch 6] Batch 147, Loss 0.24235734343528748\n",
      "[Training Epoch 6] Batch 148, Loss 0.2585439383983612\n",
      "[Training Epoch 6] Batch 149, Loss 0.24378320574760437\n",
      "[Training Epoch 6] Batch 150, Loss 0.2534317970275879\n",
      "[Training Epoch 6] Batch 151, Loss 0.2733643352985382\n",
      "[Training Epoch 6] Batch 152, Loss 0.26055437326431274\n",
      "[Training Epoch 6] Batch 153, Loss 0.25745823979377747\n",
      "[Training Epoch 6] Batch 154, Loss 0.23438726365566254\n",
      "[Training Epoch 6] Batch 155, Loss 0.285591721534729\n",
      "[Training Epoch 6] Batch 156, Loss 0.24221837520599365\n",
      "[Training Epoch 6] Batch 157, Loss 0.25410887598991394\n",
      "[Training Epoch 6] Batch 158, Loss 0.26846635341644287\n",
      "[Training Epoch 6] Batch 159, Loss 0.24894627928733826\n",
      "[Training Epoch 6] Batch 160, Loss 0.22910219430923462\n",
      "[Training Epoch 6] Batch 161, Loss 0.2965902090072632\n",
      "[Training Epoch 6] Batch 162, Loss 0.27107787132263184\n",
      "[Training Epoch 6] Batch 163, Loss 0.28468888998031616\n",
      "[Training Epoch 6] Batch 164, Loss 0.23078083992004395\n",
      "[Training Epoch 6] Batch 165, Loss 0.2602722942829132\n",
      "[Training Epoch 6] Batch 166, Loss 0.24547725915908813\n",
      "[Training Epoch 6] Batch 167, Loss 0.2425495982170105\n",
      "[Training Epoch 6] Batch 168, Loss 0.2891652584075928\n",
      "[Training Epoch 6] Batch 169, Loss 0.2611883878707886\n",
      "[Training Epoch 6] Batch 170, Loss 0.24946579337120056\n",
      "[Training Epoch 6] Batch 171, Loss 0.2507232129573822\n",
      "[Training Epoch 6] Batch 172, Loss 0.228593111038208\n",
      "[Training Epoch 6] Batch 173, Loss 0.2632664144039154\n",
      "[Training Epoch 6] Batch 174, Loss 0.24742111563682556\n",
      "[Training Epoch 6] Batch 175, Loss 0.2694988548755646\n",
      "[Training Epoch 6] Batch 176, Loss 0.25519734621047974\n",
      "[Training Epoch 6] Batch 177, Loss 0.2649471163749695\n",
      "[Training Epoch 6] Batch 178, Loss 0.2675858736038208\n",
      "[Training Epoch 6] Batch 179, Loss 0.26768797636032104\n",
      "[Training Epoch 6] Batch 180, Loss 0.28811976313591003\n",
      "[Training Epoch 6] Batch 181, Loss 0.2391144037246704\n",
      "[Training Epoch 6] Batch 182, Loss 0.22350479662418365\n",
      "[Training Epoch 6] Batch 183, Loss 0.2597811818122864\n",
      "[Training Epoch 6] Batch 184, Loss 0.2545595169067383\n",
      "[Training Epoch 6] Batch 185, Loss 0.25573810935020447\n",
      "[Training Epoch 6] Batch 186, Loss 0.2824612259864807\n",
      "[Training Epoch 6] Batch 187, Loss 0.2745875120162964\n",
      "[Training Epoch 6] Batch 188, Loss 0.26312169432640076\n",
      "[Training Epoch 6] Batch 189, Loss 0.2717128098011017\n",
      "[Training Epoch 6] Batch 190, Loss 0.253131628036499\n",
      "[Training Epoch 6] Batch 191, Loss 0.24310550093650818\n",
      "[Training Epoch 6] Batch 192, Loss 0.2419172078371048\n",
      "[Training Epoch 6] Batch 193, Loss 0.26255881786346436\n",
      "[Training Epoch 6] Batch 194, Loss 0.2781454920768738\n",
      "[Training Epoch 6] Batch 195, Loss 0.25482311844825745\n",
      "[Training Epoch 6] Batch 196, Loss 0.19716551899909973\n",
      "[Training Epoch 6] Batch 197, Loss 0.26091980934143066\n",
      "[Training Epoch 6] Batch 198, Loss 0.2493298053741455\n",
      "[Training Epoch 6] Batch 199, Loss 0.265554279088974\n",
      "[Training Epoch 6] Batch 200, Loss 0.2588801383972168\n",
      "[Training Epoch 6] Batch 201, Loss 0.2483307272195816\n",
      "[Training Epoch 6] Batch 202, Loss 0.23751017451286316\n",
      "[Training Epoch 6] Batch 203, Loss 0.2605215907096863\n",
      "[Training Epoch 6] Batch 204, Loss 0.2582235634326935\n",
      "[Training Epoch 6] Batch 205, Loss 0.2506089508533478\n",
      "[Training Epoch 6] Batch 206, Loss 0.2464844435453415\n",
      "[Training Epoch 6] Batch 207, Loss 0.25829097628593445\n",
      "[Training Epoch 6] Batch 208, Loss 0.23397594690322876\n",
      "[Training Epoch 6] Batch 209, Loss 0.24218440055847168\n",
      "[Training Epoch 6] Batch 210, Loss 0.2463589757680893\n",
      "[Training Epoch 6] Batch 211, Loss 0.2770771384239197\n",
      "[Training Epoch 6] Batch 212, Loss 0.2617465853691101\n",
      "[Training Epoch 6] Batch 213, Loss 0.26326239109039307\n",
      "[Training Epoch 6] Batch 214, Loss 0.28351759910583496\n",
      "[Training Epoch 6] Batch 215, Loss 0.232377827167511\n",
      "[Training Epoch 6] Batch 216, Loss 0.22789913415908813\n",
      "[Training Epoch 6] Batch 217, Loss 0.2391393780708313\n",
      "[Training Epoch 6] Batch 218, Loss 0.2565611004829407\n",
      "[Training Epoch 6] Batch 219, Loss 0.24439740180969238\n",
      "[Training Epoch 6] Batch 220, Loss 0.24242272973060608\n",
      "[Training Epoch 6] Batch 221, Loss 0.25281956791877747\n",
      "[Training Epoch 6] Batch 222, Loss 0.22547520697116852\n",
      "[Training Epoch 6] Batch 223, Loss 0.23050852119922638\n",
      "[Training Epoch 6] Batch 224, Loss 0.2507055997848511\n",
      "[Training Epoch 6] Batch 225, Loss 0.23852506279945374\n",
      "[Training Epoch 6] Batch 226, Loss 0.2609215974807739\n",
      "[Training Epoch 6] Batch 227, Loss 0.23635120689868927\n",
      "[Training Epoch 6] Batch 228, Loss 0.23122432827949524\n",
      "[Training Epoch 6] Batch 229, Loss 0.2249365746974945\n",
      "[Training Epoch 6] Batch 230, Loss 0.2857328951358795\n",
      "[Training Epoch 6] Batch 231, Loss 0.22975119948387146\n",
      "[Training Epoch 6] Batch 232, Loss 0.25583869218826294\n",
      "[Training Epoch 6] Batch 233, Loss 0.2637655735015869\n",
      "[Training Epoch 6] Batch 234, Loss 0.24658167362213135\n",
      "[Training Epoch 6] Batch 235, Loss 0.25708481669425964\n",
      "[Training Epoch 6] Batch 236, Loss 0.2743426561355591\n",
      "[Training Epoch 6] Batch 237, Loss 0.2416047304868698\n",
      "[Training Epoch 6] Batch 238, Loss 0.2709011137485504\n",
      "[Training Epoch 6] Batch 239, Loss 0.30824825167655945\n",
      "[Training Epoch 6] Batch 240, Loss 0.26892390847206116\n",
      "[Training Epoch 6] Batch 241, Loss 0.23168732225894928\n",
      "[Training Epoch 6] Batch 242, Loss 0.28226131200790405\n",
      "[Training Epoch 6] Batch 243, Loss 0.24972157180309296\n",
      "[Training Epoch 6] Batch 244, Loss 0.2998890280723572\n",
      "[Training Epoch 6] Batch 245, Loss 0.24663427472114563\n",
      "[Training Epoch 6] Batch 246, Loss 0.2446090131998062\n",
      "[Training Epoch 6] Batch 247, Loss 0.22446811199188232\n",
      "[Training Epoch 6] Batch 248, Loss 0.2648160755634308\n",
      "[Training Epoch 6] Batch 249, Loss 0.2793513238430023\n",
      "[Training Epoch 6] Batch 250, Loss 0.2512703537940979\n",
      "[Training Epoch 6] Batch 251, Loss 0.2924048900604248\n",
      "[Training Epoch 6] Batch 252, Loss 0.2520442605018616\n",
      "[Training Epoch 6] Batch 253, Loss 0.24287116527557373\n",
      "[Training Epoch 6] Batch 254, Loss 0.25713855028152466\n",
      "[Training Epoch 6] Batch 255, Loss 0.2718869149684906\n",
      "[Training Epoch 6] Batch 256, Loss 0.27139586210250854\n",
      "[Training Epoch 6] Batch 257, Loss 0.22694134712219238\n",
      "[Training Epoch 6] Batch 258, Loss 0.25874972343444824\n",
      "[Training Epoch 6] Batch 259, Loss 0.25735652446746826\n",
      "[Training Epoch 6] Batch 260, Loss 0.23548418283462524\n",
      "[Training Epoch 6] Batch 261, Loss 0.28050878643989563\n",
      "[Training Epoch 6] Batch 262, Loss 0.26428380608558655\n",
      "[Training Epoch 6] Batch 263, Loss 0.23746976256370544\n",
      "[Training Epoch 6] Batch 264, Loss 0.24985051155090332\n",
      "[Training Epoch 6] Batch 265, Loss 0.2541636824607849\n",
      "[Training Epoch 6] Batch 266, Loss 0.24596309661865234\n",
      "[Training Epoch 6] Batch 267, Loss 0.2697569727897644\n",
      "[Training Epoch 6] Batch 268, Loss 0.24574649333953857\n",
      "[Training Epoch 6] Batch 269, Loss 0.24184170365333557\n",
      "[Training Epoch 6] Batch 270, Loss 0.28361478447914124\n",
      "[Training Epoch 6] Batch 271, Loss 0.3044760525226593\n",
      "[Training Epoch 6] Batch 272, Loss 0.2670152187347412\n",
      "[Training Epoch 6] Batch 273, Loss 0.23965613543987274\n",
      "[Training Epoch 6] Batch 274, Loss 0.25333064794540405\n",
      "[Training Epoch 6] Batch 275, Loss 0.2702122926712036\n",
      "[Training Epoch 6] Batch 276, Loss 0.2290273904800415\n",
      "[Training Epoch 6] Batch 277, Loss 0.2829848527908325\n",
      "[Training Epoch 6] Batch 278, Loss 0.246780127286911\n",
      "[Training Epoch 6] Batch 279, Loss 0.29897934198379517\n",
      "[Training Epoch 6] Batch 280, Loss 0.24741971492767334\n",
      "[Training Epoch 6] Batch 281, Loss 0.27438127994537354\n",
      "[Training Epoch 6] Batch 282, Loss 0.23198539018630981\n",
      "[Training Epoch 6] Batch 283, Loss 0.2851718068122864\n",
      "[Training Epoch 6] Batch 284, Loss 0.2447374165058136\n",
      "[Training Epoch 6] Batch 285, Loss 0.2677488625049591\n",
      "[Training Epoch 6] Batch 286, Loss 0.24645595252513885\n",
      "[Training Epoch 6] Batch 287, Loss 0.24345391988754272\n",
      "[Training Epoch 6] Batch 288, Loss 0.2340541034936905\n",
      "[Training Epoch 6] Batch 289, Loss 0.2623504400253296\n",
      "[Training Epoch 6] Batch 290, Loss 0.2843402624130249\n",
      "[Training Epoch 6] Batch 291, Loss 0.27501899003982544\n",
      "[Training Epoch 6] Batch 292, Loss 0.2573988735675812\n",
      "[Training Epoch 6] Batch 293, Loss 0.24767105281352997\n",
      "[Training Epoch 6] Batch 294, Loss 0.25298142433166504\n",
      "[Training Epoch 6] Batch 295, Loss 0.23154643177986145\n",
      "[Training Epoch 6] Batch 296, Loss 0.259743332862854\n",
      "[Training Epoch 6] Batch 297, Loss 0.25469082593917847\n",
      "[Training Epoch 6] Batch 298, Loss 0.26709797978401184\n",
      "[Training Epoch 6] Batch 299, Loss 0.2387627363204956\n",
      "[Training Epoch 6] Batch 300, Loss 0.24255922436714172\n",
      "[Training Epoch 6] Batch 301, Loss 0.2817520499229431\n",
      "[Training Epoch 6] Batch 302, Loss 0.25758612155914307\n",
      "[Training Epoch 6] Batch 303, Loss 0.2603972256183624\n",
      "[Training Epoch 6] Batch 304, Loss 0.25071269273757935\n",
      "[Training Epoch 6] Batch 305, Loss 0.24256649613380432\n",
      "[Training Epoch 6] Batch 306, Loss 0.2575679421424866\n",
      "[Training Epoch 6] Batch 307, Loss 0.2679787874221802\n",
      "[Training Epoch 6] Batch 308, Loss 0.24053527414798737\n",
      "[Training Epoch 6] Batch 309, Loss 0.24611681699752808\n",
      "[Training Epoch 6] Batch 310, Loss 0.2940709590911865\n",
      "[Training Epoch 6] Batch 311, Loss 0.2469644844532013\n",
      "[Training Epoch 6] Batch 312, Loss 0.2643347382545471\n",
      "[Training Epoch 6] Batch 313, Loss 0.27091968059539795\n",
      "[Training Epoch 6] Batch 314, Loss 0.2540273070335388\n",
      "[Training Epoch 6] Batch 315, Loss 0.252526193857193\n",
      "[Training Epoch 6] Batch 316, Loss 0.24613386392593384\n",
      "[Training Epoch 6] Batch 317, Loss 0.25016337633132935\n",
      "[Training Epoch 6] Batch 318, Loss 0.26618492603302\n",
      "[Training Epoch 6] Batch 319, Loss 0.2498418092727661\n",
      "[Training Epoch 6] Batch 320, Loss 0.28119122982025146\n",
      "[Training Epoch 6] Batch 321, Loss 0.24997246265411377\n",
      "[Training Epoch 6] Batch 322, Loss 0.25590890645980835\n",
      "[Training Epoch 6] Batch 323, Loss 0.2527204155921936\n",
      "[Training Epoch 6] Batch 324, Loss 0.26291608810424805\n",
      "[Training Epoch 6] Batch 325, Loss 0.23201435804367065\n",
      "[Training Epoch 6] Batch 326, Loss 0.2728797197341919\n",
      "[Training Epoch 6] Batch 327, Loss 0.2762172222137451\n",
      "[Training Epoch 6] Batch 328, Loss 0.24008485674858093\n",
      "[Training Epoch 6] Batch 329, Loss 0.2718983292579651\n",
      "[Training Epoch 6] Batch 330, Loss 0.2802295982837677\n",
      "[Training Epoch 6] Batch 331, Loss 0.2355213761329651\n",
      "[Training Epoch 6] Batch 332, Loss 0.2534063458442688\n",
      "[Training Epoch 6] Batch 333, Loss 0.2516629695892334\n",
      "[Training Epoch 6] Batch 334, Loss 0.26408451795578003\n",
      "[Training Epoch 6] Batch 335, Loss 0.2557436227798462\n",
      "[Training Epoch 6] Batch 336, Loss 0.24138973653316498\n",
      "[Training Epoch 6] Batch 337, Loss 0.2914614677429199\n",
      "[Training Epoch 6] Batch 338, Loss 0.2734903395175934\n",
      "[Training Epoch 6] Batch 339, Loss 0.25005775690078735\n",
      "[Training Epoch 6] Batch 340, Loss 0.2486208975315094\n",
      "[Training Epoch 6] Batch 341, Loss 0.2325378656387329\n",
      "[Training Epoch 6] Batch 342, Loss 0.23897702991962433\n",
      "[Training Epoch 6] Batch 343, Loss 0.24113115668296814\n",
      "[Training Epoch 6] Batch 344, Loss 0.2522149384021759\n",
      "[Training Epoch 6] Batch 345, Loss 0.25268998742103577\n",
      "[Training Epoch 6] Batch 346, Loss 0.23325106501579285\n",
      "[Training Epoch 6] Batch 347, Loss 0.23427829146385193\n",
      "[Training Epoch 6] Batch 348, Loss 0.23771196603775024\n",
      "[Training Epoch 6] Batch 349, Loss 0.22791622579097748\n",
      "[Training Epoch 6] Batch 350, Loss 0.2770530879497528\n",
      "[Training Epoch 6] Batch 351, Loss 0.2728685736656189\n",
      "[Training Epoch 6] Batch 352, Loss 0.26671990752220154\n",
      "[Training Epoch 6] Batch 353, Loss 0.26420778036117554\n",
      "[Training Epoch 6] Batch 354, Loss 0.25308746099472046\n",
      "[Training Epoch 6] Batch 355, Loss 0.28452834486961365\n",
      "[Training Epoch 6] Batch 356, Loss 0.24276497960090637\n",
      "[Training Epoch 6] Batch 357, Loss 0.2644168734550476\n",
      "[Training Epoch 6] Batch 358, Loss 0.24442535638809204\n",
      "[Training Epoch 6] Batch 359, Loss 0.2647102475166321\n",
      "[Training Epoch 6] Batch 360, Loss 0.26903560757637024\n",
      "[Training Epoch 6] Batch 361, Loss 0.25760525465011597\n",
      "[Training Epoch 6] Batch 362, Loss 0.28090107440948486\n",
      "[Training Epoch 6] Batch 363, Loss 0.23771387338638306\n",
      "[Training Epoch 6] Batch 364, Loss 0.24860265851020813\n",
      "[Training Epoch 6] Batch 365, Loss 0.24367228150367737\n",
      "[Training Epoch 6] Batch 366, Loss 0.28151169419288635\n",
      "[Training Epoch 6] Batch 367, Loss 0.24236217141151428\n",
      "[Training Epoch 6] Batch 368, Loss 0.24470531940460205\n",
      "[Training Epoch 6] Batch 369, Loss 0.273303747177124\n",
      "[Training Epoch 6] Batch 370, Loss 0.23495705425739288\n",
      "[Training Epoch 6] Batch 371, Loss 0.25997674465179443\n",
      "[Training Epoch 6] Batch 372, Loss 0.23668980598449707\n",
      "[Training Epoch 6] Batch 373, Loss 0.29498812556266785\n",
      "[Training Epoch 6] Batch 374, Loss 0.26050829887390137\n",
      "[Training Epoch 6] Batch 375, Loss 0.2529730796813965\n",
      "[Training Epoch 6] Batch 376, Loss 0.2585592269897461\n",
      "[Training Epoch 6] Batch 377, Loss 0.27191948890686035\n",
      "[Training Epoch 6] Batch 378, Loss 0.277854859828949\n",
      "[Training Epoch 6] Batch 379, Loss 0.2558729350566864\n",
      "[Training Epoch 6] Batch 380, Loss 0.24366873502731323\n",
      "[Training Epoch 6] Batch 381, Loss 0.2321835458278656\n",
      "[Training Epoch 6] Batch 382, Loss 0.2626338005065918\n",
      "[Training Epoch 6] Batch 383, Loss 0.2540333867073059\n",
      "[Training Epoch 6] Batch 384, Loss 0.28471431136131287\n",
      "[Training Epoch 6] Batch 385, Loss 0.2615083158016205\n",
      "[Training Epoch 6] Batch 386, Loss 0.22568896412849426\n",
      "[Training Epoch 6] Batch 387, Loss 0.2652462124824524\n",
      "[Training Epoch 6] Batch 388, Loss 0.25483405590057373\n",
      "[Training Epoch 6] Batch 389, Loss 0.260236531496048\n",
      "[Training Epoch 6] Batch 390, Loss 0.252768337726593\n",
      "[Training Epoch 6] Batch 391, Loss 0.24261996150016785\n",
      "[Training Epoch 6] Batch 392, Loss 0.25962597131729126\n",
      "[Training Epoch 6] Batch 393, Loss 0.2656155824661255\n",
      "[Training Epoch 6] Batch 394, Loss 0.2535538077354431\n",
      "[Training Epoch 6] Batch 395, Loss 0.2691602110862732\n",
      "[Training Epoch 6] Batch 396, Loss 0.25367170572280884\n",
      "[Training Epoch 6] Batch 397, Loss 0.26002517342567444\n",
      "[Training Epoch 6] Batch 398, Loss 0.25065934658050537\n",
      "[Training Epoch 6] Batch 399, Loss 0.2474694550037384\n",
      "[Training Epoch 6] Batch 400, Loss 0.2617354393005371\n",
      "[Training Epoch 6] Batch 401, Loss 0.26516956090927124\n",
      "[Training Epoch 6] Batch 402, Loss 0.23163771629333496\n",
      "[Training Epoch 6] Batch 403, Loss 0.2578648328781128\n",
      "[Training Epoch 6] Batch 404, Loss 0.25901728868484497\n",
      "[Training Epoch 6] Batch 405, Loss 0.24131107330322266\n",
      "[Training Epoch 6] Batch 406, Loss 0.26138657331466675\n",
      "[Training Epoch 6] Batch 407, Loss 0.2550150752067566\n",
      "[Training Epoch 6] Batch 408, Loss 0.25519728660583496\n",
      "[Training Epoch 6] Batch 409, Loss 0.25296133756637573\n",
      "[Training Epoch 6] Batch 410, Loss 0.2574957013130188\n",
      "[Training Epoch 6] Batch 411, Loss 0.277035117149353\n",
      "[Training Epoch 6] Batch 412, Loss 0.24770979583263397\n",
      "[Training Epoch 6] Batch 413, Loss 0.23444172739982605\n",
      "[Training Epoch 6] Batch 414, Loss 0.24406975507736206\n",
      "[Training Epoch 6] Batch 415, Loss 0.23370365798473358\n",
      "[Training Epoch 6] Batch 416, Loss 0.2813912034034729\n",
      "[Training Epoch 6] Batch 417, Loss 0.26059138774871826\n",
      "[Training Epoch 6] Batch 418, Loss 0.2248099148273468\n",
      "[Training Epoch 6] Batch 419, Loss 0.2638867497444153\n",
      "[Training Epoch 6] Batch 420, Loss 0.22490063309669495\n",
      "[Training Epoch 6] Batch 421, Loss 0.27656668424606323\n",
      "[Training Epoch 6] Batch 422, Loss 0.2376474291086197\n",
      "[Training Epoch 6] Batch 423, Loss 0.24880439043045044\n",
      "[Training Epoch 6] Batch 424, Loss 0.26211467385292053\n",
      "[Training Epoch 6] Batch 425, Loss 0.2521911859512329\n",
      "[Training Epoch 6] Batch 426, Loss 0.23475414514541626\n",
      "[Training Epoch 6] Batch 427, Loss 0.2570827007293701\n",
      "[Training Epoch 6] Batch 428, Loss 0.2786901295185089\n",
      "[Training Epoch 6] Batch 429, Loss 0.2284405380487442\n",
      "[Training Epoch 6] Batch 430, Loss 0.2751038372516632\n",
      "[Training Epoch 6] Batch 431, Loss 0.2543357014656067\n",
      "[Training Epoch 6] Batch 432, Loss 0.22943325340747833\n",
      "[Training Epoch 6] Batch 433, Loss 0.24174049496650696\n",
      "[Training Epoch 6] Batch 434, Loss 0.26489025354385376\n",
      "[Training Epoch 6] Batch 435, Loss 0.2494155466556549\n",
      "[Training Epoch 6] Batch 436, Loss 0.28483253717422485\n",
      "[Training Epoch 6] Batch 437, Loss 0.2647498548030853\n",
      "[Training Epoch 6] Batch 438, Loss 0.24414104223251343\n",
      "[Training Epoch 6] Batch 439, Loss 0.23352964222431183\n",
      "[Training Epoch 6] Batch 440, Loss 0.23359015583992004\n",
      "[Training Epoch 6] Batch 441, Loss 0.24071699380874634\n",
      "[Training Epoch 6] Batch 442, Loss 0.23614175617694855\n",
      "[Training Epoch 6] Batch 443, Loss 0.25475725531578064\n",
      "[Training Epoch 6] Batch 444, Loss 0.2785734534263611\n",
      "[Training Epoch 6] Batch 445, Loss 0.2355952262878418\n",
      "[Training Epoch 6] Batch 446, Loss 0.2542615532875061\n",
      "[Training Epoch 6] Batch 447, Loss 0.2575329840183258\n",
      "[Training Epoch 6] Batch 448, Loss 0.25075510144233704\n",
      "[Training Epoch 6] Batch 449, Loss 0.24693314731121063\n",
      "[Training Epoch 6] Batch 450, Loss 0.2673197090625763\n",
      "[Training Epoch 6] Batch 451, Loss 0.24284213781356812\n",
      "[Training Epoch 6] Batch 452, Loss 0.2397519052028656\n",
      "[Training Epoch 6] Batch 453, Loss 0.26133352518081665\n",
      "[Training Epoch 6] Batch 454, Loss 0.2689507007598877\n",
      "[Training Epoch 6] Batch 455, Loss 0.2545534670352936\n",
      "[Training Epoch 6] Batch 456, Loss 0.26509201526641846\n",
      "[Training Epoch 6] Batch 457, Loss 0.26919570565223694\n",
      "[Training Epoch 6] Batch 458, Loss 0.2655870020389557\n",
      "[Training Epoch 6] Batch 459, Loss 0.268530011177063\n",
      "[Training Epoch 6] Batch 460, Loss 0.2518509030342102\n",
      "[Training Epoch 6] Batch 461, Loss 0.2448536604642868\n",
      "[Training Epoch 6] Batch 462, Loss 0.24364443123340607\n",
      "[Training Epoch 6] Batch 463, Loss 0.25610169768333435\n",
      "[Training Epoch 6] Batch 464, Loss 0.27822113037109375\n",
      "[Training Epoch 6] Batch 465, Loss 0.26773738861083984\n",
      "[Training Epoch 6] Batch 466, Loss 0.2444610595703125\n",
      "[Training Epoch 6] Batch 467, Loss 0.2341819554567337\n",
      "[Training Epoch 6] Batch 468, Loss 0.27288660407066345\n",
      "[Training Epoch 6] Batch 469, Loss 0.24224044382572174\n",
      "[Training Epoch 6] Batch 470, Loss 0.2591284513473511\n",
      "[Training Epoch 6] Batch 471, Loss 0.2573312222957611\n",
      "[Training Epoch 6] Batch 472, Loss 0.2467832863330841\n",
      "[Training Epoch 6] Batch 473, Loss 0.2892947793006897\n",
      "[Training Epoch 6] Batch 474, Loss 0.2582237422466278\n",
      "[Training Epoch 6] Batch 475, Loss 0.2507414221763611\n",
      "[Training Epoch 6] Batch 476, Loss 0.27655380964279175\n",
      "[Training Epoch 6] Batch 477, Loss 0.23976454138755798\n",
      "[Training Epoch 6] Batch 478, Loss 0.25442177057266235\n",
      "[Training Epoch 6] Batch 479, Loss 0.26966410875320435\n",
      "[Training Epoch 6] Batch 480, Loss 0.2555835247039795\n",
      "[Training Epoch 6] Batch 481, Loss 0.24638018012046814\n",
      "[Training Epoch 6] Batch 482, Loss 0.2730664312839508\n",
      "[Training Epoch 6] Batch 483, Loss 0.2248430997133255\n",
      "[Training Epoch 6] Batch 484, Loss 0.2700618505477905\n",
      "[Training Epoch 6] Batch 485, Loss 0.25286176800727844\n",
      "[Training Epoch 6] Batch 486, Loss 0.23748789727687836\n",
      "[Training Epoch 6] Batch 487, Loss 0.26550352573394775\n",
      "[Training Epoch 6] Batch 488, Loss 0.2721041440963745\n",
      "[Training Epoch 6] Batch 489, Loss 0.2429402768611908\n",
      "[Training Epoch 6] Batch 490, Loss 0.26014986634254456\n",
      "[Training Epoch 6] Batch 491, Loss 0.26171642541885376\n",
      "[Training Epoch 6] Batch 492, Loss 0.2474822700023651\n",
      "[Training Epoch 6] Batch 493, Loss 0.2619929909706116\n",
      "[Training Epoch 6] Batch 494, Loss 0.24030038714408875\n",
      "[Training Epoch 6] Batch 495, Loss 0.24472537636756897\n",
      "[Training Epoch 6] Batch 496, Loss 0.22886626422405243\n",
      "[Training Epoch 6] Batch 497, Loss 0.24827812612056732\n",
      "[Training Epoch 6] Batch 498, Loss 0.2506377696990967\n",
      "[Training Epoch 6] Batch 499, Loss 0.24885407090187073\n",
      "[Training Epoch 6] Batch 500, Loss 0.2400517761707306\n",
      "[Training Epoch 6] Batch 501, Loss 0.25747403502464294\n",
      "[Training Epoch 6] Batch 502, Loss 0.2422572672367096\n",
      "[Training Epoch 6] Batch 503, Loss 0.2443818598985672\n",
      "[Training Epoch 6] Batch 504, Loss 0.25457778573036194\n",
      "[Training Epoch 6] Batch 505, Loss 0.2603570818901062\n",
      "[Training Epoch 6] Batch 506, Loss 0.245402529835701\n",
      "[Training Epoch 6] Batch 507, Loss 0.24283398687839508\n",
      "[Training Epoch 6] Batch 508, Loss 0.256928950548172\n",
      "[Training Epoch 6] Batch 509, Loss 0.2502961754798889\n",
      "[Training Epoch 6] Batch 510, Loss 0.24658814072608948\n",
      "[Training Epoch 6] Batch 511, Loss 0.2593676745891571\n",
      "[Training Epoch 6] Batch 512, Loss 0.2493932843208313\n",
      "[Training Epoch 6] Batch 513, Loss 0.2647605240345001\n",
      "[Training Epoch 6] Batch 514, Loss 0.28621840476989746\n",
      "[Training Epoch 6] Batch 515, Loss 0.2505936622619629\n",
      "[Training Epoch 6] Batch 516, Loss 0.23951484262943268\n",
      "[Training Epoch 6] Batch 517, Loss 0.2565823793411255\n",
      "[Training Epoch 6] Batch 518, Loss 0.2531207799911499\n",
      "[Training Epoch 6] Batch 519, Loss 0.26822030544281006\n",
      "[Training Epoch 6] Batch 520, Loss 0.27347105741500854\n",
      "[Training Epoch 6] Batch 521, Loss 0.25395089387893677\n",
      "[Training Epoch 6] Batch 522, Loss 0.2579248547554016\n",
      "[Training Epoch 6] Batch 523, Loss 0.238895446062088\n",
      "[Training Epoch 6] Batch 524, Loss 0.28217384219169617\n",
      "[Training Epoch 6] Batch 525, Loss 0.2565300464630127\n",
      "[Training Epoch 6] Batch 526, Loss 0.24224022030830383\n",
      "[Training Epoch 6] Batch 527, Loss 0.2382376790046692\n",
      "[Training Epoch 6] Batch 528, Loss 0.25770261883735657\n",
      "[Training Epoch 6] Batch 529, Loss 0.2604053020477295\n",
      "[Training Epoch 6] Batch 530, Loss 0.24198345839977264\n",
      "[Training Epoch 6] Batch 531, Loss 0.2632911205291748\n",
      "[Training Epoch 6] Batch 532, Loss 0.24286699295043945\n",
      "[Training Epoch 6] Batch 533, Loss 0.27863842248916626\n",
      "[Training Epoch 6] Batch 534, Loss 0.25147590041160583\n",
      "[Training Epoch 6] Batch 535, Loss 0.28372645378112793\n",
      "[Training Epoch 6] Batch 536, Loss 0.27034175395965576\n",
      "[Training Epoch 6] Batch 537, Loss 0.24570205807685852\n",
      "[Training Epoch 6] Batch 538, Loss 0.24076461791992188\n",
      "[Training Epoch 6] Batch 539, Loss 0.2735559344291687\n",
      "[Training Epoch 6] Batch 540, Loss 0.22098606824874878\n",
      "[Training Epoch 6] Batch 541, Loss 0.22146213054656982\n",
      "[Training Epoch 6] Batch 542, Loss 0.2566761374473572\n",
      "[Training Epoch 6] Batch 543, Loss 0.2527904212474823\n",
      "[Training Epoch 6] Batch 544, Loss 0.26611948013305664\n",
      "[Training Epoch 6] Batch 545, Loss 0.24991722404956818\n",
      "[Training Epoch 6] Batch 546, Loss 0.2387176752090454\n",
      "[Training Epoch 6] Batch 547, Loss 0.22862067818641663\n",
      "[Training Epoch 6] Batch 548, Loss 0.2721174359321594\n",
      "[Training Epoch 6] Batch 549, Loss 0.2634260654449463\n",
      "[Training Epoch 6] Batch 550, Loss 0.2620917856693268\n",
      "[Training Epoch 6] Batch 551, Loss 0.278156042098999\n",
      "[Training Epoch 6] Batch 552, Loss 0.25997069478034973\n",
      "[Training Epoch 6] Batch 553, Loss 0.2586835026741028\n",
      "[Training Epoch 6] Batch 554, Loss 0.2325962781906128\n",
      "[Training Epoch 6] Batch 555, Loss 0.24841690063476562\n",
      "[Training Epoch 6] Batch 556, Loss 0.24855422973632812\n",
      "[Training Epoch 6] Batch 557, Loss 0.23361371457576752\n",
      "[Training Epoch 6] Batch 558, Loss 0.25948798656463623\n",
      "[Training Epoch 6] Batch 559, Loss 0.25064516067504883\n",
      "[Training Epoch 6] Batch 560, Loss 0.2682666778564453\n",
      "[Training Epoch 6] Batch 561, Loss 0.23530912399291992\n",
      "[Training Epoch 6] Batch 562, Loss 0.28280651569366455\n",
      "[Training Epoch 6] Batch 563, Loss 0.27867376804351807\n",
      "[Training Epoch 6] Batch 564, Loss 0.2576485276222229\n",
      "[Training Epoch 6] Batch 565, Loss 0.249790221452713\n",
      "[Training Epoch 6] Batch 566, Loss 0.2580898404121399\n",
      "[Training Epoch 6] Batch 567, Loss 0.2532827854156494\n",
      "[Training Epoch 6] Batch 568, Loss 0.2882016897201538\n",
      "[Training Epoch 6] Batch 569, Loss 0.22520345449447632\n",
      "[Training Epoch 6] Batch 570, Loss 0.25826534628868103\n",
      "[Training Epoch 6] Batch 571, Loss 0.24711298942565918\n",
      "[Training Epoch 6] Batch 572, Loss 0.27143043279647827\n",
      "[Training Epoch 6] Batch 573, Loss 0.239815816283226\n",
      "[Training Epoch 6] Batch 574, Loss 0.2676078677177429\n",
      "[Training Epoch 6] Batch 575, Loss 0.2678118348121643\n",
      "[Training Epoch 6] Batch 576, Loss 0.23465275764465332\n",
      "[Training Epoch 6] Batch 577, Loss 0.2787887156009674\n",
      "[Training Epoch 6] Batch 578, Loss 0.25168031454086304\n",
      "[Training Epoch 6] Batch 579, Loss 0.25868433713912964\n",
      "[Training Epoch 6] Batch 580, Loss 0.28760528564453125\n",
      "[Training Epoch 6] Batch 581, Loss 0.25519639253616333\n",
      "[Training Epoch 6] Batch 582, Loss 0.24686355888843536\n",
      "[Training Epoch 6] Batch 583, Loss 0.2683889865875244\n",
      "[Training Epoch 6] Batch 584, Loss 0.2659183740615845\n",
      "[Training Epoch 6] Batch 585, Loss 0.2590194046497345\n",
      "[Training Epoch 6] Batch 586, Loss 0.29311108589172363\n",
      "[Training Epoch 6] Batch 587, Loss 0.24482347071170807\n",
      "[Training Epoch 6] Batch 588, Loss 0.27450138330459595\n",
      "[Training Epoch 6] Batch 589, Loss 0.26528942584991455\n",
      "[Training Epoch 6] Batch 590, Loss 0.2529599666595459\n",
      "[Training Epoch 6] Batch 591, Loss 0.2845216393470764\n",
      "[Training Epoch 6] Batch 592, Loss 0.278153657913208\n",
      "[Training Epoch 6] Batch 593, Loss 0.2620474696159363\n",
      "[Training Epoch 6] Batch 594, Loss 0.26226478815078735\n",
      "[Training Epoch 6] Batch 595, Loss 0.24041572213172913\n",
      "[Training Epoch 6] Batch 596, Loss 0.25713926553726196\n",
      "[Training Epoch 6] Batch 597, Loss 0.2325727939605713\n",
      "[Training Epoch 6] Batch 598, Loss 0.2375316470861435\n",
      "[Training Epoch 6] Batch 599, Loss 0.236771821975708\n",
      "[Training Epoch 6] Batch 600, Loss 0.2617112398147583\n",
      "[Training Epoch 6] Batch 601, Loss 0.2877015173435211\n",
      "[Training Epoch 6] Batch 602, Loss 0.28656095266342163\n",
      "[Training Epoch 6] Batch 603, Loss 0.2198810577392578\n",
      "[Training Epoch 6] Batch 604, Loss 0.24424540996551514\n",
      "[Training Epoch 6] Batch 605, Loss 0.2682739794254303\n",
      "[Training Epoch 6] Batch 606, Loss 0.2785719037055969\n",
      "[Training Epoch 6] Batch 607, Loss 0.29372772574424744\n",
      "[Training Epoch 6] Batch 608, Loss 0.23871484398841858\n",
      "[Training Epoch 6] Batch 609, Loss 0.28953543305397034\n",
      "[Training Epoch 6] Batch 610, Loss 0.280311644077301\n",
      "[Training Epoch 6] Batch 611, Loss 0.2570575177669525\n",
      "[Training Epoch 6] Batch 612, Loss 0.27908259630203247\n",
      "[Training Epoch 6] Batch 613, Loss 0.23389440774917603\n",
      "[Training Epoch 6] Batch 614, Loss 0.24921295046806335\n",
      "[Training Epoch 6] Batch 615, Loss 0.22325505316257477\n",
      "[Training Epoch 6] Batch 616, Loss 0.2602446973323822\n",
      "[Training Epoch 6] Batch 617, Loss 0.2476334571838379\n",
      "[Training Epoch 6] Batch 618, Loss 0.2526271939277649\n",
      "[Training Epoch 6] Batch 619, Loss 0.2599546015262604\n",
      "[Training Epoch 6] Batch 620, Loss 0.23275703191757202\n",
      "[Training Epoch 6] Batch 621, Loss 0.2514941096305847\n",
      "[Training Epoch 6] Batch 622, Loss 0.29157155752182007\n",
      "[Training Epoch 6] Batch 623, Loss 0.2503889203071594\n",
      "[Training Epoch 6] Batch 624, Loss 0.24527806043624878\n",
      "[Training Epoch 6] Batch 625, Loss 0.2667539715766907\n",
      "[Training Epoch 6] Batch 626, Loss 0.24511906504631042\n",
      "[Training Epoch 6] Batch 627, Loss 0.28330016136169434\n",
      "[Training Epoch 6] Batch 628, Loss 0.25670522451400757\n",
      "[Training Epoch 6] Batch 629, Loss 0.2849884629249573\n",
      "[Training Epoch 6] Batch 630, Loss 0.2807185649871826\n",
      "[Training Epoch 6] Batch 631, Loss 0.26181429624557495\n",
      "[Training Epoch 6] Batch 632, Loss 0.27239203453063965\n",
      "[Training Epoch 6] Batch 633, Loss 0.23566210269927979\n",
      "[Training Epoch 6] Batch 634, Loss 0.25117579102516174\n",
      "[Training Epoch 6] Batch 635, Loss 0.28178268671035767\n",
      "[Training Epoch 6] Batch 636, Loss 0.25879815220832825\n",
      "[Training Epoch 6] Batch 637, Loss 0.2545991539955139\n",
      "[Training Epoch 6] Batch 638, Loss 0.24825789034366608\n",
      "[Training Epoch 6] Batch 639, Loss 0.2400205433368683\n",
      "[Training Epoch 6] Batch 640, Loss 0.23919038474559784\n",
      "[Training Epoch 6] Batch 641, Loss 0.24076129496097565\n",
      "[Training Epoch 6] Batch 642, Loss 0.25978195667266846\n",
      "[Training Epoch 6] Batch 643, Loss 0.25423991680145264\n",
      "[Training Epoch 6] Batch 644, Loss 0.26543304324150085\n",
      "[Training Epoch 6] Batch 645, Loss 0.2258574217557907\n",
      "[Training Epoch 6] Batch 646, Loss 0.28026139736175537\n",
      "[Training Epoch 6] Batch 647, Loss 0.2640920579433441\n",
      "[Training Epoch 6] Batch 648, Loss 0.27066150307655334\n",
      "[Training Epoch 6] Batch 649, Loss 0.26307642459869385\n",
      "[Training Epoch 6] Batch 650, Loss 0.23238550126552582\n",
      "[Training Epoch 6] Batch 651, Loss 0.22596848011016846\n",
      "[Training Epoch 6] Batch 652, Loss 0.24304388463497162\n",
      "[Training Epoch 6] Batch 653, Loss 0.2684524655342102\n",
      "[Training Epoch 6] Batch 654, Loss 0.2091374695301056\n",
      "[Training Epoch 6] Batch 655, Loss 0.22930166125297546\n",
      "[Training Epoch 6] Batch 656, Loss 0.24878279864788055\n",
      "[Training Epoch 6] Batch 657, Loss 0.25178292393684387\n",
      "[Training Epoch 6] Batch 658, Loss 0.2478172481060028\n",
      "[Training Epoch 6] Batch 659, Loss 0.2643011510372162\n",
      "[Training Epoch 6] Batch 660, Loss 0.25928640365600586\n",
      "[Training Epoch 6] Batch 661, Loss 0.23337680101394653\n",
      "[Training Epoch 6] Batch 662, Loss 0.2379704862833023\n",
      "[Training Epoch 6] Batch 663, Loss 0.2876725196838379\n",
      "[Training Epoch 6] Batch 664, Loss 0.26801690459251404\n",
      "[Training Epoch 6] Batch 665, Loss 0.2535821199417114\n",
      "[Training Epoch 6] Batch 666, Loss 0.26380836963653564\n",
      "[Training Epoch 6] Batch 667, Loss 0.2254543900489807\n",
      "[Training Epoch 6] Batch 668, Loss 0.2553613483905792\n",
      "[Training Epoch 6] Batch 669, Loss 0.25014349818229675\n",
      "[Training Epoch 6] Batch 670, Loss 0.2879320979118347\n",
      "[Training Epoch 6] Batch 671, Loss 0.2569386959075928\n",
      "[Training Epoch 6] Batch 672, Loss 0.2644413411617279\n",
      "[Training Epoch 6] Batch 673, Loss 0.296277791261673\n",
      "[Training Epoch 6] Batch 674, Loss 0.2687026858329773\n",
      "[Training Epoch 6] Batch 675, Loss 0.23965051770210266\n",
      "[Training Epoch 6] Batch 676, Loss 0.25846993923187256\n",
      "[Training Epoch 6] Batch 677, Loss 0.265608012676239\n",
      "[Training Epoch 6] Batch 678, Loss 0.2747099995613098\n",
      "[Training Epoch 6] Batch 679, Loss 0.24491232633590698\n",
      "[Training Epoch 6] Batch 680, Loss 0.2678180932998657\n",
      "[Training Epoch 6] Batch 681, Loss 0.2752062678337097\n",
      "[Training Epoch 6] Batch 682, Loss 0.2502402067184448\n",
      "[Training Epoch 6] Batch 683, Loss 0.24849538505077362\n",
      "[Training Epoch 6] Batch 684, Loss 0.2565176486968994\n",
      "[Training Epoch 6] Batch 685, Loss 0.2576679289340973\n",
      "[Training Epoch 6] Batch 686, Loss 0.2571340799331665\n",
      "[Training Epoch 6] Batch 687, Loss 0.25753253698349\n",
      "[Training Epoch 6] Batch 688, Loss 0.2141800820827484\n",
      "[Training Epoch 6] Batch 689, Loss 0.2821093201637268\n",
      "[Training Epoch 6] Batch 690, Loss 0.2680515646934509\n",
      "[Training Epoch 6] Batch 691, Loss 0.2579198181629181\n",
      "[Training Epoch 6] Batch 692, Loss 0.23519398272037506\n",
      "[Training Epoch 6] Batch 693, Loss 0.2521228492259979\n",
      "[Training Epoch 6] Batch 694, Loss 0.2802799940109253\n",
      "[Training Epoch 6] Batch 695, Loss 0.2411665916442871\n",
      "[Training Epoch 6] Batch 696, Loss 0.22748686373233795\n",
      "[Training Epoch 6] Batch 697, Loss 0.23628367483615875\n",
      "[Training Epoch 6] Batch 698, Loss 0.2641065716743469\n",
      "[Training Epoch 6] Batch 699, Loss 0.2673135995864868\n",
      "[Training Epoch 6] Batch 700, Loss 0.26722806692123413\n",
      "[Training Epoch 6] Batch 701, Loss 0.22725537419319153\n",
      "[Training Epoch 6] Batch 702, Loss 0.27850252389907837\n",
      "[Training Epoch 6] Batch 703, Loss 0.25345635414123535\n",
      "[Training Epoch 6] Batch 704, Loss 0.271167129278183\n",
      "[Training Epoch 6] Batch 705, Loss 0.2561591863632202\n",
      "[Training Epoch 6] Batch 706, Loss 0.26491791009902954\n",
      "[Training Epoch 6] Batch 707, Loss 0.2805668115615845\n",
      "[Training Epoch 6] Batch 708, Loss 0.2425088882446289\n",
      "[Training Epoch 6] Batch 709, Loss 0.2524852752685547\n",
      "[Training Epoch 6] Batch 710, Loss 0.24294456839561462\n",
      "[Training Epoch 6] Batch 711, Loss 0.2559201717376709\n",
      "[Training Epoch 6] Batch 712, Loss 0.2524684965610504\n",
      "[Training Epoch 6] Batch 713, Loss 0.2895416021347046\n",
      "[Training Epoch 6] Batch 714, Loss 0.24114620685577393\n",
      "[Training Epoch 6] Batch 715, Loss 0.2715965509414673\n",
      "[Training Epoch 6] Batch 716, Loss 0.2436370849609375\n",
      "[Training Epoch 6] Batch 717, Loss 0.26303374767303467\n",
      "[Training Epoch 6] Batch 718, Loss 0.2544749081134796\n",
      "[Training Epoch 6] Batch 719, Loss 0.2358277440071106\n",
      "[Training Epoch 6] Batch 720, Loss 0.263755738735199\n",
      "[Training Epoch 6] Batch 721, Loss 0.272180438041687\n",
      "[Training Epoch 6] Batch 722, Loss 0.2777063846588135\n",
      "[Training Epoch 6] Batch 723, Loss 0.2595837712287903\n",
      "[Training Epoch 6] Batch 724, Loss 0.29269564151763916\n",
      "[Training Epoch 6] Batch 725, Loss 0.3023144006729126\n",
      "[Training Epoch 6] Batch 726, Loss 0.27460402250289917\n",
      "[Training Epoch 6] Batch 727, Loss 0.29167211055755615\n",
      "[Training Epoch 6] Batch 728, Loss 0.26941803097724915\n",
      "[Training Epoch 6] Batch 729, Loss 0.28245195746421814\n",
      "[Training Epoch 6] Batch 730, Loss 0.2620529234409332\n",
      "[Training Epoch 6] Batch 731, Loss 0.2591770887374878\n",
      "[Training Epoch 6] Batch 732, Loss 0.2535640597343445\n",
      "[Training Epoch 6] Batch 733, Loss 0.27998822927474976\n",
      "[Training Epoch 6] Batch 734, Loss 0.24321529269218445\n",
      "[Training Epoch 6] Batch 735, Loss 0.24322322010993958\n",
      "[Training Epoch 6] Batch 736, Loss 0.24660857021808624\n",
      "[Training Epoch 6] Batch 737, Loss 0.27024561166763306\n",
      "[Training Epoch 6] Batch 738, Loss 0.24327172338962555\n",
      "[Training Epoch 6] Batch 739, Loss 0.27243703603744507\n",
      "[Training Epoch 6] Batch 740, Loss 0.2685423195362091\n",
      "[Training Epoch 6] Batch 741, Loss 0.27821844816207886\n",
      "[Training Epoch 6] Batch 742, Loss 0.2460176944732666\n",
      "[Training Epoch 6] Batch 743, Loss 0.23582150042057037\n",
      "[Training Epoch 6] Batch 744, Loss 0.24517413973808289\n",
      "[Training Epoch 6] Batch 745, Loss 0.2829856276512146\n",
      "[Training Epoch 6] Batch 746, Loss 0.27107036113739014\n",
      "[Training Epoch 6] Batch 747, Loss 0.2668713927268982\n",
      "[Training Epoch 6] Batch 748, Loss 0.29059624671936035\n",
      "[Training Epoch 6] Batch 749, Loss 0.29925426840782166\n",
      "[Training Epoch 6] Batch 750, Loss 0.23755329847335815\n",
      "[Training Epoch 6] Batch 751, Loss 0.2619990408420563\n",
      "[Training Epoch 6] Batch 752, Loss 0.27208420634269714\n",
      "[Training Epoch 6] Batch 753, Loss 0.26157107949256897\n",
      "[Training Epoch 6] Batch 754, Loss 0.2429966777563095\n",
      "[Training Epoch 6] Batch 755, Loss 0.24691782891750336\n",
      "[Training Epoch 6] Batch 756, Loss 0.2618865668773651\n",
      "[Training Epoch 6] Batch 757, Loss 0.24722114205360413\n",
      "[Training Epoch 6] Batch 758, Loss 0.26677238941192627\n",
      "[Training Epoch 6] Batch 759, Loss 0.24222517013549805\n",
      "[Training Epoch 6] Batch 760, Loss 0.2777397632598877\n",
      "[Training Epoch 6] Batch 761, Loss 0.25006309151649475\n",
      "[Training Epoch 6] Batch 762, Loss 0.2516648471355438\n",
      "[Training Epoch 6] Batch 763, Loss 0.2915835380554199\n",
      "[Training Epoch 6] Batch 764, Loss 0.29094284772872925\n",
      "[Training Epoch 6] Batch 765, Loss 0.25204959511756897\n",
      "[Training Epoch 6] Batch 766, Loss 0.2528138756752014\n",
      "[Training Epoch 6] Batch 767, Loss 0.26887330412864685\n",
      "[Training Epoch 6] Batch 768, Loss 0.24818310141563416\n",
      "[Training Epoch 6] Batch 769, Loss 0.2519014775753021\n",
      "[Training Epoch 6] Batch 770, Loss 0.2654675841331482\n",
      "[Training Epoch 6] Batch 771, Loss 0.2609408497810364\n",
      "[Training Epoch 6] Batch 772, Loss 0.23575006425380707\n",
      "[Training Epoch 6] Batch 773, Loss 0.2404237985610962\n",
      "[Training Epoch 6] Batch 774, Loss 0.2516549825668335\n",
      "[Training Epoch 6] Batch 775, Loss 0.2591317892074585\n",
      "[Training Epoch 6] Batch 776, Loss 0.267264187335968\n",
      "[Training Epoch 6] Batch 777, Loss 0.2784828543663025\n",
      "[Training Epoch 6] Batch 778, Loss 0.2501322031021118\n",
      "[Training Epoch 6] Batch 779, Loss 0.22091999650001526\n",
      "[Training Epoch 6] Batch 780, Loss 0.2593705356121063\n",
      "[Training Epoch 6] Batch 781, Loss 0.2389279007911682\n",
      "[Training Epoch 6] Batch 782, Loss 0.2580893933773041\n",
      "[Training Epoch 6] Batch 783, Loss 0.24802562594413757\n",
      "[Training Epoch 6] Batch 784, Loss 0.2673044502735138\n",
      "[Training Epoch 6] Batch 785, Loss 0.2540891766548157\n",
      "[Training Epoch 6] Batch 786, Loss 0.2637277841567993\n",
      "[Training Epoch 6] Batch 787, Loss 0.2517780363559723\n",
      "[Training Epoch 6] Batch 788, Loss 0.24956268072128296\n",
      "[Training Epoch 6] Batch 789, Loss 0.2534475028514862\n",
      "[Training Epoch 6] Batch 790, Loss 0.2567638158798218\n",
      "[Training Epoch 6] Batch 791, Loss 0.2533922791481018\n",
      "[Training Epoch 6] Batch 792, Loss 0.24692460894584656\n",
      "[Training Epoch 6] Batch 793, Loss 0.285086065530777\n",
      "[Training Epoch 6] Batch 794, Loss 0.2559502422809601\n",
      "[Training Epoch 6] Batch 795, Loss 0.24410046637058258\n",
      "[Training Epoch 6] Batch 796, Loss 0.2583983242511749\n",
      "[Training Epoch 6] Batch 797, Loss 0.24301525950431824\n",
      "[Training Epoch 6] Batch 798, Loss 0.23304590582847595\n",
      "[Training Epoch 6] Batch 799, Loss 0.2672869861125946\n",
      "[Training Epoch 6] Batch 800, Loss 0.22452563047409058\n",
      "[Training Epoch 6] Batch 801, Loss 0.23188696801662445\n",
      "[Training Epoch 6] Batch 802, Loss 0.24561968445777893\n",
      "[Training Epoch 6] Batch 803, Loss 0.27782148122787476\n",
      "[Training Epoch 6] Batch 804, Loss 0.28771746158599854\n",
      "[Training Epoch 6] Batch 805, Loss 0.27501487731933594\n",
      "[Training Epoch 6] Batch 806, Loss 0.26169753074645996\n",
      "[Training Epoch 6] Batch 807, Loss 0.28897982835769653\n",
      "[Training Epoch 6] Batch 808, Loss 0.2297787070274353\n",
      "[Training Epoch 6] Batch 809, Loss 0.26177501678466797\n",
      "[Training Epoch 6] Batch 810, Loss 0.22812694311141968\n",
      "[Training Epoch 6] Batch 811, Loss 0.28780990839004517\n",
      "[Training Epoch 6] Batch 812, Loss 0.23591753840446472\n",
      "[Training Epoch 6] Batch 813, Loss 0.24747434258460999\n",
      "[Training Epoch 6] Batch 814, Loss 0.2527298331260681\n",
      "[Training Epoch 6] Batch 815, Loss 0.2403625100851059\n",
      "[Training Epoch 6] Batch 816, Loss 0.27180516719818115\n",
      "[Training Epoch 6] Batch 817, Loss 0.2822941541671753\n",
      "[Training Epoch 6] Batch 818, Loss 0.2649831175804138\n",
      "[Training Epoch 6] Batch 819, Loss 0.2861151695251465\n",
      "[Training Epoch 6] Batch 820, Loss 0.2803661525249481\n",
      "[Training Epoch 6] Batch 821, Loss 0.23128412663936615\n",
      "[Training Epoch 6] Batch 822, Loss 0.21725261211395264\n",
      "[Training Epoch 6] Batch 823, Loss 0.24962733685970306\n",
      "[Training Epoch 6] Batch 824, Loss 0.23954133689403534\n",
      "[Training Epoch 6] Batch 825, Loss 0.26375386118888855\n",
      "[Training Epoch 6] Batch 826, Loss 0.26793020963668823\n",
      "[Training Epoch 6] Batch 827, Loss 0.26012974977493286\n",
      "[Training Epoch 6] Batch 828, Loss 0.30223578214645386\n",
      "[Training Epoch 6] Batch 829, Loss 0.2545962929725647\n",
      "[Training Epoch 6] Batch 830, Loss 0.2394355982542038\n",
      "[Training Epoch 6] Batch 831, Loss 0.2323610484600067\n",
      "[Training Epoch 6] Batch 832, Loss 0.26654869318008423\n",
      "[Training Epoch 6] Batch 833, Loss 0.2565273642539978\n",
      "[Training Epoch 6] Batch 834, Loss 0.268110454082489\n",
      "[Training Epoch 6] Batch 835, Loss 0.26075422763824463\n",
      "[Training Epoch 6] Batch 836, Loss 0.27787530422210693\n",
      "[Training Epoch 6] Batch 837, Loss 0.2581934332847595\n",
      "[Training Epoch 6] Batch 838, Loss 0.25680989027023315\n",
      "[Training Epoch 6] Batch 839, Loss 0.266595721244812\n",
      "[Training Epoch 6] Batch 840, Loss 0.24972327053546906\n",
      "[Training Epoch 6] Batch 841, Loss 0.25685203075408936\n",
      "[Training Epoch 6] Batch 842, Loss 0.2530476450920105\n",
      "[Training Epoch 6] Batch 843, Loss 0.24970559775829315\n",
      "[Training Epoch 6] Batch 844, Loss 0.22707223892211914\n",
      "[Training Epoch 6] Batch 845, Loss 0.28735652565956116\n",
      "[Training Epoch 6] Batch 846, Loss 0.25016316771507263\n",
      "[Training Epoch 6] Batch 847, Loss 0.286350280046463\n",
      "[Training Epoch 6] Batch 848, Loss 0.2850363254547119\n",
      "[Training Epoch 6] Batch 849, Loss 0.2681572437286377\n",
      "[Training Epoch 6] Batch 850, Loss 0.2363985925912857\n",
      "[Training Epoch 6] Batch 851, Loss 0.2635089159011841\n",
      "[Training Epoch 6] Batch 852, Loss 0.2819426655769348\n",
      "[Training Epoch 6] Batch 853, Loss 0.21834152936935425\n",
      "[Training Epoch 6] Batch 854, Loss 0.26710420846939087\n",
      "[Training Epoch 6] Batch 855, Loss 0.2720146179199219\n",
      "[Training Epoch 6] Batch 856, Loss 0.2607467770576477\n",
      "[Training Epoch 6] Batch 857, Loss 0.24585768580436707\n",
      "[Training Epoch 6] Batch 858, Loss 0.2783218324184418\n",
      "[Training Epoch 6] Batch 859, Loss 0.2527213394641876\n",
      "[Training Epoch 6] Batch 860, Loss 0.26458030939102173\n",
      "[Training Epoch 6] Batch 861, Loss 0.2322346717119217\n",
      "[Training Epoch 6] Batch 862, Loss 0.25122949481010437\n",
      "[Training Epoch 6] Batch 863, Loss 0.2686758041381836\n",
      "[Training Epoch 6] Batch 864, Loss 0.25272196531295776\n",
      "[Training Epoch 6] Batch 865, Loss 0.260599821805954\n",
      "[Training Epoch 6] Batch 866, Loss 0.25890231132507324\n",
      "[Training Epoch 6] Batch 867, Loss 0.275680273771286\n",
      "[Training Epoch 6] Batch 868, Loss 0.26839372515678406\n",
      "[Training Epoch 6] Batch 869, Loss 0.24375379085540771\n",
      "[Training Epoch 6] Batch 870, Loss 0.27013692259788513\n",
      "[Training Epoch 6] Batch 871, Loss 0.24856416881084442\n",
      "[Training Epoch 6] Batch 872, Loss 0.23677456378936768\n",
      "[Training Epoch 6] Batch 873, Loss 0.25733381509780884\n",
      "[Training Epoch 6] Batch 874, Loss 0.28247755765914917\n",
      "[Training Epoch 6] Batch 875, Loss 0.23784968256950378\n",
      "[Training Epoch 6] Batch 876, Loss 0.2513324022293091\n",
      "[Training Epoch 6] Batch 877, Loss 0.25475144386291504\n",
      "[Training Epoch 6] Batch 878, Loss 0.28315940499305725\n",
      "[Training Epoch 6] Batch 879, Loss 0.2717568576335907\n",
      "[Training Epoch 6] Batch 880, Loss 0.2297665923833847\n",
      "[Training Epoch 6] Batch 881, Loss 0.2538943886756897\n",
      "[Training Epoch 6] Batch 882, Loss 0.25080615282058716\n",
      "[Training Epoch 6] Batch 883, Loss 0.277225136756897\n",
      "[Training Epoch 6] Batch 884, Loss 0.24895638227462769\n",
      "[Training Epoch 6] Batch 885, Loss 0.2455543577671051\n",
      "[Training Epoch 6] Batch 886, Loss 0.29248976707458496\n",
      "[Training Epoch 6] Batch 887, Loss 0.27634721994400024\n",
      "[Training Epoch 6] Batch 888, Loss 0.25673583149909973\n",
      "[Training Epoch 6] Batch 889, Loss 0.2685074806213379\n",
      "[Training Epoch 6] Batch 890, Loss 0.25958508253097534\n",
      "[Training Epoch 6] Batch 891, Loss 0.24783572554588318\n",
      "[Training Epoch 6] Batch 892, Loss 0.27495700120925903\n",
      "[Training Epoch 6] Batch 893, Loss 0.2801174521446228\n",
      "[Training Epoch 6] Batch 894, Loss 0.2648025155067444\n",
      "[Training Epoch 6] Batch 895, Loss 0.24451619386672974\n",
      "[Training Epoch 6] Batch 896, Loss 0.27676302194595337\n",
      "[Training Epoch 6] Batch 897, Loss 0.22543442249298096\n",
      "[Training Epoch 6] Batch 898, Loss 0.27323663234710693\n",
      "[Training Epoch 6] Batch 899, Loss 0.2746069133281708\n",
      "[Training Epoch 6] Batch 900, Loss 0.24601320922374725\n",
      "[Training Epoch 6] Batch 901, Loss 0.24618718028068542\n",
      "[Training Epoch 6] Batch 902, Loss 0.23825719952583313\n",
      "[Training Epoch 6] Batch 903, Loss 0.24532321095466614\n",
      "[Training Epoch 6] Batch 904, Loss 0.2276688814163208\n",
      "[Training Epoch 6] Batch 905, Loss 0.22835063934326172\n",
      "[Training Epoch 6] Batch 906, Loss 0.2478010356426239\n",
      "[Training Epoch 6] Batch 907, Loss 0.27331268787384033\n",
      "[Training Epoch 6] Batch 908, Loss 0.2721506655216217\n",
      "[Training Epoch 6] Batch 909, Loss 0.2594882845878601\n",
      "[Training Epoch 6] Batch 910, Loss 0.2227003276348114\n",
      "[Training Epoch 6] Batch 911, Loss 0.2517962157726288\n",
      "[Training Epoch 6] Batch 912, Loss 0.2987006604671478\n",
      "[Training Epoch 6] Batch 913, Loss 0.26872462034225464\n",
      "[Training Epoch 6] Batch 914, Loss 0.2525337338447571\n",
      "[Training Epoch 6] Batch 915, Loss 0.23128822445869446\n",
      "[Training Epoch 6] Batch 916, Loss 0.248632550239563\n",
      "[Training Epoch 6] Batch 917, Loss 0.22296524047851562\n",
      "[Training Epoch 6] Batch 918, Loss 0.2607157230377197\n",
      "[Training Epoch 6] Batch 919, Loss 0.26472729444503784\n",
      "[Training Epoch 6] Batch 920, Loss 0.25569969415664673\n",
      "[Training Epoch 6] Batch 921, Loss 0.2600802183151245\n",
      "[Training Epoch 6] Batch 922, Loss 0.22442933917045593\n",
      "[Training Epoch 6] Batch 923, Loss 0.276577889919281\n",
      "[Training Epoch 6] Batch 924, Loss 0.31045758724212646\n",
      "[Training Epoch 6] Batch 925, Loss 0.267547070980072\n",
      "[Training Epoch 6] Batch 926, Loss 0.22776412963867188\n",
      "[Training Epoch 6] Batch 927, Loss 0.26784631609916687\n",
      "[Training Epoch 6] Batch 928, Loss 0.24744880199432373\n",
      "[Training Epoch 6] Batch 929, Loss 0.2684151530265808\n",
      "[Training Epoch 6] Batch 930, Loss 0.28123417496681213\n",
      "[Training Epoch 6] Batch 931, Loss 0.27131199836730957\n",
      "[Training Epoch 6] Batch 932, Loss 0.24962268769741058\n",
      "[Training Epoch 6] Batch 933, Loss 0.3039478659629822\n",
      "[Training Epoch 6] Batch 934, Loss 0.21413305401802063\n",
      "[Training Epoch 6] Batch 935, Loss 0.2470860630273819\n",
      "[Training Epoch 6] Batch 936, Loss 0.24203069508075714\n",
      "[Training Epoch 6] Batch 937, Loss 0.2516334652900696\n",
      "[Training Epoch 6] Batch 938, Loss 0.28716087341308594\n",
      "[Training Epoch 6] Batch 939, Loss 0.24823151528835297\n",
      "[Training Epoch 6] Batch 940, Loss 0.27115121483802795\n",
      "[Training Epoch 6] Batch 941, Loss 0.2607262134552002\n",
      "[Training Epoch 6] Batch 942, Loss 0.2479519546031952\n",
      "[Training Epoch 6] Batch 943, Loss 0.25052717328071594\n",
      "[Training Epoch 6] Batch 944, Loss 0.25914180278778076\n",
      "[Training Epoch 6] Batch 945, Loss 0.2630690634250641\n",
      "[Training Epoch 6] Batch 946, Loss 0.23618432879447937\n",
      "[Training Epoch 6] Batch 947, Loss 0.26778024435043335\n",
      "[Training Epoch 6] Batch 948, Loss 0.2892893850803375\n",
      "[Training Epoch 6] Batch 949, Loss 0.2573012113571167\n",
      "[Training Epoch 6] Batch 950, Loss 0.2402878999710083\n",
      "[Training Epoch 6] Batch 951, Loss 0.23833411931991577\n",
      "[Training Epoch 6] Batch 952, Loss 0.2539556622505188\n",
      "[Training Epoch 6] Batch 953, Loss 0.27493512630462646\n",
      "[Training Epoch 6] Batch 954, Loss 0.26685214042663574\n",
      "[Training Epoch 6] Batch 955, Loss 0.28675246238708496\n",
      "[Training Epoch 6] Batch 956, Loss 0.22326558828353882\n",
      "[Training Epoch 6] Batch 957, Loss 0.2756733298301697\n",
      "[Training Epoch 6] Batch 958, Loss 0.2520502209663391\n",
      "[Training Epoch 6] Batch 959, Loss 0.2709510922431946\n",
      "[Training Epoch 6] Batch 960, Loss 0.2618056833744049\n",
      "[Training Epoch 6] Batch 961, Loss 0.25974491238594055\n",
      "[Training Epoch 6] Batch 962, Loss 0.24991852045059204\n",
      "[Training Epoch 6] Batch 963, Loss 0.28248360753059387\n",
      "[Training Epoch 6] Batch 964, Loss 0.23891161382198334\n",
      "[Training Epoch 6] Batch 965, Loss 0.2721590995788574\n",
      "[Training Epoch 6] Batch 966, Loss 0.2701268196105957\n",
      "[Training Epoch 6] Batch 967, Loss 0.2660682499408722\n",
      "[Training Epoch 6] Batch 968, Loss 0.26661211252212524\n",
      "[Training Epoch 6] Batch 969, Loss 0.23620662093162537\n",
      "[Training Epoch 6] Batch 970, Loss 0.24406471848487854\n",
      "[Training Epoch 6] Batch 971, Loss 0.25301942229270935\n",
      "[Training Epoch 6] Batch 972, Loss 0.24629193544387817\n",
      "[Training Epoch 6] Batch 973, Loss 0.22295285761356354\n",
      "[Training Epoch 6] Batch 974, Loss 0.23475739359855652\n",
      "[Training Epoch 6] Batch 975, Loss 0.23382316529750824\n",
      "[Training Epoch 6] Batch 976, Loss 0.2409166693687439\n",
      "[Training Epoch 6] Batch 977, Loss 0.2389717698097229\n",
      "[Training Epoch 6] Batch 978, Loss 0.23114994168281555\n",
      "[Training Epoch 6] Batch 979, Loss 0.24292582273483276\n",
      "[Training Epoch 6] Batch 980, Loss 0.2841191291809082\n",
      "[Training Epoch 6] Batch 981, Loss 0.28637993335723877\n",
      "[Training Epoch 6] Batch 982, Loss 0.2644268572330475\n",
      "[Training Epoch 6] Batch 983, Loss 0.25346118211746216\n",
      "[Training Epoch 6] Batch 984, Loss 0.25976842641830444\n",
      "[Training Epoch 6] Batch 985, Loss 0.2502524256706238\n",
      "[Training Epoch 6] Batch 986, Loss 0.2236727476119995\n",
      "[Training Epoch 6] Batch 987, Loss 0.23253211379051208\n",
      "[Training Epoch 6] Batch 988, Loss 0.2697897255420685\n",
      "[Training Epoch 6] Batch 989, Loss 0.27448999881744385\n",
      "[Training Epoch 6] Batch 990, Loss 0.28104373812675476\n",
      "[Training Epoch 6] Batch 991, Loss 0.2302171140909195\n",
      "[Training Epoch 6] Batch 992, Loss 0.2682148814201355\n",
      "[Training Epoch 6] Batch 993, Loss 0.27302536368370056\n",
      "[Training Epoch 6] Batch 994, Loss 0.2984236478805542\n",
      "[Training Epoch 6] Batch 995, Loss 0.27465304732322693\n",
      "[Training Epoch 6] Batch 996, Loss 0.2276858389377594\n",
      "[Training Epoch 6] Batch 997, Loss 0.27883416414260864\n",
      "[Training Epoch 6] Batch 998, Loss 0.2704562544822693\n",
      "[Training Epoch 6] Batch 999, Loss 0.22642311453819275\n",
      "[Training Epoch 6] Batch 1000, Loss 0.26721271872520447\n",
      "[Training Epoch 6] Batch 1001, Loss 0.2589542865753174\n",
      "[Training Epoch 6] Batch 1002, Loss 0.2388739287853241\n",
      "[Training Epoch 6] Batch 1003, Loss 0.2607329487800598\n",
      "[Training Epoch 6] Batch 1004, Loss 0.2277487814426422\n",
      "[Training Epoch 6] Batch 1005, Loss 0.2610282897949219\n",
      "[Training Epoch 6] Batch 1006, Loss 0.26523134112358093\n",
      "[Training Epoch 6] Batch 1007, Loss 0.31025150418281555\n",
      "[Training Epoch 6] Batch 1008, Loss 0.2382141351699829\n",
      "[Training Epoch 6] Batch 1009, Loss 0.23700305819511414\n",
      "[Training Epoch 6] Batch 1010, Loss 0.27344489097595215\n",
      "[Training Epoch 6] Batch 1011, Loss 0.2580440044403076\n",
      "[Training Epoch 6] Batch 1012, Loss 0.25671374797821045\n",
      "[Training Epoch 6] Batch 1013, Loss 0.2753428518772125\n",
      "[Training Epoch 6] Batch 1014, Loss 0.30621880292892456\n",
      "[Training Epoch 6] Batch 1015, Loss 0.275409460067749\n",
      "[Training Epoch 6] Batch 1016, Loss 0.25467485189437866\n",
      "[Training Epoch 6] Batch 1017, Loss 0.26016801595687866\n",
      "[Training Epoch 6] Batch 1018, Loss 0.2570958733558655\n",
      "[Training Epoch 6] Batch 1019, Loss 0.2584395110607147\n",
      "[Training Epoch 6] Batch 1020, Loss 0.25102895498275757\n",
      "[Training Epoch 6] Batch 1021, Loss 0.27588894963264465\n",
      "[Training Epoch 6] Batch 1022, Loss 0.24602621793746948\n",
      "[Training Epoch 6] Batch 1023, Loss 0.2625250816345215\n",
      "[Training Epoch 6] Batch 1024, Loss 0.24558742344379425\n",
      "[Training Epoch 6] Batch 1025, Loss 0.2728780508041382\n",
      "[Training Epoch 6] Batch 1026, Loss 0.268998920917511\n",
      "[Training Epoch 6] Batch 1027, Loss 0.2416694611310959\n",
      "[Training Epoch 6] Batch 1028, Loss 0.21878017485141754\n",
      "[Training Epoch 6] Batch 1029, Loss 0.24969787895679474\n",
      "[Training Epoch 6] Batch 1030, Loss 0.248777836561203\n",
      "[Training Epoch 6] Batch 1031, Loss 0.23563924431800842\n",
      "[Training Epoch 6] Batch 1032, Loss 0.25019729137420654\n",
      "[Training Epoch 6] Batch 1033, Loss 0.24976873397827148\n",
      "[Training Epoch 6] Batch 1034, Loss 0.2765430808067322\n",
      "[Training Epoch 6] Batch 1035, Loss 0.27519696950912476\n",
      "[Training Epoch 6] Batch 1036, Loss 0.25051212310791016\n",
      "[Training Epoch 6] Batch 1037, Loss 0.2521899342536926\n",
      "[Training Epoch 6] Batch 1038, Loss 0.27006831765174866\n",
      "[Training Epoch 6] Batch 1039, Loss 0.2734540104866028\n",
      "[Training Epoch 6] Batch 1040, Loss 0.283170223236084\n",
      "[Training Epoch 6] Batch 1041, Loss 0.24906252324581146\n",
      "[Training Epoch 6] Batch 1042, Loss 0.2841929495334625\n",
      "[Training Epoch 6] Batch 1043, Loss 0.3050677180290222\n",
      "[Training Epoch 6] Batch 1044, Loss 0.26530149579048157\n",
      "[Training Epoch 6] Batch 1045, Loss 0.23023483157157898\n",
      "[Training Epoch 6] Batch 1046, Loss 0.25451910495758057\n",
      "[Training Epoch 6] Batch 1047, Loss 0.27012068033218384\n",
      "[Training Epoch 6] Batch 1048, Loss 0.24868255853652954\n",
      "[Training Epoch 6] Batch 1049, Loss 0.2800150215625763\n",
      "[Training Epoch 6] Batch 1050, Loss 0.26418980956077576\n",
      "[Training Epoch 6] Batch 1051, Loss 0.24513478577136993\n",
      "[Training Epoch 6] Batch 1052, Loss 0.24978622794151306\n",
      "[Training Epoch 6] Batch 1053, Loss 0.26707723736763\n",
      "[Training Epoch 6] Batch 1054, Loss 0.2578295171260834\n",
      "[Training Epoch 6] Batch 1055, Loss 0.2400800585746765\n",
      "[Training Epoch 6] Batch 1056, Loss 0.2754189074039459\n",
      "[Training Epoch 6] Batch 1057, Loss 0.2611851692199707\n",
      "[Training Epoch 6] Batch 1058, Loss 0.2554144859313965\n",
      "[Training Epoch 6] Batch 1059, Loss 0.25549593567848206\n",
      "[Training Epoch 6] Batch 1060, Loss 0.2862003445625305\n",
      "[Training Epoch 6] Batch 1061, Loss 0.23573997616767883\n",
      "[Training Epoch 6] Batch 1062, Loss 0.2365734726190567\n",
      "[Training Epoch 6] Batch 1063, Loss 0.2679578959941864\n",
      "[Training Epoch 6] Batch 1064, Loss 0.2552263140678406\n",
      "[Training Epoch 6] Batch 1065, Loss 0.24382798373699188\n",
      "[Training Epoch 6] Batch 1066, Loss 0.25180909037590027\n",
      "[Training Epoch 6] Batch 1067, Loss 0.2601807713508606\n",
      "[Training Epoch 6] Batch 1068, Loss 0.2528931796550751\n",
      "[Training Epoch 6] Batch 1069, Loss 0.27341118454933167\n",
      "[Training Epoch 6] Batch 1070, Loss 0.27314120531082153\n",
      "[Training Epoch 6] Batch 1071, Loss 0.21672862768173218\n",
      "[Training Epoch 6] Batch 1072, Loss 0.23876731097698212\n",
      "[Training Epoch 6] Batch 1073, Loss 0.23313994705677032\n",
      "[Training Epoch 6] Batch 1074, Loss 0.27814844250679016\n",
      "[Training Epoch 6] Batch 1075, Loss 0.2572671175003052\n",
      "[Training Epoch 6] Batch 1076, Loss 0.27855056524276733\n",
      "[Training Epoch 6] Batch 1077, Loss 0.25115448236465454\n",
      "[Training Epoch 6] Batch 1078, Loss 0.2561146020889282\n",
      "[Training Epoch 6] Batch 1079, Loss 0.2555168569087982\n",
      "[Training Epoch 6] Batch 1080, Loss 0.25963956117630005\n",
      "[Training Epoch 6] Batch 1081, Loss 0.2639331519603729\n",
      "[Training Epoch 6] Batch 1082, Loss 0.27883976697921753\n",
      "[Training Epoch 6] Batch 1083, Loss 0.2554997205734253\n",
      "[Training Epoch 6] Batch 1084, Loss 0.2562946677207947\n",
      "[Training Epoch 6] Batch 1085, Loss 0.233931303024292\n",
      "[Training Epoch 6] Batch 1086, Loss 0.2767084240913391\n",
      "[Training Epoch 6] Batch 1087, Loss 0.265755295753479\n",
      "[Training Epoch 6] Batch 1088, Loss 0.26832684874534607\n",
      "[Training Epoch 6] Batch 1089, Loss 0.26969796419143677\n",
      "[Training Epoch 6] Batch 1090, Loss 0.2811213433742523\n",
      "[Training Epoch 6] Batch 1091, Loss 0.276121586561203\n",
      "[Training Epoch 6] Batch 1092, Loss 0.2549329400062561\n",
      "[Training Epoch 6] Batch 1093, Loss 0.2805076241493225\n",
      "[Training Epoch 6] Batch 1094, Loss 0.2668801546096802\n",
      "[Training Epoch 6] Batch 1095, Loss 0.28247755765914917\n",
      "[Training Epoch 6] Batch 1096, Loss 0.26817813515663147\n",
      "[Training Epoch 6] Batch 1097, Loss 0.24957343935966492\n",
      "[Training Epoch 6] Batch 1098, Loss 0.2568714916706085\n",
      "[Training Epoch 6] Batch 1099, Loss 0.2690883278846741\n",
      "[Training Epoch 6] Batch 1100, Loss 0.2680312395095825\n",
      "[Training Epoch 6] Batch 1101, Loss 0.23720130324363708\n",
      "[Training Epoch 6] Batch 1102, Loss 0.23740316927433014\n",
      "[Training Epoch 6] Batch 1103, Loss 0.2373131364583969\n",
      "[Training Epoch 6] Batch 1104, Loss 0.2620242238044739\n",
      "[Training Epoch 6] Batch 1105, Loss 0.2705410122871399\n",
      "[Training Epoch 6] Batch 1106, Loss 0.25933292508125305\n",
      "[Training Epoch 6] Batch 1107, Loss 0.25163912773132324\n",
      "[Training Epoch 6] Batch 1108, Loss 0.2560826241970062\n",
      "[Training Epoch 6] Batch 1109, Loss 0.26463955640792847\n",
      "[Training Epoch 6] Batch 1110, Loss 0.2653627097606659\n",
      "[Training Epoch 6] Batch 1111, Loss 0.2699635624885559\n",
      "[Training Epoch 6] Batch 1112, Loss 0.24917453527450562\n",
      "[Training Epoch 6] Batch 1113, Loss 0.25192010402679443\n",
      "[Training Epoch 6] Batch 1114, Loss 0.2704681158065796\n",
      "[Training Epoch 6] Batch 1115, Loss 0.27921611070632935\n",
      "[Training Epoch 6] Batch 1116, Loss 0.2635420560836792\n",
      "[Training Epoch 6] Batch 1117, Loss 0.2765699028968811\n",
      "[Training Epoch 6] Batch 1118, Loss 0.2620708644390106\n",
      "[Training Epoch 6] Batch 1119, Loss 0.2557586431503296\n",
      "[Training Epoch 6] Batch 1120, Loss 0.2563243508338928\n",
      "[Training Epoch 6] Batch 1121, Loss 0.27213776111602783\n",
      "[Training Epoch 6] Batch 1122, Loss 0.25041332840919495\n",
      "[Training Epoch 6] Batch 1123, Loss 0.25051677227020264\n",
      "[Training Epoch 6] Batch 1124, Loss 0.23084336519241333\n",
      "[Training Epoch 6] Batch 1125, Loss 0.2621040642261505\n",
      "[Training Epoch 6] Batch 1126, Loss 0.23788104951381683\n",
      "[Training Epoch 6] Batch 1127, Loss 0.23496104776859283\n",
      "[Training Epoch 6] Batch 1128, Loss 0.23121251165866852\n",
      "[Training Epoch 6] Batch 1129, Loss 0.2626369595527649\n",
      "[Training Epoch 6] Batch 1130, Loss 0.2339634746313095\n",
      "[Training Epoch 6] Batch 1131, Loss 0.2384563535451889\n",
      "[Training Epoch 6] Batch 1132, Loss 0.2564229369163513\n",
      "[Training Epoch 6] Batch 1133, Loss 0.23146158456802368\n",
      "[Training Epoch 6] Batch 1134, Loss 0.2693713307380676\n",
      "[Training Epoch 6] Batch 1135, Loss 0.25294333696365356\n",
      "[Training Epoch 6] Batch 1136, Loss 0.2644237279891968\n",
      "[Training Epoch 6] Batch 1137, Loss 0.23894208669662476\n",
      "[Training Epoch 6] Batch 1138, Loss 0.2813365161418915\n",
      "[Training Epoch 6] Batch 1139, Loss 0.2518662214279175\n",
      "[Training Epoch 6] Batch 1140, Loss 0.2524641156196594\n",
      "[Training Epoch 6] Batch 1141, Loss 0.2854865789413452\n",
      "[Training Epoch 6] Batch 1142, Loss 0.26018789410591125\n",
      "[Training Epoch 6] Batch 1143, Loss 0.23293015360832214\n",
      "[Training Epoch 6] Batch 1144, Loss 0.24537938833236694\n",
      "[Training Epoch 6] Batch 1145, Loss 0.2688409686088562\n",
      "[Training Epoch 6] Batch 1146, Loss 0.2739499807357788\n",
      "[Training Epoch 6] Batch 1147, Loss 0.26339972019195557\n",
      "[Training Epoch 6] Batch 1148, Loss 0.2648579776287079\n",
      "[Training Epoch 6] Batch 1149, Loss 0.22341035306453705\n",
      "[Training Epoch 6] Batch 1150, Loss 0.24101269245147705\n",
      "[Training Epoch 6] Batch 1151, Loss 0.2561407685279846\n",
      "[Training Epoch 6] Batch 1152, Loss 0.2771809697151184\n",
      "[Training Epoch 6] Batch 1153, Loss 0.28915366530418396\n",
      "[Training Epoch 6] Batch 1154, Loss 0.2318483591079712\n",
      "[Training Epoch 6] Batch 1155, Loss 0.235207661986351\n",
      "[Training Epoch 6] Batch 1156, Loss 0.22975939512252808\n",
      "[Training Epoch 6] Batch 1157, Loss 0.2518661618232727\n",
      "[Training Epoch 6] Batch 1158, Loss 0.24579475820064545\n",
      "[Training Epoch 6] Batch 1159, Loss 0.2775140106678009\n",
      "[Training Epoch 6] Batch 1160, Loss 0.24662622809410095\n",
      "[Training Epoch 6] Batch 1161, Loss 0.24134400486946106\n",
      "[Training Epoch 6] Batch 1162, Loss 0.27120599150657654\n",
      "[Training Epoch 6] Batch 1163, Loss 0.2989305853843689\n",
      "[Training Epoch 6] Batch 1164, Loss 0.27002081274986267\n",
      "[Training Epoch 6] Batch 1165, Loss 0.24672889709472656\n",
      "[Training Epoch 6] Batch 1166, Loss 0.27855491638183594\n",
      "[Training Epoch 6] Batch 1167, Loss 0.26057565212249756\n",
      "[Training Epoch 6] Batch 1168, Loss 0.22244524955749512\n",
      "[Training Epoch 6] Batch 1169, Loss 0.2374999076128006\n",
      "[Training Epoch 6] Batch 1170, Loss 0.25135165452957153\n",
      "[Training Epoch 6] Batch 1171, Loss 0.28158336877822876\n",
      "[Training Epoch 6] Batch 1172, Loss 0.26180416345596313\n",
      "[Training Epoch 6] Batch 1173, Loss 0.25421756505966187\n",
      "[Training Epoch 6] Batch 1174, Loss 0.2624432444572449\n",
      "[Training Epoch 6] Batch 1175, Loss 0.2651408612728119\n",
      "[Training Epoch 6] Batch 1176, Loss 0.25188300013542175\n",
      "[Training Epoch 6] Batch 1177, Loss 0.23616403341293335\n",
      "[Training Epoch 6] Batch 1178, Loss 0.27408450841903687\n",
      "[Training Epoch 6] Batch 1179, Loss 0.26185861229896545\n",
      "[Training Epoch 6] Batch 1180, Loss 0.24702303111553192\n",
      "[Training Epoch 6] Batch 1181, Loss 0.24762757122516632\n",
      "[Training Epoch 6] Batch 1182, Loss 0.2404659390449524\n",
      "[Training Epoch 6] Batch 1183, Loss 0.23183555901050568\n",
      "[Training Epoch 6] Batch 1184, Loss 0.22248223423957825\n",
      "[Training Epoch 6] Batch 1185, Loss 0.25930583477020264\n",
      "[Training Epoch 6] Batch 1186, Loss 0.280001163482666\n",
      "[Training Epoch 6] Batch 1187, Loss 0.2786095142364502\n",
      "[Training Epoch 6] Batch 1188, Loss 0.26045799255371094\n",
      "[Training Epoch 6] Batch 1189, Loss 0.2535105049610138\n",
      "[Training Epoch 6] Batch 1190, Loss 0.30740490555763245\n",
      "[Training Epoch 6] Batch 1191, Loss 0.25086653232574463\n",
      "[Training Epoch 6] Batch 1192, Loss 0.2570222020149231\n",
      "[Training Epoch 6] Batch 1193, Loss 0.26820850372314453\n",
      "[Training Epoch 6] Batch 1194, Loss 0.2921540141105652\n",
      "[Training Epoch 6] Batch 1195, Loss 0.2589092254638672\n",
      "[Training Epoch 6] Batch 1196, Loss 0.2504154145717621\n",
      "[Training Epoch 6] Batch 1197, Loss 0.25218719244003296\n",
      "[Training Epoch 6] Batch 1198, Loss 0.2311401069164276\n",
      "[Training Epoch 6] Batch 1199, Loss 0.24883639812469482\n",
      "[Training Epoch 6] Batch 1200, Loss 0.25575417280197144\n",
      "[Training Epoch 6] Batch 1201, Loss 0.23637282848358154\n",
      "[Training Epoch 6] Batch 1202, Loss 0.2363251894712448\n",
      "[Training Epoch 6] Batch 1203, Loss 0.27262526750564575\n",
      "[Training Epoch 6] Batch 1204, Loss 0.2521989047527313\n",
      "[Training Epoch 6] Batch 1205, Loss 0.24218276143074036\n",
      "[Training Epoch 6] Batch 1206, Loss 0.25250762701034546\n",
      "[Training Epoch 6] Batch 1207, Loss 0.2459685206413269\n",
      "[Training Epoch 6] Batch 1208, Loss 0.2571612000465393\n",
      "[Training Epoch 6] Batch 1209, Loss 0.250593900680542\n",
      "[Training Epoch 6] Batch 1210, Loss 0.2400212436914444\n",
      "[Training Epoch 6] Batch 1211, Loss 0.2674872577190399\n",
      "[Training Epoch 6] Batch 1212, Loss 0.26772457361221313\n",
      "[Training Epoch 6] Batch 1213, Loss 0.24998526275157928\n",
      "[Training Epoch 6] Batch 1214, Loss 0.24501529335975647\n",
      "[Training Epoch 6] Batch 1215, Loss 0.28169846534729004\n",
      "[Training Epoch 6] Batch 1216, Loss 0.2883453667163849\n",
      "[Training Epoch 6] Batch 1217, Loss 0.2616361379623413\n",
      "[Training Epoch 6] Batch 1218, Loss 0.24622821807861328\n",
      "[Training Epoch 6] Batch 1219, Loss 0.27996182441711426\n",
      "[Training Epoch 6] Batch 1220, Loss 0.25563281774520874\n",
      "[Training Epoch 6] Batch 1221, Loss 0.23670239746570587\n",
      "[Training Epoch 6] Batch 1222, Loss 0.25822627544403076\n",
      "[Training Epoch 6] Batch 1223, Loss 0.258802592754364\n",
      "[Training Epoch 6] Batch 1224, Loss 0.2785182595252991\n",
      "[Training Epoch 6] Batch 1225, Loss 0.25725191831588745\n",
      "[Training Epoch 6] Batch 1226, Loss 0.25506049394607544\n",
      "[Training Epoch 6] Batch 1227, Loss 0.27073031663894653\n",
      "[Training Epoch 6] Batch 1228, Loss 0.25942927598953247\n",
      "[Training Epoch 6] Batch 1229, Loss 0.26003801822662354\n",
      "[Training Epoch 6] Batch 1230, Loss 0.2427980899810791\n",
      "[Training Epoch 6] Batch 1231, Loss 0.2991212010383606\n",
      "[Training Epoch 6] Batch 1232, Loss 0.27085843682289124\n",
      "[Training Epoch 6] Batch 1233, Loss 0.27153873443603516\n",
      "[Training Epoch 6] Batch 1234, Loss 0.2671351134777069\n",
      "[Training Epoch 6] Batch 1235, Loss 0.24675217270851135\n",
      "[Training Epoch 6] Batch 1236, Loss 0.2559024393558502\n",
      "[Training Epoch 6] Batch 1237, Loss 0.2544831335544586\n",
      "[Training Epoch 6] Batch 1238, Loss 0.2262638509273529\n",
      "[Training Epoch 6] Batch 1239, Loss 0.24202729761600494\n",
      "[Training Epoch 6] Batch 1240, Loss 0.25655364990234375\n",
      "[Training Epoch 6] Batch 1241, Loss 0.28907814621925354\n",
      "[Training Epoch 6] Batch 1242, Loss 0.2554481327533722\n",
      "[Training Epoch 6] Batch 1243, Loss 0.2378046214580536\n",
      "[Training Epoch 6] Batch 1244, Loss 0.2594603896141052\n",
      "[Training Epoch 6] Batch 1245, Loss 0.27347511053085327\n",
      "[Training Epoch 6] Batch 1246, Loss 0.29640185832977295\n",
      "[Training Epoch 6] Batch 1247, Loss 0.24670499563217163\n",
      "[Training Epoch 6] Batch 1248, Loss 0.2674633860588074\n",
      "[Training Epoch 6] Batch 1249, Loss 0.24420946836471558\n",
      "[Training Epoch 6] Batch 1250, Loss 0.25902095437049866\n",
      "[Training Epoch 6] Batch 1251, Loss 0.25613653659820557\n",
      "[Training Epoch 6] Batch 1252, Loss 0.23187494277954102\n",
      "[Training Epoch 6] Batch 1253, Loss 0.2566138207912445\n",
      "[Training Epoch 6] Batch 1254, Loss 0.2691050171852112\n",
      "[Training Epoch 6] Batch 1255, Loss 0.23060669004917145\n",
      "[Training Epoch 6] Batch 1256, Loss 0.2603934407234192\n",
      "[Training Epoch 6] Batch 1257, Loss 0.2511352300643921\n",
      "[Training Epoch 6] Batch 1258, Loss 0.2631896138191223\n",
      "[Training Epoch 6] Batch 1259, Loss 0.24156683683395386\n",
      "[Training Epoch 6] Batch 1260, Loss 0.25807422399520874\n",
      "[Training Epoch 6] Batch 1261, Loss 0.24317049980163574\n",
      "[Training Epoch 6] Batch 1262, Loss 0.27187579870224\n",
      "[Training Epoch 6] Batch 1263, Loss 0.2594519257545471\n",
      "[Training Epoch 6] Batch 1264, Loss 0.23355871438980103\n",
      "[Training Epoch 6] Batch 1265, Loss 0.287067174911499\n",
      "[Training Epoch 6] Batch 1266, Loss 0.24267879128456116\n",
      "[Training Epoch 6] Batch 1267, Loss 0.25639963150024414\n",
      "[Training Epoch 6] Batch 1268, Loss 0.26358139514923096\n",
      "[Training Epoch 6] Batch 1269, Loss 0.2877504527568817\n",
      "[Training Epoch 6] Batch 1270, Loss 0.2645629644393921\n",
      "[Training Epoch 6] Batch 1271, Loss 0.2651127874851227\n",
      "[Training Epoch 6] Batch 1272, Loss 0.23860886693000793\n",
      "[Training Epoch 6] Batch 1273, Loss 0.2696676254272461\n",
      "[Training Epoch 6] Batch 1274, Loss 0.2758837938308716\n",
      "[Training Epoch 6] Batch 1275, Loss 0.2594057321548462\n",
      "[Training Epoch 6] Batch 1276, Loss 0.26948606967926025\n",
      "[Training Epoch 6] Batch 1277, Loss 0.28079670667648315\n",
      "[Training Epoch 6] Batch 1278, Loss 0.21948012709617615\n",
      "[Training Epoch 6] Batch 1279, Loss 0.2800820767879486\n",
      "[Training Epoch 6] Batch 1280, Loss 0.27986738085746765\n",
      "[Training Epoch 6] Batch 1281, Loss 0.2638745903968811\n",
      "[Training Epoch 6] Batch 1282, Loss 0.27764636278152466\n",
      "[Training Epoch 6] Batch 1283, Loss 0.2544458210468292\n",
      "[Training Epoch 6] Batch 1284, Loss 0.2521449327468872\n",
      "[Training Epoch 6] Batch 1285, Loss 0.2630959749221802\n",
      "[Training Epoch 6] Batch 1286, Loss 0.2511250972747803\n",
      "[Training Epoch 6] Batch 1287, Loss 0.27598124742507935\n",
      "[Training Epoch 6] Batch 1288, Loss 0.2691301703453064\n",
      "[Training Epoch 6] Batch 1289, Loss 0.23169450461864471\n",
      "[Training Epoch 6] Batch 1290, Loss 0.27128317952156067\n",
      "[Training Epoch 6] Batch 1291, Loss 0.24054983258247375\n",
      "[Training Epoch 6] Batch 1292, Loss 0.2625390887260437\n",
      "[Training Epoch 6] Batch 1293, Loss 0.269143670797348\n",
      "[Training Epoch 6] Batch 1294, Loss 0.26193422079086304\n",
      "[Training Epoch 6] Batch 1295, Loss 0.23401671648025513\n",
      "[Training Epoch 6] Batch 1296, Loss 0.26286762952804565\n",
      "[Training Epoch 6] Batch 1297, Loss 0.26710742712020874\n",
      "[Training Epoch 6] Batch 1298, Loss 0.24185672402381897\n",
      "[Training Epoch 6] Batch 1299, Loss 0.2614228129386902\n",
      "[Training Epoch 6] Batch 1300, Loss 0.24541305005550385\n",
      "[Training Epoch 6] Batch 1301, Loss 0.2896600365638733\n",
      "[Training Epoch 6] Batch 1302, Loss 0.2671073079109192\n",
      "[Training Epoch 6] Batch 1303, Loss 0.2556714117527008\n",
      "[Training Epoch 6] Batch 1304, Loss 0.2476198971271515\n",
      "[Training Epoch 6] Batch 1305, Loss 0.25183773040771484\n",
      "[Training Epoch 6] Batch 1306, Loss 0.2325044870376587\n",
      "[Training Epoch 6] Batch 1307, Loss 0.28087276220321655\n",
      "[Training Epoch 6] Batch 1308, Loss 0.25013265013694763\n",
      "[Training Epoch 6] Batch 1309, Loss 0.2841030955314636\n",
      "[Training Epoch 6] Batch 1310, Loss 0.26138269901275635\n",
      "[Training Epoch 6] Batch 1311, Loss 0.22818168997764587\n",
      "[Training Epoch 6] Batch 1312, Loss 0.24297747015953064\n",
      "[Training Epoch 6] Batch 1313, Loss 0.2581445872783661\n",
      "[Training Epoch 6] Batch 1314, Loss 0.2658475637435913\n",
      "[Training Epoch 6] Batch 1315, Loss 0.2606918215751648\n",
      "[Training Epoch 6] Batch 1316, Loss 0.25301602482795715\n",
      "[Training Epoch 6] Batch 1317, Loss 0.2650465667247772\n",
      "[Training Epoch 6] Batch 1318, Loss 0.2714739441871643\n",
      "[Training Epoch 6] Batch 1319, Loss 0.2966552972793579\n",
      "[Training Epoch 6] Batch 1320, Loss 0.25558626651763916\n",
      "[Training Epoch 6] Batch 1321, Loss 0.26762816309928894\n",
      "[Training Epoch 6] Batch 1322, Loss 0.26526910066604614\n",
      "[Training Epoch 6] Batch 1323, Loss 0.26685911417007446\n",
      "[Training Epoch 6] Batch 1324, Loss 0.27011266350746155\n",
      "[Training Epoch 6] Batch 1325, Loss 0.2660905718803406\n",
      "[Training Epoch 6] Batch 1326, Loss 0.2505795657634735\n",
      "[Training Epoch 6] Batch 1327, Loss 0.2609937787055969\n",
      "[Training Epoch 6] Batch 1328, Loss 0.24545904994010925\n",
      "[Training Epoch 6] Batch 1329, Loss 0.27573496103286743\n",
      "[Training Epoch 6] Batch 1330, Loss 0.24862957000732422\n",
      "[Training Epoch 6] Batch 1331, Loss 0.2645378112792969\n",
      "[Training Epoch 6] Batch 1332, Loss 0.28180742263793945\n",
      "[Training Epoch 6] Batch 1333, Loss 0.24756038188934326\n",
      "[Training Epoch 6] Batch 1334, Loss 0.25546586513519287\n",
      "[Training Epoch 6] Batch 1335, Loss 0.2498054802417755\n",
      "[Training Epoch 6] Batch 1336, Loss 0.2667878270149231\n",
      "[Training Epoch 6] Batch 1337, Loss 0.2582692503929138\n",
      "[Training Epoch 6] Batch 1338, Loss 0.24320906400680542\n",
      "[Training Epoch 6] Batch 1339, Loss 0.2368883192539215\n",
      "[Training Epoch 6] Batch 1340, Loss 0.2567669749259949\n",
      "[Training Epoch 6] Batch 1341, Loss 0.245578795671463\n",
      "[Training Epoch 6] Batch 1342, Loss 0.2735140919685364\n",
      "[Training Epoch 6] Batch 1343, Loss 0.24865809082984924\n",
      "[Training Epoch 6] Batch 1344, Loss 0.26288938522338867\n",
      "[Training Epoch 6] Batch 1345, Loss 0.26966819167137146\n",
      "[Training Epoch 6] Batch 1346, Loss 0.25654882192611694\n",
      "[Training Epoch 6] Batch 1347, Loss 0.26660817861557007\n",
      "[Training Epoch 6] Batch 1348, Loss 0.2515031695365906\n",
      "[Training Epoch 6] Batch 1349, Loss 0.2879607379436493\n",
      "[Training Epoch 6] Batch 1350, Loss 0.27501505613327026\n",
      "[Training Epoch 6] Batch 1351, Loss 0.2626022696495056\n",
      "[Training Epoch 6] Batch 1352, Loss 0.22279702126979828\n",
      "[Training Epoch 6] Batch 1353, Loss 0.28156015276908875\n",
      "[Training Epoch 6] Batch 1354, Loss 0.24379245936870575\n",
      "[Training Epoch 6] Batch 1355, Loss 0.2987217903137207\n",
      "[Training Epoch 6] Batch 1356, Loss 0.2680217921733856\n",
      "[Training Epoch 6] Batch 1357, Loss 0.2752969264984131\n",
      "[Training Epoch 6] Batch 1358, Loss 0.23886612057685852\n",
      "[Training Epoch 6] Batch 1359, Loss 0.25764715671539307\n",
      "[Training Epoch 6] Batch 1360, Loss 0.26468026638031006\n",
      "[Training Epoch 6] Batch 1361, Loss 0.24508251249790192\n",
      "[Training Epoch 6] Batch 1362, Loss 0.25539395213127136\n",
      "[Training Epoch 6] Batch 1363, Loss 0.2699914574623108\n",
      "[Training Epoch 6] Batch 1364, Loss 0.2564394772052765\n",
      "[Training Epoch 6] Batch 1365, Loss 0.24310562014579773\n",
      "[Training Epoch 6] Batch 1366, Loss 0.23214508593082428\n",
      "[Training Epoch 6] Batch 1367, Loss 0.28084051609039307\n",
      "[Training Epoch 6] Batch 1368, Loss 0.26888346672058105\n",
      "[Training Epoch 6] Batch 1369, Loss 0.26197248697280884\n",
      "[Training Epoch 6] Batch 1370, Loss 0.24215713143348694\n",
      "[Training Epoch 6] Batch 1371, Loss 0.25479787588119507\n",
      "[Training Epoch 6] Batch 1372, Loss 0.2965383231639862\n",
      "[Training Epoch 6] Batch 1373, Loss 0.28560423851013184\n",
      "[Training Epoch 6] Batch 1374, Loss 0.2716054916381836\n",
      "[Training Epoch 6] Batch 1375, Loss 0.26429203152656555\n",
      "[Training Epoch 6] Batch 1376, Loss 0.2520902752876282\n",
      "[Training Epoch 6] Batch 1377, Loss 0.24716763198375702\n",
      "[Training Epoch 6] Batch 1378, Loss 0.2703230082988739\n",
      "[Training Epoch 6] Batch 1379, Loss 0.24533504247665405\n",
      "[Training Epoch 6] Batch 1380, Loss 0.2588483691215515\n",
      "[Training Epoch 6] Batch 1381, Loss 0.24077028036117554\n",
      "[Training Epoch 6] Batch 1382, Loss 0.2650158405303955\n",
      "[Training Epoch 6] Batch 1383, Loss 0.2625177800655365\n",
      "[Training Epoch 6] Batch 1384, Loss 0.236731618642807\n",
      "[Training Epoch 6] Batch 1385, Loss 0.25411665439605713\n",
      "[Training Epoch 6] Batch 1386, Loss 0.2235826551914215\n",
      "[Training Epoch 6] Batch 1387, Loss 0.275117963552475\n",
      "[Training Epoch 6] Batch 1388, Loss 0.282514750957489\n",
      "[Training Epoch 6] Batch 1389, Loss 0.24491530656814575\n",
      "[Training Epoch 6] Batch 1390, Loss 0.2699393033981323\n",
      "[Training Epoch 6] Batch 1391, Loss 0.27561068534851074\n",
      "[Training Epoch 6] Batch 1392, Loss 0.25197675824165344\n",
      "[Training Epoch 6] Batch 1393, Loss 0.25075024366378784\n",
      "[Training Epoch 6] Batch 1394, Loss 0.2774413824081421\n",
      "[Training Epoch 6] Batch 1395, Loss 0.25325778126716614\n",
      "[Training Epoch 6] Batch 1396, Loss 0.24747440218925476\n",
      "[Training Epoch 6] Batch 1397, Loss 0.27067476511001587\n",
      "[Training Epoch 6] Batch 1398, Loss 0.24253031611442566\n",
      "[Training Epoch 6] Batch 1399, Loss 0.2849114239215851\n",
      "[Training Epoch 6] Batch 1400, Loss 0.24767544865608215\n",
      "[Training Epoch 6] Batch 1401, Loss 0.2620388865470886\n",
      "[Training Epoch 6] Batch 1402, Loss 0.285957396030426\n",
      "[Training Epoch 6] Batch 1403, Loss 0.24032536149024963\n",
      "[Training Epoch 6] Batch 1404, Loss 0.2581097483634949\n",
      "[Training Epoch 6] Batch 1405, Loss 0.24398934841156006\n",
      "[Training Epoch 6] Batch 1406, Loss 0.2511039078235626\n",
      "[Training Epoch 6] Batch 1407, Loss 0.24894945323467255\n",
      "[Training Epoch 6] Batch 1408, Loss 0.23302710056304932\n",
      "[Training Epoch 6] Batch 1409, Loss 0.2601419985294342\n",
      "[Training Epoch 6] Batch 1410, Loss 0.26577723026275635\n",
      "[Training Epoch 6] Batch 1411, Loss 0.26151517033576965\n",
      "[Training Epoch 6] Batch 1412, Loss 0.23890343308448792\n",
      "[Training Epoch 6] Batch 1413, Loss 0.2998506724834442\n",
      "[Training Epoch 6] Batch 1414, Loss 0.24035517871379852\n",
      "[Training Epoch 6] Batch 1415, Loss 0.28111934661865234\n",
      "[Training Epoch 6] Batch 1416, Loss 0.25722551345825195\n",
      "[Training Epoch 6] Batch 1417, Loss 0.25036805868148804\n",
      "[Training Epoch 6] Batch 1418, Loss 0.2650940418243408\n",
      "[Training Epoch 6] Batch 1419, Loss 0.23830223083496094\n",
      "[Training Epoch 6] Batch 1420, Loss 0.2876434326171875\n",
      "[Training Epoch 6] Batch 1421, Loss 0.2330026924610138\n",
      "[Training Epoch 6] Batch 1422, Loss 0.23011308908462524\n",
      "[Training Epoch 6] Batch 1423, Loss 0.2448059320449829\n",
      "[Training Epoch 6] Batch 1424, Loss 0.2489110231399536\n",
      "[Training Epoch 6] Batch 1425, Loss 0.25071191787719727\n",
      "[Training Epoch 6] Batch 1426, Loss 0.28331461548805237\n",
      "[Training Epoch 6] Batch 1427, Loss 0.25632190704345703\n",
      "[Training Epoch 6] Batch 1428, Loss 0.24451303482055664\n",
      "[Training Epoch 6] Batch 1429, Loss 0.24000248312950134\n",
      "[Training Epoch 6] Batch 1430, Loss 0.24177008867263794\n",
      "[Training Epoch 6] Batch 1431, Loss 0.2579934597015381\n",
      "[Training Epoch 6] Batch 1432, Loss 0.25778186321258545\n",
      "[Training Epoch 6] Batch 1433, Loss 0.2821309268474579\n",
      "[Training Epoch 6] Batch 1434, Loss 0.2596698999404907\n",
      "[Training Epoch 6] Batch 1435, Loss 0.26719769835472107\n",
      "[Training Epoch 6] Batch 1436, Loss 0.2810109853744507\n",
      "[Training Epoch 6] Batch 1437, Loss 0.24547770619392395\n",
      "[Training Epoch 6] Batch 1438, Loss 0.277553915977478\n",
      "[Training Epoch 6] Batch 1439, Loss 0.25775280594825745\n",
      "[Training Epoch 6] Batch 1440, Loss 0.25988954305648804\n",
      "[Training Epoch 6] Batch 1441, Loss 0.23379302024841309\n",
      "[Training Epoch 6] Batch 1442, Loss 0.2647233307361603\n",
      "[Training Epoch 6] Batch 1443, Loss 0.31506410241127014\n",
      "[Training Epoch 6] Batch 1444, Loss 0.28174927830696106\n",
      "[Training Epoch 6] Batch 1445, Loss 0.25142866373062134\n",
      "[Training Epoch 6] Batch 1446, Loss 0.2921212911605835\n",
      "[Training Epoch 6] Batch 1447, Loss 0.2912234663963318\n",
      "[Training Epoch 6] Batch 1448, Loss 0.2877155840396881\n",
      "[Training Epoch 6] Batch 1449, Loss 0.2382778376340866\n",
      "[Training Epoch 6] Batch 1450, Loss 0.2584977149963379\n",
      "[Training Epoch 6] Batch 1451, Loss 0.27565062046051025\n",
      "[Training Epoch 6] Batch 1452, Loss 0.26581871509552\n",
      "[Training Epoch 6] Batch 1453, Loss 0.26541703939437866\n",
      "[Training Epoch 6] Batch 1454, Loss 0.24588912725448608\n",
      "[Training Epoch 6] Batch 1455, Loss 0.2572524845600128\n",
      "[Training Epoch 6] Batch 1456, Loss 0.2528253495693207\n",
      "[Training Epoch 6] Batch 1457, Loss 0.2512235939502716\n",
      "[Training Epoch 6] Batch 1458, Loss 0.26608148217201233\n",
      "[Training Epoch 6] Batch 1459, Loss 0.28186434507369995\n",
      "[Training Epoch 6] Batch 1460, Loss 0.23274117708206177\n",
      "[Training Epoch 6] Batch 1461, Loss 0.23904183506965637\n",
      "[Training Epoch 6] Batch 1462, Loss 0.2696755528450012\n",
      "[Training Epoch 6] Batch 1463, Loss 0.2708079218864441\n",
      "[Training Epoch 6] Batch 1464, Loss 0.2486647665500641\n",
      "[Training Epoch 6] Batch 1465, Loss 0.2943178415298462\n",
      "[Training Epoch 6] Batch 1466, Loss 0.2690260410308838\n",
      "[Training Epoch 6] Batch 1467, Loss 0.2746015787124634\n",
      "[Training Epoch 6] Batch 1468, Loss 0.23483562469482422\n",
      "[Training Epoch 6] Batch 1469, Loss 0.26631733775138855\n",
      "[Training Epoch 6] Batch 1470, Loss 0.271411269903183\n",
      "[Training Epoch 6] Batch 1471, Loss 0.2553673982620239\n",
      "[Training Epoch 6] Batch 1472, Loss 0.24140143394470215\n",
      "[Training Epoch 6] Batch 1473, Loss 0.2852385640144348\n",
      "[Training Epoch 6] Batch 1474, Loss 0.2583981156349182\n",
      "[Training Epoch 6] Batch 1475, Loss 0.2677944004535675\n",
      "[Training Epoch 6] Batch 1476, Loss 0.26333504915237427\n",
      "[Training Epoch 6] Batch 1477, Loss 0.22595331072807312\n",
      "[Training Epoch 6] Batch 1478, Loss 0.2746857702732086\n",
      "[Training Epoch 6] Batch 1479, Loss 0.2717337906360626\n",
      "[Training Epoch 6] Batch 1480, Loss 0.2501356601715088\n",
      "[Training Epoch 6] Batch 1481, Loss 0.28630125522613525\n",
      "[Training Epoch 6] Batch 1482, Loss 0.28168755769729614\n",
      "[Training Epoch 6] Batch 1483, Loss 0.24822621047496796\n",
      "[Training Epoch 6] Batch 1484, Loss 0.29629480838775635\n",
      "[Training Epoch 6] Batch 1485, Loss 0.23967930674552917\n",
      "[Training Epoch 6] Batch 1486, Loss 0.26608189940452576\n",
      "[Training Epoch 6] Batch 1487, Loss 0.24266567826271057\n",
      "[Training Epoch 6] Batch 1488, Loss 0.261030375957489\n",
      "[Training Epoch 6] Batch 1489, Loss 0.2651234269142151\n",
      "[Training Epoch 6] Batch 1490, Loss 0.2670150399208069\n",
      "[Training Epoch 6] Batch 1491, Loss 0.247340589761734\n",
      "[Training Epoch 6] Batch 1492, Loss 0.2689087390899658\n",
      "[Training Epoch 6] Batch 1493, Loss 0.2584225535392761\n",
      "[Training Epoch 6] Batch 1494, Loss 0.23171406984329224\n",
      "[Training Epoch 6] Batch 1495, Loss 0.2787601351737976\n",
      "[Training Epoch 6] Batch 1496, Loss 0.23722627758979797\n",
      "[Training Epoch 6] Batch 1497, Loss 0.2353791743516922\n",
      "[Training Epoch 6] Batch 1498, Loss 0.25086936354637146\n",
      "[Training Epoch 6] Batch 1499, Loss 0.2514469623565674\n",
      "[Training Epoch 6] Batch 1500, Loss 0.2572041451931\n",
      "[Training Epoch 6] Batch 1501, Loss 0.2770814895629883\n",
      "[Training Epoch 6] Batch 1502, Loss 0.2703717350959778\n",
      "[Training Epoch 6] Batch 1503, Loss 0.2629988193511963\n",
      "[Training Epoch 6] Batch 1504, Loss 0.2620527744293213\n",
      "[Training Epoch 6] Batch 1505, Loss 0.254891574382782\n",
      "[Training Epoch 6] Batch 1506, Loss 0.2914150357246399\n",
      "[Training Epoch 6] Batch 1507, Loss 0.24602165818214417\n",
      "[Training Epoch 6] Batch 1508, Loss 0.242184579372406\n",
      "[Training Epoch 6] Batch 1509, Loss 0.27448058128356934\n",
      "[Training Epoch 6] Batch 1510, Loss 0.24151268601417542\n",
      "[Training Epoch 6] Batch 1511, Loss 0.23909305036067963\n",
      "[Training Epoch 6] Batch 1512, Loss 0.2415294498205185\n",
      "[Training Epoch 6] Batch 1513, Loss 0.2705768346786499\n",
      "[Training Epoch 6] Batch 1514, Loss 0.24662694334983826\n",
      "[Training Epoch 6] Batch 1515, Loss 0.24625945091247559\n",
      "[Training Epoch 6] Batch 1516, Loss 0.25156426429748535\n",
      "[Training Epoch 6] Batch 1517, Loss 0.2574957609176636\n",
      "[Training Epoch 6] Batch 1518, Loss 0.2927153706550598\n",
      "[Training Epoch 6] Batch 1519, Loss 0.2540140748023987\n",
      "[Training Epoch 6] Batch 1520, Loss 0.2728239893913269\n",
      "[Training Epoch 6] Batch 1521, Loss 0.2702997922897339\n",
      "[Training Epoch 6] Batch 1522, Loss 0.23211833834648132\n",
      "[Training Epoch 6] Batch 1523, Loss 0.27237188816070557\n",
      "[Training Epoch 6] Batch 1524, Loss 0.26479098200798035\n",
      "[Training Epoch 6] Batch 1525, Loss 0.2609885334968567\n",
      "[Training Epoch 6] Batch 1526, Loss 0.2545069754123688\n",
      "[Training Epoch 6] Batch 1527, Loss 0.2625289559364319\n",
      "[Training Epoch 6] Batch 1528, Loss 0.27031153440475464\n",
      "[Training Epoch 6] Batch 1529, Loss 0.2525118589401245\n",
      "[Training Epoch 6] Batch 1530, Loss 0.2870141565799713\n",
      "[Training Epoch 6] Batch 1531, Loss 0.23479756712913513\n",
      "[Training Epoch 6] Batch 1532, Loss 0.2905905246734619\n",
      "[Training Epoch 6] Batch 1533, Loss 0.3052799701690674\n",
      "[Training Epoch 6] Batch 1534, Loss 0.24411261081695557\n",
      "[Training Epoch 6] Batch 1535, Loss 0.26122456789016724\n",
      "[Training Epoch 6] Batch 1536, Loss 0.24021995067596436\n",
      "[Training Epoch 6] Batch 1537, Loss 0.2510930001735687\n",
      "[Training Epoch 6] Batch 1538, Loss 0.2559373378753662\n",
      "[Training Epoch 6] Batch 1539, Loss 0.246902197599411\n",
      "[Training Epoch 6] Batch 1540, Loss 0.2510685622692108\n",
      "[Training Epoch 6] Batch 1541, Loss 0.24024179577827454\n",
      "[Training Epoch 6] Batch 1542, Loss 0.2729146480560303\n",
      "[Training Epoch 6] Batch 1543, Loss 0.27646756172180176\n",
      "[Training Epoch 6] Batch 1544, Loss 0.26824498176574707\n",
      "[Training Epoch 6] Batch 1545, Loss 0.251361221075058\n",
      "[Training Epoch 6] Batch 1546, Loss 0.25884971022605896\n",
      "[Training Epoch 6] Batch 1547, Loss 0.244257390499115\n",
      "[Training Epoch 6] Batch 1548, Loss 0.22035053372383118\n",
      "[Training Epoch 6] Batch 1549, Loss 0.25684240460395813\n",
      "[Training Epoch 6] Batch 1550, Loss 0.266700804233551\n",
      "[Training Epoch 6] Batch 1551, Loss 0.2520209848880768\n",
      "[Training Epoch 6] Batch 1552, Loss 0.24622343480587006\n",
      "[Training Epoch 6] Batch 1553, Loss 0.26383841037750244\n",
      "[Training Epoch 6] Batch 1554, Loss 0.2793996036052704\n",
      "[Training Epoch 6] Batch 1555, Loss 0.2544383704662323\n",
      "[Training Epoch 6] Batch 1556, Loss 0.25711220502853394\n",
      "[Training Epoch 6] Batch 1557, Loss 0.25044912099838257\n",
      "[Training Epoch 6] Batch 1558, Loss 0.2502763271331787\n",
      "[Training Epoch 6] Batch 1559, Loss 0.27837228775024414\n",
      "[Training Epoch 6] Batch 1560, Loss 0.2625519335269928\n",
      "[Training Epoch 6] Batch 1561, Loss 0.2612425684928894\n",
      "[Training Epoch 6] Batch 1562, Loss 0.2507951855659485\n",
      "[Training Epoch 6] Batch 1563, Loss 0.27200132608413696\n",
      "[Training Epoch 6] Batch 1564, Loss 0.23191134631633759\n",
      "[Training Epoch 6] Batch 1565, Loss 0.2490520477294922\n",
      "[Training Epoch 6] Batch 1566, Loss 0.2555834650993347\n",
      "[Training Epoch 6] Batch 1567, Loss 0.22487978637218475\n",
      "[Training Epoch 6] Batch 1568, Loss 0.2620652914047241\n",
      "[Training Epoch 6] Batch 1569, Loss 0.2642098069190979\n",
      "[Training Epoch 6] Batch 1570, Loss 0.25334009528160095\n",
      "[Training Epoch 6] Batch 1571, Loss 0.23319664597511292\n",
      "[Training Epoch 6] Batch 1572, Loss 0.28556832671165466\n",
      "[Training Epoch 6] Batch 1573, Loss 0.25913500785827637\n",
      "[Training Epoch 6] Batch 1574, Loss 0.22131143510341644\n",
      "[Training Epoch 6] Batch 1575, Loss 0.2625521123409271\n",
      "[Training Epoch 6] Batch 1576, Loss 0.24761638045310974\n",
      "[Training Epoch 6] Batch 1577, Loss 0.2850075364112854\n",
      "[Training Epoch 6] Batch 1578, Loss 0.2560163736343384\n",
      "[Training Epoch 6] Batch 1579, Loss 0.2606407701969147\n",
      "[Training Epoch 6] Batch 1580, Loss 0.23939837515354156\n",
      "[Training Epoch 6] Batch 1581, Loss 0.24699462950229645\n",
      "[Training Epoch 6] Batch 1582, Loss 0.25183409452438354\n",
      "[Training Epoch 6] Batch 1583, Loss 0.2369062900543213\n",
      "[Training Epoch 6] Batch 1584, Loss 0.2556695342063904\n",
      "[Training Epoch 6] Batch 1585, Loss 0.2776191234588623\n",
      "[Training Epoch 6] Batch 1586, Loss 0.23902937769889832\n",
      "[Training Epoch 6] Batch 1587, Loss 0.27113106846809387\n",
      "[Training Epoch 6] Batch 1588, Loss 0.2906721830368042\n",
      "[Training Epoch 6] Batch 1589, Loss 0.25296908617019653\n",
      "[Training Epoch 6] Batch 1590, Loss 0.23400716483592987\n",
      "[Training Epoch 6] Batch 1591, Loss 0.2449205219745636\n",
      "[Training Epoch 6] Batch 1592, Loss 0.2341432273387909\n",
      "[Training Epoch 6] Batch 1593, Loss 0.23058761656284332\n",
      "[Training Epoch 6] Batch 1594, Loss 0.23642899096012115\n",
      "[Training Epoch 6] Batch 1595, Loss 0.26066854596138\n",
      "[Training Epoch 6] Batch 1596, Loss 0.24720744788646698\n",
      "[Training Epoch 6] Batch 1597, Loss 0.2539563477039337\n",
      "[Training Epoch 6] Batch 1598, Loss 0.2575700283050537\n",
      "[Training Epoch 6] Batch 1599, Loss 0.2676760256290436\n",
      "[Training Epoch 6] Batch 1600, Loss 0.26815617084503174\n",
      "[Training Epoch 6] Batch 1601, Loss 0.24200190603733063\n",
      "[Training Epoch 6] Batch 1602, Loss 0.2554352879524231\n",
      "[Training Epoch 6] Batch 1603, Loss 0.23675669729709625\n",
      "[Training Epoch 6] Batch 1604, Loss 0.2664458453655243\n",
      "[Training Epoch 6] Batch 1605, Loss 0.2533120810985565\n",
      "[Training Epoch 6] Batch 1606, Loss 0.2534599006175995\n",
      "[Training Epoch 6] Batch 1607, Loss 0.3079657554626465\n",
      "[Training Epoch 6] Batch 1608, Loss 0.2579444944858551\n",
      "[Training Epoch 6] Batch 1609, Loss 0.2733262777328491\n",
      "[Training Epoch 6] Batch 1610, Loss 0.282184362411499\n",
      "[Training Epoch 6] Batch 1611, Loss 0.2486356496810913\n",
      "[Training Epoch 6] Batch 1612, Loss 0.23098841309547424\n",
      "[Training Epoch 6] Batch 1613, Loss 0.28745701909065247\n",
      "[Training Epoch 6] Batch 1614, Loss 0.2507475018501282\n",
      "[Training Epoch 6] Batch 1615, Loss 0.2598506808280945\n",
      "[Training Epoch 6] Batch 1616, Loss 0.24506473541259766\n",
      "[Training Epoch 6] Batch 1617, Loss 0.23251336812973022\n",
      "[Training Epoch 6] Batch 1618, Loss 0.2607837915420532\n",
      "[Training Epoch 6] Batch 1619, Loss 0.25258880853652954\n",
      "[Training Epoch 6] Batch 1620, Loss 0.2675846219062805\n",
      "[Training Epoch 6] Batch 1621, Loss 0.25361403822898865\n",
      "[Training Epoch 6] Batch 1622, Loss 0.2667376399040222\n",
      "[Training Epoch 6] Batch 1623, Loss 0.24799805879592896\n",
      "[Training Epoch 6] Batch 1624, Loss 0.2775137424468994\n",
      "[Training Epoch 6] Batch 1625, Loss 0.236967533826828\n",
      "[Training Epoch 6] Batch 1626, Loss 0.2889353632926941\n",
      "[Training Epoch 6] Batch 1627, Loss 0.2811071574687958\n",
      "[Training Epoch 6] Batch 1628, Loss 0.23728281259536743\n",
      "[Training Epoch 6] Batch 1629, Loss 0.22108763456344604\n",
      "[Training Epoch 6] Batch 1630, Loss 0.2529674172401428\n",
      "[Training Epoch 6] Batch 1631, Loss 0.2758252024650574\n",
      "[Training Epoch 6] Batch 1632, Loss 0.25246360898017883\n",
      "[Training Epoch 6] Batch 1633, Loss 0.31516730785369873\n",
      "[Training Epoch 6] Batch 1634, Loss 0.2700954079627991\n",
      "[Training Epoch 6] Batch 1635, Loss 0.2725585699081421\n",
      "[Training Epoch 6] Batch 1636, Loss 0.2535374164581299\n",
      "[Training Epoch 6] Batch 1637, Loss 0.2520841956138611\n",
      "[Training Epoch 6] Batch 1638, Loss 0.24971544742584229\n",
      "[Training Epoch 6] Batch 1639, Loss 0.23861277103424072\n",
      "[Training Epoch 6] Batch 1640, Loss 0.27159780263900757\n",
      "[Training Epoch 6] Batch 1641, Loss 0.2506023049354553\n",
      "[Training Epoch 6] Batch 1642, Loss 0.24953049421310425\n",
      "[Training Epoch 6] Batch 1643, Loss 0.25773173570632935\n",
      "[Training Epoch 6] Batch 1644, Loss 0.24189603328704834\n",
      "[Training Epoch 6] Batch 1645, Loss 0.24895018339157104\n",
      "[Training Epoch 6] Batch 1646, Loss 0.29126405715942383\n",
      "[Training Epoch 6] Batch 1647, Loss 0.2404298484325409\n",
      "[Training Epoch 6] Batch 1648, Loss 0.27790138125419617\n",
      "[Training Epoch 6] Batch 1649, Loss 0.25470632314682007\n",
      "[Training Epoch 6] Batch 1650, Loss 0.27061110734939575\n",
      "[Training Epoch 6] Batch 1651, Loss 0.2653766870498657\n",
      "[Training Epoch 6] Batch 1652, Loss 0.2821776270866394\n",
      "[Training Epoch 6] Batch 1653, Loss 0.26392510533332825\n",
      "[Training Epoch 6] Batch 1654, Loss 0.2767912745475769\n",
      "[Training Epoch 6] Batch 1655, Loss 0.2802271246910095\n",
      "[Training Epoch 6] Batch 1656, Loss 0.27406466007232666\n",
      "[Training Epoch 6] Batch 1657, Loss 0.2956298589706421\n",
      "[Training Epoch 6] Batch 1658, Loss 0.2564545273780823\n",
      "[Training Epoch 6] Batch 1659, Loss 0.3020458221435547\n",
      "[Training Epoch 6] Batch 1660, Loss 0.25993141531944275\n",
      "[Training Epoch 6] Batch 1661, Loss 0.2628680467605591\n",
      "[Training Epoch 6] Batch 1662, Loss 0.27031436562538147\n",
      "[Training Epoch 6] Batch 1663, Loss 0.2556154727935791\n",
      "[Training Epoch 6] Batch 1664, Loss 0.24715586006641388\n",
      "[Training Epoch 6] Batch 1665, Loss 0.25967589020729065\n",
      "[Training Epoch 6] Batch 1666, Loss 0.2763327360153198\n",
      "[Training Epoch 6] Batch 1667, Loss 0.24054527282714844\n",
      "[Training Epoch 6] Batch 1668, Loss 0.2761019170284271\n",
      "[Training Epoch 6] Batch 1669, Loss 0.2769187390804291\n",
      "[Training Epoch 6] Batch 1670, Loss 0.25254955887794495\n",
      "[Training Epoch 6] Batch 1671, Loss 0.24003848433494568\n",
      "[Training Epoch 6] Batch 1672, Loss 0.25811418890953064\n",
      "[Training Epoch 6] Batch 1673, Loss 0.26031091809272766\n",
      "[Training Epoch 6] Batch 1674, Loss 0.2613365650177002\n",
      "[Training Epoch 6] Batch 1675, Loss 0.24240155518054962\n",
      "[Training Epoch 6] Batch 1676, Loss 0.2227233201265335\n",
      "[Training Epoch 6] Batch 1677, Loss 0.26298415660858154\n",
      "[Training Epoch 6] Batch 1678, Loss 0.2549855411052704\n",
      "[Training Epoch 6] Batch 1679, Loss 0.2829591929912567\n",
      "[Training Epoch 6] Batch 1680, Loss 0.26032140851020813\n",
      "[Training Epoch 6] Batch 1681, Loss 0.23972083628177643\n",
      "[Training Epoch 6] Batch 1682, Loss 0.26532232761383057\n",
      "[Training Epoch 6] Batch 1683, Loss 0.2501230835914612\n",
      "[Training Epoch 6] Batch 1684, Loss 0.23817574977874756\n",
      "[Training Epoch 6] Batch 1685, Loss 0.24240586161613464\n",
      "[Training Epoch 6] Batch 1686, Loss 0.29541677236557007\n",
      "[Training Epoch 6] Batch 1687, Loss 0.2532046437263489\n",
      "[Training Epoch 6] Batch 1688, Loss 0.25313255190849304\n",
      "[Training Epoch 6] Batch 1689, Loss 0.2268431931734085\n",
      "[Training Epoch 6] Batch 1690, Loss 0.26736485958099365\n",
      "[Training Epoch 6] Batch 1691, Loss 0.2559633255004883\n",
      "[Training Epoch 6] Batch 1692, Loss 0.28173571825027466\n",
      "[Training Epoch 6] Batch 1693, Loss 0.22337913513183594\n",
      "[Training Epoch 6] Batch 1694, Loss 0.23534312844276428\n",
      "[Training Epoch 6] Batch 1695, Loss 0.2497994601726532\n",
      "[Training Epoch 6] Batch 1696, Loss 0.25944405794143677\n",
      "[Training Epoch 6] Batch 1697, Loss 0.26145508885383606\n",
      "[Training Epoch 6] Batch 1698, Loss 0.27312996983528137\n",
      "[Training Epoch 6] Batch 1699, Loss 0.26135390996932983\n",
      "[Training Epoch 6] Batch 1700, Loss 0.2421475350856781\n",
      "[Training Epoch 6] Batch 1701, Loss 0.2924448251724243\n",
      "[Training Epoch 6] Batch 1702, Loss 0.2518724799156189\n",
      "[Training Epoch 6] Batch 1703, Loss 0.25927549600601196\n",
      "[Training Epoch 6] Batch 1704, Loss 0.23817458748817444\n",
      "[Training Epoch 6] Batch 1705, Loss 0.26889508962631226\n",
      "[Training Epoch 6] Batch 1706, Loss 0.24869763851165771\n",
      "[Training Epoch 6] Batch 1707, Loss 0.2852480113506317\n",
      "[Training Epoch 6] Batch 1708, Loss 0.2633664608001709\n",
      "[Training Epoch 6] Batch 1709, Loss 0.26080161333084106\n",
      "[Training Epoch 6] Batch 1710, Loss 0.2562667727470398\n",
      "[Training Epoch 6] Batch 1711, Loss 0.24363498389720917\n",
      "[Training Epoch 6] Batch 1712, Loss 0.257632315158844\n",
      "[Training Epoch 6] Batch 1713, Loss 0.26664644479751587\n",
      "[Training Epoch 6] Batch 1714, Loss 0.23876351118087769\n",
      "[Training Epoch 6] Batch 1715, Loss 0.24360884726047516\n",
      "[Training Epoch 6] Batch 1716, Loss 0.27275967597961426\n",
      "[Training Epoch 6] Batch 1717, Loss 0.26572051644325256\n",
      "[Training Epoch 6] Batch 1718, Loss 0.256516695022583\n",
      "[Training Epoch 6] Batch 1719, Loss 0.25552240014076233\n",
      "[Training Epoch 6] Batch 1720, Loss 0.2624081075191498\n",
      "[Training Epoch 6] Batch 1721, Loss 0.26728299260139465\n",
      "[Training Epoch 6] Batch 1722, Loss 0.28345903754234314\n",
      "[Training Epoch 6] Batch 1723, Loss 0.22821477055549622\n",
      "[Training Epoch 6] Batch 1724, Loss 0.2742581367492676\n",
      "[Training Epoch 6] Batch 1725, Loss 0.25179821252822876\n",
      "[Training Epoch 6] Batch 1726, Loss 0.2578429877758026\n",
      "[Training Epoch 6] Batch 1727, Loss 0.2605317533016205\n",
      "[Training Epoch 6] Batch 1728, Loss 0.3065389394760132\n",
      "[Training Epoch 6] Batch 1729, Loss 0.263127863407135\n",
      "[Training Epoch 6] Batch 1730, Loss 0.25373169779777527\n",
      "[Training Epoch 6] Batch 1731, Loss 0.25714778900146484\n",
      "[Training Epoch 6] Batch 1732, Loss 0.2740444540977478\n",
      "[Training Epoch 6] Batch 1733, Loss 0.23059521615505219\n",
      "[Training Epoch 6] Batch 1734, Loss 0.2622019052505493\n",
      "[Training Epoch 6] Batch 1735, Loss 0.22678303718566895\n",
      "[Training Epoch 6] Batch 1736, Loss 0.24796147644519806\n",
      "[Training Epoch 6] Batch 1737, Loss 0.2578345239162445\n",
      "[Training Epoch 6] Batch 1738, Loss 0.26079708337783813\n",
      "[Training Epoch 6] Batch 1739, Loss 0.25955143570899963\n",
      "[Training Epoch 6] Batch 1740, Loss 0.2524625062942505\n",
      "[Training Epoch 6] Batch 1741, Loss 0.24503695964813232\n",
      "[Training Epoch 6] Batch 1742, Loss 0.2707338333129883\n",
      "[Training Epoch 6] Batch 1743, Loss 0.2513348162174225\n",
      "[Training Epoch 6] Batch 1744, Loss 0.27243879437446594\n",
      "[Training Epoch 6] Batch 1745, Loss 0.2594625949859619\n",
      "[Training Epoch 6] Batch 1746, Loss 0.24998226761817932\n",
      "[Training Epoch 6] Batch 1747, Loss 0.2422069013118744\n",
      "[Training Epoch 6] Batch 1748, Loss 0.25511664152145386\n",
      "[Training Epoch 6] Batch 1749, Loss 0.2573055922985077\n",
      "[Training Epoch 6] Batch 1750, Loss 0.22389928996562958\n",
      "[Training Epoch 6] Batch 1751, Loss 0.24691475927829742\n",
      "[Training Epoch 6] Batch 1752, Loss 0.2795473337173462\n",
      "[Training Epoch 6] Batch 1753, Loss 0.2522040605545044\n",
      "[Training Epoch 6] Batch 1754, Loss 0.2885132431983948\n",
      "[Training Epoch 6] Batch 1755, Loss 0.25166213512420654\n",
      "[Training Epoch 6] Batch 1756, Loss 0.24269668757915497\n",
      "[Training Epoch 6] Batch 1757, Loss 0.2376602292060852\n",
      "[Training Epoch 6] Batch 1758, Loss 0.2963418960571289\n",
      "[Training Epoch 6] Batch 1759, Loss 0.2525474429130554\n",
      "[Training Epoch 6] Batch 1760, Loss 0.2656552493572235\n",
      "[Training Epoch 6] Batch 1761, Loss 0.2523699402809143\n",
      "[Training Epoch 6] Batch 1762, Loss 0.28793394565582275\n",
      "[Training Epoch 6] Batch 1763, Loss 0.26805779337882996\n",
      "[Training Epoch 6] Batch 1764, Loss 0.2683258056640625\n",
      "[Training Epoch 6] Batch 1765, Loss 0.23869141936302185\n",
      "[Training Epoch 6] Batch 1766, Loss 0.23511677980422974\n",
      "[Training Epoch 6] Batch 1767, Loss 0.27551984786987305\n",
      "[Training Epoch 6] Batch 1768, Loss 0.2382294088602066\n",
      "[Training Epoch 6] Batch 1769, Loss 0.2838544249534607\n",
      "[Training Epoch 6] Batch 1770, Loss 0.2676980793476105\n",
      "[Training Epoch 6] Batch 1771, Loss 0.2480878233909607\n",
      "[Training Epoch 6] Batch 1772, Loss 0.27859535813331604\n",
      "[Training Epoch 6] Batch 1773, Loss 0.2581357955932617\n",
      "[Training Epoch 6] Batch 1774, Loss 0.2506297528743744\n",
      "[Training Epoch 6] Batch 1775, Loss 0.26533737778663635\n",
      "[Training Epoch 6] Batch 1776, Loss 0.2646600008010864\n",
      "[Training Epoch 6] Batch 1777, Loss 0.25920334458351135\n",
      "[Training Epoch 6] Batch 1778, Loss 0.28233450651168823\n",
      "[Training Epoch 6] Batch 1779, Loss 0.2715514898300171\n",
      "[Training Epoch 6] Batch 1780, Loss 0.24037307500839233\n",
      "[Training Epoch 6] Batch 1781, Loss 0.22930362820625305\n",
      "[Training Epoch 6] Batch 1782, Loss 0.25496625900268555\n",
      "[Training Epoch 6] Batch 1783, Loss 0.2723250985145569\n",
      "[Training Epoch 6] Batch 1784, Loss 0.27136388421058655\n",
      "[Training Epoch 6] Batch 1785, Loss 0.2706787586212158\n",
      "[Training Epoch 6] Batch 1786, Loss 0.2609134018421173\n",
      "[Training Epoch 6] Batch 1787, Loss 0.2569121718406677\n",
      "[Training Epoch 6] Batch 1788, Loss 0.2568076252937317\n",
      "[Training Epoch 6] Batch 1789, Loss 0.24564357101917267\n",
      "[Training Epoch 6] Batch 1790, Loss 0.2446948140859604\n",
      "[Training Epoch 6] Batch 1791, Loss 0.2879539728164673\n",
      "[Training Epoch 6] Batch 1792, Loss 0.2791317105293274\n",
      "[Training Epoch 6] Batch 1793, Loss 0.24378812313079834\n",
      "[Training Epoch 6] Batch 1794, Loss 0.2445226013660431\n",
      "[Training Epoch 6] Batch 1795, Loss 0.27019935846328735\n",
      "[Training Epoch 6] Batch 1796, Loss 0.2721341848373413\n",
      "[Training Epoch 6] Batch 1797, Loss 0.27573341131210327\n",
      "[Training Epoch 6] Batch 1798, Loss 0.23427246510982513\n",
      "[Training Epoch 6] Batch 1799, Loss 0.2565358877182007\n",
      "[Training Epoch 6] Batch 1800, Loss 0.27935677766799927\n",
      "[Training Epoch 6] Batch 1801, Loss 0.28834420442581177\n",
      "[Training Epoch 6] Batch 1802, Loss 0.273909330368042\n",
      "[Training Epoch 6] Batch 1803, Loss 0.28072670102119446\n",
      "[Training Epoch 6] Batch 1804, Loss 0.28669261932373047\n",
      "[Training Epoch 6] Batch 1805, Loss 0.2731098234653473\n",
      "[Training Epoch 6] Batch 1806, Loss 0.2560056447982788\n",
      "[Training Epoch 6] Batch 1807, Loss 0.2375737726688385\n",
      "[Training Epoch 6] Batch 1808, Loss 0.24350540339946747\n",
      "[Training Epoch 6] Batch 1809, Loss 0.270746111869812\n",
      "[Training Epoch 6] Batch 1810, Loss 0.26621580123901367\n",
      "[Training Epoch 6] Batch 1811, Loss 0.25071144104003906\n",
      "[Training Epoch 6] Batch 1812, Loss 0.250478595495224\n",
      "[Training Epoch 6] Batch 1813, Loss 0.2613939642906189\n",
      "[Training Epoch 6] Batch 1814, Loss 0.2383934110403061\n",
      "[Training Epoch 6] Batch 1815, Loss 0.26340124011039734\n",
      "[Training Epoch 6] Batch 1816, Loss 0.2735661268234253\n",
      "[Training Epoch 6] Batch 1817, Loss 0.2873709797859192\n",
      "[Training Epoch 6] Batch 1818, Loss 0.2532199025154114\n",
      "[Training Epoch 6] Batch 1819, Loss 0.27632370591163635\n",
      "[Training Epoch 6] Batch 1820, Loss 0.28391969203948975\n",
      "[Training Epoch 6] Batch 1821, Loss 0.2482382208108902\n",
      "[Training Epoch 6] Batch 1822, Loss 0.23983536660671234\n",
      "[Training Epoch 6] Batch 1823, Loss 0.28606680035591125\n",
      "[Training Epoch 6] Batch 1824, Loss 0.2735289931297302\n",
      "[Training Epoch 6] Batch 1825, Loss 0.29149824380874634\n",
      "[Training Epoch 6] Batch 1826, Loss 0.25108611583709717\n",
      "[Training Epoch 6] Batch 1827, Loss 0.26684242486953735\n",
      "[Training Epoch 6] Batch 1828, Loss 0.2549089789390564\n",
      "[Training Epoch 6] Batch 1829, Loss 0.25982093811035156\n",
      "[Training Epoch 6] Batch 1830, Loss 0.26670271158218384\n",
      "[Training Epoch 6] Batch 1831, Loss 0.2532854974269867\n",
      "[Training Epoch 6] Batch 1832, Loss 0.28536877036094666\n",
      "[Training Epoch 6] Batch 1833, Loss 0.2504670023918152\n",
      "[Training Epoch 6] Batch 1834, Loss 0.24267998337745667\n",
      "[Training Epoch 6] Batch 1835, Loss 0.2565452754497528\n",
      "[Training Epoch 6] Batch 1836, Loss 0.26195043325424194\n",
      "[Training Epoch 6] Batch 1837, Loss 0.2833177149295807\n",
      "[Training Epoch 6] Batch 1838, Loss 0.22546568512916565\n",
      "[Training Epoch 6] Batch 1839, Loss 0.2957608103752136\n",
      "[Training Epoch 6] Batch 1840, Loss 0.2406613826751709\n",
      "[Training Epoch 6] Batch 1841, Loss 0.23834924399852753\n",
      "[Training Epoch 6] Batch 1842, Loss 0.24731263518333435\n",
      "[Training Epoch 6] Batch 1843, Loss 0.2772875428199768\n",
      "[Training Epoch 6] Batch 1844, Loss 0.27531731128692627\n",
      "[Training Epoch 6] Batch 1845, Loss 0.2347511649131775\n",
      "[Training Epoch 6] Batch 1846, Loss 0.2607446312904358\n",
      "[Training Epoch 6] Batch 1847, Loss 0.2604970932006836\n",
      "[Training Epoch 6] Batch 1848, Loss 0.2559353709220886\n",
      "[Training Epoch 6] Batch 1849, Loss 0.25784236192703247\n",
      "[Training Epoch 6] Batch 1850, Loss 0.23286446928977966\n",
      "[Training Epoch 6] Batch 1851, Loss 0.25109153985977173\n",
      "[Training Epoch 6] Batch 1852, Loss 0.2518218159675598\n",
      "[Training Epoch 6] Batch 1853, Loss 0.2729259729385376\n",
      "[Training Epoch 6] Batch 1854, Loss 0.23378321528434753\n",
      "[Training Epoch 6] Batch 1855, Loss 0.2823113799095154\n",
      "[Training Epoch 6] Batch 1856, Loss 0.24906200170516968\n",
      "[Training Epoch 6] Batch 1857, Loss 0.2865011990070343\n",
      "[Training Epoch 6] Batch 1858, Loss 0.2631188929080963\n",
      "[Training Epoch 6] Batch 1859, Loss 0.2657962441444397\n",
      "[Training Epoch 6] Batch 1860, Loss 0.25554752349853516\n",
      "[Training Epoch 6] Batch 1861, Loss 0.27135658264160156\n",
      "[Training Epoch 6] Batch 1862, Loss 0.2414179891347885\n",
      "[Training Epoch 6] Batch 1863, Loss 0.26513034105300903\n",
      "[Training Epoch 6] Batch 1864, Loss 0.25868672132492065\n",
      "[Training Epoch 6] Batch 1865, Loss 0.2812904417514801\n",
      "[Training Epoch 6] Batch 1866, Loss 0.2577155828475952\n",
      "[Training Epoch 6] Batch 1867, Loss 0.2779783308506012\n",
      "[Training Epoch 6] Batch 1868, Loss 0.28258275985717773\n",
      "[Training Epoch 6] Batch 1869, Loss 0.2740170657634735\n",
      "[Training Epoch 6] Batch 1870, Loss 0.2537379264831543\n",
      "[Training Epoch 6] Batch 1871, Loss 0.2678343951702118\n",
      "[Training Epoch 6] Batch 1872, Loss 0.23348352313041687\n",
      "[Training Epoch 6] Batch 1873, Loss 0.284199595451355\n",
      "[Training Epoch 6] Batch 1874, Loss 0.2567654252052307\n",
      "[Training Epoch 6] Batch 1875, Loss 0.262431263923645\n",
      "[Training Epoch 6] Batch 1876, Loss 0.25228095054626465\n",
      "[Training Epoch 6] Batch 1877, Loss 0.26793813705444336\n",
      "[Training Epoch 6] Batch 1878, Loss 0.25808849930763245\n",
      "[Training Epoch 6] Batch 1879, Loss 0.27311980724334717\n",
      "[Training Epoch 6] Batch 1880, Loss 0.25234219431877136\n",
      "[Training Epoch 6] Batch 1881, Loss 0.25492164492607117\n",
      "[Training Epoch 6] Batch 1882, Loss 0.25148823857307434\n",
      "[Training Epoch 6] Batch 1883, Loss 0.2602592706680298\n",
      "[Training Epoch 6] Batch 1884, Loss 0.2254597693681717\n",
      "[Training Epoch 6] Batch 1885, Loss 0.2580680549144745\n",
      "[Training Epoch 6] Batch 1886, Loss 0.2760036587715149\n",
      "[Training Epoch 6] Batch 1887, Loss 0.28240975737571716\n",
      "[Training Epoch 6] Batch 1888, Loss 0.26123252511024475\n",
      "[Training Epoch 6] Batch 1889, Loss 0.2430625557899475\n",
      "[Training Epoch 6] Batch 1890, Loss 0.26433783769607544\n",
      "[Training Epoch 6] Batch 1891, Loss 0.259498655796051\n",
      "[Training Epoch 6] Batch 1892, Loss 0.29477888345718384\n",
      "[Training Epoch 6] Batch 1893, Loss 0.22515921294689178\n",
      "[Training Epoch 6] Batch 1894, Loss 0.24011953175067902\n",
      "[Training Epoch 6] Batch 1895, Loss 0.28758516907691956\n",
      "[Training Epoch 6] Batch 1896, Loss 0.2643435597419739\n",
      "[Training Epoch 6] Batch 1897, Loss 0.2684973478317261\n",
      "[Training Epoch 6] Batch 1898, Loss 0.28815051913261414\n",
      "[Training Epoch 6] Batch 1899, Loss 0.238050177693367\n",
      "[Training Epoch 6] Batch 1900, Loss 0.23958654701709747\n",
      "[Training Epoch 6] Batch 1901, Loss 0.24901607632637024\n",
      "[Training Epoch 6] Batch 1902, Loss 0.2799072563648224\n",
      "[Training Epoch 6] Batch 1903, Loss 0.2745600938796997\n",
      "[Training Epoch 6] Batch 1904, Loss 0.2791741192340851\n",
      "[Training Epoch 6] Batch 1905, Loss 0.2878689169883728\n",
      "[Training Epoch 6] Batch 1906, Loss 0.24310952425003052\n",
      "[Training Epoch 6] Batch 1907, Loss 0.27260622382164\n",
      "[Training Epoch 6] Batch 1908, Loss 0.2768848240375519\n",
      "[Training Epoch 6] Batch 1909, Loss 0.25064972043037415\n",
      "[Training Epoch 6] Batch 1910, Loss 0.2643342614173889\n",
      "[Training Epoch 6] Batch 1911, Loss 0.27887895703315735\n",
      "[Training Epoch 6] Batch 1912, Loss 0.24260877072811127\n",
      "[Training Epoch 6] Batch 1913, Loss 0.24195267260074615\n",
      "[Training Epoch 6] Batch 1914, Loss 0.2635955810546875\n",
      "[Training Epoch 6] Batch 1915, Loss 0.30798453092575073\n",
      "[Training Epoch 6] Batch 1916, Loss 0.24230539798736572\n",
      "[Training Epoch 6] Batch 1917, Loss 0.24792656302452087\n",
      "[Training Epoch 6] Batch 1918, Loss 0.25214192271232605\n",
      "[Training Epoch 6] Batch 1919, Loss 0.2560683488845825\n",
      "[Training Epoch 6] Batch 1920, Loss 0.2554212212562561\n",
      "[Training Epoch 6] Batch 1921, Loss 0.25033897161483765\n",
      "[Training Epoch 6] Batch 1922, Loss 0.2625567615032196\n",
      "[Training Epoch 6] Batch 1923, Loss 0.23973657190799713\n",
      "[Training Epoch 6] Batch 1924, Loss 0.2717464566230774\n",
      "[Training Epoch 6] Batch 1925, Loss 0.2501542866230011\n",
      "[Training Epoch 6] Batch 1926, Loss 0.2536391317844391\n",
      "[Training Epoch 6] Batch 1927, Loss 0.28373682498931885\n",
      "[Training Epoch 6] Batch 1928, Loss 0.23695611953735352\n",
      "[Training Epoch 6] Batch 1929, Loss 0.2697126269340515\n",
      "[Training Epoch 6] Batch 1930, Loss 0.26185035705566406\n",
      "[Training Epoch 6] Batch 1931, Loss 0.2729966640472412\n",
      "[Training Epoch 6] Batch 1932, Loss 0.2522449493408203\n",
      "[Training Epoch 6] Batch 1933, Loss 0.24081632494926453\n",
      "[Training Epoch 6] Batch 1934, Loss 0.2329627275466919\n",
      "[Training Epoch 6] Batch 1935, Loss 0.26104721426963806\n",
      "[Training Epoch 6] Batch 1936, Loss 0.26188114285469055\n",
      "[Training Epoch 6] Batch 1937, Loss 0.22684833407402039\n",
      "[Training Epoch 6] Batch 1938, Loss 0.2568112313747406\n",
      "[Training Epoch 6] Batch 1939, Loss 0.25028616189956665\n",
      "[Training Epoch 6] Batch 1940, Loss 0.23328569531440735\n",
      "[Training Epoch 6] Batch 1941, Loss 0.24541355669498444\n",
      "[Training Epoch 6] Batch 1942, Loss 0.2548028230667114\n",
      "[Training Epoch 6] Batch 1943, Loss 0.23788586258888245\n",
      "[Training Epoch 6] Batch 1944, Loss 0.25981253385543823\n",
      "[Training Epoch 6] Batch 1945, Loss 0.25212711095809937\n",
      "[Training Epoch 6] Batch 1946, Loss 0.25770699977874756\n",
      "[Training Epoch 6] Batch 1947, Loss 0.2424064576625824\n",
      "[Training Epoch 6] Batch 1948, Loss 0.2689099609851837\n",
      "[Training Epoch 6] Batch 1949, Loss 0.27002373337745667\n",
      "[Training Epoch 6] Batch 1950, Loss 0.2594112157821655\n",
      "[Training Epoch 6] Batch 1951, Loss 0.24282467365264893\n",
      "[Training Epoch 6] Batch 1952, Loss 0.2504783868789673\n",
      "[Training Epoch 6] Batch 1953, Loss 0.2714661955833435\n",
      "[Training Epoch 6] Batch 1954, Loss 0.2422662079334259\n",
      "[Training Epoch 6] Batch 1955, Loss 0.25157687067985535\n",
      "[Training Epoch 6] Batch 1956, Loss 0.25234758853912354\n",
      "[Training Epoch 6] Batch 1957, Loss 0.24566775560379028\n",
      "[Training Epoch 6] Batch 1958, Loss 0.2984568774700165\n",
      "[Training Epoch 6] Batch 1959, Loss 0.2438504695892334\n",
      "[Training Epoch 6] Batch 1960, Loss 0.26196378469467163\n",
      "[Training Epoch 6] Batch 1961, Loss 0.26303184032440186\n",
      "[Training Epoch 6] Batch 1962, Loss 0.2637086510658264\n",
      "[Training Epoch 6] Batch 1963, Loss 0.3026588559150696\n",
      "[Training Epoch 6] Batch 1964, Loss 0.2590498924255371\n",
      "[Training Epoch 6] Batch 1965, Loss 0.2842828035354614\n",
      "[Training Epoch 6] Batch 1966, Loss 0.2380291223526001\n",
      "[Training Epoch 6] Batch 1967, Loss 0.24783936142921448\n",
      "[Training Epoch 6] Batch 1968, Loss 0.26458740234375\n",
      "[Training Epoch 6] Batch 1969, Loss 0.2501034736633301\n",
      "[Training Epoch 6] Batch 1970, Loss 0.2695057988166809\n",
      "[Training Epoch 6] Batch 1971, Loss 0.24844595789909363\n",
      "[Training Epoch 6] Batch 1972, Loss 0.2833890914916992\n",
      "[Training Epoch 6] Batch 1973, Loss 0.23390674591064453\n",
      "[Training Epoch 6] Batch 1974, Loss 0.2385905683040619\n",
      "[Training Epoch 6] Batch 1975, Loss 0.2700589895248413\n",
      "[Training Epoch 6] Batch 1976, Loss 0.26487791538238525\n",
      "[Training Epoch 6] Batch 1977, Loss 0.29614508152008057\n",
      "[Training Epoch 6] Batch 1978, Loss 0.2661052346229553\n",
      "[Training Epoch 6] Batch 1979, Loss 0.2622361481189728\n",
      "[Training Epoch 6] Batch 1980, Loss 0.25616535544395447\n",
      "[Training Epoch 6] Batch 1981, Loss 0.26561862230300903\n",
      "[Training Epoch 6] Batch 1982, Loss 0.2543019652366638\n",
      "[Training Epoch 6] Batch 1983, Loss 0.25621503591537476\n",
      "[Training Epoch 6] Batch 1984, Loss 0.2590942978858948\n",
      "[Training Epoch 6] Batch 1985, Loss 0.27665919065475464\n",
      "[Training Epoch 6] Batch 1986, Loss 0.24519100785255432\n",
      "[Training Epoch 6] Batch 1987, Loss 0.2553815245628357\n",
      "[Training Epoch 6] Batch 1988, Loss 0.2546537220478058\n",
      "[Training Epoch 6] Batch 1989, Loss 0.2511132061481476\n",
      "[Training Epoch 6] Batch 1990, Loss 0.2653391659259796\n",
      "[Training Epoch 6] Batch 1991, Loss 0.2770119905471802\n",
      "[Training Epoch 6] Batch 1992, Loss 0.2563198208808899\n",
      "[Training Epoch 6] Batch 1993, Loss 0.2752028703689575\n",
      "[Training Epoch 6] Batch 1994, Loss 0.2539243996143341\n",
      "[Training Epoch 6] Batch 1995, Loss 0.2676279842853546\n",
      "[Training Epoch 6] Batch 1996, Loss 0.256997674703598\n",
      "[Training Epoch 6] Batch 1997, Loss 0.24821242690086365\n",
      "[Training Epoch 6] Batch 1998, Loss 0.24813620746135712\n",
      "[Training Epoch 6] Batch 1999, Loss 0.29917067289352417\n",
      "[Training Epoch 6] Batch 2000, Loss 0.2741726338863373\n",
      "[Training Epoch 6] Batch 2001, Loss 0.24958373606204987\n",
      "[Training Epoch 6] Batch 2002, Loss 0.26025643944740295\n",
      "[Training Epoch 6] Batch 2003, Loss 0.2266126424074173\n",
      "[Training Epoch 6] Batch 2004, Loss 0.2752823829650879\n",
      "[Training Epoch 6] Batch 2005, Loss 0.2555385231971741\n",
      "[Training Epoch 6] Batch 2006, Loss 0.26419922709465027\n",
      "[Training Epoch 6] Batch 2007, Loss 0.24437262117862701\n",
      "[Training Epoch 6] Batch 2008, Loss 0.2476920634508133\n",
      "[Training Epoch 6] Batch 2009, Loss 0.23776869475841522\n",
      "[Training Epoch 6] Batch 2010, Loss 0.27117225527763367\n",
      "[Training Epoch 6] Batch 2011, Loss 0.29026028513908386\n",
      "[Training Epoch 6] Batch 2012, Loss 0.25723400712013245\n",
      "[Training Epoch 6] Batch 2013, Loss 0.24185429513454437\n",
      "[Training Epoch 6] Batch 2014, Loss 0.24863667786121368\n",
      "[Training Epoch 6] Batch 2015, Loss 0.2441123127937317\n",
      "[Training Epoch 6] Batch 2016, Loss 0.2915002703666687\n",
      "[Training Epoch 6] Batch 2017, Loss 0.25776609778404236\n",
      "[Training Epoch 6] Batch 2018, Loss 0.23192667961120605\n",
      "[Training Epoch 6] Batch 2019, Loss 0.22461287677288055\n",
      "[Training Epoch 6] Batch 2020, Loss 0.2595028877258301\n",
      "[Training Epoch 6] Batch 2021, Loss 0.2770697772502899\n",
      "[Training Epoch 6] Batch 2022, Loss 0.25440314412117004\n",
      "[Training Epoch 6] Batch 2023, Loss 0.24655430018901825\n",
      "[Training Epoch 6] Batch 2024, Loss 0.26883912086486816\n",
      "[Training Epoch 6] Batch 2025, Loss 0.25760334730148315\n",
      "[Training Epoch 6] Batch 2026, Loss 0.23875130712985992\n",
      "[Training Epoch 6] Batch 2027, Loss 0.2902080714702606\n",
      "[Training Epoch 6] Batch 2028, Loss 0.2905406653881073\n",
      "[Training Epoch 6] Batch 2029, Loss 0.25626707077026367\n",
      "[Training Epoch 6] Batch 2030, Loss 0.2697853744029999\n",
      "[Training Epoch 6] Batch 2031, Loss 0.27268069982528687\n",
      "[Training Epoch 6] Batch 2032, Loss 0.27207624912261963\n",
      "[Training Epoch 6] Batch 2033, Loss 0.2588517665863037\n",
      "[Training Epoch 6] Batch 2034, Loss 0.21324077248573303\n",
      "[Training Epoch 6] Batch 2035, Loss 0.25912749767303467\n",
      "[Training Epoch 6] Batch 2036, Loss 0.2746411859989166\n",
      "[Training Epoch 6] Batch 2037, Loss 0.27936604619026184\n",
      "[Training Epoch 6] Batch 2038, Loss 0.2477697730064392\n",
      "[Training Epoch 6] Batch 2039, Loss 0.2689632773399353\n",
      "[Training Epoch 6] Batch 2040, Loss 0.2857491970062256\n",
      "[Training Epoch 6] Batch 2041, Loss 0.24981515109539032\n",
      "[Training Epoch 6] Batch 2042, Loss 0.27133482694625854\n",
      "[Training Epoch 6] Batch 2043, Loss 0.2591804265975952\n",
      "[Training Epoch 6] Batch 2044, Loss 0.2515559196472168\n",
      "[Training Epoch 6] Batch 2045, Loss 0.23923280835151672\n",
      "[Training Epoch 6] Batch 2046, Loss 0.25412681698799133\n",
      "[Training Epoch 6] Batch 2047, Loss 0.2573911249637604\n",
      "[Training Epoch 6] Batch 2048, Loss 0.26932716369628906\n",
      "[Training Epoch 6] Batch 2049, Loss 0.2571437954902649\n",
      "[Training Epoch 6] Batch 2050, Loss 0.24739670753479004\n",
      "[Training Epoch 6] Batch 2051, Loss 0.2890801429748535\n",
      "[Training Epoch 6] Batch 2052, Loss 0.2361544966697693\n",
      "[Training Epoch 6] Batch 2053, Loss 0.2770768404006958\n",
      "[Training Epoch 6] Batch 2054, Loss 0.22893008589744568\n",
      "[Training Epoch 6] Batch 2055, Loss 0.25384625792503357\n",
      "[Training Epoch 6] Batch 2056, Loss 0.24269263446331024\n",
      "[Training Epoch 6] Batch 2057, Loss 0.2534666657447815\n",
      "[Training Epoch 6] Batch 2058, Loss 0.24321281909942627\n",
      "[Training Epoch 6] Batch 2059, Loss 0.28527987003326416\n",
      "[Training Epoch 6] Batch 2060, Loss 0.2683963179588318\n",
      "[Training Epoch 6] Batch 2061, Loss 0.277499794960022\n",
      "[Training Epoch 6] Batch 2062, Loss 0.22488248348236084\n",
      "[Training Epoch 6] Batch 2063, Loss 0.2627916634082794\n",
      "[Training Epoch 6] Batch 2064, Loss 0.2466174066066742\n",
      "[Training Epoch 6] Batch 2065, Loss 0.27894163131713867\n",
      "[Training Epoch 6] Batch 2066, Loss 0.24159318208694458\n",
      "[Training Epoch 6] Batch 2067, Loss 0.2606894373893738\n",
      "[Training Epoch 6] Batch 2068, Loss 0.2516579329967499\n",
      "[Training Epoch 6] Batch 2069, Loss 0.26376450061798096\n",
      "[Training Epoch 6] Batch 2070, Loss 0.2676715850830078\n",
      "[Training Epoch 6] Batch 2071, Loss 0.2645018994808197\n",
      "[Training Epoch 6] Batch 2072, Loss 0.2696121335029602\n",
      "[Training Epoch 6] Batch 2073, Loss 0.27656957507133484\n",
      "[Training Epoch 6] Batch 2074, Loss 0.2595321238040924\n",
      "[Training Epoch 6] Batch 2075, Loss 0.2637200355529785\n",
      "[Training Epoch 6] Batch 2076, Loss 0.25459885597229004\n",
      "[Training Epoch 6] Batch 2077, Loss 0.2543001174926758\n",
      "[Training Epoch 6] Batch 2078, Loss 0.2583867907524109\n",
      "[Training Epoch 6] Batch 2079, Loss 0.2589743137359619\n",
      "[Training Epoch 6] Batch 2080, Loss 0.2581464648246765\n",
      "[Training Epoch 6] Batch 2081, Loss 0.24124759435653687\n",
      "[Training Epoch 6] Batch 2082, Loss 0.28001320362091064\n",
      "[Training Epoch 6] Batch 2083, Loss 0.264741450548172\n",
      "[Training Epoch 6] Batch 2084, Loss 0.24422726035118103\n",
      "[Training Epoch 6] Batch 2085, Loss 0.2824404239654541\n",
      "[Training Epoch 6] Batch 2086, Loss 0.25135794281959534\n",
      "[Training Epoch 6] Batch 2087, Loss 0.26464468240737915\n",
      "[Training Epoch 6] Batch 2088, Loss 0.2620786726474762\n",
      "[Training Epoch 6] Batch 2089, Loss 0.25432077050209045\n",
      "[Training Epoch 6] Batch 2090, Loss 0.25316545367240906\n",
      "[Training Epoch 6] Batch 2091, Loss 0.26574280858039856\n",
      "[Training Epoch 6] Batch 2092, Loss 0.2576257288455963\n",
      "[Training Epoch 6] Batch 2093, Loss 0.24266786873340607\n",
      "[Training Epoch 6] Batch 2094, Loss 0.23650941252708435\n",
      "[Training Epoch 6] Batch 2095, Loss 0.24811527132987976\n",
      "[Training Epoch 6] Batch 2096, Loss 0.2562956213951111\n",
      "[Training Epoch 6] Batch 2097, Loss 0.25590968132019043\n",
      "[Training Epoch 6] Batch 2098, Loss 0.2658897638320923\n",
      "[Training Epoch 6] Batch 2099, Loss 0.246702641248703\n",
      "[Training Epoch 6] Batch 2100, Loss 0.27615535259246826\n",
      "[Training Epoch 6] Batch 2101, Loss 0.2597793638706207\n",
      "[Training Epoch 6] Batch 2102, Loss 0.24386976659297943\n",
      "[Training Epoch 6] Batch 2103, Loss 0.26422592997550964\n",
      "[Training Epoch 6] Batch 2104, Loss 0.2776802182197571\n",
      "[Training Epoch 6] Batch 2105, Loss 0.2408667355775833\n",
      "[Training Epoch 6] Batch 2106, Loss 0.2535117268562317\n",
      "[Training Epoch 6] Batch 2107, Loss 0.222763329744339\n",
      "[Training Epoch 6] Batch 2108, Loss 0.26009517908096313\n",
      "[Training Epoch 6] Batch 2109, Loss 0.26819154620170593\n",
      "[Training Epoch 6] Batch 2110, Loss 0.25739550590515137\n",
      "[Training Epoch 6] Batch 2111, Loss 0.27256304025650024\n",
      "[Training Epoch 6] Batch 2112, Loss 0.26226887106895447\n",
      "[Training Epoch 6] Batch 2113, Loss 0.26593708992004395\n",
      "[Training Epoch 6] Batch 2114, Loss 0.25318458676338196\n",
      "[Training Epoch 6] Batch 2115, Loss 0.2564324140548706\n",
      "[Training Epoch 6] Batch 2116, Loss 0.2524696886539459\n",
      "[Training Epoch 6] Batch 2117, Loss 0.25518327951431274\n",
      "[Training Epoch 6] Batch 2118, Loss 0.2385609894990921\n",
      "[Training Epoch 6] Batch 2119, Loss 0.26267945766448975\n",
      "[Training Epoch 6] Batch 2120, Loss 0.26012691855430603\n",
      "[Training Epoch 6] Batch 2121, Loss 0.24567323923110962\n",
      "[Training Epoch 6] Batch 2122, Loss 0.26441118121147156\n",
      "[Training Epoch 6] Batch 2123, Loss 0.273204505443573\n",
      "[Training Epoch 6] Batch 2124, Loss 0.27977249026298523\n",
      "[Training Epoch 6] Batch 2125, Loss 0.283027708530426\n",
      "[Training Epoch 6] Batch 2126, Loss 0.22872868180274963\n",
      "[Training Epoch 6] Batch 2127, Loss 0.27449750900268555\n",
      "[Training Epoch 6] Batch 2128, Loss 0.24692991375923157\n",
      "[Training Epoch 6] Batch 2129, Loss 0.24262726306915283\n",
      "[Training Epoch 6] Batch 2130, Loss 0.2742099165916443\n",
      "[Training Epoch 6] Batch 2131, Loss 0.2535950839519501\n",
      "[Training Epoch 6] Batch 2132, Loss 0.2624906301498413\n",
      "[Training Epoch 6] Batch 2133, Loss 0.2735247313976288\n",
      "[Training Epoch 6] Batch 2134, Loss 0.28263068199157715\n",
      "[Training Epoch 6] Batch 2135, Loss 0.24951407313346863\n",
      "[Training Epoch 6] Batch 2136, Loss 0.2756217420101166\n",
      "[Training Epoch 6] Batch 2137, Loss 0.24054042994976044\n",
      "[Training Epoch 6] Batch 2138, Loss 0.2430184930562973\n",
      "[Training Epoch 6] Batch 2139, Loss 0.22023239731788635\n",
      "[Training Epoch 6] Batch 2140, Loss 0.25035884976387024\n",
      "[Training Epoch 6] Batch 2141, Loss 0.23295438289642334\n",
      "[Training Epoch 6] Batch 2142, Loss 0.24443167448043823\n",
      "[Training Epoch 6] Batch 2143, Loss 0.24877551198005676\n",
      "[Training Epoch 6] Batch 2144, Loss 0.24151916801929474\n",
      "[Training Epoch 6] Batch 2145, Loss 0.2689828872680664\n",
      "[Training Epoch 6] Batch 2146, Loss 0.22968442738056183\n",
      "[Training Epoch 6] Batch 2147, Loss 0.2414184808731079\n",
      "[Training Epoch 6] Batch 2148, Loss 0.24283595383167267\n",
      "[Training Epoch 6] Batch 2149, Loss 0.2510959506034851\n",
      "[Training Epoch 6] Batch 2150, Loss 0.2636452615261078\n",
      "[Training Epoch 6] Batch 2151, Loss 0.2693958282470703\n",
      "[Training Epoch 6] Batch 2152, Loss 0.2642585039138794\n",
      "[Training Epoch 6] Batch 2153, Loss 0.2853361964225769\n",
      "[Training Epoch 6] Batch 2154, Loss 0.20928724110126495\n",
      "[Training Epoch 6] Batch 2155, Loss 0.2528344392776489\n",
      "[Training Epoch 6] Batch 2156, Loss 0.24910326302051544\n",
      "[Training Epoch 6] Batch 2157, Loss 0.267153799533844\n",
      "[Training Epoch 6] Batch 2158, Loss 0.2582772970199585\n",
      "[Training Epoch 6] Batch 2159, Loss 0.2600064277648926\n",
      "[Training Epoch 6] Batch 2160, Loss 0.2871663570404053\n",
      "[Training Epoch 6] Batch 2161, Loss 0.2540174424648285\n",
      "[Training Epoch 6] Batch 2162, Loss 0.26537132263183594\n",
      "[Training Epoch 6] Batch 2163, Loss 0.247166246175766\n",
      "[Training Epoch 6] Batch 2164, Loss 0.21914514899253845\n",
      "[Training Epoch 6] Batch 2165, Loss 0.2615925967693329\n",
      "[Training Epoch 6] Batch 2166, Loss 0.2590399980545044\n",
      "[Training Epoch 6] Batch 2167, Loss 0.2570975720882416\n",
      "[Training Epoch 6] Batch 2168, Loss 0.27295175194740295\n",
      "[Training Epoch 6] Batch 2169, Loss 0.24529209733009338\n",
      "[Training Epoch 6] Batch 2170, Loss 0.2715464234352112\n",
      "[Training Epoch 6] Batch 2171, Loss 0.2604289948940277\n",
      "[Training Epoch 6] Batch 2172, Loss 0.2513725757598877\n",
      "[Training Epoch 6] Batch 2173, Loss 0.24013477563858032\n",
      "[Training Epoch 6] Batch 2174, Loss 0.2739972472190857\n",
      "[Training Epoch 6] Batch 2175, Loss 0.24249258637428284\n",
      "[Training Epoch 6] Batch 2176, Loss 0.2849430739879608\n",
      "[Training Epoch 6] Batch 2177, Loss 0.27749937772750854\n",
      "[Training Epoch 6] Batch 2178, Loss 0.23757076263427734\n",
      "[Training Epoch 6] Batch 2179, Loss 0.2513309121131897\n",
      "[Training Epoch 6] Batch 2180, Loss 0.2800194025039673\n",
      "[Training Epoch 6] Batch 2181, Loss 0.25013190507888794\n",
      "[Training Epoch 6] Batch 2182, Loss 0.24026556313037872\n",
      "[Training Epoch 6] Batch 2183, Loss 0.2616084814071655\n",
      "[Training Epoch 6] Batch 2184, Loss 0.2581462860107422\n",
      "[Training Epoch 6] Batch 2185, Loss 0.26682400703430176\n",
      "[Training Epoch 6] Batch 2186, Loss 0.23749086260795593\n",
      "[Training Epoch 6] Batch 2187, Loss 0.24494999647140503\n",
      "[Training Epoch 6] Batch 2188, Loss 0.25060996413230896\n",
      "[Training Epoch 6] Batch 2189, Loss 0.23125144839286804\n",
      "[Training Epoch 6] Batch 2190, Loss 0.27901309728622437\n",
      "[Training Epoch 6] Batch 2191, Loss 0.23393262922763824\n",
      "[Training Epoch 6] Batch 2192, Loss 0.2563377618789673\n",
      "[Training Epoch 6] Batch 2193, Loss 0.2695305049419403\n",
      "[Training Epoch 6] Batch 2194, Loss 0.2428315281867981\n",
      "[Training Epoch 6] Batch 2195, Loss 0.2568056583404541\n",
      "[Training Epoch 6] Batch 2196, Loss 0.24943022429943085\n",
      "[Training Epoch 6] Batch 2197, Loss 0.24169227480888367\n",
      "[Training Epoch 6] Batch 2198, Loss 0.2629047632217407\n",
      "[Training Epoch 6] Batch 2199, Loss 0.23467384278774261\n",
      "[Training Epoch 6] Batch 2200, Loss 0.26715153455734253\n",
      "[Training Epoch 6] Batch 2201, Loss 0.2785125970840454\n",
      "[Training Epoch 6] Batch 2202, Loss 0.245279461145401\n",
      "[Training Epoch 6] Batch 2203, Loss 0.2488255798816681\n",
      "[Training Epoch 6] Batch 2204, Loss 0.2571118175983429\n",
      "[Training Epoch 6] Batch 2205, Loss 0.24596033990383148\n",
      "[Training Epoch 6] Batch 2206, Loss 0.25274550914764404\n",
      "[Training Epoch 6] Batch 2207, Loss 0.24914023280143738\n",
      "[Training Epoch 6] Batch 2208, Loss 0.2529903054237366\n",
      "[Training Epoch 6] Batch 2209, Loss 0.27219095826148987\n",
      "[Training Epoch 6] Batch 2210, Loss 0.2834119200706482\n",
      "[Training Epoch 6] Batch 2211, Loss 0.2703004479408264\n",
      "[Training Epoch 6] Batch 2212, Loss 0.24323588609695435\n",
      "[Training Epoch 6] Batch 2213, Loss 0.2731073498725891\n",
      "[Training Epoch 6] Batch 2214, Loss 0.25114038586616516\n",
      "[Training Epoch 6] Batch 2215, Loss 0.23096561431884766\n",
      "[Training Epoch 6] Batch 2216, Loss 0.2788360118865967\n",
      "[Training Epoch 6] Batch 2217, Loss 0.24815498292446136\n",
      "[Training Epoch 6] Batch 2218, Loss 0.27710044384002686\n",
      "[Training Epoch 6] Batch 2219, Loss 0.2572990953922272\n",
      "[Training Epoch 6] Batch 2220, Loss 0.23075824975967407\n",
      "[Training Epoch 6] Batch 2221, Loss 0.24282097816467285\n",
      "[Training Epoch 6] Batch 2222, Loss 0.2544735074043274\n",
      "[Training Epoch 6] Batch 2223, Loss 0.23802998661994934\n",
      "[Training Epoch 6] Batch 2224, Loss 0.24803787469863892\n",
      "[Training Epoch 6] Batch 2225, Loss 0.24662058055400848\n",
      "[Training Epoch 6] Batch 2226, Loss 0.2520103454589844\n",
      "[Training Epoch 6] Batch 2227, Loss 0.27621108293533325\n",
      "[Training Epoch 6] Batch 2228, Loss 0.22808419167995453\n",
      "[Training Epoch 6] Batch 2229, Loss 0.2715124487876892\n",
      "[Training Epoch 6] Batch 2230, Loss 0.24670356512069702\n",
      "[Training Epoch 6] Batch 2231, Loss 0.2773698568344116\n",
      "[Training Epoch 6] Batch 2232, Loss 0.24696601927280426\n",
      "[Training Epoch 6] Batch 2233, Loss 0.2512311339378357\n",
      "[Training Epoch 6] Batch 2234, Loss 0.24652642011642456\n",
      "[Training Epoch 6] Batch 2235, Loss 0.2659948468208313\n",
      "[Training Epoch 6] Batch 2236, Loss 0.2346084713935852\n",
      "[Training Epoch 6] Batch 2237, Loss 0.2570561468601227\n",
      "[Training Epoch 6] Batch 2238, Loss 0.2389553189277649\n",
      "[Training Epoch 6] Batch 2239, Loss 0.29953187704086304\n",
      "[Training Epoch 6] Batch 2240, Loss 0.24896515905857086\n",
      "[Training Epoch 6] Batch 2241, Loss 0.2622140347957611\n",
      "[Training Epoch 6] Batch 2242, Loss 0.25251635909080505\n",
      "[Training Epoch 6] Batch 2243, Loss 0.26475459337234497\n",
      "[Training Epoch 6] Batch 2244, Loss 0.25983962416648865\n",
      "[Training Epoch 6] Batch 2245, Loss 0.2339549958705902\n",
      "[Training Epoch 6] Batch 2246, Loss 0.2762589454650879\n",
      "[Training Epoch 6] Batch 2247, Loss 0.26356396079063416\n",
      "[Training Epoch 6] Batch 2248, Loss 0.24233290553092957\n",
      "[Training Epoch 6] Batch 2249, Loss 0.24371850490570068\n",
      "[Training Epoch 6] Batch 2250, Loss 0.23619891703128815\n",
      "[Training Epoch 6] Batch 2251, Loss 0.27035754919052124\n",
      "[Training Epoch 6] Batch 2252, Loss 0.25672447681427\n",
      "[Training Epoch 6] Batch 2253, Loss 0.29111340641975403\n",
      "[Training Epoch 6] Batch 2254, Loss 0.27301183342933655\n",
      "[Training Epoch 6] Batch 2255, Loss 0.24595800042152405\n",
      "[Training Epoch 6] Batch 2256, Loss 0.30607983469963074\n",
      "[Training Epoch 6] Batch 2257, Loss 0.2760750651359558\n",
      "[Training Epoch 6] Batch 2258, Loss 0.2859726548194885\n",
      "[Training Epoch 6] Batch 2259, Loss 0.2474212348461151\n",
      "[Training Epoch 6] Batch 2260, Loss 0.28204938769340515\n",
      "[Training Epoch 6] Batch 2261, Loss 0.2592601776123047\n",
      "[Training Epoch 6] Batch 2262, Loss 0.287666380405426\n",
      "[Training Epoch 6] Batch 2263, Loss 0.25483784079551697\n",
      "[Training Epoch 6] Batch 2264, Loss 0.27211904525756836\n",
      "[Training Epoch 6] Batch 2265, Loss 0.2524822950363159\n",
      "[Training Epoch 6] Batch 2266, Loss 0.2459663599729538\n",
      "[Training Epoch 6] Batch 2267, Loss 0.2708319425582886\n",
      "[Training Epoch 6] Batch 2268, Loss 0.2583225965499878\n",
      "[Training Epoch 6] Batch 2269, Loss 0.29737338423728943\n",
      "[Training Epoch 6] Batch 2270, Loss 0.26340389251708984\n",
      "[Training Epoch 6] Batch 2271, Loss 0.2640032172203064\n",
      "[Training Epoch 6] Batch 2272, Loss 0.25131893157958984\n",
      "[Training Epoch 6] Batch 2273, Loss 0.25182580947875977\n",
      "[Training Epoch 6] Batch 2274, Loss 0.23252403736114502\n",
      "[Training Epoch 6] Batch 2275, Loss 0.23637285828590393\n",
      "[Training Epoch 6] Batch 2276, Loss 0.25706642866134644\n",
      "[Training Epoch 6] Batch 2277, Loss 0.2712485194206238\n",
      "[Training Epoch 6] Batch 2278, Loss 0.2589445114135742\n",
      "[Training Epoch 6] Batch 2279, Loss 0.26308006048202515\n",
      "[Training Epoch 6] Batch 2280, Loss 0.2386394888162613\n",
      "[Training Epoch 6] Batch 2281, Loss 0.2809002995491028\n",
      "[Training Epoch 6] Batch 2282, Loss 0.23739027976989746\n",
      "[Training Epoch 6] Batch 2283, Loss 0.278805136680603\n",
      "[Training Epoch 6] Batch 2284, Loss 0.26014870405197144\n",
      "[Training Epoch 6] Batch 2285, Loss 0.2614406943321228\n",
      "[Training Epoch 6] Batch 2286, Loss 0.28910332918167114\n",
      "[Training Epoch 6] Batch 2287, Loss 0.24414873123168945\n",
      "[Training Epoch 6] Batch 2288, Loss 0.23611706495285034\n",
      "[Training Epoch 6] Batch 2289, Loss 0.25574931502342224\n",
      "[Training Epoch 6] Batch 2290, Loss 0.25919586420059204\n",
      "[Training Epoch 6] Batch 2291, Loss 0.24664781987667084\n",
      "[Training Epoch 6] Batch 2292, Loss 0.27841806411743164\n",
      "[Training Epoch 6] Batch 2293, Loss 0.2706778645515442\n",
      "[Training Epoch 6] Batch 2294, Loss 0.2521914541721344\n",
      "[Training Epoch 6] Batch 2295, Loss 0.2521003782749176\n",
      "[Training Epoch 6] Batch 2296, Loss 0.23757304251194\n",
      "[Training Epoch 6] Batch 2297, Loss 0.2507760226726532\n",
      "[Training Epoch 6] Batch 2298, Loss 0.25522711873054504\n",
      "[Training Epoch 6] Batch 2299, Loss 0.2549854815006256\n",
      "[Training Epoch 6] Batch 2300, Loss 0.2536146640777588\n",
      "[Training Epoch 6] Batch 2301, Loss 0.2632486820220947\n",
      "[Training Epoch 6] Batch 2302, Loss 0.25723668932914734\n",
      "[Training Epoch 6] Batch 2303, Loss 0.24977684020996094\n",
      "[Training Epoch 6] Batch 2304, Loss 0.24536147713661194\n",
      "[Training Epoch 6] Batch 2305, Loss 0.2529560327529907\n",
      "[Training Epoch 6] Batch 2306, Loss 0.26157113909721375\n",
      "[Training Epoch 6] Batch 2307, Loss 0.27656081318855286\n",
      "[Training Epoch 6] Batch 2308, Loss 0.2631334066390991\n",
      "[Training Epoch 6] Batch 2309, Loss 0.2824646532535553\n",
      "[Training Epoch 6] Batch 2310, Loss 0.24149900674819946\n",
      "[Training Epoch 6] Batch 2311, Loss 0.24733400344848633\n",
      "[Training Epoch 6] Batch 2312, Loss 0.26748746633529663\n",
      "[Training Epoch 6] Batch 2313, Loss 0.25698864459991455\n",
      "[Training Epoch 6] Batch 2314, Loss 0.2551031708717346\n",
      "[Training Epoch 6] Batch 2315, Loss 0.25050777196884155\n",
      "[Training Epoch 6] Batch 2316, Loss 0.288895845413208\n",
      "[Training Epoch 6] Batch 2317, Loss 0.25391268730163574\n",
      "[Training Epoch 6] Batch 2318, Loss 0.2592446804046631\n",
      "[Training Epoch 6] Batch 2319, Loss 0.25911903381347656\n",
      "[Training Epoch 6] Batch 2320, Loss 0.2527291178703308\n",
      "[Training Epoch 6] Batch 2321, Loss 0.2569233179092407\n",
      "[Training Epoch 6] Batch 2322, Loss 0.27042847871780396\n",
      "[Training Epoch 6] Batch 2323, Loss 0.24708563089370728\n",
      "[Training Epoch 6] Batch 2324, Loss 0.2497025728225708\n",
      "[Training Epoch 6] Batch 2325, Loss 0.25672751665115356\n",
      "[Training Epoch 6] Batch 2326, Loss 0.2487182468175888\n",
      "[Training Epoch 6] Batch 2327, Loss 0.23602524399757385\n",
      "[Training Epoch 6] Batch 2328, Loss 0.22303557395935059\n",
      "[Training Epoch 6] Batch 2329, Loss 0.2663005590438843\n",
      "[Training Epoch 6] Batch 2330, Loss 0.25778692960739136\n",
      "[Training Epoch 6] Batch 2331, Loss 0.2331119179725647\n",
      "[Training Epoch 6] Batch 2332, Loss 0.2707012891769409\n",
      "[Training Epoch 6] Batch 2333, Loss 0.2565164566040039\n",
      "[Training Epoch 6] Batch 2334, Loss 0.24980831146240234\n",
      "[Training Epoch 6] Batch 2335, Loss 0.27898532152175903\n",
      "[Training Epoch 6] Batch 2336, Loss 0.2780209481716156\n",
      "[Training Epoch 6] Batch 2337, Loss 0.26362085342407227\n",
      "[Training Epoch 6] Batch 2338, Loss 0.2647497057914734\n",
      "[Training Epoch 6] Batch 2339, Loss 0.2439182549715042\n",
      "[Training Epoch 6] Batch 2340, Loss 0.2564290761947632\n",
      "[Training Epoch 6] Batch 2341, Loss 0.2904186248779297\n",
      "[Training Epoch 6] Batch 2342, Loss 0.24056795239448547\n",
      "[Training Epoch 6] Batch 2343, Loss 0.2541269063949585\n",
      "[Training Epoch 6] Batch 2344, Loss 0.3009318709373474\n",
      "[Training Epoch 6] Batch 2345, Loss 0.2342376410961151\n",
      "[Training Epoch 6] Batch 2346, Loss 0.2656482458114624\n",
      "[Training Epoch 6] Batch 2347, Loss 0.29637864232063293\n",
      "[Training Epoch 6] Batch 2348, Loss 0.2571687698364258\n",
      "[Training Epoch 6] Batch 2349, Loss 0.25594276189804077\n",
      "[Training Epoch 6] Batch 2350, Loss 0.24271321296691895\n",
      "[Training Epoch 6] Batch 2351, Loss 0.24745553731918335\n",
      "[Training Epoch 6] Batch 2352, Loss 0.23529750108718872\n",
      "[Training Epoch 6] Batch 2353, Loss 0.29701805114746094\n",
      "[Training Epoch 6] Batch 2354, Loss 0.25163260102272034\n",
      "[Training Epoch 6] Batch 2355, Loss 0.267232745885849\n",
      "[Training Epoch 6] Batch 2356, Loss 0.2607802152633667\n",
      "[Training Epoch 6] Batch 2357, Loss 0.2559833526611328\n",
      "[Training Epoch 6] Batch 2358, Loss 0.2400827258825302\n",
      "[Training Epoch 6] Batch 2359, Loss 0.2778041362762451\n",
      "[Training Epoch 6] Batch 2360, Loss 0.2286904752254486\n",
      "[Training Epoch 6] Batch 2361, Loss 0.2654455900192261\n",
      "[Training Epoch 6] Batch 2362, Loss 0.2646315097808838\n",
      "[Training Epoch 6] Batch 2363, Loss 0.22600573301315308\n",
      "[Training Epoch 6] Batch 2364, Loss 0.2646183371543884\n",
      "[Training Epoch 6] Batch 2365, Loss 0.23457804322242737\n",
      "[Training Epoch 6] Batch 2366, Loss 0.25147420167922974\n",
      "[Training Epoch 6] Batch 2367, Loss 0.28109288215637207\n",
      "[Training Epoch 6] Batch 2368, Loss 0.24999558925628662\n",
      "[Training Epoch 6] Batch 2369, Loss 0.256672739982605\n",
      "[Training Epoch 6] Batch 2370, Loss 0.2474849820137024\n",
      "[Training Epoch 6] Batch 2371, Loss 0.2397610992193222\n",
      "[Training Epoch 6] Batch 2372, Loss 0.24959856271743774\n",
      "[Training Epoch 6] Batch 2373, Loss 0.2558096647262573\n",
      "[Training Epoch 6] Batch 2374, Loss 0.24251152575016022\n",
      "[Training Epoch 6] Batch 2375, Loss 0.25739622116088867\n",
      "[Training Epoch 6] Batch 2376, Loss 0.24008525907993317\n",
      "[Training Epoch 6] Batch 2377, Loss 0.29255443811416626\n",
      "[Training Epoch 6] Batch 2378, Loss 0.2540103495121002\n",
      "[Training Epoch 6] Batch 2379, Loss 0.2747099995613098\n",
      "[Training Epoch 6] Batch 2380, Loss 0.26653656363487244\n",
      "[Training Epoch 6] Batch 2381, Loss 0.24985137581825256\n",
      "[Training Epoch 6] Batch 2382, Loss 0.2562825083732605\n",
      "[Training Epoch 6] Batch 2383, Loss 0.2548489570617676\n",
      "[Training Epoch 6] Batch 2384, Loss 0.27221858501434326\n",
      "[Training Epoch 6] Batch 2385, Loss 0.26310503482818604\n",
      "[Training Epoch 6] Batch 2386, Loss 0.26919975876808167\n",
      "[Training Epoch 6] Batch 2387, Loss 0.2892470359802246\n",
      "[Training Epoch 6] Batch 2388, Loss 0.2563777267932892\n",
      "[Training Epoch 6] Batch 2389, Loss 0.2698633372783661\n",
      "[Training Epoch 6] Batch 2390, Loss 0.2688802182674408\n",
      "[Training Epoch 6] Batch 2391, Loss 0.21403291821479797\n",
      "[Training Epoch 6] Batch 2392, Loss 0.2964581549167633\n",
      "[Training Epoch 6] Batch 2393, Loss 0.2304268777370453\n",
      "[Training Epoch 6] Batch 2394, Loss 0.29253923892974854\n",
      "[Training Epoch 6] Batch 2395, Loss 0.240146204829216\n",
      "[Training Epoch 6] Batch 2396, Loss 0.2766834795475006\n",
      "[Training Epoch 6] Batch 2397, Loss 0.2349407821893692\n",
      "[Training Epoch 6] Batch 2398, Loss 0.297296941280365\n",
      "[Training Epoch 6] Batch 2399, Loss 0.2900499105453491\n",
      "[Training Epoch 6] Batch 2400, Loss 0.24762384593486786\n",
      "[Training Epoch 6] Batch 2401, Loss 0.2588644027709961\n",
      "[Training Epoch 6] Batch 2402, Loss 0.2636513411998749\n",
      "[Training Epoch 6] Batch 2403, Loss 0.2739925980567932\n",
      "[Training Epoch 6] Batch 2404, Loss 0.2486468404531479\n",
      "[Training Epoch 6] Batch 2405, Loss 0.25461214780807495\n",
      "[Training Epoch 6] Batch 2406, Loss 0.25648948550224304\n",
      "[Training Epoch 6] Batch 2407, Loss 0.2524091601371765\n",
      "[Training Epoch 6] Batch 2408, Loss 0.23671188950538635\n",
      "[Training Epoch 6] Batch 2409, Loss 0.2604791522026062\n",
      "[Training Epoch 6] Batch 2410, Loss 0.2852097153663635\n",
      "[Training Epoch 6] Batch 2411, Loss 0.20012739300727844\n",
      "[Training Epoch 6] Batch 2412, Loss 0.24832072854042053\n",
      "[Training Epoch 6] Batch 2413, Loss 0.24636507034301758\n",
      "[Training Epoch 6] Batch 2414, Loss 0.2914537787437439\n",
      "[Training Epoch 6] Batch 2415, Loss 0.24722597002983093\n",
      "[Training Epoch 6] Batch 2416, Loss 0.2523518204689026\n",
      "[Training Epoch 6] Batch 2417, Loss 0.2727798819541931\n",
      "[Training Epoch 6] Batch 2418, Loss 0.2649252414703369\n",
      "[Training Epoch 6] Batch 2419, Loss 0.26047858595848083\n",
      "[Training Epoch 6] Batch 2420, Loss 0.2532210946083069\n",
      "[Training Epoch 6] Batch 2421, Loss 0.24281293153762817\n",
      "[Training Epoch 6] Batch 2422, Loss 0.26395606994628906\n",
      "[Training Epoch 6] Batch 2423, Loss 0.23596253991127014\n",
      "[Training Epoch 6] Batch 2424, Loss 0.25677692890167236\n",
      "[Training Epoch 6] Batch 2425, Loss 0.27012425661087036\n",
      "[Training Epoch 6] Batch 2426, Loss 0.26299360394477844\n",
      "[Training Epoch 6] Batch 2427, Loss 0.2474386841058731\n",
      "[Training Epoch 6] Batch 2428, Loss 0.23423877358436584\n",
      "[Training Epoch 6] Batch 2429, Loss 0.2448757290840149\n",
      "[Training Epoch 6] Batch 2430, Loss 0.2740369737148285\n",
      "[Training Epoch 6] Batch 2431, Loss 0.2700642943382263\n",
      "[Training Epoch 6] Batch 2432, Loss 0.26106876134872437\n",
      "[Training Epoch 6] Batch 2433, Loss 0.25647106766700745\n",
      "[Training Epoch 6] Batch 2434, Loss 0.2647273540496826\n",
      "[Training Epoch 6] Batch 2435, Loss 0.26109758019447327\n",
      "[Training Epoch 6] Batch 2436, Loss 0.25777626037597656\n",
      "[Training Epoch 6] Batch 2437, Loss 0.27772536873817444\n",
      "[Training Epoch 6] Batch 2438, Loss 0.24931512773036957\n",
      "[Training Epoch 6] Batch 2439, Loss 0.2898023724555969\n",
      "[Training Epoch 6] Batch 2440, Loss 0.26263654232025146\n",
      "[Training Epoch 6] Batch 2441, Loss 0.24386534094810486\n",
      "[Training Epoch 6] Batch 2442, Loss 0.28566908836364746\n",
      "[Training Epoch 6] Batch 2443, Loss 0.2806241512298584\n",
      "[Training Epoch 6] Batch 2444, Loss 0.272269606590271\n",
      "[Training Epoch 6] Batch 2445, Loss 0.26030436158180237\n",
      "[Training Epoch 6] Batch 2446, Loss 0.2628457546234131\n",
      "[Training Epoch 6] Batch 2447, Loss 0.25135374069213867\n",
      "[Training Epoch 6] Batch 2448, Loss 0.25599461793899536\n",
      "[Training Epoch 6] Batch 2449, Loss 0.2853788137435913\n",
      "[Training Epoch 6] Batch 2450, Loss 0.2582395672798157\n",
      "[Training Epoch 6] Batch 2451, Loss 0.26220816373825073\n",
      "[Training Epoch 6] Batch 2452, Loss 0.2492496371269226\n",
      "[Training Epoch 6] Batch 2453, Loss 0.2681480944156647\n",
      "[Training Epoch 6] Batch 2454, Loss 0.258772611618042\n",
      "[Training Epoch 6] Batch 2455, Loss 0.2301524132490158\n",
      "[Training Epoch 6] Batch 2456, Loss 0.24304120242595673\n",
      "[Training Epoch 6] Batch 2457, Loss 0.2765378952026367\n",
      "[Training Epoch 6] Batch 2458, Loss 0.2691483199596405\n",
      "[Training Epoch 6] Batch 2459, Loss 0.25876760482788086\n",
      "[Training Epoch 6] Batch 2460, Loss 0.2790924906730652\n",
      "[Training Epoch 6] Batch 2461, Loss 0.2742023766040802\n",
      "[Training Epoch 6] Batch 2462, Loss 0.26509779691696167\n",
      "[Training Epoch 6] Batch 2463, Loss 0.29343682527542114\n",
      "[Training Epoch 6] Batch 2464, Loss 0.23196092247962952\n",
      "[Training Epoch 6] Batch 2465, Loss 0.2558038830757141\n",
      "[Training Epoch 6] Batch 2466, Loss 0.25273430347442627\n",
      "[Training Epoch 6] Batch 2467, Loss 0.2722629904747009\n",
      "[Training Epoch 6] Batch 2468, Loss 0.2568151354789734\n",
      "[Training Epoch 6] Batch 2469, Loss 0.27567124366760254\n",
      "[Training Epoch 6] Batch 2470, Loss 0.2454938292503357\n",
      "[Training Epoch 6] Batch 2471, Loss 0.28065937757492065\n",
      "[Training Epoch 6] Batch 2472, Loss 0.25891825556755066\n",
      "[Training Epoch 6] Batch 2473, Loss 0.260051429271698\n",
      "[Training Epoch 6] Batch 2474, Loss 0.2525353729724884\n",
      "[Training Epoch 6] Batch 2475, Loss 0.2575317621231079\n",
      "[Training Epoch 6] Batch 2476, Loss 0.2527894377708435\n",
      "[Training Epoch 6] Batch 2477, Loss 0.27250128984451294\n",
      "[Training Epoch 6] Batch 2478, Loss 0.2508526146411896\n",
      "[Training Epoch 6] Batch 2479, Loss 0.2469533085823059\n",
      "[Training Epoch 6] Batch 2480, Loss 0.2532418966293335\n",
      "[Training Epoch 6] Batch 2481, Loss 0.23977240920066833\n",
      "[Training Epoch 6] Batch 2482, Loss 0.22124268114566803\n",
      "[Training Epoch 6] Batch 2483, Loss 0.2681509852409363\n",
      "[Training Epoch 6] Batch 2484, Loss 0.279977947473526\n",
      "[Training Epoch 6] Batch 2485, Loss 0.2521924376487732\n",
      "[Training Epoch 6] Batch 2486, Loss 0.25042104721069336\n",
      "[Training Epoch 6] Batch 2487, Loss 0.2456878274679184\n",
      "[Training Epoch 6] Batch 2488, Loss 0.2521415054798126\n",
      "[Training Epoch 6] Batch 2489, Loss 0.27691707015037537\n",
      "[Training Epoch 6] Batch 2490, Loss 0.2627218961715698\n",
      "[Training Epoch 6] Batch 2491, Loss 0.2689131498336792\n",
      "[Training Epoch 6] Batch 2492, Loss 0.2655494809150696\n",
      "[Training Epoch 6] Batch 2493, Loss 0.25741714239120483\n",
      "[Training Epoch 6] Batch 2494, Loss 0.2820362448692322\n",
      "[Training Epoch 6] Batch 2495, Loss 0.2692214250564575\n",
      "[Training Epoch 6] Batch 2496, Loss 0.2774549722671509\n",
      "[Training Epoch 6] Batch 2497, Loss 0.26101014018058777\n",
      "[Training Epoch 6] Batch 2498, Loss 0.2469387650489807\n",
      "[Training Epoch 6] Batch 2499, Loss 0.23675334453582764\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2505754828453064\n",
      "[Training Epoch 6] Batch 2501, Loss 0.2335236519575119\n",
      "[Training Epoch 6] Batch 2502, Loss 0.278567910194397\n",
      "[Training Epoch 6] Batch 2503, Loss 0.24822047352790833\n",
      "[Training Epoch 6] Batch 2504, Loss 0.26833969354629517\n",
      "[Training Epoch 6] Batch 2505, Loss 0.2705894410610199\n",
      "[Training Epoch 6] Batch 2506, Loss 0.2515653371810913\n",
      "[Training Epoch 6] Batch 2507, Loss 0.2625681459903717\n",
      "[Training Epoch 6] Batch 2508, Loss 0.23105090856552124\n",
      "[Training Epoch 6] Batch 2509, Loss 0.275809645652771\n",
      "[Training Epoch 6] Batch 2510, Loss 0.2361205518245697\n",
      "[Training Epoch 6] Batch 2511, Loss 0.25685250759124756\n",
      "[Training Epoch 6] Batch 2512, Loss 0.26762115955352783\n",
      "[Training Epoch 6] Batch 2513, Loss 0.2757837176322937\n",
      "[Training Epoch 6] Batch 2514, Loss 0.27138662338256836\n",
      "[Training Epoch 6] Batch 2515, Loss 0.27521300315856934\n",
      "[Training Epoch 6] Batch 2516, Loss 0.26944059133529663\n",
      "[Training Epoch 6] Batch 2517, Loss 0.24969899654388428\n",
      "[Training Epoch 6] Batch 2518, Loss 0.2791949510574341\n",
      "[Training Epoch 6] Batch 2519, Loss 0.25987789034843445\n",
      "[Training Epoch 6] Batch 2520, Loss 0.24341687560081482\n",
      "[Training Epoch 6] Batch 2521, Loss 0.269104540348053\n",
      "[Training Epoch 6] Batch 2522, Loss 0.2742699384689331\n",
      "[Training Epoch 6] Batch 2523, Loss 0.22887744009494781\n",
      "[Training Epoch 6] Batch 2524, Loss 0.23216259479522705\n",
      "[Training Epoch 6] Batch 2525, Loss 0.28044480085372925\n",
      "[Training Epoch 6] Batch 2526, Loss 0.2858459949493408\n",
      "[Training Epoch 6] Batch 2527, Loss 0.26406678557395935\n",
      "[Training Epoch 6] Batch 2528, Loss 0.26354286074638367\n",
      "[Training Epoch 6] Batch 2529, Loss 0.2536599338054657\n",
      "[Training Epoch 6] Batch 2530, Loss 0.24917355179786682\n",
      "[Training Epoch 6] Batch 2531, Loss 0.24530348181724548\n",
      "[Training Epoch 6] Batch 2532, Loss 0.2598593533039093\n",
      "[Training Epoch 6] Batch 2533, Loss 0.2562245726585388\n",
      "[Training Epoch 6] Batch 2534, Loss 0.2613895535469055\n",
      "[Training Epoch 6] Batch 2535, Loss 0.27741146087646484\n",
      "[Training Epoch 6] Batch 2536, Loss 0.2608136534690857\n",
      "[Training Epoch 6] Batch 2537, Loss 0.2329762876033783\n",
      "[Training Epoch 6] Batch 2538, Loss 0.26778659224510193\n",
      "[Training Epoch 6] Batch 2539, Loss 0.22814945876598358\n",
      "[Training Epoch 6] Batch 2540, Loss 0.27065497636795044\n",
      "[Training Epoch 6] Batch 2541, Loss 0.25970810651779175\n",
      "[Training Epoch 6] Batch 2542, Loss 0.26153576374053955\n",
      "[Training Epoch 6] Batch 2543, Loss 0.2550637722015381\n",
      "[Training Epoch 6] Batch 2544, Loss 0.2667348384857178\n",
      "[Training Epoch 6] Batch 2545, Loss 0.262736439704895\n",
      "[Training Epoch 6] Batch 2546, Loss 0.32170212268829346\n",
      "[Training Epoch 6] Batch 2547, Loss 0.2544845640659332\n",
      "[Training Epoch 6] Batch 2548, Loss 0.2680390477180481\n",
      "[Training Epoch 6] Batch 2549, Loss 0.2839966118335724\n",
      "[Training Epoch 6] Batch 2550, Loss 0.2349027693271637\n",
      "[Training Epoch 6] Batch 2551, Loss 0.24903811514377594\n",
      "[Training Epoch 6] Batch 2552, Loss 0.2436169981956482\n",
      "[Training Epoch 6] Batch 2553, Loss 0.25768396258354187\n",
      "[Training Epoch 6] Batch 2554, Loss 0.2779560685157776\n",
      "[Training Epoch 6] Batch 2555, Loss 0.2692021429538727\n",
      "[Training Epoch 6] Batch 2556, Loss 0.26980337500572205\n",
      "[Training Epoch 6] Batch 2557, Loss 0.290524959564209\n",
      "[Training Epoch 6] Batch 2558, Loss 0.27111414074897766\n",
      "[Training Epoch 6] Batch 2559, Loss 0.2755618095397949\n",
      "[Training Epoch 6] Batch 2560, Loss 0.26582857966423035\n",
      "[Training Epoch 6] Batch 2561, Loss 0.2350422739982605\n",
      "[Training Epoch 6] Batch 2562, Loss 0.22866851091384888\n",
      "[Training Epoch 6] Batch 2563, Loss 0.25302654504776\n",
      "[Training Epoch 6] Batch 2564, Loss 0.2598770260810852\n",
      "[Training Epoch 6] Batch 2565, Loss 0.26455533504486084\n",
      "[Training Epoch 6] Batch 2566, Loss 0.240799218416214\n",
      "[Training Epoch 6] Batch 2567, Loss 0.27018848061561584\n",
      "[Training Epoch 6] Batch 2568, Loss 0.2711678147315979\n",
      "[Training Epoch 6] Batch 2569, Loss 0.28200092911720276\n",
      "[Training Epoch 6] Batch 2570, Loss 0.2465050220489502\n",
      "[Training Epoch 6] Batch 2571, Loss 0.25521737337112427\n",
      "[Training Epoch 6] Batch 2572, Loss 0.24528124928474426\n",
      "[Training Epoch 6] Batch 2573, Loss 0.278700590133667\n",
      "[Training Epoch 6] Batch 2574, Loss 0.25023001432418823\n",
      "[Training Epoch 6] Batch 2575, Loss 0.26496320962905884\n",
      "[Training Epoch 6] Batch 2576, Loss 0.2702904939651489\n",
      "[Training Epoch 6] Batch 2577, Loss 0.2770897150039673\n",
      "[Training Epoch 6] Batch 2578, Loss 0.2357425093650818\n",
      "[Training Epoch 6] Batch 2579, Loss 0.25473952293395996\n",
      "[Training Epoch 6] Batch 2580, Loss 0.2503836154937744\n",
      "[Training Epoch 6] Batch 2581, Loss 0.2571007013320923\n",
      "[Training Epoch 6] Batch 2582, Loss 0.26618367433547974\n",
      "[Training Epoch 6] Batch 2583, Loss 0.24246960878372192\n",
      "[Training Epoch 6] Batch 2584, Loss 0.2595751881599426\n",
      "[Training Epoch 6] Batch 2585, Loss 0.2645379900932312\n",
      "[Training Epoch 6] Batch 2586, Loss 0.2591297924518585\n",
      "[Training Epoch 6] Batch 2587, Loss 0.26048120856285095\n",
      "[Training Epoch 6] Batch 2588, Loss 0.26538074016571045\n",
      "[Training Epoch 6] Batch 2589, Loss 0.27213460206985474\n",
      "[Training Epoch 6] Batch 2590, Loss 0.2526213526725769\n",
      "[Training Epoch 6] Batch 2591, Loss 0.24598871171474457\n",
      "[Training Epoch 6] Batch 2592, Loss 0.26440244913101196\n",
      "[Training Epoch 6] Batch 2593, Loss 0.26801103353500366\n",
      "[Training Epoch 6] Batch 2594, Loss 0.27570146322250366\n",
      "[Training Epoch 6] Batch 2595, Loss 0.24270129203796387\n",
      "[Training Epoch 6] Batch 2596, Loss 0.24660994112491608\n",
      "[Training Epoch 6] Batch 2597, Loss 0.2464829981327057\n",
      "[Training Epoch 6] Batch 2598, Loss 0.2570480406284332\n",
      "[Training Epoch 6] Batch 2599, Loss 0.24053630232810974\n",
      "[Training Epoch 6] Batch 2600, Loss 0.24224753677845\n",
      "[Training Epoch 6] Batch 2601, Loss 0.24313382804393768\n",
      "[Training Epoch 6] Batch 2602, Loss 0.26154953241348267\n",
      "[Training Epoch 6] Batch 2603, Loss 0.2500488758087158\n",
      "[Training Epoch 6] Batch 2604, Loss 0.24287112057209015\n",
      "[Training Epoch 6] Batch 2605, Loss 0.25638526678085327\n",
      "[Training Epoch 6] Batch 2606, Loss 0.26306647062301636\n",
      "[Training Epoch 6] Batch 2607, Loss 0.2684376835823059\n",
      "[Training Epoch 6] Batch 2608, Loss 0.2693738341331482\n",
      "[Training Epoch 6] Batch 2609, Loss 0.25719115138053894\n",
      "[Training Epoch 6] Batch 2610, Loss 0.24156074225902557\n",
      "[Training Epoch 6] Batch 2611, Loss 0.2516692876815796\n",
      "[Training Epoch 6] Batch 2612, Loss 0.2598639726638794\n",
      "[Training Epoch 6] Batch 2613, Loss 0.23861545324325562\n",
      "[Training Epoch 6] Batch 2614, Loss 0.24476152658462524\n",
      "[Training Epoch 6] Batch 2615, Loss 0.2274673730134964\n",
      "[Training Epoch 6] Batch 2616, Loss 0.27990126609802246\n",
      "[Training Epoch 6] Batch 2617, Loss 0.28466248512268066\n",
      "[Training Epoch 6] Batch 2618, Loss 0.2774287462234497\n",
      "[Training Epoch 6] Batch 2619, Loss 0.22266632318496704\n",
      "[Training Epoch 6] Batch 2620, Loss 0.24790087342262268\n",
      "[Training Epoch 6] Batch 2621, Loss 0.3000432252883911\n",
      "[Training Epoch 6] Batch 2622, Loss 0.2304050624370575\n",
      "[Training Epoch 6] Batch 2623, Loss 0.248646080493927\n",
      "[Training Epoch 6] Batch 2624, Loss 0.24770700931549072\n",
      "[Training Epoch 6] Batch 2625, Loss 0.3049669861793518\n",
      "[Training Epoch 6] Batch 2626, Loss 0.2430461198091507\n",
      "[Training Epoch 6] Batch 2627, Loss 0.22880420088768005\n",
      "[Training Epoch 6] Batch 2628, Loss 0.2746427655220032\n",
      "[Training Epoch 6] Batch 2629, Loss 0.28769415616989136\n",
      "[Training Epoch 6] Batch 2630, Loss 0.23200112581253052\n",
      "[Training Epoch 6] Batch 2631, Loss 0.2502422034740448\n",
      "[Training Epoch 6] Batch 2632, Loss 0.25137749314308167\n",
      "[Training Epoch 6] Batch 2633, Loss 0.23490676283836365\n",
      "[Training Epoch 6] Batch 2634, Loss 0.24220888316631317\n",
      "[Training Epoch 6] Batch 2635, Loss 0.23569916188716888\n",
      "[Training Epoch 6] Batch 2636, Loss 0.2472754567861557\n",
      "[Training Epoch 6] Batch 2637, Loss 0.254609614610672\n",
      "[Training Epoch 6] Batch 2638, Loss 0.2675807476043701\n",
      "[Training Epoch 6] Batch 2639, Loss 0.26699185371398926\n",
      "[Training Epoch 6] Batch 2640, Loss 0.23550516366958618\n",
      "[Training Epoch 6] Batch 2641, Loss 0.24381490051746368\n",
      "[Training Epoch 6] Batch 2642, Loss 0.25952261686325073\n",
      "[Training Epoch 6] Batch 2643, Loss 0.22994226217269897\n",
      "[Training Epoch 6] Batch 2644, Loss 0.27122020721435547\n",
      "[Training Epoch 6] Batch 2645, Loss 0.2829381227493286\n",
      "[Training Epoch 6] Batch 2646, Loss 0.25818347930908203\n",
      "[Training Epoch 6] Batch 2647, Loss 0.2507036328315735\n",
      "[Training Epoch 6] Batch 2648, Loss 0.2999052405357361\n",
      "[Training Epoch 6] Batch 2649, Loss 0.27335625886917114\n",
      "[Training Epoch 6] Batch 2650, Loss 0.25283485651016235\n",
      "[Training Epoch 6] Batch 2651, Loss 0.2549059987068176\n",
      "[Training Epoch 6] Batch 2652, Loss 0.2721913456916809\n",
      "[Training Epoch 6] Batch 2653, Loss 0.24609540402889252\n",
      "[Training Epoch 6] Batch 2654, Loss 0.24912667274475098\n",
      "[Training Epoch 6] Batch 2655, Loss 0.26547256112098694\n",
      "[Training Epoch 6] Batch 2656, Loss 0.2669951319694519\n",
      "[Training Epoch 6] Batch 2657, Loss 0.2736978530883789\n",
      "[Training Epoch 6] Batch 2658, Loss 0.26187586784362793\n",
      "[Training Epoch 6] Batch 2659, Loss 0.266885906457901\n",
      "[Training Epoch 6] Batch 2660, Loss 0.2600446343421936\n",
      "[Training Epoch 6] Batch 2661, Loss 0.2518453598022461\n",
      "[Training Epoch 6] Batch 2662, Loss 0.2577659487724304\n",
      "[Training Epoch 6] Batch 2663, Loss 0.25456181168556213\n",
      "[Training Epoch 6] Batch 2664, Loss 0.23949845135211945\n",
      "[Training Epoch 6] Batch 2665, Loss 0.258783221244812\n",
      "[Training Epoch 6] Batch 2666, Loss 0.26619189977645874\n",
      "[Training Epoch 6] Batch 2667, Loss 0.23239527642726898\n",
      "[Training Epoch 6] Batch 2668, Loss 0.25241684913635254\n",
      "[Training Epoch 6] Batch 2669, Loss 0.2739291191101074\n",
      "[Training Epoch 6] Batch 2670, Loss 0.27314698696136475\n",
      "[Training Epoch 6] Batch 2671, Loss 0.26661014556884766\n",
      "[Training Epoch 6] Batch 2672, Loss 0.2691146731376648\n",
      "[Training Epoch 6] Batch 2673, Loss 0.2634281516075134\n",
      "[Training Epoch 6] Batch 2674, Loss 0.24083518981933594\n",
      "[Training Epoch 6] Batch 2675, Loss 0.25031226873397827\n",
      "[Training Epoch 6] Batch 2676, Loss 0.2500999867916107\n",
      "[Training Epoch 6] Batch 2677, Loss 0.25475773215293884\n",
      "[Training Epoch 6] Batch 2678, Loss 0.26231327652931213\n",
      "[Training Epoch 6] Batch 2679, Loss 0.254740834236145\n",
      "[Training Epoch 6] Batch 2680, Loss 0.27360519766807556\n",
      "[Training Epoch 6] Batch 2681, Loss 0.27798962593078613\n",
      "[Training Epoch 6] Batch 2682, Loss 0.26026877760887146\n",
      "[Training Epoch 6] Batch 2683, Loss 0.28460633754730225\n",
      "[Training Epoch 6] Batch 2684, Loss 0.2771051526069641\n",
      "[Training Epoch 6] Batch 2685, Loss 0.27158188819885254\n",
      "[Training Epoch 6] Batch 2686, Loss 0.2620123624801636\n",
      "[Training Epoch 6] Batch 2687, Loss 0.2570881247520447\n",
      "[Training Epoch 6] Batch 2688, Loss 0.2793790400028229\n",
      "[Training Epoch 6] Batch 2689, Loss 0.2724790573120117\n",
      "[Training Epoch 6] Batch 2690, Loss 0.2734996974468231\n",
      "[Training Epoch 6] Batch 2691, Loss 0.25130414962768555\n",
      "[Training Epoch 6] Batch 2692, Loss 0.2301519811153412\n",
      "[Training Epoch 6] Batch 2693, Loss 0.2603839039802551\n",
      "[Training Epoch 6] Batch 2694, Loss 0.2620612680912018\n",
      "[Training Epoch 6] Batch 2695, Loss 0.25785762071609497\n",
      "[Training Epoch 6] Batch 2696, Loss 0.26533734798431396\n",
      "[Training Epoch 6] Batch 2697, Loss 0.25181007385253906\n",
      "[Training Epoch 6] Batch 2698, Loss 0.27968114614486694\n",
      "[Training Epoch 6] Batch 2699, Loss 0.2773572504520416\n",
      "[Training Epoch 6] Batch 2700, Loss 0.2522718608379364\n",
      "[Training Epoch 6] Batch 2701, Loss 0.23580777645111084\n",
      "[Training Epoch 6] Batch 2702, Loss 0.24199356138706207\n",
      "[Training Epoch 6] Batch 2703, Loss 0.2666073441505432\n",
      "[Training Epoch 6] Batch 2704, Loss 0.258877694606781\n",
      "[Training Epoch 6] Batch 2705, Loss 0.23657338321208954\n",
      "[Training Epoch 6] Batch 2706, Loss 0.24295301735401154\n",
      "[Training Epoch 6] Batch 2707, Loss 0.28574275970458984\n",
      "[Training Epoch 6] Batch 2708, Loss 0.2638964354991913\n",
      "[Training Epoch 6] Batch 2709, Loss 0.27659571170806885\n",
      "[Training Epoch 6] Batch 2710, Loss 0.23411990702152252\n",
      "[Training Epoch 6] Batch 2711, Loss 0.2689110338687897\n",
      "[Training Epoch 6] Batch 2712, Loss 0.2418021708726883\n",
      "[Training Epoch 6] Batch 2713, Loss 0.2288564294576645\n",
      "[Training Epoch 6] Batch 2714, Loss 0.2365427017211914\n",
      "[Training Epoch 6] Batch 2715, Loss 0.2194618284702301\n",
      "[Training Epoch 6] Batch 2716, Loss 0.2612660527229309\n",
      "[Training Epoch 6] Batch 2717, Loss 0.24428217113018036\n",
      "[Training Epoch 6] Batch 2718, Loss 0.2608167827129364\n",
      "[Training Epoch 6] Batch 2719, Loss 0.2841265797615051\n",
      "[Training Epoch 6] Batch 2720, Loss 0.25292354822158813\n",
      "[Training Epoch 6] Batch 2721, Loss 0.2447500079870224\n",
      "[Training Epoch 6] Batch 2722, Loss 0.23896914720535278\n",
      "[Training Epoch 6] Batch 2723, Loss 0.28280895948410034\n",
      "[Training Epoch 6] Batch 2724, Loss 0.2351258397102356\n",
      "[Training Epoch 6] Batch 2725, Loss 0.22917930781841278\n",
      "[Training Epoch 6] Batch 2726, Loss 0.23609423637390137\n",
      "[Training Epoch 6] Batch 2727, Loss 0.25289538502693176\n",
      "[Training Epoch 6] Batch 2728, Loss 0.2455967217683792\n",
      "[Training Epoch 6] Batch 2729, Loss 0.25467032194137573\n",
      "[Training Epoch 6] Batch 2730, Loss 0.24138081073760986\n",
      "[Training Epoch 6] Batch 2731, Loss 0.2532636225223541\n",
      "[Training Epoch 6] Batch 2732, Loss 0.23586305975914001\n",
      "[Training Epoch 6] Batch 2733, Loss 0.2624979615211487\n",
      "[Training Epoch 6] Batch 2734, Loss 0.24739906191825867\n",
      "[Training Epoch 6] Batch 2735, Loss 0.25970131158828735\n",
      "[Training Epoch 6] Batch 2736, Loss 0.2499597668647766\n",
      "[Training Epoch 6] Batch 2737, Loss 0.2548818588256836\n",
      "[Training Epoch 6] Batch 2738, Loss 0.2319062352180481\n",
      "[Training Epoch 6] Batch 2739, Loss 0.24729669094085693\n",
      "[Training Epoch 6] Batch 2740, Loss 0.2478528767824173\n",
      "[Training Epoch 6] Batch 2741, Loss 0.23283912241458893\n",
      "[Training Epoch 6] Batch 2742, Loss 0.2553617060184479\n",
      "[Training Epoch 6] Batch 2743, Loss 0.2784780263900757\n",
      "[Training Epoch 6] Batch 2744, Loss 0.260622501373291\n",
      "[Training Epoch 6] Batch 2745, Loss 0.2758735418319702\n",
      "[Training Epoch 6] Batch 2746, Loss 0.27072110772132874\n",
      "[Training Epoch 6] Batch 2747, Loss 0.24317467212677002\n",
      "[Training Epoch 6] Batch 2748, Loss 0.25816673040390015\n",
      "[Training Epoch 6] Batch 2749, Loss 0.2939590513706207\n",
      "[Training Epoch 6] Batch 2750, Loss 0.28161385655403137\n",
      "[Training Epoch 6] Batch 2751, Loss 0.2897856831550598\n",
      "[Training Epoch 6] Batch 2752, Loss 0.24748407304286957\n",
      "[Training Epoch 6] Batch 2753, Loss 0.2776836156845093\n",
      "[Training Epoch 6] Batch 2754, Loss 0.2542875409126282\n",
      "[Training Epoch 6] Batch 2755, Loss 0.2529991865158081\n",
      "[Training Epoch 6] Batch 2756, Loss 0.2562713325023651\n",
      "[Training Epoch 6] Batch 2757, Loss 0.26336169242858887\n",
      "[Training Epoch 6] Batch 2758, Loss 0.24206487834453583\n",
      "[Training Epoch 6] Batch 2759, Loss 0.26684361696243286\n",
      "[Training Epoch 6] Batch 2760, Loss 0.22616149485111237\n",
      "[Training Epoch 6] Batch 2761, Loss 0.23004387319087982\n",
      "[Training Epoch 6] Batch 2762, Loss 0.23455625772476196\n",
      "[Training Epoch 6] Batch 2763, Loss 0.2322719544172287\n",
      "[Training Epoch 6] Batch 2764, Loss 0.3023317754268646\n",
      "[Training Epoch 6] Batch 2765, Loss 0.22102521359920502\n",
      "[Training Epoch 6] Batch 2766, Loss 0.2889774441719055\n",
      "[Training Epoch 6] Batch 2767, Loss 0.25763797760009766\n",
      "[Training Epoch 6] Batch 2768, Loss 0.2599002718925476\n",
      "[Training Epoch 6] Batch 2769, Loss 0.24772053956985474\n",
      "[Training Epoch 6] Batch 2770, Loss 0.2646774351596832\n",
      "[Training Epoch 6] Batch 2771, Loss 0.23752376437187195\n",
      "[Training Epoch 6] Batch 2772, Loss 0.2519208490848541\n",
      "[Training Epoch 6] Batch 2773, Loss 0.25076112151145935\n",
      "[Training Epoch 6] Batch 2774, Loss 0.2655688524246216\n",
      "[Training Epoch 6] Batch 2775, Loss 0.25059813261032104\n",
      "[Training Epoch 6] Batch 2776, Loss 0.24329876899719238\n",
      "[Training Epoch 6] Batch 2777, Loss 0.24270813167095184\n",
      "[Training Epoch 6] Batch 2778, Loss 0.24325361847877502\n",
      "[Training Epoch 6] Batch 2779, Loss 0.2745647132396698\n",
      "[Training Epoch 6] Batch 2780, Loss 0.27042749524116516\n",
      "[Training Epoch 6] Batch 2781, Loss 0.26660865545272827\n",
      "[Training Epoch 6] Batch 2782, Loss 0.26229363679885864\n",
      "[Training Epoch 6] Batch 2783, Loss 0.2552887201309204\n",
      "[Training Epoch 6] Batch 2784, Loss 0.24998867511749268\n",
      "[Training Epoch 6] Batch 2785, Loss 0.24557188153266907\n",
      "[Training Epoch 6] Batch 2786, Loss 0.2607930898666382\n",
      "[Training Epoch 6] Batch 2787, Loss 0.28910669684410095\n",
      "[Training Epoch 6] Batch 2788, Loss 0.2631899416446686\n",
      "[Training Epoch 6] Batch 2789, Loss 0.2539549469947815\n",
      "[Training Epoch 6] Batch 2790, Loss 0.2700740694999695\n",
      "[Training Epoch 6] Batch 2791, Loss 0.26213663816452026\n",
      "[Training Epoch 6] Batch 2792, Loss 0.26090770959854126\n",
      "[Training Epoch 6] Batch 2793, Loss 0.24341288208961487\n",
      "[Training Epoch 6] Batch 2794, Loss 0.26845985651016235\n",
      "[Training Epoch 6] Batch 2795, Loss 0.26620447635650635\n",
      "[Training Epoch 6] Batch 2796, Loss 0.24045398831367493\n",
      "[Training Epoch 6] Batch 2797, Loss 0.2663213312625885\n",
      "[Training Epoch 6] Batch 2798, Loss 0.278030127286911\n",
      "[Training Epoch 6] Batch 2799, Loss 0.25709640979766846\n",
      "[Training Epoch 6] Batch 2800, Loss 0.25612324476242065\n",
      "[Training Epoch 6] Batch 2801, Loss 0.3134573698043823\n",
      "[Training Epoch 6] Batch 2802, Loss 0.28867051005363464\n",
      "[Training Epoch 6] Batch 2803, Loss 0.2738606929779053\n",
      "[Training Epoch 6] Batch 2804, Loss 0.2332773208618164\n",
      "[Training Epoch 6] Batch 2805, Loss 0.26760175824165344\n",
      "[Training Epoch 6] Batch 2806, Loss 0.23161770403385162\n",
      "[Training Epoch 6] Batch 2807, Loss 0.27103111147880554\n",
      "[Training Epoch 6] Batch 2808, Loss 0.29099297523498535\n",
      "[Training Epoch 6] Batch 2809, Loss 0.27465516328811646\n",
      "[Training Epoch 6] Batch 2810, Loss 0.24700698256492615\n",
      "[Training Epoch 6] Batch 2811, Loss 0.24201729893684387\n",
      "[Training Epoch 6] Batch 2812, Loss 0.27472519874572754\n",
      "[Training Epoch 6] Batch 2813, Loss 0.2803542912006378\n",
      "[Training Epoch 6] Batch 2814, Loss 0.27537113428115845\n",
      "[Training Epoch 6] Batch 2815, Loss 0.2636643648147583\n",
      "[Training Epoch 6] Batch 2816, Loss 0.2633047103881836\n",
      "[Training Epoch 6] Batch 2817, Loss 0.26254650950431824\n",
      "[Training Epoch 6] Batch 2818, Loss 0.25660374760627747\n",
      "[Training Epoch 6] Batch 2819, Loss 0.261531800031662\n",
      "[Training Epoch 6] Batch 2820, Loss 0.26173898577690125\n",
      "[Training Epoch 6] Batch 2821, Loss 0.26693713665008545\n",
      "[Training Epoch 6] Batch 2822, Loss 0.2382207214832306\n",
      "[Training Epoch 6] Batch 2823, Loss 0.28252536058425903\n",
      "[Training Epoch 6] Batch 2824, Loss 0.26750630140304565\n",
      "[Training Epoch 6] Batch 2825, Loss 0.2492140233516693\n",
      "[Training Epoch 6] Batch 2826, Loss 0.23571747541427612\n",
      "[Training Epoch 6] Batch 2827, Loss 0.2166774421930313\n",
      "[Training Epoch 6] Batch 2828, Loss 0.25444167852401733\n",
      "[Training Epoch 6] Batch 2829, Loss 0.2309608906507492\n",
      "[Training Epoch 6] Batch 2830, Loss 0.2679291069507599\n",
      "[Training Epoch 6] Batch 2831, Loss 0.2545659840106964\n",
      "[Training Epoch 6] Batch 2832, Loss 0.23310783505439758\n",
      "[Training Epoch 6] Batch 2833, Loss 0.24143309891223907\n",
      "[Training Epoch 6] Batch 2834, Loss 0.2681821286678314\n",
      "[Training Epoch 6] Batch 2835, Loss 0.24566218256950378\n",
      "[Training Epoch 6] Batch 2836, Loss 0.2537745237350464\n",
      "[Training Epoch 6] Batch 2837, Loss 0.2443160116672516\n",
      "[Training Epoch 6] Batch 2838, Loss 0.26241397857666016\n",
      "[Training Epoch 6] Batch 2839, Loss 0.2627936005592346\n",
      "[Training Epoch 6] Batch 2840, Loss 0.23029725253582\n",
      "[Training Epoch 6] Batch 2841, Loss 0.2244378626346588\n",
      "[Training Epoch 6] Batch 2842, Loss 0.25502660870552063\n",
      "[Training Epoch 6] Batch 2843, Loss 0.22418075799942017\n",
      "[Training Epoch 6] Batch 2844, Loss 0.26573091745376587\n",
      "[Training Epoch 6] Batch 2845, Loss 0.2676016390323639\n",
      "[Training Epoch 6] Batch 2846, Loss 0.23472633957862854\n",
      "[Training Epoch 6] Batch 2847, Loss 0.2748758792877197\n",
      "[Training Epoch 6] Batch 2848, Loss 0.27230533957481384\n",
      "[Training Epoch 6] Batch 2849, Loss 0.23282262682914734\n",
      "[Training Epoch 6] Batch 2850, Loss 0.246957466006279\n",
      "[Training Epoch 6] Batch 2851, Loss 0.2901989221572876\n",
      "[Training Epoch 6] Batch 2852, Loss 0.25401464104652405\n",
      "[Training Epoch 6] Batch 2853, Loss 0.24205540120601654\n",
      "[Training Epoch 6] Batch 2854, Loss 0.2711114287376404\n",
      "[Training Epoch 6] Batch 2855, Loss 0.24559831619262695\n",
      "[Training Epoch 6] Batch 2856, Loss 0.265723317861557\n",
      "[Training Epoch 6] Batch 2857, Loss 0.23858307301998138\n",
      "[Training Epoch 6] Batch 2858, Loss 0.2687338888645172\n",
      "[Training Epoch 6] Batch 2859, Loss 0.26266539096832275\n",
      "[Training Epoch 6] Batch 2860, Loss 0.2604484558105469\n",
      "[Training Epoch 6] Batch 2861, Loss 0.2719140946865082\n",
      "[Training Epoch 6] Batch 2862, Loss 0.2787540555000305\n",
      "[Training Epoch 6] Batch 2863, Loss 0.26332685351371765\n",
      "[Training Epoch 6] Batch 2864, Loss 0.24409523606300354\n",
      "[Training Epoch 6] Batch 2865, Loss 0.25026750564575195\n",
      "[Training Epoch 6] Batch 2866, Loss 0.3057059943675995\n",
      "[Training Epoch 6] Batch 2867, Loss 0.2833966016769409\n",
      "[Training Epoch 6] Batch 2868, Loss 0.29434216022491455\n",
      "[Training Epoch 6] Batch 2869, Loss 0.24947035312652588\n",
      "[Training Epoch 6] Batch 2870, Loss 0.25876396894454956\n",
      "[Training Epoch 6] Batch 2871, Loss 0.2517944574356079\n",
      "[Training Epoch 6] Batch 2872, Loss 0.30718833208084106\n",
      "[Training Epoch 6] Batch 2873, Loss 0.21634864807128906\n",
      "[Training Epoch 6] Batch 2874, Loss 0.23838603496551514\n",
      "[Training Epoch 6] Batch 2875, Loss 0.26699844002723694\n",
      "[Training Epoch 6] Batch 2876, Loss 0.23352941870689392\n",
      "[Training Epoch 6] Batch 2877, Loss 0.2562585771083832\n",
      "[Training Epoch 6] Batch 2878, Loss 0.26618748903274536\n",
      "[Training Epoch 6] Batch 2879, Loss 0.2504851520061493\n",
      "[Training Epoch 6] Batch 2880, Loss 0.2990615665912628\n",
      "[Training Epoch 6] Batch 2881, Loss 0.24930021166801453\n",
      "[Training Epoch 6] Batch 2882, Loss 0.2778087854385376\n",
      "[Training Epoch 6] Batch 2883, Loss 0.2668784260749817\n",
      "[Training Epoch 6] Batch 2884, Loss 0.28350672125816345\n",
      "[Training Epoch 6] Batch 2885, Loss 0.24274489283561707\n",
      "[Training Epoch 6] Batch 2886, Loss 0.2305213212966919\n",
      "[Training Epoch 6] Batch 2887, Loss 0.26201629638671875\n",
      "[Training Epoch 6] Batch 2888, Loss 0.2722778916358948\n",
      "[Training Epoch 6] Batch 2889, Loss 0.2556365430355072\n",
      "[Training Epoch 6] Batch 2890, Loss 0.2766319513320923\n",
      "[Training Epoch 6] Batch 2891, Loss 0.2529064416885376\n",
      "[Training Epoch 6] Batch 2892, Loss 0.25860488414764404\n",
      "[Training Epoch 6] Batch 2893, Loss 0.22881615161895752\n",
      "[Training Epoch 6] Batch 2894, Loss 0.2290509045124054\n",
      "[Training Epoch 6] Batch 2895, Loss 0.2639343738555908\n",
      "[Training Epoch 6] Batch 2896, Loss 0.28466522693634033\n",
      "[Training Epoch 6] Batch 2897, Loss 0.27215513586997986\n",
      "[Training Epoch 6] Batch 2898, Loss 0.2270452231168747\n",
      "[Training Epoch 6] Batch 2899, Loss 0.24814212322235107\n",
      "[Training Epoch 6] Batch 2900, Loss 0.2707485854625702\n",
      "[Training Epoch 6] Batch 2901, Loss 0.2701248228549957\n",
      "[Training Epoch 6] Batch 2902, Loss 0.2724546492099762\n",
      "[Training Epoch 6] Batch 2903, Loss 0.27472394704818726\n",
      "[Training Epoch 6] Batch 2904, Loss 0.26713621616363525\n",
      "[Training Epoch 6] Batch 2905, Loss 0.24645210802555084\n",
      "[Training Epoch 6] Batch 2906, Loss 0.26386067271232605\n",
      "[Training Epoch 6] Batch 2907, Loss 0.22713182866573334\n",
      "[Training Epoch 6] Batch 2908, Loss 0.2591407895088196\n",
      "[Training Epoch 6] Batch 2909, Loss 0.24579499661922455\n",
      "[Training Epoch 6] Batch 2910, Loss 0.2597167491912842\n",
      "[Training Epoch 6] Batch 2911, Loss 0.27736154198646545\n",
      "[Training Epoch 6] Batch 2912, Loss 0.27449578046798706\n",
      "[Training Epoch 6] Batch 2913, Loss 0.2648795247077942\n",
      "[Training Epoch 6] Batch 2914, Loss 0.2209623008966446\n",
      "[Training Epoch 6] Batch 2915, Loss 0.24619072675704956\n",
      "[Training Epoch 6] Batch 2916, Loss 0.23561322689056396\n",
      "[Training Epoch 6] Batch 2917, Loss 0.27383530139923096\n",
      "[Training Epoch 6] Batch 2918, Loss 0.27908650040626526\n",
      "[Training Epoch 6] Batch 2919, Loss 0.2615380883216858\n",
      "[Training Epoch 6] Batch 2920, Loss 0.2843945026397705\n",
      "[Training Epoch 6] Batch 2921, Loss 0.2581610679626465\n",
      "[Training Epoch 6] Batch 2922, Loss 0.26535141468048096\n",
      "[Training Epoch 6] Batch 2923, Loss 0.2711612582206726\n",
      "[Training Epoch 6] Batch 2924, Loss 0.24876490235328674\n",
      "[Training Epoch 6] Batch 2925, Loss 0.2602066695690155\n",
      "[Training Epoch 6] Batch 2926, Loss 0.23459452390670776\n",
      "[Training Epoch 6] Batch 2927, Loss 0.2622557580471039\n",
      "[Training Epoch 6] Batch 2928, Loss 0.2715035080909729\n",
      "[Training Epoch 6] Batch 2929, Loss 0.2445823848247528\n",
      "[Training Epoch 6] Batch 2930, Loss 0.2954789996147156\n",
      "[Training Epoch 6] Batch 2931, Loss 0.2667875587940216\n",
      "[Training Epoch 6] Batch 2932, Loss 0.24426347017288208\n",
      "[Training Epoch 6] Batch 2933, Loss 0.25827035307884216\n",
      "[Training Epoch 6] Batch 2934, Loss 0.22023971378803253\n",
      "[Training Epoch 6] Batch 2935, Loss 0.27367526292800903\n",
      "[Training Epoch 6] Batch 2936, Loss 0.3041464686393738\n",
      "[Training Epoch 6] Batch 2937, Loss 0.2514188587665558\n",
      "[Training Epoch 6] Batch 2938, Loss 0.22959385812282562\n",
      "[Training Epoch 6] Batch 2939, Loss 0.2782340347766876\n",
      "[Training Epoch 6] Batch 2940, Loss 0.26257947087287903\n",
      "[Training Epoch 6] Batch 2941, Loss 0.2686206102371216\n",
      "[Training Epoch 6] Batch 2942, Loss 0.24299222230911255\n",
      "[Training Epoch 6] Batch 2943, Loss 0.2534583508968353\n",
      "[Training Epoch 6] Batch 2944, Loss 0.22664541006088257\n",
      "[Training Epoch 6] Batch 2945, Loss 0.2681906819343567\n",
      "[Training Epoch 6] Batch 2946, Loss 0.2926705777645111\n",
      "[Training Epoch 6] Batch 2947, Loss 0.24685338139533997\n",
      "[Training Epoch 6] Batch 2948, Loss 0.2451092004776001\n",
      "[Training Epoch 6] Batch 2949, Loss 0.2605891823768616\n",
      "[Training Epoch 6] Batch 2950, Loss 0.24595198035240173\n",
      "[Training Epoch 6] Batch 2951, Loss 0.2581772208213806\n",
      "[Training Epoch 6] Batch 2952, Loss 0.2626187801361084\n",
      "[Training Epoch 6] Batch 2953, Loss 0.27289193868637085\n",
      "[Training Epoch 6] Batch 2954, Loss 0.23873645067214966\n",
      "[Training Epoch 6] Batch 2955, Loss 0.2813637852668762\n",
      "[Training Epoch 6] Batch 2956, Loss 0.24842055141925812\n",
      "[Training Epoch 6] Batch 2957, Loss 0.2696484327316284\n",
      "[Training Epoch 6] Batch 2958, Loss 0.25016501545906067\n",
      "[Training Epoch 6] Batch 2959, Loss 0.24220436811447144\n",
      "[Training Epoch 6] Batch 2960, Loss 0.23894107341766357\n",
      "[Training Epoch 6] Batch 2961, Loss 0.28207331895828247\n",
      "[Training Epoch 6] Batch 2962, Loss 0.27264946699142456\n",
      "[Training Epoch 6] Batch 2963, Loss 0.2749628722667694\n",
      "[Training Epoch 6] Batch 2964, Loss 0.2361888289451599\n",
      "[Training Epoch 6] Batch 2965, Loss 0.2763846218585968\n",
      "[Training Epoch 6] Batch 2966, Loss 0.2540379762649536\n",
      "[Training Epoch 6] Batch 2967, Loss 0.24428483843803406\n",
      "[Training Epoch 6] Batch 2968, Loss 0.26063650846481323\n",
      "[Training Epoch 6] Batch 2969, Loss 0.2627384662628174\n",
      "[Training Epoch 6] Batch 2970, Loss 0.2670612037181854\n",
      "[Training Epoch 6] Batch 2971, Loss 0.24627462029457092\n",
      "[Training Epoch 6] Batch 2972, Loss 0.2840672433376312\n",
      "[Training Epoch 6] Batch 2973, Loss 0.2644294500350952\n",
      "[Training Epoch 6] Batch 2974, Loss 0.2692639231681824\n",
      "[Training Epoch 6] Batch 2975, Loss 0.25738486647605896\n",
      "[Training Epoch 6] Batch 2976, Loss 0.25356799364089966\n",
      "[Training Epoch 6] Batch 2977, Loss 0.2608526349067688\n",
      "[Training Epoch 6] Batch 2978, Loss 0.2515387535095215\n",
      "[Training Epoch 6] Batch 2979, Loss 0.26266565918922424\n",
      "[Training Epoch 6] Batch 2980, Loss 0.28722333908081055\n",
      "[Training Epoch 6] Batch 2981, Loss 0.25093722343444824\n",
      "[Training Epoch 6] Batch 2982, Loss 0.2481948435306549\n",
      "[Training Epoch 6] Batch 2983, Loss 0.2706199586391449\n",
      "[Training Epoch 6] Batch 2984, Loss 0.252383291721344\n",
      "[Training Epoch 6] Batch 2985, Loss 0.25700345635414124\n",
      "[Training Epoch 6] Batch 2986, Loss 0.24557450413703918\n",
      "[Training Epoch 6] Batch 2987, Loss 0.2507687211036682\n",
      "[Training Epoch 6] Batch 2988, Loss 0.2803959250450134\n",
      "[Training Epoch 6] Batch 2989, Loss 0.23586218059062958\n",
      "[Training Epoch 6] Batch 2990, Loss 0.243005633354187\n",
      "[Training Epoch 6] Batch 2991, Loss 0.2667700946331024\n",
      "[Training Epoch 6] Batch 2992, Loss 0.23337742686271667\n",
      "[Training Epoch 6] Batch 2993, Loss 0.2583885192871094\n",
      "[Training Epoch 6] Batch 2994, Loss 0.2727319598197937\n",
      "[Training Epoch 6] Batch 2995, Loss 0.24866747856140137\n",
      "[Training Epoch 6] Batch 2996, Loss 0.21463489532470703\n",
      "[Training Epoch 6] Batch 2997, Loss 0.2672128975391388\n",
      "[Training Epoch 6] Batch 2998, Loss 0.2585567235946655\n",
      "[Training Epoch 6] Batch 2999, Loss 0.24369633197784424\n",
      "[Training Epoch 6] Batch 3000, Loss 0.2402268499135971\n",
      "[Training Epoch 6] Batch 3001, Loss 0.2592542767524719\n",
      "[Training Epoch 6] Batch 3002, Loss 0.23178113996982574\n",
      "[Training Epoch 6] Batch 3003, Loss 0.24055320024490356\n",
      "[Training Epoch 6] Batch 3004, Loss 0.2452182173728943\n",
      "[Training Epoch 6] Batch 3005, Loss 0.2572610378265381\n",
      "[Training Epoch 6] Batch 3006, Loss 0.2731706500053406\n",
      "[Training Epoch 6] Batch 3007, Loss 0.2626315653324127\n",
      "[Training Epoch 6] Batch 3008, Loss 0.2591652274131775\n",
      "[Training Epoch 6] Batch 3009, Loss 0.2629091441631317\n",
      "[Training Epoch 6] Batch 3010, Loss 0.2673308849334717\n",
      "[Training Epoch 6] Batch 3011, Loss 0.2728569507598877\n",
      "[Training Epoch 6] Batch 3012, Loss 0.27656152844429016\n",
      "[Training Epoch 6] Batch 3013, Loss 0.2699696123600006\n",
      "[Training Epoch 6] Batch 3014, Loss 0.2726207971572876\n",
      "[Training Epoch 6] Batch 3015, Loss 0.2638542950153351\n",
      "[Training Epoch 6] Batch 3016, Loss 0.3067435622215271\n",
      "[Training Epoch 6] Batch 3017, Loss 0.26030784845352173\n",
      "[Training Epoch 6] Batch 3018, Loss 0.25154030323028564\n",
      "[Training Epoch 6] Batch 3019, Loss 0.2250671088695526\n",
      "[Training Epoch 6] Batch 3020, Loss 0.24792318046092987\n",
      "[Training Epoch 6] Batch 3021, Loss 0.23998931050300598\n",
      "[Training Epoch 6] Batch 3022, Loss 0.23044952750205994\n",
      "[Training Epoch 6] Batch 3023, Loss 0.28819793462753296\n",
      "[Training Epoch 6] Batch 3024, Loss 0.24965913593769073\n",
      "[Training Epoch 6] Batch 3025, Loss 0.25050050020217896\n",
      "[Training Epoch 6] Batch 3026, Loss 0.2778434753417969\n",
      "[Training Epoch 6] Batch 3027, Loss 0.23397192358970642\n",
      "[Training Epoch 6] Batch 3028, Loss 0.2830333709716797\n",
      "[Training Epoch 6] Batch 3029, Loss 0.24717803299427032\n",
      "[Training Epoch 6] Batch 3030, Loss 0.2611866593360901\n",
      "[Training Epoch 6] Batch 3031, Loss 0.2774586081504822\n",
      "[Training Epoch 6] Batch 3032, Loss 0.2636995017528534\n",
      "[Training Epoch 6] Batch 3033, Loss 0.20144090056419373\n",
      "[Training Epoch 6] Batch 3034, Loss 0.2755298912525177\n",
      "[Training Epoch 6] Batch 3035, Loss 0.27723217010498047\n",
      "[Training Epoch 6] Batch 3036, Loss 0.23592476546764374\n",
      "[Training Epoch 6] Batch 3037, Loss 0.26535892486572266\n",
      "[Training Epoch 6] Batch 3038, Loss 0.2740209698677063\n",
      "[Training Epoch 6] Batch 3039, Loss 0.26449427008628845\n",
      "[Training Epoch 6] Batch 3040, Loss 0.2700246274471283\n",
      "[Training Epoch 6] Batch 3041, Loss 0.2828574776649475\n",
      "[Training Epoch 6] Batch 3042, Loss 0.2540737986564636\n",
      "[Training Epoch 6] Batch 3043, Loss 0.23937323689460754\n",
      "[Training Epoch 6] Batch 3044, Loss 0.27311480045318604\n",
      "[Training Epoch 6] Batch 3045, Loss 0.28671586513519287\n",
      "[Training Epoch 6] Batch 3046, Loss 0.22336062788963318\n",
      "[Training Epoch 6] Batch 3047, Loss 0.26671427488327026\n",
      "[Training Epoch 6] Batch 3048, Loss 0.2936854064464569\n",
      "[Training Epoch 6] Batch 3049, Loss 0.24037908017635345\n",
      "[Training Epoch 6] Batch 3050, Loss 0.2628015875816345\n",
      "[Training Epoch 6] Batch 3051, Loss 0.27405381202697754\n",
      "[Training Epoch 6] Batch 3052, Loss 0.2997311055660248\n",
      "[Training Epoch 6] Batch 3053, Loss 0.24558179080486298\n",
      "[Training Epoch 6] Batch 3054, Loss 0.2641315460205078\n",
      "[Training Epoch 6] Batch 3055, Loss 0.2511422038078308\n",
      "[Training Epoch 6] Batch 3056, Loss 0.24253147840499878\n",
      "[Training Epoch 6] Batch 3057, Loss 0.26947933435440063\n",
      "[Training Epoch 6] Batch 3058, Loss 0.26287081837654114\n",
      "[Training Epoch 6] Batch 3059, Loss 0.2332860231399536\n",
      "[Training Epoch 6] Batch 3060, Loss 0.27064183354377747\n",
      "[Training Epoch 6] Batch 3061, Loss 0.21299594640731812\n",
      "[Training Epoch 6] Batch 3062, Loss 0.26778700947761536\n",
      "[Training Epoch 6] Batch 3063, Loss 0.2500978708267212\n",
      "[Training Epoch 6] Batch 3064, Loss 0.2972540259361267\n",
      "[Training Epoch 6] Batch 3065, Loss 0.23911838233470917\n",
      "[Training Epoch 6] Batch 3066, Loss 0.2572857141494751\n",
      "[Training Epoch 6] Batch 3067, Loss 0.2799217700958252\n",
      "[Training Epoch 6] Batch 3068, Loss 0.27041712403297424\n",
      "[Training Epoch 6] Batch 3069, Loss 0.25967472791671753\n",
      "[Training Epoch 6] Batch 3070, Loss 0.2686789631843567\n",
      "[Training Epoch 6] Batch 3071, Loss 0.26243406534194946\n",
      "[Training Epoch 6] Batch 3072, Loss 0.24402926862239838\n",
      "[Training Epoch 6] Batch 3073, Loss 0.23582594096660614\n",
      "[Training Epoch 6] Batch 3074, Loss 0.2789228856563568\n",
      "[Training Epoch 6] Batch 3075, Loss 0.24993352591991425\n",
      "[Training Epoch 6] Batch 3076, Loss 0.2622522711753845\n",
      "[Training Epoch 6] Batch 3077, Loss 0.2666177749633789\n",
      "[Training Epoch 6] Batch 3078, Loss 0.2418050765991211\n",
      "[Training Epoch 6] Batch 3079, Loss 0.24498966336250305\n",
      "[Training Epoch 6] Batch 3080, Loss 0.2538273334503174\n",
      "[Training Epoch 6] Batch 3081, Loss 0.2306957244873047\n",
      "[Training Epoch 6] Batch 3082, Loss 0.2855145335197449\n",
      "[Training Epoch 6] Batch 3083, Loss 0.24125537276268005\n",
      "[Training Epoch 6] Batch 3084, Loss 0.2279440015554428\n",
      "[Training Epoch 6] Batch 3085, Loss 0.27333229780197144\n",
      "[Training Epoch 6] Batch 3086, Loss 0.24136580526828766\n",
      "[Training Epoch 6] Batch 3087, Loss 0.2996388077735901\n",
      "[Training Epoch 6] Batch 3088, Loss 0.2678367495536804\n",
      "[Training Epoch 6] Batch 3089, Loss 0.26223844289779663\n",
      "[Training Epoch 6] Batch 3090, Loss 0.27116209268569946\n",
      "[Training Epoch 6] Batch 3091, Loss 0.28816211223602295\n",
      "[Training Epoch 6] Batch 3092, Loss 0.2504116892814636\n",
      "[Training Epoch 6] Batch 3093, Loss 0.26122719049453735\n",
      "[Training Epoch 6] Batch 3094, Loss 0.265421062707901\n",
      "[Training Epoch 6] Batch 3095, Loss 0.279243528842926\n",
      "[Training Epoch 6] Batch 3096, Loss 0.2579643428325653\n",
      "[Training Epoch 6] Batch 3097, Loss 0.26397377252578735\n",
      "[Training Epoch 6] Batch 3098, Loss 0.26980891823768616\n",
      "[Training Epoch 6] Batch 3099, Loss 0.2626187801361084\n",
      "[Training Epoch 6] Batch 3100, Loss 0.24838978052139282\n",
      "[Training Epoch 6] Batch 3101, Loss 0.27735769748687744\n",
      "[Training Epoch 6] Batch 3102, Loss 0.27318114042282104\n",
      "[Training Epoch 6] Batch 3103, Loss 0.2583893835544586\n",
      "[Training Epoch 6] Batch 3104, Loss 0.2742781341075897\n",
      "[Training Epoch 6] Batch 3105, Loss 0.2657546401023865\n",
      "[Training Epoch 6] Batch 3106, Loss 0.2767820358276367\n",
      "[Training Epoch 6] Batch 3107, Loss 0.2542544901371002\n",
      "[Training Epoch 6] Batch 3108, Loss 0.23590248823165894\n",
      "[Training Epoch 6] Batch 3109, Loss 0.25193214416503906\n",
      "[Training Epoch 6] Batch 3110, Loss 0.28250622749328613\n",
      "[Training Epoch 6] Batch 3111, Loss 0.27435600757598877\n",
      "[Training Epoch 6] Batch 3112, Loss 0.25188177824020386\n",
      "[Training Epoch 6] Batch 3113, Loss 0.2590022087097168\n",
      "[Training Epoch 6] Batch 3114, Loss 0.27042216062545776\n",
      "[Training Epoch 6] Batch 3115, Loss 0.26475951075553894\n",
      "[Training Epoch 6] Batch 3116, Loss 0.24582678079605103\n",
      "[Training Epoch 6] Batch 3117, Loss 0.2551480531692505\n",
      "[Training Epoch 6] Batch 3118, Loss 0.2731168866157532\n",
      "[Training Epoch 6] Batch 3119, Loss 0.2623104453086853\n",
      "[Training Epoch 6] Batch 3120, Loss 0.25601285696029663\n",
      "[Training Epoch 6] Batch 3121, Loss 0.2487926483154297\n",
      "[Training Epoch 6] Batch 3122, Loss 0.29489123821258545\n",
      "[Training Epoch 6] Batch 3123, Loss 0.2835272550582886\n",
      "[Training Epoch 6] Batch 3124, Loss 0.2656758725643158\n",
      "[Training Epoch 6] Batch 3125, Loss 0.2409813106060028\n",
      "[Training Epoch 6] Batch 3126, Loss 0.2503189444541931\n",
      "[Training Epoch 6] Batch 3127, Loss 0.2594657242298126\n",
      "[Training Epoch 6] Batch 3128, Loss 0.2664656639099121\n",
      "[Training Epoch 6] Batch 3129, Loss 0.24122554063796997\n",
      "[Training Epoch 6] Batch 3130, Loss 0.24588868021965027\n",
      "[Training Epoch 6] Batch 3131, Loss 0.27146321535110474\n",
      "[Training Epoch 6] Batch 3132, Loss 0.25947311520576477\n",
      "[Training Epoch 6] Batch 3133, Loss 0.2566819489002228\n",
      "[Training Epoch 6] Batch 3134, Loss 0.275420218706131\n",
      "[Training Epoch 6] Batch 3135, Loss 0.2664283514022827\n",
      "[Training Epoch 6] Batch 3136, Loss 0.2810630202293396\n",
      "[Training Epoch 6] Batch 3137, Loss 0.24801906943321228\n",
      "[Training Epoch 6] Batch 3138, Loss 0.2621806263923645\n",
      "[Training Epoch 6] Batch 3139, Loss 0.2579161524772644\n",
      "[Training Epoch 6] Batch 3140, Loss 0.2453974187374115\n",
      "[Training Epoch 6] Batch 3141, Loss 0.24306170642375946\n",
      "[Training Epoch 6] Batch 3142, Loss 0.26933252811431885\n",
      "[Training Epoch 6] Batch 3143, Loss 0.283754825592041\n",
      "[Training Epoch 6] Batch 3144, Loss 0.2295374572277069\n",
      "[Training Epoch 6] Batch 3145, Loss 0.24866996705532074\n",
      "[Training Epoch 6] Batch 3146, Loss 0.231078639626503\n",
      "[Training Epoch 6] Batch 3147, Loss 0.24508120119571686\n",
      "[Training Epoch 6] Batch 3148, Loss 0.27059394121170044\n",
      "[Training Epoch 6] Batch 3149, Loss 0.27134859561920166\n",
      "[Training Epoch 6] Batch 3150, Loss 0.2444830685853958\n",
      "[Training Epoch 6] Batch 3151, Loss 0.24492403864860535\n",
      "[Training Epoch 6] Batch 3152, Loss 0.28614625334739685\n",
      "[Training Epoch 6] Batch 3153, Loss 0.2607698440551758\n",
      "[Training Epoch 6] Batch 3154, Loss 0.26114463806152344\n",
      "[Training Epoch 6] Batch 3155, Loss 0.2782953679561615\n",
      "[Training Epoch 6] Batch 3156, Loss 0.22164595127105713\n",
      "[Training Epoch 6] Batch 3157, Loss 0.26461488008499146\n",
      "[Training Epoch 6] Batch 3158, Loss 0.270023375749588\n",
      "[Training Epoch 6] Batch 3159, Loss 0.2454322874546051\n",
      "[Training Epoch 6] Batch 3160, Loss 0.28771233558654785\n",
      "[Training Epoch 6] Batch 3161, Loss 0.2805669903755188\n",
      "[Training Epoch 6] Batch 3162, Loss 0.26227301359176636\n",
      "[Training Epoch 6] Batch 3163, Loss 0.28249770402908325\n",
      "[Training Epoch 6] Batch 3164, Loss 0.28246474266052246\n",
      "[Training Epoch 6] Batch 3165, Loss 0.23531484603881836\n",
      "[Training Epoch 6] Batch 3166, Loss 0.28030622005462646\n",
      "[Training Epoch 6] Batch 3167, Loss 0.26785513758659363\n",
      "[Training Epoch 6] Batch 3168, Loss 0.2615622878074646\n",
      "[Training Epoch 6] Batch 3169, Loss 0.2523448169231415\n",
      "[Training Epoch 6] Batch 3170, Loss 0.25742560625076294\n",
      "[Training Epoch 6] Batch 3171, Loss 0.2750438153743744\n",
      "[Training Epoch 6] Batch 3172, Loss 0.30873000621795654\n",
      "[Training Epoch 6] Batch 3173, Loss 0.24606873095035553\n",
      "[Training Epoch 6] Batch 3174, Loss 0.27354180812835693\n",
      "[Training Epoch 6] Batch 3175, Loss 0.25730466842651367\n",
      "[Training Epoch 6] Batch 3176, Loss 0.25525254011154175\n",
      "[Training Epoch 6] Batch 3177, Loss 0.25381767749786377\n",
      "[Training Epoch 6] Batch 3178, Loss 0.24736632406711578\n",
      "[Training Epoch 6] Batch 3179, Loss 0.22054535150527954\n",
      "[Training Epoch 6] Batch 3180, Loss 0.2844993472099304\n",
      "[Training Epoch 6] Batch 3181, Loss 0.26057168841362\n",
      "[Training Epoch 6] Batch 3182, Loss 0.24958059191703796\n",
      "[Training Epoch 6] Batch 3183, Loss 0.27418583631515503\n",
      "[Training Epoch 6] Batch 3184, Loss 0.24728184938430786\n",
      "[Training Epoch 6] Batch 3185, Loss 0.2684890925884247\n",
      "[Training Epoch 6] Batch 3186, Loss 0.2545250356197357\n",
      "[Training Epoch 6] Batch 3187, Loss 0.28113263845443726\n",
      "[Training Epoch 6] Batch 3188, Loss 0.22395676374435425\n",
      "[Training Epoch 6] Batch 3189, Loss 0.26400381326675415\n",
      "[Training Epoch 6] Batch 3190, Loss 0.24364785850048065\n",
      "[Training Epoch 6] Batch 3191, Loss 0.243065744638443\n",
      "[Training Epoch 6] Batch 3192, Loss 0.2606959640979767\n",
      "[Training Epoch 6] Batch 3193, Loss 0.27469271421432495\n",
      "[Training Epoch 6] Batch 3194, Loss 0.2967184782028198\n",
      "[Training Epoch 6] Batch 3195, Loss 0.2738248109817505\n",
      "[Training Epoch 6] Batch 3196, Loss 0.25596699118614197\n",
      "[Training Epoch 6] Batch 3197, Loss 0.23704946041107178\n",
      "[Training Epoch 6] Batch 3198, Loss 0.25057560205459595\n",
      "[Training Epoch 6] Batch 3199, Loss 0.26898545026779175\n",
      "[Training Epoch 6] Batch 3200, Loss 0.24946662783622742\n",
      "[Training Epoch 6] Batch 3201, Loss 0.2479029893875122\n",
      "[Training Epoch 6] Batch 3202, Loss 0.2594350576400757\n",
      "[Training Epoch 6] Batch 3203, Loss 0.23244456946849823\n",
      "[Training Epoch 6] Batch 3204, Loss 0.2546505928039551\n",
      "[Training Epoch 6] Batch 3205, Loss 0.24242639541625977\n",
      "[Training Epoch 6] Batch 3206, Loss 0.2705753445625305\n",
      "[Training Epoch 6] Batch 3207, Loss 0.26087141036987305\n",
      "[Training Epoch 6] Batch 3208, Loss 0.23959651589393616\n",
      "[Training Epoch 6] Batch 3209, Loss 0.25796106457710266\n",
      "[Training Epoch 6] Batch 3210, Loss 0.2614712715148926\n",
      "[Training Epoch 6] Batch 3211, Loss 0.2476629763841629\n",
      "[Training Epoch 6] Batch 3212, Loss 0.24817036092281342\n",
      "[Training Epoch 6] Batch 3213, Loss 0.273375540971756\n",
      "[Training Epoch 6] Batch 3214, Loss 0.25428444147109985\n",
      "[Training Epoch 6] Batch 3215, Loss 0.2666657567024231\n",
      "[Training Epoch 6] Batch 3216, Loss 0.258841872215271\n",
      "[Training Epoch 6] Batch 3217, Loss 0.2966969907283783\n",
      "[Training Epoch 6] Batch 3218, Loss 0.2613307535648346\n",
      "[Training Epoch 6] Batch 3219, Loss 0.2559903860092163\n",
      "[Training Epoch 6] Batch 3220, Loss 0.28237462043762207\n",
      "[Training Epoch 6] Batch 3221, Loss 0.24538743495941162\n",
      "[Training Epoch 6] Batch 3222, Loss 0.25628888607025146\n",
      "[Training Epoch 6] Batch 3223, Loss 0.24272224307060242\n",
      "[Training Epoch 6] Batch 3224, Loss 0.24919992685317993\n",
      "[Training Epoch 6] Batch 3225, Loss 0.2783040404319763\n",
      "[Training Epoch 6] Batch 3226, Loss 0.24781270325183868\n",
      "[Training Epoch 6] Batch 3227, Loss 0.23425811529159546\n",
      "[Training Epoch 6] Batch 3228, Loss 0.2618104815483093\n",
      "[Training Epoch 6] Batch 3229, Loss 0.24599067866802216\n",
      "[Training Epoch 6] Batch 3230, Loss 0.26898321509361267\n",
      "[Training Epoch 6] Batch 3231, Loss 0.25085946917533875\n",
      "[Training Epoch 6] Batch 3232, Loss 0.2482423633337021\n",
      "[Training Epoch 6] Batch 3233, Loss 0.24548324942588806\n",
      "[Training Epoch 6] Batch 3234, Loss 0.2385953962802887\n",
      "[Training Epoch 6] Batch 3235, Loss 0.25611671805381775\n",
      "[Training Epoch 6] Batch 3236, Loss 0.27892887592315674\n",
      "[Training Epoch 6] Batch 3237, Loss 0.26896053552627563\n",
      "[Training Epoch 6] Batch 3238, Loss 0.25820642709732056\n",
      "[Training Epoch 6] Batch 3239, Loss 0.2617952525615692\n",
      "[Training Epoch 6] Batch 3240, Loss 0.2869582772254944\n",
      "[Training Epoch 6] Batch 3241, Loss 0.23675379157066345\n",
      "[Training Epoch 6] Batch 3242, Loss 0.25502246618270874\n",
      "[Training Epoch 6] Batch 3243, Loss 0.2593876123428345\n",
      "[Training Epoch 6] Batch 3244, Loss 0.23634907603263855\n",
      "[Training Epoch 6] Batch 3245, Loss 0.24009504914283752\n",
      "[Training Epoch 6] Batch 3246, Loss 0.27037426829338074\n",
      "[Training Epoch 6] Batch 3247, Loss 0.25538718700408936\n",
      "[Training Epoch 6] Batch 3248, Loss 0.24003496766090393\n",
      "[Training Epoch 6] Batch 3249, Loss 0.24954409897327423\n",
      "[Training Epoch 6] Batch 3250, Loss 0.26565003395080566\n",
      "[Training Epoch 6] Batch 3251, Loss 0.24313132464885712\n",
      "[Training Epoch 6] Batch 3252, Loss 0.271243155002594\n",
      "[Training Epoch 6] Batch 3253, Loss 0.2756825089454651\n",
      "[Training Epoch 6] Batch 3254, Loss 0.2529679536819458\n",
      "[Training Epoch 6] Batch 3255, Loss 0.25492042303085327\n",
      "[Training Epoch 6] Batch 3256, Loss 0.24640019237995148\n",
      "[Training Epoch 6] Batch 3257, Loss 0.26185905933380127\n",
      "[Training Epoch 6] Batch 3258, Loss 0.24111169576644897\n",
      "[Training Epoch 6] Batch 3259, Loss 0.2499237358570099\n",
      "[Training Epoch 6] Batch 3260, Loss 0.2561756670475006\n",
      "[Training Epoch 6] Batch 3261, Loss 0.2746928930282593\n",
      "[Training Epoch 6] Batch 3262, Loss 0.25179529190063477\n",
      "[Training Epoch 6] Batch 3263, Loss 0.24009959399700165\n",
      "[Training Epoch 6] Batch 3264, Loss 0.28175532817840576\n",
      "[Training Epoch 6] Batch 3265, Loss 0.25403133034706116\n",
      "[Training Epoch 6] Batch 3266, Loss 0.2482653260231018\n",
      "[Training Epoch 6] Batch 3267, Loss 0.2925978899002075\n",
      "[Training Epoch 6] Batch 3268, Loss 0.25333529710769653\n",
      "[Training Epoch 6] Batch 3269, Loss 0.2440810203552246\n",
      "[Training Epoch 6] Batch 3270, Loss 0.24953068792819977\n",
      "[Training Epoch 6] Batch 3271, Loss 0.22869712114334106\n",
      "[Training Epoch 6] Batch 3272, Loss 0.24814924597740173\n",
      "[Training Epoch 6] Batch 3273, Loss 0.28945091366767883\n",
      "[Training Epoch 6] Batch 3274, Loss 0.2449568808078766\n",
      "[Training Epoch 6] Batch 3275, Loss 0.22258274257183075\n",
      "[Training Epoch 6] Batch 3276, Loss 0.27479028701782227\n",
      "[Training Epoch 6] Batch 3277, Loss 0.2833440899848938\n",
      "[Training Epoch 6] Batch 3278, Loss 0.2948927581310272\n",
      "[Training Epoch 6] Batch 3279, Loss 0.23400163650512695\n",
      "[Training Epoch 6] Batch 3280, Loss 0.2334645539522171\n",
      "[Training Epoch 6] Batch 3281, Loss 0.2842111587524414\n",
      "[Training Epoch 6] Batch 3282, Loss 0.27312028408050537\n",
      "[Training Epoch 6] Batch 3283, Loss 0.255484402179718\n",
      "[Training Epoch 6] Batch 3284, Loss 0.25101158022880554\n",
      "[Training Epoch 6] Batch 3285, Loss 0.2632278800010681\n",
      "[Training Epoch 6] Batch 3286, Loss 0.2573336362838745\n",
      "[Training Epoch 6] Batch 3287, Loss 0.2261454313993454\n",
      "[Training Epoch 6] Batch 3288, Loss 0.26142823696136475\n",
      "[Training Epoch 6] Batch 3289, Loss 0.268697053194046\n",
      "[Training Epoch 6] Batch 3290, Loss 0.22730162739753723\n",
      "[Training Epoch 6] Batch 3291, Loss 0.25957852602005005\n",
      "[Training Epoch 6] Batch 3292, Loss 0.25148898363113403\n",
      "[Training Epoch 6] Batch 3293, Loss 0.25994840264320374\n",
      "[Training Epoch 6] Batch 3294, Loss 0.24920020997524261\n",
      "[Training Epoch 6] Batch 3295, Loss 0.2501470446586609\n",
      "[Training Epoch 6] Batch 3296, Loss 0.23147954046726227\n",
      "[Training Epoch 6] Batch 3297, Loss 0.24092935025691986\n",
      "[Training Epoch 6] Batch 3298, Loss 0.2466646134853363\n",
      "[Training Epoch 6] Batch 3299, Loss 0.2474660873413086\n",
      "[Training Epoch 6] Batch 3300, Loss 0.22320248186588287\n",
      "[Training Epoch 6] Batch 3301, Loss 0.2497091144323349\n",
      "[Training Epoch 6] Batch 3302, Loss 0.26068904995918274\n",
      "[Training Epoch 6] Batch 3303, Loss 0.26083904504776\n",
      "[Training Epoch 6] Batch 3304, Loss 0.26635730266571045\n",
      "[Training Epoch 6] Batch 3305, Loss 0.24086523056030273\n",
      "[Training Epoch 6] Batch 3306, Loss 0.2613094449043274\n",
      "[Training Epoch 6] Batch 3307, Loss 0.2335561215877533\n",
      "[Training Epoch 6] Batch 3308, Loss 0.2666736841201782\n",
      "[Training Epoch 6] Batch 3309, Loss 0.25069230794906616\n",
      "[Training Epoch 6] Batch 3310, Loss 0.2692629098892212\n",
      "[Training Epoch 6] Batch 3311, Loss 0.2547709345817566\n",
      "[Training Epoch 6] Batch 3312, Loss 0.26229649782180786\n",
      "[Training Epoch 6] Batch 3313, Loss 0.2724994421005249\n",
      "[Training Epoch 6] Batch 3314, Loss 0.26560115814208984\n",
      "[Training Epoch 6] Batch 3315, Loss 0.26668623089790344\n",
      "[Training Epoch 6] Batch 3316, Loss 0.2643270194530487\n",
      "[Training Epoch 6] Batch 3317, Loss 0.2654845118522644\n",
      "[Training Epoch 6] Batch 3318, Loss 0.28031623363494873\n",
      "[Training Epoch 6] Batch 3319, Loss 0.2438640147447586\n",
      "[Training Epoch 6] Batch 3320, Loss 0.2890033721923828\n",
      "[Training Epoch 6] Batch 3321, Loss 0.24717339873313904\n",
      "[Training Epoch 6] Batch 3322, Loss 0.24804776906967163\n",
      "[Training Epoch 6] Batch 3323, Loss 0.2769157290458679\n",
      "[Training Epoch 6] Batch 3324, Loss 0.26577645540237427\n",
      "[Training Epoch 6] Batch 3325, Loss 0.24723756313323975\n",
      "[Training Epoch 6] Batch 3326, Loss 0.257079541683197\n",
      "[Training Epoch 6] Batch 3327, Loss 0.26524144411087036\n",
      "[Training Epoch 6] Batch 3328, Loss 0.28278857469558716\n",
      "[Training Epoch 6] Batch 3329, Loss 0.23482894897460938\n",
      "[Training Epoch 6] Batch 3330, Loss 0.24658161401748657\n",
      "[Training Epoch 6] Batch 3331, Loss 0.2649931311607361\n",
      "[Training Epoch 6] Batch 3332, Loss 0.2489531934261322\n",
      "[Training Epoch 6] Batch 3333, Loss 0.2702344059944153\n",
      "[Training Epoch 6] Batch 3334, Loss 0.24172252416610718\n",
      "[Training Epoch 6] Batch 3335, Loss 0.3021093010902405\n",
      "[Training Epoch 6] Batch 3336, Loss 0.24336731433868408\n",
      "[Training Epoch 6] Batch 3337, Loss 0.25087136030197144\n",
      "[Training Epoch 6] Batch 3338, Loss 0.26706525683403015\n",
      "[Training Epoch 6] Batch 3339, Loss 0.30258458852767944\n",
      "[Training Epoch 6] Batch 3340, Loss 0.2742255926132202\n",
      "[Training Epoch 6] Batch 3341, Loss 0.25802892446517944\n",
      "[Training Epoch 6] Batch 3342, Loss 0.23037858307361603\n",
      "[Training Epoch 6] Batch 3343, Loss 0.2580938935279846\n",
      "[Training Epoch 6] Batch 3344, Loss 0.2846294343471527\n",
      "[Training Epoch 6] Batch 3345, Loss 0.2615295648574829\n",
      "[Training Epoch 6] Batch 3346, Loss 0.2452690303325653\n",
      "[Training Epoch 6] Batch 3347, Loss 0.26802173256874084\n",
      "[Training Epoch 6] Batch 3348, Loss 0.25603073835372925\n",
      "[Training Epoch 6] Batch 3349, Loss 0.24842119216918945\n",
      "[Training Epoch 6] Batch 3350, Loss 0.25501060485839844\n",
      "[Training Epoch 6] Batch 3351, Loss 0.27123939990997314\n",
      "[Training Epoch 6] Batch 3352, Loss 0.26321089267730713\n",
      "[Training Epoch 6] Batch 3353, Loss 0.26633864641189575\n",
      "[Training Epoch 6] Batch 3354, Loss 0.2611442506313324\n",
      "[Training Epoch 6] Batch 3355, Loss 0.2774289846420288\n",
      "[Training Epoch 6] Batch 3356, Loss 0.268571674823761\n",
      "[Training Epoch 6] Batch 3357, Loss 0.2525600790977478\n",
      "[Training Epoch 6] Batch 3358, Loss 0.26024067401885986\n",
      "[Training Epoch 6] Batch 3359, Loss 0.24802395701408386\n",
      "[Training Epoch 6] Batch 3360, Loss 0.2811361253261566\n",
      "[Training Epoch 6] Batch 3361, Loss 0.25773268938064575\n",
      "[Training Epoch 6] Batch 3362, Loss 0.27867597341537476\n",
      "[Training Epoch 6] Batch 3363, Loss 0.2850777506828308\n",
      "[Training Epoch 6] Batch 3364, Loss 0.27801427245140076\n",
      "[Training Epoch 6] Batch 3365, Loss 0.2267482727766037\n",
      "[Training Epoch 6] Batch 3366, Loss 0.27027279138565063\n",
      "[Training Epoch 6] Batch 3367, Loss 0.2694449722766876\n",
      "[Training Epoch 6] Batch 3368, Loss 0.2424352616071701\n",
      "[Training Epoch 6] Batch 3369, Loss 0.2425445318222046\n",
      "[Training Epoch 6] Batch 3370, Loss 0.2878859043121338\n",
      "[Training Epoch 6] Batch 3371, Loss 0.2792731821537018\n",
      "[Training Epoch 6] Batch 3372, Loss 0.2532052993774414\n",
      "[Training Epoch 6] Batch 3373, Loss 0.2411803901195526\n",
      "[Training Epoch 6] Batch 3374, Loss 0.2664507031440735\n",
      "[Training Epoch 6] Batch 3375, Loss 0.2708337604999542\n",
      "[Training Epoch 6] Batch 3376, Loss 0.2579306364059448\n",
      "[Training Epoch 6] Batch 3377, Loss 0.25372114777565\n",
      "[Training Epoch 6] Batch 3378, Loss 0.2682900130748749\n",
      "[Training Epoch 6] Batch 3379, Loss 0.25453686714172363\n",
      "[Training Epoch 6] Batch 3380, Loss 0.24113352596759796\n",
      "[Training Epoch 6] Batch 3381, Loss 0.24824577569961548\n",
      "[Training Epoch 6] Batch 3382, Loss 0.26895037293434143\n",
      "[Training Epoch 6] Batch 3383, Loss 0.23613111674785614\n",
      "[Training Epoch 6] Batch 3384, Loss 0.2610102891921997\n",
      "[Training Epoch 6] Batch 3385, Loss 0.2722870111465454\n",
      "[Training Epoch 6] Batch 3386, Loss 0.2732403874397278\n",
      "[Training Epoch 6] Batch 3387, Loss 0.23656611144542694\n",
      "[Training Epoch 6] Batch 3388, Loss 0.24621692299842834\n",
      "[Training Epoch 6] Batch 3389, Loss 0.2575428783893585\n",
      "[Training Epoch 6] Batch 3390, Loss 0.26084989309310913\n",
      "[Training Epoch 6] Batch 3391, Loss 0.2564384341239929\n",
      "[Training Epoch 6] Batch 3392, Loss 0.24067911505699158\n",
      "[Training Epoch 6] Batch 3393, Loss 0.25197556614875793\n",
      "[Training Epoch 6] Batch 3394, Loss 0.266570508480072\n",
      "[Training Epoch 6] Batch 3395, Loss 0.2518072724342346\n",
      "[Training Epoch 6] Batch 3396, Loss 0.27249154448509216\n",
      "[Training Epoch 6] Batch 3397, Loss 0.25742965936660767\n",
      "[Training Epoch 6] Batch 3398, Loss 0.2723758816719055\n",
      "[Training Epoch 6] Batch 3399, Loss 0.2647966146469116\n",
      "[Training Epoch 6] Batch 3400, Loss 0.24092736840248108\n",
      "[Training Epoch 6] Batch 3401, Loss 0.26761966943740845\n",
      "[Training Epoch 6] Batch 3402, Loss 0.23578044772148132\n",
      "[Training Epoch 6] Batch 3403, Loss 0.24821045994758606\n",
      "[Training Epoch 6] Batch 3404, Loss 0.2518371045589447\n",
      "[Training Epoch 6] Batch 3405, Loss 0.2617712616920471\n",
      "[Training Epoch 6] Batch 3406, Loss 0.26739364862442017\n",
      "[Training Epoch 6] Batch 3407, Loss 0.2453136295080185\n",
      "[Training Epoch 6] Batch 3408, Loss 0.24124765396118164\n",
      "[Training Epoch 6] Batch 3409, Loss 0.2936229705810547\n",
      "[Training Epoch 6] Batch 3410, Loss 0.2711793780326843\n",
      "[Training Epoch 6] Batch 3411, Loss 0.22751137614250183\n",
      "[Training Epoch 6] Batch 3412, Loss 0.23963439464569092\n",
      "[Training Epoch 6] Batch 3413, Loss 0.22902822494506836\n",
      "[Training Epoch 6] Batch 3414, Loss 0.24844709038734436\n",
      "[Training Epoch 6] Batch 3415, Loss 0.23804205656051636\n",
      "[Training Epoch 6] Batch 3416, Loss 0.2608660161495209\n",
      "[Training Epoch 6] Batch 3417, Loss 0.2661726474761963\n",
      "[Training Epoch 6] Batch 3418, Loss 0.25921279191970825\n",
      "[Training Epoch 6] Batch 3419, Loss 0.24230240285396576\n",
      "[Training Epoch 6] Batch 3420, Loss 0.24599742889404297\n",
      "[Training Epoch 6] Batch 3421, Loss 0.26017481088638306\n",
      "[Training Epoch 6] Batch 3422, Loss 0.27332884073257446\n",
      "[Training Epoch 6] Batch 3423, Loss 0.2745562791824341\n",
      "[Training Epoch 6] Batch 3424, Loss 0.28271883726119995\n",
      "[Training Epoch 6] Batch 3425, Loss 0.24670523405075073\n",
      "[Training Epoch 6] Batch 3426, Loss 0.2678302526473999\n",
      "[Training Epoch 6] Batch 3427, Loss 0.26618534326553345\n",
      "[Training Epoch 6] Batch 3428, Loss 0.27256885170936584\n",
      "[Training Epoch 6] Batch 3429, Loss 0.2611391842365265\n",
      "[Training Epoch 6] Batch 3430, Loss 0.2786005139350891\n",
      "[Training Epoch 6] Batch 3431, Loss 0.23260825872421265\n",
      "[Training Epoch 6] Batch 3432, Loss 0.28532740473747253\n",
      "[Training Epoch 6] Batch 3433, Loss 0.25135037302970886\n",
      "[Training Epoch 6] Batch 3434, Loss 0.2358093112707138\n",
      "[Training Epoch 6] Batch 3435, Loss 0.24606245756149292\n",
      "[Training Epoch 6] Batch 3436, Loss 0.2658262848854065\n",
      "[Training Epoch 6] Batch 3437, Loss 0.2911945581436157\n",
      "[Training Epoch 6] Batch 3438, Loss 0.2648712396621704\n",
      "[Training Epoch 6] Batch 3439, Loss 0.27978426218032837\n",
      "[Training Epoch 6] Batch 3440, Loss 0.277754545211792\n",
      "[Training Epoch 6] Batch 3441, Loss 0.23716604709625244\n",
      "[Training Epoch 6] Batch 3442, Loss 0.24226872622966766\n",
      "[Training Epoch 6] Batch 3443, Loss 0.23822739720344543\n",
      "[Training Epoch 6] Batch 3444, Loss 0.26574546098709106\n",
      "[Training Epoch 6] Batch 3445, Loss 0.23452654480934143\n",
      "[Training Epoch 6] Batch 3446, Loss 0.26845628023147583\n",
      "[Training Epoch 6] Batch 3447, Loss 0.2681964039802551\n",
      "[Training Epoch 6] Batch 3448, Loss 0.2443901002407074\n",
      "[Training Epoch 6] Batch 3449, Loss 0.2824752628803253\n",
      "[Training Epoch 6] Batch 3450, Loss 0.24628213047981262\n",
      "[Training Epoch 6] Batch 3451, Loss 0.2329452633857727\n",
      "[Training Epoch 6] Batch 3452, Loss 0.2890397906303406\n",
      "[Training Epoch 6] Batch 3453, Loss 0.25124216079711914\n",
      "[Training Epoch 6] Batch 3454, Loss 0.2624123692512512\n",
      "[Training Epoch 6] Batch 3455, Loss 0.23717975616455078\n",
      "[Training Epoch 6] Batch 3456, Loss 0.25399982929229736\n",
      "[Training Epoch 6] Batch 3457, Loss 0.30044054985046387\n",
      "[Training Epoch 6] Batch 3458, Loss 0.25620192289352417\n",
      "[Training Epoch 6] Batch 3459, Loss 0.2360602617263794\n",
      "[Training Epoch 6] Batch 3460, Loss 0.24086016416549683\n",
      "[Training Epoch 6] Batch 3461, Loss 0.24356651306152344\n",
      "[Training Epoch 6] Batch 3462, Loss 0.25463834404945374\n",
      "[Training Epoch 6] Batch 3463, Loss 0.23128563165664673\n",
      "[Training Epoch 6] Batch 3464, Loss 0.2665747106075287\n",
      "[Training Epoch 6] Batch 3465, Loss 0.26526615023612976\n",
      "[Training Epoch 6] Batch 3466, Loss 0.24359546601772308\n",
      "[Training Epoch 6] Batch 3467, Loss 0.25933992862701416\n",
      "[Training Epoch 6] Batch 3468, Loss 0.2470507174730301\n",
      "[Training Epoch 6] Batch 3469, Loss 0.2403489351272583\n",
      "[Training Epoch 6] Batch 3470, Loss 0.25180327892303467\n",
      "[Training Epoch 6] Batch 3471, Loss 0.2495996654033661\n",
      "[Training Epoch 6] Batch 3472, Loss 0.2409386783838272\n",
      "[Training Epoch 6] Batch 3473, Loss 0.25218290090560913\n",
      "[Training Epoch 6] Batch 3474, Loss 0.243248850107193\n",
      "[Training Epoch 6] Batch 3475, Loss 0.2508130669593811\n",
      "[Training Epoch 6] Batch 3476, Loss 0.2528597116470337\n",
      "[Training Epoch 6] Batch 3477, Loss 0.29565441608428955\n",
      "[Training Epoch 6] Batch 3478, Loss 0.2593831419944763\n",
      "[Training Epoch 6] Batch 3479, Loss 0.25789332389831543\n",
      "[Training Epoch 6] Batch 3480, Loss 0.274699330329895\n",
      "[Training Epoch 6] Batch 3481, Loss 0.25681769847869873\n",
      "[Training Epoch 6] Batch 3482, Loss 0.2732713222503662\n",
      "[Training Epoch 6] Batch 3483, Loss 0.25569310784339905\n",
      "[Training Epoch 6] Batch 3484, Loss 0.2644723653793335\n",
      "[Training Epoch 6] Batch 3485, Loss 0.26414117217063904\n",
      "[Training Epoch 6] Batch 3486, Loss 0.2899132966995239\n",
      "[Training Epoch 6] Batch 3487, Loss 0.2866520285606384\n",
      "[Training Epoch 6] Batch 3488, Loss 0.25076910853385925\n",
      "[Training Epoch 6] Batch 3489, Loss 0.2714466452598572\n",
      "[Training Epoch 6] Batch 3490, Loss 0.25669294595718384\n",
      "[Training Epoch 6] Batch 3491, Loss 0.24750050902366638\n",
      "[Training Epoch 6] Batch 3492, Loss 0.248786062002182\n",
      "[Training Epoch 6] Batch 3493, Loss 0.22742265462875366\n",
      "[Training Epoch 6] Batch 3494, Loss 0.2481391280889511\n",
      "[Training Epoch 6] Batch 3495, Loss 0.25952038168907166\n",
      "[Training Epoch 6] Batch 3496, Loss 0.2943376302719116\n",
      "[Training Epoch 6] Batch 3497, Loss 0.2557169795036316\n",
      "[Training Epoch 6] Batch 3498, Loss 0.25426799058914185\n",
      "[Training Epoch 6] Batch 3499, Loss 0.2944220304489136\n",
      "[Training Epoch 6] Batch 3500, Loss 0.27560436725616455\n",
      "[Training Epoch 6] Batch 3501, Loss 0.2923286557197571\n",
      "[Training Epoch 6] Batch 3502, Loss 0.273809552192688\n",
      "[Training Epoch 6] Batch 3503, Loss 0.2607932388782501\n",
      "[Training Epoch 6] Batch 3504, Loss 0.24174663424491882\n",
      "[Training Epoch 6] Batch 3505, Loss 0.24217060208320618\n",
      "[Training Epoch 6] Batch 3506, Loss 0.2575218975543976\n",
      "[Training Epoch 6] Batch 3507, Loss 0.258798211812973\n",
      "[Training Epoch 6] Batch 3508, Loss 0.2210267335176468\n",
      "[Training Epoch 6] Batch 3509, Loss 0.2738252282142639\n",
      "[Training Epoch 6] Batch 3510, Loss 0.2785031199455261\n",
      "[Training Epoch 6] Batch 3511, Loss 0.26216256618499756\n",
      "[Training Epoch 6] Batch 3512, Loss 0.23434577882289886\n",
      "[Training Epoch 6] Batch 3513, Loss 0.2819159924983978\n",
      "[Training Epoch 6] Batch 3514, Loss 0.2609480917453766\n",
      "[Training Epoch 6] Batch 3515, Loss 0.26489901542663574\n",
      "[Training Epoch 6] Batch 3516, Loss 0.2748569846153259\n",
      "[Training Epoch 6] Batch 3517, Loss 0.27697670459747314\n",
      "[Training Epoch 6] Batch 3518, Loss 0.2539018988609314\n",
      "[Training Epoch 6] Batch 3519, Loss 0.2482820749282837\n",
      "[Training Epoch 6] Batch 3520, Loss 0.25924015045166016\n",
      "[Training Epoch 6] Batch 3521, Loss 0.24660463631153107\n",
      "[Training Epoch 6] Batch 3522, Loss 0.3011833727359772\n",
      "[Training Epoch 6] Batch 3523, Loss 0.25741058588027954\n",
      "[Training Epoch 6] Batch 3524, Loss 0.2577473223209381\n",
      "[Training Epoch 6] Batch 3525, Loss 0.24692364037036896\n",
      "[Training Epoch 6] Batch 3526, Loss 0.2397443652153015\n",
      "[Training Epoch 6] Batch 3527, Loss 0.24806097149848938\n",
      "[Training Epoch 6] Batch 3528, Loss 0.2647210955619812\n",
      "[Training Epoch 6] Batch 3529, Loss 0.24718280136585236\n",
      "[Training Epoch 6] Batch 3530, Loss 0.26705265045166016\n",
      "[Training Epoch 6] Batch 3531, Loss 0.2757703959941864\n",
      "[Training Epoch 6] Batch 3532, Loss 0.26090264320373535\n",
      "[Training Epoch 6] Batch 3533, Loss 0.25255101919174194\n",
      "[Training Epoch 6] Batch 3534, Loss 0.2546682059764862\n",
      "[Training Epoch 6] Batch 3535, Loss 0.2621987760066986\n",
      "[Training Epoch 6] Batch 3536, Loss 0.24874736368656158\n",
      "[Training Epoch 6] Batch 3537, Loss 0.2532930076122284\n",
      "[Training Epoch 6] Batch 3538, Loss 0.2654050588607788\n",
      "[Training Epoch 6] Batch 3539, Loss 0.2748626470565796\n",
      "[Training Epoch 6] Batch 3540, Loss 0.2601454555988312\n",
      "[Training Epoch 6] Batch 3541, Loss 0.2596437931060791\n",
      "[Training Epoch 6] Batch 3542, Loss 0.29665619134902954\n",
      "[Training Epoch 6] Batch 3543, Loss 0.2509005665779114\n",
      "[Training Epoch 6] Batch 3544, Loss 0.26448899507522583\n",
      "[Training Epoch 6] Batch 3545, Loss 0.23929041624069214\n",
      "[Training Epoch 6] Batch 3546, Loss 0.25682079792022705\n",
      "[Training Epoch 6] Batch 3547, Loss 0.2651446461677551\n",
      "[Training Epoch 6] Batch 3548, Loss 0.24683934450149536\n",
      "[Training Epoch 6] Batch 3549, Loss 0.2675565481185913\n",
      "[Training Epoch 6] Batch 3550, Loss 0.24005131423473358\n",
      "[Training Epoch 6] Batch 3551, Loss 0.24941536784172058\n",
      "[Training Epoch 6] Batch 3552, Loss 0.25237134099006653\n",
      "[Training Epoch 6] Batch 3553, Loss 0.24382750689983368\n",
      "[Training Epoch 6] Batch 3554, Loss 0.29216885566711426\n",
      "[Training Epoch 6] Batch 3555, Loss 0.2368907630443573\n",
      "[Training Epoch 6] Batch 3556, Loss 0.24117103219032288\n",
      "[Training Epoch 6] Batch 3557, Loss 0.26748892664909363\n",
      "[Training Epoch 6] Batch 3558, Loss 0.27416056394577026\n",
      "[Training Epoch 6] Batch 3559, Loss 0.24814942479133606\n",
      "[Training Epoch 6] Batch 3560, Loss 0.22946271300315857\n",
      "[Training Epoch 6] Batch 3561, Loss 0.2429552972316742\n",
      "[Training Epoch 6] Batch 3562, Loss 0.23466810584068298\n",
      "[Training Epoch 6] Batch 3563, Loss 0.3043617308139801\n",
      "[Training Epoch 6] Batch 3564, Loss 0.25394928455352783\n",
      "[Training Epoch 6] Batch 3565, Loss 0.27003705501556396\n",
      "[Training Epoch 6] Batch 3566, Loss 0.23380498588085175\n",
      "[Training Epoch 6] Batch 3567, Loss 0.24497340619564056\n",
      "[Training Epoch 6] Batch 3568, Loss 0.2671007215976715\n",
      "[Training Epoch 6] Batch 3569, Loss 0.259138822555542\n",
      "[Training Epoch 6] Batch 3570, Loss 0.30254340171813965\n",
      "[Training Epoch 6] Batch 3571, Loss 0.2514275014400482\n",
      "[Training Epoch 6] Batch 3572, Loss 0.27941590547561646\n",
      "[Training Epoch 6] Batch 3573, Loss 0.2610687017440796\n",
      "[Training Epoch 6] Batch 3574, Loss 0.25491464138031006\n",
      "[Training Epoch 6] Batch 3575, Loss 0.28857389092445374\n",
      "[Training Epoch 6] Batch 3576, Loss 0.23504585027694702\n",
      "[Training Epoch 6] Batch 3577, Loss 0.24710482358932495\n",
      "[Training Epoch 6] Batch 3578, Loss 0.269695520401001\n",
      "[Training Epoch 6] Batch 3579, Loss 0.24652475118637085\n",
      "[Training Epoch 6] Batch 3580, Loss 0.26254716515541077\n",
      "[Training Epoch 6] Batch 3581, Loss 0.25124529004096985\n",
      "[Training Epoch 6] Batch 3582, Loss 0.22867700457572937\n",
      "[Training Epoch 6] Batch 3583, Loss 0.23279333114624023\n",
      "[Training Epoch 6] Batch 3584, Loss 0.2460576891899109\n",
      "[Training Epoch 6] Batch 3585, Loss 0.2446330189704895\n",
      "[Training Epoch 6] Batch 3586, Loss 0.24188107252120972\n",
      "[Training Epoch 6] Batch 3587, Loss 0.2416674792766571\n",
      "[Training Epoch 6] Batch 3588, Loss 0.27616190910339355\n",
      "[Training Epoch 6] Batch 3589, Loss 0.27745071053504944\n",
      "[Training Epoch 6] Batch 3590, Loss 0.2610511779785156\n",
      "[Training Epoch 6] Batch 3591, Loss 0.2429434359073639\n",
      "[Training Epoch 6] Batch 3592, Loss 0.2693037688732147\n",
      "[Training Epoch 6] Batch 3593, Loss 0.2293941080570221\n",
      "[Training Epoch 6] Batch 3594, Loss 0.2586050033569336\n",
      "[Training Epoch 6] Batch 3595, Loss 0.27669957280158997\n",
      "[Training Epoch 6] Batch 3596, Loss 0.23714610934257507\n",
      "[Training Epoch 6] Batch 3597, Loss 0.263591468334198\n",
      "[Training Epoch 6] Batch 3598, Loss 0.2893664836883545\n",
      "[Training Epoch 6] Batch 3599, Loss 0.24293087422847748\n",
      "[Training Epoch 6] Batch 3600, Loss 0.28670650720596313\n",
      "[Training Epoch 6] Batch 3601, Loss 0.2817264497280121\n",
      "[Training Epoch 6] Batch 3602, Loss 0.26454222202301025\n",
      "[Training Epoch 6] Batch 3603, Loss 0.237260103225708\n",
      "[Training Epoch 6] Batch 3604, Loss 0.24925751984119415\n",
      "[Training Epoch 6] Batch 3605, Loss 0.3008574843406677\n",
      "[Training Epoch 6] Batch 3606, Loss 0.27737414836883545\n",
      "[Training Epoch 6] Batch 3607, Loss 0.2543625831604004\n",
      "[Training Epoch 6] Batch 3608, Loss 0.252686083316803\n",
      "[Training Epoch 6] Batch 3609, Loss 0.2548183798789978\n",
      "[Training Epoch 6] Batch 3610, Loss 0.2748837471008301\n",
      "[Training Epoch 6] Batch 3611, Loss 0.26575255393981934\n",
      "[Training Epoch 6] Batch 3612, Loss 0.25266218185424805\n",
      "[Training Epoch 6] Batch 3613, Loss 0.23881292343139648\n",
      "[Training Epoch 6] Batch 3614, Loss 0.2738444209098816\n",
      "[Training Epoch 6] Batch 3615, Loss 0.2886733412742615\n",
      "[Training Epoch 6] Batch 3616, Loss 0.24762439727783203\n",
      "[Training Epoch 6] Batch 3617, Loss 0.27879077196121216\n",
      "[Training Epoch 6] Batch 3618, Loss 0.24655024707317352\n",
      "[Training Epoch 6] Batch 3619, Loss 0.26925569772720337\n",
      "[Training Epoch 6] Batch 3620, Loss 0.2795485854148865\n",
      "[Training Epoch 6] Batch 3621, Loss 0.234579399228096\n",
      "[Training Epoch 6] Batch 3622, Loss 0.2806381583213806\n",
      "[Training Epoch 6] Batch 3623, Loss 0.2779836058616638\n",
      "[Training Epoch 6] Batch 3624, Loss 0.2695488929748535\n",
      "[Training Epoch 6] Batch 3625, Loss 0.25446557998657227\n",
      "[Training Epoch 6] Batch 3626, Loss 0.257653146982193\n",
      "[Training Epoch 6] Batch 3627, Loss 0.25624269247055054\n",
      "[Training Epoch 6] Batch 3628, Loss 0.25582268834114075\n",
      "[Training Epoch 6] Batch 3629, Loss 0.24744826555252075\n",
      "[Training Epoch 6] Batch 3630, Loss 0.2688177227973938\n",
      "[Training Epoch 6] Batch 3631, Loss 0.2642051875591278\n",
      "[Training Epoch 6] Batch 3632, Loss 0.2711279094219208\n",
      "[Training Epoch 6] Batch 3633, Loss 0.2796441316604614\n",
      "[Training Epoch 6] Batch 3634, Loss 0.23152342438697815\n",
      "[Training Epoch 6] Batch 3635, Loss 0.2530635595321655\n",
      "[Training Epoch 6] Batch 3636, Loss 0.2613319158554077\n",
      "[Training Epoch 6] Batch 3637, Loss 0.26031920313835144\n",
      "[Training Epoch 6] Batch 3638, Loss 0.28399646282196045\n",
      "[Training Epoch 6] Batch 3639, Loss 0.2934315800666809\n",
      "[Training Epoch 6] Batch 3640, Loss 0.24414587020874023\n",
      "[Training Epoch 6] Batch 3641, Loss 0.2560019791126251\n",
      "[Training Epoch 6] Batch 3642, Loss 0.26601454615592957\n",
      "[Training Epoch 6] Batch 3643, Loss 0.25748878717422485\n",
      "[Training Epoch 6] Batch 3644, Loss 0.2649085521697998\n",
      "[Training Epoch 6] Batch 3645, Loss 0.23491328954696655\n",
      "[Training Epoch 6] Batch 3646, Loss 0.2269168496131897\n",
      "[Training Epoch 6] Batch 3647, Loss 0.25962695479393005\n",
      "[Training Epoch 6] Batch 3648, Loss 0.23894286155700684\n",
      "[Training Epoch 6] Batch 3649, Loss 0.2390994280576706\n",
      "[Training Epoch 6] Batch 3650, Loss 0.2353878617286682\n",
      "[Training Epoch 6] Batch 3651, Loss 0.2887052595615387\n",
      "[Training Epoch 6] Batch 3652, Loss 0.26717549562454224\n",
      "[Training Epoch 6] Batch 3653, Loss 0.25475990772247314\n",
      "[Training Epoch 6] Batch 3654, Loss 0.24117350578308105\n",
      "[Training Epoch 6] Batch 3655, Loss 0.279391884803772\n",
      "[Training Epoch 6] Batch 3656, Loss 0.24101722240447998\n",
      "[Training Epoch 6] Batch 3657, Loss 0.24597200751304626\n",
      "[Training Epoch 6] Batch 3658, Loss 0.200764000415802\n",
      "[Training Epoch 6] Batch 3659, Loss 0.24669165909290314\n",
      "[Training Epoch 6] Batch 3660, Loss 0.2608886957168579\n",
      "[Training Epoch 6] Batch 3661, Loss 0.25663241744041443\n",
      "[Training Epoch 6] Batch 3662, Loss 0.23056621849536896\n",
      "[Training Epoch 6] Batch 3663, Loss 0.24983566999435425\n",
      "[Training Epoch 6] Batch 3664, Loss 0.2648314833641052\n",
      "[Training Epoch 6] Batch 3665, Loss 0.25498801469802856\n",
      "[Training Epoch 6] Batch 3666, Loss 0.2691163420677185\n",
      "[Training Epoch 6] Batch 3667, Loss 0.24600689113140106\n",
      "[Training Epoch 6] Batch 3668, Loss 0.28601324558258057\n",
      "[Training Epoch 6] Batch 3669, Loss 0.27016666531562805\n",
      "[Training Epoch 6] Batch 3670, Loss 0.264654278755188\n",
      "[Training Epoch 6] Batch 3671, Loss 0.2410006821155548\n",
      "[Training Epoch 6] Batch 3672, Loss 0.26006391644477844\n",
      "[Training Epoch 6] Batch 3673, Loss 0.23742616176605225\n",
      "[Training Epoch 6] Batch 3674, Loss 0.2627366781234741\n",
      "[Training Epoch 6] Batch 3675, Loss 0.23879078030586243\n",
      "[Training Epoch 6] Batch 3676, Loss 0.28803932666778564\n",
      "[Training Epoch 6] Batch 3677, Loss 0.23603859543800354\n",
      "[Training Epoch 6] Batch 3678, Loss 0.24039505422115326\n",
      "[Training Epoch 6] Batch 3679, Loss 0.23575156927108765\n",
      "[Training Epoch 6] Batch 3680, Loss 0.25382405519485474\n",
      "[Training Epoch 6] Batch 3681, Loss 0.2622317671775818\n",
      "[Training Epoch 6] Batch 3682, Loss 0.26492398977279663\n",
      "[Training Epoch 6] Batch 3683, Loss 0.25055816769599915\n",
      "[Training Epoch 6] Batch 3684, Loss 0.23174583911895752\n",
      "[Training Epoch 6] Batch 3685, Loss 0.273379385471344\n",
      "[Training Epoch 6] Batch 3686, Loss 0.24451744556427002\n",
      "[Training Epoch 6] Batch 3687, Loss 0.24585027992725372\n",
      "[Training Epoch 6] Batch 3688, Loss 0.26726797223091125\n",
      "[Training Epoch 6] Batch 3689, Loss 0.26150572299957275\n",
      "[Training Epoch 6] Batch 3690, Loss 0.25429391860961914\n",
      "[Training Epoch 6] Batch 3691, Loss 0.24329370260238647\n",
      "[Training Epoch 6] Batch 3692, Loss 0.26795268058776855\n",
      "[Training Epoch 6] Batch 3693, Loss 0.24950367212295532\n",
      "[Training Epoch 6] Batch 3694, Loss 0.2772446870803833\n",
      "[Training Epoch 6] Batch 3695, Loss 0.24692204594612122\n",
      "[Training Epoch 6] Batch 3696, Loss 0.25127169489860535\n",
      "[Training Epoch 6] Batch 3697, Loss 0.26260003447532654\n",
      "[Training Epoch 6] Batch 3698, Loss 0.26268771290779114\n",
      "[Training Epoch 6] Batch 3699, Loss 0.22737765312194824\n",
      "[Training Epoch 6] Batch 3700, Loss 0.24662262201309204\n",
      "[Training Epoch 6] Batch 3701, Loss 0.2557920217514038\n",
      "[Training Epoch 6] Batch 3702, Loss 0.27001893520355225\n",
      "[Training Epoch 6] Batch 3703, Loss 0.26110780239105225\n",
      "[Training Epoch 6] Batch 3704, Loss 0.2388451248407364\n",
      "[Training Epoch 6] Batch 3705, Loss 0.25718384981155396\n",
      "[Training Epoch 6] Batch 3706, Loss 0.2760699391365051\n",
      "[Training Epoch 6] Batch 3707, Loss 0.2456422746181488\n",
      "[Training Epoch 6] Batch 3708, Loss 0.283475786447525\n",
      "[Training Epoch 6] Batch 3709, Loss 0.24396032094955444\n",
      "[Training Epoch 6] Batch 3710, Loss 0.2535301446914673\n",
      "[Training Epoch 6] Batch 3711, Loss 0.2845916748046875\n",
      "[Training Epoch 6] Batch 3712, Loss 0.28712528944015503\n",
      "[Training Epoch 6] Batch 3713, Loss 0.27994680404663086\n",
      "[Training Epoch 6] Batch 3714, Loss 0.25226226449012756\n",
      "[Training Epoch 6] Batch 3715, Loss 0.26223623752593994\n",
      "[Training Epoch 6] Batch 3716, Loss 0.2505563497543335\n",
      "[Training Epoch 6] Batch 3717, Loss 0.2423984408378601\n",
      "[Training Epoch 6] Batch 3718, Loss 0.23743301630020142\n",
      "[Training Epoch 6] Batch 3719, Loss 0.23606401681900024\n",
      "[Training Epoch 6] Batch 3720, Loss 0.27191686630249023\n",
      "[Training Epoch 6] Batch 3721, Loss 0.2710104286670685\n",
      "[Training Epoch 6] Batch 3722, Loss 0.2778015732765198\n",
      "[Training Epoch 6] Batch 3723, Loss 0.27032989263534546\n",
      "[Training Epoch 6] Batch 3724, Loss 0.2681735157966614\n",
      "[Training Epoch 6] Batch 3725, Loss 0.2608666718006134\n",
      "[Training Epoch 6] Batch 3726, Loss 0.272793173789978\n",
      "[Training Epoch 6] Batch 3727, Loss 0.2612690329551697\n",
      "[Training Epoch 6] Batch 3728, Loss 0.2951292097568512\n",
      "[Training Epoch 6] Batch 3729, Loss 0.26878297328948975\n",
      "[Training Epoch 6] Batch 3730, Loss 0.247303768992424\n",
      "[Training Epoch 6] Batch 3731, Loss 0.24719193577766418\n",
      "[Training Epoch 6] Batch 3732, Loss 0.26537084579467773\n",
      "[Training Epoch 6] Batch 3733, Loss 0.28436577320098877\n",
      "[Training Epoch 6] Batch 3734, Loss 0.24391084909439087\n",
      "[Training Epoch 6] Batch 3735, Loss 0.2698546051979065\n",
      "[Training Epoch 6] Batch 3736, Loss 0.23970580101013184\n",
      "[Training Epoch 6] Batch 3737, Loss 0.27626314759254456\n",
      "[Training Epoch 6] Batch 3738, Loss 0.2555634677410126\n",
      "[Training Epoch 6] Batch 3739, Loss 0.24558906257152557\n",
      "[Training Epoch 6] Batch 3740, Loss 0.2545016407966614\n",
      "[Training Epoch 6] Batch 3741, Loss 0.24089491367340088\n",
      "[Training Epoch 6] Batch 3742, Loss 0.25508081912994385\n",
      "[Training Epoch 6] Batch 3743, Loss 0.25160863995552063\n",
      "[Training Epoch 6] Batch 3744, Loss 0.24854405224323273\n",
      "[Training Epoch 6] Batch 3745, Loss 0.25294989347457886\n",
      "[Training Epoch 6] Batch 3746, Loss 0.2425975501537323\n",
      "[Training Epoch 6] Batch 3747, Loss 0.24561242759227753\n",
      "[Training Epoch 6] Batch 3748, Loss 0.24869954586029053\n",
      "[Training Epoch 6] Batch 3749, Loss 0.25576794147491455\n",
      "[Training Epoch 6] Batch 3750, Loss 0.280106782913208\n",
      "[Training Epoch 6] Batch 3751, Loss 0.26405733823776245\n",
      "[Training Epoch 6] Batch 3752, Loss 0.25637489557266235\n",
      "[Training Epoch 6] Batch 3753, Loss 0.23794010281562805\n",
      "[Training Epoch 6] Batch 3754, Loss 0.2548111081123352\n",
      "[Training Epoch 6] Batch 3755, Loss 0.269950807094574\n",
      "[Training Epoch 6] Batch 3756, Loss 0.25085902214050293\n",
      "[Training Epoch 6] Batch 3757, Loss 0.24677129089832306\n",
      "[Training Epoch 6] Batch 3758, Loss 0.2615862488746643\n",
      "[Training Epoch 6] Batch 3759, Loss 0.24801751971244812\n",
      "[Training Epoch 6] Batch 3760, Loss 0.26289939880371094\n",
      "[Training Epoch 6] Batch 3761, Loss 0.2803245186805725\n",
      "[Training Epoch 6] Batch 3762, Loss 0.26225489377975464\n",
      "[Training Epoch 6] Batch 3763, Loss 0.2735041081905365\n",
      "[Training Epoch 6] Batch 3764, Loss 0.2662590742111206\n",
      "[Training Epoch 6] Batch 3765, Loss 0.2589311897754669\n",
      "[Training Epoch 6] Batch 3766, Loss 0.26514923572540283\n",
      "[Training Epoch 6] Batch 3767, Loss 0.25428226590156555\n",
      "[Training Epoch 6] Batch 3768, Loss 0.25319936871528625\n",
      "[Training Epoch 6] Batch 3769, Loss 0.24409951269626617\n",
      "[Training Epoch 6] Batch 3770, Loss 0.2670268416404724\n",
      "[Training Epoch 6] Batch 3771, Loss 0.2686874568462372\n",
      "[Training Epoch 6] Batch 3772, Loss 0.2575500011444092\n",
      "[Training Epoch 6] Batch 3773, Loss 0.28573521971702576\n",
      "[Training Epoch 6] Batch 3774, Loss 0.26898378133773804\n",
      "[Training Epoch 6] Batch 3775, Loss 0.24403810501098633\n",
      "[Training Epoch 6] Batch 3776, Loss 0.30234295129776\n",
      "[Training Epoch 6] Batch 3777, Loss 0.23875528573989868\n",
      "[Training Epoch 6] Batch 3778, Loss 0.25627511739730835\n",
      "[Training Epoch 6] Batch 3779, Loss 0.25966644287109375\n",
      "[Training Epoch 6] Batch 3780, Loss 0.251953125\n",
      "[Training Epoch 6] Batch 3781, Loss 0.27294033765792847\n",
      "[Training Epoch 6] Batch 3782, Loss 0.24383041262626648\n",
      "[Training Epoch 6] Batch 3783, Loss 0.23347657918930054\n",
      "[Training Epoch 6] Batch 3784, Loss 0.2727586627006531\n",
      "[Training Epoch 6] Batch 3785, Loss 0.26190996170043945\n",
      "[Training Epoch 6] Batch 3786, Loss 0.2664587199687958\n",
      "[Training Epoch 6] Batch 3787, Loss 0.2755865454673767\n",
      "[Training Epoch 6] Batch 3788, Loss 0.23953187465667725\n",
      "[Training Epoch 6] Batch 3789, Loss 0.2807588279247284\n",
      "[Training Epoch 6] Batch 3790, Loss 0.2744956612586975\n",
      "[Training Epoch 6] Batch 3791, Loss 0.28897666931152344\n",
      "[Training Epoch 6] Batch 3792, Loss 0.2355595976114273\n",
      "[Training Epoch 6] Batch 3793, Loss 0.26104676723480225\n",
      "[Training Epoch 6] Batch 3794, Loss 0.24455028772354126\n",
      "[Training Epoch 6] Batch 3795, Loss 0.2470424771308899\n",
      "[Training Epoch 6] Batch 3796, Loss 0.23533210158348083\n",
      "[Training Epoch 6] Batch 3797, Loss 0.2721836268901825\n",
      "[Training Epoch 6] Batch 3798, Loss 0.2502567768096924\n",
      "[Training Epoch 6] Batch 3799, Loss 0.2587643265724182\n",
      "[Training Epoch 6] Batch 3800, Loss 0.25755009055137634\n",
      "[Training Epoch 6] Batch 3801, Loss 0.25863805413246155\n",
      "[Training Epoch 6] Batch 3802, Loss 0.24344097077846527\n",
      "[Training Epoch 6] Batch 3803, Loss 0.2493872046470642\n",
      "[Training Epoch 6] Batch 3804, Loss 0.25431424379348755\n",
      "[Training Epoch 6] Batch 3805, Loss 0.25611886382102966\n",
      "[Training Epoch 6] Batch 3806, Loss 0.26773014664649963\n",
      "[Training Epoch 6] Batch 3807, Loss 0.24235929548740387\n",
      "[Training Epoch 6] Batch 3808, Loss 0.27678871154785156\n",
      "[Training Epoch 6] Batch 3809, Loss 0.25039801001548767\n",
      "[Training Epoch 6] Batch 3810, Loss 0.2555387318134308\n",
      "[Training Epoch 6] Batch 3811, Loss 0.2573712468147278\n",
      "[Training Epoch 6] Batch 3812, Loss 0.2538474202156067\n",
      "[Training Epoch 6] Batch 3813, Loss 0.24202024936676025\n",
      "[Training Epoch 6] Batch 3814, Loss 0.2581093907356262\n",
      "[Training Epoch 6] Batch 3815, Loss 0.25726601481437683\n",
      "[Training Epoch 6] Batch 3816, Loss 0.2578932046890259\n",
      "[Training Epoch 6] Batch 3817, Loss 0.2790488600730896\n",
      "[Training Epoch 6] Batch 3818, Loss 0.23789972066879272\n",
      "[Training Epoch 6] Batch 3819, Loss 0.2561683654785156\n",
      "[Training Epoch 6] Batch 3820, Loss 0.2664491534233093\n",
      "[Training Epoch 6] Batch 3821, Loss 0.23583009839057922\n",
      "[Training Epoch 6] Batch 3822, Loss 0.2530704736709595\n",
      "[Training Epoch 6] Batch 3823, Loss 0.276878297328949\n",
      "[Training Epoch 6] Batch 3824, Loss 0.25513243675231934\n",
      "[Training Epoch 6] Batch 3825, Loss 0.2826226055622101\n",
      "[Training Epoch 6] Batch 3826, Loss 0.273012638092041\n",
      "[Training Epoch 6] Batch 3827, Loss 0.2847384810447693\n",
      "[Training Epoch 6] Batch 3828, Loss 0.2675630450248718\n",
      "[Training Epoch 6] Batch 3829, Loss 0.2660036087036133\n",
      "[Training Epoch 6] Batch 3830, Loss 0.2628811001777649\n",
      "[Training Epoch 6] Batch 3831, Loss 0.26461875438690186\n",
      "[Training Epoch 6] Batch 3832, Loss 0.2782896161079407\n",
      "[Training Epoch 6] Batch 3833, Loss 0.2662346363067627\n",
      "[Training Epoch 6] Batch 3834, Loss 0.2564404308795929\n",
      "[Training Epoch 6] Batch 3835, Loss 0.2702239453792572\n",
      "[Training Epoch 6] Batch 3836, Loss 0.25331932306289673\n",
      "[Training Epoch 6] Batch 3837, Loss 0.28174394369125366\n",
      "[Training Epoch 6] Batch 3838, Loss 0.2871396839618683\n",
      "[Training Epoch 6] Batch 3839, Loss 0.26382747292518616\n",
      "[Training Epoch 6] Batch 3840, Loss 0.2568487823009491\n",
      "[Training Epoch 6] Batch 3841, Loss 0.3023066520690918\n",
      "[Training Epoch 6] Batch 3842, Loss 0.26732927560806274\n",
      "[Training Epoch 6] Batch 3843, Loss 0.2659633755683899\n",
      "[Training Epoch 6] Batch 3844, Loss 0.2700088918209076\n",
      "[Training Epoch 6] Batch 3845, Loss 0.2650410234928131\n",
      "[Training Epoch 6] Batch 3846, Loss 0.2771186828613281\n",
      "[Training Epoch 6] Batch 3847, Loss 0.2513231933116913\n",
      "[Training Epoch 6] Batch 3848, Loss 0.23732706904411316\n",
      "[Training Epoch 6] Batch 3849, Loss 0.2485021948814392\n",
      "[Training Epoch 6] Batch 3850, Loss 0.24765151739120483\n",
      "[Training Epoch 6] Batch 3851, Loss 0.26555389165878296\n",
      "[Training Epoch 6] Batch 3852, Loss 0.26389896869659424\n",
      "[Training Epoch 6] Batch 3853, Loss 0.26527026295661926\n",
      "[Training Epoch 6] Batch 3854, Loss 0.2718483805656433\n",
      "[Training Epoch 6] Batch 3855, Loss 0.24153459072113037\n",
      "[Training Epoch 6] Batch 3856, Loss 0.2649242877960205\n",
      "[Training Epoch 6] Batch 3857, Loss 0.2694386839866638\n",
      "[Training Epoch 6] Batch 3858, Loss 0.2728733718395233\n",
      "[Training Epoch 6] Batch 3859, Loss 0.2682267427444458\n",
      "[Training Epoch 6] Batch 3860, Loss 0.2768004238605499\n",
      "[Training Epoch 6] Batch 3861, Loss 0.2452944815158844\n",
      "[Training Epoch 6] Batch 3862, Loss 0.2728719413280487\n",
      "[Training Epoch 6] Batch 3863, Loss 0.26423728466033936\n",
      "[Training Epoch 6] Batch 3864, Loss 0.2710214853286743\n",
      "[Training Epoch 6] Batch 3865, Loss 0.2538253366947174\n",
      "[Training Epoch 6] Batch 3866, Loss 0.26176631450653076\n",
      "[Training Epoch 6] Batch 3867, Loss 0.2409072071313858\n",
      "[Training Epoch 6] Batch 3868, Loss 0.2404279112815857\n",
      "[Training Epoch 6] Batch 3869, Loss 0.26668286323547363\n",
      "[Training Epoch 6] Batch 3870, Loss 0.2372686266899109\n",
      "[Training Epoch 6] Batch 3871, Loss 0.2639361619949341\n",
      "[Training Epoch 6] Batch 3872, Loss 0.2629459500312805\n",
      "[Training Epoch 6] Batch 3873, Loss 0.26152127981185913\n",
      "[Training Epoch 6] Batch 3874, Loss 0.2874305248260498\n",
      "[Training Epoch 6] Batch 3875, Loss 0.27312177419662476\n",
      "[Training Epoch 6] Batch 3876, Loss 0.26412785053253174\n",
      "[Training Epoch 6] Batch 3877, Loss 0.24937988817691803\n",
      "[Training Epoch 6] Batch 3878, Loss 0.2639329433441162\n",
      "[Training Epoch 6] Batch 3879, Loss 0.2843043804168701\n",
      "[Training Epoch 6] Batch 3880, Loss 0.2624529004096985\n",
      "[Training Epoch 6] Batch 3881, Loss 0.255049467086792\n",
      "[Training Epoch 6] Batch 3882, Loss 0.2655661106109619\n",
      "[Training Epoch 6] Batch 3883, Loss 0.2735624313354492\n",
      "[Training Epoch 6] Batch 3884, Loss 0.26581186056137085\n",
      "[Training Epoch 6] Batch 3885, Loss 0.24186109006404877\n",
      "[Training Epoch 6] Batch 3886, Loss 0.264593243598938\n",
      "[Training Epoch 6] Batch 3887, Loss 0.2618103623390198\n",
      "[Training Epoch 6] Batch 3888, Loss 0.279221773147583\n",
      "[Training Epoch 6] Batch 3889, Loss 0.26501792669296265\n",
      "[Training Epoch 6] Batch 3890, Loss 0.26245415210723877\n",
      "[Training Epoch 6] Batch 3891, Loss 0.2764911353588104\n",
      "[Training Epoch 6] Batch 3892, Loss 0.26270735263824463\n",
      "[Training Epoch 6] Batch 3893, Loss 0.2469833493232727\n",
      "[Training Epoch 6] Batch 3894, Loss 0.2995103597640991\n",
      "[Training Epoch 6] Batch 3895, Loss 0.24988801777362823\n",
      "[Training Epoch 6] Batch 3896, Loss 0.25996673107147217\n",
      "[Training Epoch 6] Batch 3897, Loss 0.26564550399780273\n",
      "[Training Epoch 6] Batch 3898, Loss 0.2541266679763794\n",
      "[Training Epoch 6] Batch 3899, Loss 0.23563970625400543\n",
      "[Training Epoch 6] Batch 3900, Loss 0.27155810594558716\n",
      "[Training Epoch 6] Batch 3901, Loss 0.2758718729019165\n",
      "[Training Epoch 6] Batch 3902, Loss 0.2406211942434311\n",
      "[Training Epoch 6] Batch 3903, Loss 0.26102516055107117\n",
      "[Training Epoch 6] Batch 3904, Loss 0.2463328093290329\n",
      "[Training Epoch 6] Batch 3905, Loss 0.2635989189147949\n",
      "[Training Epoch 6] Batch 3906, Loss 0.25976991653442383\n",
      "[Training Epoch 6] Batch 3907, Loss 0.26288914680480957\n",
      "[Training Epoch 6] Batch 3908, Loss 0.287455677986145\n",
      "[Training Epoch 6] Batch 3909, Loss 0.24720759689807892\n",
      "[Training Epoch 6] Batch 3910, Loss 0.27247631549835205\n",
      "[Training Epoch 6] Batch 3911, Loss 0.27315905690193176\n",
      "[Training Epoch 6] Batch 3912, Loss 0.2928507924079895\n",
      "[Training Epoch 6] Batch 3913, Loss 0.25558459758758545\n",
      "[Training Epoch 6] Batch 3914, Loss 0.24852195382118225\n",
      "[Training Epoch 6] Batch 3915, Loss 0.2613826394081116\n",
      "[Training Epoch 6] Batch 3916, Loss 0.28598153591156006\n",
      "[Training Epoch 6] Batch 3917, Loss 0.246746227145195\n",
      "[Training Epoch 6] Batch 3918, Loss 0.24487163126468658\n",
      "[Training Epoch 6] Batch 3919, Loss 0.2530668377876282\n",
      "[Training Epoch 6] Batch 3920, Loss 0.28832927346229553\n",
      "[Training Epoch 6] Batch 3921, Loss 0.22966685891151428\n",
      "[Training Epoch 6] Batch 3922, Loss 0.26091229915618896\n",
      "[Training Epoch 6] Batch 3923, Loss 0.27961280941963196\n",
      "[Training Epoch 6] Batch 3924, Loss 0.2673741579055786\n",
      "[Training Epoch 6] Batch 3925, Loss 0.23593999445438385\n",
      "[Training Epoch 6] Batch 3926, Loss 0.26623058319091797\n",
      "[Training Epoch 6] Batch 3927, Loss 0.2559358477592468\n",
      "[Training Epoch 6] Batch 3928, Loss 0.27386927604675293\n",
      "[Training Epoch 6] Batch 3929, Loss 0.24391674995422363\n",
      "[Training Epoch 6] Batch 3930, Loss 0.2762221693992615\n",
      "[Training Epoch 6] Batch 3931, Loss 0.2701338827610016\n",
      "[Training Epoch 6] Batch 3932, Loss 0.2727101743221283\n",
      "[Training Epoch 6] Batch 3933, Loss 0.2505713105201721\n",
      "[Training Epoch 6] Batch 3934, Loss 0.25685548782348633\n",
      "[Training Epoch 6] Batch 3935, Loss 0.27507129311561584\n",
      "[Training Epoch 6] Batch 3936, Loss 0.26090437173843384\n",
      "[Training Epoch 6] Batch 3937, Loss 0.2662813067436218\n",
      "[Training Epoch 6] Batch 3938, Loss 0.2716182470321655\n",
      "[Training Epoch 6] Batch 3939, Loss 0.2517157196998596\n",
      "[Training Epoch 6] Batch 3940, Loss 0.25404006242752075\n",
      "[Training Epoch 6] Batch 3941, Loss 0.25206032395362854\n",
      "[Training Epoch 6] Batch 3942, Loss 0.27619796991348267\n",
      "[Training Epoch 6] Batch 3943, Loss 0.2575141191482544\n",
      "[Training Epoch 6] Batch 3944, Loss 0.2682386040687561\n",
      "[Training Epoch 6] Batch 3945, Loss 0.2605961561203003\n",
      "[Training Epoch 6] Batch 3946, Loss 0.25881266593933105\n",
      "[Training Epoch 6] Batch 3947, Loss 0.27507251501083374\n",
      "[Training Epoch 6] Batch 3948, Loss 0.2670286297798157\n",
      "[Training Epoch 6] Batch 3949, Loss 0.27000337839126587\n",
      "[Training Epoch 6] Batch 3950, Loss 0.27351289987564087\n",
      "[Training Epoch 6] Batch 3951, Loss 0.24223144352436066\n",
      "[Training Epoch 6] Batch 3952, Loss 0.2799234390258789\n",
      "[Training Epoch 6] Batch 3953, Loss 0.25796031951904297\n",
      "[Training Epoch 6] Batch 3954, Loss 0.28262585401535034\n",
      "[Training Epoch 6] Batch 3955, Loss 0.2613224983215332\n",
      "[Training Epoch 6] Batch 3956, Loss 0.27467596530914307\n",
      "[Training Epoch 6] Batch 3957, Loss 0.2576631009578705\n",
      "[Training Epoch 6] Batch 3958, Loss 0.2618907690048218\n",
      "[Training Epoch 6] Batch 3959, Loss 0.24498684704303741\n",
      "[Training Epoch 6] Batch 3960, Loss 0.30412203073501587\n",
      "[Training Epoch 6] Batch 3961, Loss 0.260555624961853\n",
      "[Training Epoch 6] Batch 3962, Loss 0.2728157043457031\n",
      "[Training Epoch 6] Batch 3963, Loss 0.2662805914878845\n",
      "[Training Epoch 6] Batch 3964, Loss 0.24656878411769867\n",
      "[Training Epoch 6] Batch 3965, Loss 0.2325650155544281\n",
      "[Training Epoch 6] Batch 3966, Loss 0.2534770369529724\n",
      "[Training Epoch 6] Batch 3967, Loss 0.23899222910404205\n",
      "[Training Epoch 6] Batch 3968, Loss 0.2614201307296753\n",
      "[Training Epoch 6] Batch 3969, Loss 0.2536516487598419\n",
      "[Training Epoch 6] Batch 3970, Loss 0.24033209681510925\n",
      "[Training Epoch 6] Batch 3971, Loss 0.2698777914047241\n",
      "[Training Epoch 6] Batch 3972, Loss 0.24608168005943298\n",
      "[Training Epoch 6] Batch 3973, Loss 0.2765445113182068\n",
      "[Training Epoch 6] Batch 3974, Loss 0.2783539891242981\n",
      "[Training Epoch 6] Batch 3975, Loss 0.2539072036743164\n",
      "[Training Epoch 6] Batch 3976, Loss 0.2375022917985916\n",
      "[Training Epoch 6] Batch 3977, Loss 0.24664092063903809\n",
      "[Training Epoch 6] Batch 3978, Loss 0.23245888948440552\n",
      "[Training Epoch 6] Batch 3979, Loss 0.2727868854999542\n",
      "[Training Epoch 6] Batch 3980, Loss 0.24762755632400513\n",
      "[Training Epoch 6] Batch 3981, Loss 0.2715725302696228\n",
      "[Training Epoch 6] Batch 3982, Loss 0.2824075222015381\n",
      "[Training Epoch 6] Batch 3983, Loss 0.2517979145050049\n",
      "[Training Epoch 6] Batch 3984, Loss 0.24858632683753967\n",
      "[Training Epoch 6] Batch 3985, Loss 0.25470927357673645\n",
      "[Training Epoch 6] Batch 3986, Loss 0.25162994861602783\n",
      "[Training Epoch 6] Batch 3987, Loss 0.2660852074623108\n",
      "[Training Epoch 6] Batch 3988, Loss 0.24763837456703186\n",
      "[Training Epoch 6] Batch 3989, Loss 0.2693236470222473\n",
      "[Training Epoch 6] Batch 3990, Loss 0.2445557862520218\n",
      "[Training Epoch 6] Batch 3991, Loss 0.2601568102836609\n",
      "[Training Epoch 6] Batch 3992, Loss 0.2461477667093277\n",
      "[Training Epoch 6] Batch 3993, Loss 0.284810334444046\n",
      "[Training Epoch 6] Batch 3994, Loss 0.24857594072818756\n",
      "[Training Epoch 6] Batch 3995, Loss 0.26797282695770264\n",
      "[Training Epoch 6] Batch 3996, Loss 0.2879040241241455\n",
      "[Training Epoch 6] Batch 3997, Loss 0.2601984739303589\n",
      "[Training Epoch 6] Batch 3998, Loss 0.2581750750541687\n",
      "[Training Epoch 6] Batch 3999, Loss 0.233313649892807\n",
      "[Training Epoch 6] Batch 4000, Loss 0.2672406733036041\n",
      "[Training Epoch 6] Batch 4001, Loss 0.2582438290119171\n",
      "[Training Epoch 6] Batch 4002, Loss 0.24186690151691437\n",
      "[Training Epoch 6] Batch 4003, Loss 0.27401500940322876\n",
      "[Training Epoch 6] Batch 4004, Loss 0.28029322624206543\n",
      "[Training Epoch 6] Batch 4005, Loss 0.24862134456634521\n",
      "[Training Epoch 6] Batch 4006, Loss 0.2963327169418335\n",
      "[Training Epoch 6] Batch 4007, Loss 0.2665155827999115\n",
      "[Training Epoch 6] Batch 4008, Loss 0.25320690870285034\n",
      "[Training Epoch 6] Batch 4009, Loss 0.2740321159362793\n",
      "[Training Epoch 6] Batch 4010, Loss 0.25365668535232544\n",
      "[Training Epoch 6] Batch 4011, Loss 0.2691458463668823\n",
      "[Training Epoch 6] Batch 4012, Loss 0.27123093605041504\n",
      "[Training Epoch 6] Batch 4013, Loss 0.2405284196138382\n",
      "[Training Epoch 6] Batch 4014, Loss 0.24922454357147217\n",
      "[Training Epoch 6] Batch 4015, Loss 0.23271289467811584\n",
      "[Training Epoch 6] Batch 4016, Loss 0.23739704489707947\n",
      "[Training Epoch 6] Batch 4017, Loss 0.24632278084754944\n",
      "[Training Epoch 6] Batch 4018, Loss 0.24567392468452454\n",
      "[Training Epoch 6] Batch 4019, Loss 0.26776695251464844\n",
      "[Training Epoch 6] Batch 4020, Loss 0.2822296619415283\n",
      "[Training Epoch 6] Batch 4021, Loss 0.2719704210758209\n",
      "[Training Epoch 6] Batch 4022, Loss 0.26503896713256836\n",
      "[Training Epoch 6] Batch 4023, Loss 0.25208476185798645\n",
      "[Training Epoch 6] Batch 4024, Loss 0.27267515659332275\n",
      "[Training Epoch 6] Batch 4025, Loss 0.25481659173965454\n",
      "[Training Epoch 6] Batch 4026, Loss 0.2736298739910126\n",
      "[Training Epoch 6] Batch 4027, Loss 0.24641185998916626\n",
      "[Training Epoch 6] Batch 4028, Loss 0.24462291598320007\n",
      "[Training Epoch 6] Batch 4029, Loss 0.2868390679359436\n",
      "[Training Epoch 6] Batch 4030, Loss 0.2389485239982605\n",
      "[Training Epoch 6] Batch 4031, Loss 0.2670109272003174\n",
      "[Training Epoch 6] Batch 4032, Loss 0.2702100872993469\n",
      "[Training Epoch 6] Batch 4033, Loss 0.2738880217075348\n",
      "[Training Epoch 6] Batch 4034, Loss 0.2588741183280945\n",
      "[Training Epoch 6] Batch 4035, Loss 0.23752346634864807\n",
      "[Training Epoch 6] Batch 4036, Loss 0.23980695009231567\n",
      "[Training Epoch 6] Batch 4037, Loss 0.25393956899642944\n",
      "[Training Epoch 6] Batch 4038, Loss 0.2731093764305115\n",
      "[Training Epoch 6] Batch 4039, Loss 0.24671822786331177\n",
      "[Training Epoch 6] Batch 4040, Loss 0.24916674196720123\n",
      "[Training Epoch 6] Batch 4041, Loss 0.25699523091316223\n",
      "[Training Epoch 6] Batch 4042, Loss 0.24927113950252533\n",
      "[Training Epoch 6] Batch 4043, Loss 0.26109403371810913\n",
      "[Training Epoch 6] Batch 4044, Loss 0.25460270047187805\n",
      "[Training Epoch 6] Batch 4045, Loss 0.2880415916442871\n",
      "[Training Epoch 6] Batch 4046, Loss 0.2518645226955414\n",
      "[Training Epoch 6] Batch 4047, Loss 0.27112460136413574\n",
      "[Training Epoch 6] Batch 4048, Loss 0.2804229259490967\n",
      "[Training Epoch 6] Batch 4049, Loss 0.2623977065086365\n",
      "[Training Epoch 6] Batch 4050, Loss 0.2607320547103882\n",
      "[Training Epoch 6] Batch 4051, Loss 0.267320454120636\n",
      "[Training Epoch 6] Batch 4052, Loss 0.27348095178604126\n",
      "[Training Epoch 6] Batch 4053, Loss 0.2518889904022217\n",
      "[Training Epoch 6] Batch 4054, Loss 0.25249311327934265\n",
      "[Training Epoch 6] Batch 4055, Loss 0.24680961668491364\n",
      "[Training Epoch 6] Batch 4056, Loss 0.24127382040023804\n",
      "[Training Epoch 6] Batch 4057, Loss 0.28327637910842896\n",
      "[Training Epoch 6] Batch 4058, Loss 0.26302099227905273\n",
      "[Training Epoch 6] Batch 4059, Loss 0.28197866678237915\n",
      "[Training Epoch 6] Batch 4060, Loss 0.2441597878932953\n",
      "[Training Epoch 6] Batch 4061, Loss 0.23768271505832672\n",
      "[Training Epoch 6] Batch 4062, Loss 0.2521684169769287\n",
      "[Training Epoch 6] Batch 4063, Loss 0.2649572491645813\n",
      "[Training Epoch 6] Batch 4064, Loss 0.25162917375564575\n",
      "[Training Epoch 6] Batch 4065, Loss 0.26194971799850464\n",
      "[Training Epoch 6] Batch 4066, Loss 0.2639201283454895\n",
      "[Training Epoch 6] Batch 4067, Loss 0.2426869124174118\n",
      "[Training Epoch 6] Batch 4068, Loss 0.2726421058177948\n",
      "[Training Epoch 6] Batch 4069, Loss 0.27113762497901917\n",
      "[Training Epoch 6] Batch 4070, Loss 0.287352979183197\n",
      "[Training Epoch 6] Batch 4071, Loss 0.2628970742225647\n",
      "[Training Epoch 6] Batch 4072, Loss 0.2503829598426819\n",
      "[Training Epoch 6] Batch 4073, Loss 0.26440390944480896\n",
      "[Training Epoch 6] Batch 4074, Loss 0.29225894808769226\n",
      "[Training Epoch 6] Batch 4075, Loss 0.253450870513916\n",
      "[Training Epoch 6] Batch 4076, Loss 0.2885914444923401\n",
      "[Training Epoch 6] Batch 4077, Loss 0.2780483365058899\n",
      "[Training Epoch 6] Batch 4078, Loss 0.2598734498023987\n",
      "[Training Epoch 6] Batch 4079, Loss 0.23794811964035034\n",
      "[Training Epoch 6] Batch 4080, Loss 0.2792883515357971\n",
      "[Training Epoch 6] Batch 4081, Loss 0.24518144130706787\n",
      "[Training Epoch 6] Batch 4082, Loss 0.2789371609687805\n",
      "[Training Epoch 6] Batch 4083, Loss 0.2703743577003479\n",
      "[Training Epoch 6] Batch 4084, Loss 0.254645973443985\n",
      "[Training Epoch 6] Batch 4085, Loss 0.28087756037712097\n",
      "[Training Epoch 6] Batch 4086, Loss 0.2588884234428406\n",
      "[Training Epoch 6] Batch 4087, Loss 0.26838332414627075\n",
      "[Training Epoch 6] Batch 4088, Loss 0.2566351294517517\n",
      "[Training Epoch 6] Batch 4089, Loss 0.2586406171321869\n",
      "[Training Epoch 6] Batch 4090, Loss 0.27563145756721497\n",
      "[Training Epoch 6] Batch 4091, Loss 0.23571047186851501\n",
      "[Training Epoch 6] Batch 4092, Loss 0.26425862312316895\n",
      "[Training Epoch 6] Batch 4093, Loss 0.22135013341903687\n",
      "[Training Epoch 6] Batch 4094, Loss 0.2542385160923004\n",
      "[Training Epoch 6] Batch 4095, Loss 0.2389654517173767\n",
      "[Training Epoch 6] Batch 4096, Loss 0.2502952814102173\n",
      "[Training Epoch 6] Batch 4097, Loss 0.27160754799842834\n",
      "[Training Epoch 6] Batch 4098, Loss 0.26417768001556396\n",
      "[Training Epoch 6] Batch 4099, Loss 0.2444475293159485\n",
      "[Training Epoch 6] Batch 4100, Loss 0.2299654483795166\n",
      "[Training Epoch 6] Batch 4101, Loss 0.24594344198703766\n",
      "[Training Epoch 6] Batch 4102, Loss 0.27040278911590576\n",
      "[Training Epoch 6] Batch 4103, Loss 0.27928823232650757\n",
      "[Training Epoch 6] Batch 4104, Loss 0.25571006536483765\n",
      "[Training Epoch 6] Batch 4105, Loss 0.28710198402404785\n",
      "[Training Epoch 6] Batch 4106, Loss 0.2655358910560608\n",
      "[Training Epoch 6] Batch 4107, Loss 0.272646427154541\n",
      "[Training Epoch 6] Batch 4108, Loss 0.28025126457214355\n",
      "[Training Epoch 6] Batch 4109, Loss 0.24292311072349548\n",
      "[Training Epoch 6] Batch 4110, Loss 0.2624506950378418\n",
      "[Training Epoch 6] Batch 4111, Loss 0.23910969495773315\n",
      "[Training Epoch 6] Batch 4112, Loss 0.25705665349960327\n",
      "[Training Epoch 6] Batch 4113, Loss 0.2842813730239868\n",
      "[Training Epoch 6] Batch 4114, Loss 0.2679280638694763\n",
      "[Training Epoch 6] Batch 4115, Loss 0.25902533531188965\n",
      "[Training Epoch 6] Batch 4116, Loss 0.2322787344455719\n",
      "[Training Epoch 6] Batch 4117, Loss 0.29350775480270386\n",
      "[Training Epoch 6] Batch 4118, Loss 0.25858697295188904\n",
      "[Training Epoch 6] Batch 4119, Loss 0.2623046040534973\n",
      "[Training Epoch 6] Batch 4120, Loss 0.2539035379886627\n",
      "[Training Epoch 6] Batch 4121, Loss 0.2646661400794983\n",
      "[Training Epoch 6] Batch 4122, Loss 0.2732727527618408\n",
      "[Training Epoch 6] Batch 4123, Loss 0.26247069239616394\n",
      "[Training Epoch 6] Batch 4124, Loss 0.25494274497032166\n",
      "[Training Epoch 6] Batch 4125, Loss 0.2773139774799347\n",
      "[Training Epoch 6] Batch 4126, Loss 0.2802667021751404\n",
      "[Training Epoch 6] Batch 4127, Loss 0.2722311317920685\n",
      "[Training Epoch 6] Batch 4128, Loss 0.28150010108947754\n",
      "[Training Epoch 6] Batch 4129, Loss 0.2380972057580948\n",
      "[Training Epoch 6] Batch 4130, Loss 0.26976320147514343\n",
      "[Training Epoch 6] Batch 4131, Loss 0.2695092558860779\n",
      "[Training Epoch 6] Batch 4132, Loss 0.24533748626708984\n",
      "[Training Epoch 6] Batch 4133, Loss 0.25396645069122314\n",
      "[Training Epoch 6] Batch 4134, Loss 0.2927817702293396\n",
      "[Training Epoch 6] Batch 4135, Loss 0.2511727213859558\n",
      "[Training Epoch 6] Batch 4136, Loss 0.30301862955093384\n",
      "[Training Epoch 6] Batch 4137, Loss 0.28437498211860657\n",
      "[Training Epoch 6] Batch 4138, Loss 0.23843008279800415\n",
      "[Training Epoch 6] Batch 4139, Loss 0.24895262718200684\n",
      "[Training Epoch 6] Batch 4140, Loss 0.26107823848724365\n",
      "[Training Epoch 6] Batch 4141, Loss 0.2880014479160309\n",
      "[Training Epoch 6] Batch 4142, Loss 0.257129967212677\n",
      "[Training Epoch 6] Batch 4143, Loss 0.24113444983959198\n",
      "[Training Epoch 6] Batch 4144, Loss 0.275567889213562\n",
      "[Training Epoch 6] Batch 4145, Loss 0.2478688359260559\n",
      "[Training Epoch 6] Batch 4146, Loss 0.2616763412952423\n",
      "[Training Epoch 6] Batch 4147, Loss 0.2758011817932129\n",
      "[Training Epoch 6] Batch 4148, Loss 0.24732446670532227\n",
      "[Training Epoch 6] Batch 4149, Loss 0.2652142643928528\n",
      "[Training Epoch 6] Batch 4150, Loss 0.27336809039115906\n",
      "[Training Epoch 6] Batch 4151, Loss 0.2331482321023941\n",
      "[Training Epoch 6] Batch 4152, Loss 0.23999708890914917\n",
      "[Training Epoch 6] Batch 4153, Loss 0.28752437233924866\n",
      "[Training Epoch 6] Batch 4154, Loss 0.22891920804977417\n",
      "[Training Epoch 6] Batch 4155, Loss 0.258116751909256\n",
      "[Training Epoch 6] Batch 4156, Loss 0.2636604905128479\n",
      "[Training Epoch 6] Batch 4157, Loss 0.2525341510772705\n",
      "[Training Epoch 6] Batch 4158, Loss 0.23412203788757324\n",
      "[Training Epoch 6] Batch 4159, Loss 0.2692306935787201\n",
      "[Training Epoch 6] Batch 4160, Loss 0.2518429458141327\n",
      "[Training Epoch 6] Batch 4161, Loss 0.25946110486984253\n",
      "[Training Epoch 6] Batch 4162, Loss 0.2492954134941101\n",
      "[Training Epoch 6] Batch 4163, Loss 0.259577214717865\n",
      "[Training Epoch 6] Batch 4164, Loss 0.258428692817688\n",
      "[Training Epoch 6] Batch 4165, Loss 0.2759193778038025\n",
      "[Training Epoch 6] Batch 4166, Loss 0.2324426770210266\n",
      "[Training Epoch 6] Batch 4167, Loss 0.26226937770843506\n",
      "[Training Epoch 6] Batch 4168, Loss 0.2771471440792084\n",
      "[Training Epoch 6] Batch 4169, Loss 0.26910534501075745\n",
      "[Training Epoch 6] Batch 4170, Loss 0.2490403801202774\n",
      "[Training Epoch 6] Batch 4171, Loss 0.25944530963897705\n",
      "[Training Epoch 6] Batch 4172, Loss 0.2562604546546936\n",
      "[Training Epoch 6] Batch 4173, Loss 0.26327213644981384\n",
      "[Training Epoch 6] Batch 4174, Loss 0.23477086424827576\n",
      "[Training Epoch 6] Batch 4175, Loss 0.2724253535270691\n",
      "[Training Epoch 6] Batch 4176, Loss 0.24362942576408386\n",
      "[Training Epoch 6] Batch 4177, Loss 0.2656691372394562\n",
      "[Training Epoch 6] Batch 4178, Loss 0.2546606659889221\n",
      "[Training Epoch 6] Batch 4179, Loss 0.2703331708908081\n",
      "[Training Epoch 6] Batch 4180, Loss 0.25986045598983765\n",
      "[Training Epoch 6] Batch 4181, Loss 0.27922719717025757\n",
      "[Training Epoch 6] Batch 4182, Loss 0.24773091077804565\n",
      "[Training Epoch 6] Batch 4183, Loss 0.24901530146598816\n",
      "[Training Epoch 6] Batch 4184, Loss 0.26518988609313965\n",
      "[Training Epoch 6] Batch 4185, Loss 0.2585224211215973\n",
      "[Training Epoch 6] Batch 4186, Loss 0.25869399309158325\n",
      "[Training Epoch 6] Batch 4187, Loss 0.2763879597187042\n",
      "[Training Epoch 6] Batch 4188, Loss 0.2644747495651245\n",
      "[Training Epoch 6] Batch 4189, Loss 0.2527098059654236\n",
      "[Training Epoch 6] Batch 4190, Loss 0.28368428349494934\n",
      "[Training Epoch 6] Batch 4191, Loss 0.28896188735961914\n",
      "[Training Epoch 6] Batch 4192, Loss 0.24835637211799622\n",
      "[Training Epoch 6] Batch 4193, Loss 0.2577129602432251\n",
      "[Training Epoch 6] Batch 4194, Loss 0.22184127569198608\n",
      "[Training Epoch 6] Batch 4195, Loss 0.2641203701496124\n",
      "[Training Epoch 6] Batch 4196, Loss 0.2302466481924057\n",
      "[Training Epoch 6] Batch 4197, Loss 0.2632195055484772\n",
      "[Training Epoch 6] Batch 4198, Loss 0.26086175441741943\n",
      "[Training Epoch 6] Batch 4199, Loss 0.24559351801872253\n",
      "[Training Epoch 6] Batch 4200, Loss 0.25839751958847046\n",
      "[Training Epoch 6] Batch 4201, Loss 0.2670910656452179\n",
      "[Training Epoch 6] Batch 4202, Loss 0.2743341326713562\n",
      "[Training Epoch 6] Batch 4203, Loss 0.24770505726337433\n",
      "[Training Epoch 6] Batch 4204, Loss 0.27057528495788574\n",
      "[Training Epoch 6] Batch 4205, Loss 0.2518298625946045\n",
      "[Training Epoch 6] Batch 4206, Loss 0.3009568750858307\n",
      "[Training Epoch 6] Batch 4207, Loss 0.28321897983551025\n",
      "[Training Epoch 6] Batch 4208, Loss 0.2637293040752411\n",
      "[Training Epoch 6] Batch 4209, Loss 0.25745630264282227\n",
      "[Training Epoch 6] Batch 4210, Loss 0.25406506657600403\n",
      "[Training Epoch 6] Batch 4211, Loss 0.2635200023651123\n",
      "[Training Epoch 6] Batch 4212, Loss 0.2827357053756714\n",
      "[Training Epoch 6] Batch 4213, Loss 0.22711974382400513\n",
      "[Training Epoch 6] Batch 4214, Loss 0.2615628242492676\n",
      "[Training Epoch 6] Batch 4215, Loss 0.24811169505119324\n",
      "[Training Epoch 6] Batch 4216, Loss 0.28399577736854553\n",
      "[Training Epoch 6] Batch 4217, Loss 0.26121896505355835\n",
      "[Training Epoch 6] Batch 4218, Loss 0.251986563205719\n",
      "[Training Epoch 6] Batch 4219, Loss 0.2559245228767395\n",
      "[Training Epoch 6] Batch 4220, Loss 0.28189268708229065\n",
      "[Training Epoch 6] Batch 4221, Loss 0.26621779799461365\n",
      "[Training Epoch 6] Batch 4222, Loss 0.23716393113136292\n",
      "[Training Epoch 6] Batch 4223, Loss 0.27736908197402954\n",
      "[Training Epoch 6] Batch 4224, Loss 0.2725508511066437\n",
      "[Training Epoch 6] Batch 4225, Loss 0.29741907119750977\n",
      "[Training Epoch 6] Batch 4226, Loss 0.25568050146102905\n",
      "[Training Epoch 6] Batch 4227, Loss 0.23474253714084625\n",
      "[Training Epoch 6] Batch 4228, Loss 0.2593786418437958\n",
      "[Training Epoch 6] Batch 4229, Loss 0.2493118792772293\n",
      "[Training Epoch 6] Batch 4230, Loss 0.2605600357055664\n",
      "[Training Epoch 6] Batch 4231, Loss 0.2422378808259964\n",
      "[Training Epoch 6] Batch 4232, Loss 0.2393512725830078\n",
      "[Training Epoch 6] Batch 4233, Loss 0.2650318741798401\n",
      "[Training Epoch 6] Batch 4234, Loss 0.2358710765838623\n",
      "[Training Epoch 6] Batch 4235, Loss 0.28304457664489746\n",
      "[Training Epoch 6] Batch 4236, Loss 0.24110764265060425\n",
      "[Training Epoch 6] Batch 4237, Loss 0.27401772141456604\n",
      "[Training Epoch 6] Batch 4238, Loss 0.2737191617488861\n",
      "[Training Epoch 6] Batch 4239, Loss 0.2437342405319214\n",
      "[Training Epoch 6] Batch 4240, Loss 0.24231937527656555\n",
      "[Training Epoch 6] Batch 4241, Loss 0.27029192447662354\n",
      "[Training Epoch 6] Batch 4242, Loss 0.26374170184135437\n",
      "[Training Epoch 6] Batch 4243, Loss 0.26490819454193115\n",
      "[Training Epoch 6] Batch 4244, Loss 0.2782312035560608\n",
      "[Training Epoch 6] Batch 4245, Loss 0.2505921721458435\n",
      "[Training Epoch 6] Batch 4246, Loss 0.2991328537464142\n",
      "[Training Epoch 6] Batch 4247, Loss 0.23790210485458374\n",
      "[Training Epoch 6] Batch 4248, Loss 0.2764705717563629\n",
      "[Training Epoch 6] Batch 4249, Loss 0.27514150738716125\n",
      "[Training Epoch 6] Batch 4250, Loss 0.2760373055934906\n",
      "[Training Epoch 6] Batch 4251, Loss 0.2811853885650635\n",
      "[Training Epoch 6] Batch 4252, Loss 0.2533988952636719\n",
      "[Training Epoch 6] Batch 4253, Loss 0.2386111468076706\n",
      "[Training Epoch 6] Batch 4254, Loss 0.2711445093154907\n",
      "[Training Epoch 6] Batch 4255, Loss 0.27632883191108704\n",
      "[Training Epoch 6] Batch 4256, Loss 0.27210426330566406\n",
      "[Training Epoch 6] Batch 4257, Loss 0.24191498756408691\n",
      "[Training Epoch 6] Batch 4258, Loss 0.24420014023780823\n",
      "[Training Epoch 6] Batch 4259, Loss 0.26548120379447937\n",
      "[Training Epoch 6] Batch 4260, Loss 0.2544654607772827\n",
      "[Training Epoch 6] Batch 4261, Loss 0.26690101623535156\n",
      "[Training Epoch 6] Batch 4262, Loss 0.2489635944366455\n",
      "[Training Epoch 6] Batch 4263, Loss 0.2549152076244354\n",
      "[Training Epoch 6] Batch 4264, Loss 0.24697114527225494\n",
      "[Training Epoch 6] Batch 4265, Loss 0.24830090999603271\n",
      "[Training Epoch 6] Batch 4266, Loss 0.25388669967651367\n",
      "[Training Epoch 6] Batch 4267, Loss 0.2681896686553955\n",
      "[Training Epoch 6] Batch 4268, Loss 0.25545167922973633\n",
      "[Training Epoch 6] Batch 4269, Loss 0.26762259006500244\n",
      "[Training Epoch 6] Batch 4270, Loss 0.26601579785346985\n",
      "[Training Epoch 6] Batch 4271, Loss 0.25096094608306885\n",
      "[Training Epoch 6] Batch 4272, Loss 0.2572278380393982\n",
      "[Training Epoch 6] Batch 4273, Loss 0.24024005234241486\n",
      "[Training Epoch 6] Batch 4274, Loss 0.26620009541511536\n",
      "[Training Epoch 6] Batch 4275, Loss 0.26685571670532227\n",
      "[Training Epoch 6] Batch 4276, Loss 0.26413694024086\n",
      "[Training Epoch 6] Batch 4277, Loss 0.2529090344905853\n",
      "[Training Epoch 6] Batch 4278, Loss 0.2364848554134369\n",
      "[Training Epoch 6] Batch 4279, Loss 0.27234160900115967\n",
      "[Training Epoch 6] Batch 4280, Loss 0.25842297077178955\n",
      "[Training Epoch 6] Batch 4281, Loss 0.26026982069015503\n",
      "[Training Epoch 6] Batch 4282, Loss 0.2968851327896118\n",
      "[Training Epoch 6] Batch 4283, Loss 0.24763071537017822\n",
      "[Training Epoch 6] Batch 4284, Loss 0.2686651349067688\n",
      "[Training Epoch 6] Batch 4285, Loss 0.25355225801467896\n",
      "[Training Epoch 6] Batch 4286, Loss 0.25878211855888367\n",
      "[Training Epoch 6] Batch 4287, Loss 0.2554135322570801\n",
      "[Training Epoch 6] Batch 4288, Loss 0.2657381296157837\n",
      "[Training Epoch 6] Batch 4289, Loss 0.2473195493221283\n",
      "[Training Epoch 6] Batch 4290, Loss 0.23979198932647705\n",
      "[Training Epoch 6] Batch 4291, Loss 0.2497219443321228\n",
      "[Training Epoch 6] Batch 4292, Loss 0.2666207253932953\n",
      "[Training Epoch 6] Batch 4293, Loss 0.2505258619785309\n",
      "[Training Epoch 6] Batch 4294, Loss 0.25868165493011475\n",
      "[Training Epoch 6] Batch 4295, Loss 0.23784185945987701\n",
      "[Training Epoch 6] Batch 4296, Loss 0.26180994510650635\n",
      "[Training Epoch 6] Batch 4297, Loss 0.26580238342285156\n",
      "[Training Epoch 6] Batch 4298, Loss 0.2624939978122711\n",
      "[Training Epoch 6] Batch 4299, Loss 0.2582680284976959\n",
      "[Training Epoch 6] Batch 4300, Loss 0.25433623790740967\n",
      "[Training Epoch 6] Batch 4301, Loss 0.2499689757823944\n",
      "[Training Epoch 6] Batch 4302, Loss 0.23837274312973022\n",
      "[Training Epoch 6] Batch 4303, Loss 0.2454521209001541\n",
      "[Training Epoch 6] Batch 4304, Loss 0.27550649642944336\n",
      "[Training Epoch 6] Batch 4305, Loss 0.25718551874160767\n",
      "[Training Epoch 6] Batch 4306, Loss 0.2625277042388916\n",
      "[Training Epoch 6] Batch 4307, Loss 0.2594235837459564\n",
      "[Training Epoch 6] Batch 4308, Loss 0.28871774673461914\n",
      "[Training Epoch 6] Batch 4309, Loss 0.2650787830352783\n",
      "[Training Epoch 6] Batch 4310, Loss 0.26151660084724426\n",
      "[Training Epoch 6] Batch 4311, Loss 0.26747894287109375\n",
      "[Training Epoch 6] Batch 4312, Loss 0.2718621492385864\n",
      "[Training Epoch 6] Batch 4313, Loss 0.28630998730659485\n",
      "[Training Epoch 6] Batch 4314, Loss 0.26189422607421875\n",
      "[Training Epoch 6] Batch 4315, Loss 0.2319391518831253\n",
      "[Training Epoch 6] Batch 4316, Loss 0.28965866565704346\n",
      "[Training Epoch 6] Batch 4317, Loss 0.2622907757759094\n",
      "[Training Epoch 6] Batch 4318, Loss 0.24220001697540283\n",
      "[Training Epoch 6] Batch 4319, Loss 0.27346503734588623\n",
      "[Training Epoch 6] Batch 4320, Loss 0.29207658767700195\n",
      "[Training Epoch 6] Batch 4321, Loss 0.27533358335494995\n",
      "[Training Epoch 6] Batch 4322, Loss 0.22548739612102509\n",
      "[Training Epoch 6] Batch 4323, Loss 0.26640138030052185\n",
      "[Training Epoch 6] Batch 4324, Loss 0.23521584272384644\n",
      "[Training Epoch 6] Batch 4325, Loss 0.2770657539367676\n",
      "[Training Epoch 6] Batch 4326, Loss 0.22071972489356995\n",
      "[Training Epoch 6] Batch 4327, Loss 0.25293588638305664\n",
      "[Training Epoch 6] Batch 4328, Loss 0.23750382661819458\n",
      "[Training Epoch 6] Batch 4329, Loss 0.2493426501750946\n",
      "[Training Epoch 6] Batch 4330, Loss 0.2594071626663208\n",
      "[Training Epoch 6] Batch 4331, Loss 0.24756036698818207\n",
      "[Training Epoch 6] Batch 4332, Loss 0.24496543407440186\n",
      "[Training Epoch 6] Batch 4333, Loss 0.2685849666595459\n",
      "[Training Epoch 6] Batch 4334, Loss 0.2620203197002411\n",
      "[Training Epoch 6] Batch 4335, Loss 0.23884683847427368\n",
      "[Training Epoch 6] Batch 4336, Loss 0.24008437991142273\n",
      "[Training Epoch 6] Batch 4337, Loss 0.2661026120185852\n",
      "[Training Epoch 6] Batch 4338, Loss 0.23411262035369873\n",
      "[Training Epoch 6] Batch 4339, Loss 0.24292580783367157\n",
      "[Training Epoch 6] Batch 4340, Loss 0.2830983102321625\n",
      "[Training Epoch 6] Batch 4341, Loss 0.26572176814079285\n",
      "[Training Epoch 6] Batch 4342, Loss 0.28216928243637085\n",
      "[Training Epoch 6] Batch 4343, Loss 0.24698394536972046\n",
      "[Training Epoch 6] Batch 4344, Loss 0.25531309843063354\n",
      "[Training Epoch 6] Batch 4345, Loss 0.2613639235496521\n",
      "[Training Epoch 6] Batch 4346, Loss 0.25333935022354126\n",
      "[Training Epoch 6] Batch 4347, Loss 0.24300360679626465\n",
      "[Training Epoch 6] Batch 4348, Loss 0.2712376117706299\n",
      "[Training Epoch 6] Batch 4349, Loss 0.2616717219352722\n",
      "[Training Epoch 6] Batch 4350, Loss 0.23629307746887207\n",
      "[Training Epoch 6] Batch 4351, Loss 0.28073424100875854\n",
      "[Training Epoch 6] Batch 4352, Loss 0.27866679430007935\n",
      "[Training Epoch 6] Batch 4353, Loss 0.2332925647497177\n",
      "[Training Epoch 6] Batch 4354, Loss 0.24688398838043213\n",
      "[Training Epoch 6] Batch 4355, Loss 0.2495907098054886\n",
      "[Training Epoch 6] Batch 4356, Loss 0.26183244585990906\n",
      "[Training Epoch 6] Batch 4357, Loss 0.23448151350021362\n",
      "[Training Epoch 6] Batch 4358, Loss 0.24905073642730713\n",
      "[Training Epoch 6] Batch 4359, Loss 0.2568143606185913\n",
      "[Training Epoch 6] Batch 4360, Loss 0.2600460648536682\n",
      "[Training Epoch 6] Batch 4361, Loss 0.2762008607387543\n",
      "[Training Epoch 6] Batch 4362, Loss 0.26298049092292786\n",
      "[Training Epoch 6] Batch 4363, Loss 0.28295207023620605\n",
      "[Training Epoch 6] Batch 4364, Loss 0.24266241490840912\n",
      "[Training Epoch 6] Batch 4365, Loss 0.2473243772983551\n",
      "[Training Epoch 6] Batch 4366, Loss 0.2930333614349365\n",
      "[Training Epoch 6] Batch 4367, Loss 0.2792981266975403\n",
      "[Training Epoch 6] Batch 4368, Loss 0.23826706409454346\n",
      "[Training Epoch 6] Batch 4369, Loss 0.25845497846603394\n",
      "[Training Epoch 6] Batch 4370, Loss 0.25344350934028625\n",
      "[Training Epoch 6] Batch 4371, Loss 0.24739408493041992\n",
      "[Training Epoch 6] Batch 4372, Loss 0.2397284209728241\n",
      "[Training Epoch 6] Batch 4373, Loss 0.26582515239715576\n",
      "[Training Epoch 6] Batch 4374, Loss 0.2635447382926941\n",
      "[Training Epoch 6] Batch 4375, Loss 0.25926685333251953\n",
      "[Training Epoch 6] Batch 4376, Loss 0.2818109393119812\n",
      "[Training Epoch 6] Batch 4377, Loss 0.2722322344779968\n",
      "[Training Epoch 6] Batch 4378, Loss 0.28405794501304626\n",
      "[Training Epoch 6] Batch 4379, Loss 0.2653670310974121\n",
      "[Training Epoch 6] Batch 4380, Loss 0.25235480070114136\n",
      "[Training Epoch 6] Batch 4381, Loss 0.25179195404052734\n",
      "[Training Epoch 6] Batch 4382, Loss 0.22327680885791779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2249.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 6] Precision = 0.2616, Recall = 0.7742\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.26679566502571106\n",
      "[Training Epoch 7] Batch 1, Loss 0.26301443576812744\n",
      "[Training Epoch 7] Batch 2, Loss 0.2682517468929291\n",
      "[Training Epoch 7] Batch 3, Loss 0.25288259983062744\n",
      "[Training Epoch 7] Batch 4, Loss 0.2551305890083313\n",
      "[Training Epoch 7] Batch 5, Loss 0.25813063979148865\n",
      "[Training Epoch 7] Batch 6, Loss 0.24982918798923492\n",
      "[Training Epoch 7] Batch 7, Loss 0.21772320568561554\n",
      "[Training Epoch 7] Batch 8, Loss 0.26245301961898804\n",
      "[Training Epoch 7] Batch 9, Loss 0.2542659044265747\n",
      "[Training Epoch 7] Batch 10, Loss 0.24336403608322144\n",
      "[Training Epoch 7] Batch 11, Loss 0.2431619018316269\n",
      "[Training Epoch 7] Batch 12, Loss 0.2563738226890564\n",
      "[Training Epoch 7] Batch 13, Loss 0.2836723327636719\n",
      "[Training Epoch 7] Batch 14, Loss 0.24442994594573975\n",
      "[Training Epoch 7] Batch 15, Loss 0.26480746269226074\n",
      "[Training Epoch 7] Batch 16, Loss 0.261958509683609\n",
      "[Training Epoch 7] Batch 17, Loss 0.2165023386478424\n",
      "[Training Epoch 7] Batch 18, Loss 0.244117870926857\n",
      "[Training Epoch 7] Batch 19, Loss 0.23200640082359314\n",
      "[Training Epoch 7] Batch 20, Loss 0.22260209918022156\n",
      "[Training Epoch 7] Batch 21, Loss 0.2458728849887848\n",
      "[Training Epoch 7] Batch 22, Loss 0.2589629888534546\n",
      "[Training Epoch 7] Batch 23, Loss 0.23393714427947998\n",
      "[Training Epoch 7] Batch 24, Loss 0.22960267961025238\n",
      "[Training Epoch 7] Batch 25, Loss 0.252963125705719\n",
      "[Training Epoch 7] Batch 26, Loss 0.2757716774940491\n",
      "[Training Epoch 7] Batch 27, Loss 0.2844642996788025\n",
      "[Training Epoch 7] Batch 28, Loss 0.2584901750087738\n",
      "[Training Epoch 7] Batch 29, Loss 0.2312055081129074\n",
      "[Training Epoch 7] Batch 30, Loss 0.2730025351047516\n",
      "[Training Epoch 7] Batch 31, Loss 0.27230679988861084\n",
      "[Training Epoch 7] Batch 32, Loss 0.23945355415344238\n",
      "[Training Epoch 7] Batch 33, Loss 0.24518561363220215\n",
      "[Training Epoch 7] Batch 34, Loss 0.24724164605140686\n",
      "[Training Epoch 7] Batch 35, Loss 0.2732497751712799\n",
      "[Training Epoch 7] Batch 36, Loss 0.25508642196655273\n",
      "[Training Epoch 7] Batch 37, Loss 0.23256631195545197\n",
      "[Training Epoch 7] Batch 38, Loss 0.24552875757217407\n",
      "[Training Epoch 7] Batch 39, Loss 0.25105178356170654\n",
      "[Training Epoch 7] Batch 40, Loss 0.24556559324264526\n",
      "[Training Epoch 7] Batch 41, Loss 0.2508088946342468\n",
      "[Training Epoch 7] Batch 42, Loss 0.25663211941719055\n",
      "[Training Epoch 7] Batch 43, Loss 0.24013173580169678\n",
      "[Training Epoch 7] Batch 44, Loss 0.220302551984787\n",
      "[Training Epoch 7] Batch 45, Loss 0.25898897647857666\n",
      "[Training Epoch 7] Batch 46, Loss 0.2297837734222412\n",
      "[Training Epoch 7] Batch 47, Loss 0.21631227433681488\n",
      "[Training Epoch 7] Batch 48, Loss 0.2827344834804535\n",
      "[Training Epoch 7] Batch 49, Loss 0.2594621777534485\n",
      "[Training Epoch 7] Batch 50, Loss 0.25657397508621216\n",
      "[Training Epoch 7] Batch 51, Loss 0.2507665157318115\n",
      "[Training Epoch 7] Batch 52, Loss 0.23111514747142792\n",
      "[Training Epoch 7] Batch 53, Loss 0.2490505576133728\n",
      "[Training Epoch 7] Batch 54, Loss 0.21816329658031464\n",
      "[Training Epoch 7] Batch 55, Loss 0.26721638441085815\n",
      "[Training Epoch 7] Batch 56, Loss 0.26387667655944824\n",
      "[Training Epoch 7] Batch 57, Loss 0.22961589694023132\n",
      "[Training Epoch 7] Batch 58, Loss 0.22680139541625977\n",
      "[Training Epoch 7] Batch 59, Loss 0.24811896681785583\n",
      "[Training Epoch 7] Batch 60, Loss 0.2440982460975647\n",
      "[Training Epoch 7] Batch 61, Loss 0.2671235203742981\n",
      "[Training Epoch 7] Batch 62, Loss 0.260895699262619\n",
      "[Training Epoch 7] Batch 63, Loss 0.23647110164165497\n",
      "[Training Epoch 7] Batch 64, Loss 0.2758854627609253\n",
      "[Training Epoch 7] Batch 65, Loss 0.2585638761520386\n",
      "[Training Epoch 7] Batch 66, Loss 0.2486145794391632\n",
      "[Training Epoch 7] Batch 67, Loss 0.2750990390777588\n",
      "[Training Epoch 7] Batch 68, Loss 0.27324700355529785\n",
      "[Training Epoch 7] Batch 69, Loss 0.2681473195552826\n",
      "[Training Epoch 7] Batch 70, Loss 0.22177061438560486\n",
      "[Training Epoch 7] Batch 71, Loss 0.25129085779190063\n",
      "[Training Epoch 7] Batch 72, Loss 0.25282615423202515\n",
      "[Training Epoch 7] Batch 73, Loss 0.25948262214660645\n",
      "[Training Epoch 7] Batch 74, Loss 0.2626808285713196\n",
      "[Training Epoch 7] Batch 75, Loss 0.2806679308414459\n",
      "[Training Epoch 7] Batch 76, Loss 0.24725444614887238\n",
      "[Training Epoch 7] Batch 77, Loss 0.22846722602844238\n",
      "[Training Epoch 7] Batch 78, Loss 0.2623823881149292\n",
      "[Training Epoch 7] Batch 79, Loss 0.25634756684303284\n",
      "[Training Epoch 7] Batch 80, Loss 0.2675541043281555\n",
      "[Training Epoch 7] Batch 81, Loss 0.24784669280052185\n",
      "[Training Epoch 7] Batch 82, Loss 0.25816741585731506\n",
      "[Training Epoch 7] Batch 83, Loss 0.2583543062210083\n",
      "[Training Epoch 7] Batch 84, Loss 0.26326310634613037\n",
      "[Training Epoch 7] Batch 85, Loss 0.2519829273223877\n",
      "[Training Epoch 7] Batch 86, Loss 0.2528226971626282\n",
      "[Training Epoch 7] Batch 87, Loss 0.2556813955307007\n",
      "[Training Epoch 7] Batch 88, Loss 0.2737232446670532\n",
      "[Training Epoch 7] Batch 89, Loss 0.26241666078567505\n",
      "[Training Epoch 7] Batch 90, Loss 0.2222362458705902\n",
      "[Training Epoch 7] Batch 91, Loss 0.2485399842262268\n",
      "[Training Epoch 7] Batch 92, Loss 0.2552821636199951\n",
      "[Training Epoch 7] Batch 93, Loss 0.2368645817041397\n",
      "[Training Epoch 7] Batch 94, Loss 0.26439207792282104\n",
      "[Training Epoch 7] Batch 95, Loss 0.2411465346813202\n",
      "[Training Epoch 7] Batch 96, Loss 0.254660964012146\n",
      "[Training Epoch 7] Batch 97, Loss 0.23608191311359406\n",
      "[Training Epoch 7] Batch 98, Loss 0.2310323417186737\n",
      "[Training Epoch 7] Batch 99, Loss 0.26260894536972046\n",
      "[Training Epoch 7] Batch 100, Loss 0.231950044631958\n",
      "[Training Epoch 7] Batch 101, Loss 0.2534976601600647\n",
      "[Training Epoch 7] Batch 102, Loss 0.23391981422901154\n",
      "[Training Epoch 7] Batch 103, Loss 0.24356737732887268\n",
      "[Training Epoch 7] Batch 104, Loss 0.23784396052360535\n",
      "[Training Epoch 7] Batch 105, Loss 0.24773187935352325\n",
      "[Training Epoch 7] Batch 106, Loss 0.19543179869651794\n",
      "[Training Epoch 7] Batch 107, Loss 0.2616787254810333\n",
      "[Training Epoch 7] Batch 108, Loss 0.25137859582901\n",
      "[Training Epoch 7] Batch 109, Loss 0.25121787190437317\n",
      "[Training Epoch 7] Batch 110, Loss 0.2633298635482788\n",
      "[Training Epoch 7] Batch 111, Loss 0.26361358165740967\n",
      "[Training Epoch 7] Batch 112, Loss 0.24126337468624115\n",
      "[Training Epoch 7] Batch 113, Loss 0.26780498027801514\n",
      "[Training Epoch 7] Batch 114, Loss 0.2344617247581482\n",
      "[Training Epoch 7] Batch 115, Loss 0.2662579417228699\n",
      "[Training Epoch 7] Batch 116, Loss 0.24781250953674316\n",
      "[Training Epoch 7] Batch 117, Loss 0.23806701600551605\n",
      "[Training Epoch 7] Batch 118, Loss 0.27854275703430176\n",
      "[Training Epoch 7] Batch 119, Loss 0.26239466667175293\n",
      "[Training Epoch 7] Batch 120, Loss 0.2532786726951599\n",
      "[Training Epoch 7] Batch 121, Loss 0.23903438448905945\n",
      "[Training Epoch 7] Batch 122, Loss 0.24661342799663544\n",
      "[Training Epoch 7] Batch 123, Loss 0.26070934534072876\n",
      "[Training Epoch 7] Batch 124, Loss 0.27230197191238403\n",
      "[Training Epoch 7] Batch 125, Loss 0.2784515619277954\n",
      "[Training Epoch 7] Batch 126, Loss 0.2578931748867035\n",
      "[Training Epoch 7] Batch 127, Loss 0.2521416246891022\n",
      "[Training Epoch 7] Batch 128, Loss 0.249797061085701\n",
      "[Training Epoch 7] Batch 129, Loss 0.26950228214263916\n",
      "[Training Epoch 7] Batch 130, Loss 0.24091926217079163\n",
      "[Training Epoch 7] Batch 131, Loss 0.24469512701034546\n",
      "[Training Epoch 7] Batch 132, Loss 0.2603864073753357\n",
      "[Training Epoch 7] Batch 133, Loss 0.24101783335208893\n",
      "[Training Epoch 7] Batch 134, Loss 0.24474196135997772\n",
      "[Training Epoch 7] Batch 135, Loss 0.26213377714157104\n",
      "[Training Epoch 7] Batch 136, Loss 0.26144134998321533\n",
      "[Training Epoch 7] Batch 137, Loss 0.27036941051483154\n",
      "[Training Epoch 7] Batch 138, Loss 0.2591915726661682\n",
      "[Training Epoch 7] Batch 139, Loss 0.2400924414396286\n",
      "[Training Epoch 7] Batch 140, Loss 0.26339292526245117\n",
      "[Training Epoch 7] Batch 141, Loss 0.2709598243236542\n",
      "[Training Epoch 7] Batch 142, Loss 0.2567165493965149\n",
      "[Training Epoch 7] Batch 143, Loss 0.2806990146636963\n",
      "[Training Epoch 7] Batch 144, Loss 0.24979406595230103\n",
      "[Training Epoch 7] Batch 145, Loss 0.2694818377494812\n",
      "[Training Epoch 7] Batch 146, Loss 0.24930962920188904\n",
      "[Training Epoch 7] Batch 147, Loss 0.24316132068634033\n",
      "[Training Epoch 7] Batch 148, Loss 0.24987508356571198\n",
      "[Training Epoch 7] Batch 149, Loss 0.2756739556789398\n",
      "[Training Epoch 7] Batch 150, Loss 0.25597071647644043\n",
      "[Training Epoch 7] Batch 151, Loss 0.2911902964115143\n",
      "[Training Epoch 7] Batch 152, Loss 0.25643134117126465\n",
      "[Training Epoch 7] Batch 153, Loss 0.24166801571846008\n",
      "[Training Epoch 7] Batch 154, Loss 0.27401024103164673\n",
      "[Training Epoch 7] Batch 155, Loss 0.26798272132873535\n",
      "[Training Epoch 7] Batch 156, Loss 0.2788453698158264\n",
      "[Training Epoch 7] Batch 157, Loss 0.23368209600448608\n",
      "[Training Epoch 7] Batch 158, Loss 0.23584115505218506\n",
      "[Training Epoch 7] Batch 159, Loss 0.23355913162231445\n",
      "[Training Epoch 7] Batch 160, Loss 0.2697249948978424\n",
      "[Training Epoch 7] Batch 161, Loss 0.26683321595191956\n",
      "[Training Epoch 7] Batch 162, Loss 0.25336483120918274\n",
      "[Training Epoch 7] Batch 163, Loss 0.2127062827348709\n",
      "[Training Epoch 7] Batch 164, Loss 0.2646930515766144\n",
      "[Training Epoch 7] Batch 165, Loss 0.27983781695365906\n",
      "[Training Epoch 7] Batch 166, Loss 0.26454100012779236\n",
      "[Training Epoch 7] Batch 167, Loss 0.25510042905807495\n",
      "[Training Epoch 7] Batch 168, Loss 0.2938009202480316\n",
      "[Training Epoch 7] Batch 169, Loss 0.23153948783874512\n",
      "[Training Epoch 7] Batch 170, Loss 0.24535946547985077\n",
      "[Training Epoch 7] Batch 171, Loss 0.2685408294200897\n",
      "[Training Epoch 7] Batch 172, Loss 0.25352025032043457\n",
      "[Training Epoch 7] Batch 173, Loss 0.23415735363960266\n",
      "[Training Epoch 7] Batch 174, Loss 0.26651424169540405\n",
      "[Training Epoch 7] Batch 175, Loss 0.24300546944141388\n",
      "[Training Epoch 7] Batch 176, Loss 0.2584006190299988\n",
      "[Training Epoch 7] Batch 177, Loss 0.2575727105140686\n",
      "[Training Epoch 7] Batch 178, Loss 0.21839839220046997\n",
      "[Training Epoch 7] Batch 179, Loss 0.2674475908279419\n",
      "[Training Epoch 7] Batch 180, Loss 0.27439242601394653\n",
      "[Training Epoch 7] Batch 181, Loss 0.25867322087287903\n",
      "[Training Epoch 7] Batch 182, Loss 0.257990300655365\n",
      "[Training Epoch 7] Batch 183, Loss 0.2399284690618515\n",
      "[Training Epoch 7] Batch 184, Loss 0.2716231346130371\n",
      "[Training Epoch 7] Batch 185, Loss 0.25296729803085327\n",
      "[Training Epoch 7] Batch 186, Loss 0.23022612929344177\n",
      "[Training Epoch 7] Batch 187, Loss 0.24890999495983124\n",
      "[Training Epoch 7] Batch 188, Loss 0.2449284791946411\n",
      "[Training Epoch 7] Batch 189, Loss 0.24066567420959473\n",
      "[Training Epoch 7] Batch 190, Loss 0.27739039063453674\n",
      "[Training Epoch 7] Batch 191, Loss 0.2504968047142029\n",
      "[Training Epoch 7] Batch 192, Loss 0.2612594962120056\n",
      "[Training Epoch 7] Batch 193, Loss 0.2510678768157959\n",
      "[Training Epoch 7] Batch 194, Loss 0.2573137879371643\n",
      "[Training Epoch 7] Batch 195, Loss 0.2902628481388092\n",
      "[Training Epoch 7] Batch 196, Loss 0.2652069926261902\n",
      "[Training Epoch 7] Batch 197, Loss 0.266431599855423\n",
      "[Training Epoch 7] Batch 198, Loss 0.26783519983291626\n",
      "[Training Epoch 7] Batch 199, Loss 0.23737388849258423\n",
      "[Training Epoch 7] Batch 200, Loss 0.2675025761127472\n",
      "[Training Epoch 7] Batch 201, Loss 0.2976504862308502\n",
      "[Training Epoch 7] Batch 202, Loss 0.24376234412193298\n",
      "[Training Epoch 7] Batch 203, Loss 0.24650618433952332\n",
      "[Training Epoch 7] Batch 204, Loss 0.2757799029350281\n",
      "[Training Epoch 7] Batch 205, Loss 0.23401355743408203\n",
      "[Training Epoch 7] Batch 206, Loss 0.254472941160202\n",
      "[Training Epoch 7] Batch 207, Loss 0.25586721301078796\n",
      "[Training Epoch 7] Batch 208, Loss 0.2571216821670532\n",
      "[Training Epoch 7] Batch 209, Loss 0.27139657735824585\n",
      "[Training Epoch 7] Batch 210, Loss 0.2413516342639923\n",
      "[Training Epoch 7] Batch 211, Loss 0.2547149360179901\n",
      "[Training Epoch 7] Batch 212, Loss 0.2611488997936249\n",
      "[Training Epoch 7] Batch 213, Loss 0.25704777240753174\n",
      "[Training Epoch 7] Batch 214, Loss 0.24765945971012115\n",
      "[Training Epoch 7] Batch 215, Loss 0.26654934883117676\n",
      "[Training Epoch 7] Batch 216, Loss 0.2617571949958801\n",
      "[Training Epoch 7] Batch 217, Loss 0.23710162937641144\n",
      "[Training Epoch 7] Batch 218, Loss 0.24957367777824402\n",
      "[Training Epoch 7] Batch 219, Loss 0.27083927392959595\n",
      "[Training Epoch 7] Batch 220, Loss 0.23980435729026794\n",
      "[Training Epoch 7] Batch 221, Loss 0.24181559681892395\n",
      "[Training Epoch 7] Batch 222, Loss 0.2387084662914276\n",
      "[Training Epoch 7] Batch 223, Loss 0.26177310943603516\n",
      "[Training Epoch 7] Batch 224, Loss 0.2651834487915039\n",
      "[Training Epoch 7] Batch 225, Loss 0.3019871711730957\n",
      "[Training Epoch 7] Batch 226, Loss 0.22280758619308472\n",
      "[Training Epoch 7] Batch 227, Loss 0.2662571668624878\n",
      "[Training Epoch 7] Batch 228, Loss 0.25470903515815735\n",
      "[Training Epoch 7] Batch 229, Loss 0.2607637941837311\n",
      "[Training Epoch 7] Batch 230, Loss 0.270460307598114\n",
      "[Training Epoch 7] Batch 231, Loss 0.23430362343788147\n",
      "[Training Epoch 7] Batch 232, Loss 0.24973717331886292\n",
      "[Training Epoch 7] Batch 233, Loss 0.24452807009220123\n",
      "[Training Epoch 7] Batch 234, Loss 0.26060259342193604\n",
      "[Training Epoch 7] Batch 235, Loss 0.23613035678863525\n",
      "[Training Epoch 7] Batch 236, Loss 0.23255008459091187\n",
      "[Training Epoch 7] Batch 237, Loss 0.26955974102020264\n",
      "[Training Epoch 7] Batch 238, Loss 0.25431710481643677\n",
      "[Training Epoch 7] Batch 239, Loss 0.22897297143936157\n",
      "[Training Epoch 7] Batch 240, Loss 0.2759181261062622\n",
      "[Training Epoch 7] Batch 241, Loss 0.2593116760253906\n",
      "[Training Epoch 7] Batch 242, Loss 0.24786944687366486\n",
      "[Training Epoch 7] Batch 243, Loss 0.25620198249816895\n",
      "[Training Epoch 7] Batch 244, Loss 0.2756299376487732\n",
      "[Training Epoch 7] Batch 245, Loss 0.2594740092754364\n",
      "[Training Epoch 7] Batch 246, Loss 0.2702750861644745\n",
      "[Training Epoch 7] Batch 247, Loss 0.2417421042919159\n",
      "[Training Epoch 7] Batch 248, Loss 0.24936679005622864\n",
      "[Training Epoch 7] Batch 249, Loss 0.2605518698692322\n",
      "[Training Epoch 7] Batch 250, Loss 0.2752349376678467\n",
      "[Training Epoch 7] Batch 251, Loss 0.25028157234191895\n",
      "[Training Epoch 7] Batch 252, Loss 0.23133772611618042\n",
      "[Training Epoch 7] Batch 253, Loss 0.2306491881608963\n",
      "[Training Epoch 7] Batch 254, Loss 0.27908623218536377\n",
      "[Training Epoch 7] Batch 255, Loss 0.23282736539840698\n",
      "[Training Epoch 7] Batch 256, Loss 0.236943319439888\n",
      "[Training Epoch 7] Batch 257, Loss 0.24818679690361023\n",
      "[Training Epoch 7] Batch 258, Loss 0.23825222253799438\n",
      "[Training Epoch 7] Batch 259, Loss 0.25293105840682983\n",
      "[Training Epoch 7] Batch 260, Loss 0.27330222725868225\n",
      "[Training Epoch 7] Batch 261, Loss 0.26700982451438904\n",
      "[Training Epoch 7] Batch 262, Loss 0.2446787804365158\n",
      "[Training Epoch 7] Batch 263, Loss 0.2206411212682724\n",
      "[Training Epoch 7] Batch 264, Loss 0.24210481345653534\n",
      "[Training Epoch 7] Batch 265, Loss 0.2625802755355835\n",
      "[Training Epoch 7] Batch 266, Loss 0.273098349571228\n",
      "[Training Epoch 7] Batch 267, Loss 0.2574394941329956\n",
      "[Training Epoch 7] Batch 268, Loss 0.2380678653717041\n",
      "[Training Epoch 7] Batch 269, Loss 0.27638134360313416\n",
      "[Training Epoch 7] Batch 270, Loss 0.2693508267402649\n",
      "[Training Epoch 7] Batch 271, Loss 0.2439766824245453\n",
      "[Training Epoch 7] Batch 272, Loss 0.2607885003089905\n",
      "[Training Epoch 7] Batch 273, Loss 0.26017826795578003\n",
      "[Training Epoch 7] Batch 274, Loss 0.25201791524887085\n",
      "[Training Epoch 7] Batch 275, Loss 0.23813144862651825\n",
      "[Training Epoch 7] Batch 276, Loss 0.25971150398254395\n",
      "[Training Epoch 7] Batch 277, Loss 0.2607685923576355\n",
      "[Training Epoch 7] Batch 278, Loss 0.25270670652389526\n",
      "[Training Epoch 7] Batch 279, Loss 0.23380404710769653\n",
      "[Training Epoch 7] Batch 280, Loss 0.24157294631004333\n",
      "[Training Epoch 7] Batch 281, Loss 0.2531207203865051\n",
      "[Training Epoch 7] Batch 282, Loss 0.23961377143859863\n",
      "[Training Epoch 7] Batch 283, Loss 0.24184051156044006\n",
      "[Training Epoch 7] Batch 284, Loss 0.24937109649181366\n",
      "[Training Epoch 7] Batch 285, Loss 0.2550755739212036\n",
      "[Training Epoch 7] Batch 286, Loss 0.24798454344272614\n",
      "[Training Epoch 7] Batch 287, Loss 0.2608964443206787\n",
      "[Training Epoch 7] Batch 288, Loss 0.2287008911371231\n",
      "[Training Epoch 7] Batch 289, Loss 0.2990646958351135\n",
      "[Training Epoch 7] Batch 290, Loss 0.25561437010765076\n",
      "[Training Epoch 7] Batch 291, Loss 0.2117360234260559\n",
      "[Training Epoch 7] Batch 292, Loss 0.27629491686820984\n",
      "[Training Epoch 7] Batch 293, Loss 0.2580586373806\n",
      "[Training Epoch 7] Batch 294, Loss 0.24163982272148132\n",
      "[Training Epoch 7] Batch 295, Loss 0.2277802973985672\n",
      "[Training Epoch 7] Batch 296, Loss 0.241016685962677\n",
      "[Training Epoch 7] Batch 297, Loss 0.24946758151054382\n",
      "[Training Epoch 7] Batch 298, Loss 0.21015115082263947\n",
      "[Training Epoch 7] Batch 299, Loss 0.24223846197128296\n",
      "[Training Epoch 7] Batch 300, Loss 0.2859278917312622\n",
      "[Training Epoch 7] Batch 301, Loss 0.23685409128665924\n",
      "[Training Epoch 7] Batch 302, Loss 0.2561876177787781\n",
      "[Training Epoch 7] Batch 303, Loss 0.2515804171562195\n",
      "[Training Epoch 7] Batch 304, Loss 0.2659294009208679\n",
      "[Training Epoch 7] Batch 305, Loss 0.25693726539611816\n",
      "[Training Epoch 7] Batch 306, Loss 0.2707480192184448\n",
      "[Training Epoch 7] Batch 307, Loss 0.2529885172843933\n",
      "[Training Epoch 7] Batch 308, Loss 0.23900796473026276\n",
      "[Training Epoch 7] Batch 309, Loss 0.2655099630355835\n",
      "[Training Epoch 7] Batch 310, Loss 0.2226695418357849\n",
      "[Training Epoch 7] Batch 311, Loss 0.23840753734111786\n",
      "[Training Epoch 7] Batch 312, Loss 0.2673113942146301\n",
      "[Training Epoch 7] Batch 313, Loss 0.2544785141944885\n",
      "[Training Epoch 7] Batch 314, Loss 0.23766173422336578\n",
      "[Training Epoch 7] Batch 315, Loss 0.24999135732650757\n",
      "[Training Epoch 7] Batch 316, Loss 0.2610279321670532\n",
      "[Training Epoch 7] Batch 317, Loss 0.2731276750564575\n",
      "[Training Epoch 7] Batch 318, Loss 0.2713490128517151\n",
      "[Training Epoch 7] Batch 319, Loss 0.25432664155960083\n",
      "[Training Epoch 7] Batch 320, Loss 0.2790519893169403\n",
      "[Training Epoch 7] Batch 321, Loss 0.2586361765861511\n",
      "[Training Epoch 7] Batch 322, Loss 0.2549874186515808\n",
      "[Training Epoch 7] Batch 323, Loss 0.23353058099746704\n",
      "[Training Epoch 7] Batch 324, Loss 0.2557510733604431\n",
      "[Training Epoch 7] Batch 325, Loss 0.2292468547821045\n",
      "[Training Epoch 7] Batch 326, Loss 0.2615105211734772\n",
      "[Training Epoch 7] Batch 327, Loss 0.25941452383995056\n",
      "[Training Epoch 7] Batch 328, Loss 0.24800387024879456\n",
      "[Training Epoch 7] Batch 329, Loss 0.2937037944793701\n",
      "[Training Epoch 7] Batch 330, Loss 0.23254597187042236\n",
      "[Training Epoch 7] Batch 331, Loss 0.24419206380844116\n",
      "[Training Epoch 7] Batch 332, Loss 0.2294098436832428\n",
      "[Training Epoch 7] Batch 333, Loss 0.26745176315307617\n",
      "[Training Epoch 7] Batch 334, Loss 0.2690759301185608\n",
      "[Training Epoch 7] Batch 335, Loss 0.23684531450271606\n",
      "[Training Epoch 7] Batch 336, Loss 0.2770971953868866\n",
      "[Training Epoch 7] Batch 337, Loss 0.22528788447380066\n",
      "[Training Epoch 7] Batch 338, Loss 0.2456565499305725\n",
      "[Training Epoch 7] Batch 339, Loss 0.25141480565071106\n",
      "[Training Epoch 7] Batch 340, Loss 0.2515687644481659\n",
      "[Training Epoch 7] Batch 341, Loss 0.24700352549552917\n",
      "[Training Epoch 7] Batch 342, Loss 0.22918960452079773\n",
      "[Training Epoch 7] Batch 343, Loss 0.2793515920639038\n",
      "[Training Epoch 7] Batch 344, Loss 0.2466142177581787\n",
      "[Training Epoch 7] Batch 345, Loss 0.24411644041538239\n",
      "[Training Epoch 7] Batch 346, Loss 0.24526037275791168\n",
      "[Training Epoch 7] Batch 347, Loss 0.2678900957107544\n",
      "[Training Epoch 7] Batch 348, Loss 0.2411859631538391\n",
      "[Training Epoch 7] Batch 349, Loss 0.2525019645690918\n",
      "[Training Epoch 7] Batch 350, Loss 0.26266732811927795\n",
      "[Training Epoch 7] Batch 351, Loss 0.2568119764328003\n",
      "[Training Epoch 7] Batch 352, Loss 0.23742488026618958\n",
      "[Training Epoch 7] Batch 353, Loss 0.276621013879776\n",
      "[Training Epoch 7] Batch 354, Loss 0.23884527385234833\n",
      "[Training Epoch 7] Batch 355, Loss 0.24520963430404663\n",
      "[Training Epoch 7] Batch 356, Loss 0.24524611234664917\n",
      "[Training Epoch 7] Batch 357, Loss 0.27485281229019165\n",
      "[Training Epoch 7] Batch 358, Loss 0.2379215806722641\n",
      "[Training Epoch 7] Batch 359, Loss 0.25416338443756104\n",
      "[Training Epoch 7] Batch 360, Loss 0.2727380692958832\n",
      "[Training Epoch 7] Batch 361, Loss 0.2619541883468628\n",
      "[Training Epoch 7] Batch 362, Loss 0.26420488953590393\n",
      "[Training Epoch 7] Batch 363, Loss 0.2561953067779541\n",
      "[Training Epoch 7] Batch 364, Loss 0.28008735179901123\n",
      "[Training Epoch 7] Batch 365, Loss 0.2521517872810364\n",
      "[Training Epoch 7] Batch 366, Loss 0.26321500539779663\n",
      "[Training Epoch 7] Batch 367, Loss 0.2440575361251831\n",
      "[Training Epoch 7] Batch 368, Loss 0.2529033422470093\n",
      "[Training Epoch 7] Batch 369, Loss 0.2642589509487152\n",
      "[Training Epoch 7] Batch 370, Loss 0.23646271228790283\n",
      "[Training Epoch 7] Batch 371, Loss 0.2707907259464264\n",
      "[Training Epoch 7] Batch 372, Loss 0.26823288202285767\n",
      "[Training Epoch 7] Batch 373, Loss 0.26468992233276367\n",
      "[Training Epoch 7] Batch 374, Loss 0.23488983511924744\n",
      "[Training Epoch 7] Batch 375, Loss 0.26999956369400024\n",
      "[Training Epoch 7] Batch 376, Loss 0.2698245644569397\n",
      "[Training Epoch 7] Batch 377, Loss 0.24468398094177246\n",
      "[Training Epoch 7] Batch 378, Loss 0.23606432974338531\n",
      "[Training Epoch 7] Batch 379, Loss 0.24363400042057037\n",
      "[Training Epoch 7] Batch 380, Loss 0.2802896797657013\n",
      "[Training Epoch 7] Batch 381, Loss 0.25408023595809937\n",
      "[Training Epoch 7] Batch 382, Loss 0.2646639943122864\n",
      "[Training Epoch 7] Batch 383, Loss 0.2559075355529785\n",
      "[Training Epoch 7] Batch 384, Loss 0.2438901960849762\n",
      "[Training Epoch 7] Batch 385, Loss 0.2627437710762024\n",
      "[Training Epoch 7] Batch 386, Loss 0.25134241580963135\n",
      "[Training Epoch 7] Batch 387, Loss 0.24956351518630981\n",
      "[Training Epoch 7] Batch 388, Loss 0.2300492227077484\n",
      "[Training Epoch 7] Batch 389, Loss 0.24151644110679626\n",
      "[Training Epoch 7] Batch 390, Loss 0.2601315975189209\n",
      "[Training Epoch 7] Batch 391, Loss 0.26069581508636475\n",
      "[Training Epoch 7] Batch 392, Loss 0.22512727975845337\n",
      "[Training Epoch 7] Batch 393, Loss 0.29022979736328125\n",
      "[Training Epoch 7] Batch 394, Loss 0.2749549150466919\n",
      "[Training Epoch 7] Batch 395, Loss 0.276287704706192\n",
      "[Training Epoch 7] Batch 396, Loss 0.24848629534244537\n",
      "[Training Epoch 7] Batch 397, Loss 0.25268223881721497\n",
      "[Training Epoch 7] Batch 398, Loss 0.2652058005332947\n",
      "[Training Epoch 7] Batch 399, Loss 0.24012678861618042\n",
      "[Training Epoch 7] Batch 400, Loss 0.2660863399505615\n",
      "[Training Epoch 7] Batch 401, Loss 0.2739391326904297\n",
      "[Training Epoch 7] Batch 402, Loss 0.2571890354156494\n",
      "[Training Epoch 7] Batch 403, Loss 0.2529093027114868\n",
      "[Training Epoch 7] Batch 404, Loss 0.2864789366722107\n",
      "[Training Epoch 7] Batch 405, Loss 0.24913480877876282\n",
      "[Training Epoch 7] Batch 406, Loss 0.2663033604621887\n",
      "[Training Epoch 7] Batch 407, Loss 0.254009872674942\n",
      "[Training Epoch 7] Batch 408, Loss 0.27029094099998474\n",
      "[Training Epoch 7] Batch 409, Loss 0.25224417448043823\n",
      "[Training Epoch 7] Batch 410, Loss 0.2867453694343567\n",
      "[Training Epoch 7] Batch 411, Loss 0.2614961564540863\n",
      "[Training Epoch 7] Batch 412, Loss 0.2420145869255066\n",
      "[Training Epoch 7] Batch 413, Loss 0.23165710270404816\n",
      "[Training Epoch 7] Batch 414, Loss 0.2451304942369461\n",
      "[Training Epoch 7] Batch 415, Loss 0.2533874213695526\n",
      "[Training Epoch 7] Batch 416, Loss 0.25654441118240356\n",
      "[Training Epoch 7] Batch 417, Loss 0.24602586030960083\n",
      "[Training Epoch 7] Batch 418, Loss 0.2664215564727783\n",
      "[Training Epoch 7] Batch 419, Loss 0.2511135935783386\n",
      "[Training Epoch 7] Batch 420, Loss 0.27764689922332764\n",
      "[Training Epoch 7] Batch 421, Loss 0.2532840371131897\n",
      "[Training Epoch 7] Batch 422, Loss 0.27289849519729614\n",
      "[Training Epoch 7] Batch 423, Loss 0.27272820472717285\n",
      "[Training Epoch 7] Batch 424, Loss 0.2578926086425781\n",
      "[Training Epoch 7] Batch 425, Loss 0.26411306858062744\n",
      "[Training Epoch 7] Batch 426, Loss 0.24865952134132385\n",
      "[Training Epoch 7] Batch 427, Loss 0.2533482611179352\n",
      "[Training Epoch 7] Batch 428, Loss 0.2530321478843689\n",
      "[Training Epoch 7] Batch 429, Loss 0.2515853941440582\n",
      "[Training Epoch 7] Batch 430, Loss 0.24885888397693634\n",
      "[Training Epoch 7] Batch 431, Loss 0.25317153334617615\n",
      "[Training Epoch 7] Batch 432, Loss 0.2560460865497589\n",
      "[Training Epoch 7] Batch 433, Loss 0.24339480698108673\n",
      "[Training Epoch 7] Batch 434, Loss 0.27722883224487305\n",
      "[Training Epoch 7] Batch 435, Loss 0.2733381390571594\n",
      "[Training Epoch 7] Batch 436, Loss 0.2373993992805481\n",
      "[Training Epoch 7] Batch 437, Loss 0.2601433992385864\n",
      "[Training Epoch 7] Batch 438, Loss 0.24793389439582825\n",
      "[Training Epoch 7] Batch 439, Loss 0.2530020773410797\n",
      "[Training Epoch 7] Batch 440, Loss 0.27500563859939575\n",
      "[Training Epoch 7] Batch 441, Loss 0.25407394766807556\n",
      "[Training Epoch 7] Batch 442, Loss 0.24052852392196655\n",
      "[Training Epoch 7] Batch 443, Loss 0.24031519889831543\n",
      "[Training Epoch 7] Batch 444, Loss 0.2542961835861206\n",
      "[Training Epoch 7] Batch 445, Loss 0.2606129050254822\n",
      "[Training Epoch 7] Batch 446, Loss 0.25511622428894043\n",
      "[Training Epoch 7] Batch 447, Loss 0.2545948028564453\n",
      "[Training Epoch 7] Batch 448, Loss 0.2678038477897644\n",
      "[Training Epoch 7] Batch 449, Loss 0.25164639949798584\n",
      "[Training Epoch 7] Batch 450, Loss 0.24223452806472778\n",
      "[Training Epoch 7] Batch 451, Loss 0.2581826448440552\n",
      "[Training Epoch 7] Batch 452, Loss 0.25332117080688477\n",
      "[Training Epoch 7] Batch 453, Loss 0.2633844017982483\n",
      "[Training Epoch 7] Batch 454, Loss 0.2415957748889923\n",
      "[Training Epoch 7] Batch 455, Loss 0.2743290960788727\n",
      "[Training Epoch 7] Batch 456, Loss 0.25564807653427124\n",
      "[Training Epoch 7] Batch 457, Loss 0.24172651767730713\n",
      "[Training Epoch 7] Batch 458, Loss 0.22549542784690857\n",
      "[Training Epoch 7] Batch 459, Loss 0.22539013624191284\n",
      "[Training Epoch 7] Batch 460, Loss 0.2652798593044281\n",
      "[Training Epoch 7] Batch 461, Loss 0.27589982748031616\n",
      "[Training Epoch 7] Batch 462, Loss 0.257734090089798\n",
      "[Training Epoch 7] Batch 463, Loss 0.27918004989624023\n",
      "[Training Epoch 7] Batch 464, Loss 0.29765570163726807\n",
      "[Training Epoch 7] Batch 465, Loss 0.2285989224910736\n",
      "[Training Epoch 7] Batch 466, Loss 0.24278122186660767\n",
      "[Training Epoch 7] Batch 467, Loss 0.2735751271247864\n",
      "[Training Epoch 7] Batch 468, Loss 0.2385914921760559\n",
      "[Training Epoch 7] Batch 469, Loss 0.26335853338241577\n",
      "[Training Epoch 7] Batch 470, Loss 0.24653980135917664\n",
      "[Training Epoch 7] Batch 471, Loss 0.24770528078079224\n",
      "[Training Epoch 7] Batch 472, Loss 0.2587774693965912\n",
      "[Training Epoch 7] Batch 473, Loss 0.2726319134235382\n",
      "[Training Epoch 7] Batch 474, Loss 0.26986995339393616\n",
      "[Training Epoch 7] Batch 475, Loss 0.24812093377113342\n",
      "[Training Epoch 7] Batch 476, Loss 0.2928645610809326\n",
      "[Training Epoch 7] Batch 477, Loss 0.22718289494514465\n",
      "[Training Epoch 7] Batch 478, Loss 0.25750643014907837\n",
      "[Training Epoch 7] Batch 479, Loss 0.2803337574005127\n",
      "[Training Epoch 7] Batch 480, Loss 0.26751071214675903\n",
      "[Training Epoch 7] Batch 481, Loss 0.24193823337554932\n",
      "[Training Epoch 7] Batch 482, Loss 0.2618560791015625\n",
      "[Training Epoch 7] Batch 483, Loss 0.26796817779541016\n",
      "[Training Epoch 7] Batch 484, Loss 0.24671432375907898\n",
      "[Training Epoch 7] Batch 485, Loss 0.26020702719688416\n",
      "[Training Epoch 7] Batch 486, Loss 0.2520807981491089\n",
      "[Training Epoch 7] Batch 487, Loss 0.26286911964416504\n",
      "[Training Epoch 7] Batch 488, Loss 0.2607885003089905\n",
      "[Training Epoch 7] Batch 489, Loss 0.23225344717502594\n",
      "[Training Epoch 7] Batch 490, Loss 0.2514225244522095\n",
      "[Training Epoch 7] Batch 491, Loss 0.2827486991882324\n",
      "[Training Epoch 7] Batch 492, Loss 0.2559880316257477\n",
      "[Training Epoch 7] Batch 493, Loss 0.2553931176662445\n",
      "[Training Epoch 7] Batch 494, Loss 0.26589882373809814\n",
      "[Training Epoch 7] Batch 495, Loss 0.246919184923172\n",
      "[Training Epoch 7] Batch 496, Loss 0.2590363025665283\n",
      "[Training Epoch 7] Batch 497, Loss 0.25120365619659424\n",
      "[Training Epoch 7] Batch 498, Loss 0.2596490681171417\n",
      "[Training Epoch 7] Batch 499, Loss 0.2725004553794861\n",
      "[Training Epoch 7] Batch 500, Loss 0.23920609056949615\n",
      "[Training Epoch 7] Batch 501, Loss 0.2101200371980667\n",
      "[Training Epoch 7] Batch 502, Loss 0.24507607519626617\n",
      "[Training Epoch 7] Batch 503, Loss 0.26011672616004944\n",
      "[Training Epoch 7] Batch 504, Loss 0.2516520321369171\n",
      "[Training Epoch 7] Batch 505, Loss 0.24086754024028778\n",
      "[Training Epoch 7] Batch 506, Loss 0.23743875324726105\n",
      "[Training Epoch 7] Batch 507, Loss 0.2767583727836609\n",
      "[Training Epoch 7] Batch 508, Loss 0.24973350763320923\n",
      "[Training Epoch 7] Batch 509, Loss 0.27364519238471985\n",
      "[Training Epoch 7] Batch 510, Loss 0.2491465061903\n",
      "[Training Epoch 7] Batch 511, Loss 0.22596655786037445\n",
      "[Training Epoch 7] Batch 512, Loss 0.24965256452560425\n",
      "[Training Epoch 7] Batch 513, Loss 0.23909354209899902\n",
      "[Training Epoch 7] Batch 514, Loss 0.2453567534685135\n",
      "[Training Epoch 7] Batch 515, Loss 0.29269692301750183\n",
      "[Training Epoch 7] Batch 516, Loss 0.26336002349853516\n",
      "[Training Epoch 7] Batch 517, Loss 0.2619566321372986\n",
      "[Training Epoch 7] Batch 518, Loss 0.22881454229354858\n",
      "[Training Epoch 7] Batch 519, Loss 0.2505503296852112\n",
      "[Training Epoch 7] Batch 520, Loss 0.25048112869262695\n",
      "[Training Epoch 7] Batch 521, Loss 0.2681308090686798\n",
      "[Training Epoch 7] Batch 522, Loss 0.24518123269081116\n",
      "[Training Epoch 7] Batch 523, Loss 0.2364855408668518\n",
      "[Training Epoch 7] Batch 524, Loss 0.27318084239959717\n",
      "[Training Epoch 7] Batch 525, Loss 0.2653692960739136\n",
      "[Training Epoch 7] Batch 526, Loss 0.251590371131897\n",
      "[Training Epoch 7] Batch 527, Loss 0.22211602330207825\n",
      "[Training Epoch 7] Batch 528, Loss 0.24231156706809998\n",
      "[Training Epoch 7] Batch 529, Loss 0.25496721267700195\n",
      "[Training Epoch 7] Batch 530, Loss 0.28304150700569153\n",
      "[Training Epoch 7] Batch 531, Loss 0.2574899196624756\n",
      "[Training Epoch 7] Batch 532, Loss 0.2613542377948761\n",
      "[Training Epoch 7] Batch 533, Loss 0.24257686734199524\n",
      "[Training Epoch 7] Batch 534, Loss 0.25774651765823364\n",
      "[Training Epoch 7] Batch 535, Loss 0.22123949229717255\n",
      "[Training Epoch 7] Batch 536, Loss 0.23695559799671173\n",
      "[Training Epoch 7] Batch 537, Loss 0.2932469844818115\n",
      "[Training Epoch 7] Batch 538, Loss 0.2315247356891632\n",
      "[Training Epoch 7] Batch 539, Loss 0.24997711181640625\n",
      "[Training Epoch 7] Batch 540, Loss 0.24206170439720154\n",
      "[Training Epoch 7] Batch 541, Loss 0.24912166595458984\n",
      "[Training Epoch 7] Batch 542, Loss 0.2594785690307617\n",
      "[Training Epoch 7] Batch 543, Loss 0.2552594542503357\n",
      "[Training Epoch 7] Batch 544, Loss 0.2513609528541565\n",
      "[Training Epoch 7] Batch 545, Loss 0.24925866723060608\n",
      "[Training Epoch 7] Batch 546, Loss 0.2551790177822113\n",
      "[Training Epoch 7] Batch 547, Loss 0.25306379795074463\n",
      "[Training Epoch 7] Batch 548, Loss 0.28174591064453125\n",
      "[Training Epoch 7] Batch 549, Loss 0.24502046406269073\n",
      "[Training Epoch 7] Batch 550, Loss 0.24631696939468384\n",
      "[Training Epoch 7] Batch 551, Loss 0.23989814519882202\n",
      "[Training Epoch 7] Batch 552, Loss 0.24144354462623596\n",
      "[Training Epoch 7] Batch 553, Loss 0.2429177165031433\n",
      "[Training Epoch 7] Batch 554, Loss 0.25640183687210083\n",
      "[Training Epoch 7] Batch 555, Loss 0.25079771876335144\n",
      "[Training Epoch 7] Batch 556, Loss 0.26812195777893066\n",
      "[Training Epoch 7] Batch 557, Loss 0.2570505738258362\n",
      "[Training Epoch 7] Batch 558, Loss 0.26425814628601074\n",
      "[Training Epoch 7] Batch 559, Loss 0.29042646288871765\n",
      "[Training Epoch 7] Batch 560, Loss 0.2590302526950836\n",
      "[Training Epoch 7] Batch 561, Loss 0.2597888708114624\n",
      "[Training Epoch 7] Batch 562, Loss 0.2367783784866333\n",
      "[Training Epoch 7] Batch 563, Loss 0.2444971203804016\n",
      "[Training Epoch 7] Batch 564, Loss 0.2544827461242676\n",
      "[Training Epoch 7] Batch 565, Loss 0.2574031949043274\n",
      "[Training Epoch 7] Batch 566, Loss 0.22812461853027344\n",
      "[Training Epoch 7] Batch 567, Loss 0.2526445984840393\n",
      "[Training Epoch 7] Batch 568, Loss 0.24324531853199005\n",
      "[Training Epoch 7] Batch 569, Loss 0.23940406739711761\n",
      "[Training Epoch 7] Batch 570, Loss 0.25453895330429077\n",
      "[Training Epoch 7] Batch 571, Loss 0.24762952327728271\n",
      "[Training Epoch 7] Batch 572, Loss 0.26695716381073\n",
      "[Training Epoch 7] Batch 573, Loss 0.24709059298038483\n",
      "[Training Epoch 7] Batch 574, Loss 0.27127474546432495\n",
      "[Training Epoch 7] Batch 575, Loss 0.2820695638656616\n",
      "[Training Epoch 7] Batch 576, Loss 0.26040562987327576\n",
      "[Training Epoch 7] Batch 577, Loss 0.2647911012172699\n",
      "[Training Epoch 7] Batch 578, Loss 0.2554333806037903\n",
      "[Training Epoch 7] Batch 579, Loss 0.25345224142074585\n",
      "[Training Epoch 7] Batch 580, Loss 0.2727604806423187\n",
      "[Training Epoch 7] Batch 581, Loss 0.25451236963272095\n",
      "[Training Epoch 7] Batch 582, Loss 0.27078646421432495\n",
      "[Training Epoch 7] Batch 583, Loss 0.25230491161346436\n",
      "[Training Epoch 7] Batch 584, Loss 0.27629661560058594\n",
      "[Training Epoch 7] Batch 585, Loss 0.26166000962257385\n",
      "[Training Epoch 7] Batch 586, Loss 0.2662835121154785\n",
      "[Training Epoch 7] Batch 587, Loss 0.25186023116111755\n",
      "[Training Epoch 7] Batch 588, Loss 0.2573833465576172\n",
      "[Training Epoch 7] Batch 589, Loss 0.24785205721855164\n",
      "[Training Epoch 7] Batch 590, Loss 0.2465888112783432\n",
      "[Training Epoch 7] Batch 591, Loss 0.2203306406736374\n",
      "[Training Epoch 7] Batch 592, Loss 0.24352054297924042\n",
      "[Training Epoch 7] Batch 593, Loss 0.2463655322790146\n",
      "[Training Epoch 7] Batch 594, Loss 0.2730039954185486\n",
      "[Training Epoch 7] Batch 595, Loss 0.25052982568740845\n",
      "[Training Epoch 7] Batch 596, Loss 0.25919926166534424\n",
      "[Training Epoch 7] Batch 597, Loss 0.25149911642074585\n",
      "[Training Epoch 7] Batch 598, Loss 0.23378843069076538\n",
      "[Training Epoch 7] Batch 599, Loss 0.2845303416252136\n",
      "[Training Epoch 7] Batch 600, Loss 0.2588212788105011\n",
      "[Training Epoch 7] Batch 601, Loss 0.26625919342041016\n",
      "[Training Epoch 7] Batch 602, Loss 0.2799839973449707\n",
      "[Training Epoch 7] Batch 603, Loss 0.2307029813528061\n",
      "[Training Epoch 7] Batch 604, Loss 0.24962811172008514\n",
      "[Training Epoch 7] Batch 605, Loss 0.28178635239601135\n",
      "[Training Epoch 7] Batch 606, Loss 0.26656481623649597\n",
      "[Training Epoch 7] Batch 607, Loss 0.2664271593093872\n",
      "[Training Epoch 7] Batch 608, Loss 0.2882493734359741\n",
      "[Training Epoch 7] Batch 609, Loss 0.23385655879974365\n",
      "[Training Epoch 7] Batch 610, Loss 0.2643992602825165\n",
      "[Training Epoch 7] Batch 611, Loss 0.23245646059513092\n",
      "[Training Epoch 7] Batch 612, Loss 0.26892662048339844\n",
      "[Training Epoch 7] Batch 613, Loss 0.2515106797218323\n",
      "[Training Epoch 7] Batch 614, Loss 0.2476336508989334\n",
      "[Training Epoch 7] Batch 615, Loss 0.26785188913345337\n",
      "[Training Epoch 7] Batch 616, Loss 0.26732337474823\n",
      "[Training Epoch 7] Batch 617, Loss 0.2565973997116089\n",
      "[Training Epoch 7] Batch 618, Loss 0.24662573635578156\n",
      "[Training Epoch 7] Batch 619, Loss 0.27750256657600403\n",
      "[Training Epoch 7] Batch 620, Loss 0.24988561868667603\n",
      "[Training Epoch 7] Batch 621, Loss 0.2590136229991913\n",
      "[Training Epoch 7] Batch 622, Loss 0.2526230216026306\n",
      "[Training Epoch 7] Batch 623, Loss 0.24560190737247467\n",
      "[Training Epoch 7] Batch 624, Loss 0.24082982540130615\n",
      "[Training Epoch 7] Batch 625, Loss 0.24059979617595673\n",
      "[Training Epoch 7] Batch 626, Loss 0.24998542666435242\n",
      "[Training Epoch 7] Batch 627, Loss 0.262275755405426\n",
      "[Training Epoch 7] Batch 628, Loss 0.25291579961776733\n",
      "[Training Epoch 7] Batch 629, Loss 0.2480420172214508\n",
      "[Training Epoch 7] Batch 630, Loss 0.2607857584953308\n",
      "[Training Epoch 7] Batch 631, Loss 0.22392359375953674\n",
      "[Training Epoch 7] Batch 632, Loss 0.26928243041038513\n",
      "[Training Epoch 7] Batch 633, Loss 0.24173592031002045\n",
      "[Training Epoch 7] Batch 634, Loss 0.2673693299293518\n",
      "[Training Epoch 7] Batch 635, Loss 0.2358832061290741\n",
      "[Training Epoch 7] Batch 636, Loss 0.2728407680988312\n",
      "[Training Epoch 7] Batch 637, Loss 0.25152790546417236\n",
      "[Training Epoch 7] Batch 638, Loss 0.25848597288131714\n",
      "[Training Epoch 7] Batch 639, Loss 0.230294331908226\n",
      "[Training Epoch 7] Batch 640, Loss 0.23989194631576538\n",
      "[Training Epoch 7] Batch 641, Loss 0.24308854341506958\n",
      "[Training Epoch 7] Batch 642, Loss 0.2623530328273773\n",
      "[Training Epoch 7] Batch 643, Loss 0.27726316452026367\n",
      "[Training Epoch 7] Batch 644, Loss 0.24778929352760315\n",
      "[Training Epoch 7] Batch 645, Loss 0.23861658573150635\n",
      "[Training Epoch 7] Batch 646, Loss 0.3057706356048584\n",
      "[Training Epoch 7] Batch 647, Loss 0.2268531620502472\n",
      "[Training Epoch 7] Batch 648, Loss 0.25347065925598145\n",
      "[Training Epoch 7] Batch 649, Loss 0.227649986743927\n",
      "[Training Epoch 7] Batch 650, Loss 0.25085997581481934\n",
      "[Training Epoch 7] Batch 651, Loss 0.27291807532310486\n",
      "[Training Epoch 7] Batch 652, Loss 0.24095365405082703\n",
      "[Training Epoch 7] Batch 653, Loss 0.2520531415939331\n",
      "[Training Epoch 7] Batch 654, Loss 0.26007482409477234\n",
      "[Training Epoch 7] Batch 655, Loss 0.26781290769577026\n",
      "[Training Epoch 7] Batch 656, Loss 0.24715638160705566\n",
      "[Training Epoch 7] Batch 657, Loss 0.25153693556785583\n",
      "[Training Epoch 7] Batch 658, Loss 0.24527296423912048\n",
      "[Training Epoch 7] Batch 659, Loss 0.24903002381324768\n",
      "[Training Epoch 7] Batch 660, Loss 0.262015163898468\n",
      "[Training Epoch 7] Batch 661, Loss 0.22853876650333405\n",
      "[Training Epoch 7] Batch 662, Loss 0.27089375257492065\n",
      "[Training Epoch 7] Batch 663, Loss 0.2521299123764038\n",
      "[Training Epoch 7] Batch 664, Loss 0.25134265422821045\n",
      "[Training Epoch 7] Batch 665, Loss 0.2681054472923279\n",
      "[Training Epoch 7] Batch 666, Loss 0.28065767884254456\n",
      "[Training Epoch 7] Batch 667, Loss 0.25079530477523804\n",
      "[Training Epoch 7] Batch 668, Loss 0.24366478621959686\n",
      "[Training Epoch 7] Batch 669, Loss 0.26560789346694946\n",
      "[Training Epoch 7] Batch 670, Loss 0.2672756612300873\n",
      "[Training Epoch 7] Batch 671, Loss 0.27555572986602783\n",
      "[Training Epoch 7] Batch 672, Loss 0.278236448764801\n",
      "[Training Epoch 7] Batch 673, Loss 0.25606095790863037\n",
      "[Training Epoch 7] Batch 674, Loss 0.24821847677230835\n",
      "[Training Epoch 7] Batch 675, Loss 0.279341459274292\n",
      "[Training Epoch 7] Batch 676, Loss 0.29061317443847656\n",
      "[Training Epoch 7] Batch 677, Loss 0.2726317048072815\n",
      "[Training Epoch 7] Batch 678, Loss 0.2668915092945099\n",
      "[Training Epoch 7] Batch 679, Loss 0.27403369545936584\n",
      "[Training Epoch 7] Batch 680, Loss 0.2729955017566681\n",
      "[Training Epoch 7] Batch 681, Loss 0.26374709606170654\n",
      "[Training Epoch 7] Batch 682, Loss 0.24830615520477295\n",
      "[Training Epoch 7] Batch 683, Loss 0.2565979063510895\n",
      "[Training Epoch 7] Batch 684, Loss 0.23888012766838074\n",
      "[Training Epoch 7] Batch 685, Loss 0.28196150064468384\n",
      "[Training Epoch 7] Batch 686, Loss 0.25832948088645935\n",
      "[Training Epoch 7] Batch 687, Loss 0.24316386878490448\n",
      "[Training Epoch 7] Batch 688, Loss 0.23079228401184082\n",
      "[Training Epoch 7] Batch 689, Loss 0.23689517378807068\n",
      "[Training Epoch 7] Batch 690, Loss 0.24294006824493408\n",
      "[Training Epoch 7] Batch 691, Loss 0.2209993302822113\n",
      "[Training Epoch 7] Batch 692, Loss 0.2645992040634155\n",
      "[Training Epoch 7] Batch 693, Loss 0.23687860369682312\n",
      "[Training Epoch 7] Batch 694, Loss 0.2536197304725647\n",
      "[Training Epoch 7] Batch 695, Loss 0.28959041833877563\n",
      "[Training Epoch 7] Batch 696, Loss 0.2763323187828064\n",
      "[Training Epoch 7] Batch 697, Loss 0.26902520656585693\n",
      "[Training Epoch 7] Batch 698, Loss 0.28174126148223877\n",
      "[Training Epoch 7] Batch 699, Loss 0.25452062487602234\n",
      "[Training Epoch 7] Batch 700, Loss 0.25475794076919556\n",
      "[Training Epoch 7] Batch 701, Loss 0.2493959665298462\n",
      "[Training Epoch 7] Batch 702, Loss 0.2594241797924042\n",
      "[Training Epoch 7] Batch 703, Loss 0.22902143001556396\n",
      "[Training Epoch 7] Batch 704, Loss 0.2435137927532196\n",
      "[Training Epoch 7] Batch 705, Loss 0.2640397548675537\n",
      "[Training Epoch 7] Batch 706, Loss 0.25655797123908997\n",
      "[Training Epoch 7] Batch 707, Loss 0.24429075419902802\n",
      "[Training Epoch 7] Batch 708, Loss 0.2799903154373169\n",
      "[Training Epoch 7] Batch 709, Loss 0.23837462067604065\n",
      "[Training Epoch 7] Batch 710, Loss 0.235044926404953\n",
      "[Training Epoch 7] Batch 711, Loss 0.24114546179771423\n",
      "[Training Epoch 7] Batch 712, Loss 0.2551698386669159\n",
      "[Training Epoch 7] Batch 713, Loss 0.2828068137168884\n",
      "[Training Epoch 7] Batch 714, Loss 0.24962085485458374\n",
      "[Training Epoch 7] Batch 715, Loss 0.24310284852981567\n",
      "[Training Epoch 7] Batch 716, Loss 0.24815219640731812\n",
      "[Training Epoch 7] Batch 717, Loss 0.27486366033554077\n",
      "[Training Epoch 7] Batch 718, Loss 0.2795547842979431\n",
      "[Training Epoch 7] Batch 719, Loss 0.2575245499610901\n",
      "[Training Epoch 7] Batch 720, Loss 0.2764609456062317\n",
      "[Training Epoch 7] Batch 721, Loss 0.25531426072120667\n",
      "[Training Epoch 7] Batch 722, Loss 0.25718894600868225\n",
      "[Training Epoch 7] Batch 723, Loss 0.2579067051410675\n",
      "[Training Epoch 7] Batch 724, Loss 0.24675890803337097\n",
      "[Training Epoch 7] Batch 725, Loss 0.24754181504249573\n",
      "[Training Epoch 7] Batch 726, Loss 0.25709325075149536\n",
      "[Training Epoch 7] Batch 727, Loss 0.2692895233631134\n",
      "[Training Epoch 7] Batch 728, Loss 0.26380079984664917\n",
      "[Training Epoch 7] Batch 729, Loss 0.2502945363521576\n",
      "[Training Epoch 7] Batch 730, Loss 0.2241652011871338\n",
      "[Training Epoch 7] Batch 731, Loss 0.26914462447166443\n",
      "[Training Epoch 7] Batch 732, Loss 0.26089048385620117\n",
      "[Training Epoch 7] Batch 733, Loss 0.25838232040405273\n",
      "[Training Epoch 7] Batch 734, Loss 0.22447746992111206\n",
      "[Training Epoch 7] Batch 735, Loss 0.23501518368721008\n",
      "[Training Epoch 7] Batch 736, Loss 0.24005009233951569\n",
      "[Training Epoch 7] Batch 737, Loss 0.24214020371437073\n",
      "[Training Epoch 7] Batch 738, Loss 0.28016233444213867\n",
      "[Training Epoch 7] Batch 739, Loss 0.2772834599018097\n",
      "[Training Epoch 7] Batch 740, Loss 0.2529430687427521\n",
      "[Training Epoch 7] Batch 741, Loss 0.23600122332572937\n",
      "[Training Epoch 7] Batch 742, Loss 0.24966035783290863\n",
      "[Training Epoch 7] Batch 743, Loss 0.2529933750629425\n",
      "[Training Epoch 7] Batch 744, Loss 0.2750239372253418\n",
      "[Training Epoch 7] Batch 745, Loss 0.23969301581382751\n",
      "[Training Epoch 7] Batch 746, Loss 0.25838369131088257\n",
      "[Training Epoch 7] Batch 747, Loss 0.24603423476219177\n",
      "[Training Epoch 7] Batch 748, Loss 0.23255354166030884\n",
      "[Training Epoch 7] Batch 749, Loss 0.24262364208698273\n",
      "[Training Epoch 7] Batch 750, Loss 0.24336117506027222\n",
      "[Training Epoch 7] Batch 751, Loss 0.2610473036766052\n",
      "[Training Epoch 7] Batch 752, Loss 0.24923521280288696\n",
      "[Training Epoch 7] Batch 753, Loss 0.24484556913375854\n",
      "[Training Epoch 7] Batch 754, Loss 0.2734312415122986\n",
      "[Training Epoch 7] Batch 755, Loss 0.2569897174835205\n",
      "[Training Epoch 7] Batch 756, Loss 0.2704663872718811\n",
      "[Training Epoch 7] Batch 757, Loss 0.24759483337402344\n",
      "[Training Epoch 7] Batch 758, Loss 0.28645268082618713\n",
      "[Training Epoch 7] Batch 759, Loss 0.24167054891586304\n",
      "[Training Epoch 7] Batch 760, Loss 0.23753006756305695\n",
      "[Training Epoch 7] Batch 761, Loss 0.26914122700691223\n",
      "[Training Epoch 7] Batch 762, Loss 0.23012781143188477\n",
      "[Training Epoch 7] Batch 763, Loss 0.23572242259979248\n",
      "[Training Epoch 7] Batch 764, Loss 0.27244362235069275\n",
      "[Training Epoch 7] Batch 765, Loss 0.24410003423690796\n",
      "[Training Epoch 7] Batch 766, Loss 0.2605304718017578\n",
      "[Training Epoch 7] Batch 767, Loss 0.25796744227409363\n",
      "[Training Epoch 7] Batch 768, Loss 0.2554520070552826\n",
      "[Training Epoch 7] Batch 769, Loss 0.2530534863471985\n",
      "[Training Epoch 7] Batch 770, Loss 0.2369726300239563\n",
      "[Training Epoch 7] Batch 771, Loss 0.2365419566631317\n",
      "[Training Epoch 7] Batch 772, Loss 0.25359922647476196\n",
      "[Training Epoch 7] Batch 773, Loss 0.24520573019981384\n",
      "[Training Epoch 7] Batch 774, Loss 0.2268807590007782\n",
      "[Training Epoch 7] Batch 775, Loss 0.25527942180633545\n",
      "[Training Epoch 7] Batch 776, Loss 0.24858063459396362\n",
      "[Training Epoch 7] Batch 777, Loss 0.2566010653972626\n",
      "[Training Epoch 7] Batch 778, Loss 0.24539421498775482\n",
      "[Training Epoch 7] Batch 779, Loss 0.2551369071006775\n",
      "[Training Epoch 7] Batch 780, Loss 0.26695889234542847\n",
      "[Training Epoch 7] Batch 781, Loss 0.23937296867370605\n",
      "[Training Epoch 7] Batch 782, Loss 0.2604689300060272\n",
      "[Training Epoch 7] Batch 783, Loss 0.25527721643447876\n",
      "[Training Epoch 7] Batch 784, Loss 0.23512014746665955\n",
      "[Training Epoch 7] Batch 785, Loss 0.26415377855300903\n",
      "[Training Epoch 7] Batch 786, Loss 0.2510819733142853\n",
      "[Training Epoch 7] Batch 787, Loss 0.26691386103630066\n",
      "[Training Epoch 7] Batch 788, Loss 0.2789483070373535\n",
      "[Training Epoch 7] Batch 789, Loss 0.30338358879089355\n",
      "[Training Epoch 7] Batch 790, Loss 0.26207152009010315\n",
      "[Training Epoch 7] Batch 791, Loss 0.2704296112060547\n",
      "[Training Epoch 7] Batch 792, Loss 0.24005433917045593\n",
      "[Training Epoch 7] Batch 793, Loss 0.2563992738723755\n",
      "[Training Epoch 7] Batch 794, Loss 0.2766934335231781\n",
      "[Training Epoch 7] Batch 795, Loss 0.2372587025165558\n",
      "[Training Epoch 7] Batch 796, Loss 0.24068060517311096\n",
      "[Training Epoch 7] Batch 797, Loss 0.22461554408073425\n",
      "[Training Epoch 7] Batch 798, Loss 0.24419733881950378\n",
      "[Training Epoch 7] Batch 799, Loss 0.26111453771591187\n",
      "[Training Epoch 7] Batch 800, Loss 0.2693302035331726\n",
      "[Training Epoch 7] Batch 801, Loss 0.26128363609313965\n",
      "[Training Epoch 7] Batch 802, Loss 0.21743658185005188\n",
      "[Training Epoch 7] Batch 803, Loss 0.2807362973690033\n",
      "[Training Epoch 7] Batch 804, Loss 0.24352465569972992\n",
      "[Training Epoch 7] Batch 805, Loss 0.24545931816101074\n",
      "[Training Epoch 7] Batch 806, Loss 0.2554980516433716\n",
      "[Training Epoch 7] Batch 807, Loss 0.2409619837999344\n",
      "[Training Epoch 7] Batch 808, Loss 0.25367656350135803\n",
      "[Training Epoch 7] Batch 809, Loss 0.24645712971687317\n",
      "[Training Epoch 7] Batch 810, Loss 0.23975853621959686\n",
      "[Training Epoch 7] Batch 811, Loss 0.2966260313987732\n",
      "[Training Epoch 7] Batch 812, Loss 0.25616270303726196\n",
      "[Training Epoch 7] Batch 813, Loss 0.236691415309906\n",
      "[Training Epoch 7] Batch 814, Loss 0.2621627748012543\n",
      "[Training Epoch 7] Batch 815, Loss 0.25981074571609497\n",
      "[Training Epoch 7] Batch 816, Loss 0.25179144740104675\n",
      "[Training Epoch 7] Batch 817, Loss 0.2512699365615845\n",
      "[Training Epoch 7] Batch 818, Loss 0.2409341037273407\n",
      "[Training Epoch 7] Batch 819, Loss 0.2528381943702698\n",
      "[Training Epoch 7] Batch 820, Loss 0.24914176762104034\n",
      "[Training Epoch 7] Batch 821, Loss 0.23925915360450745\n",
      "[Training Epoch 7] Batch 822, Loss 0.2591053545475006\n",
      "[Training Epoch 7] Batch 823, Loss 0.2137729823589325\n",
      "[Training Epoch 7] Batch 824, Loss 0.22016924619674683\n",
      "[Training Epoch 7] Batch 825, Loss 0.2520587742328644\n",
      "[Training Epoch 7] Batch 826, Loss 0.23116278648376465\n",
      "[Training Epoch 7] Batch 827, Loss 0.2501262426376343\n",
      "[Training Epoch 7] Batch 828, Loss 0.24380509555339813\n",
      "[Training Epoch 7] Batch 829, Loss 0.2595871090888977\n",
      "[Training Epoch 7] Batch 830, Loss 0.24547839164733887\n",
      "[Training Epoch 7] Batch 831, Loss 0.2614084482192993\n",
      "[Training Epoch 7] Batch 832, Loss 0.24731317162513733\n",
      "[Training Epoch 7] Batch 833, Loss 0.24353736639022827\n",
      "[Training Epoch 7] Batch 834, Loss 0.240365669131279\n",
      "[Training Epoch 7] Batch 835, Loss 0.26228994131088257\n",
      "[Training Epoch 7] Batch 836, Loss 0.26162096858024597\n",
      "[Training Epoch 7] Batch 837, Loss 0.2387448400259018\n",
      "[Training Epoch 7] Batch 838, Loss 0.25888872146606445\n",
      "[Training Epoch 7] Batch 839, Loss 0.28799667954444885\n",
      "[Training Epoch 7] Batch 840, Loss 0.26230281591415405\n",
      "[Training Epoch 7] Batch 841, Loss 0.2738930583000183\n",
      "[Training Epoch 7] Batch 842, Loss 0.26227396726608276\n",
      "[Training Epoch 7] Batch 843, Loss 0.26628291606903076\n",
      "[Training Epoch 7] Batch 844, Loss 0.2600632607936859\n",
      "[Training Epoch 7] Batch 845, Loss 0.24795517325401306\n",
      "[Training Epoch 7] Batch 846, Loss 0.2760331928730011\n",
      "[Training Epoch 7] Batch 847, Loss 0.2539740204811096\n",
      "[Training Epoch 7] Batch 848, Loss 0.2393675148487091\n",
      "[Training Epoch 7] Batch 849, Loss 0.24869117140769958\n",
      "[Training Epoch 7] Batch 850, Loss 0.22189727425575256\n",
      "[Training Epoch 7] Batch 851, Loss 0.2540799677371979\n",
      "[Training Epoch 7] Batch 852, Loss 0.2653166353702545\n",
      "[Training Epoch 7] Batch 853, Loss 0.258877158164978\n",
      "[Training Epoch 7] Batch 854, Loss 0.24980521202087402\n",
      "[Training Epoch 7] Batch 855, Loss 0.28238117694854736\n",
      "[Training Epoch 7] Batch 856, Loss 0.26989075541496277\n",
      "[Training Epoch 7] Batch 857, Loss 0.2571398615837097\n",
      "[Training Epoch 7] Batch 858, Loss 0.23605936765670776\n",
      "[Training Epoch 7] Batch 859, Loss 0.21776427328586578\n",
      "[Training Epoch 7] Batch 860, Loss 0.24990949034690857\n",
      "[Training Epoch 7] Batch 861, Loss 0.2615453004837036\n",
      "[Training Epoch 7] Batch 862, Loss 0.24845342338085175\n",
      "[Training Epoch 7] Batch 863, Loss 0.2706224024295807\n",
      "[Training Epoch 7] Batch 864, Loss 0.28215137124061584\n",
      "[Training Epoch 7] Batch 865, Loss 0.24957275390625\n",
      "[Training Epoch 7] Batch 866, Loss 0.27670514583587646\n",
      "[Training Epoch 7] Batch 867, Loss 0.24685275554656982\n",
      "[Training Epoch 7] Batch 868, Loss 0.24766041338443756\n",
      "[Training Epoch 7] Batch 869, Loss 0.2560079097747803\n",
      "[Training Epoch 7] Batch 870, Loss 0.24757198989391327\n",
      "[Training Epoch 7] Batch 871, Loss 0.21430890262126923\n",
      "[Training Epoch 7] Batch 872, Loss 0.26099106669425964\n",
      "[Training Epoch 7] Batch 873, Loss 0.24840103089809418\n",
      "[Training Epoch 7] Batch 874, Loss 0.23268193006515503\n",
      "[Training Epoch 7] Batch 875, Loss 0.2647271752357483\n",
      "[Training Epoch 7] Batch 876, Loss 0.2555697560310364\n",
      "[Training Epoch 7] Batch 877, Loss 0.24448400735855103\n",
      "[Training Epoch 7] Batch 878, Loss 0.2806047797203064\n",
      "[Training Epoch 7] Batch 879, Loss 0.2673838436603546\n",
      "[Training Epoch 7] Batch 880, Loss 0.2495982050895691\n",
      "[Training Epoch 7] Batch 881, Loss 0.256342351436615\n",
      "[Training Epoch 7] Batch 882, Loss 0.2588599920272827\n",
      "[Training Epoch 7] Batch 883, Loss 0.2556120753288269\n",
      "[Training Epoch 7] Batch 884, Loss 0.25641271471977234\n",
      "[Training Epoch 7] Batch 885, Loss 0.2505377233028412\n",
      "[Training Epoch 7] Batch 886, Loss 0.2367396205663681\n",
      "[Training Epoch 7] Batch 887, Loss 0.24807465076446533\n",
      "[Training Epoch 7] Batch 888, Loss 0.26535549759864807\n",
      "[Training Epoch 7] Batch 889, Loss 0.24129700660705566\n",
      "[Training Epoch 7] Batch 890, Loss 0.27747824788093567\n",
      "[Training Epoch 7] Batch 891, Loss 0.23776909708976746\n",
      "[Training Epoch 7] Batch 892, Loss 0.26536351442337036\n",
      "[Training Epoch 7] Batch 893, Loss 0.24395844340324402\n",
      "[Training Epoch 7] Batch 894, Loss 0.26613473892211914\n",
      "[Training Epoch 7] Batch 895, Loss 0.2518313527107239\n",
      "[Training Epoch 7] Batch 896, Loss 0.2677652835845947\n",
      "[Training Epoch 7] Batch 897, Loss 0.2623409032821655\n",
      "[Training Epoch 7] Batch 898, Loss 0.2528376877307892\n",
      "[Training Epoch 7] Batch 899, Loss 0.24889123439788818\n",
      "[Training Epoch 7] Batch 900, Loss 0.25560611486434937\n",
      "[Training Epoch 7] Batch 901, Loss 0.2383798360824585\n",
      "[Training Epoch 7] Batch 902, Loss 0.25885510444641113\n",
      "[Training Epoch 7] Batch 903, Loss 0.26465022563934326\n",
      "[Training Epoch 7] Batch 904, Loss 0.23392142355442047\n",
      "[Training Epoch 7] Batch 905, Loss 0.2814801335334778\n",
      "[Training Epoch 7] Batch 906, Loss 0.2499123513698578\n",
      "[Training Epoch 7] Batch 907, Loss 0.2679464817047119\n",
      "[Training Epoch 7] Batch 908, Loss 0.28612658381462097\n",
      "[Training Epoch 7] Batch 909, Loss 0.2623406946659088\n",
      "[Training Epoch 7] Batch 910, Loss 0.27056628465652466\n",
      "[Training Epoch 7] Batch 911, Loss 0.26467442512512207\n",
      "[Training Epoch 7] Batch 912, Loss 0.25708210468292236\n",
      "[Training Epoch 7] Batch 913, Loss 0.2544540762901306\n",
      "[Training Epoch 7] Batch 914, Loss 0.2741850018501282\n",
      "[Training Epoch 7] Batch 915, Loss 0.26481765508651733\n",
      "[Training Epoch 7] Batch 916, Loss 0.23486991226673126\n",
      "[Training Epoch 7] Batch 917, Loss 0.2766382396221161\n",
      "[Training Epoch 7] Batch 918, Loss 0.2554721534252167\n",
      "[Training Epoch 7] Batch 919, Loss 0.2526041865348816\n",
      "[Training Epoch 7] Batch 920, Loss 0.25864100456237793\n",
      "[Training Epoch 7] Batch 921, Loss 0.24342790246009827\n",
      "[Training Epoch 7] Batch 922, Loss 0.2839764654636383\n",
      "[Training Epoch 7] Batch 923, Loss 0.25666600465774536\n",
      "[Training Epoch 7] Batch 924, Loss 0.2509503960609436\n",
      "[Training Epoch 7] Batch 925, Loss 0.22874626517295837\n",
      "[Training Epoch 7] Batch 926, Loss 0.24636125564575195\n",
      "[Training Epoch 7] Batch 927, Loss 0.250529408454895\n",
      "[Training Epoch 7] Batch 928, Loss 0.26576313376426697\n",
      "[Training Epoch 7] Batch 929, Loss 0.25176405906677246\n",
      "[Training Epoch 7] Batch 930, Loss 0.2633756697177887\n",
      "[Training Epoch 7] Batch 931, Loss 0.26364850997924805\n",
      "[Training Epoch 7] Batch 932, Loss 0.2527835965156555\n",
      "[Training Epoch 7] Batch 933, Loss 0.28001904487609863\n",
      "[Training Epoch 7] Batch 934, Loss 0.2705516815185547\n",
      "[Training Epoch 7] Batch 935, Loss 0.2526460289955139\n",
      "[Training Epoch 7] Batch 936, Loss 0.2593950033187866\n",
      "[Training Epoch 7] Batch 937, Loss 0.2797819972038269\n",
      "[Training Epoch 7] Batch 938, Loss 0.22866395115852356\n",
      "[Training Epoch 7] Batch 939, Loss 0.2236013114452362\n",
      "[Training Epoch 7] Batch 940, Loss 0.26370614767074585\n",
      "[Training Epoch 7] Batch 941, Loss 0.2741186320781708\n",
      "[Training Epoch 7] Batch 942, Loss 0.25280165672302246\n",
      "[Training Epoch 7] Batch 943, Loss 0.25708669424057007\n",
      "[Training Epoch 7] Batch 944, Loss 0.25331610441207886\n",
      "[Training Epoch 7] Batch 945, Loss 0.23142686486244202\n",
      "[Training Epoch 7] Batch 946, Loss 0.2641458213329315\n",
      "[Training Epoch 7] Batch 947, Loss 0.2761397361755371\n",
      "[Training Epoch 7] Batch 948, Loss 0.25624245405197144\n",
      "[Training Epoch 7] Batch 949, Loss 0.25981175899505615\n",
      "[Training Epoch 7] Batch 950, Loss 0.2696184515953064\n",
      "[Training Epoch 7] Batch 951, Loss 0.2451152354478836\n",
      "[Training Epoch 7] Batch 952, Loss 0.2677430212497711\n",
      "[Training Epoch 7] Batch 953, Loss 0.2628670334815979\n",
      "[Training Epoch 7] Batch 954, Loss 0.23456135392189026\n",
      "[Training Epoch 7] Batch 955, Loss 0.2574779987335205\n",
      "[Training Epoch 7] Batch 956, Loss 0.2532152533531189\n",
      "[Training Epoch 7] Batch 957, Loss 0.2603986859321594\n",
      "[Training Epoch 7] Batch 958, Loss 0.27784448862075806\n",
      "[Training Epoch 7] Batch 959, Loss 0.26711031794548035\n",
      "[Training Epoch 7] Batch 960, Loss 0.2533271610736847\n",
      "[Training Epoch 7] Batch 961, Loss 0.3079589307308197\n",
      "[Training Epoch 7] Batch 962, Loss 0.23689419031143188\n",
      "[Training Epoch 7] Batch 963, Loss 0.23868757486343384\n",
      "[Training Epoch 7] Batch 964, Loss 0.262319415807724\n",
      "[Training Epoch 7] Batch 965, Loss 0.24523097276687622\n",
      "[Training Epoch 7] Batch 966, Loss 0.28224843740463257\n",
      "[Training Epoch 7] Batch 967, Loss 0.2672993540763855\n",
      "[Training Epoch 7] Batch 968, Loss 0.2552453279495239\n",
      "[Training Epoch 7] Batch 969, Loss 0.24715101718902588\n",
      "[Training Epoch 7] Batch 970, Loss 0.255423903465271\n",
      "[Training Epoch 7] Batch 971, Loss 0.26223140954971313\n",
      "[Training Epoch 7] Batch 972, Loss 0.2482161819934845\n",
      "[Training Epoch 7] Batch 973, Loss 0.26183560490608215\n",
      "[Training Epoch 7] Batch 974, Loss 0.257381796836853\n",
      "[Training Epoch 7] Batch 975, Loss 0.27469760179519653\n",
      "[Training Epoch 7] Batch 976, Loss 0.2763894200325012\n",
      "[Training Epoch 7] Batch 977, Loss 0.2478688657283783\n",
      "[Training Epoch 7] Batch 978, Loss 0.23006044328212738\n",
      "[Training Epoch 7] Batch 979, Loss 0.23761886358261108\n",
      "[Training Epoch 7] Batch 980, Loss 0.2554088234901428\n",
      "[Training Epoch 7] Batch 981, Loss 0.2773052155971527\n",
      "[Training Epoch 7] Batch 982, Loss 0.25857552886009216\n",
      "[Training Epoch 7] Batch 983, Loss 0.2723895013332367\n",
      "[Training Epoch 7] Batch 984, Loss 0.24136702716350555\n",
      "[Training Epoch 7] Batch 985, Loss 0.25657081604003906\n",
      "[Training Epoch 7] Batch 986, Loss 0.245500385761261\n",
      "[Training Epoch 7] Batch 987, Loss 0.2715865671634674\n",
      "[Training Epoch 7] Batch 988, Loss 0.2383742481470108\n",
      "[Training Epoch 7] Batch 989, Loss 0.24177831411361694\n",
      "[Training Epoch 7] Batch 990, Loss 0.2422177493572235\n",
      "[Training Epoch 7] Batch 991, Loss 0.28953462839126587\n",
      "[Training Epoch 7] Batch 992, Loss 0.2733709216117859\n",
      "[Training Epoch 7] Batch 993, Loss 0.25381234288215637\n",
      "[Training Epoch 7] Batch 994, Loss 0.23126952350139618\n",
      "[Training Epoch 7] Batch 995, Loss 0.2623201012611389\n",
      "[Training Epoch 7] Batch 996, Loss 0.2209353744983673\n",
      "[Training Epoch 7] Batch 997, Loss 0.26898205280303955\n",
      "[Training Epoch 7] Batch 998, Loss 0.25637921690940857\n",
      "[Training Epoch 7] Batch 999, Loss 0.262366384267807\n",
      "[Training Epoch 7] Batch 1000, Loss 0.2388271987438202\n",
      "[Training Epoch 7] Batch 1001, Loss 0.22375407814979553\n",
      "[Training Epoch 7] Batch 1002, Loss 0.2819787859916687\n",
      "[Training Epoch 7] Batch 1003, Loss 0.24554789066314697\n",
      "[Training Epoch 7] Batch 1004, Loss 0.3033020496368408\n",
      "[Training Epoch 7] Batch 1005, Loss 0.24376368522644043\n",
      "[Training Epoch 7] Batch 1006, Loss 0.2808367609977722\n",
      "[Training Epoch 7] Batch 1007, Loss 0.23479190468788147\n",
      "[Training Epoch 7] Batch 1008, Loss 0.25566497445106506\n",
      "[Training Epoch 7] Batch 1009, Loss 0.26201122999191284\n",
      "[Training Epoch 7] Batch 1010, Loss 0.2557603716850281\n",
      "[Training Epoch 7] Batch 1011, Loss 0.25229412317276\n",
      "[Training Epoch 7] Batch 1012, Loss 0.24270111322402954\n",
      "[Training Epoch 7] Batch 1013, Loss 0.2866917848587036\n",
      "[Training Epoch 7] Batch 1014, Loss 0.2701590061187744\n",
      "[Training Epoch 7] Batch 1015, Loss 0.22398993372917175\n",
      "[Training Epoch 7] Batch 1016, Loss 0.257142573595047\n",
      "[Training Epoch 7] Batch 1017, Loss 0.27229899168014526\n",
      "[Training Epoch 7] Batch 1018, Loss 0.26803529262542725\n",
      "[Training Epoch 7] Batch 1019, Loss 0.28437596559524536\n",
      "[Training Epoch 7] Batch 1020, Loss 0.24307295680046082\n",
      "[Training Epoch 7] Batch 1021, Loss 0.24144017696380615\n",
      "[Training Epoch 7] Batch 1022, Loss 0.25567954778671265\n",
      "[Training Epoch 7] Batch 1023, Loss 0.2527647912502289\n",
      "[Training Epoch 7] Batch 1024, Loss 0.22825388610363007\n",
      "[Training Epoch 7] Batch 1025, Loss 0.25684717297554016\n",
      "[Training Epoch 7] Batch 1026, Loss 0.23948387801647186\n",
      "[Training Epoch 7] Batch 1027, Loss 0.23188439011573792\n",
      "[Training Epoch 7] Batch 1028, Loss 0.2543784976005554\n",
      "[Training Epoch 7] Batch 1029, Loss 0.25378015637397766\n",
      "[Training Epoch 7] Batch 1030, Loss 0.2544073462486267\n",
      "[Training Epoch 7] Batch 1031, Loss 0.25645285844802856\n",
      "[Training Epoch 7] Batch 1032, Loss 0.276566743850708\n",
      "[Training Epoch 7] Batch 1033, Loss 0.24945449829101562\n",
      "[Training Epoch 7] Batch 1034, Loss 0.244148850440979\n",
      "[Training Epoch 7] Batch 1035, Loss 0.2488546073436737\n",
      "[Training Epoch 7] Batch 1036, Loss 0.2975696921348572\n",
      "[Training Epoch 7] Batch 1037, Loss 0.22963327169418335\n",
      "[Training Epoch 7] Batch 1038, Loss 0.25715699791908264\n",
      "[Training Epoch 7] Batch 1039, Loss 0.24960502982139587\n",
      "[Training Epoch 7] Batch 1040, Loss 0.27115389704704285\n",
      "[Training Epoch 7] Batch 1041, Loss 0.2515009939670563\n",
      "[Training Epoch 7] Batch 1042, Loss 0.27262043952941895\n",
      "[Training Epoch 7] Batch 1043, Loss 0.245981827378273\n",
      "[Training Epoch 7] Batch 1044, Loss 0.2374635636806488\n",
      "[Training Epoch 7] Batch 1045, Loss 0.2631838917732239\n",
      "[Training Epoch 7] Batch 1046, Loss 0.2547316253185272\n",
      "[Training Epoch 7] Batch 1047, Loss 0.2503350079059601\n",
      "[Training Epoch 7] Batch 1048, Loss 0.24939554929733276\n",
      "[Training Epoch 7] Batch 1049, Loss 0.2412213236093521\n",
      "[Training Epoch 7] Batch 1050, Loss 0.2630399763584137\n",
      "[Training Epoch 7] Batch 1051, Loss 0.24118070304393768\n",
      "[Training Epoch 7] Batch 1052, Loss 0.22518104314804077\n",
      "[Training Epoch 7] Batch 1053, Loss 0.2530072331428528\n",
      "[Training Epoch 7] Batch 1054, Loss 0.24183550477027893\n",
      "[Training Epoch 7] Batch 1055, Loss 0.24084030091762543\n",
      "[Training Epoch 7] Batch 1056, Loss 0.2803093194961548\n",
      "[Training Epoch 7] Batch 1057, Loss 0.2233695536851883\n",
      "[Training Epoch 7] Batch 1058, Loss 0.2740405797958374\n",
      "[Training Epoch 7] Batch 1059, Loss 0.2365155816078186\n",
      "[Training Epoch 7] Batch 1060, Loss 0.26079457998275757\n",
      "[Training Epoch 7] Batch 1061, Loss 0.28694838285446167\n",
      "[Training Epoch 7] Batch 1062, Loss 0.24582140147686005\n",
      "[Training Epoch 7] Batch 1063, Loss 0.2797297537326813\n",
      "[Training Epoch 7] Batch 1064, Loss 0.24756744503974915\n",
      "[Training Epoch 7] Batch 1065, Loss 0.25706008076667786\n",
      "[Training Epoch 7] Batch 1066, Loss 0.24871662259101868\n",
      "[Training Epoch 7] Batch 1067, Loss 0.2498610019683838\n",
      "[Training Epoch 7] Batch 1068, Loss 0.2549760043621063\n",
      "[Training Epoch 7] Batch 1069, Loss 0.2493278980255127\n",
      "[Training Epoch 7] Batch 1070, Loss 0.2605581283569336\n",
      "[Training Epoch 7] Batch 1071, Loss 0.24684825539588928\n",
      "[Training Epoch 7] Batch 1072, Loss 0.2637730836868286\n",
      "[Training Epoch 7] Batch 1073, Loss 0.23319068551063538\n",
      "[Training Epoch 7] Batch 1074, Loss 0.26541250944137573\n",
      "[Training Epoch 7] Batch 1075, Loss 0.2648664116859436\n",
      "[Training Epoch 7] Batch 1076, Loss 0.2630043029785156\n",
      "[Training Epoch 7] Batch 1077, Loss 0.22925323247909546\n",
      "[Training Epoch 7] Batch 1078, Loss 0.23916597664356232\n",
      "[Training Epoch 7] Batch 1079, Loss 0.25259822607040405\n",
      "[Training Epoch 7] Batch 1080, Loss 0.245131254196167\n",
      "[Training Epoch 7] Batch 1081, Loss 0.24581065773963928\n",
      "[Training Epoch 7] Batch 1082, Loss 0.24037612974643707\n",
      "[Training Epoch 7] Batch 1083, Loss 0.25749874114990234\n",
      "[Training Epoch 7] Batch 1084, Loss 0.22452490031719208\n",
      "[Training Epoch 7] Batch 1085, Loss 0.26264235377311707\n",
      "[Training Epoch 7] Batch 1086, Loss 0.29666584730148315\n",
      "[Training Epoch 7] Batch 1087, Loss 0.2586721181869507\n",
      "[Training Epoch 7] Batch 1088, Loss 0.2575904428958893\n",
      "[Training Epoch 7] Batch 1089, Loss 0.25100642442703247\n",
      "[Training Epoch 7] Batch 1090, Loss 0.2721345126628876\n",
      "[Training Epoch 7] Batch 1091, Loss 0.2537907660007477\n",
      "[Training Epoch 7] Batch 1092, Loss 0.2526364326477051\n",
      "[Training Epoch 7] Batch 1093, Loss 0.24182800948619843\n",
      "[Training Epoch 7] Batch 1094, Loss 0.26225054264068604\n",
      "[Training Epoch 7] Batch 1095, Loss 0.26399901509284973\n",
      "[Training Epoch 7] Batch 1096, Loss 0.23859618604183197\n",
      "[Training Epoch 7] Batch 1097, Loss 0.27719083428382874\n",
      "[Training Epoch 7] Batch 1098, Loss 0.25496193766593933\n",
      "[Training Epoch 7] Batch 1099, Loss 0.24706289172172546\n",
      "[Training Epoch 7] Batch 1100, Loss 0.2779609262943268\n",
      "[Training Epoch 7] Batch 1101, Loss 0.23874717950820923\n",
      "[Training Epoch 7] Batch 1102, Loss 0.2562742829322815\n",
      "[Training Epoch 7] Batch 1103, Loss 0.2548533082008362\n",
      "[Training Epoch 7] Batch 1104, Loss 0.23617421090602875\n",
      "[Training Epoch 7] Batch 1105, Loss 0.25707948207855225\n",
      "[Training Epoch 7] Batch 1106, Loss 0.2719383239746094\n",
      "[Training Epoch 7] Batch 1107, Loss 0.25282546877861023\n",
      "[Training Epoch 7] Batch 1108, Loss 0.2310311496257782\n",
      "[Training Epoch 7] Batch 1109, Loss 0.2459985315799713\n",
      "[Training Epoch 7] Batch 1110, Loss 0.2530306577682495\n",
      "[Training Epoch 7] Batch 1111, Loss 0.2672578692436218\n",
      "[Training Epoch 7] Batch 1112, Loss 0.253675252199173\n",
      "[Training Epoch 7] Batch 1113, Loss 0.2270985245704651\n",
      "[Training Epoch 7] Batch 1114, Loss 0.2590201497077942\n",
      "[Training Epoch 7] Batch 1115, Loss 0.25791364908218384\n",
      "[Training Epoch 7] Batch 1116, Loss 0.2637966275215149\n",
      "[Training Epoch 7] Batch 1117, Loss 0.260312020778656\n",
      "[Training Epoch 7] Batch 1118, Loss 0.24888989329338074\n",
      "[Training Epoch 7] Batch 1119, Loss 0.2832159698009491\n",
      "[Training Epoch 7] Batch 1120, Loss 0.2730937600135803\n",
      "[Training Epoch 7] Batch 1121, Loss 0.2643175423145294\n",
      "[Training Epoch 7] Batch 1122, Loss 0.2615815997123718\n",
      "[Training Epoch 7] Batch 1123, Loss 0.24150270223617554\n",
      "[Training Epoch 7] Batch 1124, Loss 0.2652932405471802\n",
      "[Training Epoch 7] Batch 1125, Loss 0.25665777921676636\n",
      "[Training Epoch 7] Batch 1126, Loss 0.2690470218658447\n",
      "[Training Epoch 7] Batch 1127, Loss 0.22872984409332275\n",
      "[Training Epoch 7] Batch 1128, Loss 0.21950063109397888\n",
      "[Training Epoch 7] Batch 1129, Loss 0.26308828592300415\n",
      "[Training Epoch 7] Batch 1130, Loss 0.2567228078842163\n",
      "[Training Epoch 7] Batch 1131, Loss 0.295698881149292\n",
      "[Training Epoch 7] Batch 1132, Loss 0.24864697456359863\n",
      "[Training Epoch 7] Batch 1133, Loss 0.2685167193412781\n",
      "[Training Epoch 7] Batch 1134, Loss 0.27597925066947937\n",
      "[Training Epoch 7] Batch 1135, Loss 0.23976367712020874\n",
      "[Training Epoch 7] Batch 1136, Loss 0.2516932785511017\n",
      "[Training Epoch 7] Batch 1137, Loss 0.23641642928123474\n",
      "[Training Epoch 7] Batch 1138, Loss 0.2699499726295471\n",
      "[Training Epoch 7] Batch 1139, Loss 0.2889572083950043\n",
      "[Training Epoch 7] Batch 1140, Loss 0.28038716316223145\n",
      "[Training Epoch 7] Batch 1141, Loss 0.24856218695640564\n",
      "[Training Epoch 7] Batch 1142, Loss 0.23916839063167572\n",
      "[Training Epoch 7] Batch 1143, Loss 0.2580702304840088\n",
      "[Training Epoch 7] Batch 1144, Loss 0.2631291449069977\n",
      "[Training Epoch 7] Batch 1145, Loss 0.28377074003219604\n",
      "[Training Epoch 7] Batch 1146, Loss 0.2527260482311249\n",
      "[Training Epoch 7] Batch 1147, Loss 0.24365834891796112\n",
      "[Training Epoch 7] Batch 1148, Loss 0.23283490538597107\n",
      "[Training Epoch 7] Batch 1149, Loss 0.2613016963005066\n",
      "[Training Epoch 7] Batch 1150, Loss 0.21637630462646484\n",
      "[Training Epoch 7] Batch 1151, Loss 0.22425055503845215\n",
      "[Training Epoch 7] Batch 1152, Loss 0.26790183782577515\n",
      "[Training Epoch 7] Batch 1153, Loss 0.2526642680168152\n",
      "[Training Epoch 7] Batch 1154, Loss 0.24685698747634888\n",
      "[Training Epoch 7] Batch 1155, Loss 0.24894283711910248\n",
      "[Training Epoch 7] Batch 1156, Loss 0.28416427969932556\n",
      "[Training Epoch 7] Batch 1157, Loss 0.25429511070251465\n",
      "[Training Epoch 7] Batch 1158, Loss 0.25187399983406067\n",
      "[Training Epoch 7] Batch 1159, Loss 0.26866787672042847\n",
      "[Training Epoch 7] Batch 1160, Loss 0.2647347152233124\n",
      "[Training Epoch 7] Batch 1161, Loss 0.24927878379821777\n",
      "[Training Epoch 7] Batch 1162, Loss 0.2614409625530243\n",
      "[Training Epoch 7] Batch 1163, Loss 0.26746666431427\n",
      "[Training Epoch 7] Batch 1164, Loss 0.25714290142059326\n",
      "[Training Epoch 7] Batch 1165, Loss 0.22195884585380554\n",
      "[Training Epoch 7] Batch 1166, Loss 0.2680055797100067\n",
      "[Training Epoch 7] Batch 1167, Loss 0.27889901399612427\n",
      "[Training Epoch 7] Batch 1168, Loss 0.2576494812965393\n",
      "[Training Epoch 7] Batch 1169, Loss 0.28908759355545044\n",
      "[Training Epoch 7] Batch 1170, Loss 0.24038486182689667\n",
      "[Training Epoch 7] Batch 1171, Loss 0.2652904987335205\n",
      "[Training Epoch 7] Batch 1172, Loss 0.26178398728370667\n",
      "[Training Epoch 7] Batch 1173, Loss 0.2742576003074646\n",
      "[Training Epoch 7] Batch 1174, Loss 0.25600606203079224\n",
      "[Training Epoch 7] Batch 1175, Loss 0.24030518531799316\n",
      "[Training Epoch 7] Batch 1176, Loss 0.21398277580738068\n",
      "[Training Epoch 7] Batch 1177, Loss 0.24050390720367432\n",
      "[Training Epoch 7] Batch 1178, Loss 0.2940666675567627\n",
      "[Training Epoch 7] Batch 1179, Loss 0.2538582682609558\n",
      "[Training Epoch 7] Batch 1180, Loss 0.23864516615867615\n",
      "[Training Epoch 7] Batch 1181, Loss 0.2531844973564148\n",
      "[Training Epoch 7] Batch 1182, Loss 0.2823371887207031\n",
      "[Training Epoch 7] Batch 1183, Loss 0.2427443265914917\n",
      "[Training Epoch 7] Batch 1184, Loss 0.27945268154144287\n",
      "[Training Epoch 7] Batch 1185, Loss 0.2622767686843872\n",
      "[Training Epoch 7] Batch 1186, Loss 0.23950360715389252\n",
      "[Training Epoch 7] Batch 1187, Loss 0.24927833676338196\n",
      "[Training Epoch 7] Batch 1188, Loss 0.25684425234794617\n",
      "[Training Epoch 7] Batch 1189, Loss 0.26743388175964355\n",
      "[Training Epoch 7] Batch 1190, Loss 0.25526612997055054\n",
      "[Training Epoch 7] Batch 1191, Loss 0.286749005317688\n",
      "[Training Epoch 7] Batch 1192, Loss 0.26405149698257446\n",
      "[Training Epoch 7] Batch 1193, Loss 0.24832198023796082\n",
      "[Training Epoch 7] Batch 1194, Loss 0.22248926758766174\n",
      "[Training Epoch 7] Batch 1195, Loss 0.25964924693107605\n",
      "[Training Epoch 7] Batch 1196, Loss 0.2552061975002289\n",
      "[Training Epoch 7] Batch 1197, Loss 0.2312767058610916\n",
      "[Training Epoch 7] Batch 1198, Loss 0.23368847370147705\n",
      "[Training Epoch 7] Batch 1199, Loss 0.2627929449081421\n",
      "[Training Epoch 7] Batch 1200, Loss 0.24013292789459229\n",
      "[Training Epoch 7] Batch 1201, Loss 0.26811230182647705\n",
      "[Training Epoch 7] Batch 1202, Loss 0.27889353036880493\n",
      "[Training Epoch 7] Batch 1203, Loss 0.2600351572036743\n",
      "[Training Epoch 7] Batch 1204, Loss 0.27557995915412903\n",
      "[Training Epoch 7] Batch 1205, Loss 0.2668333351612091\n",
      "[Training Epoch 7] Batch 1206, Loss 0.2412227839231491\n",
      "[Training Epoch 7] Batch 1207, Loss 0.24299852550029755\n",
      "[Training Epoch 7] Batch 1208, Loss 0.2719860374927521\n",
      "[Training Epoch 7] Batch 1209, Loss 0.27022260427474976\n",
      "[Training Epoch 7] Batch 1210, Loss 0.25103360414505005\n",
      "[Training Epoch 7] Batch 1211, Loss 0.2510499358177185\n",
      "[Training Epoch 7] Batch 1212, Loss 0.24611203372478485\n",
      "[Training Epoch 7] Batch 1213, Loss 0.27018553018569946\n",
      "[Training Epoch 7] Batch 1214, Loss 0.260012686252594\n",
      "[Training Epoch 7] Batch 1215, Loss 0.2618662416934967\n",
      "[Training Epoch 7] Batch 1216, Loss 0.272044837474823\n",
      "[Training Epoch 7] Batch 1217, Loss 0.2638756036758423\n",
      "[Training Epoch 7] Batch 1218, Loss 0.2288805991411209\n",
      "[Training Epoch 7] Batch 1219, Loss 0.22742590308189392\n",
      "[Training Epoch 7] Batch 1220, Loss 0.25729846954345703\n",
      "[Training Epoch 7] Batch 1221, Loss 0.24436213076114655\n",
      "[Training Epoch 7] Batch 1222, Loss 0.26149600744247437\n",
      "[Training Epoch 7] Batch 1223, Loss 0.24946700036525726\n",
      "[Training Epoch 7] Batch 1224, Loss 0.2342691570520401\n",
      "[Training Epoch 7] Batch 1225, Loss 0.24307486414909363\n",
      "[Training Epoch 7] Batch 1226, Loss 0.2814720571041107\n",
      "[Training Epoch 7] Batch 1227, Loss 0.2408396154642105\n",
      "[Training Epoch 7] Batch 1228, Loss 0.266552597284317\n",
      "[Training Epoch 7] Batch 1229, Loss 0.28282997012138367\n",
      "[Training Epoch 7] Batch 1230, Loss 0.24184218049049377\n",
      "[Training Epoch 7] Batch 1231, Loss 0.24508905410766602\n",
      "[Training Epoch 7] Batch 1232, Loss 0.25817999243736267\n",
      "[Training Epoch 7] Batch 1233, Loss 0.2319129854440689\n",
      "[Training Epoch 7] Batch 1234, Loss 0.26212725043296814\n",
      "[Training Epoch 7] Batch 1235, Loss 0.22658658027648926\n",
      "[Training Epoch 7] Batch 1236, Loss 0.27445554733276367\n",
      "[Training Epoch 7] Batch 1237, Loss 0.28736844658851624\n",
      "[Training Epoch 7] Batch 1238, Loss 0.26633238792419434\n",
      "[Training Epoch 7] Batch 1239, Loss 0.2697826027870178\n",
      "[Training Epoch 7] Batch 1240, Loss 0.2389337122440338\n",
      "[Training Epoch 7] Batch 1241, Loss 0.23421403765678406\n",
      "[Training Epoch 7] Batch 1242, Loss 0.23917895555496216\n",
      "[Training Epoch 7] Batch 1243, Loss 0.23958000540733337\n",
      "[Training Epoch 7] Batch 1244, Loss 0.2766375243663788\n",
      "[Training Epoch 7] Batch 1245, Loss 0.24264290928840637\n",
      "[Training Epoch 7] Batch 1246, Loss 0.24275478720664978\n",
      "[Training Epoch 7] Batch 1247, Loss 0.26056069135665894\n",
      "[Training Epoch 7] Batch 1248, Loss 0.26148325204849243\n",
      "[Training Epoch 7] Batch 1249, Loss 0.29234766960144043\n",
      "[Training Epoch 7] Batch 1250, Loss 0.2686351537704468\n",
      "[Training Epoch 7] Batch 1251, Loss 0.2595258355140686\n",
      "[Training Epoch 7] Batch 1252, Loss 0.2648237943649292\n",
      "[Training Epoch 7] Batch 1253, Loss 0.21369343996047974\n",
      "[Training Epoch 7] Batch 1254, Loss 0.24912111461162567\n",
      "[Training Epoch 7] Batch 1255, Loss 0.2594184875488281\n",
      "[Training Epoch 7] Batch 1256, Loss 0.23209571838378906\n",
      "[Training Epoch 7] Batch 1257, Loss 0.24725869297981262\n",
      "[Training Epoch 7] Batch 1258, Loss 0.2356511652469635\n",
      "[Training Epoch 7] Batch 1259, Loss 0.26750776171684265\n",
      "[Training Epoch 7] Batch 1260, Loss 0.26899558305740356\n",
      "[Training Epoch 7] Batch 1261, Loss 0.24830442667007446\n",
      "[Training Epoch 7] Batch 1262, Loss 0.22675743699073792\n",
      "[Training Epoch 7] Batch 1263, Loss 0.250927209854126\n",
      "[Training Epoch 7] Batch 1264, Loss 0.2596934139728546\n",
      "[Training Epoch 7] Batch 1265, Loss 0.26070648431777954\n",
      "[Training Epoch 7] Batch 1266, Loss 0.22818362712860107\n",
      "[Training Epoch 7] Batch 1267, Loss 0.2491219937801361\n",
      "[Training Epoch 7] Batch 1268, Loss 0.2719164788722992\n",
      "[Training Epoch 7] Batch 1269, Loss 0.24918866157531738\n",
      "[Training Epoch 7] Batch 1270, Loss 0.27604013681411743\n",
      "[Training Epoch 7] Batch 1271, Loss 0.2653989791870117\n",
      "[Training Epoch 7] Batch 1272, Loss 0.26039060950279236\n",
      "[Training Epoch 7] Batch 1273, Loss 0.28102022409439087\n",
      "[Training Epoch 7] Batch 1274, Loss 0.2624978721141815\n",
      "[Training Epoch 7] Batch 1275, Loss 0.24534988403320312\n",
      "[Training Epoch 7] Batch 1276, Loss 0.24246403574943542\n",
      "[Training Epoch 7] Batch 1277, Loss 0.2546173334121704\n",
      "[Training Epoch 7] Batch 1278, Loss 0.2513737678527832\n",
      "[Training Epoch 7] Batch 1279, Loss 0.2871667742729187\n",
      "[Training Epoch 7] Batch 1280, Loss 0.25575512647628784\n",
      "[Training Epoch 7] Batch 1281, Loss 0.25495004653930664\n",
      "[Training Epoch 7] Batch 1282, Loss 0.26509588956832886\n",
      "[Training Epoch 7] Batch 1283, Loss 0.2859596312046051\n",
      "[Training Epoch 7] Batch 1284, Loss 0.22797320783138275\n",
      "[Training Epoch 7] Batch 1285, Loss 0.26717090606689453\n",
      "[Training Epoch 7] Batch 1286, Loss 0.2562495470046997\n",
      "[Training Epoch 7] Batch 1287, Loss 0.2589172124862671\n",
      "[Training Epoch 7] Batch 1288, Loss 0.24224767088890076\n",
      "[Training Epoch 7] Batch 1289, Loss 0.25080394744873047\n",
      "[Training Epoch 7] Batch 1290, Loss 0.25088173151016235\n",
      "[Training Epoch 7] Batch 1291, Loss 0.27676963806152344\n",
      "[Training Epoch 7] Batch 1292, Loss 0.28078529238700867\n",
      "[Training Epoch 7] Batch 1293, Loss 0.25727578997612\n",
      "[Training Epoch 7] Batch 1294, Loss 0.2601964771747589\n",
      "[Training Epoch 7] Batch 1295, Loss 0.2373638153076172\n",
      "[Training Epoch 7] Batch 1296, Loss 0.2663145065307617\n",
      "[Training Epoch 7] Batch 1297, Loss 0.2699492871761322\n",
      "[Training Epoch 7] Batch 1298, Loss 0.28472813963890076\n",
      "[Training Epoch 7] Batch 1299, Loss 0.2660338282585144\n",
      "[Training Epoch 7] Batch 1300, Loss 0.23519925773143768\n",
      "[Training Epoch 7] Batch 1301, Loss 0.2692420482635498\n",
      "[Training Epoch 7] Batch 1302, Loss 0.25403791666030884\n",
      "[Training Epoch 7] Batch 1303, Loss 0.2411629855632782\n",
      "[Training Epoch 7] Batch 1304, Loss 0.24827469885349274\n",
      "[Training Epoch 7] Batch 1305, Loss 0.2441003918647766\n",
      "[Training Epoch 7] Batch 1306, Loss 0.2465229332447052\n",
      "[Training Epoch 7] Batch 1307, Loss 0.2524685561656952\n",
      "[Training Epoch 7] Batch 1308, Loss 0.24984142184257507\n",
      "[Training Epoch 7] Batch 1309, Loss 0.2595505118370056\n",
      "[Training Epoch 7] Batch 1310, Loss 0.2586270570755005\n",
      "[Training Epoch 7] Batch 1311, Loss 0.26382899284362793\n",
      "[Training Epoch 7] Batch 1312, Loss 0.2605717182159424\n",
      "[Training Epoch 7] Batch 1313, Loss 0.24538007378578186\n",
      "[Training Epoch 7] Batch 1314, Loss 0.30755746364593506\n",
      "[Training Epoch 7] Batch 1315, Loss 0.27862370014190674\n",
      "[Training Epoch 7] Batch 1316, Loss 0.2571118474006653\n",
      "[Training Epoch 7] Batch 1317, Loss 0.2380780428647995\n",
      "[Training Epoch 7] Batch 1318, Loss 0.25322580337524414\n",
      "[Training Epoch 7] Batch 1319, Loss 0.2650575339794159\n",
      "[Training Epoch 7] Batch 1320, Loss 0.25875839591026306\n",
      "[Training Epoch 7] Batch 1321, Loss 0.2440049648284912\n",
      "[Training Epoch 7] Batch 1322, Loss 0.27366068959236145\n",
      "[Training Epoch 7] Batch 1323, Loss 0.26891225576400757\n",
      "[Training Epoch 7] Batch 1324, Loss 0.2564051151275635\n",
      "[Training Epoch 7] Batch 1325, Loss 0.24750395119190216\n",
      "[Training Epoch 7] Batch 1326, Loss 0.23583854734897614\n",
      "[Training Epoch 7] Batch 1327, Loss 0.24486510455608368\n",
      "[Training Epoch 7] Batch 1328, Loss 0.2602372169494629\n",
      "[Training Epoch 7] Batch 1329, Loss 0.25356122851371765\n",
      "[Training Epoch 7] Batch 1330, Loss 0.2486369013786316\n",
      "[Training Epoch 7] Batch 1331, Loss 0.2531804144382477\n",
      "[Training Epoch 7] Batch 1332, Loss 0.2547883689403534\n",
      "[Training Epoch 7] Batch 1333, Loss 0.2267235964536667\n",
      "[Training Epoch 7] Batch 1334, Loss 0.2701401710510254\n",
      "[Training Epoch 7] Batch 1335, Loss 0.24539697170257568\n",
      "[Training Epoch 7] Batch 1336, Loss 0.2605876624584198\n",
      "[Training Epoch 7] Batch 1337, Loss 0.263714462518692\n",
      "[Training Epoch 7] Batch 1338, Loss 0.262549489736557\n",
      "[Training Epoch 7] Batch 1339, Loss 0.26006725430488586\n",
      "[Training Epoch 7] Batch 1340, Loss 0.26223576068878174\n",
      "[Training Epoch 7] Batch 1341, Loss 0.26651817560195923\n",
      "[Training Epoch 7] Batch 1342, Loss 0.271465927362442\n",
      "[Training Epoch 7] Batch 1343, Loss 0.2755924463272095\n",
      "[Training Epoch 7] Batch 1344, Loss 0.25137341022491455\n",
      "[Training Epoch 7] Batch 1345, Loss 0.2357492446899414\n",
      "[Training Epoch 7] Batch 1346, Loss 0.24672040343284607\n",
      "[Training Epoch 7] Batch 1347, Loss 0.2702147960662842\n",
      "[Training Epoch 7] Batch 1348, Loss 0.27260786294937134\n",
      "[Training Epoch 7] Batch 1349, Loss 0.23298192024230957\n",
      "[Training Epoch 7] Batch 1350, Loss 0.2408997118473053\n",
      "[Training Epoch 7] Batch 1351, Loss 0.25303441286087036\n",
      "[Training Epoch 7] Batch 1352, Loss 0.24407055974006653\n",
      "[Training Epoch 7] Batch 1353, Loss 0.2700466513633728\n",
      "[Training Epoch 7] Batch 1354, Loss 0.24745595455169678\n",
      "[Training Epoch 7] Batch 1355, Loss 0.25790098309516907\n",
      "[Training Epoch 7] Batch 1356, Loss 0.2831297516822815\n",
      "[Training Epoch 7] Batch 1357, Loss 0.25734856724739075\n",
      "[Training Epoch 7] Batch 1358, Loss 0.24365252256393433\n",
      "[Training Epoch 7] Batch 1359, Loss 0.2429550439119339\n",
      "[Training Epoch 7] Batch 1360, Loss 0.2866940498352051\n",
      "[Training Epoch 7] Batch 1361, Loss 0.27726203203201294\n",
      "[Training Epoch 7] Batch 1362, Loss 0.2694413661956787\n",
      "[Training Epoch 7] Batch 1363, Loss 0.26877638697624207\n",
      "[Training Epoch 7] Batch 1364, Loss 0.2503514885902405\n",
      "[Training Epoch 7] Batch 1365, Loss 0.2637529671192169\n",
      "[Training Epoch 7] Batch 1366, Loss 0.2629290223121643\n",
      "[Training Epoch 7] Batch 1367, Loss 0.2823264002799988\n",
      "[Training Epoch 7] Batch 1368, Loss 0.2530984878540039\n",
      "[Training Epoch 7] Batch 1369, Loss 0.27342212200164795\n",
      "[Training Epoch 7] Batch 1370, Loss 0.24514487385749817\n",
      "[Training Epoch 7] Batch 1371, Loss 0.27194660902023315\n",
      "[Training Epoch 7] Batch 1372, Loss 0.24011415243148804\n",
      "[Training Epoch 7] Batch 1373, Loss 0.24580013751983643\n",
      "[Training Epoch 7] Batch 1374, Loss 0.2680509686470032\n",
      "[Training Epoch 7] Batch 1375, Loss 0.25525251030921936\n",
      "[Training Epoch 7] Batch 1376, Loss 0.24439552426338196\n",
      "[Training Epoch 7] Batch 1377, Loss 0.27297794818878174\n",
      "[Training Epoch 7] Batch 1378, Loss 0.24255402386188507\n",
      "[Training Epoch 7] Batch 1379, Loss 0.27612632513046265\n",
      "[Training Epoch 7] Batch 1380, Loss 0.22298267483711243\n",
      "[Training Epoch 7] Batch 1381, Loss 0.2559385299682617\n",
      "[Training Epoch 7] Batch 1382, Loss 0.25614893436431885\n",
      "[Training Epoch 7] Batch 1383, Loss 0.2587777376174927\n",
      "[Training Epoch 7] Batch 1384, Loss 0.24079099297523499\n",
      "[Training Epoch 7] Batch 1385, Loss 0.24598076939582825\n",
      "[Training Epoch 7] Batch 1386, Loss 0.2577780485153198\n",
      "[Training Epoch 7] Batch 1387, Loss 0.23654085397720337\n",
      "[Training Epoch 7] Batch 1388, Loss 0.25883370637893677\n",
      "[Training Epoch 7] Batch 1389, Loss 0.23398718237876892\n",
      "[Training Epoch 7] Batch 1390, Loss 0.2661067247390747\n",
      "[Training Epoch 7] Batch 1391, Loss 0.2965560257434845\n",
      "[Training Epoch 7] Batch 1392, Loss 0.26389068365097046\n",
      "[Training Epoch 7] Batch 1393, Loss 0.24912983179092407\n",
      "[Training Epoch 7] Batch 1394, Loss 0.22029633820056915\n",
      "[Training Epoch 7] Batch 1395, Loss 0.23817965388298035\n",
      "[Training Epoch 7] Batch 1396, Loss 0.257733553647995\n",
      "[Training Epoch 7] Batch 1397, Loss 0.30174630880355835\n",
      "[Training Epoch 7] Batch 1398, Loss 0.23660027980804443\n",
      "[Training Epoch 7] Batch 1399, Loss 0.25397631525993347\n",
      "[Training Epoch 7] Batch 1400, Loss 0.25796425342559814\n",
      "[Training Epoch 7] Batch 1401, Loss 0.29578453302383423\n",
      "[Training Epoch 7] Batch 1402, Loss 0.25277000665664673\n",
      "[Training Epoch 7] Batch 1403, Loss 0.2588416039943695\n",
      "[Training Epoch 7] Batch 1404, Loss 0.2308417558670044\n",
      "[Training Epoch 7] Batch 1405, Loss 0.26307064294815063\n",
      "[Training Epoch 7] Batch 1406, Loss 0.258449524641037\n",
      "[Training Epoch 7] Batch 1407, Loss 0.2948870062828064\n",
      "[Training Epoch 7] Batch 1408, Loss 0.24595893919467926\n",
      "[Training Epoch 7] Batch 1409, Loss 0.2613430619239807\n",
      "[Training Epoch 7] Batch 1410, Loss 0.27450910210609436\n",
      "[Training Epoch 7] Batch 1411, Loss 0.27466270327568054\n",
      "[Training Epoch 7] Batch 1412, Loss 0.25573256611824036\n",
      "[Training Epoch 7] Batch 1413, Loss 0.24443921446800232\n",
      "[Training Epoch 7] Batch 1414, Loss 0.22795403003692627\n",
      "[Training Epoch 7] Batch 1415, Loss 0.2497563362121582\n",
      "[Training Epoch 7] Batch 1416, Loss 0.26365017890930176\n",
      "[Training Epoch 7] Batch 1417, Loss 0.2714740037918091\n",
      "[Training Epoch 7] Batch 1418, Loss 0.25860458612442017\n",
      "[Training Epoch 7] Batch 1419, Loss 0.27244558930397034\n",
      "[Training Epoch 7] Batch 1420, Loss 0.23558324575424194\n",
      "[Training Epoch 7] Batch 1421, Loss 0.24745677411556244\n",
      "[Training Epoch 7] Batch 1422, Loss 0.2437286376953125\n",
      "[Training Epoch 7] Batch 1423, Loss 0.2622890770435333\n",
      "[Training Epoch 7] Batch 1424, Loss 0.2693275213241577\n",
      "[Training Epoch 7] Batch 1425, Loss 0.2808024287223816\n",
      "[Training Epoch 7] Batch 1426, Loss 0.24992457032203674\n",
      "[Training Epoch 7] Batch 1427, Loss 0.23074287176132202\n",
      "[Training Epoch 7] Batch 1428, Loss 0.23229366540908813\n",
      "[Training Epoch 7] Batch 1429, Loss 0.23280733823776245\n",
      "[Training Epoch 7] Batch 1430, Loss 0.2870017886161804\n",
      "[Training Epoch 7] Batch 1431, Loss 0.2763654291629791\n",
      "[Training Epoch 7] Batch 1432, Loss 0.26798543334007263\n",
      "[Training Epoch 7] Batch 1433, Loss 0.2732572555541992\n",
      "[Training Epoch 7] Batch 1434, Loss 0.24060532450675964\n",
      "[Training Epoch 7] Batch 1435, Loss 0.21000592410564423\n",
      "[Training Epoch 7] Batch 1436, Loss 0.2529531717300415\n",
      "[Training Epoch 7] Batch 1437, Loss 0.24168014526367188\n",
      "[Training Epoch 7] Batch 1438, Loss 0.2638966739177704\n",
      "[Training Epoch 7] Batch 1439, Loss 0.24193277955055237\n",
      "[Training Epoch 7] Batch 1440, Loss 0.25770270824432373\n",
      "[Training Epoch 7] Batch 1441, Loss 0.2541959285736084\n",
      "[Training Epoch 7] Batch 1442, Loss 0.2604846656322479\n",
      "[Training Epoch 7] Batch 1443, Loss 0.2444339245557785\n",
      "[Training Epoch 7] Batch 1444, Loss 0.2699083685874939\n",
      "[Training Epoch 7] Batch 1445, Loss 0.25020769238471985\n",
      "[Training Epoch 7] Batch 1446, Loss 0.23174519836902618\n",
      "[Training Epoch 7] Batch 1447, Loss 0.24425402283668518\n",
      "[Training Epoch 7] Batch 1448, Loss 0.2696778178215027\n",
      "[Training Epoch 7] Batch 1449, Loss 0.26195424795150757\n",
      "[Training Epoch 7] Batch 1450, Loss 0.25456222891807556\n",
      "[Training Epoch 7] Batch 1451, Loss 0.2461700141429901\n",
      "[Training Epoch 7] Batch 1452, Loss 0.26008379459381104\n",
      "[Training Epoch 7] Batch 1453, Loss 0.2602105736732483\n",
      "[Training Epoch 7] Batch 1454, Loss 0.27036285400390625\n",
      "[Training Epoch 7] Batch 1455, Loss 0.23966917395591736\n",
      "[Training Epoch 7] Batch 1456, Loss 0.2466847449541092\n",
      "[Training Epoch 7] Batch 1457, Loss 0.28431570529937744\n",
      "[Training Epoch 7] Batch 1458, Loss 0.25192368030548096\n",
      "[Training Epoch 7] Batch 1459, Loss 0.2853695750236511\n",
      "[Training Epoch 7] Batch 1460, Loss 0.2439979463815689\n",
      "[Training Epoch 7] Batch 1461, Loss 0.2657013535499573\n",
      "[Training Epoch 7] Batch 1462, Loss 0.267697811126709\n",
      "[Training Epoch 7] Batch 1463, Loss 0.28293025493621826\n",
      "[Training Epoch 7] Batch 1464, Loss 0.27218836545944214\n",
      "[Training Epoch 7] Batch 1465, Loss 0.2569645643234253\n",
      "[Training Epoch 7] Batch 1466, Loss 0.2818595767021179\n",
      "[Training Epoch 7] Batch 1467, Loss 0.25689083337783813\n",
      "[Training Epoch 7] Batch 1468, Loss 0.25977498292922974\n",
      "[Training Epoch 7] Batch 1469, Loss 0.26411452889442444\n",
      "[Training Epoch 7] Batch 1470, Loss 0.24780675768852234\n",
      "[Training Epoch 7] Batch 1471, Loss 0.2527337372303009\n",
      "[Training Epoch 7] Batch 1472, Loss 0.2536551356315613\n",
      "[Training Epoch 7] Batch 1473, Loss 0.2523643374443054\n",
      "[Training Epoch 7] Batch 1474, Loss 0.2654455304145813\n",
      "[Training Epoch 7] Batch 1475, Loss 0.2459317147731781\n",
      "[Training Epoch 7] Batch 1476, Loss 0.2632434368133545\n",
      "[Training Epoch 7] Batch 1477, Loss 0.280354380607605\n",
      "[Training Epoch 7] Batch 1478, Loss 0.2592272162437439\n",
      "[Training Epoch 7] Batch 1479, Loss 0.23111334443092346\n",
      "[Training Epoch 7] Batch 1480, Loss 0.2385859489440918\n",
      "[Training Epoch 7] Batch 1481, Loss 0.259278267621994\n",
      "[Training Epoch 7] Batch 1482, Loss 0.26341307163238525\n",
      "[Training Epoch 7] Batch 1483, Loss 0.2848033010959625\n",
      "[Training Epoch 7] Batch 1484, Loss 0.26923179626464844\n",
      "[Training Epoch 7] Batch 1485, Loss 0.23839473724365234\n",
      "[Training Epoch 7] Batch 1486, Loss 0.24355286359786987\n",
      "[Training Epoch 7] Batch 1487, Loss 0.2695693373680115\n",
      "[Training Epoch 7] Batch 1488, Loss 0.28680136799812317\n",
      "[Training Epoch 7] Batch 1489, Loss 0.24512718617916107\n",
      "[Training Epoch 7] Batch 1490, Loss 0.2426016926765442\n",
      "[Training Epoch 7] Batch 1491, Loss 0.24786518514156342\n",
      "[Training Epoch 7] Batch 1492, Loss 0.2279340922832489\n",
      "[Training Epoch 7] Batch 1493, Loss 0.23658888041973114\n",
      "[Training Epoch 7] Batch 1494, Loss 0.2450050413608551\n",
      "[Training Epoch 7] Batch 1495, Loss 0.27084895968437195\n",
      "[Training Epoch 7] Batch 1496, Loss 0.2401946634054184\n",
      "[Training Epoch 7] Batch 1497, Loss 0.25254541635513306\n",
      "[Training Epoch 7] Batch 1498, Loss 0.24843347072601318\n",
      "[Training Epoch 7] Batch 1499, Loss 0.2437771111726761\n",
      "[Training Epoch 7] Batch 1500, Loss 0.25184547901153564\n",
      "[Training Epoch 7] Batch 1501, Loss 0.2856229245662689\n",
      "[Training Epoch 7] Batch 1502, Loss 0.2695142924785614\n",
      "[Training Epoch 7] Batch 1503, Loss 0.23422083258628845\n",
      "[Training Epoch 7] Batch 1504, Loss 0.2496965229511261\n",
      "[Training Epoch 7] Batch 1505, Loss 0.22530493140220642\n",
      "[Training Epoch 7] Batch 1506, Loss 0.26837098598480225\n",
      "[Training Epoch 7] Batch 1507, Loss 0.28080248832702637\n",
      "[Training Epoch 7] Batch 1508, Loss 0.27565157413482666\n",
      "[Training Epoch 7] Batch 1509, Loss 0.257005900144577\n",
      "[Training Epoch 7] Batch 1510, Loss 0.24582940340042114\n",
      "[Training Epoch 7] Batch 1511, Loss 0.25262385606765747\n",
      "[Training Epoch 7] Batch 1512, Loss 0.23535394668579102\n",
      "[Training Epoch 7] Batch 1513, Loss 0.25532370805740356\n",
      "[Training Epoch 7] Batch 1514, Loss 0.23316451907157898\n",
      "[Training Epoch 7] Batch 1515, Loss 0.256289541721344\n",
      "[Training Epoch 7] Batch 1516, Loss 0.25678831338882446\n",
      "[Training Epoch 7] Batch 1517, Loss 0.2498278021812439\n",
      "[Training Epoch 7] Batch 1518, Loss 0.22797253727912903\n",
      "[Training Epoch 7] Batch 1519, Loss 0.2511923611164093\n",
      "[Training Epoch 7] Batch 1520, Loss 0.24322772026062012\n",
      "[Training Epoch 7] Batch 1521, Loss 0.2799546718597412\n",
      "[Training Epoch 7] Batch 1522, Loss 0.25010716915130615\n",
      "[Training Epoch 7] Batch 1523, Loss 0.2621420919895172\n",
      "[Training Epoch 7] Batch 1524, Loss 0.23039650917053223\n",
      "[Training Epoch 7] Batch 1525, Loss 0.2662062346935272\n",
      "[Training Epoch 7] Batch 1526, Loss 0.2607189118862152\n",
      "[Training Epoch 7] Batch 1527, Loss 0.26330703496932983\n",
      "[Training Epoch 7] Batch 1528, Loss 0.2845258116722107\n",
      "[Training Epoch 7] Batch 1529, Loss 0.24469083547592163\n",
      "[Training Epoch 7] Batch 1530, Loss 0.2833271324634552\n",
      "[Training Epoch 7] Batch 1531, Loss 0.2660839855670929\n",
      "[Training Epoch 7] Batch 1532, Loss 0.253849595785141\n",
      "[Training Epoch 7] Batch 1533, Loss 0.22215285897254944\n",
      "[Training Epoch 7] Batch 1534, Loss 0.26849091053009033\n",
      "[Training Epoch 7] Batch 1535, Loss 0.24843133985996246\n",
      "[Training Epoch 7] Batch 1536, Loss 0.27787190675735474\n",
      "[Training Epoch 7] Batch 1537, Loss 0.2325480580329895\n",
      "[Training Epoch 7] Batch 1538, Loss 0.2596021294593811\n",
      "[Training Epoch 7] Batch 1539, Loss 0.23942595720291138\n",
      "[Training Epoch 7] Batch 1540, Loss 0.26764723658561707\n",
      "[Training Epoch 7] Batch 1541, Loss 0.2750673294067383\n",
      "[Training Epoch 7] Batch 1542, Loss 0.2519766688346863\n",
      "[Training Epoch 7] Batch 1543, Loss 0.24448256194591522\n",
      "[Training Epoch 7] Batch 1544, Loss 0.2849327623844147\n",
      "[Training Epoch 7] Batch 1545, Loss 0.24312889575958252\n",
      "[Training Epoch 7] Batch 1546, Loss 0.28104695677757263\n",
      "[Training Epoch 7] Batch 1547, Loss 0.24134071171283722\n",
      "[Training Epoch 7] Batch 1548, Loss 0.2490728795528412\n",
      "[Training Epoch 7] Batch 1549, Loss 0.2565387487411499\n",
      "[Training Epoch 7] Batch 1550, Loss 0.2645729184150696\n",
      "[Training Epoch 7] Batch 1551, Loss 0.253930002450943\n",
      "[Training Epoch 7] Batch 1552, Loss 0.2493729591369629\n",
      "[Training Epoch 7] Batch 1553, Loss 0.28334158658981323\n",
      "[Training Epoch 7] Batch 1554, Loss 0.28495216369628906\n",
      "[Training Epoch 7] Batch 1555, Loss 0.2703685760498047\n",
      "[Training Epoch 7] Batch 1556, Loss 0.2657950222492218\n",
      "[Training Epoch 7] Batch 1557, Loss 0.24828839302062988\n",
      "[Training Epoch 7] Batch 1558, Loss 0.25554439425468445\n",
      "[Training Epoch 7] Batch 1559, Loss 0.2683635950088501\n",
      "[Training Epoch 7] Batch 1560, Loss 0.26882830262184143\n",
      "[Training Epoch 7] Batch 1561, Loss 0.2581869661808014\n",
      "[Training Epoch 7] Batch 1562, Loss 0.25509321689605713\n",
      "[Training Epoch 7] Batch 1563, Loss 0.2783045768737793\n",
      "[Training Epoch 7] Batch 1564, Loss 0.25584107637405396\n",
      "[Training Epoch 7] Batch 1565, Loss 0.2564833164215088\n",
      "[Training Epoch 7] Batch 1566, Loss 0.2514050006866455\n",
      "[Training Epoch 7] Batch 1567, Loss 0.2366282045841217\n",
      "[Training Epoch 7] Batch 1568, Loss 0.26335692405700684\n",
      "[Training Epoch 7] Batch 1569, Loss 0.26860666275024414\n",
      "[Training Epoch 7] Batch 1570, Loss 0.25474944710731506\n",
      "[Training Epoch 7] Batch 1571, Loss 0.25617748498916626\n",
      "[Training Epoch 7] Batch 1572, Loss 0.24164241552352905\n",
      "[Training Epoch 7] Batch 1573, Loss 0.2627317011356354\n",
      "[Training Epoch 7] Batch 1574, Loss 0.2377377152442932\n",
      "[Training Epoch 7] Batch 1575, Loss 0.2724662721157074\n",
      "[Training Epoch 7] Batch 1576, Loss 0.2613494396209717\n",
      "[Training Epoch 7] Batch 1577, Loss 0.23276613652706146\n",
      "[Training Epoch 7] Batch 1578, Loss 0.26296570897102356\n",
      "[Training Epoch 7] Batch 1579, Loss 0.2814050316810608\n",
      "[Training Epoch 7] Batch 1580, Loss 0.29153820872306824\n",
      "[Training Epoch 7] Batch 1581, Loss 0.21157193183898926\n",
      "[Training Epoch 7] Batch 1582, Loss 0.26950961351394653\n",
      "[Training Epoch 7] Batch 1583, Loss 0.2651106119155884\n",
      "[Training Epoch 7] Batch 1584, Loss 0.26388460397720337\n",
      "[Training Epoch 7] Batch 1585, Loss 0.23521839082241058\n",
      "[Training Epoch 7] Batch 1586, Loss 0.27940595149993896\n",
      "[Training Epoch 7] Batch 1587, Loss 0.23287548124790192\n",
      "[Training Epoch 7] Batch 1588, Loss 0.2597898244857788\n",
      "[Training Epoch 7] Batch 1589, Loss 0.28142017126083374\n",
      "[Training Epoch 7] Batch 1590, Loss 0.26826852560043335\n",
      "[Training Epoch 7] Batch 1591, Loss 0.2576633095741272\n",
      "[Training Epoch 7] Batch 1592, Loss 0.2567112147808075\n",
      "[Training Epoch 7] Batch 1593, Loss 0.2615703046321869\n",
      "[Training Epoch 7] Batch 1594, Loss 0.2597115933895111\n",
      "[Training Epoch 7] Batch 1595, Loss 0.25125545263290405\n",
      "[Training Epoch 7] Batch 1596, Loss 0.26192641258239746\n",
      "[Training Epoch 7] Batch 1597, Loss 0.2725091278553009\n",
      "[Training Epoch 7] Batch 1598, Loss 0.2612766921520233\n",
      "[Training Epoch 7] Batch 1599, Loss 0.27511417865753174\n",
      "[Training Epoch 7] Batch 1600, Loss 0.24607495963573456\n",
      "[Training Epoch 7] Batch 1601, Loss 0.23963120579719543\n",
      "[Training Epoch 7] Batch 1602, Loss 0.26071375608444214\n",
      "[Training Epoch 7] Batch 1603, Loss 0.2595615088939667\n",
      "[Training Epoch 7] Batch 1604, Loss 0.24734778702259064\n",
      "[Training Epoch 7] Batch 1605, Loss 0.2617090046405792\n",
      "[Training Epoch 7] Batch 1606, Loss 0.23506981134414673\n",
      "[Training Epoch 7] Batch 1607, Loss 0.23783555626869202\n",
      "[Training Epoch 7] Batch 1608, Loss 0.24589112401008606\n",
      "[Training Epoch 7] Batch 1609, Loss 0.2656746804714203\n",
      "[Training Epoch 7] Batch 1610, Loss 0.25138992071151733\n",
      "[Training Epoch 7] Batch 1611, Loss 0.22565850615501404\n",
      "[Training Epoch 7] Batch 1612, Loss 0.24559468030929565\n",
      "[Training Epoch 7] Batch 1613, Loss 0.2574836015701294\n",
      "[Training Epoch 7] Batch 1614, Loss 0.27482762932777405\n",
      "[Training Epoch 7] Batch 1615, Loss 0.25909221172332764\n",
      "[Training Epoch 7] Batch 1616, Loss 0.25851041078567505\n",
      "[Training Epoch 7] Batch 1617, Loss 0.23838362097740173\n",
      "[Training Epoch 7] Batch 1618, Loss 0.2645454406738281\n",
      "[Training Epoch 7] Batch 1619, Loss 0.2641381621360779\n",
      "[Training Epoch 7] Batch 1620, Loss 0.24700918793678284\n",
      "[Training Epoch 7] Batch 1621, Loss 0.2498306781053543\n",
      "[Training Epoch 7] Batch 1622, Loss 0.2619593143463135\n",
      "[Training Epoch 7] Batch 1623, Loss 0.24498605728149414\n",
      "[Training Epoch 7] Batch 1624, Loss 0.25766056776046753\n",
      "[Training Epoch 7] Batch 1625, Loss 0.2594599723815918\n",
      "[Training Epoch 7] Batch 1626, Loss 0.27297472953796387\n",
      "[Training Epoch 7] Batch 1627, Loss 0.24832293391227722\n",
      "[Training Epoch 7] Batch 1628, Loss 0.27669140696525574\n",
      "[Training Epoch 7] Batch 1629, Loss 0.28388962149620056\n",
      "[Training Epoch 7] Batch 1630, Loss 0.26273906230926514\n",
      "[Training Epoch 7] Batch 1631, Loss 0.265073299407959\n",
      "[Training Epoch 7] Batch 1632, Loss 0.23422563076019287\n",
      "[Training Epoch 7] Batch 1633, Loss 0.2661944031715393\n",
      "[Training Epoch 7] Batch 1634, Loss 0.26920434832572937\n",
      "[Training Epoch 7] Batch 1635, Loss 0.24348874390125275\n",
      "[Training Epoch 7] Batch 1636, Loss 0.2524139881134033\n",
      "[Training Epoch 7] Batch 1637, Loss 0.23408615589141846\n",
      "[Training Epoch 7] Batch 1638, Loss 0.266940712928772\n",
      "[Training Epoch 7] Batch 1639, Loss 0.2612353563308716\n",
      "[Training Epoch 7] Batch 1640, Loss 0.23914310336112976\n",
      "[Training Epoch 7] Batch 1641, Loss 0.29698026180267334\n",
      "[Training Epoch 7] Batch 1642, Loss 0.2905401289463043\n",
      "[Training Epoch 7] Batch 1643, Loss 0.24721753597259521\n",
      "[Training Epoch 7] Batch 1644, Loss 0.2914571166038513\n",
      "[Training Epoch 7] Batch 1645, Loss 0.28343665599823\n",
      "[Training Epoch 7] Batch 1646, Loss 0.22622554004192352\n",
      "[Training Epoch 7] Batch 1647, Loss 0.25563323497772217\n",
      "[Training Epoch 7] Batch 1648, Loss 0.25492459535598755\n",
      "[Training Epoch 7] Batch 1649, Loss 0.2805348038673401\n",
      "[Training Epoch 7] Batch 1650, Loss 0.25839170813560486\n",
      "[Training Epoch 7] Batch 1651, Loss 0.26591742038726807\n",
      "[Training Epoch 7] Batch 1652, Loss 0.2703970670700073\n",
      "[Training Epoch 7] Batch 1653, Loss 0.2607649862766266\n",
      "[Training Epoch 7] Batch 1654, Loss 0.27086350321769714\n",
      "[Training Epoch 7] Batch 1655, Loss 0.24622376263141632\n",
      "[Training Epoch 7] Batch 1656, Loss 0.25632867217063904\n",
      "[Training Epoch 7] Batch 1657, Loss 0.22930431365966797\n",
      "[Training Epoch 7] Batch 1658, Loss 0.2591775059700012\n",
      "[Training Epoch 7] Batch 1659, Loss 0.23745059967041016\n",
      "[Training Epoch 7] Batch 1660, Loss 0.272359699010849\n",
      "[Training Epoch 7] Batch 1661, Loss 0.25421637296676636\n",
      "[Training Epoch 7] Batch 1662, Loss 0.23995381593704224\n",
      "[Training Epoch 7] Batch 1663, Loss 0.2307504415512085\n",
      "[Training Epoch 7] Batch 1664, Loss 0.2536073327064514\n",
      "[Training Epoch 7] Batch 1665, Loss 0.29419004917144775\n",
      "[Training Epoch 7] Batch 1666, Loss 0.25865310430526733\n",
      "[Training Epoch 7] Batch 1667, Loss 0.249141663312912\n",
      "[Training Epoch 7] Batch 1668, Loss 0.26001542806625366\n",
      "[Training Epoch 7] Batch 1669, Loss 0.2543489336967468\n",
      "[Training Epoch 7] Batch 1670, Loss 0.2745247781276703\n",
      "[Training Epoch 7] Batch 1671, Loss 0.263754665851593\n",
      "[Training Epoch 7] Batch 1672, Loss 0.24978333711624146\n",
      "[Training Epoch 7] Batch 1673, Loss 0.2549791634082794\n",
      "[Training Epoch 7] Batch 1674, Loss 0.2554962635040283\n",
      "[Training Epoch 7] Batch 1675, Loss 0.25579971075057983\n",
      "[Training Epoch 7] Batch 1676, Loss 0.2334887832403183\n",
      "[Training Epoch 7] Batch 1677, Loss 0.23958376049995422\n",
      "[Training Epoch 7] Batch 1678, Loss 0.2325880080461502\n",
      "[Training Epoch 7] Batch 1679, Loss 0.24503323435783386\n",
      "[Training Epoch 7] Batch 1680, Loss 0.2676178514957428\n",
      "[Training Epoch 7] Batch 1681, Loss 0.24530933797359467\n",
      "[Training Epoch 7] Batch 1682, Loss 0.246860072016716\n",
      "[Training Epoch 7] Batch 1683, Loss 0.24358922243118286\n",
      "[Training Epoch 7] Batch 1684, Loss 0.24567121267318726\n",
      "[Training Epoch 7] Batch 1685, Loss 0.2250436246395111\n",
      "[Training Epoch 7] Batch 1686, Loss 0.23919829726219177\n",
      "[Training Epoch 7] Batch 1687, Loss 0.21614430844783783\n",
      "[Training Epoch 7] Batch 1688, Loss 0.256661593914032\n",
      "[Training Epoch 7] Batch 1689, Loss 0.24434055387973785\n",
      "[Training Epoch 7] Batch 1690, Loss 0.2643207907676697\n",
      "[Training Epoch 7] Batch 1691, Loss 0.27258390188217163\n",
      "[Training Epoch 7] Batch 1692, Loss 0.24937845766544342\n",
      "[Training Epoch 7] Batch 1693, Loss 0.21477743983268738\n",
      "[Training Epoch 7] Batch 1694, Loss 0.2674168646335602\n",
      "[Training Epoch 7] Batch 1695, Loss 0.24475638568401337\n",
      "[Training Epoch 7] Batch 1696, Loss 0.28314417600631714\n",
      "[Training Epoch 7] Batch 1697, Loss 0.2597317695617676\n",
      "[Training Epoch 7] Batch 1698, Loss 0.2626887559890747\n",
      "[Training Epoch 7] Batch 1699, Loss 0.2589206099510193\n",
      "[Training Epoch 7] Batch 1700, Loss 0.25970780849456787\n",
      "[Training Epoch 7] Batch 1701, Loss 0.2657226622104645\n",
      "[Training Epoch 7] Batch 1702, Loss 0.2538650631904602\n",
      "[Training Epoch 7] Batch 1703, Loss 0.27868133783340454\n",
      "[Training Epoch 7] Batch 1704, Loss 0.2641001343727112\n",
      "[Training Epoch 7] Batch 1705, Loss 0.2621946930885315\n",
      "[Training Epoch 7] Batch 1706, Loss 0.30339616537094116\n",
      "[Training Epoch 7] Batch 1707, Loss 0.23784634470939636\n",
      "[Training Epoch 7] Batch 1708, Loss 0.2414354681968689\n",
      "[Training Epoch 7] Batch 1709, Loss 0.2425670176744461\n",
      "[Training Epoch 7] Batch 1710, Loss 0.25938963890075684\n",
      "[Training Epoch 7] Batch 1711, Loss 0.2933756709098816\n",
      "[Training Epoch 7] Batch 1712, Loss 0.24250428378582\n",
      "[Training Epoch 7] Batch 1713, Loss 0.24880775809288025\n",
      "[Training Epoch 7] Batch 1714, Loss 0.25144508481025696\n",
      "[Training Epoch 7] Batch 1715, Loss 0.26945844292640686\n",
      "[Training Epoch 7] Batch 1716, Loss 0.263285756111145\n",
      "[Training Epoch 7] Batch 1717, Loss 0.25634169578552246\n",
      "[Training Epoch 7] Batch 1718, Loss 0.2581911087036133\n",
      "[Training Epoch 7] Batch 1719, Loss 0.260311484336853\n",
      "[Training Epoch 7] Batch 1720, Loss 0.27105405926704407\n",
      "[Training Epoch 7] Batch 1721, Loss 0.27082860469818115\n",
      "[Training Epoch 7] Batch 1722, Loss 0.23869073390960693\n",
      "[Training Epoch 7] Batch 1723, Loss 0.27034059166908264\n",
      "[Training Epoch 7] Batch 1724, Loss 0.2872273027896881\n",
      "[Training Epoch 7] Batch 1725, Loss 0.2550734877586365\n",
      "[Training Epoch 7] Batch 1726, Loss 0.24609097838401794\n",
      "[Training Epoch 7] Batch 1727, Loss 0.2715429961681366\n",
      "[Training Epoch 7] Batch 1728, Loss 0.2658112049102783\n",
      "[Training Epoch 7] Batch 1729, Loss 0.2666356861591339\n",
      "[Training Epoch 7] Batch 1730, Loss 0.25254130363464355\n",
      "[Training Epoch 7] Batch 1731, Loss 0.2537332773208618\n",
      "[Training Epoch 7] Batch 1732, Loss 0.2694718837738037\n",
      "[Training Epoch 7] Batch 1733, Loss 0.24543650448322296\n",
      "[Training Epoch 7] Batch 1734, Loss 0.2694157063961029\n",
      "[Training Epoch 7] Batch 1735, Loss 0.24705171585083008\n",
      "[Training Epoch 7] Batch 1736, Loss 0.2465428113937378\n",
      "[Training Epoch 7] Batch 1737, Loss 0.24681976437568665\n",
      "[Training Epoch 7] Batch 1738, Loss 0.2486298531293869\n",
      "[Training Epoch 7] Batch 1739, Loss 0.26313284039497375\n",
      "[Training Epoch 7] Batch 1740, Loss 0.22728128731250763\n",
      "[Training Epoch 7] Batch 1741, Loss 0.2713244557380676\n",
      "[Training Epoch 7] Batch 1742, Loss 0.26517242193222046\n",
      "[Training Epoch 7] Batch 1743, Loss 0.2581283450126648\n",
      "[Training Epoch 7] Batch 1744, Loss 0.25370222330093384\n",
      "[Training Epoch 7] Batch 1745, Loss 0.23655462265014648\n",
      "[Training Epoch 7] Batch 1746, Loss 0.2950902283191681\n",
      "[Training Epoch 7] Batch 1747, Loss 0.2733875811100006\n",
      "[Training Epoch 7] Batch 1748, Loss 0.24025414884090424\n",
      "[Training Epoch 7] Batch 1749, Loss 0.2435322403907776\n",
      "[Training Epoch 7] Batch 1750, Loss 0.23833858966827393\n",
      "[Training Epoch 7] Batch 1751, Loss 0.26678577065467834\n",
      "[Training Epoch 7] Batch 1752, Loss 0.2688482999801636\n",
      "[Training Epoch 7] Batch 1753, Loss 0.29215359687805176\n",
      "[Training Epoch 7] Batch 1754, Loss 0.276556134223938\n",
      "[Training Epoch 7] Batch 1755, Loss 0.26480627059936523\n",
      "[Training Epoch 7] Batch 1756, Loss 0.25284451246261597\n",
      "[Training Epoch 7] Batch 1757, Loss 0.2505715489387512\n",
      "[Training Epoch 7] Batch 1758, Loss 0.24911415576934814\n",
      "[Training Epoch 7] Batch 1759, Loss 0.24240237474441528\n",
      "[Training Epoch 7] Batch 1760, Loss 0.2429947853088379\n",
      "[Training Epoch 7] Batch 1761, Loss 0.25980836153030396\n",
      "[Training Epoch 7] Batch 1762, Loss 0.28566694259643555\n",
      "[Training Epoch 7] Batch 1763, Loss 0.24171678721904755\n",
      "[Training Epoch 7] Batch 1764, Loss 0.2460162341594696\n",
      "[Training Epoch 7] Batch 1765, Loss 0.2510925233364105\n",
      "[Training Epoch 7] Batch 1766, Loss 0.27649253606796265\n",
      "[Training Epoch 7] Batch 1767, Loss 0.2699064016342163\n",
      "[Training Epoch 7] Batch 1768, Loss 0.24989411234855652\n",
      "[Training Epoch 7] Batch 1769, Loss 0.27409884333610535\n",
      "[Training Epoch 7] Batch 1770, Loss 0.2670881152153015\n",
      "[Training Epoch 7] Batch 1771, Loss 0.24420678615570068\n",
      "[Training Epoch 7] Batch 1772, Loss 0.24206280708312988\n",
      "[Training Epoch 7] Batch 1773, Loss 0.25646114349365234\n",
      "[Training Epoch 7] Batch 1774, Loss 0.24943028390407562\n",
      "[Training Epoch 7] Batch 1775, Loss 0.2628095746040344\n",
      "[Training Epoch 7] Batch 1776, Loss 0.29071205854415894\n",
      "[Training Epoch 7] Batch 1777, Loss 0.2622252106666565\n",
      "[Training Epoch 7] Batch 1778, Loss 0.2528935670852661\n",
      "[Training Epoch 7] Batch 1779, Loss 0.26225388050079346\n",
      "[Training Epoch 7] Batch 1780, Loss 0.28142106533050537\n",
      "[Training Epoch 7] Batch 1781, Loss 0.23537477850914001\n",
      "[Training Epoch 7] Batch 1782, Loss 0.23808404803276062\n",
      "[Training Epoch 7] Batch 1783, Loss 0.28968119621276855\n",
      "[Training Epoch 7] Batch 1784, Loss 0.22934946417808533\n",
      "[Training Epoch 7] Batch 1785, Loss 0.2949340343475342\n",
      "[Training Epoch 7] Batch 1786, Loss 0.22989192605018616\n",
      "[Training Epoch 7] Batch 1787, Loss 0.27252084016799927\n",
      "[Training Epoch 7] Batch 1788, Loss 0.23972167074680328\n",
      "[Training Epoch 7] Batch 1789, Loss 0.24576210975646973\n",
      "[Training Epoch 7] Batch 1790, Loss 0.2575055658817291\n",
      "[Training Epoch 7] Batch 1791, Loss 0.26449790596961975\n",
      "[Training Epoch 7] Batch 1792, Loss 0.25864726305007935\n",
      "[Training Epoch 7] Batch 1793, Loss 0.253581166267395\n",
      "[Training Epoch 7] Batch 1794, Loss 0.26266413927078247\n",
      "[Training Epoch 7] Batch 1795, Loss 0.27501654624938965\n",
      "[Training Epoch 7] Batch 1796, Loss 0.2783716320991516\n",
      "[Training Epoch 7] Batch 1797, Loss 0.2509081959724426\n",
      "[Training Epoch 7] Batch 1798, Loss 0.254710853099823\n",
      "[Training Epoch 7] Batch 1799, Loss 0.24637947976589203\n",
      "[Training Epoch 7] Batch 1800, Loss 0.22406379878520966\n",
      "[Training Epoch 7] Batch 1801, Loss 0.26548928022384644\n",
      "[Training Epoch 7] Batch 1802, Loss 0.2594843804836273\n",
      "[Training Epoch 7] Batch 1803, Loss 0.2497849464416504\n",
      "[Training Epoch 7] Batch 1804, Loss 0.2588897943496704\n",
      "[Training Epoch 7] Batch 1805, Loss 0.2638589143753052\n",
      "[Training Epoch 7] Batch 1806, Loss 0.25699159502983093\n",
      "[Training Epoch 7] Batch 1807, Loss 0.2626231610774994\n",
      "[Training Epoch 7] Batch 1808, Loss 0.2330613136291504\n",
      "[Training Epoch 7] Batch 1809, Loss 0.27709096670150757\n",
      "[Training Epoch 7] Batch 1810, Loss 0.2421063482761383\n",
      "[Training Epoch 7] Batch 1811, Loss 0.26279327273368835\n",
      "[Training Epoch 7] Batch 1812, Loss 0.25832492113113403\n",
      "[Training Epoch 7] Batch 1813, Loss 0.25799211859703064\n",
      "[Training Epoch 7] Batch 1814, Loss 0.27816951274871826\n",
      "[Training Epoch 7] Batch 1815, Loss 0.2605288028717041\n",
      "[Training Epoch 7] Batch 1816, Loss 0.244991272687912\n",
      "[Training Epoch 7] Batch 1817, Loss 0.2531987726688385\n",
      "[Training Epoch 7] Batch 1818, Loss 0.2812325358390808\n",
      "[Training Epoch 7] Batch 1819, Loss 0.263059139251709\n",
      "[Training Epoch 7] Batch 1820, Loss 0.23477144539356232\n",
      "[Training Epoch 7] Batch 1821, Loss 0.22706303000450134\n",
      "[Training Epoch 7] Batch 1822, Loss 0.2600470781326294\n",
      "[Training Epoch 7] Batch 1823, Loss 0.26695963740348816\n",
      "[Training Epoch 7] Batch 1824, Loss 0.23549363017082214\n",
      "[Training Epoch 7] Batch 1825, Loss 0.2281455397605896\n",
      "[Training Epoch 7] Batch 1826, Loss 0.2529541850090027\n",
      "[Training Epoch 7] Batch 1827, Loss 0.23672641813755035\n",
      "[Training Epoch 7] Batch 1828, Loss 0.26390108466148376\n",
      "[Training Epoch 7] Batch 1829, Loss 0.26055294275283813\n",
      "[Training Epoch 7] Batch 1830, Loss 0.24984398484230042\n",
      "[Training Epoch 7] Batch 1831, Loss 0.2869298458099365\n",
      "[Training Epoch 7] Batch 1832, Loss 0.23635928332805634\n",
      "[Training Epoch 7] Batch 1833, Loss 0.26597899198532104\n",
      "[Training Epoch 7] Batch 1834, Loss 0.2623547613620758\n",
      "[Training Epoch 7] Batch 1835, Loss 0.23829972743988037\n",
      "[Training Epoch 7] Batch 1836, Loss 0.24445147812366486\n",
      "[Training Epoch 7] Batch 1837, Loss 0.27786627411842346\n",
      "[Training Epoch 7] Batch 1838, Loss 0.2660354971885681\n",
      "[Training Epoch 7] Batch 1839, Loss 0.2660386860370636\n",
      "[Training Epoch 7] Batch 1840, Loss 0.25491589307785034\n",
      "[Training Epoch 7] Batch 1841, Loss 0.2595519721508026\n",
      "[Training Epoch 7] Batch 1842, Loss 0.26375001668930054\n",
      "[Training Epoch 7] Batch 1843, Loss 0.2793727517127991\n",
      "[Training Epoch 7] Batch 1844, Loss 0.25608089566230774\n",
      "[Training Epoch 7] Batch 1845, Loss 0.24148768186569214\n",
      "[Training Epoch 7] Batch 1846, Loss 0.27676934003829956\n",
      "[Training Epoch 7] Batch 1847, Loss 0.286581426858902\n",
      "[Training Epoch 7] Batch 1848, Loss 0.27126139402389526\n",
      "[Training Epoch 7] Batch 1849, Loss 0.2600800395011902\n",
      "[Training Epoch 7] Batch 1850, Loss 0.22908121347427368\n",
      "[Training Epoch 7] Batch 1851, Loss 0.2538754940032959\n",
      "[Training Epoch 7] Batch 1852, Loss 0.2776848077774048\n",
      "[Training Epoch 7] Batch 1853, Loss 0.2798376679420471\n",
      "[Training Epoch 7] Batch 1854, Loss 0.23577561974525452\n",
      "[Training Epoch 7] Batch 1855, Loss 0.22790181636810303\n",
      "[Training Epoch 7] Batch 1856, Loss 0.26573076844215393\n",
      "[Training Epoch 7] Batch 1857, Loss 0.25207334756851196\n",
      "[Training Epoch 7] Batch 1858, Loss 0.28712040185928345\n",
      "[Training Epoch 7] Batch 1859, Loss 0.25147175788879395\n",
      "[Training Epoch 7] Batch 1860, Loss 0.2636251449584961\n",
      "[Training Epoch 7] Batch 1861, Loss 0.24081921577453613\n",
      "[Training Epoch 7] Batch 1862, Loss 0.2692093849182129\n",
      "[Training Epoch 7] Batch 1863, Loss 0.24579165875911713\n",
      "[Training Epoch 7] Batch 1864, Loss 0.2644336223602295\n",
      "[Training Epoch 7] Batch 1865, Loss 0.2873556613922119\n",
      "[Training Epoch 7] Batch 1866, Loss 0.2542002499103546\n",
      "[Training Epoch 7] Batch 1867, Loss 0.2630555033683777\n",
      "[Training Epoch 7] Batch 1868, Loss 0.2466834932565689\n",
      "[Training Epoch 7] Batch 1869, Loss 0.24092304706573486\n",
      "[Training Epoch 7] Batch 1870, Loss 0.25315576791763306\n",
      "[Training Epoch 7] Batch 1871, Loss 0.2671942114830017\n",
      "[Training Epoch 7] Batch 1872, Loss 0.255199670791626\n",
      "[Training Epoch 7] Batch 1873, Loss 0.2791707515716553\n",
      "[Training Epoch 7] Batch 1874, Loss 0.26351648569107056\n",
      "[Training Epoch 7] Batch 1875, Loss 0.22228612005710602\n",
      "[Training Epoch 7] Batch 1876, Loss 0.24237264692783356\n",
      "[Training Epoch 7] Batch 1877, Loss 0.2511465549468994\n",
      "[Training Epoch 7] Batch 1878, Loss 0.2517489492893219\n",
      "[Training Epoch 7] Batch 1879, Loss 0.28220534324645996\n",
      "[Training Epoch 7] Batch 1880, Loss 0.2420646846294403\n",
      "[Training Epoch 7] Batch 1881, Loss 0.2698947489261627\n",
      "[Training Epoch 7] Batch 1882, Loss 0.2920990288257599\n",
      "[Training Epoch 7] Batch 1883, Loss 0.21617326140403748\n",
      "[Training Epoch 7] Batch 1884, Loss 0.2286786437034607\n",
      "[Training Epoch 7] Batch 1885, Loss 0.24422281980514526\n",
      "[Training Epoch 7] Batch 1886, Loss 0.24336089193820953\n",
      "[Training Epoch 7] Batch 1887, Loss 0.25150859355926514\n",
      "[Training Epoch 7] Batch 1888, Loss 0.2348911166191101\n",
      "[Training Epoch 7] Batch 1889, Loss 0.2743102014064789\n",
      "[Training Epoch 7] Batch 1890, Loss 0.24609030783176422\n",
      "[Training Epoch 7] Batch 1891, Loss 0.2470841258764267\n",
      "[Training Epoch 7] Batch 1892, Loss 0.2327015995979309\n",
      "[Training Epoch 7] Batch 1893, Loss 0.24832575023174286\n",
      "[Training Epoch 7] Batch 1894, Loss 0.2530883252620697\n",
      "[Training Epoch 7] Batch 1895, Loss 0.2756318747997284\n",
      "[Training Epoch 7] Batch 1896, Loss 0.25171446800231934\n",
      "[Training Epoch 7] Batch 1897, Loss 0.23412449657917023\n",
      "[Training Epoch 7] Batch 1898, Loss 0.2527082860469818\n",
      "[Training Epoch 7] Batch 1899, Loss 0.2499971091747284\n",
      "[Training Epoch 7] Batch 1900, Loss 0.2714633047580719\n",
      "[Training Epoch 7] Batch 1901, Loss 0.22687143087387085\n",
      "[Training Epoch 7] Batch 1902, Loss 0.27873581647872925\n",
      "[Training Epoch 7] Batch 1903, Loss 0.24100244045257568\n",
      "[Training Epoch 7] Batch 1904, Loss 0.2512491047382355\n",
      "[Training Epoch 7] Batch 1905, Loss 0.24554325640201569\n",
      "[Training Epoch 7] Batch 1906, Loss 0.24287444353103638\n",
      "[Training Epoch 7] Batch 1907, Loss 0.21732762455940247\n",
      "[Training Epoch 7] Batch 1908, Loss 0.233491912484169\n",
      "[Training Epoch 7] Batch 1909, Loss 0.2694067358970642\n",
      "[Training Epoch 7] Batch 1910, Loss 0.275468647480011\n",
      "[Training Epoch 7] Batch 1911, Loss 0.2686541676521301\n",
      "[Training Epoch 7] Batch 1912, Loss 0.26603996753692627\n",
      "[Training Epoch 7] Batch 1913, Loss 0.25065821409225464\n",
      "[Training Epoch 7] Batch 1914, Loss 0.26417210698127747\n",
      "[Training Epoch 7] Batch 1915, Loss 0.2575986385345459\n",
      "[Training Epoch 7] Batch 1916, Loss 0.22712871432304382\n",
      "[Training Epoch 7] Batch 1917, Loss 0.24776263535022736\n",
      "[Training Epoch 7] Batch 1918, Loss 0.23945766687393188\n",
      "[Training Epoch 7] Batch 1919, Loss 0.2669633626937866\n",
      "[Training Epoch 7] Batch 1920, Loss 0.24039442837238312\n",
      "[Training Epoch 7] Batch 1921, Loss 0.2589836120605469\n",
      "[Training Epoch 7] Batch 1922, Loss 0.23355521261692047\n",
      "[Training Epoch 7] Batch 1923, Loss 0.21779701113700867\n",
      "[Training Epoch 7] Batch 1924, Loss 0.27745354175567627\n",
      "[Training Epoch 7] Batch 1925, Loss 0.24615469574928284\n",
      "[Training Epoch 7] Batch 1926, Loss 0.23195959627628326\n",
      "[Training Epoch 7] Batch 1927, Loss 0.23310135304927826\n",
      "[Training Epoch 7] Batch 1928, Loss 0.26308178901672363\n",
      "[Training Epoch 7] Batch 1929, Loss 0.2684020698070526\n",
      "[Training Epoch 7] Batch 1930, Loss 0.2470863163471222\n",
      "[Training Epoch 7] Batch 1931, Loss 0.23960968852043152\n",
      "[Training Epoch 7] Batch 1932, Loss 0.2719172537326813\n",
      "[Training Epoch 7] Batch 1933, Loss 0.2678160071372986\n",
      "[Training Epoch 7] Batch 1934, Loss 0.2835785150527954\n",
      "[Training Epoch 7] Batch 1935, Loss 0.2458067387342453\n",
      "[Training Epoch 7] Batch 1936, Loss 0.27328503131866455\n",
      "[Training Epoch 7] Batch 1937, Loss 0.23065784573554993\n",
      "[Training Epoch 7] Batch 1938, Loss 0.26489681005477905\n",
      "[Training Epoch 7] Batch 1939, Loss 0.2684369683265686\n",
      "[Training Epoch 7] Batch 1940, Loss 0.24647732079029083\n",
      "[Training Epoch 7] Batch 1941, Loss 0.2582862377166748\n",
      "[Training Epoch 7] Batch 1942, Loss 0.26980286836624146\n",
      "[Training Epoch 7] Batch 1943, Loss 0.2563011646270752\n",
      "[Training Epoch 7] Batch 1944, Loss 0.25866106152534485\n",
      "[Training Epoch 7] Batch 1945, Loss 0.26245445013046265\n",
      "[Training Epoch 7] Batch 1946, Loss 0.27644428610801697\n",
      "[Training Epoch 7] Batch 1947, Loss 0.23399734497070312\n",
      "[Training Epoch 7] Batch 1948, Loss 0.24942907691001892\n",
      "[Training Epoch 7] Batch 1949, Loss 0.2442980408668518\n",
      "[Training Epoch 7] Batch 1950, Loss 0.2746644616127014\n",
      "[Training Epoch 7] Batch 1951, Loss 0.263639360666275\n",
      "[Training Epoch 7] Batch 1952, Loss 0.266297310590744\n",
      "[Training Epoch 7] Batch 1953, Loss 0.26496559381484985\n",
      "[Training Epoch 7] Batch 1954, Loss 0.24561621248722076\n",
      "[Training Epoch 7] Batch 1955, Loss 0.2616429626941681\n",
      "[Training Epoch 7] Batch 1956, Loss 0.2670692205429077\n",
      "[Training Epoch 7] Batch 1957, Loss 0.2531663179397583\n",
      "[Training Epoch 7] Batch 1958, Loss 0.24241763353347778\n",
      "[Training Epoch 7] Batch 1959, Loss 0.2776047885417938\n",
      "[Training Epoch 7] Batch 1960, Loss 0.26117485761642456\n",
      "[Training Epoch 7] Batch 1961, Loss 0.21346308290958405\n",
      "[Training Epoch 7] Batch 1962, Loss 0.23002946376800537\n",
      "[Training Epoch 7] Batch 1963, Loss 0.26239335536956787\n",
      "[Training Epoch 7] Batch 1964, Loss 0.25818824768066406\n",
      "[Training Epoch 7] Batch 1965, Loss 0.24996206164360046\n",
      "[Training Epoch 7] Batch 1966, Loss 0.26857465505599976\n",
      "[Training Epoch 7] Batch 1967, Loss 0.2384648472070694\n",
      "[Training Epoch 7] Batch 1968, Loss 0.2557114362716675\n",
      "[Training Epoch 7] Batch 1969, Loss 0.2566336989402771\n",
      "[Training Epoch 7] Batch 1970, Loss 0.2343270480632782\n",
      "[Training Epoch 7] Batch 1971, Loss 0.24844932556152344\n",
      "[Training Epoch 7] Batch 1972, Loss 0.2591004967689514\n",
      "[Training Epoch 7] Batch 1973, Loss 0.2553016245365143\n",
      "[Training Epoch 7] Batch 1974, Loss 0.24523033201694489\n",
      "[Training Epoch 7] Batch 1975, Loss 0.2591528594493866\n",
      "[Training Epoch 7] Batch 1976, Loss 0.25314655900001526\n",
      "[Training Epoch 7] Batch 1977, Loss 0.22696249186992645\n",
      "[Training Epoch 7] Batch 1978, Loss 0.25571945309638977\n",
      "[Training Epoch 7] Batch 1979, Loss 0.24510951340198517\n",
      "[Training Epoch 7] Batch 1980, Loss 0.25418516993522644\n",
      "[Training Epoch 7] Batch 1981, Loss 0.2571074366569519\n",
      "[Training Epoch 7] Batch 1982, Loss 0.2532675266265869\n",
      "[Training Epoch 7] Batch 1983, Loss 0.22974975407123566\n",
      "[Training Epoch 7] Batch 1984, Loss 0.26095861196517944\n",
      "[Training Epoch 7] Batch 1985, Loss 0.26428085565567017\n",
      "[Training Epoch 7] Batch 1986, Loss 0.2651892900466919\n",
      "[Training Epoch 7] Batch 1987, Loss 0.240266814827919\n",
      "[Training Epoch 7] Batch 1988, Loss 0.24297241866588593\n",
      "[Training Epoch 7] Batch 1989, Loss 0.26956042647361755\n",
      "[Training Epoch 7] Batch 1990, Loss 0.24226266145706177\n",
      "[Training Epoch 7] Batch 1991, Loss 0.23127610981464386\n",
      "[Training Epoch 7] Batch 1992, Loss 0.2695980966091156\n",
      "[Training Epoch 7] Batch 1993, Loss 0.2501632273197174\n",
      "[Training Epoch 7] Batch 1994, Loss 0.2687270939350128\n",
      "[Training Epoch 7] Batch 1995, Loss 0.2687286138534546\n",
      "[Training Epoch 7] Batch 1996, Loss 0.26067477464675903\n",
      "[Training Epoch 7] Batch 1997, Loss 0.23434248566627502\n",
      "[Training Epoch 7] Batch 1998, Loss 0.26003679633140564\n",
      "[Training Epoch 7] Batch 1999, Loss 0.25838345289230347\n",
      "[Training Epoch 7] Batch 2000, Loss 0.2554764151573181\n",
      "[Training Epoch 7] Batch 2001, Loss 0.2775726914405823\n",
      "[Training Epoch 7] Batch 2002, Loss 0.2451702058315277\n",
      "[Training Epoch 7] Batch 2003, Loss 0.25544172525405884\n",
      "[Training Epoch 7] Batch 2004, Loss 0.24345079064369202\n",
      "[Training Epoch 7] Batch 2005, Loss 0.2589288055896759\n",
      "[Training Epoch 7] Batch 2006, Loss 0.26925766468048096\n",
      "[Training Epoch 7] Batch 2007, Loss 0.2737704813480377\n",
      "[Training Epoch 7] Batch 2008, Loss 0.25856783986091614\n",
      "[Training Epoch 7] Batch 2009, Loss 0.23725451529026031\n",
      "[Training Epoch 7] Batch 2010, Loss 0.2710367739200592\n",
      "[Training Epoch 7] Batch 2011, Loss 0.2552410364151001\n",
      "[Training Epoch 7] Batch 2012, Loss 0.2594837546348572\n",
      "[Training Epoch 7] Batch 2013, Loss 0.2513822317123413\n",
      "[Training Epoch 7] Batch 2014, Loss 0.2708951532840729\n",
      "[Training Epoch 7] Batch 2015, Loss 0.2381535917520523\n",
      "[Training Epoch 7] Batch 2016, Loss 0.26308876276016235\n",
      "[Training Epoch 7] Batch 2017, Loss 0.2583262622356415\n",
      "[Training Epoch 7] Batch 2018, Loss 0.23654936254024506\n",
      "[Training Epoch 7] Batch 2019, Loss 0.27204301953315735\n",
      "[Training Epoch 7] Batch 2020, Loss 0.27762848138809204\n",
      "[Training Epoch 7] Batch 2021, Loss 0.2626878023147583\n",
      "[Training Epoch 7] Batch 2022, Loss 0.25305822491645813\n",
      "[Training Epoch 7] Batch 2023, Loss 0.28237056732177734\n",
      "[Training Epoch 7] Batch 2024, Loss 0.2501523792743683\n",
      "[Training Epoch 7] Batch 2025, Loss 0.2516627907752991\n",
      "[Training Epoch 7] Batch 2026, Loss 0.26188209652900696\n",
      "[Training Epoch 7] Batch 2027, Loss 0.255565881729126\n",
      "[Training Epoch 7] Batch 2028, Loss 0.2577704191207886\n",
      "[Training Epoch 7] Batch 2029, Loss 0.25171172618865967\n",
      "[Training Epoch 7] Batch 2030, Loss 0.29165881872177124\n",
      "[Training Epoch 7] Batch 2031, Loss 0.26681676506996155\n",
      "[Training Epoch 7] Batch 2032, Loss 0.2590959072113037\n",
      "[Training Epoch 7] Batch 2033, Loss 0.261762797832489\n",
      "[Training Epoch 7] Batch 2034, Loss 0.2650606036186218\n",
      "[Training Epoch 7] Batch 2035, Loss 0.2492729127407074\n",
      "[Training Epoch 7] Batch 2036, Loss 0.29304224252700806\n",
      "[Training Epoch 7] Batch 2037, Loss 0.2807820439338684\n",
      "[Training Epoch 7] Batch 2038, Loss 0.2649076282978058\n",
      "[Training Epoch 7] Batch 2039, Loss 0.2609941363334656\n",
      "[Training Epoch 7] Batch 2040, Loss 0.2894481420516968\n",
      "[Training Epoch 7] Batch 2041, Loss 0.2453482747077942\n",
      "[Training Epoch 7] Batch 2042, Loss 0.28023064136505127\n",
      "[Training Epoch 7] Batch 2043, Loss 0.22965309023857117\n",
      "[Training Epoch 7] Batch 2044, Loss 0.2536775767803192\n",
      "[Training Epoch 7] Batch 2045, Loss 0.2366480827331543\n",
      "[Training Epoch 7] Batch 2046, Loss 0.2425748109817505\n",
      "[Training Epoch 7] Batch 2047, Loss 0.2509925961494446\n",
      "[Training Epoch 7] Batch 2048, Loss 0.23957830667495728\n",
      "[Training Epoch 7] Batch 2049, Loss 0.257875919342041\n",
      "[Training Epoch 7] Batch 2050, Loss 0.2646368741989136\n",
      "[Training Epoch 7] Batch 2051, Loss 0.2421179711818695\n",
      "[Training Epoch 7] Batch 2052, Loss 0.25759610533714294\n",
      "[Training Epoch 7] Batch 2053, Loss 0.25794652104377747\n",
      "[Training Epoch 7] Batch 2054, Loss 0.25583726167678833\n",
      "[Training Epoch 7] Batch 2055, Loss 0.27064770460128784\n",
      "[Training Epoch 7] Batch 2056, Loss 0.26906946301460266\n",
      "[Training Epoch 7] Batch 2057, Loss 0.25202396512031555\n",
      "[Training Epoch 7] Batch 2058, Loss 0.2742930054664612\n",
      "[Training Epoch 7] Batch 2059, Loss 0.3131439685821533\n",
      "[Training Epoch 7] Batch 2060, Loss 0.27973294258117676\n",
      "[Training Epoch 7] Batch 2061, Loss 0.24682258069515228\n",
      "[Training Epoch 7] Batch 2062, Loss 0.2618411183357239\n",
      "[Training Epoch 7] Batch 2063, Loss 0.2489423155784607\n",
      "[Training Epoch 7] Batch 2064, Loss 0.250455379486084\n",
      "[Training Epoch 7] Batch 2065, Loss 0.27077800035476685\n",
      "[Training Epoch 7] Batch 2066, Loss 0.2634257674217224\n",
      "[Training Epoch 7] Batch 2067, Loss 0.2563621401786804\n",
      "[Training Epoch 7] Batch 2068, Loss 0.24686792492866516\n",
      "[Training Epoch 7] Batch 2069, Loss 0.24843284487724304\n",
      "[Training Epoch 7] Batch 2070, Loss 0.24990694224834442\n",
      "[Training Epoch 7] Batch 2071, Loss 0.27686330676078796\n",
      "[Training Epoch 7] Batch 2072, Loss 0.2670685648918152\n",
      "[Training Epoch 7] Batch 2073, Loss 0.2612293064594269\n",
      "[Training Epoch 7] Batch 2074, Loss 0.2601785957813263\n",
      "[Training Epoch 7] Batch 2075, Loss 0.2316957414150238\n",
      "[Training Epoch 7] Batch 2076, Loss 0.24528789520263672\n",
      "[Training Epoch 7] Batch 2077, Loss 0.27506160736083984\n",
      "[Training Epoch 7] Batch 2078, Loss 0.2573680877685547\n",
      "[Training Epoch 7] Batch 2079, Loss 0.2601580023765564\n",
      "[Training Epoch 7] Batch 2080, Loss 0.2298479825258255\n",
      "[Training Epoch 7] Batch 2081, Loss 0.23648478090763092\n",
      "[Training Epoch 7] Batch 2082, Loss 0.2542375922203064\n",
      "[Training Epoch 7] Batch 2083, Loss 0.2380739450454712\n",
      "[Training Epoch 7] Batch 2084, Loss 0.27672356367111206\n",
      "[Training Epoch 7] Batch 2085, Loss 0.27102285623550415\n",
      "[Training Epoch 7] Batch 2086, Loss 0.260407418012619\n",
      "[Training Epoch 7] Batch 2087, Loss 0.26259297132492065\n",
      "[Training Epoch 7] Batch 2088, Loss 0.25729507207870483\n",
      "[Training Epoch 7] Batch 2089, Loss 0.2542707324028015\n",
      "[Training Epoch 7] Batch 2090, Loss 0.2384418398141861\n",
      "[Training Epoch 7] Batch 2091, Loss 0.272532194852829\n",
      "[Training Epoch 7] Batch 2092, Loss 0.2734622657299042\n",
      "[Training Epoch 7] Batch 2093, Loss 0.2725740075111389\n",
      "[Training Epoch 7] Batch 2094, Loss 0.25458407402038574\n",
      "[Training Epoch 7] Batch 2095, Loss 0.2510717511177063\n",
      "[Training Epoch 7] Batch 2096, Loss 0.27128851413726807\n",
      "[Training Epoch 7] Batch 2097, Loss 0.2365424633026123\n",
      "[Training Epoch 7] Batch 2098, Loss 0.2500026226043701\n",
      "[Training Epoch 7] Batch 2099, Loss 0.25401628017425537\n",
      "[Training Epoch 7] Batch 2100, Loss 0.2551347017288208\n",
      "[Training Epoch 7] Batch 2101, Loss 0.2577935457229614\n",
      "[Training Epoch 7] Batch 2102, Loss 0.23127852380275726\n",
      "[Training Epoch 7] Batch 2103, Loss 0.25916123390197754\n",
      "[Training Epoch 7] Batch 2104, Loss 0.2499382495880127\n",
      "[Training Epoch 7] Batch 2105, Loss 0.26978975534439087\n",
      "[Training Epoch 7] Batch 2106, Loss 0.24072769284248352\n",
      "[Training Epoch 7] Batch 2107, Loss 0.22877803444862366\n",
      "[Training Epoch 7] Batch 2108, Loss 0.26697224378585815\n",
      "[Training Epoch 7] Batch 2109, Loss 0.23927323520183563\n",
      "[Training Epoch 7] Batch 2110, Loss 0.26634854078292847\n",
      "[Training Epoch 7] Batch 2111, Loss 0.264530748128891\n",
      "[Training Epoch 7] Batch 2112, Loss 0.27457451820373535\n",
      "[Training Epoch 7] Batch 2113, Loss 0.2726198434829712\n",
      "[Training Epoch 7] Batch 2114, Loss 0.224037766456604\n",
      "[Training Epoch 7] Batch 2115, Loss 0.2483910173177719\n",
      "[Training Epoch 7] Batch 2116, Loss 0.239913210272789\n",
      "[Training Epoch 7] Batch 2117, Loss 0.2515392601490021\n",
      "[Training Epoch 7] Batch 2118, Loss 0.2645510137081146\n",
      "[Training Epoch 7] Batch 2119, Loss 0.2565467357635498\n",
      "[Training Epoch 7] Batch 2120, Loss 0.2598482370376587\n",
      "[Training Epoch 7] Batch 2121, Loss 0.25202110409736633\n",
      "[Training Epoch 7] Batch 2122, Loss 0.26353341341018677\n",
      "[Training Epoch 7] Batch 2123, Loss 0.2914506196975708\n",
      "[Training Epoch 7] Batch 2124, Loss 0.23999841511249542\n",
      "[Training Epoch 7] Batch 2125, Loss 0.2785651385784149\n",
      "[Training Epoch 7] Batch 2126, Loss 0.2578938603401184\n",
      "[Training Epoch 7] Batch 2127, Loss 0.22979027032852173\n",
      "[Training Epoch 7] Batch 2128, Loss 0.28814905881881714\n",
      "[Training Epoch 7] Batch 2129, Loss 0.2625817656517029\n",
      "[Training Epoch 7] Batch 2130, Loss 0.23877499997615814\n",
      "[Training Epoch 7] Batch 2131, Loss 0.2528489828109741\n",
      "[Training Epoch 7] Batch 2132, Loss 0.23857268691062927\n",
      "[Training Epoch 7] Batch 2133, Loss 0.2771674394607544\n",
      "[Training Epoch 7] Batch 2134, Loss 0.2380850464105606\n",
      "[Training Epoch 7] Batch 2135, Loss 0.2603882849216461\n",
      "[Training Epoch 7] Batch 2136, Loss 0.24883225560188293\n",
      "[Training Epoch 7] Batch 2137, Loss 0.23151075839996338\n",
      "[Training Epoch 7] Batch 2138, Loss 0.24475297331809998\n",
      "[Training Epoch 7] Batch 2139, Loss 0.2395937144756317\n",
      "[Training Epoch 7] Batch 2140, Loss 0.2627222537994385\n",
      "[Training Epoch 7] Batch 2141, Loss 0.2589361369609833\n",
      "[Training Epoch 7] Batch 2142, Loss 0.24466778337955475\n",
      "[Training Epoch 7] Batch 2143, Loss 0.24290215969085693\n",
      "[Training Epoch 7] Batch 2144, Loss 0.25760966539382935\n",
      "[Training Epoch 7] Batch 2145, Loss 0.2745351493358612\n",
      "[Training Epoch 7] Batch 2146, Loss 0.2805989980697632\n",
      "[Training Epoch 7] Batch 2147, Loss 0.2605801224708557\n",
      "[Training Epoch 7] Batch 2148, Loss 0.26555031538009644\n",
      "[Training Epoch 7] Batch 2149, Loss 0.24963179230690002\n",
      "[Training Epoch 7] Batch 2150, Loss 0.21821807324886322\n",
      "[Training Epoch 7] Batch 2151, Loss 0.24407880008220673\n",
      "[Training Epoch 7] Batch 2152, Loss 0.26683199405670166\n",
      "[Training Epoch 7] Batch 2153, Loss 0.25761497020721436\n",
      "[Training Epoch 7] Batch 2154, Loss 0.25491949915885925\n",
      "[Training Epoch 7] Batch 2155, Loss 0.24423068761825562\n",
      "[Training Epoch 7] Batch 2156, Loss 0.23488718271255493\n",
      "[Training Epoch 7] Batch 2157, Loss 0.2698974907398224\n",
      "[Training Epoch 7] Batch 2158, Loss 0.26060545444488525\n",
      "[Training Epoch 7] Batch 2159, Loss 0.2470954954624176\n",
      "[Training Epoch 7] Batch 2160, Loss 0.25525790452957153\n",
      "[Training Epoch 7] Batch 2161, Loss 0.24826417863368988\n",
      "[Training Epoch 7] Batch 2162, Loss 0.28166845440864563\n",
      "[Training Epoch 7] Batch 2163, Loss 0.25466009974479675\n",
      "[Training Epoch 7] Batch 2164, Loss 0.26477086544036865\n",
      "[Training Epoch 7] Batch 2165, Loss 0.2616938650608063\n",
      "[Training Epoch 7] Batch 2166, Loss 0.27533045411109924\n",
      "[Training Epoch 7] Batch 2167, Loss 0.2888021469116211\n",
      "[Training Epoch 7] Batch 2168, Loss 0.2730816602706909\n",
      "[Training Epoch 7] Batch 2169, Loss 0.2528517246246338\n",
      "[Training Epoch 7] Batch 2170, Loss 0.24720881879329681\n",
      "[Training Epoch 7] Batch 2171, Loss 0.2675135135650635\n",
      "[Training Epoch 7] Batch 2172, Loss 0.22562602162361145\n",
      "[Training Epoch 7] Batch 2173, Loss 0.2503318190574646\n",
      "[Training Epoch 7] Batch 2174, Loss 0.2386065125465393\n",
      "[Training Epoch 7] Batch 2175, Loss 0.28787392377853394\n",
      "[Training Epoch 7] Batch 2176, Loss 0.30668413639068604\n",
      "[Training Epoch 7] Batch 2177, Loss 0.2575245499610901\n",
      "[Training Epoch 7] Batch 2178, Loss 0.23602700233459473\n",
      "[Training Epoch 7] Batch 2179, Loss 0.2743135988712311\n",
      "[Training Epoch 7] Batch 2180, Loss 0.27078914642333984\n",
      "[Training Epoch 7] Batch 2181, Loss 0.2783249318599701\n",
      "[Training Epoch 7] Batch 2182, Loss 0.25699758529663086\n",
      "[Training Epoch 7] Batch 2183, Loss 0.25550323724746704\n",
      "[Training Epoch 7] Batch 2184, Loss 0.23966577649116516\n",
      "[Training Epoch 7] Batch 2185, Loss 0.22136688232421875\n",
      "[Training Epoch 7] Batch 2186, Loss 0.2878386974334717\n",
      "[Training Epoch 7] Batch 2187, Loss 0.26042991876602173\n",
      "[Training Epoch 7] Batch 2188, Loss 0.2712295651435852\n",
      "[Training Epoch 7] Batch 2189, Loss 0.2573012113571167\n",
      "[Training Epoch 7] Batch 2190, Loss 0.2565044164657593\n",
      "[Training Epoch 7] Batch 2191, Loss 0.29123756289482117\n",
      "[Training Epoch 7] Batch 2192, Loss 0.2435852587223053\n",
      "[Training Epoch 7] Batch 2193, Loss 0.2681402862071991\n",
      "[Training Epoch 7] Batch 2194, Loss 0.2856229543685913\n",
      "[Training Epoch 7] Batch 2195, Loss 0.249808669090271\n",
      "[Training Epoch 7] Batch 2196, Loss 0.26182857155799866\n",
      "[Training Epoch 7] Batch 2197, Loss 0.23726630210876465\n",
      "[Training Epoch 7] Batch 2198, Loss 0.2591363191604614\n",
      "[Training Epoch 7] Batch 2199, Loss 0.25767451524734497\n",
      "[Training Epoch 7] Batch 2200, Loss 0.275398313999176\n",
      "[Training Epoch 7] Batch 2201, Loss 0.23362016677856445\n",
      "[Training Epoch 7] Batch 2202, Loss 0.2683310806751251\n",
      "[Training Epoch 7] Batch 2203, Loss 0.2300306111574173\n",
      "[Training Epoch 7] Batch 2204, Loss 0.26276904344558716\n",
      "[Training Epoch 7] Batch 2205, Loss 0.23965275287628174\n",
      "[Training Epoch 7] Batch 2206, Loss 0.27898678183555603\n",
      "[Training Epoch 7] Batch 2207, Loss 0.28468939661979675\n",
      "[Training Epoch 7] Batch 2208, Loss 0.29921263456344604\n",
      "[Training Epoch 7] Batch 2209, Loss 0.2588638663291931\n",
      "[Training Epoch 7] Batch 2210, Loss 0.25584664940834045\n",
      "[Training Epoch 7] Batch 2211, Loss 0.23273494839668274\n",
      "[Training Epoch 7] Batch 2212, Loss 0.24150937795639038\n",
      "[Training Epoch 7] Batch 2213, Loss 0.25098899006843567\n",
      "[Training Epoch 7] Batch 2214, Loss 0.241938054561615\n",
      "[Training Epoch 7] Batch 2215, Loss 0.24457944929599762\n",
      "[Training Epoch 7] Batch 2216, Loss 0.25306522846221924\n",
      "[Training Epoch 7] Batch 2217, Loss 0.25762730836868286\n",
      "[Training Epoch 7] Batch 2218, Loss 0.2508649230003357\n",
      "[Training Epoch 7] Batch 2219, Loss 0.2775239944458008\n",
      "[Training Epoch 7] Batch 2220, Loss 0.24116332828998566\n",
      "[Training Epoch 7] Batch 2221, Loss 0.2700394093990326\n",
      "[Training Epoch 7] Batch 2222, Loss 0.27234089374542236\n",
      "[Training Epoch 7] Batch 2223, Loss 0.2645232081413269\n",
      "[Training Epoch 7] Batch 2224, Loss 0.2445732057094574\n",
      "[Training Epoch 7] Batch 2225, Loss 0.23088926076889038\n",
      "[Training Epoch 7] Batch 2226, Loss 0.26895052194595337\n",
      "[Training Epoch 7] Batch 2227, Loss 0.24905607104301453\n",
      "[Training Epoch 7] Batch 2228, Loss 0.2890689969062805\n",
      "[Training Epoch 7] Batch 2229, Loss 0.24386411905288696\n",
      "[Training Epoch 7] Batch 2230, Loss 0.24801231920719147\n",
      "[Training Epoch 7] Batch 2231, Loss 0.2453317493200302\n",
      "[Training Epoch 7] Batch 2232, Loss 0.24273312091827393\n",
      "[Training Epoch 7] Batch 2233, Loss 0.27896174788475037\n",
      "[Training Epoch 7] Batch 2234, Loss 0.24528177082538605\n",
      "[Training Epoch 7] Batch 2235, Loss 0.262859046459198\n",
      "[Training Epoch 7] Batch 2236, Loss 0.25145989656448364\n",
      "[Training Epoch 7] Batch 2237, Loss 0.28602924942970276\n",
      "[Training Epoch 7] Batch 2238, Loss 0.2654810845851898\n",
      "[Training Epoch 7] Batch 2239, Loss 0.26948511600494385\n",
      "[Training Epoch 7] Batch 2240, Loss 0.2498723715543747\n",
      "[Training Epoch 7] Batch 2241, Loss 0.2824404239654541\n",
      "[Training Epoch 7] Batch 2242, Loss 0.28219079971313477\n",
      "[Training Epoch 7] Batch 2243, Loss 0.24988821148872375\n",
      "[Training Epoch 7] Batch 2244, Loss 0.25592973828315735\n",
      "[Training Epoch 7] Batch 2245, Loss 0.24630244076251984\n",
      "[Training Epoch 7] Batch 2246, Loss 0.24202756583690643\n",
      "[Training Epoch 7] Batch 2247, Loss 0.25347602367401123\n",
      "[Training Epoch 7] Batch 2248, Loss 0.2819458842277527\n",
      "[Training Epoch 7] Batch 2249, Loss 0.26286011934280396\n",
      "[Training Epoch 7] Batch 2250, Loss 0.24542391300201416\n",
      "[Training Epoch 7] Batch 2251, Loss 0.2501155734062195\n",
      "[Training Epoch 7] Batch 2252, Loss 0.2557145953178406\n",
      "[Training Epoch 7] Batch 2253, Loss 0.26578736305236816\n",
      "[Training Epoch 7] Batch 2254, Loss 0.24899059534072876\n",
      "[Training Epoch 7] Batch 2255, Loss 0.25220900774002075\n",
      "[Training Epoch 7] Batch 2256, Loss 0.26570552587509155\n",
      "[Training Epoch 7] Batch 2257, Loss 0.24902862310409546\n",
      "[Training Epoch 7] Batch 2258, Loss 0.27921634912490845\n",
      "[Training Epoch 7] Batch 2259, Loss 0.23964571952819824\n",
      "[Training Epoch 7] Batch 2260, Loss 0.2670114040374756\n",
      "[Training Epoch 7] Batch 2261, Loss 0.2342422902584076\n",
      "[Training Epoch 7] Batch 2262, Loss 0.2438538670539856\n",
      "[Training Epoch 7] Batch 2263, Loss 0.24388453364372253\n",
      "[Training Epoch 7] Batch 2264, Loss 0.2434283345937729\n",
      "[Training Epoch 7] Batch 2265, Loss 0.2610663175582886\n",
      "[Training Epoch 7] Batch 2266, Loss 0.26003792881965637\n",
      "[Training Epoch 7] Batch 2267, Loss 0.26117825508117676\n",
      "[Training Epoch 7] Batch 2268, Loss 0.25112974643707275\n",
      "[Training Epoch 7] Batch 2269, Loss 0.28090834617614746\n",
      "[Training Epoch 7] Batch 2270, Loss 0.2664431631565094\n",
      "[Training Epoch 7] Batch 2271, Loss 0.26016485691070557\n",
      "[Training Epoch 7] Batch 2272, Loss 0.2908567190170288\n",
      "[Training Epoch 7] Batch 2273, Loss 0.2660669684410095\n",
      "[Training Epoch 7] Batch 2274, Loss 0.26652026176452637\n",
      "[Training Epoch 7] Batch 2275, Loss 0.24933061003684998\n",
      "[Training Epoch 7] Batch 2276, Loss 0.2703564167022705\n",
      "[Training Epoch 7] Batch 2277, Loss 0.26369667053222656\n",
      "[Training Epoch 7] Batch 2278, Loss 0.27359840273857117\n",
      "[Training Epoch 7] Batch 2279, Loss 0.2487090826034546\n",
      "[Training Epoch 7] Batch 2280, Loss 0.26656365394592285\n",
      "[Training Epoch 7] Batch 2281, Loss 0.2480349838733673\n",
      "[Training Epoch 7] Batch 2282, Loss 0.2645573616027832\n",
      "[Training Epoch 7] Batch 2283, Loss 0.30334073305130005\n",
      "[Training Epoch 7] Batch 2284, Loss 0.268365740776062\n",
      "[Training Epoch 7] Batch 2285, Loss 0.2353564202785492\n",
      "[Training Epoch 7] Batch 2286, Loss 0.25054875016212463\n",
      "[Training Epoch 7] Batch 2287, Loss 0.2654988765716553\n",
      "[Training Epoch 7] Batch 2288, Loss 0.24449396133422852\n",
      "[Training Epoch 7] Batch 2289, Loss 0.2081427276134491\n",
      "[Training Epoch 7] Batch 2290, Loss 0.27749964594841003\n",
      "[Training Epoch 7] Batch 2291, Loss 0.23720791935920715\n",
      "[Training Epoch 7] Batch 2292, Loss 0.25267893075942993\n",
      "[Training Epoch 7] Batch 2293, Loss 0.2901023030281067\n",
      "[Training Epoch 7] Batch 2294, Loss 0.25396090745925903\n",
      "[Training Epoch 7] Batch 2295, Loss 0.2655503749847412\n",
      "[Training Epoch 7] Batch 2296, Loss 0.2608664035797119\n",
      "[Training Epoch 7] Batch 2297, Loss 0.2670552134513855\n",
      "[Training Epoch 7] Batch 2298, Loss 0.23258565366268158\n",
      "[Training Epoch 7] Batch 2299, Loss 0.23520290851593018\n",
      "[Training Epoch 7] Batch 2300, Loss 0.23566827178001404\n",
      "[Training Epoch 7] Batch 2301, Loss 0.25452977418899536\n",
      "[Training Epoch 7] Batch 2302, Loss 0.26013410091400146\n",
      "[Training Epoch 7] Batch 2303, Loss 0.2611040771007538\n",
      "[Training Epoch 7] Batch 2304, Loss 0.28686216473579407\n",
      "[Training Epoch 7] Batch 2305, Loss 0.2951960563659668\n",
      "[Training Epoch 7] Batch 2306, Loss 0.2442149519920349\n",
      "[Training Epoch 7] Batch 2307, Loss 0.24784669280052185\n",
      "[Training Epoch 7] Batch 2308, Loss 0.2644195258617401\n",
      "[Training Epoch 7] Batch 2309, Loss 0.23128077387809753\n",
      "[Training Epoch 7] Batch 2310, Loss 0.2229768931865692\n",
      "[Training Epoch 7] Batch 2311, Loss 0.23400065302848816\n",
      "[Training Epoch 7] Batch 2312, Loss 0.2553773820400238\n",
      "[Training Epoch 7] Batch 2313, Loss 0.2519027292728424\n",
      "[Training Epoch 7] Batch 2314, Loss 0.266826331615448\n",
      "[Training Epoch 7] Batch 2315, Loss 0.23952868580818176\n",
      "[Training Epoch 7] Batch 2316, Loss 0.25448864698410034\n",
      "[Training Epoch 7] Batch 2317, Loss 0.2500801682472229\n",
      "[Training Epoch 7] Batch 2318, Loss 0.27348530292510986\n",
      "[Training Epoch 7] Batch 2319, Loss 0.2575794458389282\n",
      "[Training Epoch 7] Batch 2320, Loss 0.24869132041931152\n",
      "[Training Epoch 7] Batch 2321, Loss 0.24724924564361572\n",
      "[Training Epoch 7] Batch 2322, Loss 0.2681453227996826\n",
      "[Training Epoch 7] Batch 2323, Loss 0.284016877412796\n",
      "[Training Epoch 7] Batch 2324, Loss 0.22165784239768982\n",
      "[Training Epoch 7] Batch 2325, Loss 0.24033263325691223\n",
      "[Training Epoch 7] Batch 2326, Loss 0.25047001242637634\n",
      "[Training Epoch 7] Batch 2327, Loss 0.23075079917907715\n",
      "[Training Epoch 7] Batch 2328, Loss 0.2691818177700043\n",
      "[Training Epoch 7] Batch 2329, Loss 0.2775527834892273\n",
      "[Training Epoch 7] Batch 2330, Loss 0.23090842366218567\n",
      "[Training Epoch 7] Batch 2331, Loss 0.27700281143188477\n",
      "[Training Epoch 7] Batch 2332, Loss 0.2308294177055359\n",
      "[Training Epoch 7] Batch 2333, Loss 0.2420596480369568\n",
      "[Training Epoch 7] Batch 2334, Loss 0.28939715027809143\n",
      "[Training Epoch 7] Batch 2335, Loss 0.22976958751678467\n",
      "[Training Epoch 7] Batch 2336, Loss 0.25111767649650574\n",
      "[Training Epoch 7] Batch 2337, Loss 0.2654167413711548\n",
      "[Training Epoch 7] Batch 2338, Loss 0.2650144398212433\n",
      "[Training Epoch 7] Batch 2339, Loss 0.2504023015499115\n",
      "[Training Epoch 7] Batch 2340, Loss 0.2776491641998291\n",
      "[Training Epoch 7] Batch 2341, Loss 0.2546449303627014\n",
      "[Training Epoch 7] Batch 2342, Loss 0.23481324315071106\n",
      "[Training Epoch 7] Batch 2343, Loss 0.26670557260513306\n",
      "[Training Epoch 7] Batch 2344, Loss 0.2649356424808502\n",
      "[Training Epoch 7] Batch 2345, Loss 0.21198777854442596\n",
      "[Training Epoch 7] Batch 2346, Loss 0.2723621129989624\n",
      "[Training Epoch 7] Batch 2347, Loss 0.2410968542098999\n",
      "[Training Epoch 7] Batch 2348, Loss 0.26628413796424866\n",
      "[Training Epoch 7] Batch 2349, Loss 0.23862986266613007\n",
      "[Training Epoch 7] Batch 2350, Loss 0.26324111223220825\n",
      "[Training Epoch 7] Batch 2351, Loss 0.2601466774940491\n",
      "[Training Epoch 7] Batch 2352, Loss 0.2737208902835846\n",
      "[Training Epoch 7] Batch 2353, Loss 0.2651064097881317\n",
      "[Training Epoch 7] Batch 2354, Loss 0.2583473324775696\n",
      "[Training Epoch 7] Batch 2355, Loss 0.27071869373321533\n",
      "[Training Epoch 7] Batch 2356, Loss 0.2759590148925781\n",
      "[Training Epoch 7] Batch 2357, Loss 0.263460636138916\n",
      "[Training Epoch 7] Batch 2358, Loss 0.2649191617965698\n",
      "[Training Epoch 7] Batch 2359, Loss 0.26517418026924133\n",
      "[Training Epoch 7] Batch 2360, Loss 0.2715875804424286\n",
      "[Training Epoch 7] Batch 2361, Loss 0.25462621450424194\n",
      "[Training Epoch 7] Batch 2362, Loss 0.24405086040496826\n",
      "[Training Epoch 7] Batch 2363, Loss 0.24420800805091858\n",
      "[Training Epoch 7] Batch 2364, Loss 0.29128098487854004\n",
      "[Training Epoch 7] Batch 2365, Loss 0.24483826756477356\n",
      "[Training Epoch 7] Batch 2366, Loss 0.24427971243858337\n",
      "[Training Epoch 7] Batch 2367, Loss 0.2687665522098541\n",
      "[Training Epoch 7] Batch 2368, Loss 0.2927023470401764\n",
      "[Training Epoch 7] Batch 2369, Loss 0.24953821301460266\n",
      "[Training Epoch 7] Batch 2370, Loss 0.2733314037322998\n",
      "[Training Epoch 7] Batch 2371, Loss 0.24505817890167236\n",
      "[Training Epoch 7] Batch 2372, Loss 0.2561734616756439\n",
      "[Training Epoch 7] Batch 2373, Loss 0.23253865540027618\n",
      "[Training Epoch 7] Batch 2374, Loss 0.23684363067150116\n",
      "[Training Epoch 7] Batch 2375, Loss 0.24326510727405548\n",
      "[Training Epoch 7] Batch 2376, Loss 0.23742789030075073\n",
      "[Training Epoch 7] Batch 2377, Loss 0.2577478885650635\n",
      "[Training Epoch 7] Batch 2378, Loss 0.24126873910427094\n",
      "[Training Epoch 7] Batch 2379, Loss 0.21781933307647705\n",
      "[Training Epoch 7] Batch 2380, Loss 0.2608588635921478\n",
      "[Training Epoch 7] Batch 2381, Loss 0.2365482747554779\n",
      "[Training Epoch 7] Batch 2382, Loss 0.2787969708442688\n",
      "[Training Epoch 7] Batch 2383, Loss 0.26047128438949585\n",
      "[Training Epoch 7] Batch 2384, Loss 0.262635737657547\n",
      "[Training Epoch 7] Batch 2385, Loss 0.23434288799762726\n",
      "[Training Epoch 7] Batch 2386, Loss 0.2657254934310913\n",
      "[Training Epoch 7] Batch 2387, Loss 0.2567484676837921\n",
      "[Training Epoch 7] Batch 2388, Loss 0.2584308981895447\n",
      "[Training Epoch 7] Batch 2389, Loss 0.2706105709075928\n",
      "[Training Epoch 7] Batch 2390, Loss 0.26486003398895264\n",
      "[Training Epoch 7] Batch 2391, Loss 0.2617226541042328\n",
      "[Training Epoch 7] Batch 2392, Loss 0.2548316717147827\n",
      "[Training Epoch 7] Batch 2393, Loss 0.2653356194496155\n",
      "[Training Epoch 7] Batch 2394, Loss 0.2568025588989258\n",
      "[Training Epoch 7] Batch 2395, Loss 0.24353118240833282\n",
      "[Training Epoch 7] Batch 2396, Loss 0.2501629590988159\n",
      "[Training Epoch 7] Batch 2397, Loss 0.24481201171875\n",
      "[Training Epoch 7] Batch 2398, Loss 0.2773491144180298\n",
      "[Training Epoch 7] Batch 2399, Loss 0.2707827091217041\n",
      "[Training Epoch 7] Batch 2400, Loss 0.24549804627895355\n",
      "[Training Epoch 7] Batch 2401, Loss 0.2575288414955139\n",
      "[Training Epoch 7] Batch 2402, Loss 0.2303268313407898\n",
      "[Training Epoch 7] Batch 2403, Loss 0.2686397433280945\n",
      "[Training Epoch 7] Batch 2404, Loss 0.25156155228614807\n",
      "[Training Epoch 7] Batch 2405, Loss 0.25146612524986267\n",
      "[Training Epoch 7] Batch 2406, Loss 0.23296959698200226\n",
      "[Training Epoch 7] Batch 2407, Loss 0.29169055819511414\n",
      "[Training Epoch 7] Batch 2408, Loss 0.24389979243278503\n",
      "[Training Epoch 7] Batch 2409, Loss 0.2493426501750946\n",
      "[Training Epoch 7] Batch 2410, Loss 0.28127622604370117\n",
      "[Training Epoch 7] Batch 2411, Loss 0.2606167197227478\n",
      "[Training Epoch 7] Batch 2412, Loss 0.24809101223945618\n",
      "[Training Epoch 7] Batch 2413, Loss 0.2648199200630188\n",
      "[Training Epoch 7] Batch 2414, Loss 0.2646997570991516\n",
      "[Training Epoch 7] Batch 2415, Loss 0.2896483540534973\n",
      "[Training Epoch 7] Batch 2416, Loss 0.2440873086452484\n",
      "[Training Epoch 7] Batch 2417, Loss 0.243860125541687\n",
      "[Training Epoch 7] Batch 2418, Loss 0.2581197917461395\n",
      "[Training Epoch 7] Batch 2419, Loss 0.28069785237312317\n",
      "[Training Epoch 7] Batch 2420, Loss 0.25602948665618896\n",
      "[Training Epoch 7] Batch 2421, Loss 0.23036572337150574\n",
      "[Training Epoch 7] Batch 2422, Loss 0.252620130777359\n",
      "[Training Epoch 7] Batch 2423, Loss 0.27239787578582764\n",
      "[Training Epoch 7] Batch 2424, Loss 0.2661605477333069\n",
      "[Training Epoch 7] Batch 2425, Loss 0.25745058059692383\n",
      "[Training Epoch 7] Batch 2426, Loss 0.27665048837661743\n",
      "[Training Epoch 7] Batch 2427, Loss 0.2519155740737915\n",
      "[Training Epoch 7] Batch 2428, Loss 0.2353457808494568\n",
      "[Training Epoch 7] Batch 2429, Loss 0.253814160823822\n",
      "[Training Epoch 7] Batch 2430, Loss 0.2384822517633438\n",
      "[Training Epoch 7] Batch 2431, Loss 0.25145506858825684\n",
      "[Training Epoch 7] Batch 2432, Loss 0.23727160692214966\n",
      "[Training Epoch 7] Batch 2433, Loss 0.23157450556755066\n",
      "[Training Epoch 7] Batch 2434, Loss 0.25617069005966187\n",
      "[Training Epoch 7] Batch 2435, Loss 0.2641863226890564\n",
      "[Training Epoch 7] Batch 2436, Loss 0.25608015060424805\n",
      "[Training Epoch 7] Batch 2437, Loss 0.27401095628738403\n",
      "[Training Epoch 7] Batch 2438, Loss 0.24928244948387146\n",
      "[Training Epoch 7] Batch 2439, Loss 0.27206045389175415\n",
      "[Training Epoch 7] Batch 2440, Loss 0.25540637969970703\n",
      "[Training Epoch 7] Batch 2441, Loss 0.2710145115852356\n",
      "[Training Epoch 7] Batch 2442, Loss 0.23570071160793304\n",
      "[Training Epoch 7] Batch 2443, Loss 0.24173271656036377\n",
      "[Training Epoch 7] Batch 2444, Loss 0.23509806394577026\n",
      "[Training Epoch 7] Batch 2445, Loss 0.2588038146495819\n",
      "[Training Epoch 7] Batch 2446, Loss 0.24939793348312378\n",
      "[Training Epoch 7] Batch 2447, Loss 0.2373710572719574\n",
      "[Training Epoch 7] Batch 2448, Loss 0.22456330060958862\n",
      "[Training Epoch 7] Batch 2449, Loss 0.2785664200782776\n",
      "[Training Epoch 7] Batch 2450, Loss 0.250686913728714\n",
      "[Training Epoch 7] Batch 2451, Loss 0.2544998824596405\n",
      "[Training Epoch 7] Batch 2452, Loss 0.2376440465450287\n",
      "[Training Epoch 7] Batch 2453, Loss 0.21825233101844788\n",
      "[Training Epoch 7] Batch 2454, Loss 0.27136072516441345\n",
      "[Training Epoch 7] Batch 2455, Loss 0.28085821866989136\n",
      "[Training Epoch 7] Batch 2456, Loss 0.25786250829696655\n",
      "[Training Epoch 7] Batch 2457, Loss 0.2874454855918884\n",
      "[Training Epoch 7] Batch 2458, Loss 0.23609685897827148\n",
      "[Training Epoch 7] Batch 2459, Loss 0.2646394371986389\n",
      "[Training Epoch 7] Batch 2460, Loss 0.26982471346855164\n",
      "[Training Epoch 7] Batch 2461, Loss 0.23805877566337585\n",
      "[Training Epoch 7] Batch 2462, Loss 0.26406604051589966\n",
      "[Training Epoch 7] Batch 2463, Loss 0.2460421621799469\n",
      "[Training Epoch 7] Batch 2464, Loss 0.2640540897846222\n",
      "[Training Epoch 7] Batch 2465, Loss 0.26644426584243774\n",
      "[Training Epoch 7] Batch 2466, Loss 0.28634965419769287\n",
      "[Training Epoch 7] Batch 2467, Loss 0.24804961681365967\n",
      "[Training Epoch 7] Batch 2468, Loss 0.25914525985717773\n",
      "[Training Epoch 7] Batch 2469, Loss 0.2684108018875122\n",
      "[Training Epoch 7] Batch 2470, Loss 0.2790079712867737\n",
      "[Training Epoch 7] Batch 2471, Loss 0.26779547333717346\n",
      "[Training Epoch 7] Batch 2472, Loss 0.26416584849357605\n",
      "[Training Epoch 7] Batch 2473, Loss 0.2520211338996887\n",
      "[Training Epoch 7] Batch 2474, Loss 0.28134486079216003\n",
      "[Training Epoch 7] Batch 2475, Loss 0.25332021713256836\n",
      "[Training Epoch 7] Batch 2476, Loss 0.2529105842113495\n",
      "[Training Epoch 7] Batch 2477, Loss 0.2965543568134308\n",
      "[Training Epoch 7] Batch 2478, Loss 0.2802897095680237\n",
      "[Training Epoch 7] Batch 2479, Loss 0.2587141990661621\n",
      "[Training Epoch 7] Batch 2480, Loss 0.2685677409172058\n",
      "[Training Epoch 7] Batch 2481, Loss 0.2761152982711792\n",
      "[Training Epoch 7] Batch 2482, Loss 0.23235227167606354\n",
      "[Training Epoch 7] Batch 2483, Loss 0.24430134892463684\n",
      "[Training Epoch 7] Batch 2484, Loss 0.2662440538406372\n",
      "[Training Epoch 7] Batch 2485, Loss 0.2492336630821228\n",
      "[Training Epoch 7] Batch 2486, Loss 0.24353322386741638\n",
      "[Training Epoch 7] Batch 2487, Loss 0.242374986410141\n",
      "[Training Epoch 7] Batch 2488, Loss 0.27727705240249634\n",
      "[Training Epoch 7] Batch 2489, Loss 0.2535758316516876\n",
      "[Training Epoch 7] Batch 2490, Loss 0.28515827655792236\n",
      "[Training Epoch 7] Batch 2491, Loss 0.23795485496520996\n",
      "[Training Epoch 7] Batch 2492, Loss 0.3059900403022766\n",
      "[Training Epoch 7] Batch 2493, Loss 0.22569233179092407\n",
      "[Training Epoch 7] Batch 2494, Loss 0.251932829618454\n",
      "[Training Epoch 7] Batch 2495, Loss 0.2546098828315735\n",
      "[Training Epoch 7] Batch 2496, Loss 0.27783048152923584\n",
      "[Training Epoch 7] Batch 2497, Loss 0.25195735692977905\n",
      "[Training Epoch 7] Batch 2498, Loss 0.2622440457344055\n",
      "[Training Epoch 7] Batch 2499, Loss 0.24936461448669434\n",
      "[Training Epoch 7] Batch 2500, Loss 0.23204191029071808\n",
      "[Training Epoch 7] Batch 2501, Loss 0.2793964743614197\n",
      "[Training Epoch 7] Batch 2502, Loss 0.2620229125022888\n",
      "[Training Epoch 7] Batch 2503, Loss 0.2758755087852478\n",
      "[Training Epoch 7] Batch 2504, Loss 0.2505442500114441\n",
      "[Training Epoch 7] Batch 2505, Loss 0.25251418352127075\n",
      "[Training Epoch 7] Batch 2506, Loss 0.25509384274482727\n",
      "[Training Epoch 7] Batch 2507, Loss 0.24925871193408966\n",
      "[Training Epoch 7] Batch 2508, Loss 0.27314531803131104\n",
      "[Training Epoch 7] Batch 2509, Loss 0.24655143916606903\n",
      "[Training Epoch 7] Batch 2510, Loss 0.2535651922225952\n",
      "[Training Epoch 7] Batch 2511, Loss 0.2589893937110901\n",
      "[Training Epoch 7] Batch 2512, Loss 0.26791101694107056\n",
      "[Training Epoch 7] Batch 2513, Loss 0.23668640851974487\n",
      "[Training Epoch 7] Batch 2514, Loss 0.2749839425086975\n",
      "[Training Epoch 7] Batch 2515, Loss 0.2601858675479889\n",
      "[Training Epoch 7] Batch 2516, Loss 0.24073415994644165\n",
      "[Training Epoch 7] Batch 2517, Loss 0.2643926739692688\n",
      "[Training Epoch 7] Batch 2518, Loss 0.25094276666641235\n",
      "[Training Epoch 7] Batch 2519, Loss 0.2560872435569763\n",
      "[Training Epoch 7] Batch 2520, Loss 0.26135510206222534\n",
      "[Training Epoch 7] Batch 2521, Loss 0.25681453943252563\n",
      "[Training Epoch 7] Batch 2522, Loss 0.23653951287269592\n",
      "[Training Epoch 7] Batch 2523, Loss 0.25251880288124084\n",
      "[Training Epoch 7] Batch 2524, Loss 0.2529563903808594\n",
      "[Training Epoch 7] Batch 2525, Loss 0.25681236386299133\n",
      "[Training Epoch 7] Batch 2526, Loss 0.2733892798423767\n",
      "[Training Epoch 7] Batch 2527, Loss 0.2497272789478302\n",
      "[Training Epoch 7] Batch 2528, Loss 0.2566494345664978\n",
      "[Training Epoch 7] Batch 2529, Loss 0.24134179949760437\n",
      "[Training Epoch 7] Batch 2530, Loss 0.2583090662956238\n",
      "[Training Epoch 7] Batch 2531, Loss 0.28468528389930725\n",
      "[Training Epoch 7] Batch 2532, Loss 0.28668540716171265\n",
      "[Training Epoch 7] Batch 2533, Loss 0.2796698808670044\n",
      "[Training Epoch 7] Batch 2534, Loss 0.25602108240127563\n",
      "[Training Epoch 7] Batch 2535, Loss 0.24487899243831635\n",
      "[Training Epoch 7] Batch 2536, Loss 0.25723540782928467\n",
      "[Training Epoch 7] Batch 2537, Loss 0.26895231008529663\n",
      "[Training Epoch 7] Batch 2538, Loss 0.2430747002363205\n",
      "[Training Epoch 7] Batch 2539, Loss 0.2402040958404541\n",
      "[Training Epoch 7] Batch 2540, Loss 0.2574816346168518\n",
      "[Training Epoch 7] Batch 2541, Loss 0.24998754262924194\n",
      "[Training Epoch 7] Batch 2542, Loss 0.29811638593673706\n",
      "[Training Epoch 7] Batch 2543, Loss 0.24290607869625092\n",
      "[Training Epoch 7] Batch 2544, Loss 0.269424170255661\n",
      "[Training Epoch 7] Batch 2545, Loss 0.2462669014930725\n",
      "[Training Epoch 7] Batch 2546, Loss 0.27374404668807983\n",
      "[Training Epoch 7] Batch 2547, Loss 0.2901192903518677\n",
      "[Training Epoch 7] Batch 2548, Loss 0.25857245922088623\n",
      "[Training Epoch 7] Batch 2549, Loss 0.25745052099227905\n",
      "[Training Epoch 7] Batch 2550, Loss 0.2765786647796631\n",
      "[Training Epoch 7] Batch 2551, Loss 0.2563345432281494\n",
      "[Training Epoch 7] Batch 2552, Loss 0.2500777244567871\n",
      "[Training Epoch 7] Batch 2553, Loss 0.26296481490135193\n",
      "[Training Epoch 7] Batch 2554, Loss 0.27525103092193604\n",
      "[Training Epoch 7] Batch 2555, Loss 0.2559584379196167\n",
      "[Training Epoch 7] Batch 2556, Loss 0.2631281912326813\n",
      "[Training Epoch 7] Batch 2557, Loss 0.2346663922071457\n",
      "[Training Epoch 7] Batch 2558, Loss 0.2709629535675049\n",
      "[Training Epoch 7] Batch 2559, Loss 0.27568942308425903\n",
      "[Training Epoch 7] Batch 2560, Loss 0.2364669144153595\n",
      "[Training Epoch 7] Batch 2561, Loss 0.245782732963562\n",
      "[Training Epoch 7] Batch 2562, Loss 0.2773894965648651\n",
      "[Training Epoch 7] Batch 2563, Loss 0.26517409086227417\n",
      "[Training Epoch 7] Batch 2564, Loss 0.26845771074295044\n",
      "[Training Epoch 7] Batch 2565, Loss 0.2632693648338318\n",
      "[Training Epoch 7] Batch 2566, Loss 0.2719576954841614\n",
      "[Training Epoch 7] Batch 2567, Loss 0.25035712122917175\n",
      "[Training Epoch 7] Batch 2568, Loss 0.2768835425376892\n",
      "[Training Epoch 7] Batch 2569, Loss 0.27107709646224976\n",
      "[Training Epoch 7] Batch 2570, Loss 0.2654770612716675\n",
      "[Training Epoch 7] Batch 2571, Loss 0.2596958875656128\n",
      "[Training Epoch 7] Batch 2572, Loss 0.2375197410583496\n",
      "[Training Epoch 7] Batch 2573, Loss 0.25090983510017395\n",
      "[Training Epoch 7] Batch 2574, Loss 0.2631012499332428\n",
      "[Training Epoch 7] Batch 2575, Loss 0.2753453552722931\n",
      "[Training Epoch 7] Batch 2576, Loss 0.29813146591186523\n",
      "[Training Epoch 7] Batch 2577, Loss 0.2691742777824402\n",
      "[Training Epoch 7] Batch 2578, Loss 0.23699457943439484\n",
      "[Training Epoch 7] Batch 2579, Loss 0.2653108835220337\n",
      "[Training Epoch 7] Batch 2580, Loss 0.24988438189029694\n",
      "[Training Epoch 7] Batch 2581, Loss 0.26059967279434204\n",
      "[Training Epoch 7] Batch 2582, Loss 0.26206815242767334\n",
      "[Training Epoch 7] Batch 2583, Loss 0.29801100492477417\n",
      "[Training Epoch 7] Batch 2584, Loss 0.25792941451072693\n",
      "[Training Epoch 7] Batch 2585, Loss 0.2979975938796997\n",
      "[Training Epoch 7] Batch 2586, Loss 0.2629174590110779\n",
      "[Training Epoch 7] Batch 2587, Loss 0.2716812491416931\n",
      "[Training Epoch 7] Batch 2588, Loss 0.25597986578941345\n",
      "[Training Epoch 7] Batch 2589, Loss 0.2623289227485657\n",
      "[Training Epoch 7] Batch 2590, Loss 0.25539809465408325\n",
      "[Training Epoch 7] Batch 2591, Loss 0.26030489802360535\n",
      "[Training Epoch 7] Batch 2592, Loss 0.26974600553512573\n",
      "[Training Epoch 7] Batch 2593, Loss 0.23845714330673218\n",
      "[Training Epoch 7] Batch 2594, Loss 0.23877960443496704\n",
      "[Training Epoch 7] Batch 2595, Loss 0.24597220122814178\n",
      "[Training Epoch 7] Batch 2596, Loss 0.25148284435272217\n",
      "[Training Epoch 7] Batch 2597, Loss 0.27601125836372375\n",
      "[Training Epoch 7] Batch 2598, Loss 0.2886260747909546\n",
      "[Training Epoch 7] Batch 2599, Loss 0.28473812341690063\n",
      "[Training Epoch 7] Batch 2600, Loss 0.23916402459144592\n",
      "[Training Epoch 7] Batch 2601, Loss 0.24894723296165466\n",
      "[Training Epoch 7] Batch 2602, Loss 0.2557414174079895\n",
      "[Training Epoch 7] Batch 2603, Loss 0.25114861130714417\n",
      "[Training Epoch 7] Batch 2604, Loss 0.24771301448345184\n",
      "[Training Epoch 7] Batch 2605, Loss 0.26154616475105286\n",
      "[Training Epoch 7] Batch 2606, Loss 0.25453653931617737\n",
      "[Training Epoch 7] Batch 2607, Loss 0.2539655268192291\n",
      "[Training Epoch 7] Batch 2608, Loss 0.2501091957092285\n",
      "[Training Epoch 7] Batch 2609, Loss 0.25360140204429626\n",
      "[Training Epoch 7] Batch 2610, Loss 0.2563922703266144\n",
      "[Training Epoch 7] Batch 2611, Loss 0.2618449926376343\n",
      "[Training Epoch 7] Batch 2612, Loss 0.2814289629459381\n",
      "[Training Epoch 7] Batch 2613, Loss 0.22850669920444489\n",
      "[Training Epoch 7] Batch 2614, Loss 0.2608131766319275\n",
      "[Training Epoch 7] Batch 2615, Loss 0.2638261318206787\n",
      "[Training Epoch 7] Batch 2616, Loss 0.2581970691680908\n",
      "[Training Epoch 7] Batch 2617, Loss 0.28216928243637085\n",
      "[Training Epoch 7] Batch 2618, Loss 0.2455860674381256\n",
      "[Training Epoch 7] Batch 2619, Loss 0.28045397996902466\n",
      "[Training Epoch 7] Batch 2620, Loss 0.25292736291885376\n",
      "[Training Epoch 7] Batch 2621, Loss 0.26641565561294556\n",
      "[Training Epoch 7] Batch 2622, Loss 0.277641624212265\n",
      "[Training Epoch 7] Batch 2623, Loss 0.2436107099056244\n",
      "[Training Epoch 7] Batch 2624, Loss 0.22927744686603546\n",
      "[Training Epoch 7] Batch 2625, Loss 0.24234506487846375\n",
      "[Training Epoch 7] Batch 2626, Loss 0.27423295378685\n",
      "[Training Epoch 7] Batch 2627, Loss 0.2616618871688843\n",
      "[Training Epoch 7] Batch 2628, Loss 0.25380539894104004\n",
      "[Training Epoch 7] Batch 2629, Loss 0.26491600275039673\n",
      "[Training Epoch 7] Batch 2630, Loss 0.3080630898475647\n",
      "[Training Epoch 7] Batch 2631, Loss 0.28592410683631897\n",
      "[Training Epoch 7] Batch 2632, Loss 0.24337543547153473\n",
      "[Training Epoch 7] Batch 2633, Loss 0.2754270136356354\n",
      "[Training Epoch 7] Batch 2634, Loss 0.2406606674194336\n",
      "[Training Epoch 7] Batch 2635, Loss 0.2807340621948242\n",
      "[Training Epoch 7] Batch 2636, Loss 0.2669108211994171\n",
      "[Training Epoch 7] Batch 2637, Loss 0.2394881397485733\n",
      "[Training Epoch 7] Batch 2638, Loss 0.2537752091884613\n",
      "[Training Epoch 7] Batch 2639, Loss 0.2702447772026062\n",
      "[Training Epoch 7] Batch 2640, Loss 0.2542133331298828\n",
      "[Training Epoch 7] Batch 2641, Loss 0.2732735872268677\n",
      "[Training Epoch 7] Batch 2642, Loss 0.2577453553676605\n",
      "[Training Epoch 7] Batch 2643, Loss 0.2661217451095581\n",
      "[Training Epoch 7] Batch 2644, Loss 0.2605782151222229\n",
      "[Training Epoch 7] Batch 2645, Loss 0.2447977364063263\n",
      "[Training Epoch 7] Batch 2646, Loss 0.25711527466773987\n",
      "[Training Epoch 7] Batch 2647, Loss 0.2454969584941864\n",
      "[Training Epoch 7] Batch 2648, Loss 0.27760013937950134\n",
      "[Training Epoch 7] Batch 2649, Loss 0.2644714117050171\n",
      "[Training Epoch 7] Batch 2650, Loss 0.2515619397163391\n",
      "[Training Epoch 7] Batch 2651, Loss 0.2573291063308716\n",
      "[Training Epoch 7] Batch 2652, Loss 0.23737654089927673\n",
      "[Training Epoch 7] Batch 2653, Loss 0.27845144271850586\n",
      "[Training Epoch 7] Batch 2654, Loss 0.235988050699234\n",
      "[Training Epoch 7] Batch 2655, Loss 0.2481827288866043\n",
      "[Training Epoch 7] Batch 2656, Loss 0.23994359374046326\n",
      "[Training Epoch 7] Batch 2657, Loss 0.24260486662387848\n",
      "[Training Epoch 7] Batch 2658, Loss 0.2726942002773285\n",
      "[Training Epoch 7] Batch 2659, Loss 0.21527574956417084\n",
      "[Training Epoch 7] Batch 2660, Loss 0.25226134061813354\n",
      "[Training Epoch 7] Batch 2661, Loss 0.2564411759376526\n",
      "[Training Epoch 7] Batch 2662, Loss 0.2593158483505249\n",
      "[Training Epoch 7] Batch 2663, Loss 0.2646518051624298\n",
      "[Training Epoch 7] Batch 2664, Loss 0.2499600648880005\n",
      "[Training Epoch 7] Batch 2665, Loss 0.25635749101638794\n",
      "[Training Epoch 7] Batch 2666, Loss 0.2393202781677246\n",
      "[Training Epoch 7] Batch 2667, Loss 0.27904778718948364\n",
      "[Training Epoch 7] Batch 2668, Loss 0.2526929974555969\n",
      "[Training Epoch 7] Batch 2669, Loss 0.2581976652145386\n",
      "[Training Epoch 7] Batch 2670, Loss 0.25960394740104675\n",
      "[Training Epoch 7] Batch 2671, Loss 0.23435096442699432\n",
      "[Training Epoch 7] Batch 2672, Loss 0.25778523087501526\n",
      "[Training Epoch 7] Batch 2673, Loss 0.2687462270259857\n",
      "[Training Epoch 7] Batch 2674, Loss 0.28360801935195923\n",
      "[Training Epoch 7] Batch 2675, Loss 0.2688467502593994\n",
      "[Training Epoch 7] Batch 2676, Loss 0.3123740255832672\n",
      "[Training Epoch 7] Batch 2677, Loss 0.2686428427696228\n",
      "[Training Epoch 7] Batch 2678, Loss 0.29947900772094727\n",
      "[Training Epoch 7] Batch 2679, Loss 0.257417231798172\n",
      "[Training Epoch 7] Batch 2680, Loss 0.23965665698051453\n",
      "[Training Epoch 7] Batch 2681, Loss 0.24012461304664612\n",
      "[Training Epoch 7] Batch 2682, Loss 0.22770553827285767\n",
      "[Training Epoch 7] Batch 2683, Loss 0.26736781001091003\n",
      "[Training Epoch 7] Batch 2684, Loss 0.22440215945243835\n",
      "[Training Epoch 7] Batch 2685, Loss 0.2528466582298279\n",
      "[Training Epoch 7] Batch 2686, Loss 0.26338547468185425\n",
      "[Training Epoch 7] Batch 2687, Loss 0.2723633944988251\n",
      "[Training Epoch 7] Batch 2688, Loss 0.2875789403915405\n",
      "[Training Epoch 7] Batch 2689, Loss 0.22451220452785492\n",
      "[Training Epoch 7] Batch 2690, Loss 0.247168630361557\n",
      "[Training Epoch 7] Batch 2691, Loss 0.24989870190620422\n",
      "[Training Epoch 7] Batch 2692, Loss 0.2631952166557312\n",
      "[Training Epoch 7] Batch 2693, Loss 0.24639460444450378\n",
      "[Training Epoch 7] Batch 2694, Loss 0.23747418820858002\n",
      "[Training Epoch 7] Batch 2695, Loss 0.22897206246852875\n",
      "[Training Epoch 7] Batch 2696, Loss 0.2501577138900757\n",
      "[Training Epoch 7] Batch 2697, Loss 0.2636476755142212\n",
      "[Training Epoch 7] Batch 2698, Loss 0.2558954656124115\n",
      "[Training Epoch 7] Batch 2699, Loss 0.24226416647434235\n",
      "[Training Epoch 7] Batch 2700, Loss 0.2536521553993225\n",
      "[Training Epoch 7] Batch 2701, Loss 0.23806986212730408\n",
      "[Training Epoch 7] Batch 2702, Loss 0.22683659195899963\n",
      "[Training Epoch 7] Batch 2703, Loss 0.2484164834022522\n",
      "[Training Epoch 7] Batch 2704, Loss 0.24915769696235657\n",
      "[Training Epoch 7] Batch 2705, Loss 0.28706562519073486\n",
      "[Training Epoch 7] Batch 2706, Loss 0.25290989875793457\n",
      "[Training Epoch 7] Batch 2707, Loss 0.24316145479679108\n",
      "[Training Epoch 7] Batch 2708, Loss 0.23996412754058838\n",
      "[Training Epoch 7] Batch 2709, Loss 0.26938843727111816\n",
      "[Training Epoch 7] Batch 2710, Loss 0.2509557008743286\n",
      "[Training Epoch 7] Batch 2711, Loss 0.24762457609176636\n",
      "[Training Epoch 7] Batch 2712, Loss 0.2558116018772125\n",
      "[Training Epoch 7] Batch 2713, Loss 0.2719447910785675\n",
      "[Training Epoch 7] Batch 2714, Loss 0.25230672955513\n",
      "[Training Epoch 7] Batch 2715, Loss 0.2393147051334381\n",
      "[Training Epoch 7] Batch 2716, Loss 0.2649376392364502\n",
      "[Training Epoch 7] Batch 2717, Loss 0.27470266819000244\n",
      "[Training Epoch 7] Batch 2718, Loss 0.28930407762527466\n",
      "[Training Epoch 7] Batch 2719, Loss 0.24201369285583496\n",
      "[Training Epoch 7] Batch 2720, Loss 0.27921393513679504\n",
      "[Training Epoch 7] Batch 2721, Loss 0.2690929174423218\n",
      "[Training Epoch 7] Batch 2722, Loss 0.2593567371368408\n",
      "[Training Epoch 7] Batch 2723, Loss 0.2582077980041504\n",
      "[Training Epoch 7] Batch 2724, Loss 0.266068696975708\n",
      "[Training Epoch 7] Batch 2725, Loss 0.2655671238899231\n",
      "[Training Epoch 7] Batch 2726, Loss 0.2291373908519745\n",
      "[Training Epoch 7] Batch 2727, Loss 0.24854567646980286\n",
      "[Training Epoch 7] Batch 2728, Loss 0.24082836508750916\n",
      "[Training Epoch 7] Batch 2729, Loss 0.23379111289978027\n",
      "[Training Epoch 7] Batch 2730, Loss 0.2641913890838623\n",
      "[Training Epoch 7] Batch 2731, Loss 0.2412354052066803\n",
      "[Training Epoch 7] Batch 2732, Loss 0.2378704845905304\n",
      "[Training Epoch 7] Batch 2733, Loss 0.25740593671798706\n",
      "[Training Epoch 7] Batch 2734, Loss 0.25694626569747925\n",
      "[Training Epoch 7] Batch 2735, Loss 0.23391957581043243\n",
      "[Training Epoch 7] Batch 2736, Loss 0.2736457884311676\n",
      "[Training Epoch 7] Batch 2737, Loss 0.2560848593711853\n",
      "[Training Epoch 7] Batch 2738, Loss 0.2569256126880646\n",
      "[Training Epoch 7] Batch 2739, Loss 0.28222721815109253\n",
      "[Training Epoch 7] Batch 2740, Loss 0.2555343508720398\n",
      "[Training Epoch 7] Batch 2741, Loss 0.26533010601997375\n",
      "[Training Epoch 7] Batch 2742, Loss 0.28470197319984436\n",
      "[Training Epoch 7] Batch 2743, Loss 0.2342512309551239\n",
      "[Training Epoch 7] Batch 2744, Loss 0.2507028579711914\n",
      "[Training Epoch 7] Batch 2745, Loss 0.26097196340560913\n",
      "[Training Epoch 7] Batch 2746, Loss 0.24470803141593933\n",
      "[Training Epoch 7] Batch 2747, Loss 0.2739221453666687\n",
      "[Training Epoch 7] Batch 2748, Loss 0.25696104764938354\n",
      "[Training Epoch 7] Batch 2749, Loss 0.2418912649154663\n",
      "[Training Epoch 7] Batch 2750, Loss 0.24916258454322815\n",
      "[Training Epoch 7] Batch 2751, Loss 0.24495196342468262\n",
      "[Training Epoch 7] Batch 2752, Loss 0.29715496301651\n",
      "[Training Epoch 7] Batch 2753, Loss 0.27452224493026733\n",
      "[Training Epoch 7] Batch 2754, Loss 0.25798138976097107\n",
      "[Training Epoch 7] Batch 2755, Loss 0.2438666820526123\n",
      "[Training Epoch 7] Batch 2756, Loss 0.22038534283638\n",
      "[Training Epoch 7] Batch 2757, Loss 0.2917931377887726\n",
      "[Training Epoch 7] Batch 2758, Loss 0.24015900492668152\n",
      "[Training Epoch 7] Batch 2759, Loss 0.24314823746681213\n",
      "[Training Epoch 7] Batch 2760, Loss 0.26432690024375916\n",
      "[Training Epoch 7] Batch 2761, Loss 0.2406049221754074\n",
      "[Training Epoch 7] Batch 2762, Loss 0.23433363437652588\n",
      "[Training Epoch 7] Batch 2763, Loss 0.2725166380405426\n",
      "[Training Epoch 7] Batch 2764, Loss 0.24896562099456787\n",
      "[Training Epoch 7] Batch 2765, Loss 0.2712344527244568\n",
      "[Training Epoch 7] Batch 2766, Loss 0.2555874288082123\n",
      "[Training Epoch 7] Batch 2767, Loss 0.2658839821815491\n",
      "[Training Epoch 7] Batch 2768, Loss 0.28470170497894287\n",
      "[Training Epoch 7] Batch 2769, Loss 0.24841785430908203\n",
      "[Training Epoch 7] Batch 2770, Loss 0.2313186526298523\n",
      "[Training Epoch 7] Batch 2771, Loss 0.2588156461715698\n",
      "[Training Epoch 7] Batch 2772, Loss 0.24433842301368713\n",
      "[Training Epoch 7] Batch 2773, Loss 0.23807674646377563\n",
      "[Training Epoch 7] Batch 2774, Loss 0.2356763482093811\n",
      "[Training Epoch 7] Batch 2775, Loss 0.26991406083106995\n",
      "[Training Epoch 7] Batch 2776, Loss 0.27377554774284363\n",
      "[Training Epoch 7] Batch 2777, Loss 0.279624879360199\n",
      "[Training Epoch 7] Batch 2778, Loss 0.25749483704566956\n",
      "[Training Epoch 7] Batch 2779, Loss 0.27655524015426636\n",
      "[Training Epoch 7] Batch 2780, Loss 0.2608720362186432\n",
      "[Training Epoch 7] Batch 2781, Loss 0.2514936327934265\n",
      "[Training Epoch 7] Batch 2782, Loss 0.24418243765830994\n",
      "[Training Epoch 7] Batch 2783, Loss 0.2436760812997818\n",
      "[Training Epoch 7] Batch 2784, Loss 0.277601033449173\n",
      "[Training Epoch 7] Batch 2785, Loss 0.30914491415023804\n",
      "[Training Epoch 7] Batch 2786, Loss 0.2753984332084656\n",
      "[Training Epoch 7] Batch 2787, Loss 0.25433915853500366\n",
      "[Training Epoch 7] Batch 2788, Loss 0.25251084566116333\n",
      "[Training Epoch 7] Batch 2789, Loss 0.24313408136367798\n",
      "[Training Epoch 7] Batch 2790, Loss 0.2535223066806793\n",
      "[Training Epoch 7] Batch 2791, Loss 0.25459790229797363\n",
      "[Training Epoch 7] Batch 2792, Loss 0.2564460039138794\n",
      "[Training Epoch 7] Batch 2793, Loss 0.2974711060523987\n",
      "[Training Epoch 7] Batch 2794, Loss 0.2601009011268616\n",
      "[Training Epoch 7] Batch 2795, Loss 0.22264817357063293\n",
      "[Training Epoch 7] Batch 2796, Loss 0.28547877073287964\n",
      "[Training Epoch 7] Batch 2797, Loss 0.27627477049827576\n",
      "[Training Epoch 7] Batch 2798, Loss 0.26850584149360657\n",
      "[Training Epoch 7] Batch 2799, Loss 0.24213449656963348\n",
      "[Training Epoch 7] Batch 2800, Loss 0.2654271721839905\n",
      "[Training Epoch 7] Batch 2801, Loss 0.261272668838501\n",
      "[Training Epoch 7] Batch 2802, Loss 0.23542934656143188\n",
      "[Training Epoch 7] Batch 2803, Loss 0.2526727318763733\n",
      "[Training Epoch 7] Batch 2804, Loss 0.24141573905944824\n",
      "[Training Epoch 7] Batch 2805, Loss 0.2566502094268799\n",
      "[Training Epoch 7] Batch 2806, Loss 0.2709684371948242\n",
      "[Training Epoch 7] Batch 2807, Loss 0.25845101475715637\n",
      "[Training Epoch 7] Batch 2808, Loss 0.26037344336509705\n",
      "[Training Epoch 7] Batch 2809, Loss 0.27288681268692017\n",
      "[Training Epoch 7] Batch 2810, Loss 0.24846170842647552\n",
      "[Training Epoch 7] Batch 2811, Loss 0.26980939507484436\n",
      "[Training Epoch 7] Batch 2812, Loss 0.2543184757232666\n",
      "[Training Epoch 7] Batch 2813, Loss 0.23701301217079163\n",
      "[Training Epoch 7] Batch 2814, Loss 0.2612268328666687\n",
      "[Training Epoch 7] Batch 2815, Loss 0.2611733376979828\n",
      "[Training Epoch 7] Batch 2816, Loss 0.28155389428138733\n",
      "[Training Epoch 7] Batch 2817, Loss 0.2507472634315491\n",
      "[Training Epoch 7] Batch 2818, Loss 0.24568963050842285\n",
      "[Training Epoch 7] Batch 2819, Loss 0.2704680860042572\n",
      "[Training Epoch 7] Batch 2820, Loss 0.24974463880062103\n",
      "[Training Epoch 7] Batch 2821, Loss 0.24926996231079102\n",
      "[Training Epoch 7] Batch 2822, Loss 0.24546174705028534\n",
      "[Training Epoch 7] Batch 2823, Loss 0.26229459047317505\n",
      "[Training Epoch 7] Batch 2824, Loss 0.2581079602241516\n",
      "[Training Epoch 7] Batch 2825, Loss 0.28612983226776123\n",
      "[Training Epoch 7] Batch 2826, Loss 0.2686645984649658\n",
      "[Training Epoch 7] Batch 2827, Loss 0.26075825095176697\n",
      "[Training Epoch 7] Batch 2828, Loss 0.25902310013771057\n",
      "[Training Epoch 7] Batch 2829, Loss 0.2721402645111084\n",
      "[Training Epoch 7] Batch 2830, Loss 0.254393994808197\n",
      "[Training Epoch 7] Batch 2831, Loss 0.24634277820587158\n",
      "[Training Epoch 7] Batch 2832, Loss 0.25068822503089905\n",
      "[Training Epoch 7] Batch 2833, Loss 0.2873111963272095\n",
      "[Training Epoch 7] Batch 2834, Loss 0.25504350662231445\n",
      "[Training Epoch 7] Batch 2835, Loss 0.2715880274772644\n",
      "[Training Epoch 7] Batch 2836, Loss 0.2705337405204773\n",
      "[Training Epoch 7] Batch 2837, Loss 0.25315922498703003\n",
      "[Training Epoch 7] Batch 2838, Loss 0.2481311559677124\n",
      "[Training Epoch 7] Batch 2839, Loss 0.2615507245063782\n",
      "[Training Epoch 7] Batch 2840, Loss 0.26947396993637085\n",
      "[Training Epoch 7] Batch 2841, Loss 0.29048147797584534\n",
      "[Training Epoch 7] Batch 2842, Loss 0.30116569995880127\n",
      "[Training Epoch 7] Batch 2843, Loss 0.25748199224472046\n",
      "[Training Epoch 7] Batch 2844, Loss 0.2600221633911133\n",
      "[Training Epoch 7] Batch 2845, Loss 0.26589903235435486\n",
      "[Training Epoch 7] Batch 2846, Loss 0.22975397109985352\n",
      "[Training Epoch 7] Batch 2847, Loss 0.2536429166793823\n",
      "[Training Epoch 7] Batch 2848, Loss 0.2447843849658966\n",
      "[Training Epoch 7] Batch 2849, Loss 0.24938876926898956\n",
      "[Training Epoch 7] Batch 2850, Loss 0.25260308384895325\n",
      "[Training Epoch 7] Batch 2851, Loss 0.23235614597797394\n",
      "[Training Epoch 7] Batch 2852, Loss 0.25908127427101135\n",
      "[Training Epoch 7] Batch 2853, Loss 0.2566250264644623\n",
      "[Training Epoch 7] Batch 2854, Loss 0.2562921941280365\n",
      "[Training Epoch 7] Batch 2855, Loss 0.25499701499938965\n",
      "[Training Epoch 7] Batch 2856, Loss 0.2606718838214874\n",
      "[Training Epoch 7] Batch 2857, Loss 0.27987349033355713\n",
      "[Training Epoch 7] Batch 2858, Loss 0.2219846248626709\n",
      "[Training Epoch 7] Batch 2859, Loss 0.23718877136707306\n",
      "[Training Epoch 7] Batch 2860, Loss 0.299111545085907\n",
      "[Training Epoch 7] Batch 2861, Loss 0.2524416148662567\n",
      "[Training Epoch 7] Batch 2862, Loss 0.2596611976623535\n",
      "[Training Epoch 7] Batch 2863, Loss 0.2591402530670166\n",
      "[Training Epoch 7] Batch 2864, Loss 0.26729437708854675\n",
      "[Training Epoch 7] Batch 2865, Loss 0.26323604583740234\n",
      "[Training Epoch 7] Batch 2866, Loss 0.26881688833236694\n",
      "[Training Epoch 7] Batch 2867, Loss 0.2623942494392395\n",
      "[Training Epoch 7] Batch 2868, Loss 0.23140114545822144\n",
      "[Training Epoch 7] Batch 2869, Loss 0.24558690190315247\n",
      "[Training Epoch 7] Batch 2870, Loss 0.2522892653942108\n",
      "[Training Epoch 7] Batch 2871, Loss 0.2786167860031128\n",
      "[Training Epoch 7] Batch 2872, Loss 0.26276689767837524\n",
      "[Training Epoch 7] Batch 2873, Loss 0.27966931462287903\n",
      "[Training Epoch 7] Batch 2874, Loss 0.24874219298362732\n",
      "[Training Epoch 7] Batch 2875, Loss 0.2656409442424774\n",
      "[Training Epoch 7] Batch 2876, Loss 0.26258063316345215\n",
      "[Training Epoch 7] Batch 2877, Loss 0.22746601700782776\n",
      "[Training Epoch 7] Batch 2878, Loss 0.2549673616886139\n",
      "[Training Epoch 7] Batch 2879, Loss 0.23240403831005096\n",
      "[Training Epoch 7] Batch 2880, Loss 0.2551383972167969\n",
      "[Training Epoch 7] Batch 2881, Loss 0.24767817556858063\n",
      "[Training Epoch 7] Batch 2882, Loss 0.28180253505706787\n",
      "[Training Epoch 7] Batch 2883, Loss 0.273622065782547\n",
      "[Training Epoch 7] Batch 2884, Loss 0.24576227366924286\n",
      "[Training Epoch 7] Batch 2885, Loss 0.26894932985305786\n",
      "[Training Epoch 7] Batch 2886, Loss 0.25270262360572815\n",
      "[Training Epoch 7] Batch 2887, Loss 0.27631768584251404\n",
      "[Training Epoch 7] Batch 2888, Loss 0.25263625383377075\n",
      "[Training Epoch 7] Batch 2889, Loss 0.27265363931655884\n",
      "[Training Epoch 7] Batch 2890, Loss 0.2587845027446747\n",
      "[Training Epoch 7] Batch 2891, Loss 0.24509963393211365\n",
      "[Training Epoch 7] Batch 2892, Loss 0.26558688282966614\n",
      "[Training Epoch 7] Batch 2893, Loss 0.26154372096061707\n",
      "[Training Epoch 7] Batch 2894, Loss 0.2659408450126648\n",
      "[Training Epoch 7] Batch 2895, Loss 0.24340540170669556\n",
      "[Training Epoch 7] Batch 2896, Loss 0.23886117339134216\n",
      "[Training Epoch 7] Batch 2897, Loss 0.2497692108154297\n",
      "[Training Epoch 7] Batch 2898, Loss 0.25618112087249756\n",
      "[Training Epoch 7] Batch 2899, Loss 0.2542429268360138\n",
      "[Training Epoch 7] Batch 2900, Loss 0.26311278343200684\n",
      "[Training Epoch 7] Batch 2901, Loss 0.24717342853546143\n",
      "[Training Epoch 7] Batch 2902, Loss 0.25742799043655396\n",
      "[Training Epoch 7] Batch 2903, Loss 0.26817208528518677\n",
      "[Training Epoch 7] Batch 2904, Loss 0.2565045952796936\n",
      "[Training Epoch 7] Batch 2905, Loss 0.2597510516643524\n",
      "[Training Epoch 7] Batch 2906, Loss 0.2377786785364151\n",
      "[Training Epoch 7] Batch 2907, Loss 0.25055375695228577\n",
      "[Training Epoch 7] Batch 2908, Loss 0.26952072978019714\n",
      "[Training Epoch 7] Batch 2909, Loss 0.25218963623046875\n",
      "[Training Epoch 7] Batch 2910, Loss 0.2670469284057617\n",
      "[Training Epoch 7] Batch 2911, Loss 0.23129025101661682\n",
      "[Training Epoch 7] Batch 2912, Loss 0.26224830746650696\n",
      "[Training Epoch 7] Batch 2913, Loss 0.2723507881164551\n",
      "[Training Epoch 7] Batch 2914, Loss 0.2802903354167938\n",
      "[Training Epoch 7] Batch 2915, Loss 0.2625526487827301\n",
      "[Training Epoch 7] Batch 2916, Loss 0.24994772672653198\n",
      "[Training Epoch 7] Batch 2917, Loss 0.2531577944755554\n",
      "[Training Epoch 7] Batch 2918, Loss 0.25493568181991577\n",
      "[Training Epoch 7] Batch 2919, Loss 0.24322204291820526\n",
      "[Training Epoch 7] Batch 2920, Loss 0.27316999435424805\n",
      "[Training Epoch 7] Batch 2921, Loss 0.2712816596031189\n",
      "[Training Epoch 7] Batch 2922, Loss 0.2597699463367462\n",
      "[Training Epoch 7] Batch 2923, Loss 0.26994946599006653\n",
      "[Training Epoch 7] Batch 2924, Loss 0.26260367035865784\n",
      "[Training Epoch 7] Batch 2925, Loss 0.26022055745124817\n",
      "[Training Epoch 7] Batch 2926, Loss 0.23831570148468018\n",
      "[Training Epoch 7] Batch 2927, Loss 0.2651062607765198\n",
      "[Training Epoch 7] Batch 2928, Loss 0.2689882516860962\n",
      "[Training Epoch 7] Batch 2929, Loss 0.279438316822052\n",
      "[Training Epoch 7] Batch 2930, Loss 0.2582557797431946\n",
      "[Training Epoch 7] Batch 2931, Loss 0.26048362255096436\n",
      "[Training Epoch 7] Batch 2932, Loss 0.2711540460586548\n",
      "[Training Epoch 7] Batch 2933, Loss 0.2413460910320282\n",
      "[Training Epoch 7] Batch 2934, Loss 0.2650669813156128\n",
      "[Training Epoch 7] Batch 2935, Loss 0.24374249577522278\n",
      "[Training Epoch 7] Batch 2936, Loss 0.2703838646411896\n",
      "[Training Epoch 7] Batch 2937, Loss 0.25094494223594666\n",
      "[Training Epoch 7] Batch 2938, Loss 0.2569389045238495\n",
      "[Training Epoch 7] Batch 2939, Loss 0.2408198118209839\n",
      "[Training Epoch 7] Batch 2940, Loss 0.29469597339630127\n",
      "[Training Epoch 7] Batch 2941, Loss 0.2482309341430664\n",
      "[Training Epoch 7] Batch 2942, Loss 0.24242421984672546\n",
      "[Training Epoch 7] Batch 2943, Loss 0.2640363276004791\n",
      "[Training Epoch 7] Batch 2944, Loss 0.23915094137191772\n",
      "[Training Epoch 7] Batch 2945, Loss 0.24376428127288818\n",
      "[Training Epoch 7] Batch 2946, Loss 0.26199692487716675\n",
      "[Training Epoch 7] Batch 2947, Loss 0.23911303281784058\n",
      "[Training Epoch 7] Batch 2948, Loss 0.25368261337280273\n",
      "[Training Epoch 7] Batch 2949, Loss 0.2562113404273987\n",
      "[Training Epoch 7] Batch 2950, Loss 0.2577503025531769\n",
      "[Training Epoch 7] Batch 2951, Loss 0.25293755531311035\n",
      "[Training Epoch 7] Batch 2952, Loss 0.22941674292087555\n",
      "[Training Epoch 7] Batch 2953, Loss 0.25280100107192993\n",
      "[Training Epoch 7] Batch 2954, Loss 0.26240578293800354\n",
      "[Training Epoch 7] Batch 2955, Loss 0.24641463160514832\n",
      "[Training Epoch 7] Batch 2956, Loss 0.23621293902397156\n",
      "[Training Epoch 7] Batch 2957, Loss 0.24835193157196045\n",
      "[Training Epoch 7] Batch 2958, Loss 0.2702931761741638\n",
      "[Training Epoch 7] Batch 2959, Loss 0.26177772879600525\n",
      "[Training Epoch 7] Batch 2960, Loss 0.2572075128555298\n",
      "[Training Epoch 7] Batch 2961, Loss 0.25140300393104553\n",
      "[Training Epoch 7] Batch 2962, Loss 0.23929458856582642\n",
      "[Training Epoch 7] Batch 2963, Loss 0.2905172109603882\n",
      "[Training Epoch 7] Batch 2964, Loss 0.25866252183914185\n",
      "[Training Epoch 7] Batch 2965, Loss 0.25903844833374023\n",
      "[Training Epoch 7] Batch 2966, Loss 0.2686348557472229\n",
      "[Training Epoch 7] Batch 2967, Loss 0.23910152912139893\n",
      "[Training Epoch 7] Batch 2968, Loss 0.22698749601840973\n",
      "[Training Epoch 7] Batch 2969, Loss 0.2730852961540222\n",
      "[Training Epoch 7] Batch 2970, Loss 0.2521287798881531\n",
      "[Training Epoch 7] Batch 2971, Loss 0.26050812005996704\n",
      "[Training Epoch 7] Batch 2972, Loss 0.24436743557453156\n",
      "[Training Epoch 7] Batch 2973, Loss 0.2621215581893921\n",
      "[Training Epoch 7] Batch 2974, Loss 0.24965445697307587\n",
      "[Training Epoch 7] Batch 2975, Loss 0.23110532760620117\n",
      "[Training Epoch 7] Batch 2976, Loss 0.2504327893257141\n",
      "[Training Epoch 7] Batch 2977, Loss 0.2577928304672241\n",
      "[Training Epoch 7] Batch 2978, Loss 0.2508019804954529\n",
      "[Training Epoch 7] Batch 2979, Loss 0.249779611825943\n",
      "[Training Epoch 7] Batch 2980, Loss 0.2815930247306824\n",
      "[Training Epoch 7] Batch 2981, Loss 0.2746836245059967\n",
      "[Training Epoch 7] Batch 2982, Loss 0.26997047662734985\n",
      "[Training Epoch 7] Batch 2983, Loss 0.2628207802772522\n",
      "[Training Epoch 7] Batch 2984, Loss 0.22331124544143677\n",
      "[Training Epoch 7] Batch 2985, Loss 0.28229767084121704\n",
      "[Training Epoch 7] Batch 2986, Loss 0.27038416266441345\n",
      "[Training Epoch 7] Batch 2987, Loss 0.2666322886943817\n",
      "[Training Epoch 7] Batch 2988, Loss 0.2907449007034302\n",
      "[Training Epoch 7] Batch 2989, Loss 0.25770318508148193\n",
      "[Training Epoch 7] Batch 2990, Loss 0.2734699249267578\n",
      "[Training Epoch 7] Batch 2991, Loss 0.2778294086456299\n",
      "[Training Epoch 7] Batch 2992, Loss 0.2828839123249054\n",
      "[Training Epoch 7] Batch 2993, Loss 0.27604416012763977\n",
      "[Training Epoch 7] Batch 2994, Loss 0.2768682539463043\n",
      "[Training Epoch 7] Batch 2995, Loss 0.25487780570983887\n",
      "[Training Epoch 7] Batch 2996, Loss 0.2508396506309509\n",
      "[Training Epoch 7] Batch 2997, Loss 0.2695114314556122\n",
      "[Training Epoch 7] Batch 2998, Loss 0.26559746265411377\n",
      "[Training Epoch 7] Batch 2999, Loss 0.28471246361732483\n",
      "[Training Epoch 7] Batch 3000, Loss 0.27395308017730713\n",
      "[Training Epoch 7] Batch 3001, Loss 0.23903591930866241\n",
      "[Training Epoch 7] Batch 3002, Loss 0.27732378244400024\n",
      "[Training Epoch 7] Batch 3003, Loss 0.26413050293922424\n",
      "[Training Epoch 7] Batch 3004, Loss 0.24391229450702667\n",
      "[Training Epoch 7] Batch 3005, Loss 0.27337151765823364\n",
      "[Training Epoch 7] Batch 3006, Loss 0.27250176668167114\n",
      "[Training Epoch 7] Batch 3007, Loss 0.28036805987358093\n",
      "[Training Epoch 7] Batch 3008, Loss 0.26699331402778625\n",
      "[Training Epoch 7] Batch 3009, Loss 0.22411558032035828\n",
      "[Training Epoch 7] Batch 3010, Loss 0.26181018352508545\n",
      "[Training Epoch 7] Batch 3011, Loss 0.26098108291625977\n",
      "[Training Epoch 7] Batch 3012, Loss 0.2811205983161926\n",
      "[Training Epoch 7] Batch 3013, Loss 0.26806753873825073\n",
      "[Training Epoch 7] Batch 3014, Loss 0.2648129463195801\n",
      "[Training Epoch 7] Batch 3015, Loss 0.24999943375587463\n",
      "[Training Epoch 7] Batch 3016, Loss 0.24514374136924744\n",
      "[Training Epoch 7] Batch 3017, Loss 0.27449625730514526\n",
      "[Training Epoch 7] Batch 3018, Loss 0.29214534163475037\n",
      "[Training Epoch 7] Batch 3019, Loss 0.29146435856819153\n",
      "[Training Epoch 7] Batch 3020, Loss 0.2625151574611664\n",
      "[Training Epoch 7] Batch 3021, Loss 0.23879790306091309\n",
      "[Training Epoch 7] Batch 3022, Loss 0.23348015546798706\n",
      "[Training Epoch 7] Batch 3023, Loss 0.23624283075332642\n",
      "[Training Epoch 7] Batch 3024, Loss 0.22937092185020447\n",
      "[Training Epoch 7] Batch 3025, Loss 0.27319443225860596\n",
      "[Training Epoch 7] Batch 3026, Loss 0.22296608984470367\n",
      "[Training Epoch 7] Batch 3027, Loss 0.24587473273277283\n",
      "[Training Epoch 7] Batch 3028, Loss 0.2601305842399597\n",
      "[Training Epoch 7] Batch 3029, Loss 0.2591637670993805\n",
      "[Training Epoch 7] Batch 3030, Loss 0.2639673054218292\n",
      "[Training Epoch 7] Batch 3031, Loss 0.25022804737091064\n",
      "[Training Epoch 7] Batch 3032, Loss 0.23299682140350342\n",
      "[Training Epoch 7] Batch 3033, Loss 0.2722952365875244\n",
      "[Training Epoch 7] Batch 3034, Loss 0.2570621371269226\n",
      "[Training Epoch 7] Batch 3035, Loss 0.25429749488830566\n",
      "[Training Epoch 7] Batch 3036, Loss 0.2725754976272583\n",
      "[Training Epoch 7] Batch 3037, Loss 0.25088292360305786\n",
      "[Training Epoch 7] Batch 3038, Loss 0.25223052501678467\n",
      "[Training Epoch 7] Batch 3039, Loss 0.2472158670425415\n",
      "[Training Epoch 7] Batch 3040, Loss 0.24420207738876343\n",
      "[Training Epoch 7] Batch 3041, Loss 0.27468347549438477\n",
      "[Training Epoch 7] Batch 3042, Loss 0.29678115248680115\n",
      "[Training Epoch 7] Batch 3043, Loss 0.2970273196697235\n",
      "[Training Epoch 7] Batch 3044, Loss 0.2885120213031769\n",
      "[Training Epoch 7] Batch 3045, Loss 0.2402125895023346\n",
      "[Training Epoch 7] Batch 3046, Loss 0.2726883292198181\n",
      "[Training Epoch 7] Batch 3047, Loss 0.2537042200565338\n",
      "[Training Epoch 7] Batch 3048, Loss 0.23832790553569794\n",
      "[Training Epoch 7] Batch 3049, Loss 0.2722078561782837\n",
      "[Training Epoch 7] Batch 3050, Loss 0.25226181745529175\n",
      "[Training Epoch 7] Batch 3051, Loss 0.2780275344848633\n",
      "[Training Epoch 7] Batch 3052, Loss 0.25487175583839417\n",
      "[Training Epoch 7] Batch 3053, Loss 0.2853153645992279\n",
      "[Training Epoch 7] Batch 3054, Loss 0.3122307360172272\n",
      "[Training Epoch 7] Batch 3055, Loss 0.24303263425827026\n",
      "[Training Epoch 7] Batch 3056, Loss 0.27179238200187683\n",
      "[Training Epoch 7] Batch 3057, Loss 0.2889671325683594\n",
      "[Training Epoch 7] Batch 3058, Loss 0.2671296298503876\n",
      "[Training Epoch 7] Batch 3059, Loss 0.231080561876297\n",
      "[Training Epoch 7] Batch 3060, Loss 0.23547153174877167\n",
      "[Training Epoch 7] Batch 3061, Loss 0.24563464522361755\n",
      "[Training Epoch 7] Batch 3062, Loss 0.2567403316497803\n",
      "[Training Epoch 7] Batch 3063, Loss 0.25032511353492737\n",
      "[Training Epoch 7] Batch 3064, Loss 0.2793810963630676\n",
      "[Training Epoch 7] Batch 3065, Loss 0.24367021024227142\n",
      "[Training Epoch 7] Batch 3066, Loss 0.25658321380615234\n",
      "[Training Epoch 7] Batch 3067, Loss 0.27598488330841064\n",
      "[Training Epoch 7] Batch 3068, Loss 0.27802279591560364\n",
      "[Training Epoch 7] Batch 3069, Loss 0.255264550447464\n",
      "[Training Epoch 7] Batch 3070, Loss 0.2463981807231903\n",
      "[Training Epoch 7] Batch 3071, Loss 0.2903901934623718\n",
      "[Training Epoch 7] Batch 3072, Loss 0.26435840129852295\n",
      "[Training Epoch 7] Batch 3073, Loss 0.2826685905456543\n",
      "[Training Epoch 7] Batch 3074, Loss 0.26675117015838623\n",
      "[Training Epoch 7] Batch 3075, Loss 0.25407516956329346\n",
      "[Training Epoch 7] Batch 3076, Loss 0.2648729085922241\n",
      "[Training Epoch 7] Batch 3077, Loss 0.2932201027870178\n",
      "[Training Epoch 7] Batch 3078, Loss 0.26529639959335327\n",
      "[Training Epoch 7] Batch 3079, Loss 0.23318909108638763\n",
      "[Training Epoch 7] Batch 3080, Loss 0.2634989023208618\n",
      "[Training Epoch 7] Batch 3081, Loss 0.26111239194869995\n",
      "[Training Epoch 7] Batch 3082, Loss 0.2691989541053772\n",
      "[Training Epoch 7] Batch 3083, Loss 0.2613646686077118\n",
      "[Training Epoch 7] Batch 3084, Loss 0.25122612714767456\n",
      "[Training Epoch 7] Batch 3085, Loss 0.24048122763633728\n",
      "[Training Epoch 7] Batch 3086, Loss 0.2776736617088318\n",
      "[Training Epoch 7] Batch 3087, Loss 0.24702364206314087\n",
      "[Training Epoch 7] Batch 3088, Loss 0.26465991139411926\n",
      "[Training Epoch 7] Batch 3089, Loss 0.2342420369386673\n",
      "[Training Epoch 7] Batch 3090, Loss 0.26083797216415405\n",
      "[Training Epoch 7] Batch 3091, Loss 0.24176068603992462\n",
      "[Training Epoch 7] Batch 3092, Loss 0.24429795145988464\n",
      "[Training Epoch 7] Batch 3093, Loss 0.2584000825881958\n",
      "[Training Epoch 7] Batch 3094, Loss 0.2381712794303894\n",
      "[Training Epoch 7] Batch 3095, Loss 0.26828479766845703\n",
      "[Training Epoch 7] Batch 3096, Loss 0.22756123542785645\n",
      "[Training Epoch 7] Batch 3097, Loss 0.2727964520454407\n",
      "[Training Epoch 7] Batch 3098, Loss 0.24224445223808289\n",
      "[Training Epoch 7] Batch 3099, Loss 0.2699263095855713\n",
      "[Training Epoch 7] Batch 3100, Loss 0.26452523469924927\n",
      "[Training Epoch 7] Batch 3101, Loss 0.23906688392162323\n",
      "[Training Epoch 7] Batch 3102, Loss 0.237052783370018\n",
      "[Training Epoch 7] Batch 3103, Loss 0.2638586759567261\n",
      "[Training Epoch 7] Batch 3104, Loss 0.257143497467041\n",
      "[Training Epoch 7] Batch 3105, Loss 0.244587242603302\n",
      "[Training Epoch 7] Batch 3106, Loss 0.2672779858112335\n",
      "[Training Epoch 7] Batch 3107, Loss 0.26941466331481934\n",
      "[Training Epoch 7] Batch 3108, Loss 0.25970375537872314\n",
      "[Training Epoch 7] Batch 3109, Loss 0.28951311111450195\n",
      "[Training Epoch 7] Batch 3110, Loss 0.24535855650901794\n",
      "[Training Epoch 7] Batch 3111, Loss 0.2402428537607193\n",
      "[Training Epoch 7] Batch 3112, Loss 0.26544252038002014\n",
      "[Training Epoch 7] Batch 3113, Loss 0.239949032664299\n",
      "[Training Epoch 7] Batch 3114, Loss 0.23404456675052643\n",
      "[Training Epoch 7] Batch 3115, Loss 0.2703971862792969\n",
      "[Training Epoch 7] Batch 3116, Loss 0.2467469871044159\n",
      "[Training Epoch 7] Batch 3117, Loss 0.2516416907310486\n",
      "[Training Epoch 7] Batch 3118, Loss 0.2465253323316574\n",
      "[Training Epoch 7] Batch 3119, Loss 0.23580315709114075\n",
      "[Training Epoch 7] Batch 3120, Loss 0.23222403228282928\n",
      "[Training Epoch 7] Batch 3121, Loss 0.25503864884376526\n",
      "[Training Epoch 7] Batch 3122, Loss 0.2657180428504944\n",
      "[Training Epoch 7] Batch 3123, Loss 0.2392270863056183\n",
      "[Training Epoch 7] Batch 3124, Loss 0.2313280701637268\n",
      "[Training Epoch 7] Batch 3125, Loss 0.2883567214012146\n",
      "[Training Epoch 7] Batch 3126, Loss 0.27608388662338257\n",
      "[Training Epoch 7] Batch 3127, Loss 0.25208646059036255\n",
      "[Training Epoch 7] Batch 3128, Loss 0.26010698080062866\n",
      "[Training Epoch 7] Batch 3129, Loss 0.292060911655426\n",
      "[Training Epoch 7] Batch 3130, Loss 0.27273598313331604\n",
      "[Training Epoch 7] Batch 3131, Loss 0.2365180402994156\n",
      "[Training Epoch 7] Batch 3132, Loss 0.22626808285713196\n",
      "[Training Epoch 7] Batch 3133, Loss 0.29595762491226196\n",
      "[Training Epoch 7] Batch 3134, Loss 0.28282731771469116\n",
      "[Training Epoch 7] Batch 3135, Loss 0.28072673082351685\n",
      "[Training Epoch 7] Batch 3136, Loss 0.25290048122406006\n",
      "[Training Epoch 7] Batch 3137, Loss 0.23776483535766602\n",
      "[Training Epoch 7] Batch 3138, Loss 0.2731136679649353\n",
      "[Training Epoch 7] Batch 3139, Loss 0.27658796310424805\n",
      "[Training Epoch 7] Batch 3140, Loss 0.255953311920166\n",
      "[Training Epoch 7] Batch 3141, Loss 0.24907509982585907\n",
      "[Training Epoch 7] Batch 3142, Loss 0.26876938343048096\n",
      "[Training Epoch 7] Batch 3143, Loss 0.2665294408798218\n",
      "[Training Epoch 7] Batch 3144, Loss 0.24815616011619568\n",
      "[Training Epoch 7] Batch 3145, Loss 0.24630334973335266\n",
      "[Training Epoch 7] Batch 3146, Loss 0.238277867436409\n",
      "[Training Epoch 7] Batch 3147, Loss 0.2635560929775238\n",
      "[Training Epoch 7] Batch 3148, Loss 0.24974574148654938\n",
      "[Training Epoch 7] Batch 3149, Loss 0.24619212746620178\n",
      "[Training Epoch 7] Batch 3150, Loss 0.24835190176963806\n",
      "[Training Epoch 7] Batch 3151, Loss 0.23768341541290283\n",
      "[Training Epoch 7] Batch 3152, Loss 0.25262558460235596\n",
      "[Training Epoch 7] Batch 3153, Loss 0.24868425726890564\n",
      "[Training Epoch 7] Batch 3154, Loss 0.25484156608581543\n",
      "[Training Epoch 7] Batch 3155, Loss 0.2740423083305359\n",
      "[Training Epoch 7] Batch 3156, Loss 0.2585471272468567\n",
      "[Training Epoch 7] Batch 3157, Loss 0.24682515859603882\n",
      "[Training Epoch 7] Batch 3158, Loss 0.25801581144332886\n",
      "[Training Epoch 7] Batch 3159, Loss 0.25360187888145447\n",
      "[Training Epoch 7] Batch 3160, Loss 0.23341602087020874\n",
      "[Training Epoch 7] Batch 3161, Loss 0.2466421127319336\n",
      "[Training Epoch 7] Batch 3162, Loss 0.24496512115001678\n",
      "[Training Epoch 7] Batch 3163, Loss 0.2638050615787506\n",
      "[Training Epoch 7] Batch 3164, Loss 0.26406329870224\n",
      "[Training Epoch 7] Batch 3165, Loss 0.23932483792304993\n",
      "[Training Epoch 7] Batch 3166, Loss 0.2722983658313751\n",
      "[Training Epoch 7] Batch 3167, Loss 0.28958848118782043\n",
      "[Training Epoch 7] Batch 3168, Loss 0.2764360308647156\n",
      "[Training Epoch 7] Batch 3169, Loss 0.24760928750038147\n",
      "[Training Epoch 7] Batch 3170, Loss 0.2567327618598938\n",
      "[Training Epoch 7] Batch 3171, Loss 0.2526644766330719\n",
      "[Training Epoch 7] Batch 3172, Loss 0.2406962513923645\n",
      "[Training Epoch 7] Batch 3173, Loss 0.24308453500270844\n",
      "[Training Epoch 7] Batch 3174, Loss 0.270750492811203\n",
      "[Training Epoch 7] Batch 3175, Loss 0.2511961758136749\n",
      "[Training Epoch 7] Batch 3176, Loss 0.2558000087738037\n",
      "[Training Epoch 7] Batch 3177, Loss 0.2768392562866211\n",
      "[Training Epoch 7] Batch 3178, Loss 0.2563367187976837\n",
      "[Training Epoch 7] Batch 3179, Loss 0.2775900959968567\n",
      "[Training Epoch 7] Batch 3180, Loss 0.2524288594722748\n",
      "[Training Epoch 7] Batch 3181, Loss 0.2210448980331421\n",
      "[Training Epoch 7] Batch 3182, Loss 0.27524274587631226\n",
      "[Training Epoch 7] Batch 3183, Loss 0.24087786674499512\n",
      "[Training Epoch 7] Batch 3184, Loss 0.24555373191833496\n",
      "[Training Epoch 7] Batch 3185, Loss 0.260242223739624\n",
      "[Training Epoch 7] Batch 3186, Loss 0.27380287647247314\n",
      "[Training Epoch 7] Batch 3187, Loss 0.2512316107749939\n",
      "[Training Epoch 7] Batch 3188, Loss 0.2653520405292511\n",
      "[Training Epoch 7] Batch 3189, Loss 0.2453521490097046\n",
      "[Training Epoch 7] Batch 3190, Loss 0.2503470480442047\n",
      "[Training Epoch 7] Batch 3191, Loss 0.21807420253753662\n",
      "[Training Epoch 7] Batch 3192, Loss 0.2496146261692047\n",
      "[Training Epoch 7] Batch 3193, Loss 0.2596316933631897\n",
      "[Training Epoch 7] Batch 3194, Loss 0.2508738040924072\n",
      "[Training Epoch 7] Batch 3195, Loss 0.2592248022556305\n",
      "[Training Epoch 7] Batch 3196, Loss 0.2759106159210205\n",
      "[Training Epoch 7] Batch 3197, Loss 0.26276808977127075\n",
      "[Training Epoch 7] Batch 3198, Loss 0.25333237648010254\n",
      "[Training Epoch 7] Batch 3199, Loss 0.2587567865848541\n",
      "[Training Epoch 7] Batch 3200, Loss 0.2501046061515808\n",
      "[Training Epoch 7] Batch 3201, Loss 0.2371191531419754\n",
      "[Training Epoch 7] Batch 3202, Loss 0.2693774700164795\n",
      "[Training Epoch 7] Batch 3203, Loss 0.24154400825500488\n",
      "[Training Epoch 7] Batch 3204, Loss 0.24571868777275085\n",
      "[Training Epoch 7] Batch 3205, Loss 0.2780088782310486\n",
      "[Training Epoch 7] Batch 3206, Loss 0.2782292366027832\n",
      "[Training Epoch 7] Batch 3207, Loss 0.28867027163505554\n",
      "[Training Epoch 7] Batch 3208, Loss 0.26384180784225464\n",
      "[Training Epoch 7] Batch 3209, Loss 0.2309282273054123\n",
      "[Training Epoch 7] Batch 3210, Loss 0.25403666496276855\n",
      "[Training Epoch 7] Batch 3211, Loss 0.2540496587753296\n",
      "[Training Epoch 7] Batch 3212, Loss 0.2623087763786316\n",
      "[Training Epoch 7] Batch 3213, Loss 0.2655700445175171\n",
      "[Training Epoch 7] Batch 3214, Loss 0.27817314863204956\n",
      "[Training Epoch 7] Batch 3215, Loss 0.24300548434257507\n",
      "[Training Epoch 7] Batch 3216, Loss 0.23760777711868286\n",
      "[Training Epoch 7] Batch 3217, Loss 0.2630264163017273\n",
      "[Training Epoch 7] Batch 3218, Loss 0.25535446405410767\n",
      "[Training Epoch 7] Batch 3219, Loss 0.2577369213104248\n",
      "[Training Epoch 7] Batch 3220, Loss 0.2590695917606354\n",
      "[Training Epoch 7] Batch 3221, Loss 0.2407250702381134\n",
      "[Training Epoch 7] Batch 3222, Loss 0.2657686471939087\n",
      "[Training Epoch 7] Batch 3223, Loss 0.23915594816207886\n",
      "[Training Epoch 7] Batch 3224, Loss 0.27190470695495605\n",
      "[Training Epoch 7] Batch 3225, Loss 0.21908901631832123\n",
      "[Training Epoch 7] Batch 3226, Loss 0.23055770993232727\n",
      "[Training Epoch 7] Batch 3227, Loss 0.2692793011665344\n",
      "[Training Epoch 7] Batch 3228, Loss 0.25505301356315613\n",
      "[Training Epoch 7] Batch 3229, Loss 0.24694308638572693\n",
      "[Training Epoch 7] Batch 3230, Loss 0.2715647220611572\n",
      "[Training Epoch 7] Batch 3231, Loss 0.25543472170829773\n",
      "[Training Epoch 7] Batch 3232, Loss 0.29009202122688293\n",
      "[Training Epoch 7] Batch 3233, Loss 0.27750277519226074\n",
      "[Training Epoch 7] Batch 3234, Loss 0.2681431770324707\n",
      "[Training Epoch 7] Batch 3235, Loss 0.23298749327659607\n",
      "[Training Epoch 7] Batch 3236, Loss 0.24912378191947937\n",
      "[Training Epoch 7] Batch 3237, Loss 0.2601742446422577\n",
      "[Training Epoch 7] Batch 3238, Loss 0.2440209984779358\n",
      "[Training Epoch 7] Batch 3239, Loss 0.23869085311889648\n",
      "[Training Epoch 7] Batch 3240, Loss 0.2754351496696472\n",
      "[Training Epoch 7] Batch 3241, Loss 0.2534024715423584\n",
      "[Training Epoch 7] Batch 3242, Loss 0.25935226678848267\n",
      "[Training Epoch 7] Batch 3243, Loss 0.2562446892261505\n",
      "[Training Epoch 7] Batch 3244, Loss 0.24064305424690247\n",
      "[Training Epoch 7] Batch 3245, Loss 0.25145286321640015\n",
      "[Training Epoch 7] Batch 3246, Loss 0.2547551989555359\n",
      "[Training Epoch 7] Batch 3247, Loss 0.24417291581630707\n",
      "[Training Epoch 7] Batch 3248, Loss 0.2809640169143677\n",
      "[Training Epoch 7] Batch 3249, Loss 0.27525681257247925\n",
      "[Training Epoch 7] Batch 3250, Loss 0.2685636281967163\n",
      "[Training Epoch 7] Batch 3251, Loss 0.2616599202156067\n",
      "[Training Epoch 7] Batch 3252, Loss 0.2781774401664734\n",
      "[Training Epoch 7] Batch 3253, Loss 0.2522300183773041\n",
      "[Training Epoch 7] Batch 3254, Loss 0.23331674933433533\n",
      "[Training Epoch 7] Batch 3255, Loss 0.23269429802894592\n",
      "[Training Epoch 7] Batch 3256, Loss 0.2542448043823242\n",
      "[Training Epoch 7] Batch 3257, Loss 0.24971923232078552\n",
      "[Training Epoch 7] Batch 3258, Loss 0.2717816233634949\n",
      "[Training Epoch 7] Batch 3259, Loss 0.24767106771469116\n",
      "[Training Epoch 7] Batch 3260, Loss 0.2687758505344391\n",
      "[Training Epoch 7] Batch 3261, Loss 0.27199894189834595\n",
      "[Training Epoch 7] Batch 3262, Loss 0.2597835659980774\n",
      "[Training Epoch 7] Batch 3263, Loss 0.28351131081581116\n",
      "[Training Epoch 7] Batch 3264, Loss 0.2672485411167145\n",
      "[Training Epoch 7] Batch 3265, Loss 0.2742058038711548\n",
      "[Training Epoch 7] Batch 3266, Loss 0.25470662117004395\n",
      "[Training Epoch 7] Batch 3267, Loss 0.25410640239715576\n",
      "[Training Epoch 7] Batch 3268, Loss 0.2731121778488159\n",
      "[Training Epoch 7] Batch 3269, Loss 0.23278066515922546\n",
      "[Training Epoch 7] Batch 3270, Loss 0.2562618851661682\n",
      "[Training Epoch 7] Batch 3271, Loss 0.2465585470199585\n",
      "[Training Epoch 7] Batch 3272, Loss 0.27135172486305237\n",
      "[Training Epoch 7] Batch 3273, Loss 0.23329278826713562\n",
      "[Training Epoch 7] Batch 3274, Loss 0.2600618302822113\n",
      "[Training Epoch 7] Batch 3275, Loss 0.24867384135723114\n",
      "[Training Epoch 7] Batch 3276, Loss 0.24444833397865295\n",
      "[Training Epoch 7] Batch 3277, Loss 0.24431711435317993\n",
      "[Training Epoch 7] Batch 3278, Loss 0.27230969071388245\n",
      "[Training Epoch 7] Batch 3279, Loss 0.23686806857585907\n",
      "[Training Epoch 7] Batch 3280, Loss 0.2599762976169586\n",
      "[Training Epoch 7] Batch 3281, Loss 0.2652270197868347\n",
      "[Training Epoch 7] Batch 3282, Loss 0.25557655096054077\n",
      "[Training Epoch 7] Batch 3283, Loss 0.24604199826717377\n",
      "[Training Epoch 7] Batch 3284, Loss 0.25686073303222656\n",
      "[Training Epoch 7] Batch 3285, Loss 0.2370007485151291\n",
      "[Training Epoch 7] Batch 3286, Loss 0.29667311906814575\n",
      "[Training Epoch 7] Batch 3287, Loss 0.2804041802883148\n",
      "[Training Epoch 7] Batch 3288, Loss 0.2494836449623108\n",
      "[Training Epoch 7] Batch 3289, Loss 0.2511250972747803\n",
      "[Training Epoch 7] Batch 3290, Loss 0.25826704502105713\n",
      "[Training Epoch 7] Batch 3291, Loss 0.239797443151474\n",
      "[Training Epoch 7] Batch 3292, Loss 0.23298271000385284\n",
      "[Training Epoch 7] Batch 3293, Loss 0.25292253494262695\n",
      "[Training Epoch 7] Batch 3294, Loss 0.2565877437591553\n",
      "[Training Epoch 7] Batch 3295, Loss 0.23681282997131348\n",
      "[Training Epoch 7] Batch 3296, Loss 0.26589179039001465\n",
      "[Training Epoch 7] Batch 3297, Loss 0.2631610035896301\n",
      "[Training Epoch 7] Batch 3298, Loss 0.2317509651184082\n",
      "[Training Epoch 7] Batch 3299, Loss 0.2603108584880829\n",
      "[Training Epoch 7] Batch 3300, Loss 0.22151871025562286\n",
      "[Training Epoch 7] Batch 3301, Loss 0.3024592995643616\n",
      "[Training Epoch 7] Batch 3302, Loss 0.28824877738952637\n",
      "[Training Epoch 7] Batch 3303, Loss 0.2927555441856384\n",
      "[Training Epoch 7] Batch 3304, Loss 0.24589672684669495\n",
      "[Training Epoch 7] Batch 3305, Loss 0.25073689222335815\n",
      "[Training Epoch 7] Batch 3306, Loss 0.26741135120391846\n",
      "[Training Epoch 7] Batch 3307, Loss 0.28583163022994995\n",
      "[Training Epoch 7] Batch 3308, Loss 0.27770841121673584\n",
      "[Training Epoch 7] Batch 3309, Loss 0.23914965987205505\n",
      "[Training Epoch 7] Batch 3310, Loss 0.26955461502075195\n",
      "[Training Epoch 7] Batch 3311, Loss 0.28020912408828735\n",
      "[Training Epoch 7] Batch 3312, Loss 0.26646727323532104\n",
      "[Training Epoch 7] Batch 3313, Loss 0.27480778098106384\n",
      "[Training Epoch 7] Batch 3314, Loss 0.2650910019874573\n",
      "[Training Epoch 7] Batch 3315, Loss 0.26475051045417786\n",
      "[Training Epoch 7] Batch 3316, Loss 0.27764326333999634\n",
      "[Training Epoch 7] Batch 3317, Loss 0.2799534797668457\n",
      "[Training Epoch 7] Batch 3318, Loss 0.2754477262496948\n",
      "[Training Epoch 7] Batch 3319, Loss 0.24812832474708557\n",
      "[Training Epoch 7] Batch 3320, Loss 0.2527589797973633\n",
      "[Training Epoch 7] Batch 3321, Loss 0.24835452437400818\n",
      "[Training Epoch 7] Batch 3322, Loss 0.26174455881118774\n",
      "[Training Epoch 7] Batch 3323, Loss 0.2875722646713257\n",
      "[Training Epoch 7] Batch 3324, Loss 0.25357845425605774\n",
      "[Training Epoch 7] Batch 3325, Loss 0.23766456544399261\n",
      "[Training Epoch 7] Batch 3326, Loss 0.26332780718803406\n",
      "[Training Epoch 7] Batch 3327, Loss 0.28264525532722473\n",
      "[Training Epoch 7] Batch 3328, Loss 0.25495725870132446\n",
      "[Training Epoch 7] Batch 3329, Loss 0.26098912954330444\n",
      "[Training Epoch 7] Batch 3330, Loss 0.2587794065475464\n",
      "[Training Epoch 7] Batch 3331, Loss 0.29297035932540894\n",
      "[Training Epoch 7] Batch 3332, Loss 0.2556103467941284\n",
      "[Training Epoch 7] Batch 3333, Loss 0.2826840281486511\n",
      "[Training Epoch 7] Batch 3334, Loss 0.2731170654296875\n",
      "[Training Epoch 7] Batch 3335, Loss 0.2405495047569275\n",
      "[Training Epoch 7] Batch 3336, Loss 0.2500213086605072\n",
      "[Training Epoch 7] Batch 3337, Loss 0.2518805265426636\n",
      "[Training Epoch 7] Batch 3338, Loss 0.2670488655567169\n",
      "[Training Epoch 7] Batch 3339, Loss 0.2578885555267334\n",
      "[Training Epoch 7] Batch 3340, Loss 0.2690151333808899\n",
      "[Training Epoch 7] Batch 3341, Loss 0.24811984598636627\n",
      "[Training Epoch 7] Batch 3342, Loss 0.2576446235179901\n",
      "[Training Epoch 7] Batch 3343, Loss 0.22897109389305115\n",
      "[Training Epoch 7] Batch 3344, Loss 0.28210508823394775\n",
      "[Training Epoch 7] Batch 3345, Loss 0.3040313720703125\n",
      "[Training Epoch 7] Batch 3346, Loss 0.25145208835601807\n",
      "[Training Epoch 7] Batch 3347, Loss 0.25449424982070923\n",
      "[Training Epoch 7] Batch 3348, Loss 0.2620627284049988\n",
      "[Training Epoch 7] Batch 3349, Loss 0.24877557158470154\n",
      "[Training Epoch 7] Batch 3350, Loss 0.28795912861824036\n",
      "[Training Epoch 7] Batch 3351, Loss 0.2809520959854126\n",
      "[Training Epoch 7] Batch 3352, Loss 0.26583927869796753\n",
      "[Training Epoch 7] Batch 3353, Loss 0.24944591522216797\n",
      "[Training Epoch 7] Batch 3354, Loss 0.24285613000392914\n",
      "[Training Epoch 7] Batch 3355, Loss 0.2362191528081894\n",
      "[Training Epoch 7] Batch 3356, Loss 0.2557391822338104\n",
      "[Training Epoch 7] Batch 3357, Loss 0.26913875341415405\n",
      "[Training Epoch 7] Batch 3358, Loss 0.22317439317703247\n",
      "[Training Epoch 7] Batch 3359, Loss 0.24194484949111938\n",
      "[Training Epoch 7] Batch 3360, Loss 0.24326670169830322\n",
      "[Training Epoch 7] Batch 3361, Loss 0.24670082330703735\n",
      "[Training Epoch 7] Batch 3362, Loss 0.26370662450790405\n",
      "[Training Epoch 7] Batch 3363, Loss 0.26022040843963623\n",
      "[Training Epoch 7] Batch 3364, Loss 0.2562822699546814\n",
      "[Training Epoch 7] Batch 3365, Loss 0.26279187202453613\n",
      "[Training Epoch 7] Batch 3366, Loss 0.2652067542076111\n",
      "[Training Epoch 7] Batch 3367, Loss 0.28058433532714844\n",
      "[Training Epoch 7] Batch 3368, Loss 0.2641942799091339\n",
      "[Training Epoch 7] Batch 3369, Loss 0.2648448944091797\n",
      "[Training Epoch 7] Batch 3370, Loss 0.3139089047908783\n",
      "[Training Epoch 7] Batch 3371, Loss 0.2862313687801361\n",
      "[Training Epoch 7] Batch 3372, Loss 0.2548412084579468\n",
      "[Training Epoch 7] Batch 3373, Loss 0.26586827635765076\n",
      "[Training Epoch 7] Batch 3374, Loss 0.2647004723548889\n",
      "[Training Epoch 7] Batch 3375, Loss 0.2854325771331787\n",
      "[Training Epoch 7] Batch 3376, Loss 0.2450283020734787\n",
      "[Training Epoch 7] Batch 3377, Loss 0.2572692632675171\n",
      "[Training Epoch 7] Batch 3378, Loss 0.25268495082855225\n",
      "[Training Epoch 7] Batch 3379, Loss 0.261425644159317\n",
      "[Training Epoch 7] Batch 3380, Loss 0.2667393088340759\n",
      "[Training Epoch 7] Batch 3381, Loss 0.25302502512931824\n",
      "[Training Epoch 7] Batch 3382, Loss 0.23394155502319336\n",
      "[Training Epoch 7] Batch 3383, Loss 0.27670252323150635\n",
      "[Training Epoch 7] Batch 3384, Loss 0.21851977705955505\n",
      "[Training Epoch 7] Batch 3385, Loss 0.26425135135650635\n",
      "[Training Epoch 7] Batch 3386, Loss 0.2227725237607956\n",
      "[Training Epoch 7] Batch 3387, Loss 0.2589346766471863\n",
      "[Training Epoch 7] Batch 3388, Loss 0.2276783585548401\n",
      "[Training Epoch 7] Batch 3389, Loss 0.2786976099014282\n",
      "[Training Epoch 7] Batch 3390, Loss 0.25765156745910645\n",
      "[Training Epoch 7] Batch 3391, Loss 0.24374160170555115\n",
      "[Training Epoch 7] Batch 3392, Loss 0.24771708250045776\n",
      "[Training Epoch 7] Batch 3393, Loss 0.26249903440475464\n",
      "[Training Epoch 7] Batch 3394, Loss 0.2852397859096527\n",
      "[Training Epoch 7] Batch 3395, Loss 0.2606639862060547\n",
      "[Training Epoch 7] Batch 3396, Loss 0.2665731906890869\n",
      "[Training Epoch 7] Batch 3397, Loss 0.2135145664215088\n",
      "[Training Epoch 7] Batch 3398, Loss 0.2602996826171875\n",
      "[Training Epoch 7] Batch 3399, Loss 0.27833855152130127\n",
      "[Training Epoch 7] Batch 3400, Loss 0.27185818552970886\n",
      "[Training Epoch 7] Batch 3401, Loss 0.24895954132080078\n",
      "[Training Epoch 7] Batch 3402, Loss 0.25716733932495117\n",
      "[Training Epoch 7] Batch 3403, Loss 0.2785368263721466\n",
      "[Training Epoch 7] Batch 3404, Loss 0.23587985336780548\n",
      "[Training Epoch 7] Batch 3405, Loss 0.2631455659866333\n",
      "[Training Epoch 7] Batch 3406, Loss 0.2496834695339203\n",
      "[Training Epoch 7] Batch 3407, Loss 0.23876285552978516\n",
      "[Training Epoch 7] Batch 3408, Loss 0.27145957946777344\n",
      "[Training Epoch 7] Batch 3409, Loss 0.2605837881565094\n",
      "[Training Epoch 7] Batch 3410, Loss 0.23758599162101746\n",
      "[Training Epoch 7] Batch 3411, Loss 0.2278379499912262\n",
      "[Training Epoch 7] Batch 3412, Loss 0.25764334201812744\n",
      "[Training Epoch 7] Batch 3413, Loss 0.25540482997894287\n",
      "[Training Epoch 7] Batch 3414, Loss 0.24749834835529327\n",
      "[Training Epoch 7] Batch 3415, Loss 0.2676823139190674\n",
      "[Training Epoch 7] Batch 3416, Loss 0.2454095482826233\n",
      "[Training Epoch 7] Batch 3417, Loss 0.24124258756637573\n",
      "[Training Epoch 7] Batch 3418, Loss 0.27688586711883545\n",
      "[Training Epoch 7] Batch 3419, Loss 0.25090497732162476\n",
      "[Training Epoch 7] Batch 3420, Loss 0.26579535007476807\n",
      "[Training Epoch 7] Batch 3421, Loss 0.25903183221817017\n",
      "[Training Epoch 7] Batch 3422, Loss 0.24011431634426117\n",
      "[Training Epoch 7] Batch 3423, Loss 0.2458713948726654\n",
      "[Training Epoch 7] Batch 3424, Loss 0.2599940896034241\n",
      "[Training Epoch 7] Batch 3425, Loss 0.2354375422000885\n",
      "[Training Epoch 7] Batch 3426, Loss 0.2682461738586426\n",
      "[Training Epoch 7] Batch 3427, Loss 0.21595388650894165\n",
      "[Training Epoch 7] Batch 3428, Loss 0.26363110542297363\n",
      "[Training Epoch 7] Batch 3429, Loss 0.2614564895629883\n",
      "[Training Epoch 7] Batch 3430, Loss 0.2876352071762085\n",
      "[Training Epoch 7] Batch 3431, Loss 0.2559548616409302\n",
      "[Training Epoch 7] Batch 3432, Loss 0.25389593839645386\n",
      "[Training Epoch 7] Batch 3433, Loss 0.22306178510189056\n",
      "[Training Epoch 7] Batch 3434, Loss 0.23471257090568542\n",
      "[Training Epoch 7] Batch 3435, Loss 0.2816915214061737\n",
      "[Training Epoch 7] Batch 3436, Loss 0.26525184512138367\n",
      "[Training Epoch 7] Batch 3437, Loss 0.25752925872802734\n",
      "[Training Epoch 7] Batch 3438, Loss 0.27054929733276367\n",
      "[Training Epoch 7] Batch 3439, Loss 0.2907460331916809\n",
      "[Training Epoch 7] Batch 3440, Loss 0.265163779258728\n",
      "[Training Epoch 7] Batch 3441, Loss 0.2526833415031433\n",
      "[Training Epoch 7] Batch 3442, Loss 0.2661656439304352\n",
      "[Training Epoch 7] Batch 3443, Loss 0.2548835575580597\n",
      "[Training Epoch 7] Batch 3444, Loss 0.2461131513118744\n",
      "[Training Epoch 7] Batch 3445, Loss 0.24414122104644775\n",
      "[Training Epoch 7] Batch 3446, Loss 0.31149011850357056\n",
      "[Training Epoch 7] Batch 3447, Loss 0.25740572810173035\n",
      "[Training Epoch 7] Batch 3448, Loss 0.23534798622131348\n",
      "[Training Epoch 7] Batch 3449, Loss 0.2573467791080475\n",
      "[Training Epoch 7] Batch 3450, Loss 0.23808279633522034\n",
      "[Training Epoch 7] Batch 3451, Loss 0.26143237948417664\n",
      "[Training Epoch 7] Batch 3452, Loss 0.2561054825782776\n",
      "[Training Epoch 7] Batch 3453, Loss 0.2765067517757416\n",
      "[Training Epoch 7] Batch 3454, Loss 0.2832263112068176\n",
      "[Training Epoch 7] Batch 3455, Loss 0.2633745074272156\n",
      "[Training Epoch 7] Batch 3456, Loss 0.25504422187805176\n",
      "[Training Epoch 7] Batch 3457, Loss 0.25720885396003723\n",
      "[Training Epoch 7] Batch 3458, Loss 0.2651253640651703\n",
      "[Training Epoch 7] Batch 3459, Loss 0.257102906703949\n",
      "[Training Epoch 7] Batch 3460, Loss 0.24616289138793945\n",
      "[Training Epoch 7] Batch 3461, Loss 0.24990063905715942\n",
      "[Training Epoch 7] Batch 3462, Loss 0.2547929286956787\n",
      "[Training Epoch 7] Batch 3463, Loss 0.2814211845397949\n",
      "[Training Epoch 7] Batch 3464, Loss 0.2851693034172058\n",
      "[Training Epoch 7] Batch 3465, Loss 0.25064873695373535\n",
      "[Training Epoch 7] Batch 3466, Loss 0.25743329524993896\n",
      "[Training Epoch 7] Batch 3467, Loss 0.25721466541290283\n",
      "[Training Epoch 7] Batch 3468, Loss 0.2501945495605469\n",
      "[Training Epoch 7] Batch 3469, Loss 0.2770952582359314\n",
      "[Training Epoch 7] Batch 3470, Loss 0.2665367126464844\n",
      "[Training Epoch 7] Batch 3471, Loss 0.25132307410240173\n",
      "[Training Epoch 7] Batch 3472, Loss 0.2732052803039551\n",
      "[Training Epoch 7] Batch 3473, Loss 0.2534914016723633\n",
      "[Training Epoch 7] Batch 3474, Loss 0.2560693025588989\n",
      "[Training Epoch 7] Batch 3475, Loss 0.27691027522087097\n",
      "[Training Epoch 7] Batch 3476, Loss 0.25341200828552246\n",
      "[Training Epoch 7] Batch 3477, Loss 0.2769968509674072\n",
      "[Training Epoch 7] Batch 3478, Loss 0.2692682147026062\n",
      "[Training Epoch 7] Batch 3479, Loss 0.2850412428379059\n",
      "[Training Epoch 7] Batch 3480, Loss 0.2602546215057373\n",
      "[Training Epoch 7] Batch 3481, Loss 0.260745644569397\n",
      "[Training Epoch 7] Batch 3482, Loss 0.28101032972335815\n",
      "[Training Epoch 7] Batch 3483, Loss 0.27838224172592163\n",
      "[Training Epoch 7] Batch 3484, Loss 0.2760138511657715\n",
      "[Training Epoch 7] Batch 3485, Loss 0.24427589774131775\n",
      "[Training Epoch 7] Batch 3486, Loss 0.27864524722099304\n",
      "[Training Epoch 7] Batch 3487, Loss 0.2816905975341797\n",
      "[Training Epoch 7] Batch 3488, Loss 0.23949328064918518\n",
      "[Training Epoch 7] Batch 3489, Loss 0.27990245819091797\n",
      "[Training Epoch 7] Batch 3490, Loss 0.27184534072875977\n",
      "[Training Epoch 7] Batch 3491, Loss 0.26474106311798096\n",
      "[Training Epoch 7] Batch 3492, Loss 0.24922969937324524\n",
      "[Training Epoch 7] Batch 3493, Loss 0.26309263706207275\n",
      "[Training Epoch 7] Batch 3494, Loss 0.25564730167388916\n",
      "[Training Epoch 7] Batch 3495, Loss 0.2518199682235718\n",
      "[Training Epoch 7] Batch 3496, Loss 0.25575900077819824\n",
      "[Training Epoch 7] Batch 3497, Loss 0.24641898274421692\n",
      "[Training Epoch 7] Batch 3498, Loss 0.2660280764102936\n",
      "[Training Epoch 7] Batch 3499, Loss 0.25197353959083557\n",
      "[Training Epoch 7] Batch 3500, Loss 0.25730231404304504\n",
      "[Training Epoch 7] Batch 3501, Loss 0.25369009375572205\n",
      "[Training Epoch 7] Batch 3502, Loss 0.2532263994216919\n",
      "[Training Epoch 7] Batch 3503, Loss 0.25144582986831665\n",
      "[Training Epoch 7] Batch 3504, Loss 0.24762782454490662\n",
      "[Training Epoch 7] Batch 3505, Loss 0.23613063991069794\n",
      "[Training Epoch 7] Batch 3506, Loss 0.2628597617149353\n",
      "[Training Epoch 7] Batch 3507, Loss 0.25114765763282776\n",
      "[Training Epoch 7] Batch 3508, Loss 0.2903873324394226\n",
      "[Training Epoch 7] Batch 3509, Loss 0.2205566167831421\n",
      "[Training Epoch 7] Batch 3510, Loss 0.2825155556201935\n",
      "[Training Epoch 7] Batch 3511, Loss 0.26924896240234375\n",
      "[Training Epoch 7] Batch 3512, Loss 0.25199583172798157\n",
      "[Training Epoch 7] Batch 3513, Loss 0.2350902408361435\n",
      "[Training Epoch 7] Batch 3514, Loss 0.2947095036506653\n",
      "[Training Epoch 7] Batch 3515, Loss 0.2170378565788269\n",
      "[Training Epoch 7] Batch 3516, Loss 0.24687838554382324\n",
      "[Training Epoch 7] Batch 3517, Loss 0.27234625816345215\n",
      "[Training Epoch 7] Batch 3518, Loss 0.25299519300460815\n",
      "[Training Epoch 7] Batch 3519, Loss 0.25511354207992554\n",
      "[Training Epoch 7] Batch 3520, Loss 0.2646406888961792\n",
      "[Training Epoch 7] Batch 3521, Loss 0.2811325192451477\n",
      "[Training Epoch 7] Batch 3522, Loss 0.24422608315944672\n",
      "[Training Epoch 7] Batch 3523, Loss 0.2594389021396637\n",
      "[Training Epoch 7] Batch 3524, Loss 0.27480560541152954\n",
      "[Training Epoch 7] Batch 3525, Loss 0.27430737018585205\n",
      "[Training Epoch 7] Batch 3526, Loss 0.2809463441371918\n",
      "[Training Epoch 7] Batch 3527, Loss 0.26053252816200256\n",
      "[Training Epoch 7] Batch 3528, Loss 0.2570422887802124\n",
      "[Training Epoch 7] Batch 3529, Loss 0.24703294038772583\n",
      "[Training Epoch 7] Batch 3530, Loss 0.26885610818862915\n",
      "[Training Epoch 7] Batch 3531, Loss 0.2659018337726593\n",
      "[Training Epoch 7] Batch 3532, Loss 0.24141541123390198\n",
      "[Training Epoch 7] Batch 3533, Loss 0.2479402720928192\n",
      "[Training Epoch 7] Batch 3534, Loss 0.21783511340618134\n",
      "[Training Epoch 7] Batch 3535, Loss 0.23712462186813354\n",
      "[Training Epoch 7] Batch 3536, Loss 0.24523833394050598\n",
      "[Training Epoch 7] Batch 3537, Loss 0.2662149667739868\n",
      "[Training Epoch 7] Batch 3538, Loss 0.24501240253448486\n",
      "[Training Epoch 7] Batch 3539, Loss 0.23233626782894135\n",
      "[Training Epoch 7] Batch 3540, Loss 0.2510010004043579\n",
      "[Training Epoch 7] Batch 3541, Loss 0.2647486925125122\n",
      "[Training Epoch 7] Batch 3542, Loss 0.25492626428604126\n",
      "[Training Epoch 7] Batch 3543, Loss 0.248142272233963\n",
      "[Training Epoch 7] Batch 3544, Loss 0.2516072392463684\n",
      "[Training Epoch 7] Batch 3545, Loss 0.2903839349746704\n",
      "[Training Epoch 7] Batch 3546, Loss 0.2362157702445984\n",
      "[Training Epoch 7] Batch 3547, Loss 0.2556397318840027\n",
      "[Training Epoch 7] Batch 3548, Loss 0.23195131123065948\n",
      "[Training Epoch 7] Batch 3549, Loss 0.2690035104751587\n",
      "[Training Epoch 7] Batch 3550, Loss 0.2545512914657593\n",
      "[Training Epoch 7] Batch 3551, Loss 0.267605721950531\n",
      "[Training Epoch 7] Batch 3552, Loss 0.2666139006614685\n",
      "[Training Epoch 7] Batch 3553, Loss 0.25784194469451904\n",
      "[Training Epoch 7] Batch 3554, Loss 0.2733078598976135\n",
      "[Training Epoch 7] Batch 3555, Loss 0.2710198760032654\n",
      "[Training Epoch 7] Batch 3556, Loss 0.25869467854499817\n",
      "[Training Epoch 7] Batch 3557, Loss 0.25075316429138184\n",
      "[Training Epoch 7] Batch 3558, Loss 0.2640600800514221\n",
      "[Training Epoch 7] Batch 3559, Loss 0.24818386137485504\n",
      "[Training Epoch 7] Batch 3560, Loss 0.26249486207962036\n",
      "[Training Epoch 7] Batch 3561, Loss 0.24878518283367157\n",
      "[Training Epoch 7] Batch 3562, Loss 0.25469696521759033\n",
      "[Training Epoch 7] Batch 3563, Loss 0.25397685170173645\n",
      "[Training Epoch 7] Batch 3564, Loss 0.273715615272522\n",
      "[Training Epoch 7] Batch 3565, Loss 0.2414395809173584\n",
      "[Training Epoch 7] Batch 3566, Loss 0.24472300708293915\n",
      "[Training Epoch 7] Batch 3567, Loss 0.2782423496246338\n",
      "[Training Epoch 7] Batch 3568, Loss 0.24399521946907043\n",
      "[Training Epoch 7] Batch 3569, Loss 0.246840700507164\n",
      "[Training Epoch 7] Batch 3570, Loss 0.2993326783180237\n",
      "[Training Epoch 7] Batch 3571, Loss 0.2392004281282425\n",
      "[Training Epoch 7] Batch 3572, Loss 0.24770139157772064\n",
      "[Training Epoch 7] Batch 3573, Loss 0.2517865300178528\n",
      "[Training Epoch 7] Batch 3574, Loss 0.2963756322860718\n",
      "[Training Epoch 7] Batch 3575, Loss 0.2569183111190796\n",
      "[Training Epoch 7] Batch 3576, Loss 0.24831850826740265\n",
      "[Training Epoch 7] Batch 3577, Loss 0.27248042821884155\n",
      "[Training Epoch 7] Batch 3578, Loss 0.24481108784675598\n",
      "[Training Epoch 7] Batch 3579, Loss 0.2647397518157959\n",
      "[Training Epoch 7] Batch 3580, Loss 0.25653308629989624\n",
      "[Training Epoch 7] Batch 3581, Loss 0.24915480613708496\n",
      "[Training Epoch 7] Batch 3582, Loss 0.2396600991487503\n",
      "[Training Epoch 7] Batch 3583, Loss 0.28966933488845825\n",
      "[Training Epoch 7] Batch 3584, Loss 0.26335281133651733\n",
      "[Training Epoch 7] Batch 3585, Loss 0.24743236601352692\n",
      "[Training Epoch 7] Batch 3586, Loss 0.24113553762435913\n",
      "[Training Epoch 7] Batch 3587, Loss 0.26056209206581116\n",
      "[Training Epoch 7] Batch 3588, Loss 0.24771785736083984\n",
      "[Training Epoch 7] Batch 3589, Loss 0.2563779950141907\n",
      "[Training Epoch 7] Batch 3590, Loss 0.2426740527153015\n",
      "[Training Epoch 7] Batch 3591, Loss 0.28424710035324097\n",
      "[Training Epoch 7] Batch 3592, Loss 0.25117090344429016\n",
      "[Training Epoch 7] Batch 3593, Loss 0.2739846110343933\n",
      "[Training Epoch 7] Batch 3594, Loss 0.24368952214717865\n",
      "[Training Epoch 7] Batch 3595, Loss 0.24604663252830505\n",
      "[Training Epoch 7] Batch 3596, Loss 0.2506314218044281\n",
      "[Training Epoch 7] Batch 3597, Loss 0.2750311493873596\n",
      "[Training Epoch 7] Batch 3598, Loss 0.2616095244884491\n",
      "[Training Epoch 7] Batch 3599, Loss 0.2580644488334656\n",
      "[Training Epoch 7] Batch 3600, Loss 0.23327425122261047\n",
      "[Training Epoch 7] Batch 3601, Loss 0.2417069971561432\n",
      "[Training Epoch 7] Batch 3602, Loss 0.2559790313243866\n",
      "[Training Epoch 7] Batch 3603, Loss 0.28643372654914856\n",
      "[Training Epoch 7] Batch 3604, Loss 0.25901883840560913\n",
      "[Training Epoch 7] Batch 3605, Loss 0.2887803316116333\n",
      "[Training Epoch 7] Batch 3606, Loss 0.26038798689842224\n",
      "[Training Epoch 7] Batch 3607, Loss 0.2265579253435135\n",
      "[Training Epoch 7] Batch 3608, Loss 0.2847819924354553\n",
      "[Training Epoch 7] Batch 3609, Loss 0.25023066997528076\n",
      "[Training Epoch 7] Batch 3610, Loss 0.2566602826118469\n",
      "[Training Epoch 7] Batch 3611, Loss 0.25003546476364136\n",
      "[Training Epoch 7] Batch 3612, Loss 0.2522094249725342\n",
      "[Training Epoch 7] Batch 3613, Loss 0.24961447715759277\n",
      "[Training Epoch 7] Batch 3614, Loss 0.2531334161758423\n",
      "[Training Epoch 7] Batch 3615, Loss 0.2527674436569214\n",
      "[Training Epoch 7] Batch 3616, Loss 0.29859018325805664\n",
      "[Training Epoch 7] Batch 3617, Loss 0.27018922567367554\n",
      "[Training Epoch 7] Batch 3618, Loss 0.22598999738693237\n",
      "[Training Epoch 7] Batch 3619, Loss 0.2653757929801941\n",
      "[Training Epoch 7] Batch 3620, Loss 0.24359610676765442\n",
      "[Training Epoch 7] Batch 3621, Loss 0.25650912523269653\n",
      "[Training Epoch 7] Batch 3622, Loss 0.2333286553621292\n",
      "[Training Epoch 7] Batch 3623, Loss 0.23695597052574158\n",
      "[Training Epoch 7] Batch 3624, Loss 0.23535701632499695\n",
      "[Training Epoch 7] Batch 3625, Loss 0.28592681884765625\n",
      "[Training Epoch 7] Batch 3626, Loss 0.23649775981903076\n",
      "[Training Epoch 7] Batch 3627, Loss 0.24903544783592224\n",
      "[Training Epoch 7] Batch 3628, Loss 0.27348458766937256\n",
      "[Training Epoch 7] Batch 3629, Loss 0.23511585593223572\n",
      "[Training Epoch 7] Batch 3630, Loss 0.27702054381370544\n",
      "[Training Epoch 7] Batch 3631, Loss 0.24619737267494202\n",
      "[Training Epoch 7] Batch 3632, Loss 0.2597956657409668\n",
      "[Training Epoch 7] Batch 3633, Loss 0.24109983444213867\n",
      "[Training Epoch 7] Batch 3634, Loss 0.22281190752983093\n",
      "[Training Epoch 7] Batch 3635, Loss 0.2483326494693756\n",
      "[Training Epoch 7] Batch 3636, Loss 0.25795289874076843\n",
      "[Training Epoch 7] Batch 3637, Loss 0.2633518874645233\n",
      "[Training Epoch 7] Batch 3638, Loss 0.2668234705924988\n",
      "[Training Epoch 7] Batch 3639, Loss 0.2466617226600647\n",
      "[Training Epoch 7] Batch 3640, Loss 0.24951374530792236\n",
      "[Training Epoch 7] Batch 3641, Loss 0.2525550425052643\n",
      "[Training Epoch 7] Batch 3642, Loss 0.2567395567893982\n",
      "[Training Epoch 7] Batch 3643, Loss 0.25774991512298584\n",
      "[Training Epoch 7] Batch 3644, Loss 0.26886773109436035\n",
      "[Training Epoch 7] Batch 3645, Loss 0.269206166267395\n",
      "[Training Epoch 7] Batch 3646, Loss 0.26165610551834106\n",
      "[Training Epoch 7] Batch 3647, Loss 0.2358817458152771\n",
      "[Training Epoch 7] Batch 3648, Loss 0.25059008598327637\n",
      "[Training Epoch 7] Batch 3649, Loss 0.2809123396873474\n",
      "[Training Epoch 7] Batch 3650, Loss 0.24236907064914703\n",
      "[Training Epoch 7] Batch 3651, Loss 0.2696834206581116\n",
      "[Training Epoch 7] Batch 3652, Loss 0.2593875527381897\n",
      "[Training Epoch 7] Batch 3653, Loss 0.24966374039649963\n",
      "[Training Epoch 7] Batch 3654, Loss 0.27605491876602173\n",
      "[Training Epoch 7] Batch 3655, Loss 0.25530189275741577\n",
      "[Training Epoch 7] Batch 3656, Loss 0.2646760940551758\n",
      "[Training Epoch 7] Batch 3657, Loss 0.23311400413513184\n",
      "[Training Epoch 7] Batch 3658, Loss 0.24054881930351257\n",
      "[Training Epoch 7] Batch 3659, Loss 0.275747686624527\n",
      "[Training Epoch 7] Batch 3660, Loss 0.2681230902671814\n",
      "[Training Epoch 7] Batch 3661, Loss 0.27224647998809814\n",
      "[Training Epoch 7] Batch 3662, Loss 0.23243317008018494\n",
      "[Training Epoch 7] Batch 3663, Loss 0.24442079663276672\n",
      "[Training Epoch 7] Batch 3664, Loss 0.28933489322662354\n",
      "[Training Epoch 7] Batch 3665, Loss 0.2559168040752411\n",
      "[Training Epoch 7] Batch 3666, Loss 0.27229851484298706\n",
      "[Training Epoch 7] Batch 3667, Loss 0.25011390447616577\n",
      "[Training Epoch 7] Batch 3668, Loss 0.23494571447372437\n",
      "[Training Epoch 7] Batch 3669, Loss 0.24711985886096954\n",
      "[Training Epoch 7] Batch 3670, Loss 0.2623429298400879\n",
      "[Training Epoch 7] Batch 3671, Loss 0.2543001174926758\n",
      "[Training Epoch 7] Batch 3672, Loss 0.2579532265663147\n",
      "[Training Epoch 7] Batch 3673, Loss 0.24289776384830475\n",
      "[Training Epoch 7] Batch 3674, Loss 0.2638247609138489\n",
      "[Training Epoch 7] Batch 3675, Loss 0.2776469588279724\n",
      "[Training Epoch 7] Batch 3676, Loss 0.2585054934024811\n",
      "[Training Epoch 7] Batch 3677, Loss 0.24736888706684113\n",
      "[Training Epoch 7] Batch 3678, Loss 0.22127893567085266\n",
      "[Training Epoch 7] Batch 3679, Loss 0.24532873928546906\n",
      "[Training Epoch 7] Batch 3680, Loss 0.2517223358154297\n",
      "[Training Epoch 7] Batch 3681, Loss 0.25190916657447815\n",
      "[Training Epoch 7] Batch 3682, Loss 0.26908308267593384\n",
      "[Training Epoch 7] Batch 3683, Loss 0.2566068768501282\n",
      "[Training Epoch 7] Batch 3684, Loss 0.2407923936843872\n",
      "[Training Epoch 7] Batch 3685, Loss 0.2635354995727539\n",
      "[Training Epoch 7] Batch 3686, Loss 0.3014954924583435\n",
      "[Training Epoch 7] Batch 3687, Loss 0.25929003953933716\n",
      "[Training Epoch 7] Batch 3688, Loss 0.23797814548015594\n",
      "[Training Epoch 7] Batch 3689, Loss 0.2423509955406189\n",
      "[Training Epoch 7] Batch 3690, Loss 0.2476102113723755\n",
      "[Training Epoch 7] Batch 3691, Loss 0.28427451848983765\n",
      "[Training Epoch 7] Batch 3692, Loss 0.2278497964143753\n",
      "[Training Epoch 7] Batch 3693, Loss 0.26832810044288635\n",
      "[Training Epoch 7] Batch 3694, Loss 0.250676691532135\n",
      "[Training Epoch 7] Batch 3695, Loss 0.2537427544593811\n",
      "[Training Epoch 7] Batch 3696, Loss 0.2699506878852844\n",
      "[Training Epoch 7] Batch 3697, Loss 0.24463188648223877\n",
      "[Training Epoch 7] Batch 3698, Loss 0.28437602519989014\n",
      "[Training Epoch 7] Batch 3699, Loss 0.2506599426269531\n",
      "[Training Epoch 7] Batch 3700, Loss 0.27047163248062134\n",
      "[Training Epoch 7] Batch 3701, Loss 0.2288871556520462\n",
      "[Training Epoch 7] Batch 3702, Loss 0.22589536011219025\n",
      "[Training Epoch 7] Batch 3703, Loss 0.2462124228477478\n",
      "[Training Epoch 7] Batch 3704, Loss 0.251889705657959\n",
      "[Training Epoch 7] Batch 3705, Loss 0.2683368921279907\n",
      "[Training Epoch 7] Batch 3706, Loss 0.2360639125108719\n",
      "[Training Epoch 7] Batch 3707, Loss 0.25017818808555603\n",
      "[Training Epoch 7] Batch 3708, Loss 0.25954148173332214\n",
      "[Training Epoch 7] Batch 3709, Loss 0.27694350481033325\n",
      "[Training Epoch 7] Batch 3710, Loss 0.2722685635089874\n",
      "[Training Epoch 7] Batch 3711, Loss 0.27513882517814636\n",
      "[Training Epoch 7] Batch 3712, Loss 0.260041207075119\n",
      "[Training Epoch 7] Batch 3713, Loss 0.2908056080341339\n",
      "[Training Epoch 7] Batch 3714, Loss 0.22814181447029114\n",
      "[Training Epoch 7] Batch 3715, Loss 0.26103100180625916\n",
      "[Training Epoch 7] Batch 3716, Loss 0.2644404470920563\n",
      "[Training Epoch 7] Batch 3717, Loss 0.27617794275283813\n",
      "[Training Epoch 7] Batch 3718, Loss 0.25869083404541016\n",
      "[Training Epoch 7] Batch 3719, Loss 0.2777015268802643\n",
      "[Training Epoch 7] Batch 3720, Loss 0.25157079100608826\n",
      "[Training Epoch 7] Batch 3721, Loss 0.2833293676376343\n",
      "[Training Epoch 7] Batch 3722, Loss 0.25683334469795227\n",
      "[Training Epoch 7] Batch 3723, Loss 0.2482672929763794\n",
      "[Training Epoch 7] Batch 3724, Loss 0.2586510181427002\n",
      "[Training Epoch 7] Batch 3725, Loss 0.25404489040374756\n",
      "[Training Epoch 7] Batch 3726, Loss 0.2587185800075531\n",
      "[Training Epoch 7] Batch 3727, Loss 0.23981967568397522\n",
      "[Training Epoch 7] Batch 3728, Loss 0.27344173192977905\n",
      "[Training Epoch 7] Batch 3729, Loss 0.2467144876718521\n",
      "[Training Epoch 7] Batch 3730, Loss 0.25936129689216614\n",
      "[Training Epoch 7] Batch 3731, Loss 0.24883922934532166\n",
      "[Training Epoch 7] Batch 3732, Loss 0.23918545246124268\n",
      "[Training Epoch 7] Batch 3733, Loss 0.25097787380218506\n",
      "[Training Epoch 7] Batch 3734, Loss 0.2595489025115967\n",
      "[Training Epoch 7] Batch 3735, Loss 0.2548670172691345\n",
      "[Training Epoch 7] Batch 3736, Loss 0.23620980978012085\n",
      "[Training Epoch 7] Batch 3737, Loss 0.2572331428527832\n",
      "[Training Epoch 7] Batch 3738, Loss 0.28799721598625183\n",
      "[Training Epoch 7] Batch 3739, Loss 0.26083940267562866\n",
      "[Training Epoch 7] Batch 3740, Loss 0.2558485269546509\n",
      "[Training Epoch 7] Batch 3741, Loss 0.23876193165779114\n",
      "[Training Epoch 7] Batch 3742, Loss 0.2831532657146454\n",
      "[Training Epoch 7] Batch 3743, Loss 0.25115710496902466\n",
      "[Training Epoch 7] Batch 3744, Loss 0.24140030145645142\n",
      "[Training Epoch 7] Batch 3745, Loss 0.2663978636264801\n",
      "[Training Epoch 7] Batch 3746, Loss 0.2937825322151184\n",
      "[Training Epoch 7] Batch 3747, Loss 0.25641101598739624\n",
      "[Training Epoch 7] Batch 3748, Loss 0.23090866208076477\n",
      "[Training Epoch 7] Batch 3749, Loss 0.23695749044418335\n",
      "[Training Epoch 7] Batch 3750, Loss 0.2705599069595337\n",
      "[Training Epoch 7] Batch 3751, Loss 0.24791842699050903\n",
      "[Training Epoch 7] Batch 3752, Loss 0.27760380506515503\n",
      "[Training Epoch 7] Batch 3753, Loss 0.2615145146846771\n",
      "[Training Epoch 7] Batch 3754, Loss 0.2717041075229645\n",
      "[Training Epoch 7] Batch 3755, Loss 0.2570660710334778\n",
      "[Training Epoch 7] Batch 3756, Loss 0.24921302497386932\n",
      "[Training Epoch 7] Batch 3757, Loss 0.23633725941181183\n",
      "[Training Epoch 7] Batch 3758, Loss 0.24607053399085999\n",
      "[Training Epoch 7] Batch 3759, Loss 0.2905294895172119\n",
      "[Training Epoch 7] Batch 3760, Loss 0.2609217166900635\n",
      "[Training Epoch 7] Batch 3761, Loss 0.23074576258659363\n",
      "[Training Epoch 7] Batch 3762, Loss 0.23384179174900055\n",
      "[Training Epoch 7] Batch 3763, Loss 0.2720174491405487\n",
      "[Training Epoch 7] Batch 3764, Loss 0.2857620418071747\n",
      "[Training Epoch 7] Batch 3765, Loss 0.229536771774292\n",
      "[Training Epoch 7] Batch 3766, Loss 0.2724408507347107\n",
      "[Training Epoch 7] Batch 3767, Loss 0.26251885294914246\n",
      "[Training Epoch 7] Batch 3768, Loss 0.25034478306770325\n",
      "[Training Epoch 7] Batch 3769, Loss 0.29529839754104614\n",
      "[Training Epoch 7] Batch 3770, Loss 0.26957765221595764\n",
      "[Training Epoch 7] Batch 3771, Loss 0.24413126707077026\n",
      "[Training Epoch 7] Batch 3772, Loss 0.22363153100013733\n",
      "[Training Epoch 7] Batch 3773, Loss 0.21987879276275635\n",
      "[Training Epoch 7] Batch 3774, Loss 0.24704565107822418\n",
      "[Training Epoch 7] Batch 3775, Loss 0.2733743190765381\n",
      "[Training Epoch 7] Batch 3776, Loss 0.23886850476264954\n",
      "[Training Epoch 7] Batch 3777, Loss 0.24238981306552887\n",
      "[Training Epoch 7] Batch 3778, Loss 0.23077329993247986\n",
      "[Training Epoch 7] Batch 3779, Loss 0.2641523480415344\n",
      "[Training Epoch 7] Batch 3780, Loss 0.2277354896068573\n",
      "[Training Epoch 7] Batch 3781, Loss 0.2501456141471863\n",
      "[Training Epoch 7] Batch 3782, Loss 0.25985729694366455\n",
      "[Training Epoch 7] Batch 3783, Loss 0.2610052227973938\n",
      "[Training Epoch 7] Batch 3784, Loss 0.2655034363269806\n",
      "[Training Epoch 7] Batch 3785, Loss 0.2375783622264862\n",
      "[Training Epoch 7] Batch 3786, Loss 0.26195353269577026\n",
      "[Training Epoch 7] Batch 3787, Loss 0.252099871635437\n",
      "[Training Epoch 7] Batch 3788, Loss 0.25877493619918823\n",
      "[Training Epoch 7] Batch 3789, Loss 0.26629748940467834\n",
      "[Training Epoch 7] Batch 3790, Loss 0.25668996572494507\n",
      "[Training Epoch 7] Batch 3791, Loss 0.24533134698867798\n",
      "[Training Epoch 7] Batch 3792, Loss 0.268995076417923\n",
      "[Training Epoch 7] Batch 3793, Loss 0.27592408657073975\n",
      "[Training Epoch 7] Batch 3794, Loss 0.24477684497833252\n",
      "[Training Epoch 7] Batch 3795, Loss 0.23579928278923035\n",
      "[Training Epoch 7] Batch 3796, Loss 0.23624499142169952\n",
      "[Training Epoch 7] Batch 3797, Loss 0.2556849718093872\n",
      "[Training Epoch 7] Batch 3798, Loss 0.2222924679517746\n",
      "[Training Epoch 7] Batch 3799, Loss 0.22482389211654663\n",
      "[Training Epoch 7] Batch 3800, Loss 0.268070250749588\n",
      "[Training Epoch 7] Batch 3801, Loss 0.26315855979919434\n",
      "[Training Epoch 7] Batch 3802, Loss 0.2622298002243042\n",
      "[Training Epoch 7] Batch 3803, Loss 0.2517065703868866\n",
      "[Training Epoch 7] Batch 3804, Loss 0.2535284459590912\n",
      "[Training Epoch 7] Batch 3805, Loss 0.2677741050720215\n",
      "[Training Epoch 7] Batch 3806, Loss 0.2621997594833374\n",
      "[Training Epoch 7] Batch 3807, Loss 0.27009153366088867\n",
      "[Training Epoch 7] Batch 3808, Loss 0.25896957516670227\n",
      "[Training Epoch 7] Batch 3809, Loss 0.26288777589797974\n",
      "[Training Epoch 7] Batch 3810, Loss 0.2700878083705902\n",
      "[Training Epoch 7] Batch 3811, Loss 0.258365273475647\n",
      "[Training Epoch 7] Batch 3812, Loss 0.2579946517944336\n",
      "[Training Epoch 7] Batch 3813, Loss 0.22691166400909424\n",
      "[Training Epoch 7] Batch 3814, Loss 0.27939099073410034\n",
      "[Training Epoch 7] Batch 3815, Loss 0.2661798298358917\n",
      "[Training Epoch 7] Batch 3816, Loss 0.27897900342941284\n",
      "[Training Epoch 7] Batch 3817, Loss 0.22969093918800354\n",
      "[Training Epoch 7] Batch 3818, Loss 0.2551136314868927\n",
      "[Training Epoch 7] Batch 3819, Loss 0.2765340209007263\n",
      "[Training Epoch 7] Batch 3820, Loss 0.25909310579299927\n",
      "[Training Epoch 7] Batch 3821, Loss 0.26703059673309326\n",
      "[Training Epoch 7] Batch 3822, Loss 0.2759677767753601\n",
      "[Training Epoch 7] Batch 3823, Loss 0.2722528576850891\n",
      "[Training Epoch 7] Batch 3824, Loss 0.24677692353725433\n",
      "[Training Epoch 7] Batch 3825, Loss 0.2473902851343155\n",
      "[Training Epoch 7] Batch 3826, Loss 0.2436690628528595\n",
      "[Training Epoch 7] Batch 3827, Loss 0.2662395238876343\n",
      "[Training Epoch 7] Batch 3828, Loss 0.2490132451057434\n",
      "[Training Epoch 7] Batch 3829, Loss 0.24947655200958252\n",
      "[Training Epoch 7] Batch 3830, Loss 0.24778303503990173\n",
      "[Training Epoch 7] Batch 3831, Loss 0.25181272625923157\n",
      "[Training Epoch 7] Batch 3832, Loss 0.23674336075782776\n",
      "[Training Epoch 7] Batch 3833, Loss 0.26625579595565796\n",
      "[Training Epoch 7] Batch 3834, Loss 0.25853800773620605\n",
      "[Training Epoch 7] Batch 3835, Loss 0.2600247263908386\n",
      "[Training Epoch 7] Batch 3836, Loss 0.2462318241596222\n",
      "[Training Epoch 7] Batch 3837, Loss 0.26312750577926636\n",
      "[Training Epoch 7] Batch 3838, Loss 0.25415563583374023\n",
      "[Training Epoch 7] Batch 3839, Loss 0.26480525732040405\n",
      "[Training Epoch 7] Batch 3840, Loss 0.2556563913822174\n",
      "[Training Epoch 7] Batch 3841, Loss 0.29203516244888306\n",
      "[Training Epoch 7] Batch 3842, Loss 0.2707764506340027\n",
      "[Training Epoch 7] Batch 3843, Loss 0.2344115674495697\n",
      "[Training Epoch 7] Batch 3844, Loss 0.2493593394756317\n",
      "[Training Epoch 7] Batch 3845, Loss 0.2495170384645462\n",
      "[Training Epoch 7] Batch 3846, Loss 0.25318199396133423\n",
      "[Training Epoch 7] Batch 3847, Loss 0.2616267800331116\n",
      "[Training Epoch 7] Batch 3848, Loss 0.26274722814559937\n",
      "[Training Epoch 7] Batch 3849, Loss 0.2660747766494751\n",
      "[Training Epoch 7] Batch 3850, Loss 0.24739113450050354\n",
      "[Training Epoch 7] Batch 3851, Loss 0.2728731632232666\n",
      "[Training Epoch 7] Batch 3852, Loss 0.23673789203166962\n",
      "[Training Epoch 7] Batch 3853, Loss 0.25798654556274414\n",
      "[Training Epoch 7] Batch 3854, Loss 0.23090630769729614\n",
      "[Training Epoch 7] Batch 3855, Loss 0.2447177767753601\n",
      "[Training Epoch 7] Batch 3856, Loss 0.23080983757972717\n",
      "[Training Epoch 7] Batch 3857, Loss 0.26910001039505005\n",
      "[Training Epoch 7] Batch 3858, Loss 0.27213042974472046\n",
      "[Training Epoch 7] Batch 3859, Loss 0.2482936829328537\n",
      "[Training Epoch 7] Batch 3860, Loss 0.27149897813796997\n",
      "[Training Epoch 7] Batch 3861, Loss 0.27088791131973267\n",
      "[Training Epoch 7] Batch 3862, Loss 0.2432214617729187\n",
      "[Training Epoch 7] Batch 3863, Loss 0.25493159890174866\n",
      "[Training Epoch 7] Batch 3864, Loss 0.2771480679512024\n",
      "[Training Epoch 7] Batch 3865, Loss 0.2733129858970642\n",
      "[Training Epoch 7] Batch 3866, Loss 0.30984315276145935\n",
      "[Training Epoch 7] Batch 3867, Loss 0.2546480596065521\n",
      "[Training Epoch 7] Batch 3868, Loss 0.2813190221786499\n",
      "[Training Epoch 7] Batch 3869, Loss 0.25985440611839294\n",
      "[Training Epoch 7] Batch 3870, Loss 0.2497863471508026\n",
      "[Training Epoch 7] Batch 3871, Loss 0.24599449336528778\n",
      "[Training Epoch 7] Batch 3872, Loss 0.24491603672504425\n",
      "[Training Epoch 7] Batch 3873, Loss 0.2604852318763733\n",
      "[Training Epoch 7] Batch 3874, Loss 0.2729368805885315\n",
      "[Training Epoch 7] Batch 3875, Loss 0.2731543779373169\n",
      "[Training Epoch 7] Batch 3876, Loss 0.2648642659187317\n",
      "[Training Epoch 7] Batch 3877, Loss 0.24665093421936035\n",
      "[Training Epoch 7] Batch 3878, Loss 0.24937507510185242\n",
      "[Training Epoch 7] Batch 3879, Loss 0.25506219267845154\n",
      "[Training Epoch 7] Batch 3880, Loss 0.2736723721027374\n",
      "[Training Epoch 7] Batch 3881, Loss 0.2884061932563782\n",
      "[Training Epoch 7] Batch 3882, Loss 0.2631171941757202\n",
      "[Training Epoch 7] Batch 3883, Loss 0.2762581706047058\n",
      "[Training Epoch 7] Batch 3884, Loss 0.2794666290283203\n",
      "[Training Epoch 7] Batch 3885, Loss 0.2745012938976288\n",
      "[Training Epoch 7] Batch 3886, Loss 0.2757565379142761\n",
      "[Training Epoch 7] Batch 3887, Loss 0.24889618158340454\n",
      "[Training Epoch 7] Batch 3888, Loss 0.24818767607212067\n",
      "[Training Epoch 7] Batch 3889, Loss 0.24717344343662262\n",
      "[Training Epoch 7] Batch 3890, Loss 0.23742130398750305\n",
      "[Training Epoch 7] Batch 3891, Loss 0.25111520290374756\n",
      "[Training Epoch 7] Batch 3892, Loss 0.23477262258529663\n",
      "[Training Epoch 7] Batch 3893, Loss 0.2671966552734375\n",
      "[Training Epoch 7] Batch 3894, Loss 0.26480168104171753\n",
      "[Training Epoch 7] Batch 3895, Loss 0.2613305449485779\n",
      "[Training Epoch 7] Batch 3896, Loss 0.26749923825263977\n",
      "[Training Epoch 7] Batch 3897, Loss 0.2524697184562683\n",
      "[Training Epoch 7] Batch 3898, Loss 0.26112261414527893\n",
      "[Training Epoch 7] Batch 3899, Loss 0.2516902685165405\n",
      "[Training Epoch 7] Batch 3900, Loss 0.26231977343559265\n",
      "[Training Epoch 7] Batch 3901, Loss 0.26965731382369995\n",
      "[Training Epoch 7] Batch 3902, Loss 0.27248162031173706\n",
      "[Training Epoch 7] Batch 3903, Loss 0.24216610193252563\n",
      "[Training Epoch 7] Batch 3904, Loss 0.2569805979728699\n",
      "[Training Epoch 7] Batch 3905, Loss 0.26747146248817444\n",
      "[Training Epoch 7] Batch 3906, Loss 0.24051475524902344\n",
      "[Training Epoch 7] Batch 3907, Loss 0.2518063187599182\n",
      "[Training Epoch 7] Batch 3908, Loss 0.24861904978752136\n",
      "[Training Epoch 7] Batch 3909, Loss 0.2704434394836426\n",
      "[Training Epoch 7] Batch 3910, Loss 0.2620372474193573\n",
      "[Training Epoch 7] Batch 3911, Loss 0.2517257332801819\n",
      "[Training Epoch 7] Batch 3912, Loss 0.2664055824279785\n",
      "[Training Epoch 7] Batch 3913, Loss 0.24641242623329163\n",
      "[Training Epoch 7] Batch 3914, Loss 0.24346265196800232\n",
      "[Training Epoch 7] Batch 3915, Loss 0.26385781168937683\n",
      "[Training Epoch 7] Batch 3916, Loss 0.23511740565299988\n",
      "[Training Epoch 7] Batch 3917, Loss 0.25638481974601746\n",
      "[Training Epoch 7] Batch 3918, Loss 0.26816946268081665\n",
      "[Training Epoch 7] Batch 3919, Loss 0.24988332390785217\n",
      "[Training Epoch 7] Batch 3920, Loss 0.26137471199035645\n",
      "[Training Epoch 7] Batch 3921, Loss 0.24960806965827942\n",
      "[Training Epoch 7] Batch 3922, Loss 0.223202645778656\n",
      "[Training Epoch 7] Batch 3923, Loss 0.23811592161655426\n",
      "[Training Epoch 7] Batch 3924, Loss 0.2578234076499939\n",
      "[Training Epoch 7] Batch 3925, Loss 0.28994759917259216\n",
      "[Training Epoch 7] Batch 3926, Loss 0.25353431701660156\n",
      "[Training Epoch 7] Batch 3927, Loss 0.2670750916004181\n",
      "[Training Epoch 7] Batch 3928, Loss 0.2538995146751404\n",
      "[Training Epoch 7] Batch 3929, Loss 0.25172752141952515\n",
      "[Training Epoch 7] Batch 3930, Loss 0.28809353709220886\n",
      "[Training Epoch 7] Batch 3931, Loss 0.26601317524909973\n",
      "[Training Epoch 7] Batch 3932, Loss 0.2633248567581177\n",
      "[Training Epoch 7] Batch 3933, Loss 0.24054835736751556\n",
      "[Training Epoch 7] Batch 3934, Loss 0.24509668350219727\n",
      "[Training Epoch 7] Batch 3935, Loss 0.2532872259616852\n",
      "[Training Epoch 7] Batch 3936, Loss 0.25280940532684326\n",
      "[Training Epoch 7] Batch 3937, Loss 0.28894004225730896\n",
      "[Training Epoch 7] Batch 3938, Loss 0.24411390721797943\n",
      "[Training Epoch 7] Batch 3939, Loss 0.2473670244216919\n",
      "[Training Epoch 7] Batch 3940, Loss 0.247427299618721\n",
      "[Training Epoch 7] Batch 3941, Loss 0.2683282196521759\n",
      "[Training Epoch 7] Batch 3942, Loss 0.24171940982341766\n",
      "[Training Epoch 7] Batch 3943, Loss 0.2381581962108612\n",
      "[Training Epoch 7] Batch 3944, Loss 0.29859912395477295\n",
      "[Training Epoch 7] Batch 3945, Loss 0.2491992712020874\n",
      "[Training Epoch 7] Batch 3946, Loss 0.25614815950393677\n",
      "[Training Epoch 7] Batch 3947, Loss 0.21016162633895874\n",
      "[Training Epoch 7] Batch 3948, Loss 0.25209611654281616\n",
      "[Training Epoch 7] Batch 3949, Loss 0.23950931429862976\n",
      "[Training Epoch 7] Batch 3950, Loss 0.2513415217399597\n",
      "[Training Epoch 7] Batch 3951, Loss 0.24373401701450348\n",
      "[Training Epoch 7] Batch 3952, Loss 0.25810706615448\n",
      "[Training Epoch 7] Batch 3953, Loss 0.24507007002830505\n",
      "[Training Epoch 7] Batch 3954, Loss 0.22763708233833313\n",
      "[Training Epoch 7] Batch 3955, Loss 0.26913803815841675\n",
      "[Training Epoch 7] Batch 3956, Loss 0.2367640733718872\n",
      "[Training Epoch 7] Batch 3957, Loss 0.24679666757583618\n",
      "[Training Epoch 7] Batch 3958, Loss 0.2756918668746948\n",
      "[Training Epoch 7] Batch 3959, Loss 0.2685859799385071\n",
      "[Training Epoch 7] Batch 3960, Loss 0.276686429977417\n",
      "[Training Epoch 7] Batch 3961, Loss 0.250761479139328\n",
      "[Training Epoch 7] Batch 3962, Loss 0.22031012177467346\n",
      "[Training Epoch 7] Batch 3963, Loss 0.2699933350086212\n",
      "[Training Epoch 7] Batch 3964, Loss 0.2550618052482605\n",
      "[Training Epoch 7] Batch 3965, Loss 0.29278793931007385\n",
      "[Training Epoch 7] Batch 3966, Loss 0.2969232201576233\n",
      "[Training Epoch 7] Batch 3967, Loss 0.27752670645713806\n",
      "[Training Epoch 7] Batch 3968, Loss 0.23510423302650452\n",
      "[Training Epoch 7] Batch 3969, Loss 0.25112295150756836\n",
      "[Training Epoch 7] Batch 3970, Loss 0.26463979482650757\n",
      "[Training Epoch 7] Batch 3971, Loss 0.25482678413391113\n",
      "[Training Epoch 7] Batch 3972, Loss 0.27225497364997864\n",
      "[Training Epoch 7] Batch 3973, Loss 0.28394025564193726\n",
      "[Training Epoch 7] Batch 3974, Loss 0.23105257749557495\n",
      "[Training Epoch 7] Batch 3975, Loss 0.255453884601593\n",
      "[Training Epoch 7] Batch 3976, Loss 0.2508131265640259\n",
      "[Training Epoch 7] Batch 3977, Loss 0.28025680780410767\n",
      "[Training Epoch 7] Batch 3978, Loss 0.25462451577186584\n",
      "[Training Epoch 7] Batch 3979, Loss 0.27594277262687683\n",
      "[Training Epoch 7] Batch 3980, Loss 0.2612242102622986\n",
      "[Training Epoch 7] Batch 3981, Loss 0.24842897057533264\n",
      "[Training Epoch 7] Batch 3982, Loss 0.267361044883728\n",
      "[Training Epoch 7] Batch 3983, Loss 0.2540958523750305\n",
      "[Training Epoch 7] Batch 3984, Loss 0.25935763120651245\n",
      "[Training Epoch 7] Batch 3985, Loss 0.247264564037323\n",
      "[Training Epoch 7] Batch 3986, Loss 0.2680218517780304\n",
      "[Training Epoch 7] Batch 3987, Loss 0.24239152669906616\n",
      "[Training Epoch 7] Batch 3988, Loss 0.258535236120224\n",
      "[Training Epoch 7] Batch 3989, Loss 0.2641434371471405\n",
      "[Training Epoch 7] Batch 3990, Loss 0.24462848901748657\n",
      "[Training Epoch 7] Batch 3991, Loss 0.256412535905838\n",
      "[Training Epoch 7] Batch 3992, Loss 0.24757620692253113\n",
      "[Training Epoch 7] Batch 3993, Loss 0.24725231528282166\n",
      "[Training Epoch 7] Batch 3994, Loss 0.2821306884288788\n",
      "[Training Epoch 7] Batch 3995, Loss 0.27892255783081055\n",
      "[Training Epoch 7] Batch 3996, Loss 0.2545353174209595\n",
      "[Training Epoch 7] Batch 3997, Loss 0.26250043511390686\n",
      "[Training Epoch 7] Batch 3998, Loss 0.2544347941875458\n",
      "[Training Epoch 7] Batch 3999, Loss 0.2366970330476761\n",
      "[Training Epoch 7] Batch 4000, Loss 0.27290380001068115\n",
      "[Training Epoch 7] Batch 4001, Loss 0.2710511088371277\n",
      "[Training Epoch 7] Batch 4002, Loss 0.27271905541419983\n",
      "[Training Epoch 7] Batch 4003, Loss 0.273561030626297\n",
      "[Training Epoch 7] Batch 4004, Loss 0.2523946166038513\n",
      "[Training Epoch 7] Batch 4005, Loss 0.25573521852493286\n",
      "[Training Epoch 7] Batch 4006, Loss 0.2915164828300476\n",
      "[Training Epoch 7] Batch 4007, Loss 0.24722695350646973\n",
      "[Training Epoch 7] Batch 4008, Loss 0.27297234535217285\n",
      "[Training Epoch 7] Batch 4009, Loss 0.2491888403892517\n",
      "[Training Epoch 7] Batch 4010, Loss 0.2647705078125\n",
      "[Training Epoch 7] Batch 4011, Loss 0.27007442712783813\n",
      "[Training Epoch 7] Batch 4012, Loss 0.2538341283798218\n",
      "[Training Epoch 7] Batch 4013, Loss 0.2579832077026367\n",
      "[Training Epoch 7] Batch 4014, Loss 0.26045894622802734\n",
      "[Training Epoch 7] Batch 4015, Loss 0.24781212210655212\n",
      "[Training Epoch 7] Batch 4016, Loss 0.25945383310317993\n",
      "[Training Epoch 7] Batch 4017, Loss 0.2774420976638794\n",
      "[Training Epoch 7] Batch 4018, Loss 0.26953816413879395\n",
      "[Training Epoch 7] Batch 4019, Loss 0.2445756196975708\n",
      "[Training Epoch 7] Batch 4020, Loss 0.25861048698425293\n",
      "[Training Epoch 7] Batch 4021, Loss 0.29040729999542236\n",
      "[Training Epoch 7] Batch 4022, Loss 0.2678089737892151\n",
      "[Training Epoch 7] Batch 4023, Loss 0.2645331025123596\n",
      "[Training Epoch 7] Batch 4024, Loss 0.258472740650177\n",
      "[Training Epoch 7] Batch 4025, Loss 0.2569849193096161\n",
      "[Training Epoch 7] Batch 4026, Loss 0.25587761402130127\n",
      "[Training Epoch 7] Batch 4027, Loss 0.26125091314315796\n",
      "[Training Epoch 7] Batch 4028, Loss 0.23364479839801788\n",
      "[Training Epoch 7] Batch 4029, Loss 0.26588475704193115\n",
      "[Training Epoch 7] Batch 4030, Loss 0.2513585686683655\n",
      "[Training Epoch 7] Batch 4031, Loss 0.255102276802063\n",
      "[Training Epoch 7] Batch 4032, Loss 0.2577253580093384\n",
      "[Training Epoch 7] Batch 4033, Loss 0.24804577231407166\n",
      "[Training Epoch 7] Batch 4034, Loss 0.26832929253578186\n",
      "[Training Epoch 7] Batch 4035, Loss 0.25188151001930237\n",
      "[Training Epoch 7] Batch 4036, Loss 0.27725115418434143\n",
      "[Training Epoch 7] Batch 4037, Loss 0.24762262403964996\n",
      "[Training Epoch 7] Batch 4038, Loss 0.26617491245269775\n",
      "[Training Epoch 7] Batch 4039, Loss 0.25012996792793274\n",
      "[Training Epoch 7] Batch 4040, Loss 0.27335068583488464\n",
      "[Training Epoch 7] Batch 4041, Loss 0.2944506108760834\n",
      "[Training Epoch 7] Batch 4042, Loss 0.2872917950153351\n",
      "[Training Epoch 7] Batch 4043, Loss 0.24274678528308868\n",
      "[Training Epoch 7] Batch 4044, Loss 0.2777004837989807\n",
      "[Training Epoch 7] Batch 4045, Loss 0.24518223106861115\n",
      "[Training Epoch 7] Batch 4046, Loss 0.22646476328372955\n",
      "[Training Epoch 7] Batch 4047, Loss 0.2861022353172302\n",
      "[Training Epoch 7] Batch 4048, Loss 0.2632860541343689\n",
      "[Training Epoch 7] Batch 4049, Loss 0.276761531829834\n",
      "[Training Epoch 7] Batch 4050, Loss 0.2607467770576477\n",
      "[Training Epoch 7] Batch 4051, Loss 0.26962441205978394\n",
      "[Training Epoch 7] Batch 4052, Loss 0.24776971340179443\n",
      "[Training Epoch 7] Batch 4053, Loss 0.2698090374469757\n",
      "[Training Epoch 7] Batch 4054, Loss 0.2876913547515869\n",
      "[Training Epoch 7] Batch 4055, Loss 0.2724100351333618\n",
      "[Training Epoch 7] Batch 4056, Loss 0.22002282738685608\n",
      "[Training Epoch 7] Batch 4057, Loss 0.2558490037918091\n",
      "[Training Epoch 7] Batch 4058, Loss 0.25487005710601807\n",
      "[Training Epoch 7] Batch 4059, Loss 0.2814590334892273\n",
      "[Training Epoch 7] Batch 4060, Loss 0.26944205164909363\n",
      "[Training Epoch 7] Batch 4061, Loss 0.24605312943458557\n",
      "[Training Epoch 7] Batch 4062, Loss 0.263845831155777\n",
      "[Training Epoch 7] Batch 4063, Loss 0.29507631063461304\n",
      "[Training Epoch 7] Batch 4064, Loss 0.2757657766342163\n",
      "[Training Epoch 7] Batch 4065, Loss 0.2752443253993988\n",
      "[Training Epoch 7] Batch 4066, Loss 0.25992417335510254\n",
      "[Training Epoch 7] Batch 4067, Loss 0.2679622173309326\n",
      "[Training Epoch 7] Batch 4068, Loss 0.2496030628681183\n",
      "[Training Epoch 7] Batch 4069, Loss 0.3088448941707611\n",
      "[Training Epoch 7] Batch 4070, Loss 0.26671841740608215\n",
      "[Training Epoch 7] Batch 4071, Loss 0.2498762160539627\n",
      "[Training Epoch 7] Batch 4072, Loss 0.26674556732177734\n",
      "[Training Epoch 7] Batch 4073, Loss 0.275651216506958\n",
      "[Training Epoch 7] Batch 4074, Loss 0.2685130834579468\n",
      "[Training Epoch 7] Batch 4075, Loss 0.24869026243686676\n",
      "[Training Epoch 7] Batch 4076, Loss 0.23945268988609314\n",
      "[Training Epoch 7] Batch 4077, Loss 0.25630491971969604\n",
      "[Training Epoch 7] Batch 4078, Loss 0.2529584765434265\n",
      "[Training Epoch 7] Batch 4079, Loss 0.24606764316558838\n",
      "[Training Epoch 7] Batch 4080, Loss 0.25749850273132324\n",
      "[Training Epoch 7] Batch 4081, Loss 0.2397208958864212\n",
      "[Training Epoch 7] Batch 4082, Loss 0.2436976432800293\n",
      "[Training Epoch 7] Batch 4083, Loss 0.22545665502548218\n",
      "[Training Epoch 7] Batch 4084, Loss 0.22795429825782776\n",
      "[Training Epoch 7] Batch 4085, Loss 0.26170819997787476\n",
      "[Training Epoch 7] Batch 4086, Loss 0.2468264102935791\n",
      "[Training Epoch 7] Batch 4087, Loss 0.28097760677337646\n",
      "[Training Epoch 7] Batch 4088, Loss 0.24953089654445648\n",
      "[Training Epoch 7] Batch 4089, Loss 0.2381400316953659\n",
      "[Training Epoch 7] Batch 4090, Loss 0.21725650131702423\n",
      "[Training Epoch 7] Batch 4091, Loss 0.23709534108638763\n",
      "[Training Epoch 7] Batch 4092, Loss 0.20621255040168762\n",
      "[Training Epoch 7] Batch 4093, Loss 0.26983946561813354\n",
      "[Training Epoch 7] Batch 4094, Loss 0.2374492734670639\n",
      "[Training Epoch 7] Batch 4095, Loss 0.2623738646507263\n",
      "[Training Epoch 7] Batch 4096, Loss 0.2297290563583374\n",
      "[Training Epoch 7] Batch 4097, Loss 0.25765150785446167\n",
      "[Training Epoch 7] Batch 4098, Loss 0.2680288851261139\n",
      "[Training Epoch 7] Batch 4099, Loss 0.24048402905464172\n",
      "[Training Epoch 7] Batch 4100, Loss 0.2544190585613251\n",
      "[Training Epoch 7] Batch 4101, Loss 0.2603866755962372\n",
      "[Training Epoch 7] Batch 4102, Loss 0.23479115962982178\n",
      "[Training Epoch 7] Batch 4103, Loss 0.2635708749294281\n",
      "[Training Epoch 7] Batch 4104, Loss 0.2723100781440735\n",
      "[Training Epoch 7] Batch 4105, Loss 0.2671663165092468\n",
      "[Training Epoch 7] Batch 4106, Loss 0.23695875704288483\n",
      "[Training Epoch 7] Batch 4107, Loss 0.24744948744773865\n",
      "[Training Epoch 7] Batch 4108, Loss 0.28140243887901306\n",
      "[Training Epoch 7] Batch 4109, Loss 0.26360437273979187\n",
      "[Training Epoch 7] Batch 4110, Loss 0.2629941403865814\n",
      "[Training Epoch 7] Batch 4111, Loss 0.2602393627166748\n",
      "[Training Epoch 7] Batch 4112, Loss 0.257612943649292\n",
      "[Training Epoch 7] Batch 4113, Loss 0.2532835304737091\n",
      "[Training Epoch 7] Batch 4114, Loss 0.26598119735717773\n",
      "[Training Epoch 7] Batch 4115, Loss 0.2705531120300293\n",
      "[Training Epoch 7] Batch 4116, Loss 0.24353550374507904\n",
      "[Training Epoch 7] Batch 4117, Loss 0.2566559612751007\n",
      "[Training Epoch 7] Batch 4118, Loss 0.24502573907375336\n",
      "[Training Epoch 7] Batch 4119, Loss 0.25440698862075806\n",
      "[Training Epoch 7] Batch 4120, Loss 0.2645884156227112\n",
      "[Training Epoch 7] Batch 4121, Loss 0.24420166015625\n",
      "[Training Epoch 7] Batch 4122, Loss 0.2606624364852905\n",
      "[Training Epoch 7] Batch 4123, Loss 0.23836949467658997\n",
      "[Training Epoch 7] Batch 4124, Loss 0.258554071187973\n",
      "[Training Epoch 7] Batch 4125, Loss 0.24412810802459717\n",
      "[Training Epoch 7] Batch 4126, Loss 0.2545410394668579\n",
      "[Training Epoch 7] Batch 4127, Loss 0.2266625314950943\n",
      "[Training Epoch 7] Batch 4128, Loss 0.24055303633213043\n",
      "[Training Epoch 7] Batch 4129, Loss 0.2216797024011612\n",
      "[Training Epoch 7] Batch 4130, Loss 0.25154703855514526\n",
      "[Training Epoch 7] Batch 4131, Loss 0.2175247073173523\n",
      "[Training Epoch 7] Batch 4132, Loss 0.292123407125473\n",
      "[Training Epoch 7] Batch 4133, Loss 0.24537356197834015\n",
      "[Training Epoch 7] Batch 4134, Loss 0.3001997172832489\n",
      "[Training Epoch 7] Batch 4135, Loss 0.26039910316467285\n",
      "[Training Epoch 7] Batch 4136, Loss 0.20543411374092102\n",
      "[Training Epoch 7] Batch 4137, Loss 0.25075626373291016\n",
      "[Training Epoch 7] Batch 4138, Loss 0.2524867653846741\n",
      "[Training Epoch 7] Batch 4139, Loss 0.2656680941581726\n",
      "[Training Epoch 7] Batch 4140, Loss 0.2489902377128601\n",
      "[Training Epoch 7] Batch 4141, Loss 0.27396315336227417\n",
      "[Training Epoch 7] Batch 4142, Loss 0.22977501153945923\n",
      "[Training Epoch 7] Batch 4143, Loss 0.21703460812568665\n",
      "[Training Epoch 7] Batch 4144, Loss 0.23066072165966034\n",
      "[Training Epoch 7] Batch 4145, Loss 0.3027631938457489\n",
      "[Training Epoch 7] Batch 4146, Loss 0.2724841237068176\n",
      "[Training Epoch 7] Batch 4147, Loss 0.27168434858322144\n",
      "[Training Epoch 7] Batch 4148, Loss 0.26009681820869446\n",
      "[Training Epoch 7] Batch 4149, Loss 0.28542789816856384\n",
      "[Training Epoch 7] Batch 4150, Loss 0.2634958028793335\n",
      "[Training Epoch 7] Batch 4151, Loss 0.2619912624359131\n",
      "[Training Epoch 7] Batch 4152, Loss 0.2451813519001007\n",
      "[Training Epoch 7] Batch 4153, Loss 0.243874192237854\n",
      "[Training Epoch 7] Batch 4154, Loss 0.2317589968442917\n",
      "[Training Epoch 7] Batch 4155, Loss 0.2542317509651184\n",
      "[Training Epoch 7] Batch 4156, Loss 0.26541000604629517\n",
      "[Training Epoch 7] Batch 4157, Loss 0.2391122281551361\n",
      "[Training Epoch 7] Batch 4158, Loss 0.28719013929367065\n",
      "[Training Epoch 7] Batch 4159, Loss 0.25048720836639404\n",
      "[Training Epoch 7] Batch 4160, Loss 0.27235376834869385\n",
      "[Training Epoch 7] Batch 4161, Loss 0.26163458824157715\n",
      "[Training Epoch 7] Batch 4162, Loss 0.29889512062072754\n",
      "[Training Epoch 7] Batch 4163, Loss 0.26877808570861816\n",
      "[Training Epoch 7] Batch 4164, Loss 0.2754378020763397\n",
      "[Training Epoch 7] Batch 4165, Loss 0.23394477367401123\n",
      "[Training Epoch 7] Batch 4166, Loss 0.25014036893844604\n",
      "[Training Epoch 7] Batch 4167, Loss 0.2531754970550537\n",
      "[Training Epoch 7] Batch 4168, Loss 0.255110502243042\n",
      "[Training Epoch 7] Batch 4169, Loss 0.23290538787841797\n",
      "[Training Epoch 7] Batch 4170, Loss 0.25493356585502625\n",
      "[Training Epoch 7] Batch 4171, Loss 0.2827136516571045\n",
      "[Training Epoch 7] Batch 4172, Loss 0.2539404630661011\n",
      "[Training Epoch 7] Batch 4173, Loss 0.26496851444244385\n",
      "[Training Epoch 7] Batch 4174, Loss 0.2666025161743164\n",
      "[Training Epoch 7] Batch 4175, Loss 0.26528406143188477\n",
      "[Training Epoch 7] Batch 4176, Loss 0.24527424573898315\n",
      "[Training Epoch 7] Batch 4177, Loss 0.2725537419319153\n",
      "[Training Epoch 7] Batch 4178, Loss 0.2920779585838318\n",
      "[Training Epoch 7] Batch 4179, Loss 0.2633049488067627\n",
      "[Training Epoch 7] Batch 4180, Loss 0.264850914478302\n",
      "[Training Epoch 7] Batch 4181, Loss 0.278922438621521\n",
      "[Training Epoch 7] Batch 4182, Loss 0.2353902906179428\n",
      "[Training Epoch 7] Batch 4183, Loss 0.2723797559738159\n",
      "[Training Epoch 7] Batch 4184, Loss 0.23541580140590668\n",
      "[Training Epoch 7] Batch 4185, Loss 0.2601264715194702\n",
      "[Training Epoch 7] Batch 4186, Loss 0.2669677734375\n",
      "[Training Epoch 7] Batch 4187, Loss 0.2796454429626465\n",
      "[Training Epoch 7] Batch 4188, Loss 0.2708086669445038\n",
      "[Training Epoch 7] Batch 4189, Loss 0.25367939472198486\n",
      "[Training Epoch 7] Batch 4190, Loss 0.22995918989181519\n",
      "[Training Epoch 7] Batch 4191, Loss 0.2492336928844452\n",
      "[Training Epoch 7] Batch 4192, Loss 0.23344764113426208\n",
      "[Training Epoch 7] Batch 4193, Loss 0.2430570423603058\n",
      "[Training Epoch 7] Batch 4194, Loss 0.26852232217788696\n",
      "[Training Epoch 7] Batch 4195, Loss 0.26527875661849976\n",
      "[Training Epoch 7] Batch 4196, Loss 0.2636708617210388\n",
      "[Training Epoch 7] Batch 4197, Loss 0.2846108675003052\n",
      "[Training Epoch 7] Batch 4198, Loss 0.2625931203365326\n",
      "[Training Epoch 7] Batch 4199, Loss 0.24599096179008484\n",
      "[Training Epoch 7] Batch 4200, Loss 0.2601228952407837\n",
      "[Training Epoch 7] Batch 4201, Loss 0.27626198530197144\n",
      "[Training Epoch 7] Batch 4202, Loss 0.2688221037387848\n",
      "[Training Epoch 7] Batch 4203, Loss 0.28097546100616455\n",
      "[Training Epoch 7] Batch 4204, Loss 0.270161509513855\n",
      "[Training Epoch 7] Batch 4205, Loss 0.2561228573322296\n",
      "[Training Epoch 7] Batch 4206, Loss 0.24925455451011658\n",
      "[Training Epoch 7] Batch 4207, Loss 0.2554948031902313\n",
      "[Training Epoch 7] Batch 4208, Loss 0.27703410387039185\n",
      "[Training Epoch 7] Batch 4209, Loss 0.2701318860054016\n",
      "[Training Epoch 7] Batch 4210, Loss 0.26784849166870117\n",
      "[Training Epoch 7] Batch 4211, Loss 0.2586473822593689\n",
      "[Training Epoch 7] Batch 4212, Loss 0.288612961769104\n",
      "[Training Epoch 7] Batch 4213, Loss 0.26393115520477295\n",
      "[Training Epoch 7] Batch 4214, Loss 0.24829480051994324\n",
      "[Training Epoch 7] Batch 4215, Loss 0.2571471333503723\n",
      "[Training Epoch 7] Batch 4216, Loss 0.27202165126800537\n",
      "[Training Epoch 7] Batch 4217, Loss 0.2586056590080261\n",
      "[Training Epoch 7] Batch 4218, Loss 0.28627538681030273\n",
      "[Training Epoch 7] Batch 4219, Loss 0.25275325775146484\n",
      "[Training Epoch 7] Batch 4220, Loss 0.24385443329811096\n",
      "[Training Epoch 7] Batch 4221, Loss 0.2588057518005371\n",
      "[Training Epoch 7] Batch 4222, Loss 0.24903593957424164\n",
      "[Training Epoch 7] Batch 4223, Loss 0.2761841416358948\n",
      "[Training Epoch 7] Batch 4224, Loss 0.2661886215209961\n",
      "[Training Epoch 7] Batch 4225, Loss 0.24471139907836914\n",
      "[Training Epoch 7] Batch 4226, Loss 0.22338181734085083\n",
      "[Training Epoch 7] Batch 4227, Loss 0.2567891478538513\n",
      "[Training Epoch 7] Batch 4228, Loss 0.26524031162261963\n",
      "[Training Epoch 7] Batch 4229, Loss 0.24378693103790283\n",
      "[Training Epoch 7] Batch 4230, Loss 0.2779025435447693\n",
      "[Training Epoch 7] Batch 4231, Loss 0.2704518437385559\n",
      "[Training Epoch 7] Batch 4232, Loss 0.2838289141654968\n",
      "[Training Epoch 7] Batch 4233, Loss 0.22235356271266937\n",
      "[Training Epoch 7] Batch 4234, Loss 0.25720077753067017\n",
      "[Training Epoch 7] Batch 4235, Loss 0.2821444869041443\n",
      "[Training Epoch 7] Batch 4236, Loss 0.24918729066848755\n",
      "[Training Epoch 7] Batch 4237, Loss 0.26098188757896423\n",
      "[Training Epoch 7] Batch 4238, Loss 0.2635197043418884\n",
      "[Training Epoch 7] Batch 4239, Loss 0.24689176678657532\n",
      "[Training Epoch 7] Batch 4240, Loss 0.2592206597328186\n",
      "[Training Epoch 7] Batch 4241, Loss 0.2393178790807724\n",
      "[Training Epoch 7] Batch 4242, Loss 0.26016128063201904\n",
      "[Training Epoch 7] Batch 4243, Loss 0.2369309663772583\n",
      "[Training Epoch 7] Batch 4244, Loss 0.2836650013923645\n",
      "[Training Epoch 7] Batch 4245, Loss 0.24428662657737732\n",
      "[Training Epoch 7] Batch 4246, Loss 0.24569378793239594\n",
      "[Training Epoch 7] Batch 4247, Loss 0.2490381896495819\n",
      "[Training Epoch 7] Batch 4248, Loss 0.22698980569839478\n",
      "[Training Epoch 7] Batch 4249, Loss 0.2571059465408325\n",
      "[Training Epoch 7] Batch 4250, Loss 0.2547406256198883\n",
      "[Training Epoch 7] Batch 4251, Loss 0.2847762703895569\n",
      "[Training Epoch 7] Batch 4252, Loss 0.25806862115859985\n",
      "[Training Epoch 7] Batch 4253, Loss 0.2478727251291275\n",
      "[Training Epoch 7] Batch 4254, Loss 0.26590198278427124\n",
      "[Training Epoch 7] Batch 4255, Loss 0.2686788737773895\n",
      "[Training Epoch 7] Batch 4256, Loss 0.2401503026485443\n",
      "[Training Epoch 7] Batch 4257, Loss 0.2678070068359375\n",
      "[Training Epoch 7] Batch 4258, Loss 0.24175286293029785\n",
      "[Training Epoch 7] Batch 4259, Loss 0.2592337131500244\n",
      "[Training Epoch 7] Batch 4260, Loss 0.249294251203537\n",
      "[Training Epoch 7] Batch 4261, Loss 0.2553713321685791\n",
      "[Training Epoch 7] Batch 4262, Loss 0.24986642599105835\n",
      "[Training Epoch 7] Batch 4263, Loss 0.26222074031829834\n",
      "[Training Epoch 7] Batch 4264, Loss 0.23574785888195038\n",
      "[Training Epoch 7] Batch 4265, Loss 0.2596035301685333\n",
      "[Training Epoch 7] Batch 4266, Loss 0.2593551278114319\n",
      "[Training Epoch 7] Batch 4267, Loss 0.26861271262168884\n",
      "[Training Epoch 7] Batch 4268, Loss 0.2790526747703552\n",
      "[Training Epoch 7] Batch 4269, Loss 0.2654028832912445\n",
      "[Training Epoch 7] Batch 4270, Loss 0.22381460666656494\n",
      "[Training Epoch 7] Batch 4271, Loss 0.27734375\n",
      "[Training Epoch 7] Batch 4272, Loss 0.2593165338039398\n",
      "[Training Epoch 7] Batch 4273, Loss 0.27115151286125183\n",
      "[Training Epoch 7] Batch 4274, Loss 0.26344752311706543\n",
      "[Training Epoch 7] Batch 4275, Loss 0.2756141722202301\n",
      "[Training Epoch 7] Batch 4276, Loss 0.28203123807907104\n",
      "[Training Epoch 7] Batch 4277, Loss 0.2526431679725647\n",
      "[Training Epoch 7] Batch 4278, Loss 0.2481093853712082\n",
      "[Training Epoch 7] Batch 4279, Loss 0.2895193099975586\n",
      "[Training Epoch 7] Batch 4280, Loss 0.2516307830810547\n",
      "[Training Epoch 7] Batch 4281, Loss 0.2743430733680725\n",
      "[Training Epoch 7] Batch 4282, Loss 0.25410765409469604\n",
      "[Training Epoch 7] Batch 4283, Loss 0.2785811424255371\n",
      "[Training Epoch 7] Batch 4284, Loss 0.28913336992263794\n",
      "[Training Epoch 7] Batch 4285, Loss 0.258630633354187\n",
      "[Training Epoch 7] Batch 4286, Loss 0.26457709074020386\n",
      "[Training Epoch 7] Batch 4287, Loss 0.26778894662857056\n",
      "[Training Epoch 7] Batch 4288, Loss 0.29958099126815796\n",
      "[Training Epoch 7] Batch 4289, Loss 0.27970483899116516\n",
      "[Training Epoch 7] Batch 4290, Loss 0.26564085483551025\n",
      "[Training Epoch 7] Batch 4291, Loss 0.2324744462966919\n",
      "[Training Epoch 7] Batch 4292, Loss 0.2740042209625244\n",
      "[Training Epoch 7] Batch 4293, Loss 0.2573968470096588\n",
      "[Training Epoch 7] Batch 4294, Loss 0.27404677867889404\n",
      "[Training Epoch 7] Batch 4295, Loss 0.24200104176998138\n",
      "[Training Epoch 7] Batch 4296, Loss 0.250251829624176\n",
      "[Training Epoch 7] Batch 4297, Loss 0.2618393898010254\n",
      "[Training Epoch 7] Batch 4298, Loss 0.3124694526195526\n",
      "[Training Epoch 7] Batch 4299, Loss 0.2620031237602234\n",
      "[Training Epoch 7] Batch 4300, Loss 0.23354938626289368\n",
      "[Training Epoch 7] Batch 4301, Loss 0.27703148126602173\n",
      "[Training Epoch 7] Batch 4302, Loss 0.24853944778442383\n",
      "[Training Epoch 7] Batch 4303, Loss 0.25844594836235046\n",
      "[Training Epoch 7] Batch 4304, Loss 0.2475634515285492\n",
      "[Training Epoch 7] Batch 4305, Loss 0.2549331784248352\n",
      "[Training Epoch 7] Batch 4306, Loss 0.24853001534938812\n",
      "[Training Epoch 7] Batch 4307, Loss 0.2457566112279892\n",
      "[Training Epoch 7] Batch 4308, Loss 0.2476423680782318\n",
      "[Training Epoch 7] Batch 4309, Loss 0.25713998079299927\n",
      "[Training Epoch 7] Batch 4310, Loss 0.27176666259765625\n",
      "[Training Epoch 7] Batch 4311, Loss 0.25793617963790894\n",
      "[Training Epoch 7] Batch 4312, Loss 0.24510717391967773\n",
      "[Training Epoch 7] Batch 4313, Loss 0.24449855089187622\n",
      "[Training Epoch 7] Batch 4314, Loss 0.318040132522583\n",
      "[Training Epoch 7] Batch 4315, Loss 0.24870795011520386\n",
      "[Training Epoch 7] Batch 4316, Loss 0.2740780711174011\n",
      "[Training Epoch 7] Batch 4317, Loss 0.26096853613853455\n",
      "[Training Epoch 7] Batch 4318, Loss 0.2551305890083313\n",
      "[Training Epoch 7] Batch 4319, Loss 0.2641095519065857\n",
      "[Training Epoch 7] Batch 4320, Loss 0.24154040217399597\n",
      "[Training Epoch 7] Batch 4321, Loss 0.260612428188324\n",
      "[Training Epoch 7] Batch 4322, Loss 0.26331406831741333\n",
      "[Training Epoch 7] Batch 4323, Loss 0.29174208641052246\n",
      "[Training Epoch 7] Batch 4324, Loss 0.2288626730442047\n",
      "[Training Epoch 7] Batch 4325, Loss 0.24231350421905518\n",
      "[Training Epoch 7] Batch 4326, Loss 0.2431594282388687\n",
      "[Training Epoch 7] Batch 4327, Loss 0.22050298750400543\n",
      "[Training Epoch 7] Batch 4328, Loss 0.2497115135192871\n",
      "[Training Epoch 7] Batch 4329, Loss 0.24594195187091827\n",
      "[Training Epoch 7] Batch 4330, Loss 0.2512807846069336\n",
      "[Training Epoch 7] Batch 4331, Loss 0.2756953239440918\n",
      "[Training Epoch 7] Batch 4332, Loss 0.2824646234512329\n",
      "[Training Epoch 7] Batch 4333, Loss 0.26112616062164307\n",
      "[Training Epoch 7] Batch 4334, Loss 0.24924112856388092\n",
      "[Training Epoch 7] Batch 4335, Loss 0.28116098046302795\n",
      "[Training Epoch 7] Batch 4336, Loss 0.23725375533103943\n",
      "[Training Epoch 7] Batch 4337, Loss 0.26147669553756714\n",
      "[Training Epoch 7] Batch 4338, Loss 0.263701856136322\n",
      "[Training Epoch 7] Batch 4339, Loss 0.22784094512462616\n",
      "[Training Epoch 7] Batch 4340, Loss 0.27575254440307617\n",
      "[Training Epoch 7] Batch 4341, Loss 0.24967218935489655\n",
      "[Training Epoch 7] Batch 4342, Loss 0.28207147121429443\n",
      "[Training Epoch 7] Batch 4343, Loss 0.23991814255714417\n",
      "[Training Epoch 7] Batch 4344, Loss 0.27778202295303345\n",
      "[Training Epoch 7] Batch 4345, Loss 0.24687272310256958\n",
      "[Training Epoch 7] Batch 4346, Loss 0.2561222016811371\n",
      "[Training Epoch 7] Batch 4347, Loss 0.24045710265636444\n",
      "[Training Epoch 7] Batch 4348, Loss 0.257709264755249\n",
      "[Training Epoch 7] Batch 4349, Loss 0.26174116134643555\n",
      "[Training Epoch 7] Batch 4350, Loss 0.27461332082748413\n",
      "[Training Epoch 7] Batch 4351, Loss 0.25543004274368286\n",
      "[Training Epoch 7] Batch 4352, Loss 0.27490049600601196\n",
      "[Training Epoch 7] Batch 4353, Loss 0.3019639253616333\n",
      "[Training Epoch 7] Batch 4354, Loss 0.2820861339569092\n",
      "[Training Epoch 7] Batch 4355, Loss 0.24780046939849854\n",
      "[Training Epoch 7] Batch 4356, Loss 0.27962547540664673\n",
      "[Training Epoch 7] Batch 4357, Loss 0.26148658990859985\n",
      "[Training Epoch 7] Batch 4358, Loss 0.24337847530841827\n",
      "[Training Epoch 7] Batch 4359, Loss 0.26441094279289246\n",
      "[Training Epoch 7] Batch 4360, Loss 0.2589682638645172\n",
      "[Training Epoch 7] Batch 4361, Loss 0.28688547015190125\n",
      "[Training Epoch 7] Batch 4362, Loss 0.27651679515838623\n",
      "[Training Epoch 7] Batch 4363, Loss 0.25782111287117004\n",
      "[Training Epoch 7] Batch 4364, Loss 0.27600958943367004\n",
      "[Training Epoch 7] Batch 4365, Loss 0.23896989226341248\n",
      "[Training Epoch 7] Batch 4366, Loss 0.2556443512439728\n",
      "[Training Epoch 7] Batch 4367, Loss 0.22432054579257965\n",
      "[Training Epoch 7] Batch 4368, Loss 0.2766752243041992\n",
      "[Training Epoch 7] Batch 4369, Loss 0.2827887535095215\n",
      "[Training Epoch 7] Batch 4370, Loss 0.24668994545936584\n",
      "[Training Epoch 7] Batch 4371, Loss 0.21580056846141815\n",
      "[Training Epoch 7] Batch 4372, Loss 0.24325495958328247\n",
      "[Training Epoch 7] Batch 4373, Loss 0.2862735688686371\n",
      "[Training Epoch 7] Batch 4374, Loss 0.2510913014411926\n",
      "[Training Epoch 7] Batch 4375, Loss 0.26813656091690063\n",
      "[Training Epoch 7] Batch 4376, Loss 0.2780954837799072\n",
      "[Training Epoch 7] Batch 4377, Loss 0.2435791939496994\n",
      "[Training Epoch 7] Batch 4378, Loss 0.2548089623451233\n",
      "[Training Epoch 7] Batch 4379, Loss 0.2467094361782074\n",
      "[Training Epoch 7] Batch 4380, Loss 0.26911669969558716\n",
      "[Training Epoch 7] Batch 4381, Loss 0.3044947385787964\n",
      "[Training Epoch 7] Batch 4382, Loss 0.22724725306034088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2253.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 7] Precision = 0.2618, Recall = 0.7740\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.24442420899868011\n",
      "[Training Epoch 8] Batch 1, Loss 0.2280518114566803\n",
      "[Training Epoch 8] Batch 2, Loss 0.270414799451828\n",
      "[Training Epoch 8] Batch 3, Loss 0.25201594829559326\n",
      "[Training Epoch 8] Batch 4, Loss 0.23222514986991882\n",
      "[Training Epoch 8] Batch 5, Loss 0.22619274258613586\n",
      "[Training Epoch 8] Batch 6, Loss 0.2465355098247528\n",
      "[Training Epoch 8] Batch 7, Loss 0.2548137903213501\n",
      "[Training Epoch 8] Batch 8, Loss 0.26101115345954895\n",
      "[Training Epoch 8] Batch 9, Loss 0.26516491174697876\n",
      "[Training Epoch 8] Batch 10, Loss 0.2525690197944641\n",
      "[Training Epoch 8] Batch 11, Loss 0.2738725244998932\n",
      "[Training Epoch 8] Batch 12, Loss 0.24929918348789215\n",
      "[Training Epoch 8] Batch 13, Loss 0.2749224901199341\n",
      "[Training Epoch 8] Batch 14, Loss 0.2607760727405548\n",
      "[Training Epoch 8] Batch 15, Loss 0.2563181519508362\n",
      "[Training Epoch 8] Batch 16, Loss 0.27388235926628113\n",
      "[Training Epoch 8] Batch 17, Loss 0.26328766345977783\n",
      "[Training Epoch 8] Batch 18, Loss 0.23785926401615143\n",
      "[Training Epoch 8] Batch 19, Loss 0.2468545138835907\n",
      "[Training Epoch 8] Batch 20, Loss 0.2757231295108795\n",
      "[Training Epoch 8] Batch 21, Loss 0.2624691128730774\n",
      "[Training Epoch 8] Batch 22, Loss 0.22247415781021118\n",
      "[Training Epoch 8] Batch 23, Loss 0.2636193633079529\n",
      "[Training Epoch 8] Batch 24, Loss 0.2463768720626831\n",
      "[Training Epoch 8] Batch 25, Loss 0.25915616750717163\n",
      "[Training Epoch 8] Batch 26, Loss 0.2651081383228302\n",
      "[Training Epoch 8] Batch 27, Loss 0.2702595293521881\n",
      "[Training Epoch 8] Batch 28, Loss 0.2751312255859375\n",
      "[Training Epoch 8] Batch 29, Loss 0.25968921184539795\n",
      "[Training Epoch 8] Batch 30, Loss 0.277551531791687\n",
      "[Training Epoch 8] Batch 31, Loss 0.24680505692958832\n",
      "[Training Epoch 8] Batch 32, Loss 0.2500351667404175\n",
      "[Training Epoch 8] Batch 33, Loss 0.23883576691150665\n",
      "[Training Epoch 8] Batch 34, Loss 0.2762324810028076\n",
      "[Training Epoch 8] Batch 35, Loss 0.24497327208518982\n",
      "[Training Epoch 8] Batch 36, Loss 0.2652168869972229\n",
      "[Training Epoch 8] Batch 37, Loss 0.24662336707115173\n",
      "[Training Epoch 8] Batch 38, Loss 0.26808127760887146\n",
      "[Training Epoch 8] Batch 39, Loss 0.2603764533996582\n",
      "[Training Epoch 8] Batch 40, Loss 0.27158135175704956\n",
      "[Training Epoch 8] Batch 41, Loss 0.255948007106781\n",
      "[Training Epoch 8] Batch 42, Loss 0.2692943811416626\n",
      "[Training Epoch 8] Batch 43, Loss 0.2675010561943054\n",
      "[Training Epoch 8] Batch 44, Loss 0.242741197347641\n",
      "[Training Epoch 8] Batch 45, Loss 0.2156677097082138\n",
      "[Training Epoch 8] Batch 46, Loss 0.2881599962711334\n",
      "[Training Epoch 8] Batch 47, Loss 0.24522532522678375\n",
      "[Training Epoch 8] Batch 48, Loss 0.2635248303413391\n",
      "[Training Epoch 8] Batch 49, Loss 0.2454446256160736\n",
      "[Training Epoch 8] Batch 50, Loss 0.25118476152420044\n",
      "[Training Epoch 8] Batch 51, Loss 0.2736247777938843\n",
      "[Training Epoch 8] Batch 52, Loss 0.23580336570739746\n",
      "[Training Epoch 8] Batch 53, Loss 0.26130905747413635\n",
      "[Training Epoch 8] Batch 54, Loss 0.2221408635377884\n",
      "[Training Epoch 8] Batch 55, Loss 0.258419930934906\n",
      "[Training Epoch 8] Batch 56, Loss 0.2476908266544342\n",
      "[Training Epoch 8] Batch 57, Loss 0.2192460596561432\n",
      "[Training Epoch 8] Batch 58, Loss 0.24501515924930573\n",
      "[Training Epoch 8] Batch 59, Loss 0.25316092371940613\n",
      "[Training Epoch 8] Batch 60, Loss 0.25113070011138916\n",
      "[Training Epoch 8] Batch 61, Loss 0.2622886896133423\n",
      "[Training Epoch 8] Batch 62, Loss 0.2620643973350525\n",
      "[Training Epoch 8] Batch 63, Loss 0.24272093176841736\n",
      "[Training Epoch 8] Batch 64, Loss 0.2201441526412964\n",
      "[Training Epoch 8] Batch 65, Loss 0.2666921019554138\n",
      "[Training Epoch 8] Batch 66, Loss 0.26586371660232544\n",
      "[Training Epoch 8] Batch 67, Loss 0.24341361224651337\n",
      "[Training Epoch 8] Batch 68, Loss 0.23699037730693817\n",
      "[Training Epoch 8] Batch 69, Loss 0.26032280921936035\n",
      "[Training Epoch 8] Batch 70, Loss 0.2284884750843048\n",
      "[Training Epoch 8] Batch 71, Loss 0.2080603390932083\n",
      "[Training Epoch 8] Batch 72, Loss 0.23371687531471252\n",
      "[Training Epoch 8] Batch 73, Loss 0.24540317058563232\n",
      "[Training Epoch 8] Batch 74, Loss 0.27642375230789185\n",
      "[Training Epoch 8] Batch 75, Loss 0.23409441113471985\n",
      "[Training Epoch 8] Batch 76, Loss 0.22587022185325623\n",
      "[Training Epoch 8] Batch 77, Loss 0.267630934715271\n",
      "[Training Epoch 8] Batch 78, Loss 0.24962210655212402\n",
      "[Training Epoch 8] Batch 79, Loss 0.26712101697921753\n",
      "[Training Epoch 8] Batch 80, Loss 0.2539183795452118\n",
      "[Training Epoch 8] Batch 81, Loss 0.24910204112529755\n",
      "[Training Epoch 8] Batch 82, Loss 0.25875478982925415\n",
      "[Training Epoch 8] Batch 83, Loss 0.2524721622467041\n",
      "[Training Epoch 8] Batch 84, Loss 0.2598908841609955\n",
      "[Training Epoch 8] Batch 85, Loss 0.294689416885376\n",
      "[Training Epoch 8] Batch 86, Loss 0.24378302693367004\n",
      "[Training Epoch 8] Batch 87, Loss 0.25146734714508057\n",
      "[Training Epoch 8] Batch 88, Loss 0.24444851279258728\n",
      "[Training Epoch 8] Batch 89, Loss 0.26192283630371094\n",
      "[Training Epoch 8] Batch 90, Loss 0.24646614491939545\n",
      "[Training Epoch 8] Batch 91, Loss 0.23283982276916504\n",
      "[Training Epoch 8] Batch 92, Loss 0.2411188930273056\n",
      "[Training Epoch 8] Batch 93, Loss 0.27068138122558594\n",
      "[Training Epoch 8] Batch 94, Loss 0.25024572014808655\n",
      "[Training Epoch 8] Batch 95, Loss 0.26900744438171387\n",
      "[Training Epoch 8] Batch 96, Loss 0.2755891978740692\n",
      "[Training Epoch 8] Batch 97, Loss 0.23634833097457886\n",
      "[Training Epoch 8] Batch 98, Loss 0.2417411506175995\n",
      "[Training Epoch 8] Batch 99, Loss 0.24878042936325073\n",
      "[Training Epoch 8] Batch 100, Loss 0.23479150235652924\n",
      "[Training Epoch 8] Batch 101, Loss 0.2676612138748169\n",
      "[Training Epoch 8] Batch 102, Loss 0.26311638951301575\n",
      "[Training Epoch 8] Batch 103, Loss 0.2590823173522949\n",
      "[Training Epoch 8] Batch 104, Loss 0.24714705348014832\n",
      "[Training Epoch 8] Batch 105, Loss 0.2599906027317047\n",
      "[Training Epoch 8] Batch 106, Loss 0.28042006492614746\n",
      "[Training Epoch 8] Batch 107, Loss 0.2422797530889511\n",
      "[Training Epoch 8] Batch 108, Loss 0.2624853849411011\n",
      "[Training Epoch 8] Batch 109, Loss 0.2578575611114502\n",
      "[Training Epoch 8] Batch 110, Loss 0.3000193238258362\n",
      "[Training Epoch 8] Batch 111, Loss 0.2478545904159546\n",
      "[Training Epoch 8] Batch 112, Loss 0.2585093379020691\n",
      "[Training Epoch 8] Batch 113, Loss 0.25275635719299316\n",
      "[Training Epoch 8] Batch 114, Loss 0.24351347982883453\n",
      "[Training Epoch 8] Batch 115, Loss 0.22993311285972595\n",
      "[Training Epoch 8] Batch 116, Loss 0.24541237950325012\n",
      "[Training Epoch 8] Batch 117, Loss 0.23212900757789612\n",
      "[Training Epoch 8] Batch 118, Loss 0.24673086404800415\n",
      "[Training Epoch 8] Batch 119, Loss 0.25832462310791016\n",
      "[Training Epoch 8] Batch 120, Loss 0.2459632158279419\n",
      "[Training Epoch 8] Batch 121, Loss 0.24705395102500916\n",
      "[Training Epoch 8] Batch 122, Loss 0.22828051447868347\n",
      "[Training Epoch 8] Batch 123, Loss 0.23543621599674225\n",
      "[Training Epoch 8] Batch 124, Loss 0.2327517867088318\n",
      "[Training Epoch 8] Batch 125, Loss 0.2506754994392395\n",
      "[Training Epoch 8] Batch 126, Loss 0.21925973892211914\n",
      "[Training Epoch 8] Batch 127, Loss 0.22912508249282837\n",
      "[Training Epoch 8] Batch 128, Loss 0.2858067750930786\n",
      "[Training Epoch 8] Batch 129, Loss 0.22752022743225098\n",
      "[Training Epoch 8] Batch 130, Loss 0.24418704211711884\n",
      "[Training Epoch 8] Batch 131, Loss 0.24514754116535187\n",
      "[Training Epoch 8] Batch 132, Loss 0.254263699054718\n",
      "[Training Epoch 8] Batch 133, Loss 0.22425195574760437\n",
      "[Training Epoch 8] Batch 134, Loss 0.2338978350162506\n",
      "[Training Epoch 8] Batch 135, Loss 0.26792192459106445\n",
      "[Training Epoch 8] Batch 136, Loss 0.25557929277420044\n",
      "[Training Epoch 8] Batch 137, Loss 0.2301345318555832\n",
      "[Training Epoch 8] Batch 138, Loss 0.26015612483024597\n",
      "[Training Epoch 8] Batch 139, Loss 0.2390158772468567\n",
      "[Training Epoch 8] Batch 140, Loss 0.25961458683013916\n",
      "[Training Epoch 8] Batch 141, Loss 0.25318193435668945\n",
      "[Training Epoch 8] Batch 142, Loss 0.2520207464694977\n",
      "[Training Epoch 8] Batch 143, Loss 0.23988205194473267\n",
      "[Training Epoch 8] Batch 144, Loss 0.2654591202735901\n",
      "[Training Epoch 8] Batch 145, Loss 0.2444738894701004\n",
      "[Training Epoch 8] Batch 146, Loss 0.2882024645805359\n",
      "[Training Epoch 8] Batch 147, Loss 0.26062917709350586\n",
      "[Training Epoch 8] Batch 148, Loss 0.2552095651626587\n",
      "[Training Epoch 8] Batch 149, Loss 0.2529509663581848\n",
      "[Training Epoch 8] Batch 150, Loss 0.27156662940979004\n",
      "[Training Epoch 8] Batch 151, Loss 0.2484116554260254\n",
      "[Training Epoch 8] Batch 152, Loss 0.25050222873687744\n",
      "[Training Epoch 8] Batch 153, Loss 0.23865529894828796\n",
      "[Training Epoch 8] Batch 154, Loss 0.24410030245780945\n",
      "[Training Epoch 8] Batch 155, Loss 0.25299376249313354\n",
      "[Training Epoch 8] Batch 156, Loss 0.23718389868736267\n",
      "[Training Epoch 8] Batch 157, Loss 0.25643235445022583\n",
      "[Training Epoch 8] Batch 158, Loss 0.2523188889026642\n",
      "[Training Epoch 8] Batch 159, Loss 0.26662689447402954\n",
      "[Training Epoch 8] Batch 160, Loss 0.25254493951797485\n",
      "[Training Epoch 8] Batch 161, Loss 0.24179062247276306\n",
      "[Training Epoch 8] Batch 162, Loss 0.21619492769241333\n",
      "[Training Epoch 8] Batch 163, Loss 0.25403520464897156\n",
      "[Training Epoch 8] Batch 164, Loss 0.2572484612464905\n",
      "[Training Epoch 8] Batch 165, Loss 0.24553035199642181\n",
      "[Training Epoch 8] Batch 166, Loss 0.2719690203666687\n",
      "[Training Epoch 8] Batch 167, Loss 0.2544400691986084\n",
      "[Training Epoch 8] Batch 168, Loss 0.2642839252948761\n",
      "[Training Epoch 8] Batch 169, Loss 0.25987446308135986\n",
      "[Training Epoch 8] Batch 170, Loss 0.23761552572250366\n",
      "[Training Epoch 8] Batch 171, Loss 0.2456522285938263\n",
      "[Training Epoch 8] Batch 172, Loss 0.21859300136566162\n",
      "[Training Epoch 8] Batch 173, Loss 0.22492504119873047\n",
      "[Training Epoch 8] Batch 174, Loss 0.24236664175987244\n",
      "[Training Epoch 8] Batch 175, Loss 0.22871847450733185\n",
      "[Training Epoch 8] Batch 176, Loss 0.22630494832992554\n",
      "[Training Epoch 8] Batch 177, Loss 0.28683724999427795\n",
      "[Training Epoch 8] Batch 178, Loss 0.24170595407485962\n",
      "[Training Epoch 8] Batch 179, Loss 0.2765392065048218\n",
      "[Training Epoch 8] Batch 180, Loss 0.25587907433509827\n",
      "[Training Epoch 8] Batch 181, Loss 0.24678990244865417\n",
      "[Training Epoch 8] Batch 182, Loss 0.2517690658569336\n",
      "[Training Epoch 8] Batch 183, Loss 0.26299452781677246\n",
      "[Training Epoch 8] Batch 184, Loss 0.2607234716415405\n",
      "[Training Epoch 8] Batch 185, Loss 0.2738070487976074\n",
      "[Training Epoch 8] Batch 186, Loss 0.23703229427337646\n",
      "[Training Epoch 8] Batch 187, Loss 0.24479754269123077\n",
      "[Training Epoch 8] Batch 188, Loss 0.24122577905654907\n",
      "[Training Epoch 8] Batch 189, Loss 0.2544008493423462\n",
      "[Training Epoch 8] Batch 190, Loss 0.2774851620197296\n",
      "[Training Epoch 8] Batch 191, Loss 0.251303493976593\n",
      "[Training Epoch 8] Batch 192, Loss 0.2630363404750824\n",
      "[Training Epoch 8] Batch 193, Loss 0.2858688533306122\n",
      "[Training Epoch 8] Batch 194, Loss 0.2523528039455414\n",
      "[Training Epoch 8] Batch 195, Loss 0.25858408212661743\n",
      "[Training Epoch 8] Batch 196, Loss 0.2647441625595093\n",
      "[Training Epoch 8] Batch 197, Loss 0.24059830605983734\n",
      "[Training Epoch 8] Batch 198, Loss 0.2590025067329407\n",
      "[Training Epoch 8] Batch 199, Loss 0.2630944848060608\n",
      "[Training Epoch 8] Batch 200, Loss 0.24441567063331604\n",
      "[Training Epoch 8] Batch 201, Loss 0.22510112822055817\n",
      "[Training Epoch 8] Batch 202, Loss 0.24174165725708008\n",
      "[Training Epoch 8] Batch 203, Loss 0.2368101179599762\n",
      "[Training Epoch 8] Batch 204, Loss 0.2409118115901947\n",
      "[Training Epoch 8] Batch 205, Loss 0.2465806007385254\n",
      "[Training Epoch 8] Batch 206, Loss 0.24702021479606628\n",
      "[Training Epoch 8] Batch 207, Loss 0.25885245203971863\n",
      "[Training Epoch 8] Batch 208, Loss 0.2775958180427551\n",
      "[Training Epoch 8] Batch 209, Loss 0.25486987829208374\n",
      "[Training Epoch 8] Batch 210, Loss 0.23372480273246765\n",
      "[Training Epoch 8] Batch 211, Loss 0.2469528615474701\n",
      "[Training Epoch 8] Batch 212, Loss 0.28698503971099854\n",
      "[Training Epoch 8] Batch 213, Loss 0.24772445857524872\n",
      "[Training Epoch 8] Batch 214, Loss 0.24304735660552979\n",
      "[Training Epoch 8] Batch 215, Loss 0.24031919240951538\n",
      "[Training Epoch 8] Batch 216, Loss 0.25885385274887085\n",
      "[Training Epoch 8] Batch 217, Loss 0.2376999855041504\n",
      "[Training Epoch 8] Batch 218, Loss 0.26837974786758423\n",
      "[Training Epoch 8] Batch 219, Loss 0.24218584597110748\n",
      "[Training Epoch 8] Batch 220, Loss 0.24860702455043793\n",
      "[Training Epoch 8] Batch 221, Loss 0.231378972530365\n",
      "[Training Epoch 8] Batch 222, Loss 0.239654541015625\n",
      "[Training Epoch 8] Batch 223, Loss 0.2375258058309555\n",
      "[Training Epoch 8] Batch 224, Loss 0.2827412188053131\n",
      "[Training Epoch 8] Batch 225, Loss 0.239772230386734\n",
      "[Training Epoch 8] Batch 226, Loss 0.2436019480228424\n",
      "[Training Epoch 8] Batch 227, Loss 0.22985276579856873\n",
      "[Training Epoch 8] Batch 228, Loss 0.2668357491493225\n",
      "[Training Epoch 8] Batch 229, Loss 0.23420417308807373\n",
      "[Training Epoch 8] Batch 230, Loss 0.23886452615261078\n",
      "[Training Epoch 8] Batch 231, Loss 0.26584291458129883\n",
      "[Training Epoch 8] Batch 232, Loss 0.2201399952173233\n",
      "[Training Epoch 8] Batch 233, Loss 0.2798910439014435\n",
      "[Training Epoch 8] Batch 234, Loss 0.2381117194890976\n",
      "[Training Epoch 8] Batch 235, Loss 0.247336745262146\n",
      "[Training Epoch 8] Batch 236, Loss 0.26383495330810547\n",
      "[Training Epoch 8] Batch 237, Loss 0.25164180994033813\n",
      "[Training Epoch 8] Batch 238, Loss 0.24905280768871307\n",
      "[Training Epoch 8] Batch 239, Loss 0.2508225440979004\n",
      "[Training Epoch 8] Batch 240, Loss 0.2801499664783478\n",
      "[Training Epoch 8] Batch 241, Loss 0.23724870383739471\n",
      "[Training Epoch 8] Batch 242, Loss 0.2755289077758789\n",
      "[Training Epoch 8] Batch 243, Loss 0.26452770829200745\n",
      "[Training Epoch 8] Batch 244, Loss 0.23795533180236816\n",
      "[Training Epoch 8] Batch 245, Loss 0.24594859778881073\n",
      "[Training Epoch 8] Batch 246, Loss 0.2329857051372528\n",
      "[Training Epoch 8] Batch 247, Loss 0.2651022672653198\n",
      "[Training Epoch 8] Batch 248, Loss 0.2765953838825226\n",
      "[Training Epoch 8] Batch 249, Loss 0.22898367047309875\n",
      "[Training Epoch 8] Batch 250, Loss 0.23037374019622803\n",
      "[Training Epoch 8] Batch 251, Loss 0.23448103666305542\n",
      "[Training Epoch 8] Batch 252, Loss 0.23936358094215393\n",
      "[Training Epoch 8] Batch 253, Loss 0.2512681484222412\n",
      "[Training Epoch 8] Batch 254, Loss 0.23699912428855896\n",
      "[Training Epoch 8] Batch 255, Loss 0.21810883283615112\n",
      "[Training Epoch 8] Batch 256, Loss 0.2577805519104004\n",
      "[Training Epoch 8] Batch 257, Loss 0.28601178526878357\n",
      "[Training Epoch 8] Batch 258, Loss 0.2380053699016571\n",
      "[Training Epoch 8] Batch 259, Loss 0.2590760588645935\n",
      "[Training Epoch 8] Batch 260, Loss 0.24571111798286438\n",
      "[Training Epoch 8] Batch 261, Loss 0.25967520475387573\n",
      "[Training Epoch 8] Batch 262, Loss 0.2619872987270355\n",
      "[Training Epoch 8] Batch 263, Loss 0.2595565915107727\n",
      "[Training Epoch 8] Batch 264, Loss 0.27801525592803955\n",
      "[Training Epoch 8] Batch 265, Loss 0.2566574811935425\n",
      "[Training Epoch 8] Batch 266, Loss 0.24029478430747986\n",
      "[Training Epoch 8] Batch 267, Loss 0.24421140551567078\n",
      "[Training Epoch 8] Batch 268, Loss 0.2532253861427307\n",
      "[Training Epoch 8] Batch 269, Loss 0.26499730348587036\n",
      "[Training Epoch 8] Batch 270, Loss 0.2535093426704407\n",
      "[Training Epoch 8] Batch 271, Loss 0.2575938105583191\n",
      "[Training Epoch 8] Batch 272, Loss 0.25398316979408264\n",
      "[Training Epoch 8] Batch 273, Loss 0.24115078151226044\n",
      "[Training Epoch 8] Batch 274, Loss 0.24294592440128326\n",
      "[Training Epoch 8] Batch 275, Loss 0.27785515785217285\n",
      "[Training Epoch 8] Batch 276, Loss 0.2718958854675293\n",
      "[Training Epoch 8] Batch 277, Loss 0.2643921971321106\n",
      "[Training Epoch 8] Batch 278, Loss 0.2358560413122177\n",
      "[Training Epoch 8] Batch 279, Loss 0.28712284564971924\n",
      "[Training Epoch 8] Batch 280, Loss 0.2628173828125\n",
      "[Training Epoch 8] Batch 281, Loss 0.25776612758636475\n",
      "[Training Epoch 8] Batch 282, Loss 0.2388278841972351\n",
      "[Training Epoch 8] Batch 283, Loss 0.2795746326446533\n",
      "[Training Epoch 8] Batch 284, Loss 0.24910679459571838\n",
      "[Training Epoch 8] Batch 285, Loss 0.2667551636695862\n",
      "[Training Epoch 8] Batch 286, Loss 0.22925975918769836\n",
      "[Training Epoch 8] Batch 287, Loss 0.24596789479255676\n",
      "[Training Epoch 8] Batch 288, Loss 0.23366409540176392\n",
      "[Training Epoch 8] Batch 289, Loss 0.2435009777545929\n",
      "[Training Epoch 8] Batch 290, Loss 0.26936280727386475\n",
      "[Training Epoch 8] Batch 291, Loss 0.25212594866752625\n",
      "[Training Epoch 8] Batch 292, Loss 0.2396368533372879\n",
      "[Training Epoch 8] Batch 293, Loss 0.25501587986946106\n",
      "[Training Epoch 8] Batch 294, Loss 0.2625208795070648\n",
      "[Training Epoch 8] Batch 295, Loss 0.24950724840164185\n",
      "[Training Epoch 8] Batch 296, Loss 0.2403709590435028\n",
      "[Training Epoch 8] Batch 297, Loss 0.24745050072669983\n",
      "[Training Epoch 8] Batch 298, Loss 0.2370898574590683\n",
      "[Training Epoch 8] Batch 299, Loss 0.2492801994085312\n",
      "[Training Epoch 8] Batch 300, Loss 0.2525177299976349\n",
      "[Training Epoch 8] Batch 301, Loss 0.25460872054100037\n",
      "[Training Epoch 8] Batch 302, Loss 0.24992036819458008\n",
      "[Training Epoch 8] Batch 303, Loss 0.2405860275030136\n",
      "[Training Epoch 8] Batch 304, Loss 0.2605345845222473\n",
      "[Training Epoch 8] Batch 305, Loss 0.21285508573055267\n",
      "[Training Epoch 8] Batch 306, Loss 0.24583427608013153\n",
      "[Training Epoch 8] Batch 307, Loss 0.2624000310897827\n",
      "[Training Epoch 8] Batch 308, Loss 0.2532564401626587\n",
      "[Training Epoch 8] Batch 309, Loss 0.25489598512649536\n",
      "[Training Epoch 8] Batch 310, Loss 0.2766381502151489\n",
      "[Training Epoch 8] Batch 311, Loss 0.23779764771461487\n",
      "[Training Epoch 8] Batch 312, Loss 0.2868632376194\n",
      "[Training Epoch 8] Batch 313, Loss 0.2585650384426117\n",
      "[Training Epoch 8] Batch 314, Loss 0.24398669600486755\n",
      "[Training Epoch 8] Batch 315, Loss 0.265558660030365\n",
      "[Training Epoch 8] Batch 316, Loss 0.25153103470802307\n",
      "[Training Epoch 8] Batch 317, Loss 0.25059008598327637\n",
      "[Training Epoch 8] Batch 318, Loss 0.2589574456214905\n",
      "[Training Epoch 8] Batch 319, Loss 0.266124963760376\n",
      "[Training Epoch 8] Batch 320, Loss 0.2466743290424347\n",
      "[Training Epoch 8] Batch 321, Loss 0.2688922882080078\n",
      "[Training Epoch 8] Batch 322, Loss 0.26190608739852905\n",
      "[Training Epoch 8] Batch 323, Loss 0.2272229790687561\n",
      "[Training Epoch 8] Batch 324, Loss 0.23881730437278748\n",
      "[Training Epoch 8] Batch 325, Loss 0.24232807755470276\n",
      "[Training Epoch 8] Batch 326, Loss 0.22541451454162598\n",
      "[Training Epoch 8] Batch 327, Loss 0.23427577316761017\n",
      "[Training Epoch 8] Batch 328, Loss 0.2517872154712677\n",
      "[Training Epoch 8] Batch 329, Loss 0.2513168156147003\n",
      "[Training Epoch 8] Batch 330, Loss 0.23196551203727722\n",
      "[Training Epoch 8] Batch 331, Loss 0.22998206317424774\n",
      "[Training Epoch 8] Batch 332, Loss 0.22882312536239624\n",
      "[Training Epoch 8] Batch 333, Loss 0.24351727962493896\n",
      "[Training Epoch 8] Batch 334, Loss 0.21507295966148376\n",
      "[Training Epoch 8] Batch 335, Loss 0.2283153384923935\n",
      "[Training Epoch 8] Batch 336, Loss 0.24067939817905426\n",
      "[Training Epoch 8] Batch 337, Loss 0.22901418805122375\n",
      "[Training Epoch 8] Batch 338, Loss 0.2223377823829651\n",
      "[Training Epoch 8] Batch 339, Loss 0.24765102565288544\n",
      "[Training Epoch 8] Batch 340, Loss 0.25720342993736267\n",
      "[Training Epoch 8] Batch 341, Loss 0.23571522533893585\n",
      "[Training Epoch 8] Batch 342, Loss 0.226156547665596\n",
      "[Training Epoch 8] Batch 343, Loss 0.2817004323005676\n",
      "[Training Epoch 8] Batch 344, Loss 0.2745359539985657\n",
      "[Training Epoch 8] Batch 345, Loss 0.2725273370742798\n",
      "[Training Epoch 8] Batch 346, Loss 0.22333908081054688\n",
      "[Training Epoch 8] Batch 347, Loss 0.23471812903881073\n",
      "[Training Epoch 8] Batch 348, Loss 0.25098466873168945\n",
      "[Training Epoch 8] Batch 349, Loss 0.2439483404159546\n",
      "[Training Epoch 8] Batch 350, Loss 0.27750957012176514\n",
      "[Training Epoch 8] Batch 351, Loss 0.23845404386520386\n",
      "[Training Epoch 8] Batch 352, Loss 0.22175271809101105\n",
      "[Training Epoch 8] Batch 353, Loss 0.26812130212783813\n",
      "[Training Epoch 8] Batch 354, Loss 0.2574310302734375\n",
      "[Training Epoch 8] Batch 355, Loss 0.27162832021713257\n",
      "[Training Epoch 8] Batch 356, Loss 0.24686497449874878\n",
      "[Training Epoch 8] Batch 357, Loss 0.2569486200809479\n",
      "[Training Epoch 8] Batch 358, Loss 0.2611844837665558\n",
      "[Training Epoch 8] Batch 359, Loss 0.2721107602119446\n",
      "[Training Epoch 8] Batch 360, Loss 0.24780866503715515\n",
      "[Training Epoch 8] Batch 361, Loss 0.23991256952285767\n",
      "[Training Epoch 8] Batch 362, Loss 0.2502623200416565\n",
      "[Training Epoch 8] Batch 363, Loss 0.25991958379745483\n",
      "[Training Epoch 8] Batch 364, Loss 0.23358461260795593\n",
      "[Training Epoch 8] Batch 365, Loss 0.2701079845428467\n",
      "[Training Epoch 8] Batch 366, Loss 0.24907530844211578\n",
      "[Training Epoch 8] Batch 367, Loss 0.26287519931793213\n",
      "[Training Epoch 8] Batch 368, Loss 0.2842358946800232\n",
      "[Training Epoch 8] Batch 369, Loss 0.26040273904800415\n",
      "[Training Epoch 8] Batch 370, Loss 0.24927067756652832\n",
      "[Training Epoch 8] Batch 371, Loss 0.26318874955177307\n",
      "[Training Epoch 8] Batch 372, Loss 0.2598254978656769\n",
      "[Training Epoch 8] Batch 373, Loss 0.2363988161087036\n",
      "[Training Epoch 8] Batch 374, Loss 0.24167323112487793\n",
      "[Training Epoch 8] Batch 375, Loss 0.2816184163093567\n",
      "[Training Epoch 8] Batch 376, Loss 0.2507006525993347\n",
      "[Training Epoch 8] Batch 377, Loss 0.26970624923706055\n",
      "[Training Epoch 8] Batch 378, Loss 0.2526499629020691\n",
      "[Training Epoch 8] Batch 379, Loss 0.2685244679450989\n",
      "[Training Epoch 8] Batch 380, Loss 0.2585497498512268\n",
      "[Training Epoch 8] Batch 381, Loss 0.29724085330963135\n",
      "[Training Epoch 8] Batch 382, Loss 0.25259920954704285\n",
      "[Training Epoch 8] Batch 383, Loss 0.2347264289855957\n",
      "[Training Epoch 8] Batch 384, Loss 0.2534746825695038\n",
      "[Training Epoch 8] Batch 385, Loss 0.2714325487613678\n",
      "[Training Epoch 8] Batch 386, Loss 0.24467849731445312\n",
      "[Training Epoch 8] Batch 387, Loss 0.23582887649536133\n",
      "[Training Epoch 8] Batch 388, Loss 0.25938284397125244\n",
      "[Training Epoch 8] Batch 389, Loss 0.23955661058425903\n",
      "[Training Epoch 8] Batch 390, Loss 0.24320489168167114\n",
      "[Training Epoch 8] Batch 391, Loss 0.2522794008255005\n",
      "[Training Epoch 8] Batch 392, Loss 0.2686077654361725\n",
      "[Training Epoch 8] Batch 393, Loss 0.2436637282371521\n",
      "[Training Epoch 8] Batch 394, Loss 0.2391805648803711\n",
      "[Training Epoch 8] Batch 395, Loss 0.2365197092294693\n",
      "[Training Epoch 8] Batch 396, Loss 0.2519920766353607\n",
      "[Training Epoch 8] Batch 397, Loss 0.21688342094421387\n",
      "[Training Epoch 8] Batch 398, Loss 0.22239182889461517\n",
      "[Training Epoch 8] Batch 399, Loss 0.25505948066711426\n",
      "[Training Epoch 8] Batch 400, Loss 0.2609717845916748\n",
      "[Training Epoch 8] Batch 401, Loss 0.24491170048713684\n",
      "[Training Epoch 8] Batch 402, Loss 0.254768431186676\n",
      "[Training Epoch 8] Batch 403, Loss 0.24533942341804504\n",
      "[Training Epoch 8] Batch 404, Loss 0.27608293294906616\n",
      "[Training Epoch 8] Batch 405, Loss 0.23000656068325043\n",
      "[Training Epoch 8] Batch 406, Loss 0.27125415205955505\n",
      "[Training Epoch 8] Batch 407, Loss 0.28702980279922485\n",
      "[Training Epoch 8] Batch 408, Loss 0.24428850412368774\n",
      "[Training Epoch 8] Batch 409, Loss 0.2738461494445801\n",
      "[Training Epoch 8] Batch 410, Loss 0.2746935486793518\n",
      "[Training Epoch 8] Batch 411, Loss 0.26006531715393066\n",
      "[Training Epoch 8] Batch 412, Loss 0.24582606554031372\n",
      "[Training Epoch 8] Batch 413, Loss 0.2516626715660095\n",
      "[Training Epoch 8] Batch 414, Loss 0.2681552767753601\n",
      "[Training Epoch 8] Batch 415, Loss 0.274226576089859\n",
      "[Training Epoch 8] Batch 416, Loss 0.2668206989765167\n",
      "[Training Epoch 8] Batch 417, Loss 0.2523149251937866\n",
      "[Training Epoch 8] Batch 418, Loss 0.2512591779232025\n",
      "[Training Epoch 8] Batch 419, Loss 0.2751352787017822\n",
      "[Training Epoch 8] Batch 420, Loss 0.2677246928215027\n",
      "[Training Epoch 8] Batch 421, Loss 0.21469247341156006\n",
      "[Training Epoch 8] Batch 422, Loss 0.27480214834213257\n",
      "[Training Epoch 8] Batch 423, Loss 0.2272491455078125\n",
      "[Training Epoch 8] Batch 424, Loss 0.2610194683074951\n",
      "[Training Epoch 8] Batch 425, Loss 0.2548680305480957\n",
      "[Training Epoch 8] Batch 426, Loss 0.2552022933959961\n",
      "[Training Epoch 8] Batch 427, Loss 0.23370051383972168\n",
      "[Training Epoch 8] Batch 428, Loss 0.24679651856422424\n",
      "[Training Epoch 8] Batch 429, Loss 0.22718515992164612\n",
      "[Training Epoch 8] Batch 430, Loss 0.2614852488040924\n",
      "[Training Epoch 8] Batch 431, Loss 0.25400760769844055\n",
      "[Training Epoch 8] Batch 432, Loss 0.24728989601135254\n",
      "[Training Epoch 8] Batch 433, Loss 0.26971083879470825\n",
      "[Training Epoch 8] Batch 434, Loss 0.2443356215953827\n",
      "[Training Epoch 8] Batch 435, Loss 0.24347643554210663\n",
      "[Training Epoch 8] Batch 436, Loss 0.27279990911483765\n",
      "[Training Epoch 8] Batch 437, Loss 0.2799813747406006\n",
      "[Training Epoch 8] Batch 438, Loss 0.2315312772989273\n",
      "[Training Epoch 8] Batch 439, Loss 0.25075435638427734\n",
      "[Training Epoch 8] Batch 440, Loss 0.2334447056055069\n",
      "[Training Epoch 8] Batch 441, Loss 0.24283954501152039\n",
      "[Training Epoch 8] Batch 442, Loss 0.2629408836364746\n",
      "[Training Epoch 8] Batch 443, Loss 0.26589643955230713\n",
      "[Training Epoch 8] Batch 444, Loss 0.2761349678039551\n",
      "[Training Epoch 8] Batch 445, Loss 0.2507838010787964\n",
      "[Training Epoch 8] Batch 446, Loss 0.24479392170906067\n",
      "[Training Epoch 8] Batch 447, Loss 0.23316171765327454\n",
      "[Training Epoch 8] Batch 448, Loss 0.3050087094306946\n",
      "[Training Epoch 8] Batch 449, Loss 0.2591973543167114\n",
      "[Training Epoch 8] Batch 450, Loss 0.22805052995681763\n",
      "[Training Epoch 8] Batch 451, Loss 0.241132453083992\n",
      "[Training Epoch 8] Batch 452, Loss 0.23470303416252136\n",
      "[Training Epoch 8] Batch 453, Loss 0.25276726484298706\n",
      "[Training Epoch 8] Batch 454, Loss 0.23093272745609283\n",
      "[Training Epoch 8] Batch 455, Loss 0.23967254161834717\n",
      "[Training Epoch 8] Batch 456, Loss 0.24295571446418762\n",
      "[Training Epoch 8] Batch 457, Loss 0.26048266887664795\n",
      "[Training Epoch 8] Batch 458, Loss 0.24324141442775726\n",
      "[Training Epoch 8] Batch 459, Loss 0.25353142619132996\n",
      "[Training Epoch 8] Batch 460, Loss 0.25545820593833923\n",
      "[Training Epoch 8] Batch 461, Loss 0.2505152225494385\n",
      "[Training Epoch 8] Batch 462, Loss 0.2446315884590149\n",
      "[Training Epoch 8] Batch 463, Loss 0.25910621881484985\n",
      "[Training Epoch 8] Batch 464, Loss 0.2486163228750229\n",
      "[Training Epoch 8] Batch 465, Loss 0.24390742182731628\n",
      "[Training Epoch 8] Batch 466, Loss 0.27165818214416504\n",
      "[Training Epoch 8] Batch 467, Loss 0.2584601640701294\n",
      "[Training Epoch 8] Batch 468, Loss 0.22194188833236694\n",
      "[Training Epoch 8] Batch 469, Loss 0.2501479685306549\n",
      "[Training Epoch 8] Batch 470, Loss 0.25752875208854675\n",
      "[Training Epoch 8] Batch 471, Loss 0.23894146084785461\n",
      "[Training Epoch 8] Batch 472, Loss 0.2407492995262146\n",
      "[Training Epoch 8] Batch 473, Loss 0.24964956939220428\n",
      "[Training Epoch 8] Batch 474, Loss 0.24663667380809784\n",
      "[Training Epoch 8] Batch 475, Loss 0.24590890109539032\n",
      "[Training Epoch 8] Batch 476, Loss 0.23351076245307922\n",
      "[Training Epoch 8] Batch 477, Loss 0.2578811049461365\n",
      "[Training Epoch 8] Batch 478, Loss 0.28081536293029785\n",
      "[Training Epoch 8] Batch 479, Loss 0.2911194860935211\n",
      "[Training Epoch 8] Batch 480, Loss 0.2515951991081238\n",
      "[Training Epoch 8] Batch 481, Loss 0.26323533058166504\n",
      "[Training Epoch 8] Batch 482, Loss 0.27805623412132263\n",
      "[Training Epoch 8] Batch 483, Loss 0.22317369282245636\n",
      "[Training Epoch 8] Batch 484, Loss 0.22861404716968536\n",
      "[Training Epoch 8] Batch 485, Loss 0.24516244232654572\n",
      "[Training Epoch 8] Batch 486, Loss 0.2576703131198883\n",
      "[Training Epoch 8] Batch 487, Loss 0.25568145513534546\n",
      "[Training Epoch 8] Batch 488, Loss 0.2760302424430847\n",
      "[Training Epoch 8] Batch 489, Loss 0.27769842743873596\n",
      "[Training Epoch 8] Batch 490, Loss 0.25527894496917725\n",
      "[Training Epoch 8] Batch 491, Loss 0.2521805167198181\n",
      "[Training Epoch 8] Batch 492, Loss 0.23015156388282776\n",
      "[Training Epoch 8] Batch 493, Loss 0.24561190605163574\n",
      "[Training Epoch 8] Batch 494, Loss 0.24653401970863342\n",
      "[Training Epoch 8] Batch 495, Loss 0.26989251375198364\n",
      "[Training Epoch 8] Batch 496, Loss 0.2709123194217682\n",
      "[Training Epoch 8] Batch 497, Loss 0.29089289903640747\n",
      "[Training Epoch 8] Batch 498, Loss 0.26912516355514526\n",
      "[Training Epoch 8] Batch 499, Loss 0.25595664978027344\n",
      "[Training Epoch 8] Batch 500, Loss 0.241007998585701\n",
      "[Training Epoch 8] Batch 501, Loss 0.2754925787448883\n",
      "[Training Epoch 8] Batch 502, Loss 0.24937063455581665\n",
      "[Training Epoch 8] Batch 503, Loss 0.23245911300182343\n",
      "[Training Epoch 8] Batch 504, Loss 0.2621646225452423\n",
      "[Training Epoch 8] Batch 505, Loss 0.2326158583164215\n",
      "[Training Epoch 8] Batch 506, Loss 0.2556683123111725\n",
      "[Training Epoch 8] Batch 507, Loss 0.26996511220932007\n",
      "[Training Epoch 8] Batch 508, Loss 0.23238712549209595\n",
      "[Training Epoch 8] Batch 509, Loss 0.22755694389343262\n",
      "[Training Epoch 8] Batch 510, Loss 0.26508498191833496\n",
      "[Training Epoch 8] Batch 511, Loss 0.2558557093143463\n",
      "[Training Epoch 8] Batch 512, Loss 0.24969042837619781\n",
      "[Training Epoch 8] Batch 513, Loss 0.24768748879432678\n",
      "[Training Epoch 8] Batch 514, Loss 0.2727723717689514\n",
      "[Training Epoch 8] Batch 515, Loss 0.2682639956474304\n",
      "[Training Epoch 8] Batch 516, Loss 0.26355379819869995\n",
      "[Training Epoch 8] Batch 517, Loss 0.2585352957248688\n",
      "[Training Epoch 8] Batch 518, Loss 0.2569654583930969\n",
      "[Training Epoch 8] Batch 519, Loss 0.25543856620788574\n",
      "[Training Epoch 8] Batch 520, Loss 0.25736004114151\n",
      "[Training Epoch 8] Batch 521, Loss 0.28948619961738586\n",
      "[Training Epoch 8] Batch 522, Loss 0.24617190659046173\n",
      "[Training Epoch 8] Batch 523, Loss 0.23525431752204895\n",
      "[Training Epoch 8] Batch 524, Loss 0.27589863538742065\n",
      "[Training Epoch 8] Batch 525, Loss 0.25674158334732056\n",
      "[Training Epoch 8] Batch 526, Loss 0.248715341091156\n",
      "[Training Epoch 8] Batch 527, Loss 0.24880829453468323\n",
      "[Training Epoch 8] Batch 528, Loss 0.2321557253599167\n",
      "[Training Epoch 8] Batch 529, Loss 0.27797287702560425\n",
      "[Training Epoch 8] Batch 530, Loss 0.2422613501548767\n",
      "[Training Epoch 8] Batch 531, Loss 0.25350067019462585\n",
      "[Training Epoch 8] Batch 532, Loss 0.26571810245513916\n",
      "[Training Epoch 8] Batch 533, Loss 0.24595966935157776\n",
      "[Training Epoch 8] Batch 534, Loss 0.2535611391067505\n",
      "[Training Epoch 8] Batch 535, Loss 0.2617923617362976\n",
      "[Training Epoch 8] Batch 536, Loss 0.27666714787483215\n",
      "[Training Epoch 8] Batch 537, Loss 0.26120126247406006\n",
      "[Training Epoch 8] Batch 538, Loss 0.23416553437709808\n",
      "[Training Epoch 8] Batch 539, Loss 0.2677185535430908\n",
      "[Training Epoch 8] Batch 540, Loss 0.2644052505493164\n",
      "[Training Epoch 8] Batch 541, Loss 0.2710462808609009\n",
      "[Training Epoch 8] Batch 542, Loss 0.2446901500225067\n",
      "[Training Epoch 8] Batch 543, Loss 0.282563716173172\n",
      "[Training Epoch 8] Batch 544, Loss 0.2413060963153839\n",
      "[Training Epoch 8] Batch 545, Loss 0.24114245176315308\n",
      "[Training Epoch 8] Batch 546, Loss 0.2846860885620117\n",
      "[Training Epoch 8] Batch 547, Loss 0.22562023997306824\n",
      "[Training Epoch 8] Batch 548, Loss 0.24149036407470703\n",
      "[Training Epoch 8] Batch 549, Loss 0.2590385675430298\n",
      "[Training Epoch 8] Batch 550, Loss 0.24684152007102966\n",
      "[Training Epoch 8] Batch 551, Loss 0.2672487795352936\n",
      "[Training Epoch 8] Batch 552, Loss 0.2380368411540985\n",
      "[Training Epoch 8] Batch 553, Loss 0.2666891813278198\n",
      "[Training Epoch 8] Batch 554, Loss 0.2505001723766327\n",
      "[Training Epoch 8] Batch 555, Loss 0.2486034333705902\n",
      "[Training Epoch 8] Batch 556, Loss 0.2239806056022644\n",
      "[Training Epoch 8] Batch 557, Loss 0.2624713182449341\n",
      "[Training Epoch 8] Batch 558, Loss 0.24028854072093964\n",
      "[Training Epoch 8] Batch 559, Loss 0.2708306908607483\n",
      "[Training Epoch 8] Batch 560, Loss 0.23790818452835083\n",
      "[Training Epoch 8] Batch 561, Loss 0.2763015329837799\n",
      "[Training Epoch 8] Batch 562, Loss 0.26408833265304565\n",
      "[Training Epoch 8] Batch 563, Loss 0.2356504648923874\n",
      "[Training Epoch 8] Batch 564, Loss 0.23975196480751038\n",
      "[Training Epoch 8] Batch 565, Loss 0.2792283296585083\n",
      "[Training Epoch 8] Batch 566, Loss 0.24912135303020477\n",
      "[Training Epoch 8] Batch 567, Loss 0.2622269093990326\n",
      "[Training Epoch 8] Batch 568, Loss 0.2520334720611572\n",
      "[Training Epoch 8] Batch 569, Loss 0.2588919699192047\n",
      "[Training Epoch 8] Batch 570, Loss 0.25889015197753906\n",
      "[Training Epoch 8] Batch 571, Loss 0.2290935218334198\n",
      "[Training Epoch 8] Batch 572, Loss 0.2410500943660736\n",
      "[Training Epoch 8] Batch 573, Loss 0.2506275475025177\n",
      "[Training Epoch 8] Batch 574, Loss 0.23372626304626465\n",
      "[Training Epoch 8] Batch 575, Loss 0.2220119833946228\n",
      "[Training Epoch 8] Batch 576, Loss 0.2851051092147827\n",
      "[Training Epoch 8] Batch 577, Loss 0.2487548142671585\n",
      "[Training Epoch 8] Batch 578, Loss 0.2545987665653229\n",
      "[Training Epoch 8] Batch 579, Loss 0.2787756025791168\n",
      "[Training Epoch 8] Batch 580, Loss 0.2492811232805252\n",
      "[Training Epoch 8] Batch 581, Loss 0.27745527029037476\n",
      "[Training Epoch 8] Batch 582, Loss 0.21587598323822021\n",
      "[Training Epoch 8] Batch 583, Loss 0.250293493270874\n",
      "[Training Epoch 8] Batch 584, Loss 0.2258303165435791\n",
      "[Training Epoch 8] Batch 585, Loss 0.2738717794418335\n",
      "[Training Epoch 8] Batch 586, Loss 0.25189581513404846\n",
      "[Training Epoch 8] Batch 587, Loss 0.2697404623031616\n",
      "[Training Epoch 8] Batch 588, Loss 0.24077647924423218\n",
      "[Training Epoch 8] Batch 589, Loss 0.24025170505046844\n",
      "[Training Epoch 8] Batch 590, Loss 0.2573612928390503\n",
      "[Training Epoch 8] Batch 591, Loss 0.2416580319404602\n",
      "[Training Epoch 8] Batch 592, Loss 0.25014227628707886\n",
      "[Training Epoch 8] Batch 593, Loss 0.26105374097824097\n",
      "[Training Epoch 8] Batch 594, Loss 0.30874186754226685\n",
      "[Training Epoch 8] Batch 595, Loss 0.24707350134849548\n",
      "[Training Epoch 8] Batch 596, Loss 0.23008760809898376\n",
      "[Training Epoch 8] Batch 597, Loss 0.24349728226661682\n",
      "[Training Epoch 8] Batch 598, Loss 0.26167014241218567\n",
      "[Training Epoch 8] Batch 599, Loss 0.2536439299583435\n",
      "[Training Epoch 8] Batch 600, Loss 0.24870377779006958\n",
      "[Training Epoch 8] Batch 601, Loss 0.27503007650375366\n",
      "[Training Epoch 8] Batch 602, Loss 0.2649601697921753\n",
      "[Training Epoch 8] Batch 603, Loss 0.26216405630111694\n",
      "[Training Epoch 8] Batch 604, Loss 0.22740735113620758\n",
      "[Training Epoch 8] Batch 605, Loss 0.2677692174911499\n",
      "[Training Epoch 8] Batch 606, Loss 0.2649941146373749\n",
      "[Training Epoch 8] Batch 607, Loss 0.2743929624557495\n",
      "[Training Epoch 8] Batch 608, Loss 0.23791390657424927\n",
      "[Training Epoch 8] Batch 609, Loss 0.24457862973213196\n",
      "[Training Epoch 8] Batch 610, Loss 0.2465955764055252\n",
      "[Training Epoch 8] Batch 611, Loss 0.25823161005973816\n",
      "[Training Epoch 8] Batch 612, Loss 0.23099598288536072\n",
      "[Training Epoch 8] Batch 613, Loss 0.24096205830574036\n",
      "[Training Epoch 8] Batch 614, Loss 0.24995851516723633\n",
      "[Training Epoch 8] Batch 615, Loss 0.2736453413963318\n",
      "[Training Epoch 8] Batch 616, Loss 0.2596098482608795\n",
      "[Training Epoch 8] Batch 617, Loss 0.23107290267944336\n",
      "[Training Epoch 8] Batch 618, Loss 0.23612487316131592\n",
      "[Training Epoch 8] Batch 619, Loss 0.26346707344055176\n",
      "[Training Epoch 8] Batch 620, Loss 0.27566975355148315\n",
      "[Training Epoch 8] Batch 621, Loss 0.23508498072624207\n",
      "[Training Epoch 8] Batch 622, Loss 0.25488293170928955\n",
      "[Training Epoch 8] Batch 623, Loss 0.2735234797000885\n",
      "[Training Epoch 8] Batch 624, Loss 0.2504822313785553\n",
      "[Training Epoch 8] Batch 625, Loss 0.26874879002571106\n",
      "[Training Epoch 8] Batch 626, Loss 0.24607649445533752\n",
      "[Training Epoch 8] Batch 627, Loss 0.2537772059440613\n",
      "[Training Epoch 8] Batch 628, Loss 0.24741870164871216\n",
      "[Training Epoch 8] Batch 629, Loss 0.27134308218955994\n",
      "[Training Epoch 8] Batch 630, Loss 0.24852992594242096\n",
      "[Training Epoch 8] Batch 631, Loss 0.24189288914203644\n",
      "[Training Epoch 8] Batch 632, Loss 0.23905770480632782\n",
      "[Training Epoch 8] Batch 633, Loss 0.26582902669906616\n",
      "[Training Epoch 8] Batch 634, Loss 0.24172081053256989\n",
      "[Training Epoch 8] Batch 635, Loss 0.23930394649505615\n",
      "[Training Epoch 8] Batch 636, Loss 0.263566255569458\n",
      "[Training Epoch 8] Batch 637, Loss 0.2481229305267334\n",
      "[Training Epoch 8] Batch 638, Loss 0.2638999819755554\n",
      "[Training Epoch 8] Batch 639, Loss 0.22731554508209229\n",
      "[Training Epoch 8] Batch 640, Loss 0.27291780710220337\n",
      "[Training Epoch 8] Batch 641, Loss 0.2566429078578949\n",
      "[Training Epoch 8] Batch 642, Loss 0.2212713062763214\n",
      "[Training Epoch 8] Batch 643, Loss 0.2803020477294922\n",
      "[Training Epoch 8] Batch 644, Loss 0.2395298182964325\n",
      "[Training Epoch 8] Batch 645, Loss 0.25432127714157104\n",
      "[Training Epoch 8] Batch 646, Loss 0.25802844762802124\n",
      "[Training Epoch 8] Batch 647, Loss 0.24125128984451294\n",
      "[Training Epoch 8] Batch 648, Loss 0.29020315408706665\n",
      "[Training Epoch 8] Batch 649, Loss 0.24688327312469482\n",
      "[Training Epoch 8] Batch 650, Loss 0.2503150403499603\n",
      "[Training Epoch 8] Batch 651, Loss 0.23962324857711792\n",
      "[Training Epoch 8] Batch 652, Loss 0.26917874813079834\n",
      "[Training Epoch 8] Batch 653, Loss 0.3028065264225006\n",
      "[Training Epoch 8] Batch 654, Loss 0.23326128721237183\n",
      "[Training Epoch 8] Batch 655, Loss 0.2382032722234726\n",
      "[Training Epoch 8] Batch 656, Loss 0.23029497265815735\n",
      "[Training Epoch 8] Batch 657, Loss 0.24663105607032776\n",
      "[Training Epoch 8] Batch 658, Loss 0.2657220661640167\n",
      "[Training Epoch 8] Batch 659, Loss 0.26360371708869934\n",
      "[Training Epoch 8] Batch 660, Loss 0.2712980806827545\n",
      "[Training Epoch 8] Batch 661, Loss 0.2574685215950012\n",
      "[Training Epoch 8] Batch 662, Loss 0.2198510766029358\n",
      "[Training Epoch 8] Batch 663, Loss 0.26445096731185913\n",
      "[Training Epoch 8] Batch 664, Loss 0.23829255998134613\n",
      "[Training Epoch 8] Batch 665, Loss 0.2568913996219635\n",
      "[Training Epoch 8] Batch 666, Loss 0.28087329864501953\n",
      "[Training Epoch 8] Batch 667, Loss 0.24310162663459778\n",
      "[Training Epoch 8] Batch 668, Loss 0.2424788624048233\n",
      "[Training Epoch 8] Batch 669, Loss 0.2578468918800354\n",
      "[Training Epoch 8] Batch 670, Loss 0.2419574111700058\n",
      "[Training Epoch 8] Batch 671, Loss 0.2666356861591339\n",
      "[Training Epoch 8] Batch 672, Loss 0.26487916707992554\n",
      "[Training Epoch 8] Batch 673, Loss 0.23634666204452515\n",
      "[Training Epoch 8] Batch 674, Loss 0.26929736137390137\n",
      "[Training Epoch 8] Batch 675, Loss 0.26238030195236206\n",
      "[Training Epoch 8] Batch 676, Loss 0.2595885396003723\n",
      "[Training Epoch 8] Batch 677, Loss 0.24317368865013123\n",
      "[Training Epoch 8] Batch 678, Loss 0.234725683927536\n",
      "[Training Epoch 8] Batch 679, Loss 0.25955790281295776\n",
      "[Training Epoch 8] Batch 680, Loss 0.2556682229042053\n",
      "[Training Epoch 8] Batch 681, Loss 0.2665197253227234\n",
      "[Training Epoch 8] Batch 682, Loss 0.2620689868927002\n",
      "[Training Epoch 8] Batch 683, Loss 0.26595190167427063\n",
      "[Training Epoch 8] Batch 684, Loss 0.2461402714252472\n",
      "[Training Epoch 8] Batch 685, Loss 0.24324126541614532\n",
      "[Training Epoch 8] Batch 686, Loss 0.24360772967338562\n",
      "[Training Epoch 8] Batch 687, Loss 0.2467111498117447\n",
      "[Training Epoch 8] Batch 688, Loss 0.27622613310813904\n",
      "[Training Epoch 8] Batch 689, Loss 0.24142053723335266\n",
      "[Training Epoch 8] Batch 690, Loss 0.25449249148368835\n",
      "[Training Epoch 8] Batch 691, Loss 0.2576121687889099\n",
      "[Training Epoch 8] Batch 692, Loss 0.2596423923969269\n",
      "[Training Epoch 8] Batch 693, Loss 0.24923497438430786\n",
      "[Training Epoch 8] Batch 694, Loss 0.253631591796875\n",
      "[Training Epoch 8] Batch 695, Loss 0.2904645502567291\n",
      "[Training Epoch 8] Batch 696, Loss 0.22904978692531586\n",
      "[Training Epoch 8] Batch 697, Loss 0.28806039690971375\n",
      "[Training Epoch 8] Batch 698, Loss 0.2752307057380676\n",
      "[Training Epoch 8] Batch 699, Loss 0.24464118480682373\n",
      "[Training Epoch 8] Batch 700, Loss 0.24042928218841553\n",
      "[Training Epoch 8] Batch 701, Loss 0.2539396286010742\n",
      "[Training Epoch 8] Batch 702, Loss 0.2786658704280853\n",
      "[Training Epoch 8] Batch 703, Loss 0.2506350874900818\n",
      "[Training Epoch 8] Batch 704, Loss 0.275295615196228\n",
      "[Training Epoch 8] Batch 705, Loss 0.262574702501297\n",
      "[Training Epoch 8] Batch 706, Loss 0.2660022974014282\n",
      "[Training Epoch 8] Batch 707, Loss 0.271384060382843\n",
      "[Training Epoch 8] Batch 708, Loss 0.22896555066108704\n",
      "[Training Epoch 8] Batch 709, Loss 0.2676481604576111\n",
      "[Training Epoch 8] Batch 710, Loss 0.28352487087249756\n",
      "[Training Epoch 8] Batch 711, Loss 0.2587253451347351\n",
      "[Training Epoch 8] Batch 712, Loss 0.2708887457847595\n",
      "[Training Epoch 8] Batch 713, Loss 0.25963276624679565\n",
      "[Training Epoch 8] Batch 714, Loss 0.2719186544418335\n",
      "[Training Epoch 8] Batch 715, Loss 0.24961471557617188\n",
      "[Training Epoch 8] Batch 716, Loss 0.24658167362213135\n",
      "[Training Epoch 8] Batch 717, Loss 0.24299582839012146\n",
      "[Training Epoch 8] Batch 718, Loss 0.23930275440216064\n",
      "[Training Epoch 8] Batch 719, Loss 0.2546123266220093\n",
      "[Training Epoch 8] Batch 720, Loss 0.25060614943504333\n",
      "[Training Epoch 8] Batch 721, Loss 0.2694581151008606\n",
      "[Training Epoch 8] Batch 722, Loss 0.24827232956886292\n",
      "[Training Epoch 8] Batch 723, Loss 0.2695057988166809\n",
      "[Training Epoch 8] Batch 724, Loss 0.25774723291397095\n",
      "[Training Epoch 8] Batch 725, Loss 0.22871772944927216\n",
      "[Training Epoch 8] Batch 726, Loss 0.2443763166666031\n",
      "[Training Epoch 8] Batch 727, Loss 0.24812915921211243\n",
      "[Training Epoch 8] Batch 728, Loss 0.25032857060432434\n",
      "[Training Epoch 8] Batch 729, Loss 0.27058112621307373\n",
      "[Training Epoch 8] Batch 730, Loss 0.24820919334888458\n",
      "[Training Epoch 8] Batch 731, Loss 0.25057029724121094\n",
      "[Training Epoch 8] Batch 732, Loss 0.2591001093387604\n",
      "[Training Epoch 8] Batch 733, Loss 0.26160305738449097\n",
      "[Training Epoch 8] Batch 734, Loss 0.2432626485824585\n",
      "[Training Epoch 8] Batch 735, Loss 0.25306442379951477\n",
      "[Training Epoch 8] Batch 736, Loss 0.2727382481098175\n",
      "[Training Epoch 8] Batch 737, Loss 0.2546660006046295\n",
      "[Training Epoch 8] Batch 738, Loss 0.2554585933685303\n",
      "[Training Epoch 8] Batch 739, Loss 0.24343840777873993\n",
      "[Training Epoch 8] Batch 740, Loss 0.23522445559501648\n",
      "[Training Epoch 8] Batch 741, Loss 0.2399686574935913\n",
      "[Training Epoch 8] Batch 742, Loss 0.24852555990219116\n",
      "[Training Epoch 8] Batch 743, Loss 0.2710518538951874\n",
      "[Training Epoch 8] Batch 744, Loss 0.2597593665122986\n",
      "[Training Epoch 8] Batch 745, Loss 0.26118892431259155\n",
      "[Training Epoch 8] Batch 746, Loss 0.2413662225008011\n",
      "[Training Epoch 8] Batch 747, Loss 0.2862122654914856\n",
      "[Training Epoch 8] Batch 748, Loss 0.2593344748020172\n",
      "[Training Epoch 8] Batch 749, Loss 0.23867005109786987\n",
      "[Training Epoch 8] Batch 750, Loss 0.2364877462387085\n",
      "[Training Epoch 8] Batch 751, Loss 0.263130784034729\n",
      "[Training Epoch 8] Batch 752, Loss 0.26784300804138184\n",
      "[Training Epoch 8] Batch 753, Loss 0.2454461008310318\n",
      "[Training Epoch 8] Batch 754, Loss 0.2719430923461914\n",
      "[Training Epoch 8] Batch 755, Loss 0.2381192147731781\n",
      "[Training Epoch 8] Batch 756, Loss 0.2403803914785385\n",
      "[Training Epoch 8] Batch 757, Loss 0.21625198423862457\n",
      "[Training Epoch 8] Batch 758, Loss 0.26326364278793335\n",
      "[Training Epoch 8] Batch 759, Loss 0.25247740745544434\n",
      "[Training Epoch 8] Batch 760, Loss 0.2670431137084961\n",
      "[Training Epoch 8] Batch 761, Loss 0.25636762380599976\n",
      "[Training Epoch 8] Batch 762, Loss 0.26772576570510864\n",
      "[Training Epoch 8] Batch 763, Loss 0.2494795322418213\n",
      "[Training Epoch 8] Batch 764, Loss 0.24958808720111847\n",
      "[Training Epoch 8] Batch 765, Loss 0.27486875653266907\n",
      "[Training Epoch 8] Batch 766, Loss 0.2575063705444336\n",
      "[Training Epoch 8] Batch 767, Loss 0.22227564454078674\n",
      "[Training Epoch 8] Batch 768, Loss 0.21602541208267212\n",
      "[Training Epoch 8] Batch 769, Loss 0.25928860902786255\n",
      "[Training Epoch 8] Batch 770, Loss 0.27909308671951294\n",
      "[Training Epoch 8] Batch 771, Loss 0.26667749881744385\n",
      "[Training Epoch 8] Batch 772, Loss 0.24905623495578766\n",
      "[Training Epoch 8] Batch 773, Loss 0.2503935694694519\n",
      "[Training Epoch 8] Batch 774, Loss 0.2550239562988281\n",
      "[Training Epoch 8] Batch 775, Loss 0.2344525158405304\n",
      "[Training Epoch 8] Batch 776, Loss 0.24411489069461823\n",
      "[Training Epoch 8] Batch 777, Loss 0.286359965801239\n",
      "[Training Epoch 8] Batch 778, Loss 0.2926594018936157\n",
      "[Training Epoch 8] Batch 779, Loss 0.23101386427879333\n",
      "[Training Epoch 8] Batch 780, Loss 0.26296424865722656\n",
      "[Training Epoch 8] Batch 781, Loss 0.23399260640144348\n",
      "[Training Epoch 8] Batch 782, Loss 0.24717003107070923\n",
      "[Training Epoch 8] Batch 783, Loss 0.2683044672012329\n",
      "[Training Epoch 8] Batch 784, Loss 0.2738427519798279\n",
      "[Training Epoch 8] Batch 785, Loss 0.24846485257148743\n",
      "[Training Epoch 8] Batch 786, Loss 0.2485235184431076\n",
      "[Training Epoch 8] Batch 787, Loss 0.23176246881484985\n",
      "[Training Epoch 8] Batch 788, Loss 0.24636325240135193\n",
      "[Training Epoch 8] Batch 789, Loss 0.26943832635879517\n",
      "[Training Epoch 8] Batch 790, Loss 0.23531419038772583\n",
      "[Training Epoch 8] Batch 791, Loss 0.2814640998840332\n",
      "[Training Epoch 8] Batch 792, Loss 0.26746875047683716\n",
      "[Training Epoch 8] Batch 793, Loss 0.24280068278312683\n",
      "[Training Epoch 8] Batch 794, Loss 0.23803861439228058\n",
      "[Training Epoch 8] Batch 795, Loss 0.2711425721645355\n",
      "[Training Epoch 8] Batch 796, Loss 0.24808195233345032\n",
      "[Training Epoch 8] Batch 797, Loss 0.25793009996414185\n",
      "[Training Epoch 8] Batch 798, Loss 0.2625213861465454\n",
      "[Training Epoch 8] Batch 799, Loss 0.24030080437660217\n",
      "[Training Epoch 8] Batch 800, Loss 0.2300286889076233\n",
      "[Training Epoch 8] Batch 801, Loss 0.2899787724018097\n",
      "[Training Epoch 8] Batch 802, Loss 0.24165646731853485\n",
      "[Training Epoch 8] Batch 803, Loss 0.2597203254699707\n",
      "[Training Epoch 8] Batch 804, Loss 0.2772643566131592\n",
      "[Training Epoch 8] Batch 805, Loss 0.2789425849914551\n",
      "[Training Epoch 8] Batch 806, Loss 0.2679615020751953\n",
      "[Training Epoch 8] Batch 807, Loss 0.26455652713775635\n",
      "[Training Epoch 8] Batch 808, Loss 0.2635229229927063\n",
      "[Training Epoch 8] Batch 809, Loss 0.2690171003341675\n",
      "[Training Epoch 8] Batch 810, Loss 0.23520159721374512\n",
      "[Training Epoch 8] Batch 811, Loss 0.22951240837574005\n",
      "[Training Epoch 8] Batch 812, Loss 0.23451651632785797\n",
      "[Training Epoch 8] Batch 813, Loss 0.2542596459388733\n",
      "[Training Epoch 8] Batch 814, Loss 0.24869327247142792\n",
      "[Training Epoch 8] Batch 815, Loss 0.26918238401412964\n",
      "[Training Epoch 8] Batch 816, Loss 0.29916977882385254\n",
      "[Training Epoch 8] Batch 817, Loss 0.2525762617588043\n",
      "[Training Epoch 8] Batch 818, Loss 0.23742720484733582\n",
      "[Training Epoch 8] Batch 819, Loss 0.2488454282283783\n",
      "[Training Epoch 8] Batch 820, Loss 0.2740931808948517\n",
      "[Training Epoch 8] Batch 821, Loss 0.2649708390235901\n",
      "[Training Epoch 8] Batch 822, Loss 0.22236956655979156\n",
      "[Training Epoch 8] Batch 823, Loss 0.25570106506347656\n",
      "[Training Epoch 8] Batch 824, Loss 0.2638379633426666\n",
      "[Training Epoch 8] Batch 825, Loss 0.26541319489479065\n",
      "[Training Epoch 8] Batch 826, Loss 0.24465584754943848\n",
      "[Training Epoch 8] Batch 827, Loss 0.2596273124217987\n",
      "[Training Epoch 8] Batch 828, Loss 0.2747788429260254\n",
      "[Training Epoch 8] Batch 829, Loss 0.25088512897491455\n",
      "[Training Epoch 8] Batch 830, Loss 0.259492963552475\n",
      "[Training Epoch 8] Batch 831, Loss 0.2566303312778473\n",
      "[Training Epoch 8] Batch 832, Loss 0.24940702319145203\n",
      "[Training Epoch 8] Batch 833, Loss 0.23972409963607788\n",
      "[Training Epoch 8] Batch 834, Loss 0.26012176275253296\n",
      "[Training Epoch 8] Batch 835, Loss 0.2681899666786194\n",
      "[Training Epoch 8] Batch 836, Loss 0.25725993514060974\n",
      "[Training Epoch 8] Batch 837, Loss 0.25259166955947876\n",
      "[Training Epoch 8] Batch 838, Loss 0.23620551824569702\n",
      "[Training Epoch 8] Batch 839, Loss 0.26528337597846985\n",
      "[Training Epoch 8] Batch 840, Loss 0.25265148282051086\n",
      "[Training Epoch 8] Batch 841, Loss 0.2564505338668823\n",
      "[Training Epoch 8] Batch 842, Loss 0.23689883947372437\n",
      "[Training Epoch 8] Batch 843, Loss 0.25070253014564514\n",
      "[Training Epoch 8] Batch 844, Loss 0.26901519298553467\n",
      "[Training Epoch 8] Batch 845, Loss 0.239310160279274\n",
      "[Training Epoch 8] Batch 846, Loss 0.24522632360458374\n",
      "[Training Epoch 8] Batch 847, Loss 0.243645578622818\n",
      "[Training Epoch 8] Batch 848, Loss 0.2548510432243347\n",
      "[Training Epoch 8] Batch 849, Loss 0.25793588161468506\n",
      "[Training Epoch 8] Batch 850, Loss 0.24638843536376953\n",
      "[Training Epoch 8] Batch 851, Loss 0.2736392617225647\n",
      "[Training Epoch 8] Batch 852, Loss 0.24626359343528748\n",
      "[Training Epoch 8] Batch 853, Loss 0.27323025465011597\n",
      "[Training Epoch 8] Batch 854, Loss 0.2074582874774933\n",
      "[Training Epoch 8] Batch 855, Loss 0.22446388006210327\n",
      "[Training Epoch 8] Batch 856, Loss 0.2332824021577835\n",
      "[Training Epoch 8] Batch 857, Loss 0.27050289511680603\n",
      "[Training Epoch 8] Batch 858, Loss 0.25838568806648254\n",
      "[Training Epoch 8] Batch 859, Loss 0.29491448402404785\n",
      "[Training Epoch 8] Batch 860, Loss 0.23736411333084106\n",
      "[Training Epoch 8] Batch 861, Loss 0.28642353415489197\n",
      "[Training Epoch 8] Batch 862, Loss 0.26557013392448425\n",
      "[Training Epoch 8] Batch 863, Loss 0.2500552237033844\n",
      "[Training Epoch 8] Batch 864, Loss 0.22535598278045654\n",
      "[Training Epoch 8] Batch 865, Loss 0.2475317418575287\n",
      "[Training Epoch 8] Batch 866, Loss 0.2855930030345917\n",
      "[Training Epoch 8] Batch 867, Loss 0.2629201412200928\n",
      "[Training Epoch 8] Batch 868, Loss 0.2661854028701782\n",
      "[Training Epoch 8] Batch 869, Loss 0.27702590823173523\n",
      "[Training Epoch 8] Batch 870, Loss 0.2602643072605133\n",
      "[Training Epoch 8] Batch 871, Loss 0.2661014199256897\n",
      "[Training Epoch 8] Batch 872, Loss 0.23725318908691406\n",
      "[Training Epoch 8] Batch 873, Loss 0.2568160593509674\n",
      "[Training Epoch 8] Batch 874, Loss 0.2533210217952728\n",
      "[Training Epoch 8] Batch 875, Loss 0.24598990380764008\n",
      "[Training Epoch 8] Batch 876, Loss 0.24924641847610474\n",
      "[Training Epoch 8] Batch 877, Loss 0.2587651014328003\n",
      "[Training Epoch 8] Batch 878, Loss 0.24757987260818481\n",
      "[Training Epoch 8] Batch 879, Loss 0.2417394518852234\n",
      "[Training Epoch 8] Batch 880, Loss 0.25029146671295166\n",
      "[Training Epoch 8] Batch 881, Loss 0.22661876678466797\n",
      "[Training Epoch 8] Batch 882, Loss 0.258674681186676\n",
      "[Training Epoch 8] Batch 883, Loss 0.24439823627471924\n",
      "[Training Epoch 8] Batch 884, Loss 0.2656075358390808\n",
      "[Training Epoch 8] Batch 885, Loss 0.230756014585495\n",
      "[Training Epoch 8] Batch 886, Loss 0.2278405725955963\n",
      "[Training Epoch 8] Batch 887, Loss 0.2742276191711426\n",
      "[Training Epoch 8] Batch 888, Loss 0.26835131645202637\n",
      "[Training Epoch 8] Batch 889, Loss 0.28071701526641846\n",
      "[Training Epoch 8] Batch 890, Loss 0.23371827602386475\n",
      "[Training Epoch 8] Batch 891, Loss 0.22903162240982056\n",
      "[Training Epoch 8] Batch 892, Loss 0.2530140280723572\n",
      "[Training Epoch 8] Batch 893, Loss 0.28365689516067505\n",
      "[Training Epoch 8] Batch 894, Loss 0.24975156784057617\n",
      "[Training Epoch 8] Batch 895, Loss 0.26736533641815186\n",
      "[Training Epoch 8] Batch 896, Loss 0.2504529356956482\n",
      "[Training Epoch 8] Batch 897, Loss 0.2658383846282959\n",
      "[Training Epoch 8] Batch 898, Loss 0.22991541028022766\n",
      "[Training Epoch 8] Batch 899, Loss 0.23189009726047516\n",
      "[Training Epoch 8] Batch 900, Loss 0.25298604369163513\n",
      "[Training Epoch 8] Batch 901, Loss 0.25753921270370483\n",
      "[Training Epoch 8] Batch 902, Loss 0.2687254846096039\n",
      "[Training Epoch 8] Batch 903, Loss 0.2471306025981903\n",
      "[Training Epoch 8] Batch 904, Loss 0.24464432895183563\n",
      "[Training Epoch 8] Batch 905, Loss 0.2816312909126282\n",
      "[Training Epoch 8] Batch 906, Loss 0.24836339056491852\n",
      "[Training Epoch 8] Batch 907, Loss 0.26564690470695496\n",
      "[Training Epoch 8] Batch 908, Loss 0.228610098361969\n",
      "[Training Epoch 8] Batch 909, Loss 0.24373121559619904\n",
      "[Training Epoch 8] Batch 910, Loss 0.2610805630683899\n",
      "[Training Epoch 8] Batch 911, Loss 0.25432655215263367\n",
      "[Training Epoch 8] Batch 912, Loss 0.2377522885799408\n",
      "[Training Epoch 8] Batch 913, Loss 0.26043379306793213\n",
      "[Training Epoch 8] Batch 914, Loss 0.24686682224273682\n",
      "[Training Epoch 8] Batch 915, Loss 0.2445235550403595\n",
      "[Training Epoch 8] Batch 916, Loss 0.2621403634548187\n",
      "[Training Epoch 8] Batch 917, Loss 0.2817613184452057\n",
      "[Training Epoch 8] Batch 918, Loss 0.24797627329826355\n",
      "[Training Epoch 8] Batch 919, Loss 0.20865696668624878\n",
      "[Training Epoch 8] Batch 920, Loss 0.24374349415302277\n",
      "[Training Epoch 8] Batch 921, Loss 0.2446766495704651\n",
      "[Training Epoch 8] Batch 922, Loss 0.22663705050945282\n",
      "[Training Epoch 8] Batch 923, Loss 0.2857189178466797\n",
      "[Training Epoch 8] Batch 924, Loss 0.2412891387939453\n",
      "[Training Epoch 8] Batch 925, Loss 0.23132947087287903\n",
      "[Training Epoch 8] Batch 926, Loss 0.23812776803970337\n",
      "[Training Epoch 8] Batch 927, Loss 0.26664894819259644\n",
      "[Training Epoch 8] Batch 928, Loss 0.27774208784103394\n",
      "[Training Epoch 8] Batch 929, Loss 0.2745634615421295\n",
      "[Training Epoch 8] Batch 930, Loss 0.23388338088989258\n",
      "[Training Epoch 8] Batch 931, Loss 0.26244688034057617\n",
      "[Training Epoch 8] Batch 932, Loss 0.2627900540828705\n",
      "[Training Epoch 8] Batch 933, Loss 0.24010112881660461\n",
      "[Training Epoch 8] Batch 934, Loss 0.28809505701065063\n",
      "[Training Epoch 8] Batch 935, Loss 0.2599529027938843\n",
      "[Training Epoch 8] Batch 936, Loss 0.250244677066803\n",
      "[Training Epoch 8] Batch 937, Loss 0.24298107624053955\n",
      "[Training Epoch 8] Batch 938, Loss 0.2703552842140198\n",
      "[Training Epoch 8] Batch 939, Loss 0.25656765699386597\n",
      "[Training Epoch 8] Batch 940, Loss 0.26217663288116455\n",
      "[Training Epoch 8] Batch 941, Loss 0.23563028872013092\n",
      "[Training Epoch 8] Batch 942, Loss 0.2520611584186554\n",
      "[Training Epoch 8] Batch 943, Loss 0.2340165227651596\n",
      "[Training Epoch 8] Batch 944, Loss 0.27660679817199707\n",
      "[Training Epoch 8] Batch 945, Loss 0.2853671908378601\n",
      "[Training Epoch 8] Batch 946, Loss 0.255779504776001\n",
      "[Training Epoch 8] Batch 947, Loss 0.2513596713542938\n",
      "[Training Epoch 8] Batch 948, Loss 0.2761910557746887\n",
      "[Training Epoch 8] Batch 949, Loss 0.22505198419094086\n",
      "[Training Epoch 8] Batch 950, Loss 0.26489919424057007\n",
      "[Training Epoch 8] Batch 951, Loss 0.2522627115249634\n",
      "[Training Epoch 8] Batch 952, Loss 0.2511973977088928\n",
      "[Training Epoch 8] Batch 953, Loss 0.27071380615234375\n",
      "[Training Epoch 8] Batch 954, Loss 0.2669164836406708\n",
      "[Training Epoch 8] Batch 955, Loss 0.25787317752838135\n",
      "[Training Epoch 8] Batch 956, Loss 0.25971221923828125\n",
      "[Training Epoch 8] Batch 957, Loss 0.2523132562637329\n",
      "[Training Epoch 8] Batch 958, Loss 0.24561291933059692\n",
      "[Training Epoch 8] Batch 959, Loss 0.2186506688594818\n",
      "[Training Epoch 8] Batch 960, Loss 0.2514936327934265\n",
      "[Training Epoch 8] Batch 961, Loss 0.2582094967365265\n",
      "[Training Epoch 8] Batch 962, Loss 0.2684307098388672\n",
      "[Training Epoch 8] Batch 963, Loss 0.2463785707950592\n",
      "[Training Epoch 8] Batch 964, Loss 0.2593859136104584\n",
      "[Training Epoch 8] Batch 965, Loss 0.27317285537719727\n",
      "[Training Epoch 8] Batch 966, Loss 0.25262773036956787\n",
      "[Training Epoch 8] Batch 967, Loss 0.24226531386375427\n",
      "[Training Epoch 8] Batch 968, Loss 0.2951370179653168\n",
      "[Training Epoch 8] Batch 969, Loss 0.2864975929260254\n",
      "[Training Epoch 8] Batch 970, Loss 0.25366637110710144\n",
      "[Training Epoch 8] Batch 971, Loss 0.25513502955436707\n",
      "[Training Epoch 8] Batch 972, Loss 0.24204081296920776\n",
      "[Training Epoch 8] Batch 973, Loss 0.2652736306190491\n",
      "[Training Epoch 8] Batch 974, Loss 0.2572113871574402\n",
      "[Training Epoch 8] Batch 975, Loss 0.24289262294769287\n",
      "[Training Epoch 8] Batch 976, Loss 0.29520851373672485\n",
      "[Training Epoch 8] Batch 977, Loss 0.24519102275371552\n",
      "[Training Epoch 8] Batch 978, Loss 0.2680736780166626\n",
      "[Training Epoch 8] Batch 979, Loss 0.2386748343706131\n",
      "[Training Epoch 8] Batch 980, Loss 0.2766525149345398\n",
      "[Training Epoch 8] Batch 981, Loss 0.25296321511268616\n",
      "[Training Epoch 8] Batch 982, Loss 0.24650993943214417\n",
      "[Training Epoch 8] Batch 983, Loss 0.24650511145591736\n",
      "[Training Epoch 8] Batch 984, Loss 0.2556840777397156\n",
      "[Training Epoch 8] Batch 985, Loss 0.23738059401512146\n",
      "[Training Epoch 8] Batch 986, Loss 0.24164381623268127\n",
      "[Training Epoch 8] Batch 987, Loss 0.24563364684581757\n",
      "[Training Epoch 8] Batch 988, Loss 0.24076512455940247\n",
      "[Training Epoch 8] Batch 989, Loss 0.23517289757728577\n",
      "[Training Epoch 8] Batch 990, Loss 0.257692813873291\n",
      "[Training Epoch 8] Batch 991, Loss 0.2607857584953308\n",
      "[Training Epoch 8] Batch 992, Loss 0.2621065676212311\n",
      "[Training Epoch 8] Batch 993, Loss 0.2913389801979065\n",
      "[Training Epoch 8] Batch 994, Loss 0.25792360305786133\n",
      "[Training Epoch 8] Batch 995, Loss 0.262487530708313\n",
      "[Training Epoch 8] Batch 996, Loss 0.23982754349708557\n",
      "[Training Epoch 8] Batch 997, Loss 0.261054128408432\n",
      "[Training Epoch 8] Batch 998, Loss 0.26904216408729553\n",
      "[Training Epoch 8] Batch 999, Loss 0.24088537693023682\n",
      "[Training Epoch 8] Batch 1000, Loss 0.23461194336414337\n",
      "[Training Epoch 8] Batch 1001, Loss 0.22884654998779297\n",
      "[Training Epoch 8] Batch 1002, Loss 0.25251853466033936\n",
      "[Training Epoch 8] Batch 1003, Loss 0.24294278025627136\n",
      "[Training Epoch 8] Batch 1004, Loss 0.25816428661346436\n",
      "[Training Epoch 8] Batch 1005, Loss 0.2610602378845215\n",
      "[Training Epoch 8] Batch 1006, Loss 0.25714677572250366\n",
      "[Training Epoch 8] Batch 1007, Loss 0.2538525462150574\n",
      "[Training Epoch 8] Batch 1008, Loss 0.2602319121360779\n",
      "[Training Epoch 8] Batch 1009, Loss 0.26882708072662354\n",
      "[Training Epoch 8] Batch 1010, Loss 0.24740687012672424\n",
      "[Training Epoch 8] Batch 1011, Loss 0.26945170760154724\n",
      "[Training Epoch 8] Batch 1012, Loss 0.2730659246444702\n",
      "[Training Epoch 8] Batch 1013, Loss 0.25244587659835815\n",
      "[Training Epoch 8] Batch 1014, Loss 0.2515677809715271\n",
      "[Training Epoch 8] Batch 1015, Loss 0.2420097291469574\n",
      "[Training Epoch 8] Batch 1016, Loss 0.26075056195259094\n",
      "[Training Epoch 8] Batch 1017, Loss 0.26615601778030396\n",
      "[Training Epoch 8] Batch 1018, Loss 0.25533199310302734\n",
      "[Training Epoch 8] Batch 1019, Loss 0.2564956545829773\n",
      "[Training Epoch 8] Batch 1020, Loss 0.24972474575042725\n",
      "[Training Epoch 8] Batch 1021, Loss 0.24792394042015076\n",
      "[Training Epoch 8] Batch 1022, Loss 0.2588975727558136\n",
      "[Training Epoch 8] Batch 1023, Loss 0.23082005977630615\n",
      "[Training Epoch 8] Batch 1024, Loss 0.23687860369682312\n",
      "[Training Epoch 8] Batch 1025, Loss 0.24166202545166016\n",
      "[Training Epoch 8] Batch 1026, Loss 0.23051878809928894\n",
      "[Training Epoch 8] Batch 1027, Loss 0.2689184844493866\n",
      "[Training Epoch 8] Batch 1028, Loss 0.2459416538476944\n",
      "[Training Epoch 8] Batch 1029, Loss 0.23660007119178772\n",
      "[Training Epoch 8] Batch 1030, Loss 0.287457138299942\n",
      "[Training Epoch 8] Batch 1031, Loss 0.2638028860092163\n",
      "[Training Epoch 8] Batch 1032, Loss 0.2600115239620209\n",
      "[Training Epoch 8] Batch 1033, Loss 0.22375375032424927\n",
      "[Training Epoch 8] Batch 1034, Loss 0.2523031234741211\n",
      "[Training Epoch 8] Batch 1035, Loss 0.2301972359418869\n",
      "[Training Epoch 8] Batch 1036, Loss 0.24408315122127533\n",
      "[Training Epoch 8] Batch 1037, Loss 0.243150994181633\n",
      "[Training Epoch 8] Batch 1038, Loss 0.24986812472343445\n",
      "[Training Epoch 8] Batch 1039, Loss 0.23861107230186462\n",
      "[Training Epoch 8] Batch 1040, Loss 0.2649732232093811\n",
      "[Training Epoch 8] Batch 1041, Loss 0.23777279257774353\n",
      "[Training Epoch 8] Batch 1042, Loss 0.24140281975269318\n",
      "[Training Epoch 8] Batch 1043, Loss 0.24503403902053833\n",
      "[Training Epoch 8] Batch 1044, Loss 0.24102070927619934\n",
      "[Training Epoch 8] Batch 1045, Loss 0.2771672308444977\n",
      "[Training Epoch 8] Batch 1046, Loss 0.23602189123630524\n",
      "[Training Epoch 8] Batch 1047, Loss 0.2592063546180725\n",
      "[Training Epoch 8] Batch 1048, Loss 0.27563315629959106\n",
      "[Training Epoch 8] Batch 1049, Loss 0.24558043479919434\n",
      "[Training Epoch 8] Batch 1050, Loss 0.22953858971595764\n",
      "[Training Epoch 8] Batch 1051, Loss 0.2349654585123062\n",
      "[Training Epoch 8] Batch 1052, Loss 0.26166868209838867\n",
      "[Training Epoch 8] Batch 1053, Loss 0.26315730810165405\n",
      "[Training Epoch 8] Batch 1054, Loss 0.24322062730789185\n",
      "[Training Epoch 8] Batch 1055, Loss 0.25718027353286743\n",
      "[Training Epoch 8] Batch 1056, Loss 0.26139605045318604\n",
      "[Training Epoch 8] Batch 1057, Loss 0.2454058825969696\n",
      "[Training Epoch 8] Batch 1058, Loss 0.26409244537353516\n",
      "[Training Epoch 8] Batch 1059, Loss 0.2551040053367615\n",
      "[Training Epoch 8] Batch 1060, Loss 0.26063987612724304\n",
      "[Training Epoch 8] Batch 1061, Loss 0.23282882571220398\n",
      "[Training Epoch 8] Batch 1062, Loss 0.2697741389274597\n",
      "[Training Epoch 8] Batch 1063, Loss 0.23640215396881104\n",
      "[Training Epoch 8] Batch 1064, Loss 0.26429158449172974\n",
      "[Training Epoch 8] Batch 1065, Loss 0.25883764028549194\n",
      "[Training Epoch 8] Batch 1066, Loss 0.21629902720451355\n",
      "[Training Epoch 8] Batch 1067, Loss 0.25326547026634216\n",
      "[Training Epoch 8] Batch 1068, Loss 0.2734489440917969\n",
      "[Training Epoch 8] Batch 1069, Loss 0.23963496088981628\n",
      "[Training Epoch 8] Batch 1070, Loss 0.23764069378376007\n",
      "[Training Epoch 8] Batch 1071, Loss 0.23696798086166382\n",
      "[Training Epoch 8] Batch 1072, Loss 0.26963740587234497\n",
      "[Training Epoch 8] Batch 1073, Loss 0.2362556904554367\n",
      "[Training Epoch 8] Batch 1074, Loss 0.259759783744812\n",
      "[Training Epoch 8] Batch 1075, Loss 0.24317270517349243\n",
      "[Training Epoch 8] Batch 1076, Loss 0.23803511261940002\n",
      "[Training Epoch 8] Batch 1077, Loss 0.25368738174438477\n",
      "[Training Epoch 8] Batch 1078, Loss 0.2540091872215271\n",
      "[Training Epoch 8] Batch 1079, Loss 0.2781834006309509\n",
      "[Training Epoch 8] Batch 1080, Loss 0.26362374424934387\n",
      "[Training Epoch 8] Batch 1081, Loss 0.2395194172859192\n",
      "[Training Epoch 8] Batch 1082, Loss 0.23962968587875366\n",
      "[Training Epoch 8] Batch 1083, Loss 0.25857800245285034\n",
      "[Training Epoch 8] Batch 1084, Loss 0.23764179646968842\n",
      "[Training Epoch 8] Batch 1085, Loss 0.23215320706367493\n",
      "[Training Epoch 8] Batch 1086, Loss 0.2444058209657669\n",
      "[Training Epoch 8] Batch 1087, Loss 0.23789598047733307\n",
      "[Training Epoch 8] Batch 1088, Loss 0.2650289535522461\n",
      "[Training Epoch 8] Batch 1089, Loss 0.24260389804840088\n",
      "[Training Epoch 8] Batch 1090, Loss 0.247594952583313\n",
      "[Training Epoch 8] Batch 1091, Loss 0.2369462251663208\n",
      "[Training Epoch 8] Batch 1092, Loss 0.2558092474937439\n",
      "[Training Epoch 8] Batch 1093, Loss 0.25810253620147705\n",
      "[Training Epoch 8] Batch 1094, Loss 0.24526114761829376\n",
      "[Training Epoch 8] Batch 1095, Loss 0.2575750946998596\n",
      "[Training Epoch 8] Batch 1096, Loss 0.23220285773277283\n",
      "[Training Epoch 8] Batch 1097, Loss 0.2529948055744171\n",
      "[Training Epoch 8] Batch 1098, Loss 0.23296627402305603\n",
      "[Training Epoch 8] Batch 1099, Loss 0.2256700098514557\n",
      "[Training Epoch 8] Batch 1100, Loss 0.2390344887971878\n",
      "[Training Epoch 8] Batch 1101, Loss 0.28093406558036804\n",
      "[Training Epoch 8] Batch 1102, Loss 0.2794734239578247\n",
      "[Training Epoch 8] Batch 1103, Loss 0.23327772319316864\n",
      "[Training Epoch 8] Batch 1104, Loss 0.2602671980857849\n",
      "[Training Epoch 8] Batch 1105, Loss 0.2343590408563614\n",
      "[Training Epoch 8] Batch 1106, Loss 0.2682001292705536\n",
      "[Training Epoch 8] Batch 1107, Loss 0.2901488244533539\n",
      "[Training Epoch 8] Batch 1108, Loss 0.2813025116920471\n",
      "[Training Epoch 8] Batch 1109, Loss 0.2539163827896118\n",
      "[Training Epoch 8] Batch 1110, Loss 0.2572169303894043\n",
      "[Training Epoch 8] Batch 1111, Loss 0.2676292657852173\n",
      "[Training Epoch 8] Batch 1112, Loss 0.24773558974266052\n",
      "[Training Epoch 8] Batch 1113, Loss 0.2412024438381195\n",
      "[Training Epoch 8] Batch 1114, Loss 0.2534281611442566\n",
      "[Training Epoch 8] Batch 1115, Loss 0.24548816680908203\n",
      "[Training Epoch 8] Batch 1116, Loss 0.274755597114563\n",
      "[Training Epoch 8] Batch 1117, Loss 0.2718544602394104\n",
      "[Training Epoch 8] Batch 1118, Loss 0.2562796175479889\n",
      "[Training Epoch 8] Batch 1119, Loss 0.2624843418598175\n",
      "[Training Epoch 8] Batch 1120, Loss 0.2731372117996216\n",
      "[Training Epoch 8] Batch 1121, Loss 0.2553367614746094\n",
      "[Training Epoch 8] Batch 1122, Loss 0.29241514205932617\n",
      "[Training Epoch 8] Batch 1123, Loss 0.26720649003982544\n",
      "[Training Epoch 8] Batch 1124, Loss 0.27973276376724243\n",
      "[Training Epoch 8] Batch 1125, Loss 0.25247442722320557\n",
      "[Training Epoch 8] Batch 1126, Loss 0.2689642906188965\n",
      "[Training Epoch 8] Batch 1127, Loss 0.25505828857421875\n",
      "[Training Epoch 8] Batch 1128, Loss 0.23733606934547424\n",
      "[Training Epoch 8] Batch 1129, Loss 0.277812659740448\n",
      "[Training Epoch 8] Batch 1130, Loss 0.25554922223091125\n",
      "[Training Epoch 8] Batch 1131, Loss 0.2520073652267456\n",
      "[Training Epoch 8] Batch 1132, Loss 0.267764687538147\n",
      "[Training Epoch 8] Batch 1133, Loss 0.25195997953414917\n",
      "[Training Epoch 8] Batch 1134, Loss 0.2539938986301422\n",
      "[Training Epoch 8] Batch 1135, Loss 0.2729191184043884\n",
      "[Training Epoch 8] Batch 1136, Loss 0.24474963545799255\n",
      "[Training Epoch 8] Batch 1137, Loss 0.24434496462345123\n",
      "[Training Epoch 8] Batch 1138, Loss 0.24497944116592407\n",
      "[Training Epoch 8] Batch 1139, Loss 0.26027241349220276\n",
      "[Training Epoch 8] Batch 1140, Loss 0.2674805223941803\n",
      "[Training Epoch 8] Batch 1141, Loss 0.24845343828201294\n",
      "[Training Epoch 8] Batch 1142, Loss 0.2469114363193512\n",
      "[Training Epoch 8] Batch 1143, Loss 0.26376068592071533\n",
      "[Training Epoch 8] Batch 1144, Loss 0.24094551801681519\n",
      "[Training Epoch 8] Batch 1145, Loss 0.2572100758552551\n",
      "[Training Epoch 8] Batch 1146, Loss 0.23515693843364716\n",
      "[Training Epoch 8] Batch 1147, Loss 0.23470035195350647\n",
      "[Training Epoch 8] Batch 1148, Loss 0.25103434920310974\n",
      "[Training Epoch 8] Batch 1149, Loss 0.2637937664985657\n",
      "[Training Epoch 8] Batch 1150, Loss 0.27447113394737244\n",
      "[Training Epoch 8] Batch 1151, Loss 0.25429409742355347\n",
      "[Training Epoch 8] Batch 1152, Loss 0.2519680857658386\n",
      "[Training Epoch 8] Batch 1153, Loss 0.25345295667648315\n",
      "[Training Epoch 8] Batch 1154, Loss 0.21461310982704163\n",
      "[Training Epoch 8] Batch 1155, Loss 0.2761937975883484\n",
      "[Training Epoch 8] Batch 1156, Loss 0.26087769865989685\n",
      "[Training Epoch 8] Batch 1157, Loss 0.27596545219421387\n",
      "[Training Epoch 8] Batch 1158, Loss 0.2415057122707367\n",
      "[Training Epoch 8] Batch 1159, Loss 0.2522004246711731\n",
      "[Training Epoch 8] Batch 1160, Loss 0.24458171427249908\n",
      "[Training Epoch 8] Batch 1161, Loss 0.2544975280761719\n",
      "[Training Epoch 8] Batch 1162, Loss 0.269133061170578\n",
      "[Training Epoch 8] Batch 1163, Loss 0.2302647829055786\n",
      "[Training Epoch 8] Batch 1164, Loss 0.25668781995773315\n",
      "[Training Epoch 8] Batch 1165, Loss 0.28148752450942993\n",
      "[Training Epoch 8] Batch 1166, Loss 0.25591880083084106\n",
      "[Training Epoch 8] Batch 1167, Loss 0.24226582050323486\n",
      "[Training Epoch 8] Batch 1168, Loss 0.25887492299079895\n",
      "[Training Epoch 8] Batch 1169, Loss 0.2515232264995575\n",
      "[Training Epoch 8] Batch 1170, Loss 0.27220287919044495\n",
      "[Training Epoch 8] Batch 1171, Loss 0.2474076896905899\n",
      "[Training Epoch 8] Batch 1172, Loss 0.24718701839447021\n",
      "[Training Epoch 8] Batch 1173, Loss 0.23542474210262299\n",
      "[Training Epoch 8] Batch 1174, Loss 0.24544180929660797\n",
      "[Training Epoch 8] Batch 1175, Loss 0.24575643241405487\n",
      "[Training Epoch 8] Batch 1176, Loss 0.25423479080200195\n",
      "[Training Epoch 8] Batch 1177, Loss 0.24359118938446045\n",
      "[Training Epoch 8] Batch 1178, Loss 0.27903586626052856\n",
      "[Training Epoch 8] Batch 1179, Loss 0.243312269449234\n",
      "[Training Epoch 8] Batch 1180, Loss 0.2862844467163086\n",
      "[Training Epoch 8] Batch 1181, Loss 0.24263042211532593\n",
      "[Training Epoch 8] Batch 1182, Loss 0.2518692910671234\n",
      "[Training Epoch 8] Batch 1183, Loss 0.24052193760871887\n",
      "[Training Epoch 8] Batch 1184, Loss 0.2250421941280365\n",
      "[Training Epoch 8] Batch 1185, Loss 0.23192664980888367\n",
      "[Training Epoch 8] Batch 1186, Loss 0.2499842792749405\n",
      "[Training Epoch 8] Batch 1187, Loss 0.24262180924415588\n",
      "[Training Epoch 8] Batch 1188, Loss 0.2451719343662262\n",
      "[Training Epoch 8] Batch 1189, Loss 0.2804739475250244\n",
      "[Training Epoch 8] Batch 1190, Loss 0.2693014442920685\n",
      "[Training Epoch 8] Batch 1191, Loss 0.24953004717826843\n",
      "[Training Epoch 8] Batch 1192, Loss 0.24669936299324036\n",
      "[Training Epoch 8] Batch 1193, Loss 0.292569637298584\n",
      "[Training Epoch 8] Batch 1194, Loss 0.23508547246456146\n",
      "[Training Epoch 8] Batch 1195, Loss 0.24341902136802673\n",
      "[Training Epoch 8] Batch 1196, Loss 0.269173264503479\n",
      "[Training Epoch 8] Batch 1197, Loss 0.2328379601240158\n",
      "[Training Epoch 8] Batch 1198, Loss 0.23678162693977356\n",
      "[Training Epoch 8] Batch 1199, Loss 0.2736387848854065\n",
      "[Training Epoch 8] Batch 1200, Loss 0.2551671266555786\n",
      "[Training Epoch 8] Batch 1201, Loss 0.2906383275985718\n",
      "[Training Epoch 8] Batch 1202, Loss 0.2844107151031494\n",
      "[Training Epoch 8] Batch 1203, Loss 0.23955506086349487\n",
      "[Training Epoch 8] Batch 1204, Loss 0.2278350293636322\n",
      "[Training Epoch 8] Batch 1205, Loss 0.26126256585121155\n",
      "[Training Epoch 8] Batch 1206, Loss 0.22733505070209503\n",
      "[Training Epoch 8] Batch 1207, Loss 0.261593222618103\n",
      "[Training Epoch 8] Batch 1208, Loss 0.26267561316490173\n",
      "[Training Epoch 8] Batch 1209, Loss 0.2756034731864929\n",
      "[Training Epoch 8] Batch 1210, Loss 0.24533210694789886\n",
      "[Training Epoch 8] Batch 1211, Loss 0.2289610207080841\n",
      "[Training Epoch 8] Batch 1212, Loss 0.25384485721588135\n",
      "[Training Epoch 8] Batch 1213, Loss 0.25454339385032654\n",
      "[Training Epoch 8] Batch 1214, Loss 0.2613692283630371\n",
      "[Training Epoch 8] Batch 1215, Loss 0.2540932595729828\n",
      "[Training Epoch 8] Batch 1216, Loss 0.2500650882720947\n",
      "[Training Epoch 8] Batch 1217, Loss 0.2483489215373993\n",
      "[Training Epoch 8] Batch 1218, Loss 0.2660647928714752\n",
      "[Training Epoch 8] Batch 1219, Loss 0.25612837076187134\n",
      "[Training Epoch 8] Batch 1220, Loss 0.23875024914741516\n",
      "[Training Epoch 8] Batch 1221, Loss 0.3044746220111847\n",
      "[Training Epoch 8] Batch 1222, Loss 0.28285712003707886\n",
      "[Training Epoch 8] Batch 1223, Loss 0.22021089494228363\n",
      "[Training Epoch 8] Batch 1224, Loss 0.2700815796852112\n",
      "[Training Epoch 8] Batch 1225, Loss 0.2637939751148224\n",
      "[Training Epoch 8] Batch 1226, Loss 0.270560622215271\n",
      "[Training Epoch 8] Batch 1227, Loss 0.2694745361804962\n",
      "[Training Epoch 8] Batch 1228, Loss 0.2657968997955322\n",
      "[Training Epoch 8] Batch 1229, Loss 0.2663469910621643\n",
      "[Training Epoch 8] Batch 1230, Loss 0.2736760377883911\n",
      "[Training Epoch 8] Batch 1231, Loss 0.2347714751958847\n",
      "[Training Epoch 8] Batch 1232, Loss 0.268168181180954\n",
      "[Training Epoch 8] Batch 1233, Loss 0.2430545687675476\n",
      "[Training Epoch 8] Batch 1234, Loss 0.25140345096588135\n",
      "[Training Epoch 8] Batch 1235, Loss 0.2532092034816742\n",
      "[Training Epoch 8] Batch 1236, Loss 0.25790804624557495\n",
      "[Training Epoch 8] Batch 1237, Loss 0.22906793653964996\n",
      "[Training Epoch 8] Batch 1238, Loss 0.24129600822925568\n",
      "[Training Epoch 8] Batch 1239, Loss 0.24025490880012512\n",
      "[Training Epoch 8] Batch 1240, Loss 0.23533833026885986\n",
      "[Training Epoch 8] Batch 1241, Loss 0.28316107392311096\n",
      "[Training Epoch 8] Batch 1242, Loss 0.2316931188106537\n",
      "[Training Epoch 8] Batch 1243, Loss 0.27254754304885864\n",
      "[Training Epoch 8] Batch 1244, Loss 0.2602919638156891\n",
      "[Training Epoch 8] Batch 1245, Loss 0.2436501681804657\n",
      "[Training Epoch 8] Batch 1246, Loss 0.2570633292198181\n",
      "[Training Epoch 8] Batch 1247, Loss 0.23804053664207458\n",
      "[Training Epoch 8] Batch 1248, Loss 0.24469049274921417\n",
      "[Training Epoch 8] Batch 1249, Loss 0.2605970799922943\n",
      "[Training Epoch 8] Batch 1250, Loss 0.24974685907363892\n",
      "[Training Epoch 8] Batch 1251, Loss 0.23102141916751862\n",
      "[Training Epoch 8] Batch 1252, Loss 0.2759418785572052\n",
      "[Training Epoch 8] Batch 1253, Loss 0.23851117491722107\n",
      "[Training Epoch 8] Batch 1254, Loss 0.25148868560791016\n",
      "[Training Epoch 8] Batch 1255, Loss 0.2319391369819641\n",
      "[Training Epoch 8] Batch 1256, Loss 0.2508184611797333\n",
      "[Training Epoch 8] Batch 1257, Loss 0.21095123887062073\n",
      "[Training Epoch 8] Batch 1258, Loss 0.24443301558494568\n",
      "[Training Epoch 8] Batch 1259, Loss 0.26373472809791565\n",
      "[Training Epoch 8] Batch 1260, Loss 0.2747724950313568\n",
      "[Training Epoch 8] Batch 1261, Loss 0.2798146903514862\n",
      "[Training Epoch 8] Batch 1262, Loss 0.2774515151977539\n",
      "[Training Epoch 8] Batch 1263, Loss 0.25578534603118896\n",
      "[Training Epoch 8] Batch 1264, Loss 0.2737008333206177\n",
      "[Training Epoch 8] Batch 1265, Loss 0.2587758004665375\n",
      "[Training Epoch 8] Batch 1266, Loss 0.2425331026315689\n",
      "[Training Epoch 8] Batch 1267, Loss 0.2520504593849182\n",
      "[Training Epoch 8] Batch 1268, Loss 0.22269092500209808\n",
      "[Training Epoch 8] Batch 1269, Loss 0.2397518903017044\n",
      "[Training Epoch 8] Batch 1270, Loss 0.24845188856124878\n",
      "[Training Epoch 8] Batch 1271, Loss 0.238094761967659\n",
      "[Training Epoch 8] Batch 1272, Loss 0.2711935043334961\n",
      "[Training Epoch 8] Batch 1273, Loss 0.23373572528362274\n",
      "[Training Epoch 8] Batch 1274, Loss 0.24372297525405884\n",
      "[Training Epoch 8] Batch 1275, Loss 0.2724071741104126\n",
      "[Training Epoch 8] Batch 1276, Loss 0.2850680351257324\n",
      "[Training Epoch 8] Batch 1277, Loss 0.2527373433113098\n",
      "[Training Epoch 8] Batch 1278, Loss 0.24085481464862823\n",
      "[Training Epoch 8] Batch 1279, Loss 0.25305837392807007\n",
      "[Training Epoch 8] Batch 1280, Loss 0.25017571449279785\n",
      "[Training Epoch 8] Batch 1281, Loss 0.2582113444805145\n",
      "[Training Epoch 8] Batch 1282, Loss 0.2513662874698639\n",
      "[Training Epoch 8] Batch 1283, Loss 0.2625584006309509\n",
      "[Training Epoch 8] Batch 1284, Loss 0.2790464162826538\n",
      "[Training Epoch 8] Batch 1285, Loss 0.2527943551540375\n",
      "[Training Epoch 8] Batch 1286, Loss 0.2469482570886612\n",
      "[Training Epoch 8] Batch 1287, Loss 0.2656417489051819\n",
      "[Training Epoch 8] Batch 1288, Loss 0.2601209282875061\n",
      "[Training Epoch 8] Batch 1289, Loss 0.28326326608657837\n",
      "[Training Epoch 8] Batch 1290, Loss 0.27152371406555176\n",
      "[Training Epoch 8] Batch 1291, Loss 0.2626911997795105\n",
      "[Training Epoch 8] Batch 1292, Loss 0.24575069546699524\n",
      "[Training Epoch 8] Batch 1293, Loss 0.2605736255645752\n",
      "[Training Epoch 8] Batch 1294, Loss 0.2554627060890198\n",
      "[Training Epoch 8] Batch 1295, Loss 0.24121348559856415\n",
      "[Training Epoch 8] Batch 1296, Loss 0.24735738337039948\n",
      "[Training Epoch 8] Batch 1297, Loss 0.23919686675071716\n",
      "[Training Epoch 8] Batch 1298, Loss 0.2852880656719208\n",
      "[Training Epoch 8] Batch 1299, Loss 0.2382040023803711\n",
      "[Training Epoch 8] Batch 1300, Loss 0.27342087030410767\n",
      "[Training Epoch 8] Batch 1301, Loss 0.25285592675209045\n",
      "[Training Epoch 8] Batch 1302, Loss 0.2499426305294037\n",
      "[Training Epoch 8] Batch 1303, Loss 0.2738296687602997\n",
      "[Training Epoch 8] Batch 1304, Loss 0.25238731503486633\n",
      "[Training Epoch 8] Batch 1305, Loss 0.2553585171699524\n",
      "[Training Epoch 8] Batch 1306, Loss 0.2738195061683655\n",
      "[Training Epoch 8] Batch 1307, Loss 0.24957481026649475\n",
      "[Training Epoch 8] Batch 1308, Loss 0.22563987970352173\n",
      "[Training Epoch 8] Batch 1309, Loss 0.29069337248802185\n",
      "[Training Epoch 8] Batch 1310, Loss 0.2551875114440918\n",
      "[Training Epoch 8] Batch 1311, Loss 0.2562747001647949\n",
      "[Training Epoch 8] Batch 1312, Loss 0.24466001987457275\n",
      "[Training Epoch 8] Batch 1313, Loss 0.27039414644241333\n",
      "[Training Epoch 8] Batch 1314, Loss 0.26539838314056396\n",
      "[Training Epoch 8] Batch 1315, Loss 0.266378253698349\n",
      "[Training Epoch 8] Batch 1316, Loss 0.22803530097007751\n",
      "[Training Epoch 8] Batch 1317, Loss 0.28439652919769287\n",
      "[Training Epoch 8] Batch 1318, Loss 0.21214166283607483\n",
      "[Training Epoch 8] Batch 1319, Loss 0.24301111698150635\n",
      "[Training Epoch 8] Batch 1320, Loss 0.22883257269859314\n",
      "[Training Epoch 8] Batch 1321, Loss 0.24396713078022003\n",
      "[Training Epoch 8] Batch 1322, Loss 0.26770877838134766\n",
      "[Training Epoch 8] Batch 1323, Loss 0.24557772278785706\n",
      "[Training Epoch 8] Batch 1324, Loss 0.23207548260688782\n",
      "[Training Epoch 8] Batch 1325, Loss 0.2548140585422516\n",
      "[Training Epoch 8] Batch 1326, Loss 0.296794593334198\n",
      "[Training Epoch 8] Batch 1327, Loss 0.24705612659454346\n",
      "[Training Epoch 8] Batch 1328, Loss 0.2586480975151062\n",
      "[Training Epoch 8] Batch 1329, Loss 0.2421402931213379\n",
      "[Training Epoch 8] Batch 1330, Loss 0.244183748960495\n",
      "[Training Epoch 8] Batch 1331, Loss 0.28103551268577576\n",
      "[Training Epoch 8] Batch 1332, Loss 0.27119186520576477\n",
      "[Training Epoch 8] Batch 1333, Loss 0.23871277272701263\n",
      "[Training Epoch 8] Batch 1334, Loss 0.24155431985855103\n",
      "[Training Epoch 8] Batch 1335, Loss 0.23223690688610077\n",
      "[Training Epoch 8] Batch 1336, Loss 0.2671414613723755\n",
      "[Training Epoch 8] Batch 1337, Loss 0.24547207355499268\n",
      "[Training Epoch 8] Batch 1338, Loss 0.23908045887947083\n",
      "[Training Epoch 8] Batch 1339, Loss 0.24560776352882385\n",
      "[Training Epoch 8] Batch 1340, Loss 0.2568691372871399\n",
      "[Training Epoch 8] Batch 1341, Loss 0.26761186122894287\n",
      "[Training Epoch 8] Batch 1342, Loss 0.21958595514297485\n",
      "[Training Epoch 8] Batch 1343, Loss 0.2567925453186035\n",
      "[Training Epoch 8] Batch 1344, Loss 0.25626808404922485\n",
      "[Training Epoch 8] Batch 1345, Loss 0.24024510383605957\n",
      "[Training Epoch 8] Batch 1346, Loss 0.27013546228408813\n",
      "[Training Epoch 8] Batch 1347, Loss 0.2387467622756958\n",
      "[Training Epoch 8] Batch 1348, Loss 0.24293012917041779\n",
      "[Training Epoch 8] Batch 1349, Loss 0.271841436624527\n",
      "[Training Epoch 8] Batch 1350, Loss 0.23855532705783844\n",
      "[Training Epoch 8] Batch 1351, Loss 0.2801889479160309\n",
      "[Training Epoch 8] Batch 1352, Loss 0.27874451875686646\n",
      "[Training Epoch 8] Batch 1353, Loss 0.23958414793014526\n",
      "[Training Epoch 8] Batch 1354, Loss 0.23067864775657654\n",
      "[Training Epoch 8] Batch 1355, Loss 0.2758462429046631\n",
      "[Training Epoch 8] Batch 1356, Loss 0.2469886839389801\n",
      "[Training Epoch 8] Batch 1357, Loss 0.25670766830444336\n",
      "[Training Epoch 8] Batch 1358, Loss 0.23189905285835266\n",
      "[Training Epoch 8] Batch 1359, Loss 0.2566094994544983\n",
      "[Training Epoch 8] Batch 1360, Loss 0.24046936631202698\n",
      "[Training Epoch 8] Batch 1361, Loss 0.26264190673828125\n",
      "[Training Epoch 8] Batch 1362, Loss 0.2575334906578064\n",
      "[Training Epoch 8] Batch 1363, Loss 0.24642693996429443\n",
      "[Training Epoch 8] Batch 1364, Loss 0.2970343232154846\n",
      "[Training Epoch 8] Batch 1365, Loss 0.253009170293808\n",
      "[Training Epoch 8] Batch 1366, Loss 0.24943865835666656\n",
      "[Training Epoch 8] Batch 1367, Loss 0.2471281886100769\n",
      "[Training Epoch 8] Batch 1368, Loss 0.2618483304977417\n",
      "[Training Epoch 8] Batch 1369, Loss 0.2531856596469879\n",
      "[Training Epoch 8] Batch 1370, Loss 0.23618820309638977\n",
      "[Training Epoch 8] Batch 1371, Loss 0.22369226813316345\n",
      "[Training Epoch 8] Batch 1372, Loss 0.268752783536911\n",
      "[Training Epoch 8] Batch 1373, Loss 0.26021817326545715\n",
      "[Training Epoch 8] Batch 1374, Loss 0.24966703355312347\n",
      "[Training Epoch 8] Batch 1375, Loss 0.2423112392425537\n",
      "[Training Epoch 8] Batch 1376, Loss 0.27695006132125854\n",
      "[Training Epoch 8] Batch 1377, Loss 0.2510398030281067\n",
      "[Training Epoch 8] Batch 1378, Loss 0.2655223309993744\n",
      "[Training Epoch 8] Batch 1379, Loss 0.251582533121109\n",
      "[Training Epoch 8] Batch 1380, Loss 0.2570182979106903\n",
      "[Training Epoch 8] Batch 1381, Loss 0.24990305304527283\n",
      "[Training Epoch 8] Batch 1382, Loss 0.24595029652118683\n",
      "[Training Epoch 8] Batch 1383, Loss 0.23460820317268372\n",
      "[Training Epoch 8] Batch 1384, Loss 0.26642972230911255\n",
      "[Training Epoch 8] Batch 1385, Loss 0.2584480941295624\n",
      "[Training Epoch 8] Batch 1386, Loss 0.2510254383087158\n",
      "[Training Epoch 8] Batch 1387, Loss 0.2571413516998291\n",
      "[Training Epoch 8] Batch 1388, Loss 0.2515676021575928\n",
      "[Training Epoch 8] Batch 1389, Loss 0.2639458477497101\n",
      "[Training Epoch 8] Batch 1390, Loss 0.24948003888130188\n",
      "[Training Epoch 8] Batch 1391, Loss 0.2727726697921753\n",
      "[Training Epoch 8] Batch 1392, Loss 0.2544708251953125\n",
      "[Training Epoch 8] Batch 1393, Loss 0.2514324188232422\n",
      "[Training Epoch 8] Batch 1394, Loss 0.2810467481613159\n",
      "[Training Epoch 8] Batch 1395, Loss 0.2698793411254883\n",
      "[Training Epoch 8] Batch 1396, Loss 0.21958981454372406\n",
      "[Training Epoch 8] Batch 1397, Loss 0.2729173004627228\n",
      "[Training Epoch 8] Batch 1398, Loss 0.23882728815078735\n",
      "[Training Epoch 8] Batch 1399, Loss 0.2673705816268921\n",
      "[Training Epoch 8] Batch 1400, Loss 0.24993452429771423\n",
      "[Training Epoch 8] Batch 1401, Loss 0.23467588424682617\n",
      "[Training Epoch 8] Batch 1402, Loss 0.23173858225345612\n",
      "[Training Epoch 8] Batch 1403, Loss 0.2570754885673523\n",
      "[Training Epoch 8] Batch 1404, Loss 0.2568228542804718\n",
      "[Training Epoch 8] Batch 1405, Loss 0.26215559244155884\n",
      "[Training Epoch 8] Batch 1406, Loss 0.2532185912132263\n",
      "[Training Epoch 8] Batch 1407, Loss 0.2413462996482849\n",
      "[Training Epoch 8] Batch 1408, Loss 0.260002076625824\n",
      "[Training Epoch 8] Batch 1409, Loss 0.2821592390537262\n",
      "[Training Epoch 8] Batch 1410, Loss 0.25978171825408936\n",
      "[Training Epoch 8] Batch 1411, Loss 0.2794973850250244\n",
      "[Training Epoch 8] Batch 1412, Loss 0.2423170804977417\n",
      "[Training Epoch 8] Batch 1413, Loss 0.2655934691429138\n",
      "[Training Epoch 8] Batch 1414, Loss 0.245082288980484\n",
      "[Training Epoch 8] Batch 1415, Loss 0.24586473405361176\n",
      "[Training Epoch 8] Batch 1416, Loss 0.2915131449699402\n",
      "[Training Epoch 8] Batch 1417, Loss 0.2604233920574188\n",
      "[Training Epoch 8] Batch 1418, Loss 0.26457831263542175\n",
      "[Training Epoch 8] Batch 1419, Loss 0.22503617405891418\n",
      "[Training Epoch 8] Batch 1420, Loss 0.2508450150489807\n",
      "[Training Epoch 8] Batch 1421, Loss 0.25416484475135803\n",
      "[Training Epoch 8] Batch 1422, Loss 0.25987836718559265\n",
      "[Training Epoch 8] Batch 1423, Loss 0.2563728094100952\n",
      "[Training Epoch 8] Batch 1424, Loss 0.2459264099597931\n",
      "[Training Epoch 8] Batch 1425, Loss 0.27279841899871826\n",
      "[Training Epoch 8] Batch 1426, Loss 0.25683683156967163\n",
      "[Training Epoch 8] Batch 1427, Loss 0.2786937952041626\n",
      "[Training Epoch 8] Batch 1428, Loss 0.24517861008644104\n",
      "[Training Epoch 8] Batch 1429, Loss 0.23785385489463806\n",
      "[Training Epoch 8] Batch 1430, Loss 0.2523937225341797\n",
      "[Training Epoch 8] Batch 1431, Loss 0.25292226672172546\n",
      "[Training Epoch 8] Batch 1432, Loss 0.2568499743938446\n",
      "[Training Epoch 8] Batch 1433, Loss 0.24531938135623932\n",
      "[Training Epoch 8] Batch 1434, Loss 0.2835084795951843\n",
      "[Training Epoch 8] Batch 1435, Loss 0.2413891851902008\n",
      "[Training Epoch 8] Batch 1436, Loss 0.25917142629623413\n",
      "[Training Epoch 8] Batch 1437, Loss 0.23716121912002563\n",
      "[Training Epoch 8] Batch 1438, Loss 0.23548433184623718\n",
      "[Training Epoch 8] Batch 1439, Loss 0.26895374059677124\n",
      "[Training Epoch 8] Batch 1440, Loss 0.2419116348028183\n",
      "[Training Epoch 8] Batch 1441, Loss 0.26227667927742004\n",
      "[Training Epoch 8] Batch 1442, Loss 0.26339295506477356\n",
      "[Training Epoch 8] Batch 1443, Loss 0.2857821583747864\n",
      "[Training Epoch 8] Batch 1444, Loss 0.26559752225875854\n",
      "[Training Epoch 8] Batch 1445, Loss 0.2587962746620178\n",
      "[Training Epoch 8] Batch 1446, Loss 0.23245525360107422\n",
      "[Training Epoch 8] Batch 1447, Loss 0.24133746325969696\n",
      "[Training Epoch 8] Batch 1448, Loss 0.2489105761051178\n",
      "[Training Epoch 8] Batch 1449, Loss 0.23673418164253235\n",
      "[Training Epoch 8] Batch 1450, Loss 0.26162758469581604\n",
      "[Training Epoch 8] Batch 1451, Loss 0.25761377811431885\n",
      "[Training Epoch 8] Batch 1452, Loss 0.24100469052791595\n",
      "[Training Epoch 8] Batch 1453, Loss 0.23780789971351624\n",
      "[Training Epoch 8] Batch 1454, Loss 0.2497994601726532\n",
      "[Training Epoch 8] Batch 1455, Loss 0.239142507314682\n",
      "[Training Epoch 8] Batch 1456, Loss 0.27807867527008057\n",
      "[Training Epoch 8] Batch 1457, Loss 0.23892948031425476\n",
      "[Training Epoch 8] Batch 1458, Loss 0.244388610124588\n",
      "[Training Epoch 8] Batch 1459, Loss 0.2529040575027466\n",
      "[Training Epoch 8] Batch 1460, Loss 0.24299213290214539\n",
      "[Training Epoch 8] Batch 1461, Loss 0.23750057816505432\n",
      "[Training Epoch 8] Batch 1462, Loss 0.24588245153427124\n",
      "[Training Epoch 8] Batch 1463, Loss 0.2575182020664215\n",
      "[Training Epoch 8] Batch 1464, Loss 0.2525288760662079\n",
      "[Training Epoch 8] Batch 1465, Loss 0.25700604915618896\n",
      "[Training Epoch 8] Batch 1466, Loss 0.2463780641555786\n",
      "[Training Epoch 8] Batch 1467, Loss 0.2414504587650299\n",
      "[Training Epoch 8] Batch 1468, Loss 0.2639579772949219\n",
      "[Training Epoch 8] Batch 1469, Loss 0.2575569152832031\n",
      "[Training Epoch 8] Batch 1470, Loss 0.2429424673318863\n",
      "[Training Epoch 8] Batch 1471, Loss 0.27757638692855835\n",
      "[Training Epoch 8] Batch 1472, Loss 0.26471659541130066\n",
      "[Training Epoch 8] Batch 1473, Loss 0.23678180575370789\n",
      "[Training Epoch 8] Batch 1474, Loss 0.2644801735877991\n",
      "[Training Epoch 8] Batch 1475, Loss 0.24234303832054138\n",
      "[Training Epoch 8] Batch 1476, Loss 0.22891190648078918\n",
      "[Training Epoch 8] Batch 1477, Loss 0.2507611811161041\n",
      "[Training Epoch 8] Batch 1478, Loss 0.26206839084625244\n",
      "[Training Epoch 8] Batch 1479, Loss 0.25553253293037415\n",
      "[Training Epoch 8] Batch 1480, Loss 0.24236738681793213\n",
      "[Training Epoch 8] Batch 1481, Loss 0.2830791771411896\n",
      "[Training Epoch 8] Batch 1482, Loss 0.22319988906383514\n",
      "[Training Epoch 8] Batch 1483, Loss 0.2726537585258484\n",
      "[Training Epoch 8] Batch 1484, Loss 0.2803937792778015\n",
      "[Training Epoch 8] Batch 1485, Loss 0.2455148994922638\n",
      "[Training Epoch 8] Batch 1486, Loss 0.26847004890441895\n",
      "[Training Epoch 8] Batch 1487, Loss 0.21939323842525482\n",
      "[Training Epoch 8] Batch 1488, Loss 0.2428029477596283\n",
      "[Training Epoch 8] Batch 1489, Loss 0.25368428230285645\n",
      "[Training Epoch 8] Batch 1490, Loss 0.27039068937301636\n",
      "[Training Epoch 8] Batch 1491, Loss 0.23904195427894592\n",
      "[Training Epoch 8] Batch 1492, Loss 0.26261773705482483\n",
      "[Training Epoch 8] Batch 1493, Loss 0.25551682710647583\n",
      "[Training Epoch 8] Batch 1494, Loss 0.2669917345046997\n",
      "[Training Epoch 8] Batch 1495, Loss 0.24208569526672363\n",
      "[Training Epoch 8] Batch 1496, Loss 0.28038299083709717\n",
      "[Training Epoch 8] Batch 1497, Loss 0.25833237171173096\n",
      "[Training Epoch 8] Batch 1498, Loss 0.27923253178596497\n",
      "[Training Epoch 8] Batch 1499, Loss 0.2674359679222107\n",
      "[Training Epoch 8] Batch 1500, Loss 0.225383922457695\n",
      "[Training Epoch 8] Batch 1501, Loss 0.22258225083351135\n",
      "[Training Epoch 8] Batch 1502, Loss 0.271176278591156\n",
      "[Training Epoch 8] Batch 1503, Loss 0.2781537175178528\n",
      "[Training Epoch 8] Batch 1504, Loss 0.2592301368713379\n",
      "[Training Epoch 8] Batch 1505, Loss 0.27167409658432007\n",
      "[Training Epoch 8] Batch 1506, Loss 0.2632160186767578\n",
      "[Training Epoch 8] Batch 1507, Loss 0.2804561257362366\n",
      "[Training Epoch 8] Batch 1508, Loss 0.26793211698532104\n",
      "[Training Epoch 8] Batch 1509, Loss 0.24771060049533844\n",
      "[Training Epoch 8] Batch 1510, Loss 0.25572285056114197\n",
      "[Training Epoch 8] Batch 1511, Loss 0.23283451795578003\n",
      "[Training Epoch 8] Batch 1512, Loss 0.28951430320739746\n",
      "[Training Epoch 8] Batch 1513, Loss 0.27717000246047974\n",
      "[Training Epoch 8] Batch 1514, Loss 0.27672335505485535\n",
      "[Training Epoch 8] Batch 1515, Loss 0.25920921564102173\n",
      "[Training Epoch 8] Batch 1516, Loss 0.2650759518146515\n",
      "[Training Epoch 8] Batch 1517, Loss 0.2645964026451111\n",
      "[Training Epoch 8] Batch 1518, Loss 0.24812762439250946\n",
      "[Training Epoch 8] Batch 1519, Loss 0.24725358188152313\n",
      "[Training Epoch 8] Batch 1520, Loss 0.23516926169395447\n",
      "[Training Epoch 8] Batch 1521, Loss 0.23798269033432007\n",
      "[Training Epoch 8] Batch 1522, Loss 0.2507835328578949\n",
      "[Training Epoch 8] Batch 1523, Loss 0.2687253952026367\n",
      "[Training Epoch 8] Batch 1524, Loss 0.24767398834228516\n",
      "[Training Epoch 8] Batch 1525, Loss 0.2647055387496948\n",
      "[Training Epoch 8] Batch 1526, Loss 0.23601335287094116\n",
      "[Training Epoch 8] Batch 1527, Loss 0.23757976293563843\n",
      "[Training Epoch 8] Batch 1528, Loss 0.24079398810863495\n",
      "[Training Epoch 8] Batch 1529, Loss 0.2776142954826355\n",
      "[Training Epoch 8] Batch 1530, Loss 0.2554948627948761\n",
      "[Training Epoch 8] Batch 1531, Loss 0.25320369005203247\n",
      "[Training Epoch 8] Batch 1532, Loss 0.25633177161216736\n",
      "[Training Epoch 8] Batch 1533, Loss 0.2502935826778412\n",
      "[Training Epoch 8] Batch 1534, Loss 0.2639344334602356\n",
      "[Training Epoch 8] Batch 1535, Loss 0.25038713216781616\n",
      "[Training Epoch 8] Batch 1536, Loss 0.2589118480682373\n",
      "[Training Epoch 8] Batch 1537, Loss 0.2795383930206299\n",
      "[Training Epoch 8] Batch 1538, Loss 0.25130677223205566\n",
      "[Training Epoch 8] Batch 1539, Loss 0.2575090825557709\n",
      "[Training Epoch 8] Batch 1540, Loss 0.27113819122314453\n",
      "[Training Epoch 8] Batch 1541, Loss 0.25665849447250366\n",
      "[Training Epoch 8] Batch 1542, Loss 0.3032299280166626\n",
      "[Training Epoch 8] Batch 1543, Loss 0.2268843948841095\n",
      "[Training Epoch 8] Batch 1544, Loss 0.23498167097568512\n",
      "[Training Epoch 8] Batch 1545, Loss 0.23204991221427917\n",
      "[Training Epoch 8] Batch 1546, Loss 0.22912266850471497\n",
      "[Training Epoch 8] Batch 1547, Loss 0.23198211193084717\n",
      "[Training Epoch 8] Batch 1548, Loss 0.2430809587240219\n",
      "[Training Epoch 8] Batch 1549, Loss 0.24680760502815247\n",
      "[Training Epoch 8] Batch 1550, Loss 0.2764796316623688\n",
      "[Training Epoch 8] Batch 1551, Loss 0.2510637640953064\n",
      "[Training Epoch 8] Batch 1552, Loss 0.2920355796813965\n",
      "[Training Epoch 8] Batch 1553, Loss 0.2653389871120453\n",
      "[Training Epoch 8] Batch 1554, Loss 0.2593824863433838\n",
      "[Training Epoch 8] Batch 1555, Loss 0.26542365550994873\n",
      "[Training Epoch 8] Batch 1556, Loss 0.2370503693819046\n",
      "[Training Epoch 8] Batch 1557, Loss 0.25237584114074707\n",
      "[Training Epoch 8] Batch 1558, Loss 0.27877408266067505\n",
      "[Training Epoch 8] Batch 1559, Loss 0.2511739730834961\n",
      "[Training Epoch 8] Batch 1560, Loss 0.2550925612449646\n",
      "[Training Epoch 8] Batch 1561, Loss 0.26487261056900024\n",
      "[Training Epoch 8] Batch 1562, Loss 0.2240404337644577\n",
      "[Training Epoch 8] Batch 1563, Loss 0.25421231985092163\n",
      "[Training Epoch 8] Batch 1564, Loss 0.23894311487674713\n",
      "[Training Epoch 8] Batch 1565, Loss 0.24748572707176208\n",
      "[Training Epoch 8] Batch 1566, Loss 0.23045960068702698\n",
      "[Training Epoch 8] Batch 1567, Loss 0.2276499718427658\n",
      "[Training Epoch 8] Batch 1568, Loss 0.22340568900108337\n",
      "[Training Epoch 8] Batch 1569, Loss 0.25297290086746216\n",
      "[Training Epoch 8] Batch 1570, Loss 0.2713010013103485\n",
      "[Training Epoch 8] Batch 1571, Loss 0.25292307138442993\n",
      "[Training Epoch 8] Batch 1572, Loss 0.24900643527507782\n",
      "[Training Epoch 8] Batch 1573, Loss 0.26182031631469727\n",
      "[Training Epoch 8] Batch 1574, Loss 0.2746478021144867\n",
      "[Training Epoch 8] Batch 1575, Loss 0.23311202228069305\n",
      "[Training Epoch 8] Batch 1576, Loss 0.24103736877441406\n",
      "[Training Epoch 8] Batch 1577, Loss 0.25644904375076294\n",
      "[Training Epoch 8] Batch 1578, Loss 0.25957614183425903\n",
      "[Training Epoch 8] Batch 1579, Loss 0.24946586787700653\n",
      "[Training Epoch 8] Batch 1580, Loss 0.2259548157453537\n",
      "[Training Epoch 8] Batch 1581, Loss 0.2580350637435913\n",
      "[Training Epoch 8] Batch 1582, Loss 0.24119459092617035\n",
      "[Training Epoch 8] Batch 1583, Loss 0.23345215618610382\n",
      "[Training Epoch 8] Batch 1584, Loss 0.27425336837768555\n",
      "[Training Epoch 8] Batch 1585, Loss 0.23638126254081726\n",
      "[Training Epoch 8] Batch 1586, Loss 0.24909010529518127\n",
      "[Training Epoch 8] Batch 1587, Loss 0.26648449897766113\n",
      "[Training Epoch 8] Batch 1588, Loss 0.2888681888580322\n",
      "[Training Epoch 8] Batch 1589, Loss 0.24926629662513733\n",
      "[Training Epoch 8] Batch 1590, Loss 0.24073576927185059\n",
      "[Training Epoch 8] Batch 1591, Loss 0.26568806171417236\n",
      "[Training Epoch 8] Batch 1592, Loss 0.2453674077987671\n",
      "[Training Epoch 8] Batch 1593, Loss 0.23779991269111633\n",
      "[Training Epoch 8] Batch 1594, Loss 0.262031614780426\n",
      "[Training Epoch 8] Batch 1595, Loss 0.2169627547264099\n",
      "[Training Epoch 8] Batch 1596, Loss 0.2581852078437805\n",
      "[Training Epoch 8] Batch 1597, Loss 0.2597617506980896\n",
      "[Training Epoch 8] Batch 1598, Loss 0.2525807321071625\n",
      "[Training Epoch 8] Batch 1599, Loss 0.24611404538154602\n",
      "[Training Epoch 8] Batch 1600, Loss 0.28752583265304565\n",
      "[Training Epoch 8] Batch 1601, Loss 0.25531938672065735\n",
      "[Training Epoch 8] Batch 1602, Loss 0.2624864876270294\n",
      "[Training Epoch 8] Batch 1603, Loss 0.2655848264694214\n",
      "[Training Epoch 8] Batch 1604, Loss 0.2508362829685211\n",
      "[Training Epoch 8] Batch 1605, Loss 0.2586875259876251\n",
      "[Training Epoch 8] Batch 1606, Loss 0.2658597230911255\n",
      "[Training Epoch 8] Batch 1607, Loss 0.2637585699558258\n",
      "[Training Epoch 8] Batch 1608, Loss 0.2517448663711548\n",
      "[Training Epoch 8] Batch 1609, Loss 0.2503279447555542\n",
      "[Training Epoch 8] Batch 1610, Loss 0.2659589648246765\n",
      "[Training Epoch 8] Batch 1611, Loss 0.23660537600517273\n",
      "[Training Epoch 8] Batch 1612, Loss 0.2495131939649582\n",
      "[Training Epoch 8] Batch 1613, Loss 0.2661288380622864\n",
      "[Training Epoch 8] Batch 1614, Loss 0.24551859498023987\n",
      "[Training Epoch 8] Batch 1615, Loss 0.26578187942504883\n",
      "[Training Epoch 8] Batch 1616, Loss 0.2415182739496231\n",
      "[Training Epoch 8] Batch 1617, Loss 0.2709004282951355\n",
      "[Training Epoch 8] Batch 1618, Loss 0.2381323277950287\n",
      "[Training Epoch 8] Batch 1619, Loss 0.2822706997394562\n",
      "[Training Epoch 8] Batch 1620, Loss 0.23080071806907654\n",
      "[Training Epoch 8] Batch 1621, Loss 0.2589699625968933\n",
      "[Training Epoch 8] Batch 1622, Loss 0.28973907232284546\n",
      "[Training Epoch 8] Batch 1623, Loss 0.2507326900959015\n",
      "[Training Epoch 8] Batch 1624, Loss 0.2313394546508789\n",
      "[Training Epoch 8] Batch 1625, Loss 0.24581803381443024\n",
      "[Training Epoch 8] Batch 1626, Loss 0.21277260780334473\n",
      "[Training Epoch 8] Batch 1627, Loss 0.23912309110164642\n",
      "[Training Epoch 8] Batch 1628, Loss 0.2699878215789795\n",
      "[Training Epoch 8] Batch 1629, Loss 0.25416839122772217\n",
      "[Training Epoch 8] Batch 1630, Loss 0.2755676209926605\n",
      "[Training Epoch 8] Batch 1631, Loss 0.2345874011516571\n",
      "[Training Epoch 8] Batch 1632, Loss 0.266766756772995\n",
      "[Training Epoch 8] Batch 1633, Loss 0.2807404100894928\n",
      "[Training Epoch 8] Batch 1634, Loss 0.2635604739189148\n",
      "[Training Epoch 8] Batch 1635, Loss 0.2832731604576111\n",
      "[Training Epoch 8] Batch 1636, Loss 0.25248393416404724\n",
      "[Training Epoch 8] Batch 1637, Loss 0.21323451399803162\n",
      "[Training Epoch 8] Batch 1638, Loss 0.23659709095954895\n",
      "[Training Epoch 8] Batch 1639, Loss 0.2711767256259918\n",
      "[Training Epoch 8] Batch 1640, Loss 0.24542471766471863\n",
      "[Training Epoch 8] Batch 1641, Loss 0.25670039653778076\n",
      "[Training Epoch 8] Batch 1642, Loss 0.26300299167633057\n",
      "[Training Epoch 8] Batch 1643, Loss 0.21654994785785675\n",
      "[Training Epoch 8] Batch 1644, Loss 0.22077998518943787\n",
      "[Training Epoch 8] Batch 1645, Loss 0.2588513195514679\n",
      "[Training Epoch 8] Batch 1646, Loss 0.24179622530937195\n",
      "[Training Epoch 8] Batch 1647, Loss 0.23986056447029114\n",
      "[Training Epoch 8] Batch 1648, Loss 0.25702208280563354\n",
      "[Training Epoch 8] Batch 1649, Loss 0.23479615151882172\n",
      "[Training Epoch 8] Batch 1650, Loss 0.27019378542900085\n",
      "[Training Epoch 8] Batch 1651, Loss 0.22844231128692627\n",
      "[Training Epoch 8] Batch 1652, Loss 0.24399054050445557\n",
      "[Training Epoch 8] Batch 1653, Loss 0.2662137746810913\n",
      "[Training Epoch 8] Batch 1654, Loss 0.22288022935390472\n",
      "[Training Epoch 8] Batch 1655, Loss 0.258006751537323\n",
      "[Training Epoch 8] Batch 1656, Loss 0.24817056953907013\n",
      "[Training Epoch 8] Batch 1657, Loss 0.24452772736549377\n",
      "[Training Epoch 8] Batch 1658, Loss 0.22690212726593018\n",
      "[Training Epoch 8] Batch 1659, Loss 0.23465773463249207\n",
      "[Training Epoch 8] Batch 1660, Loss 0.24125871062278748\n",
      "[Training Epoch 8] Batch 1661, Loss 0.25124964118003845\n",
      "[Training Epoch 8] Batch 1662, Loss 0.27867591381073\n",
      "[Training Epoch 8] Batch 1663, Loss 0.23962146043777466\n",
      "[Training Epoch 8] Batch 1664, Loss 0.275873064994812\n",
      "[Training Epoch 8] Batch 1665, Loss 0.2654409110546112\n",
      "[Training Epoch 8] Batch 1666, Loss 0.23316359519958496\n",
      "[Training Epoch 8] Batch 1667, Loss 0.2585268020629883\n",
      "[Training Epoch 8] Batch 1668, Loss 0.2589590847492218\n",
      "[Training Epoch 8] Batch 1669, Loss 0.23922836780548096\n",
      "[Training Epoch 8] Batch 1670, Loss 0.26546308398246765\n",
      "[Training Epoch 8] Batch 1671, Loss 0.2346319854259491\n",
      "[Training Epoch 8] Batch 1672, Loss 0.2904825210571289\n",
      "[Training Epoch 8] Batch 1673, Loss 0.26285433769226074\n",
      "[Training Epoch 8] Batch 1674, Loss 0.2912079393863678\n",
      "[Training Epoch 8] Batch 1675, Loss 0.2728394567966461\n",
      "[Training Epoch 8] Batch 1676, Loss 0.2513110637664795\n",
      "[Training Epoch 8] Batch 1677, Loss 0.2371477484703064\n",
      "[Training Epoch 8] Batch 1678, Loss 0.2128239870071411\n",
      "[Training Epoch 8] Batch 1679, Loss 0.26051315665245056\n",
      "[Training Epoch 8] Batch 1680, Loss 0.2464851438999176\n",
      "[Training Epoch 8] Batch 1681, Loss 0.23615379631519318\n",
      "[Training Epoch 8] Batch 1682, Loss 0.3018297553062439\n",
      "[Training Epoch 8] Batch 1683, Loss 0.2689003348350525\n",
      "[Training Epoch 8] Batch 1684, Loss 0.26638320088386536\n",
      "[Training Epoch 8] Batch 1685, Loss 0.2905460000038147\n",
      "[Training Epoch 8] Batch 1686, Loss 0.2617986500263214\n",
      "[Training Epoch 8] Batch 1687, Loss 0.2702650725841522\n",
      "[Training Epoch 8] Batch 1688, Loss 0.24717086553573608\n",
      "[Training Epoch 8] Batch 1689, Loss 0.25781458616256714\n",
      "[Training Epoch 8] Batch 1690, Loss 0.2696652412414551\n",
      "[Training Epoch 8] Batch 1691, Loss 0.23916783928871155\n",
      "[Training Epoch 8] Batch 1692, Loss 0.21975234150886536\n",
      "[Training Epoch 8] Batch 1693, Loss 0.26191285252571106\n",
      "[Training Epoch 8] Batch 1694, Loss 0.22998633980751038\n",
      "[Training Epoch 8] Batch 1695, Loss 0.22343197464942932\n",
      "[Training Epoch 8] Batch 1696, Loss 0.24253538250923157\n",
      "[Training Epoch 8] Batch 1697, Loss 0.31113651394844055\n",
      "[Training Epoch 8] Batch 1698, Loss 0.2567710876464844\n",
      "[Training Epoch 8] Batch 1699, Loss 0.23855748772621155\n",
      "[Training Epoch 8] Batch 1700, Loss 0.28732025623321533\n",
      "[Training Epoch 8] Batch 1701, Loss 0.2675160765647888\n",
      "[Training Epoch 8] Batch 1702, Loss 0.25252366065979004\n",
      "[Training Epoch 8] Batch 1703, Loss 0.25882256031036377\n",
      "[Training Epoch 8] Batch 1704, Loss 0.27375614643096924\n",
      "[Training Epoch 8] Batch 1705, Loss 0.2621312141418457\n",
      "[Training Epoch 8] Batch 1706, Loss 0.2708917260169983\n",
      "[Training Epoch 8] Batch 1707, Loss 0.2526074945926666\n",
      "[Training Epoch 8] Batch 1708, Loss 0.2447315752506256\n",
      "[Training Epoch 8] Batch 1709, Loss 0.24510535597801208\n",
      "[Training Epoch 8] Batch 1710, Loss 0.25213682651519775\n",
      "[Training Epoch 8] Batch 1711, Loss 0.26261669397354126\n",
      "[Training Epoch 8] Batch 1712, Loss 0.26397979259490967\n",
      "[Training Epoch 8] Batch 1713, Loss 0.26601868867874146\n",
      "[Training Epoch 8] Batch 1714, Loss 0.23718976974487305\n",
      "[Training Epoch 8] Batch 1715, Loss 0.25650927424430847\n",
      "[Training Epoch 8] Batch 1716, Loss 0.2265975922346115\n",
      "[Training Epoch 8] Batch 1717, Loss 0.2608044147491455\n",
      "[Training Epoch 8] Batch 1718, Loss 0.25021421909332275\n",
      "[Training Epoch 8] Batch 1719, Loss 0.276962548494339\n",
      "[Training Epoch 8] Batch 1720, Loss 0.24502548575401306\n",
      "[Training Epoch 8] Batch 1721, Loss 0.28770336508750916\n",
      "[Training Epoch 8] Batch 1722, Loss 0.24396976828575134\n",
      "[Training Epoch 8] Batch 1723, Loss 0.24470388889312744\n",
      "[Training Epoch 8] Batch 1724, Loss 0.2156522274017334\n",
      "[Training Epoch 8] Batch 1725, Loss 0.23752599954605103\n",
      "[Training Epoch 8] Batch 1726, Loss 0.23328468203544617\n",
      "[Training Epoch 8] Batch 1727, Loss 0.2575758695602417\n",
      "[Training Epoch 8] Batch 1728, Loss 0.2239532172679901\n",
      "[Training Epoch 8] Batch 1729, Loss 0.2588306665420532\n",
      "[Training Epoch 8] Batch 1730, Loss 0.26423051953315735\n",
      "[Training Epoch 8] Batch 1731, Loss 0.2607683539390564\n",
      "[Training Epoch 8] Batch 1732, Loss 0.25548815727233887\n",
      "[Training Epoch 8] Batch 1733, Loss 0.22820328176021576\n",
      "[Training Epoch 8] Batch 1734, Loss 0.26514163613319397\n",
      "[Training Epoch 8] Batch 1735, Loss 0.2832699418067932\n",
      "[Training Epoch 8] Batch 1736, Loss 0.2701600193977356\n",
      "[Training Epoch 8] Batch 1737, Loss 0.2773807644844055\n",
      "[Training Epoch 8] Batch 1738, Loss 0.261149525642395\n",
      "[Training Epoch 8] Batch 1739, Loss 0.23914577066898346\n",
      "[Training Epoch 8] Batch 1740, Loss 0.2607646584510803\n",
      "[Training Epoch 8] Batch 1741, Loss 0.27799805998802185\n",
      "[Training Epoch 8] Batch 1742, Loss 0.28042250871658325\n",
      "[Training Epoch 8] Batch 1743, Loss 0.2453559935092926\n",
      "[Training Epoch 8] Batch 1744, Loss 0.2691396474838257\n",
      "[Training Epoch 8] Batch 1745, Loss 0.2322605699300766\n",
      "[Training Epoch 8] Batch 1746, Loss 0.25583890080451965\n",
      "[Training Epoch 8] Batch 1747, Loss 0.24059706926345825\n",
      "[Training Epoch 8] Batch 1748, Loss 0.2538003623485565\n",
      "[Training Epoch 8] Batch 1749, Loss 0.2560727000236511\n",
      "[Training Epoch 8] Batch 1750, Loss 0.25733527541160583\n",
      "[Training Epoch 8] Batch 1751, Loss 0.23620951175689697\n",
      "[Training Epoch 8] Batch 1752, Loss 0.24150145053863525\n",
      "[Training Epoch 8] Batch 1753, Loss 0.2682449221611023\n",
      "[Training Epoch 8] Batch 1754, Loss 0.2523621916770935\n",
      "[Training Epoch 8] Batch 1755, Loss 0.22907747328281403\n",
      "[Training Epoch 8] Batch 1756, Loss 0.254917174577713\n",
      "[Training Epoch 8] Batch 1757, Loss 0.2404835820198059\n",
      "[Training Epoch 8] Batch 1758, Loss 0.2553068995475769\n",
      "[Training Epoch 8] Batch 1759, Loss 0.3021300137042999\n",
      "[Training Epoch 8] Batch 1760, Loss 0.25074395537376404\n",
      "[Training Epoch 8] Batch 1761, Loss 0.23784810304641724\n",
      "[Training Epoch 8] Batch 1762, Loss 0.26510703563690186\n",
      "[Training Epoch 8] Batch 1763, Loss 0.2566632628440857\n",
      "[Training Epoch 8] Batch 1764, Loss 0.2368839979171753\n",
      "[Training Epoch 8] Batch 1765, Loss 0.21151600778102875\n",
      "[Training Epoch 8] Batch 1766, Loss 0.24369029700756073\n",
      "[Training Epoch 8] Batch 1767, Loss 0.24273161590099335\n",
      "[Training Epoch 8] Batch 1768, Loss 0.2530039846897125\n",
      "[Training Epoch 8] Batch 1769, Loss 0.2515919804573059\n",
      "[Training Epoch 8] Batch 1770, Loss 0.28827500343322754\n",
      "[Training Epoch 8] Batch 1771, Loss 0.248334139585495\n",
      "[Training Epoch 8] Batch 1772, Loss 0.25079143047332764\n",
      "[Training Epoch 8] Batch 1773, Loss 0.24674522876739502\n",
      "[Training Epoch 8] Batch 1774, Loss 0.2684939205646515\n",
      "[Training Epoch 8] Batch 1775, Loss 0.2587248682975769\n",
      "[Training Epoch 8] Batch 1776, Loss 0.28006628155708313\n",
      "[Training Epoch 8] Batch 1777, Loss 0.2349979132413864\n",
      "[Training Epoch 8] Batch 1778, Loss 0.253493070602417\n",
      "[Training Epoch 8] Batch 1779, Loss 0.2552845776081085\n",
      "[Training Epoch 8] Batch 1780, Loss 0.2732321619987488\n",
      "[Training Epoch 8] Batch 1781, Loss 0.28950363397598267\n",
      "[Training Epoch 8] Batch 1782, Loss 0.23058545589447021\n",
      "[Training Epoch 8] Batch 1783, Loss 0.23617200553417206\n",
      "[Training Epoch 8] Batch 1784, Loss 0.23413068056106567\n",
      "[Training Epoch 8] Batch 1785, Loss 0.26154184341430664\n",
      "[Training Epoch 8] Batch 1786, Loss 0.26426559686660767\n",
      "[Training Epoch 8] Batch 1787, Loss 0.2505154609680176\n",
      "[Training Epoch 8] Batch 1788, Loss 0.25992488861083984\n",
      "[Training Epoch 8] Batch 1789, Loss 0.22945785522460938\n",
      "[Training Epoch 8] Batch 1790, Loss 0.24136212468147278\n",
      "[Training Epoch 8] Batch 1791, Loss 0.2567986249923706\n",
      "[Training Epoch 8] Batch 1792, Loss 0.272003710269928\n",
      "[Training Epoch 8] Batch 1793, Loss 0.26222872734069824\n",
      "[Training Epoch 8] Batch 1794, Loss 0.2930487394332886\n",
      "[Training Epoch 8] Batch 1795, Loss 0.2445741593837738\n",
      "[Training Epoch 8] Batch 1796, Loss 0.26041990518569946\n",
      "[Training Epoch 8] Batch 1797, Loss 0.2499675452709198\n",
      "[Training Epoch 8] Batch 1798, Loss 0.27308228611946106\n",
      "[Training Epoch 8] Batch 1799, Loss 0.23816797137260437\n",
      "[Training Epoch 8] Batch 1800, Loss 0.2613542675971985\n",
      "[Training Epoch 8] Batch 1801, Loss 0.23314470052719116\n",
      "[Training Epoch 8] Batch 1802, Loss 0.23107144236564636\n",
      "[Training Epoch 8] Batch 1803, Loss 0.2535347640514374\n",
      "[Training Epoch 8] Batch 1804, Loss 0.2376250922679901\n",
      "[Training Epoch 8] Batch 1805, Loss 0.2500845193862915\n",
      "[Training Epoch 8] Batch 1806, Loss 0.24845671653747559\n",
      "[Training Epoch 8] Batch 1807, Loss 0.27898842096328735\n",
      "[Training Epoch 8] Batch 1808, Loss 0.2504701614379883\n",
      "[Training Epoch 8] Batch 1809, Loss 0.2541236877441406\n",
      "[Training Epoch 8] Batch 1810, Loss 0.25438445806503296\n",
      "[Training Epoch 8] Batch 1811, Loss 0.24866941571235657\n",
      "[Training Epoch 8] Batch 1812, Loss 0.26964348554611206\n",
      "[Training Epoch 8] Batch 1813, Loss 0.246055006980896\n",
      "[Training Epoch 8] Batch 1814, Loss 0.2559315264225006\n",
      "[Training Epoch 8] Batch 1815, Loss 0.23816663026809692\n",
      "[Training Epoch 8] Batch 1816, Loss 0.25827986001968384\n",
      "[Training Epoch 8] Batch 1817, Loss 0.23761986196041107\n",
      "[Training Epoch 8] Batch 1818, Loss 0.2527059316635132\n",
      "[Training Epoch 8] Batch 1819, Loss 0.2918379604816437\n",
      "[Training Epoch 8] Batch 1820, Loss 0.22670526802539825\n",
      "[Training Epoch 8] Batch 1821, Loss 0.25719979405403137\n",
      "[Training Epoch 8] Batch 1822, Loss 0.25048747658729553\n",
      "[Training Epoch 8] Batch 1823, Loss 0.244023859500885\n",
      "[Training Epoch 8] Batch 1824, Loss 0.24877624213695526\n",
      "[Training Epoch 8] Batch 1825, Loss 0.2897074520587921\n",
      "[Training Epoch 8] Batch 1826, Loss 0.24967414140701294\n",
      "[Training Epoch 8] Batch 1827, Loss 0.25940626859664917\n",
      "[Training Epoch 8] Batch 1828, Loss 0.24212852120399475\n",
      "[Training Epoch 8] Batch 1829, Loss 0.25088390707969666\n",
      "[Training Epoch 8] Batch 1830, Loss 0.26057204604148865\n",
      "[Training Epoch 8] Batch 1831, Loss 0.23272670805454254\n",
      "[Training Epoch 8] Batch 1832, Loss 0.2522253096103668\n",
      "[Training Epoch 8] Batch 1833, Loss 0.2606268525123596\n",
      "[Training Epoch 8] Batch 1834, Loss 0.2891960144042969\n",
      "[Training Epoch 8] Batch 1835, Loss 0.24208232760429382\n",
      "[Training Epoch 8] Batch 1836, Loss 0.2743121087551117\n",
      "[Training Epoch 8] Batch 1837, Loss 0.22346508502960205\n",
      "[Training Epoch 8] Batch 1838, Loss 0.2312784492969513\n",
      "[Training Epoch 8] Batch 1839, Loss 0.25870656967163086\n",
      "[Training Epoch 8] Batch 1840, Loss 0.25980305671691895\n",
      "[Training Epoch 8] Batch 1841, Loss 0.25506913661956787\n",
      "[Training Epoch 8] Batch 1842, Loss 0.23315292596817017\n",
      "[Training Epoch 8] Batch 1843, Loss 0.25716790556907654\n",
      "[Training Epoch 8] Batch 1844, Loss 0.2481328845024109\n",
      "[Training Epoch 8] Batch 1845, Loss 0.2758024334907532\n",
      "[Training Epoch 8] Batch 1846, Loss 0.2697941064834595\n",
      "[Training Epoch 8] Batch 1847, Loss 0.2595059275627136\n",
      "[Training Epoch 8] Batch 1848, Loss 0.2672882080078125\n",
      "[Training Epoch 8] Batch 1849, Loss 0.2452167570590973\n",
      "[Training Epoch 8] Batch 1850, Loss 0.22396990656852722\n",
      "[Training Epoch 8] Batch 1851, Loss 0.25314581394195557\n",
      "[Training Epoch 8] Batch 1852, Loss 0.23776867985725403\n",
      "[Training Epoch 8] Batch 1853, Loss 0.28222939372062683\n",
      "[Training Epoch 8] Batch 1854, Loss 0.24327798187732697\n",
      "[Training Epoch 8] Batch 1855, Loss 0.2736984193325043\n",
      "[Training Epoch 8] Batch 1856, Loss 0.22364330291748047\n",
      "[Training Epoch 8] Batch 1857, Loss 0.27123329043388367\n",
      "[Training Epoch 8] Batch 1858, Loss 0.22897864878177643\n",
      "[Training Epoch 8] Batch 1859, Loss 0.2641974091529846\n",
      "[Training Epoch 8] Batch 1860, Loss 0.24482965469360352\n",
      "[Training Epoch 8] Batch 1861, Loss 0.252872496843338\n",
      "[Training Epoch 8] Batch 1862, Loss 0.23596397042274475\n",
      "[Training Epoch 8] Batch 1863, Loss 0.2699970602989197\n",
      "[Training Epoch 8] Batch 1864, Loss 0.24543680250644684\n",
      "[Training Epoch 8] Batch 1865, Loss 0.28138819336891174\n",
      "[Training Epoch 8] Batch 1866, Loss 0.2552884817123413\n",
      "[Training Epoch 8] Batch 1867, Loss 0.2708933651447296\n",
      "[Training Epoch 8] Batch 1868, Loss 0.2790311276912689\n",
      "[Training Epoch 8] Batch 1869, Loss 0.25635671615600586\n",
      "[Training Epoch 8] Batch 1870, Loss 0.24875661730766296\n",
      "[Training Epoch 8] Batch 1871, Loss 0.26826211810112\n",
      "[Training Epoch 8] Batch 1872, Loss 0.24091306328773499\n",
      "[Training Epoch 8] Batch 1873, Loss 0.24630124866962433\n",
      "[Training Epoch 8] Batch 1874, Loss 0.23305021226406097\n",
      "[Training Epoch 8] Batch 1875, Loss 0.2693684697151184\n",
      "[Training Epoch 8] Batch 1876, Loss 0.2769658863544464\n",
      "[Training Epoch 8] Batch 1877, Loss 0.2979198098182678\n",
      "[Training Epoch 8] Batch 1878, Loss 0.23144176602363586\n",
      "[Training Epoch 8] Batch 1879, Loss 0.26152876019477844\n",
      "[Training Epoch 8] Batch 1880, Loss 0.25093895196914673\n",
      "[Training Epoch 8] Batch 1881, Loss 0.2633414566516876\n",
      "[Training Epoch 8] Batch 1882, Loss 0.275736540555954\n",
      "[Training Epoch 8] Batch 1883, Loss 0.24812838435173035\n",
      "[Training Epoch 8] Batch 1884, Loss 0.24968719482421875\n",
      "[Training Epoch 8] Batch 1885, Loss 0.28359317779541016\n",
      "[Training Epoch 8] Batch 1886, Loss 0.23117035627365112\n",
      "[Training Epoch 8] Batch 1887, Loss 0.25178617238998413\n",
      "[Training Epoch 8] Batch 1888, Loss 0.2561712861061096\n",
      "[Training Epoch 8] Batch 1889, Loss 0.22292941808700562\n",
      "[Training Epoch 8] Batch 1890, Loss 0.29566019773483276\n",
      "[Training Epoch 8] Batch 1891, Loss 0.2593298554420471\n",
      "[Training Epoch 8] Batch 1892, Loss 0.25493255257606506\n",
      "[Training Epoch 8] Batch 1893, Loss 0.26755356788635254\n",
      "[Training Epoch 8] Batch 1894, Loss 0.24778461456298828\n",
      "[Training Epoch 8] Batch 1895, Loss 0.2721059024333954\n",
      "[Training Epoch 8] Batch 1896, Loss 0.29225969314575195\n",
      "[Training Epoch 8] Batch 1897, Loss 0.26110899448394775\n",
      "[Training Epoch 8] Batch 1898, Loss 0.27414530515670776\n",
      "[Training Epoch 8] Batch 1899, Loss 0.2672808766365051\n",
      "[Training Epoch 8] Batch 1900, Loss 0.26312974095344543\n",
      "[Training Epoch 8] Batch 1901, Loss 0.2575167715549469\n",
      "[Training Epoch 8] Batch 1902, Loss 0.2698984444141388\n",
      "[Training Epoch 8] Batch 1903, Loss 0.2797960042953491\n",
      "[Training Epoch 8] Batch 1904, Loss 0.2840800881385803\n",
      "[Training Epoch 8] Batch 1905, Loss 0.2381868213415146\n",
      "[Training Epoch 8] Batch 1906, Loss 0.23736223578453064\n",
      "[Training Epoch 8] Batch 1907, Loss 0.24129575490951538\n",
      "[Training Epoch 8] Batch 1908, Loss 0.25336283445358276\n",
      "[Training Epoch 8] Batch 1909, Loss 0.23833203315734863\n",
      "[Training Epoch 8] Batch 1910, Loss 0.23350587487220764\n",
      "[Training Epoch 8] Batch 1911, Loss 0.26041552424430847\n",
      "[Training Epoch 8] Batch 1912, Loss 0.25371474027633667\n",
      "[Training Epoch 8] Batch 1913, Loss 0.2878037691116333\n",
      "[Training Epoch 8] Batch 1914, Loss 0.266856849193573\n",
      "[Training Epoch 8] Batch 1915, Loss 0.27621206641197205\n",
      "[Training Epoch 8] Batch 1916, Loss 0.2512804865837097\n",
      "[Training Epoch 8] Batch 1917, Loss 0.26362523436546326\n",
      "[Training Epoch 8] Batch 1918, Loss 0.24319617450237274\n",
      "[Training Epoch 8] Batch 1919, Loss 0.23974597454071045\n",
      "[Training Epoch 8] Batch 1920, Loss 0.262126088142395\n",
      "[Training Epoch 8] Batch 1921, Loss 0.25951677560806274\n",
      "[Training Epoch 8] Batch 1922, Loss 0.2603500187397003\n",
      "[Training Epoch 8] Batch 1923, Loss 0.2505660057067871\n",
      "[Training Epoch 8] Batch 1924, Loss 0.2754828929901123\n",
      "[Training Epoch 8] Batch 1925, Loss 0.24823658168315887\n",
      "[Training Epoch 8] Batch 1926, Loss 0.25110742449760437\n",
      "[Training Epoch 8] Batch 1927, Loss 0.2751087546348572\n",
      "[Training Epoch 8] Batch 1928, Loss 0.2453705072402954\n",
      "[Training Epoch 8] Batch 1929, Loss 0.2541895806789398\n",
      "[Training Epoch 8] Batch 1930, Loss 0.2388802170753479\n",
      "[Training Epoch 8] Batch 1931, Loss 0.20399436354637146\n",
      "[Training Epoch 8] Batch 1932, Loss 0.254722535610199\n",
      "[Training Epoch 8] Batch 1933, Loss 0.2656863331794739\n",
      "[Training Epoch 8] Batch 1934, Loss 0.27670735120773315\n",
      "[Training Epoch 8] Batch 1935, Loss 0.25449424982070923\n",
      "[Training Epoch 8] Batch 1936, Loss 0.25347334146499634\n",
      "[Training Epoch 8] Batch 1937, Loss 0.2565455436706543\n",
      "[Training Epoch 8] Batch 1938, Loss 0.2563416361808777\n",
      "[Training Epoch 8] Batch 1939, Loss 0.2493639886379242\n",
      "[Training Epoch 8] Batch 1940, Loss 0.27175724506378174\n",
      "[Training Epoch 8] Batch 1941, Loss 0.24993592500686646\n",
      "[Training Epoch 8] Batch 1942, Loss 0.26763463020324707\n",
      "[Training Epoch 8] Batch 1943, Loss 0.249778151512146\n",
      "[Training Epoch 8] Batch 1944, Loss 0.2428666651248932\n",
      "[Training Epoch 8] Batch 1945, Loss 0.23109015822410583\n",
      "[Training Epoch 8] Batch 1946, Loss 0.25960344076156616\n",
      "[Training Epoch 8] Batch 1947, Loss 0.2795052230358124\n",
      "[Training Epoch 8] Batch 1948, Loss 0.2506217956542969\n",
      "[Training Epoch 8] Batch 1949, Loss 0.23475323617458344\n",
      "[Training Epoch 8] Batch 1950, Loss 0.2352524995803833\n",
      "[Training Epoch 8] Batch 1951, Loss 0.227489173412323\n",
      "[Training Epoch 8] Batch 1952, Loss 0.2699522078037262\n",
      "[Training Epoch 8] Batch 1953, Loss 0.239143967628479\n",
      "[Training Epoch 8] Batch 1954, Loss 0.2622029781341553\n",
      "[Training Epoch 8] Batch 1955, Loss 0.2482566088438034\n",
      "[Training Epoch 8] Batch 1956, Loss 0.2789683938026428\n",
      "[Training Epoch 8] Batch 1957, Loss 0.2406298816204071\n",
      "[Training Epoch 8] Batch 1958, Loss 0.22759459912776947\n",
      "[Training Epoch 8] Batch 1959, Loss 0.2139098048210144\n",
      "[Training Epoch 8] Batch 1960, Loss 0.26524829864501953\n",
      "[Training Epoch 8] Batch 1961, Loss 0.2772839665412903\n",
      "[Training Epoch 8] Batch 1962, Loss 0.2241690456867218\n",
      "[Training Epoch 8] Batch 1963, Loss 0.25172528624534607\n",
      "[Training Epoch 8] Batch 1964, Loss 0.28011077642440796\n",
      "[Training Epoch 8] Batch 1965, Loss 0.2502559423446655\n",
      "[Training Epoch 8] Batch 1966, Loss 0.27321651577949524\n",
      "[Training Epoch 8] Batch 1967, Loss 0.24452248215675354\n",
      "[Training Epoch 8] Batch 1968, Loss 0.23406974971294403\n",
      "[Training Epoch 8] Batch 1969, Loss 0.24273255467414856\n",
      "[Training Epoch 8] Batch 1970, Loss 0.23749259114265442\n",
      "[Training Epoch 8] Batch 1971, Loss 0.2701406478881836\n",
      "[Training Epoch 8] Batch 1972, Loss 0.24313771724700928\n",
      "[Training Epoch 8] Batch 1973, Loss 0.2770620286464691\n",
      "[Training Epoch 8] Batch 1974, Loss 0.2519766092300415\n",
      "[Training Epoch 8] Batch 1975, Loss 0.24859307706356049\n",
      "[Training Epoch 8] Batch 1976, Loss 0.2614353895187378\n",
      "[Training Epoch 8] Batch 1977, Loss 0.25984808802604675\n",
      "[Training Epoch 8] Batch 1978, Loss 0.26938700675964355\n",
      "[Training Epoch 8] Batch 1979, Loss 0.2528606951236725\n",
      "[Training Epoch 8] Batch 1980, Loss 0.24970531463623047\n",
      "[Training Epoch 8] Batch 1981, Loss 0.2766087055206299\n",
      "[Training Epoch 8] Batch 1982, Loss 0.2809983491897583\n",
      "[Training Epoch 8] Batch 1983, Loss 0.2558346390724182\n",
      "[Training Epoch 8] Batch 1984, Loss 0.24860349297523499\n",
      "[Training Epoch 8] Batch 1985, Loss 0.26552343368530273\n",
      "[Training Epoch 8] Batch 1986, Loss 0.26595938205718994\n",
      "[Training Epoch 8] Batch 1987, Loss 0.24722883105278015\n",
      "[Training Epoch 8] Batch 1988, Loss 0.2689594626426697\n",
      "[Training Epoch 8] Batch 1989, Loss 0.2433377057313919\n",
      "[Training Epoch 8] Batch 1990, Loss 0.23900321125984192\n",
      "[Training Epoch 8] Batch 1991, Loss 0.2418571412563324\n",
      "[Training Epoch 8] Batch 1992, Loss 0.26316219568252563\n",
      "[Training Epoch 8] Batch 1993, Loss 0.2597917318344116\n",
      "[Training Epoch 8] Batch 1994, Loss 0.24116723239421844\n",
      "[Training Epoch 8] Batch 1995, Loss 0.2572886347770691\n",
      "[Training Epoch 8] Batch 1996, Loss 0.26076775789260864\n",
      "[Training Epoch 8] Batch 1997, Loss 0.255323201417923\n",
      "[Training Epoch 8] Batch 1998, Loss 0.2757607698440552\n",
      "[Training Epoch 8] Batch 1999, Loss 0.2705835998058319\n",
      "[Training Epoch 8] Batch 2000, Loss 0.26188910007476807\n",
      "[Training Epoch 8] Batch 2001, Loss 0.2549445629119873\n",
      "[Training Epoch 8] Batch 2002, Loss 0.2580834627151489\n",
      "[Training Epoch 8] Batch 2003, Loss 0.24999797344207764\n",
      "[Training Epoch 8] Batch 2004, Loss 0.24820637702941895\n",
      "[Training Epoch 8] Batch 2005, Loss 0.2823868989944458\n",
      "[Training Epoch 8] Batch 2006, Loss 0.2171824425458908\n",
      "[Training Epoch 8] Batch 2007, Loss 0.2505461573600769\n",
      "[Training Epoch 8] Batch 2008, Loss 0.29009243845939636\n",
      "[Training Epoch 8] Batch 2009, Loss 0.2752889394760132\n",
      "[Training Epoch 8] Batch 2010, Loss 0.26008397340774536\n",
      "[Training Epoch 8] Batch 2011, Loss 0.24468135833740234\n",
      "[Training Epoch 8] Batch 2012, Loss 0.25938498973846436\n",
      "[Training Epoch 8] Batch 2013, Loss 0.2764919400215149\n",
      "[Training Epoch 8] Batch 2014, Loss 0.25242459774017334\n",
      "[Training Epoch 8] Batch 2015, Loss 0.2552170157432556\n",
      "[Training Epoch 8] Batch 2016, Loss 0.2659655809402466\n",
      "[Training Epoch 8] Batch 2017, Loss 0.275829553604126\n",
      "[Training Epoch 8] Batch 2018, Loss 0.25822049379348755\n",
      "[Training Epoch 8] Batch 2019, Loss 0.2424333542585373\n",
      "[Training Epoch 8] Batch 2020, Loss 0.2565616965293884\n",
      "[Training Epoch 8] Batch 2021, Loss 0.2561195492744446\n",
      "[Training Epoch 8] Batch 2022, Loss 0.24469003081321716\n",
      "[Training Epoch 8] Batch 2023, Loss 0.24840137362480164\n",
      "[Training Epoch 8] Batch 2024, Loss 0.2655710279941559\n",
      "[Training Epoch 8] Batch 2025, Loss 0.2712571620941162\n",
      "[Training Epoch 8] Batch 2026, Loss 0.27958691120147705\n",
      "[Training Epoch 8] Batch 2027, Loss 0.26082420349121094\n",
      "[Training Epoch 8] Batch 2028, Loss 0.24373403191566467\n",
      "[Training Epoch 8] Batch 2029, Loss 0.22829341888427734\n",
      "[Training Epoch 8] Batch 2030, Loss 0.239955335855484\n",
      "[Training Epoch 8] Batch 2031, Loss 0.2715831995010376\n",
      "[Training Epoch 8] Batch 2032, Loss 0.26654621958732605\n",
      "[Training Epoch 8] Batch 2033, Loss 0.27834996581077576\n",
      "[Training Epoch 8] Batch 2034, Loss 0.28114718198776245\n",
      "[Training Epoch 8] Batch 2035, Loss 0.2660721242427826\n",
      "[Training Epoch 8] Batch 2036, Loss 0.2634337544441223\n",
      "[Training Epoch 8] Batch 2037, Loss 0.2501366138458252\n",
      "[Training Epoch 8] Batch 2038, Loss 0.2451586127281189\n",
      "[Training Epoch 8] Batch 2039, Loss 0.27706027030944824\n",
      "[Training Epoch 8] Batch 2040, Loss 0.2531125843524933\n",
      "[Training Epoch 8] Batch 2041, Loss 0.28325021266937256\n",
      "[Training Epoch 8] Batch 2042, Loss 0.2603028118610382\n",
      "[Training Epoch 8] Batch 2043, Loss 0.2653353810310364\n",
      "[Training Epoch 8] Batch 2044, Loss 0.2546589970588684\n",
      "[Training Epoch 8] Batch 2045, Loss 0.2785111665725708\n",
      "[Training Epoch 8] Batch 2046, Loss 0.24966830015182495\n",
      "[Training Epoch 8] Batch 2047, Loss 0.24729016423225403\n",
      "[Training Epoch 8] Batch 2048, Loss 0.2508729100227356\n",
      "[Training Epoch 8] Batch 2049, Loss 0.25255292654037476\n",
      "[Training Epoch 8] Batch 2050, Loss 0.24643313884735107\n",
      "[Training Epoch 8] Batch 2051, Loss 0.24710020422935486\n",
      "[Training Epoch 8] Batch 2052, Loss 0.24473749101161957\n",
      "[Training Epoch 8] Batch 2053, Loss 0.26923030614852905\n",
      "[Training Epoch 8] Batch 2054, Loss 0.2644677758216858\n",
      "[Training Epoch 8] Batch 2055, Loss 0.2517540156841278\n",
      "[Training Epoch 8] Batch 2056, Loss 0.28214746713638306\n",
      "[Training Epoch 8] Batch 2057, Loss 0.2558545470237732\n",
      "[Training Epoch 8] Batch 2058, Loss 0.2374022901058197\n",
      "[Training Epoch 8] Batch 2059, Loss 0.28323984146118164\n",
      "[Training Epoch 8] Batch 2060, Loss 0.2484518587589264\n",
      "[Training Epoch 8] Batch 2061, Loss 0.26814109086990356\n",
      "[Training Epoch 8] Batch 2062, Loss 0.2310504913330078\n",
      "[Training Epoch 8] Batch 2063, Loss 0.24587483704090118\n",
      "[Training Epoch 8] Batch 2064, Loss 0.2571030855178833\n",
      "[Training Epoch 8] Batch 2065, Loss 0.2366897314786911\n",
      "[Training Epoch 8] Batch 2066, Loss 0.25746238231658936\n",
      "[Training Epoch 8] Batch 2067, Loss 0.3036418557167053\n",
      "[Training Epoch 8] Batch 2068, Loss 0.2653118371963501\n",
      "[Training Epoch 8] Batch 2069, Loss 0.25944018363952637\n",
      "[Training Epoch 8] Batch 2070, Loss 0.3015895485877991\n",
      "[Training Epoch 8] Batch 2071, Loss 0.24441410601139069\n",
      "[Training Epoch 8] Batch 2072, Loss 0.24268317222595215\n",
      "[Training Epoch 8] Batch 2073, Loss 0.25983867049217224\n",
      "[Training Epoch 8] Batch 2074, Loss 0.2407122552394867\n",
      "[Training Epoch 8] Batch 2075, Loss 0.22724881768226624\n",
      "[Training Epoch 8] Batch 2076, Loss 0.2849715054035187\n",
      "[Training Epoch 8] Batch 2077, Loss 0.24382227659225464\n",
      "[Training Epoch 8] Batch 2078, Loss 0.22269617021083832\n",
      "[Training Epoch 8] Batch 2079, Loss 0.24548637866973877\n",
      "[Training Epoch 8] Batch 2080, Loss 0.24320931732654572\n",
      "[Training Epoch 8] Batch 2081, Loss 0.24487332999706268\n",
      "[Training Epoch 8] Batch 2082, Loss 0.30311310291290283\n",
      "[Training Epoch 8] Batch 2083, Loss 0.2497086226940155\n",
      "[Training Epoch 8] Batch 2084, Loss 0.27216774225234985\n",
      "[Training Epoch 8] Batch 2085, Loss 0.2387942671775818\n",
      "[Training Epoch 8] Batch 2086, Loss 0.25587987899780273\n",
      "[Training Epoch 8] Batch 2087, Loss 0.2641873061656952\n",
      "[Training Epoch 8] Batch 2088, Loss 0.2653200626373291\n",
      "[Training Epoch 8] Batch 2089, Loss 0.2758331894874573\n",
      "[Training Epoch 8] Batch 2090, Loss 0.2352469265460968\n",
      "[Training Epoch 8] Batch 2091, Loss 0.2584468126296997\n",
      "[Training Epoch 8] Batch 2092, Loss 0.2840210795402527\n",
      "[Training Epoch 8] Batch 2093, Loss 0.24619975686073303\n",
      "[Training Epoch 8] Batch 2094, Loss 0.24577999114990234\n",
      "[Training Epoch 8] Batch 2095, Loss 0.25221049785614014\n",
      "[Training Epoch 8] Batch 2096, Loss 0.23774568736553192\n",
      "[Training Epoch 8] Batch 2097, Loss 0.27028608322143555\n",
      "[Training Epoch 8] Batch 2098, Loss 0.25427377223968506\n",
      "[Training Epoch 8] Batch 2099, Loss 0.2631126046180725\n",
      "[Training Epoch 8] Batch 2100, Loss 0.26117146015167236\n",
      "[Training Epoch 8] Batch 2101, Loss 0.24427561461925507\n",
      "[Training Epoch 8] Batch 2102, Loss 0.24889342486858368\n",
      "[Training Epoch 8] Batch 2103, Loss 0.2957243025302887\n",
      "[Training Epoch 8] Batch 2104, Loss 0.2637900114059448\n",
      "[Training Epoch 8] Batch 2105, Loss 0.27337580919265747\n",
      "[Training Epoch 8] Batch 2106, Loss 0.24685660004615784\n",
      "[Training Epoch 8] Batch 2107, Loss 0.2770141363143921\n",
      "[Training Epoch 8] Batch 2108, Loss 0.2662785053253174\n",
      "[Training Epoch 8] Batch 2109, Loss 0.24909251928329468\n",
      "[Training Epoch 8] Batch 2110, Loss 0.23832404613494873\n",
      "[Training Epoch 8] Batch 2111, Loss 0.26639118790626526\n",
      "[Training Epoch 8] Batch 2112, Loss 0.2763408422470093\n",
      "[Training Epoch 8] Batch 2113, Loss 0.25132572650909424\n",
      "[Training Epoch 8] Batch 2114, Loss 0.2677578330039978\n",
      "[Training Epoch 8] Batch 2115, Loss 0.2520453929901123\n",
      "[Training Epoch 8] Batch 2116, Loss 0.24093778431415558\n",
      "[Training Epoch 8] Batch 2117, Loss 0.2452402412891388\n",
      "[Training Epoch 8] Batch 2118, Loss 0.27046120166778564\n",
      "[Training Epoch 8] Batch 2119, Loss 0.25813186168670654\n",
      "[Training Epoch 8] Batch 2120, Loss 0.23976317048072815\n",
      "[Training Epoch 8] Batch 2121, Loss 0.25927984714508057\n",
      "[Training Epoch 8] Batch 2122, Loss 0.29221391677856445\n",
      "[Training Epoch 8] Batch 2123, Loss 0.2507520318031311\n",
      "[Training Epoch 8] Batch 2124, Loss 0.28681331872940063\n",
      "[Training Epoch 8] Batch 2125, Loss 0.27008500695228577\n",
      "[Training Epoch 8] Batch 2126, Loss 0.258855402469635\n",
      "[Training Epoch 8] Batch 2127, Loss 0.24860873818397522\n",
      "[Training Epoch 8] Batch 2128, Loss 0.24334478378295898\n",
      "[Training Epoch 8] Batch 2129, Loss 0.23188017308712006\n",
      "[Training Epoch 8] Batch 2130, Loss 0.24257588386535645\n",
      "[Training Epoch 8] Batch 2131, Loss 0.27369001507759094\n",
      "[Training Epoch 8] Batch 2132, Loss 0.24422456324100494\n",
      "[Training Epoch 8] Batch 2133, Loss 0.26550227403640747\n",
      "[Training Epoch 8] Batch 2134, Loss 0.25247377157211304\n",
      "[Training Epoch 8] Batch 2135, Loss 0.24612849950790405\n",
      "[Training Epoch 8] Batch 2136, Loss 0.2769661545753479\n",
      "[Training Epoch 8] Batch 2137, Loss 0.23919910192489624\n",
      "[Training Epoch 8] Batch 2138, Loss 0.23335471749305725\n",
      "[Training Epoch 8] Batch 2139, Loss 0.26226675510406494\n",
      "[Training Epoch 8] Batch 2140, Loss 0.24425527453422546\n",
      "[Training Epoch 8] Batch 2141, Loss 0.2850162982940674\n",
      "[Training Epoch 8] Batch 2142, Loss 0.23673811554908752\n",
      "[Training Epoch 8] Batch 2143, Loss 0.23122237622737885\n",
      "[Training Epoch 8] Batch 2144, Loss 0.24452397227287292\n",
      "[Training Epoch 8] Batch 2145, Loss 0.24504446983337402\n",
      "[Training Epoch 8] Batch 2146, Loss 0.2803167402744293\n",
      "[Training Epoch 8] Batch 2147, Loss 0.25214987993240356\n",
      "[Training Epoch 8] Batch 2148, Loss 0.2642717957496643\n",
      "[Training Epoch 8] Batch 2149, Loss 0.25836244225502014\n",
      "[Training Epoch 8] Batch 2150, Loss 0.26053762435913086\n",
      "[Training Epoch 8] Batch 2151, Loss 0.2566186189651489\n",
      "[Training Epoch 8] Batch 2152, Loss 0.2489195317029953\n",
      "[Training Epoch 8] Batch 2153, Loss 0.23926883935928345\n",
      "[Training Epoch 8] Batch 2154, Loss 0.25192487239837646\n",
      "[Training Epoch 8] Batch 2155, Loss 0.2691922187805176\n",
      "[Training Epoch 8] Batch 2156, Loss 0.2798277735710144\n",
      "[Training Epoch 8] Batch 2157, Loss 0.2659061551094055\n",
      "[Training Epoch 8] Batch 2158, Loss 0.2650882601737976\n",
      "[Training Epoch 8] Batch 2159, Loss 0.23840437829494476\n",
      "[Training Epoch 8] Batch 2160, Loss 0.23673313856124878\n",
      "[Training Epoch 8] Batch 2161, Loss 0.24278709292411804\n",
      "[Training Epoch 8] Batch 2162, Loss 0.23923981189727783\n",
      "[Training Epoch 8] Batch 2163, Loss 0.25354713201522827\n",
      "[Training Epoch 8] Batch 2164, Loss 0.2539156675338745\n",
      "[Training Epoch 8] Batch 2165, Loss 0.2539312243461609\n",
      "[Training Epoch 8] Batch 2166, Loss 0.272705078125\n",
      "[Training Epoch 8] Batch 2167, Loss 0.26391905546188354\n",
      "[Training Epoch 8] Batch 2168, Loss 0.23778557777404785\n",
      "[Training Epoch 8] Batch 2169, Loss 0.25914323329925537\n",
      "[Training Epoch 8] Batch 2170, Loss 0.28544139862060547\n",
      "[Training Epoch 8] Batch 2171, Loss 0.25192922353744507\n",
      "[Training Epoch 8] Batch 2172, Loss 0.2407228946685791\n",
      "[Training Epoch 8] Batch 2173, Loss 0.23733143508434296\n",
      "[Training Epoch 8] Batch 2174, Loss 0.236297145485878\n",
      "[Training Epoch 8] Batch 2175, Loss 0.27381205558776855\n",
      "[Training Epoch 8] Batch 2176, Loss 0.2704436182975769\n",
      "[Training Epoch 8] Batch 2177, Loss 0.23079851269721985\n",
      "[Training Epoch 8] Batch 2178, Loss 0.24755078554153442\n",
      "[Training Epoch 8] Batch 2179, Loss 0.2497265785932541\n",
      "[Training Epoch 8] Batch 2180, Loss 0.2920795679092407\n",
      "[Training Epoch 8] Batch 2181, Loss 0.2775518298149109\n",
      "[Training Epoch 8] Batch 2182, Loss 0.257996141910553\n",
      "[Training Epoch 8] Batch 2183, Loss 0.2752073407173157\n",
      "[Training Epoch 8] Batch 2184, Loss 0.2604876756668091\n",
      "[Training Epoch 8] Batch 2185, Loss 0.2720332741737366\n",
      "[Training Epoch 8] Batch 2186, Loss 0.24478071928024292\n",
      "[Training Epoch 8] Batch 2187, Loss 0.23998130857944489\n",
      "[Training Epoch 8] Batch 2188, Loss 0.2645530700683594\n",
      "[Training Epoch 8] Batch 2189, Loss 0.25429871678352356\n",
      "[Training Epoch 8] Batch 2190, Loss 0.23284098505973816\n",
      "[Training Epoch 8] Batch 2191, Loss 0.22234755754470825\n",
      "[Training Epoch 8] Batch 2192, Loss 0.2570815682411194\n",
      "[Training Epoch 8] Batch 2193, Loss 0.2587968111038208\n",
      "[Training Epoch 8] Batch 2194, Loss 0.24163062870502472\n",
      "[Training Epoch 8] Batch 2195, Loss 0.26635295152664185\n",
      "[Training Epoch 8] Batch 2196, Loss 0.2661599814891815\n",
      "[Training Epoch 8] Batch 2197, Loss 0.23544371128082275\n",
      "[Training Epoch 8] Batch 2198, Loss 0.2712865471839905\n",
      "[Training Epoch 8] Batch 2199, Loss 0.2329070121049881\n",
      "[Training Epoch 8] Batch 2200, Loss 0.22727538645267487\n",
      "[Training Epoch 8] Batch 2201, Loss 0.26547035574913025\n",
      "[Training Epoch 8] Batch 2202, Loss 0.2862869203090668\n",
      "[Training Epoch 8] Batch 2203, Loss 0.2703620195388794\n",
      "[Training Epoch 8] Batch 2204, Loss 0.23595306277275085\n",
      "[Training Epoch 8] Batch 2205, Loss 0.28073179721832275\n",
      "[Training Epoch 8] Batch 2206, Loss 0.25023066997528076\n",
      "[Training Epoch 8] Batch 2207, Loss 0.2459644377231598\n",
      "[Training Epoch 8] Batch 2208, Loss 0.24982814490795135\n",
      "[Training Epoch 8] Batch 2209, Loss 0.24226433038711548\n",
      "[Training Epoch 8] Batch 2210, Loss 0.2554091513156891\n",
      "[Training Epoch 8] Batch 2211, Loss 0.25803321599960327\n",
      "[Training Epoch 8] Batch 2212, Loss 0.2596728801727295\n",
      "[Training Epoch 8] Batch 2213, Loss 0.2750088572502136\n",
      "[Training Epoch 8] Batch 2214, Loss 0.2639046609401703\n",
      "[Training Epoch 8] Batch 2215, Loss 0.21588289737701416\n",
      "[Training Epoch 8] Batch 2216, Loss 0.2533756494522095\n",
      "[Training Epoch 8] Batch 2217, Loss 0.24120692908763885\n",
      "[Training Epoch 8] Batch 2218, Loss 0.2643336057662964\n",
      "[Training Epoch 8] Batch 2219, Loss 0.2689879834651947\n",
      "[Training Epoch 8] Batch 2220, Loss 0.2685509920120239\n",
      "[Training Epoch 8] Batch 2221, Loss 0.24430952966213226\n",
      "[Training Epoch 8] Batch 2222, Loss 0.25544652342796326\n",
      "[Training Epoch 8] Batch 2223, Loss 0.2792131006717682\n",
      "[Training Epoch 8] Batch 2224, Loss 0.23858650028705597\n",
      "[Training Epoch 8] Batch 2225, Loss 0.25939807295799255\n",
      "[Training Epoch 8] Batch 2226, Loss 0.24932309985160828\n",
      "[Training Epoch 8] Batch 2227, Loss 0.2496950626373291\n",
      "[Training Epoch 8] Batch 2228, Loss 0.24888445436954498\n",
      "[Training Epoch 8] Batch 2229, Loss 0.26471003890037537\n",
      "[Training Epoch 8] Batch 2230, Loss 0.2778555154800415\n",
      "[Training Epoch 8] Batch 2231, Loss 0.22085513174533844\n",
      "[Training Epoch 8] Batch 2232, Loss 0.2543460428714752\n",
      "[Training Epoch 8] Batch 2233, Loss 0.25137484073638916\n",
      "[Training Epoch 8] Batch 2234, Loss 0.24350208044052124\n",
      "[Training Epoch 8] Batch 2235, Loss 0.25973522663116455\n",
      "[Training Epoch 8] Batch 2236, Loss 0.25670182704925537\n",
      "[Training Epoch 8] Batch 2237, Loss 0.26286256313323975\n",
      "[Training Epoch 8] Batch 2238, Loss 0.23974394798278809\n",
      "[Training Epoch 8] Batch 2239, Loss 0.25806882977485657\n",
      "[Training Epoch 8] Batch 2240, Loss 0.2602286636829376\n",
      "[Training Epoch 8] Batch 2241, Loss 0.20642411708831787\n",
      "[Training Epoch 8] Batch 2242, Loss 0.24786363542079926\n",
      "[Training Epoch 8] Batch 2243, Loss 0.27430957555770874\n",
      "[Training Epoch 8] Batch 2244, Loss 0.2792069911956787\n",
      "[Training Epoch 8] Batch 2245, Loss 0.25767645239830017\n",
      "[Training Epoch 8] Batch 2246, Loss 0.2437971830368042\n",
      "[Training Epoch 8] Batch 2247, Loss 0.25799235701560974\n",
      "[Training Epoch 8] Batch 2248, Loss 0.2858204245567322\n",
      "[Training Epoch 8] Batch 2249, Loss 0.26037871837615967\n",
      "[Training Epoch 8] Batch 2250, Loss 0.2721599042415619\n",
      "[Training Epoch 8] Batch 2251, Loss 0.2493969351053238\n",
      "[Training Epoch 8] Batch 2252, Loss 0.26217982172966003\n",
      "[Training Epoch 8] Batch 2253, Loss 0.2672978639602661\n",
      "[Training Epoch 8] Batch 2254, Loss 0.24002617597579956\n",
      "[Training Epoch 8] Batch 2255, Loss 0.273890882730484\n",
      "[Training Epoch 8] Batch 2256, Loss 0.24566373229026794\n",
      "[Training Epoch 8] Batch 2257, Loss 0.2568037807941437\n",
      "[Training Epoch 8] Batch 2258, Loss 0.2512092888355255\n",
      "[Training Epoch 8] Batch 2259, Loss 0.2503877282142639\n",
      "[Training Epoch 8] Batch 2260, Loss 0.2595778703689575\n",
      "[Training Epoch 8] Batch 2261, Loss 0.24255889654159546\n",
      "[Training Epoch 8] Batch 2262, Loss 0.3001605272293091\n",
      "[Training Epoch 8] Batch 2263, Loss 0.26301270723342896\n",
      "[Training Epoch 8] Batch 2264, Loss 0.2488386631011963\n",
      "[Training Epoch 8] Batch 2265, Loss 0.23826125264167786\n",
      "[Training Epoch 8] Batch 2266, Loss 0.2703418731689453\n",
      "[Training Epoch 8] Batch 2267, Loss 0.27012568712234497\n",
      "[Training Epoch 8] Batch 2268, Loss 0.2568839192390442\n",
      "[Training Epoch 8] Batch 2269, Loss 0.219887837767601\n",
      "[Training Epoch 8] Batch 2270, Loss 0.2395515739917755\n",
      "[Training Epoch 8] Batch 2271, Loss 0.25363776087760925\n",
      "[Training Epoch 8] Batch 2272, Loss 0.27330461144447327\n",
      "[Training Epoch 8] Batch 2273, Loss 0.24570077657699585\n",
      "[Training Epoch 8] Batch 2274, Loss 0.25798216462135315\n",
      "[Training Epoch 8] Batch 2275, Loss 0.2752419710159302\n",
      "[Training Epoch 8] Batch 2276, Loss 0.24431362748146057\n",
      "[Training Epoch 8] Batch 2277, Loss 0.28462332487106323\n",
      "[Training Epoch 8] Batch 2278, Loss 0.2699257731437683\n",
      "[Training Epoch 8] Batch 2279, Loss 0.2676684260368347\n",
      "[Training Epoch 8] Batch 2280, Loss 0.2611890435218811\n",
      "[Training Epoch 8] Batch 2281, Loss 0.26667872071266174\n",
      "[Training Epoch 8] Batch 2282, Loss 0.2590155005455017\n",
      "[Training Epoch 8] Batch 2283, Loss 0.270082950592041\n",
      "[Training Epoch 8] Batch 2284, Loss 0.2629340887069702\n",
      "[Training Epoch 8] Batch 2285, Loss 0.23551177978515625\n",
      "[Training Epoch 8] Batch 2286, Loss 0.2982642352581024\n",
      "[Training Epoch 8] Batch 2287, Loss 0.26186084747314453\n",
      "[Training Epoch 8] Batch 2288, Loss 0.25650203227996826\n",
      "[Training Epoch 8] Batch 2289, Loss 0.28072589635849\n",
      "[Training Epoch 8] Batch 2290, Loss 0.26635366678237915\n",
      "[Training Epoch 8] Batch 2291, Loss 0.2705343961715698\n",
      "[Training Epoch 8] Batch 2292, Loss 0.2837774157524109\n",
      "[Training Epoch 8] Batch 2293, Loss 0.25551638007164\n",
      "[Training Epoch 8] Batch 2294, Loss 0.23239058256149292\n",
      "[Training Epoch 8] Batch 2295, Loss 0.29023468494415283\n",
      "[Training Epoch 8] Batch 2296, Loss 0.21473121643066406\n",
      "[Training Epoch 8] Batch 2297, Loss 0.2698642611503601\n",
      "[Training Epoch 8] Batch 2298, Loss 0.24719011783599854\n",
      "[Training Epoch 8] Batch 2299, Loss 0.2973249554634094\n",
      "[Training Epoch 8] Batch 2300, Loss 0.2490253895521164\n",
      "[Training Epoch 8] Batch 2301, Loss 0.2492215633392334\n",
      "[Training Epoch 8] Batch 2302, Loss 0.2462988942861557\n",
      "[Training Epoch 8] Batch 2303, Loss 0.2240205705165863\n",
      "[Training Epoch 8] Batch 2304, Loss 0.24048618972301483\n",
      "[Training Epoch 8] Batch 2305, Loss 0.2320725917816162\n",
      "[Training Epoch 8] Batch 2306, Loss 0.22846373915672302\n",
      "[Training Epoch 8] Batch 2307, Loss 0.2629176676273346\n",
      "[Training Epoch 8] Batch 2308, Loss 0.2863905727863312\n",
      "[Training Epoch 8] Batch 2309, Loss 0.2857421636581421\n",
      "[Training Epoch 8] Batch 2310, Loss 0.2276977151632309\n",
      "[Training Epoch 8] Batch 2311, Loss 0.25408366322517395\n",
      "[Training Epoch 8] Batch 2312, Loss 0.23922762274742126\n",
      "[Training Epoch 8] Batch 2313, Loss 0.24502378702163696\n",
      "[Training Epoch 8] Batch 2314, Loss 0.26555556058883667\n",
      "[Training Epoch 8] Batch 2315, Loss 0.2781444191932678\n",
      "[Training Epoch 8] Batch 2316, Loss 0.2774241864681244\n",
      "[Training Epoch 8] Batch 2317, Loss 0.23006686568260193\n",
      "[Training Epoch 8] Batch 2318, Loss 0.2506336569786072\n",
      "[Training Epoch 8] Batch 2319, Loss 0.2560659646987915\n",
      "[Training Epoch 8] Batch 2320, Loss 0.22482731938362122\n",
      "[Training Epoch 8] Batch 2321, Loss 0.25027748942375183\n",
      "[Training Epoch 8] Batch 2322, Loss 0.21402910351753235\n",
      "[Training Epoch 8] Batch 2323, Loss 0.232496440410614\n",
      "[Training Epoch 8] Batch 2324, Loss 0.22883449494838715\n",
      "[Training Epoch 8] Batch 2325, Loss 0.2374606430530548\n",
      "[Training Epoch 8] Batch 2326, Loss 0.2578202486038208\n",
      "[Training Epoch 8] Batch 2327, Loss 0.25055307149887085\n",
      "[Training Epoch 8] Batch 2328, Loss 0.2555466294288635\n",
      "[Training Epoch 8] Batch 2329, Loss 0.22236114740371704\n",
      "[Training Epoch 8] Batch 2330, Loss 0.2525452971458435\n",
      "[Training Epoch 8] Batch 2331, Loss 0.24676275253295898\n",
      "[Training Epoch 8] Batch 2332, Loss 0.27855950593948364\n",
      "[Training Epoch 8] Batch 2333, Loss 0.26772540807724\n",
      "[Training Epoch 8] Batch 2334, Loss 0.2536730170249939\n",
      "[Training Epoch 8] Batch 2335, Loss 0.2723004221916199\n",
      "[Training Epoch 8] Batch 2336, Loss 0.24403297901153564\n",
      "[Training Epoch 8] Batch 2337, Loss 0.25268420577049255\n",
      "[Training Epoch 8] Batch 2338, Loss 0.2290259450674057\n",
      "[Training Epoch 8] Batch 2339, Loss 0.2506832182407379\n",
      "[Training Epoch 8] Batch 2340, Loss 0.27066296339035034\n",
      "[Training Epoch 8] Batch 2341, Loss 0.27920424938201904\n",
      "[Training Epoch 8] Batch 2342, Loss 0.25690752267837524\n",
      "[Training Epoch 8] Batch 2343, Loss 0.24831674993038177\n",
      "[Training Epoch 8] Batch 2344, Loss 0.24139490723609924\n",
      "[Training Epoch 8] Batch 2345, Loss 0.2616601586341858\n",
      "[Training Epoch 8] Batch 2346, Loss 0.2392948418855667\n",
      "[Training Epoch 8] Batch 2347, Loss 0.2365923523902893\n",
      "[Training Epoch 8] Batch 2348, Loss 0.25447335839271545\n",
      "[Training Epoch 8] Batch 2349, Loss 0.2480878233909607\n",
      "[Training Epoch 8] Batch 2350, Loss 0.2468043565750122\n",
      "[Training Epoch 8] Batch 2351, Loss 0.2689587473869324\n",
      "[Training Epoch 8] Batch 2352, Loss 0.2577316462993622\n",
      "[Training Epoch 8] Batch 2353, Loss 0.25155532360076904\n",
      "[Training Epoch 8] Batch 2354, Loss 0.2536419630050659\n",
      "[Training Epoch 8] Batch 2355, Loss 0.23362261056900024\n",
      "[Training Epoch 8] Batch 2356, Loss 0.2585579752922058\n",
      "[Training Epoch 8] Batch 2357, Loss 0.2718800902366638\n",
      "[Training Epoch 8] Batch 2358, Loss 0.27531683444976807\n",
      "[Training Epoch 8] Batch 2359, Loss 0.25215867161750793\n",
      "[Training Epoch 8] Batch 2360, Loss 0.3195789158344269\n",
      "[Training Epoch 8] Batch 2361, Loss 0.2661089301109314\n",
      "[Training Epoch 8] Batch 2362, Loss 0.29311394691467285\n",
      "[Training Epoch 8] Batch 2363, Loss 0.2502345144748688\n",
      "[Training Epoch 8] Batch 2364, Loss 0.24962963163852692\n",
      "[Training Epoch 8] Batch 2365, Loss 0.2526841163635254\n",
      "[Training Epoch 8] Batch 2366, Loss 0.24162346124649048\n",
      "[Training Epoch 8] Batch 2367, Loss 0.26386404037475586\n",
      "[Training Epoch 8] Batch 2368, Loss 0.2603791654109955\n",
      "[Training Epoch 8] Batch 2369, Loss 0.2816331684589386\n",
      "[Training Epoch 8] Batch 2370, Loss 0.23472759127616882\n",
      "[Training Epoch 8] Batch 2371, Loss 0.24436220526695251\n",
      "[Training Epoch 8] Batch 2372, Loss 0.2803826928138733\n",
      "[Training Epoch 8] Batch 2373, Loss 0.22806039452552795\n",
      "[Training Epoch 8] Batch 2374, Loss 0.2700350880622864\n",
      "[Training Epoch 8] Batch 2375, Loss 0.26779529452323914\n",
      "[Training Epoch 8] Batch 2376, Loss 0.2503419518470764\n",
      "[Training Epoch 8] Batch 2377, Loss 0.2627657651901245\n",
      "[Training Epoch 8] Batch 2378, Loss 0.2485497146844864\n",
      "[Training Epoch 8] Batch 2379, Loss 0.2738456428050995\n",
      "[Training Epoch 8] Batch 2380, Loss 0.257039338350296\n",
      "[Training Epoch 8] Batch 2381, Loss 0.25210052728652954\n",
      "[Training Epoch 8] Batch 2382, Loss 0.23798993229866028\n",
      "[Training Epoch 8] Batch 2383, Loss 0.2792540490627289\n",
      "[Training Epoch 8] Batch 2384, Loss 0.2776844799518585\n",
      "[Training Epoch 8] Batch 2385, Loss 0.2735886871814728\n",
      "[Training Epoch 8] Batch 2386, Loss 0.2399458885192871\n",
      "[Training Epoch 8] Batch 2387, Loss 0.2880273461341858\n",
      "[Training Epoch 8] Batch 2388, Loss 0.2472280114889145\n",
      "[Training Epoch 8] Batch 2389, Loss 0.2540785074234009\n",
      "[Training Epoch 8] Batch 2390, Loss 0.2548581063747406\n",
      "[Training Epoch 8] Batch 2391, Loss 0.2628288269042969\n",
      "[Training Epoch 8] Batch 2392, Loss 0.24130091071128845\n",
      "[Training Epoch 8] Batch 2393, Loss 0.25681090354919434\n",
      "[Training Epoch 8] Batch 2394, Loss 0.263979434967041\n",
      "[Training Epoch 8] Batch 2395, Loss 0.2544470429420471\n",
      "[Training Epoch 8] Batch 2396, Loss 0.2811815142631531\n",
      "[Training Epoch 8] Batch 2397, Loss 0.22964783012866974\n",
      "[Training Epoch 8] Batch 2398, Loss 0.2807379364967346\n",
      "[Training Epoch 8] Batch 2399, Loss 0.26272815465927124\n",
      "[Training Epoch 8] Batch 2400, Loss 0.24000439047813416\n",
      "[Training Epoch 8] Batch 2401, Loss 0.24169665575027466\n",
      "[Training Epoch 8] Batch 2402, Loss 0.2248232066631317\n",
      "[Training Epoch 8] Batch 2403, Loss 0.26807379722595215\n",
      "[Training Epoch 8] Batch 2404, Loss 0.26195812225341797\n",
      "[Training Epoch 8] Batch 2405, Loss 0.2632789611816406\n",
      "[Training Epoch 8] Batch 2406, Loss 0.23882216215133667\n",
      "[Training Epoch 8] Batch 2407, Loss 0.2632877826690674\n",
      "[Training Epoch 8] Batch 2408, Loss 0.24776563048362732\n",
      "[Training Epoch 8] Batch 2409, Loss 0.2616944909095764\n",
      "[Training Epoch 8] Batch 2410, Loss 0.2636919617652893\n",
      "[Training Epoch 8] Batch 2411, Loss 0.270719975233078\n",
      "[Training Epoch 8] Batch 2412, Loss 0.2570153474807739\n",
      "[Training Epoch 8] Batch 2413, Loss 0.23447230458259583\n",
      "[Training Epoch 8] Batch 2414, Loss 0.2574675977230072\n",
      "[Training Epoch 8] Batch 2415, Loss 0.26701924204826355\n",
      "[Training Epoch 8] Batch 2416, Loss 0.25200122594833374\n",
      "[Training Epoch 8] Batch 2417, Loss 0.23350289463996887\n",
      "[Training Epoch 8] Batch 2418, Loss 0.2708096504211426\n",
      "[Training Epoch 8] Batch 2419, Loss 0.2557595372200012\n",
      "[Training Epoch 8] Batch 2420, Loss 0.29686442017555237\n",
      "[Training Epoch 8] Batch 2421, Loss 0.23744630813598633\n",
      "[Training Epoch 8] Batch 2422, Loss 0.2625453770160675\n",
      "[Training Epoch 8] Batch 2423, Loss 0.2651175856590271\n",
      "[Training Epoch 8] Batch 2424, Loss 0.24014705419540405\n",
      "[Training Epoch 8] Batch 2425, Loss 0.25841665267944336\n",
      "[Training Epoch 8] Batch 2426, Loss 0.25747254490852356\n",
      "[Training Epoch 8] Batch 2427, Loss 0.2705664038658142\n",
      "[Training Epoch 8] Batch 2428, Loss 0.2630550265312195\n",
      "[Training Epoch 8] Batch 2429, Loss 0.2958749234676361\n",
      "[Training Epoch 8] Batch 2430, Loss 0.2570571303367615\n",
      "[Training Epoch 8] Batch 2431, Loss 0.22993946075439453\n",
      "[Training Epoch 8] Batch 2432, Loss 0.255482017993927\n",
      "[Training Epoch 8] Batch 2433, Loss 0.2530519664287567\n",
      "[Training Epoch 8] Batch 2434, Loss 0.2566353678703308\n",
      "[Training Epoch 8] Batch 2435, Loss 0.2582285404205322\n",
      "[Training Epoch 8] Batch 2436, Loss 0.24075651168823242\n",
      "[Training Epoch 8] Batch 2437, Loss 0.24970756471157074\n",
      "[Training Epoch 8] Batch 2438, Loss 0.267014741897583\n",
      "[Training Epoch 8] Batch 2439, Loss 0.2885500490665436\n",
      "[Training Epoch 8] Batch 2440, Loss 0.26503896713256836\n",
      "[Training Epoch 8] Batch 2441, Loss 0.25482848286628723\n",
      "[Training Epoch 8] Batch 2442, Loss 0.24720746278762817\n",
      "[Training Epoch 8] Batch 2443, Loss 0.24990737438201904\n",
      "[Training Epoch 8] Batch 2444, Loss 0.22942373156547546\n",
      "[Training Epoch 8] Batch 2445, Loss 0.24741370975971222\n",
      "[Training Epoch 8] Batch 2446, Loss 0.25248318910598755\n",
      "[Training Epoch 8] Batch 2447, Loss 0.2705574035644531\n",
      "[Training Epoch 8] Batch 2448, Loss 0.2550469934940338\n",
      "[Training Epoch 8] Batch 2449, Loss 0.2701329290866852\n",
      "[Training Epoch 8] Batch 2450, Loss 0.243514746427536\n",
      "[Training Epoch 8] Batch 2451, Loss 0.24555332958698273\n",
      "[Training Epoch 8] Batch 2452, Loss 0.24974986910820007\n",
      "[Training Epoch 8] Batch 2453, Loss 0.2524147033691406\n",
      "[Training Epoch 8] Batch 2454, Loss 0.25363826751708984\n",
      "[Training Epoch 8] Batch 2455, Loss 0.24871397018432617\n",
      "[Training Epoch 8] Batch 2456, Loss 0.2692967653274536\n",
      "[Training Epoch 8] Batch 2457, Loss 0.2654208540916443\n",
      "[Training Epoch 8] Batch 2458, Loss 0.23253697156906128\n",
      "[Training Epoch 8] Batch 2459, Loss 0.23473674058914185\n",
      "[Training Epoch 8] Batch 2460, Loss 0.2552297115325928\n",
      "[Training Epoch 8] Batch 2461, Loss 0.2522316873073578\n",
      "[Training Epoch 8] Batch 2462, Loss 0.2726239562034607\n",
      "[Training Epoch 8] Batch 2463, Loss 0.244907945394516\n",
      "[Training Epoch 8] Batch 2464, Loss 0.2343941479921341\n",
      "[Training Epoch 8] Batch 2465, Loss 0.25421619415283203\n",
      "[Training Epoch 8] Batch 2466, Loss 0.2684219479560852\n",
      "[Training Epoch 8] Batch 2467, Loss 0.24708354473114014\n",
      "[Training Epoch 8] Batch 2468, Loss 0.23259887099266052\n",
      "[Training Epoch 8] Batch 2469, Loss 0.24351269006729126\n",
      "[Training Epoch 8] Batch 2470, Loss 0.2557399868965149\n",
      "[Training Epoch 8] Batch 2471, Loss 0.25792741775512695\n",
      "[Training Epoch 8] Batch 2472, Loss 0.2454221248626709\n",
      "[Training Epoch 8] Batch 2473, Loss 0.27064400911331177\n",
      "[Training Epoch 8] Batch 2474, Loss 0.2584812641143799\n",
      "[Training Epoch 8] Batch 2475, Loss 0.24331927299499512\n",
      "[Training Epoch 8] Batch 2476, Loss 0.25047165155410767\n",
      "[Training Epoch 8] Batch 2477, Loss 0.2911745309829712\n",
      "[Training Epoch 8] Batch 2478, Loss 0.2566671371459961\n",
      "[Training Epoch 8] Batch 2479, Loss 0.252189964056015\n",
      "[Training Epoch 8] Batch 2480, Loss 0.26122552156448364\n",
      "[Training Epoch 8] Batch 2481, Loss 0.27370601892471313\n",
      "[Training Epoch 8] Batch 2482, Loss 0.25685927271842957\n",
      "[Training Epoch 8] Batch 2483, Loss 0.2642868459224701\n",
      "[Training Epoch 8] Batch 2484, Loss 0.2675991654396057\n",
      "[Training Epoch 8] Batch 2485, Loss 0.2431551069021225\n",
      "[Training Epoch 8] Batch 2486, Loss 0.20603877305984497\n",
      "[Training Epoch 8] Batch 2487, Loss 0.24135735630989075\n",
      "[Training Epoch 8] Batch 2488, Loss 0.24323832988739014\n",
      "[Training Epoch 8] Batch 2489, Loss 0.22254256904125214\n",
      "[Training Epoch 8] Batch 2490, Loss 0.2124592363834381\n",
      "[Training Epoch 8] Batch 2491, Loss 0.24579545855522156\n",
      "[Training Epoch 8] Batch 2492, Loss 0.2773149311542511\n",
      "[Training Epoch 8] Batch 2493, Loss 0.25570642948150635\n",
      "[Training Epoch 8] Batch 2494, Loss 0.24519945681095123\n",
      "[Training Epoch 8] Batch 2495, Loss 0.21379965543746948\n",
      "[Training Epoch 8] Batch 2496, Loss 0.2882031798362732\n",
      "[Training Epoch 8] Batch 2497, Loss 0.25044602155685425\n",
      "[Training Epoch 8] Batch 2498, Loss 0.25219857692718506\n",
      "[Training Epoch 8] Batch 2499, Loss 0.2802044451236725\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2538107633590698\n",
      "[Training Epoch 8] Batch 2501, Loss 0.23302766680717468\n",
      "[Training Epoch 8] Batch 2502, Loss 0.2619892358779907\n",
      "[Training Epoch 8] Batch 2503, Loss 0.2803879976272583\n",
      "[Training Epoch 8] Batch 2504, Loss 0.2525709271430969\n",
      "[Training Epoch 8] Batch 2505, Loss 0.23572763800621033\n",
      "[Training Epoch 8] Batch 2506, Loss 0.27962690591812134\n",
      "[Training Epoch 8] Batch 2507, Loss 0.24836261570453644\n",
      "[Training Epoch 8] Batch 2508, Loss 0.2896481156349182\n",
      "[Training Epoch 8] Batch 2509, Loss 0.256020188331604\n",
      "[Training Epoch 8] Batch 2510, Loss 0.2517990469932556\n",
      "[Training Epoch 8] Batch 2511, Loss 0.28563445806503296\n",
      "[Training Epoch 8] Batch 2512, Loss 0.2587045431137085\n",
      "[Training Epoch 8] Batch 2513, Loss 0.26982754468917847\n",
      "[Training Epoch 8] Batch 2514, Loss 0.2776716649532318\n",
      "[Training Epoch 8] Batch 2515, Loss 0.25449687242507935\n",
      "[Training Epoch 8] Batch 2516, Loss 0.26411694288253784\n",
      "[Training Epoch 8] Batch 2517, Loss 0.2992951273918152\n",
      "[Training Epoch 8] Batch 2518, Loss 0.2352880984544754\n",
      "[Training Epoch 8] Batch 2519, Loss 0.227259561419487\n",
      "[Training Epoch 8] Batch 2520, Loss 0.2693506181240082\n",
      "[Training Epoch 8] Batch 2521, Loss 0.26250940561294556\n",
      "[Training Epoch 8] Batch 2522, Loss 0.26350027322769165\n",
      "[Training Epoch 8] Batch 2523, Loss 0.2768998146057129\n",
      "[Training Epoch 8] Batch 2524, Loss 0.28560444712638855\n",
      "[Training Epoch 8] Batch 2525, Loss 0.2617030143737793\n",
      "[Training Epoch 8] Batch 2526, Loss 0.27458590269088745\n",
      "[Training Epoch 8] Batch 2527, Loss 0.2462514489889145\n",
      "[Training Epoch 8] Batch 2528, Loss 0.25007209181785583\n",
      "[Training Epoch 8] Batch 2529, Loss 0.24189597368240356\n",
      "[Training Epoch 8] Batch 2530, Loss 0.24747517704963684\n",
      "[Training Epoch 8] Batch 2531, Loss 0.2539884150028229\n",
      "[Training Epoch 8] Batch 2532, Loss 0.24913303554058075\n",
      "[Training Epoch 8] Batch 2533, Loss 0.24390888214111328\n",
      "[Training Epoch 8] Batch 2534, Loss 0.2659568786621094\n",
      "[Training Epoch 8] Batch 2535, Loss 0.24827587604522705\n",
      "[Training Epoch 8] Batch 2536, Loss 0.24920323491096497\n",
      "[Training Epoch 8] Batch 2537, Loss 0.24563653767108917\n",
      "[Training Epoch 8] Batch 2538, Loss 0.23868584632873535\n",
      "[Training Epoch 8] Batch 2539, Loss 0.24774020910263062\n",
      "[Training Epoch 8] Batch 2540, Loss 0.257314532995224\n",
      "[Training Epoch 8] Batch 2541, Loss 0.23118117451667786\n",
      "[Training Epoch 8] Batch 2542, Loss 0.26955536007881165\n",
      "[Training Epoch 8] Batch 2543, Loss 0.2768385708332062\n",
      "[Training Epoch 8] Batch 2544, Loss 0.2285444140434265\n",
      "[Training Epoch 8] Batch 2545, Loss 0.25393205881118774\n",
      "[Training Epoch 8] Batch 2546, Loss 0.273997962474823\n",
      "[Training Epoch 8] Batch 2547, Loss 0.29630035161972046\n",
      "[Training Epoch 8] Batch 2548, Loss 0.24787327647209167\n",
      "[Training Epoch 8] Batch 2549, Loss 0.265693724155426\n",
      "[Training Epoch 8] Batch 2550, Loss 0.2740727961063385\n",
      "[Training Epoch 8] Batch 2551, Loss 0.28306302428245544\n",
      "[Training Epoch 8] Batch 2552, Loss 0.2554646134376526\n",
      "[Training Epoch 8] Batch 2553, Loss 0.2376995086669922\n",
      "[Training Epoch 8] Batch 2554, Loss 0.3121345043182373\n",
      "[Training Epoch 8] Batch 2555, Loss 0.26972782611846924\n",
      "[Training Epoch 8] Batch 2556, Loss 0.2394781857728958\n",
      "[Training Epoch 8] Batch 2557, Loss 0.24742546677589417\n",
      "[Training Epoch 8] Batch 2558, Loss 0.23957939445972443\n",
      "[Training Epoch 8] Batch 2559, Loss 0.267971396446228\n",
      "[Training Epoch 8] Batch 2560, Loss 0.25550562143325806\n",
      "[Training Epoch 8] Batch 2561, Loss 0.2742312550544739\n",
      "[Training Epoch 8] Batch 2562, Loss 0.24296635389328003\n",
      "[Training Epoch 8] Batch 2563, Loss 0.2511497735977173\n",
      "[Training Epoch 8] Batch 2564, Loss 0.2493647336959839\n",
      "[Training Epoch 8] Batch 2565, Loss 0.26108241081237793\n",
      "[Training Epoch 8] Batch 2566, Loss 0.23315885663032532\n",
      "[Training Epoch 8] Batch 2567, Loss 0.2526010274887085\n",
      "[Training Epoch 8] Batch 2568, Loss 0.30886805057525635\n",
      "[Training Epoch 8] Batch 2569, Loss 0.2429238110780716\n",
      "[Training Epoch 8] Batch 2570, Loss 0.24791011214256287\n",
      "[Training Epoch 8] Batch 2571, Loss 0.2642642855644226\n",
      "[Training Epoch 8] Batch 2572, Loss 0.25104808807373047\n",
      "[Training Epoch 8] Batch 2573, Loss 0.2676440179347992\n",
      "[Training Epoch 8] Batch 2574, Loss 0.24739494919776917\n",
      "[Training Epoch 8] Batch 2575, Loss 0.2693774700164795\n",
      "[Training Epoch 8] Batch 2576, Loss 0.2564033269882202\n",
      "[Training Epoch 8] Batch 2577, Loss 0.27125972509384155\n",
      "[Training Epoch 8] Batch 2578, Loss 0.24145318567752838\n",
      "[Training Epoch 8] Batch 2579, Loss 0.279541552066803\n",
      "[Training Epoch 8] Batch 2580, Loss 0.2724761664867401\n",
      "[Training Epoch 8] Batch 2581, Loss 0.25291675329208374\n",
      "[Training Epoch 8] Batch 2582, Loss 0.25438982248306274\n",
      "[Training Epoch 8] Batch 2583, Loss 0.25003886222839355\n",
      "[Training Epoch 8] Batch 2584, Loss 0.2963876724243164\n",
      "[Training Epoch 8] Batch 2585, Loss 0.2404455542564392\n",
      "[Training Epoch 8] Batch 2586, Loss 0.2603680193424225\n",
      "[Training Epoch 8] Batch 2587, Loss 0.2637009024620056\n",
      "[Training Epoch 8] Batch 2588, Loss 0.2684635519981384\n",
      "[Training Epoch 8] Batch 2589, Loss 0.2758451998233795\n",
      "[Training Epoch 8] Batch 2590, Loss 0.2800447642803192\n",
      "[Training Epoch 8] Batch 2591, Loss 0.25802505016326904\n",
      "[Training Epoch 8] Batch 2592, Loss 0.28158038854599\n",
      "[Training Epoch 8] Batch 2593, Loss 0.2507442235946655\n",
      "[Training Epoch 8] Batch 2594, Loss 0.2389322966337204\n",
      "[Training Epoch 8] Batch 2595, Loss 0.23727819323539734\n",
      "[Training Epoch 8] Batch 2596, Loss 0.2658911943435669\n",
      "[Training Epoch 8] Batch 2597, Loss 0.2690942883491516\n",
      "[Training Epoch 8] Batch 2598, Loss 0.2379385083913803\n",
      "[Training Epoch 8] Batch 2599, Loss 0.2457135170698166\n",
      "[Training Epoch 8] Batch 2600, Loss 0.23417189717292786\n",
      "[Training Epoch 8] Batch 2601, Loss 0.2701227068901062\n",
      "[Training Epoch 8] Batch 2602, Loss 0.2707475423812866\n",
      "[Training Epoch 8] Batch 2603, Loss 0.29023420810699463\n",
      "[Training Epoch 8] Batch 2604, Loss 0.28911417722702026\n",
      "[Training Epoch 8] Batch 2605, Loss 0.2349248081445694\n",
      "[Training Epoch 8] Batch 2606, Loss 0.27182644605636597\n",
      "[Training Epoch 8] Batch 2607, Loss 0.24414116144180298\n",
      "[Training Epoch 8] Batch 2608, Loss 0.2745964527130127\n",
      "[Training Epoch 8] Batch 2609, Loss 0.23612405359745026\n",
      "[Training Epoch 8] Batch 2610, Loss 0.25163018703460693\n",
      "[Training Epoch 8] Batch 2611, Loss 0.25282424688339233\n",
      "[Training Epoch 8] Batch 2612, Loss 0.2608216404914856\n",
      "[Training Epoch 8] Batch 2613, Loss 0.24792884290218353\n",
      "[Training Epoch 8] Batch 2614, Loss 0.2629319429397583\n",
      "[Training Epoch 8] Batch 2615, Loss 0.2584604024887085\n",
      "[Training Epoch 8] Batch 2616, Loss 0.2594605088233948\n",
      "[Training Epoch 8] Batch 2617, Loss 0.2630404829978943\n",
      "[Training Epoch 8] Batch 2618, Loss 0.26208436489105225\n",
      "[Training Epoch 8] Batch 2619, Loss 0.24392707645893097\n",
      "[Training Epoch 8] Batch 2620, Loss 0.25862443447113037\n",
      "[Training Epoch 8] Batch 2621, Loss 0.3016613721847534\n",
      "[Training Epoch 8] Batch 2622, Loss 0.24656245112419128\n",
      "[Training Epoch 8] Batch 2623, Loss 0.24674025177955627\n",
      "[Training Epoch 8] Batch 2624, Loss 0.26984986662864685\n",
      "[Training Epoch 8] Batch 2625, Loss 0.2586909830570221\n",
      "[Training Epoch 8] Batch 2626, Loss 0.23915284872055054\n",
      "[Training Epoch 8] Batch 2627, Loss 0.28254738450050354\n",
      "[Training Epoch 8] Batch 2628, Loss 0.22605647146701813\n",
      "[Training Epoch 8] Batch 2629, Loss 0.24524985253810883\n",
      "[Training Epoch 8] Batch 2630, Loss 0.2828373610973358\n",
      "[Training Epoch 8] Batch 2631, Loss 0.27356094121932983\n",
      "[Training Epoch 8] Batch 2632, Loss 0.24175086617469788\n",
      "[Training Epoch 8] Batch 2633, Loss 0.24983200430870056\n",
      "[Training Epoch 8] Batch 2634, Loss 0.2446666657924652\n",
      "[Training Epoch 8] Batch 2635, Loss 0.2762889862060547\n",
      "[Training Epoch 8] Batch 2636, Loss 0.27351731061935425\n",
      "[Training Epoch 8] Batch 2637, Loss 0.2598635256290436\n",
      "[Training Epoch 8] Batch 2638, Loss 0.27435171604156494\n",
      "[Training Epoch 8] Batch 2639, Loss 0.23776407539844513\n",
      "[Training Epoch 8] Batch 2640, Loss 0.2722909152507782\n",
      "[Training Epoch 8] Batch 2641, Loss 0.2497386634349823\n",
      "[Training Epoch 8] Batch 2642, Loss 0.25880295038223267\n",
      "[Training Epoch 8] Batch 2643, Loss 0.2561422884464264\n",
      "[Training Epoch 8] Batch 2644, Loss 0.2534622251987457\n",
      "[Training Epoch 8] Batch 2645, Loss 0.27508848905563354\n",
      "[Training Epoch 8] Batch 2646, Loss 0.23882263898849487\n",
      "[Training Epoch 8] Batch 2647, Loss 0.26294204592704773\n",
      "[Training Epoch 8] Batch 2648, Loss 0.2510378956794739\n",
      "[Training Epoch 8] Batch 2649, Loss 0.26635265350341797\n",
      "[Training Epoch 8] Batch 2650, Loss 0.24229608476161957\n",
      "[Training Epoch 8] Batch 2651, Loss 0.24796509742736816\n",
      "[Training Epoch 8] Batch 2652, Loss 0.23883524537086487\n",
      "[Training Epoch 8] Batch 2653, Loss 0.24931001663208008\n",
      "[Training Epoch 8] Batch 2654, Loss 0.24968726933002472\n",
      "[Training Epoch 8] Batch 2655, Loss 0.2459971308708191\n",
      "[Training Epoch 8] Batch 2656, Loss 0.24920225143432617\n",
      "[Training Epoch 8] Batch 2657, Loss 0.2518993020057678\n",
      "[Training Epoch 8] Batch 2658, Loss 0.26580578088760376\n",
      "[Training Epoch 8] Batch 2659, Loss 0.2639314532279968\n",
      "[Training Epoch 8] Batch 2660, Loss 0.27964067459106445\n",
      "[Training Epoch 8] Batch 2661, Loss 0.25226378440856934\n",
      "[Training Epoch 8] Batch 2662, Loss 0.23947983980178833\n",
      "[Training Epoch 8] Batch 2663, Loss 0.23890703916549683\n",
      "[Training Epoch 8] Batch 2664, Loss 0.27066439390182495\n",
      "[Training Epoch 8] Batch 2665, Loss 0.2667362093925476\n",
      "[Training Epoch 8] Batch 2666, Loss 0.25016844272613525\n",
      "[Training Epoch 8] Batch 2667, Loss 0.26125097274780273\n",
      "[Training Epoch 8] Batch 2668, Loss 0.27643007040023804\n",
      "[Training Epoch 8] Batch 2669, Loss 0.2650206685066223\n",
      "[Training Epoch 8] Batch 2670, Loss 0.23849251866340637\n",
      "[Training Epoch 8] Batch 2671, Loss 0.23258227109909058\n",
      "[Training Epoch 8] Batch 2672, Loss 0.28312361240386963\n",
      "[Training Epoch 8] Batch 2673, Loss 0.2278139591217041\n",
      "[Training Epoch 8] Batch 2674, Loss 0.2521258592605591\n",
      "[Training Epoch 8] Batch 2675, Loss 0.2580512464046478\n",
      "[Training Epoch 8] Batch 2676, Loss 0.24061012268066406\n",
      "[Training Epoch 8] Batch 2677, Loss 0.25691449642181396\n",
      "[Training Epoch 8] Batch 2678, Loss 0.25962063670158386\n",
      "[Training Epoch 8] Batch 2679, Loss 0.2335296869277954\n",
      "[Training Epoch 8] Batch 2680, Loss 0.24392393231391907\n",
      "[Training Epoch 8] Batch 2681, Loss 0.2729770541191101\n",
      "[Training Epoch 8] Batch 2682, Loss 0.2498210072517395\n",
      "[Training Epoch 8] Batch 2683, Loss 0.2436530590057373\n",
      "[Training Epoch 8] Batch 2684, Loss 0.28757745027542114\n",
      "[Training Epoch 8] Batch 2685, Loss 0.2935432493686676\n",
      "[Training Epoch 8] Batch 2686, Loss 0.27093306183815\n",
      "[Training Epoch 8] Batch 2687, Loss 0.2752171754837036\n",
      "[Training Epoch 8] Batch 2688, Loss 0.26552921533584595\n",
      "[Training Epoch 8] Batch 2689, Loss 0.2914579510688782\n",
      "[Training Epoch 8] Batch 2690, Loss 0.25602591037750244\n",
      "[Training Epoch 8] Batch 2691, Loss 0.27337002754211426\n",
      "[Training Epoch 8] Batch 2692, Loss 0.28040796518325806\n",
      "[Training Epoch 8] Batch 2693, Loss 0.2647581994533539\n",
      "[Training Epoch 8] Batch 2694, Loss 0.2748865485191345\n",
      "[Training Epoch 8] Batch 2695, Loss 0.2493368536233902\n",
      "[Training Epoch 8] Batch 2696, Loss 0.2751205563545227\n",
      "[Training Epoch 8] Batch 2697, Loss 0.2709026336669922\n",
      "[Training Epoch 8] Batch 2698, Loss 0.26737505197525024\n",
      "[Training Epoch 8] Batch 2699, Loss 0.2630515694618225\n",
      "[Training Epoch 8] Batch 2700, Loss 0.2738496959209442\n",
      "[Training Epoch 8] Batch 2701, Loss 0.2692638635635376\n",
      "[Training Epoch 8] Batch 2702, Loss 0.24956028163433075\n",
      "[Training Epoch 8] Batch 2703, Loss 0.25261884927749634\n",
      "[Training Epoch 8] Batch 2704, Loss 0.23774780333042145\n",
      "[Training Epoch 8] Batch 2705, Loss 0.27245575189590454\n",
      "[Training Epoch 8] Batch 2706, Loss 0.27170735597610474\n",
      "[Training Epoch 8] Batch 2707, Loss 0.24325956404209137\n",
      "[Training Epoch 8] Batch 2708, Loss 0.26159781217575073\n",
      "[Training Epoch 8] Batch 2709, Loss 0.248361736536026\n",
      "[Training Epoch 8] Batch 2710, Loss 0.28665709495544434\n",
      "[Training Epoch 8] Batch 2711, Loss 0.2902165651321411\n",
      "[Training Epoch 8] Batch 2712, Loss 0.2659938335418701\n",
      "[Training Epoch 8] Batch 2713, Loss 0.2607150971889496\n",
      "[Training Epoch 8] Batch 2714, Loss 0.2712501883506775\n",
      "[Training Epoch 8] Batch 2715, Loss 0.2542073726654053\n",
      "[Training Epoch 8] Batch 2716, Loss 0.29228687286376953\n",
      "[Training Epoch 8] Batch 2717, Loss 0.23418565094470978\n",
      "[Training Epoch 8] Batch 2718, Loss 0.23975464701652527\n",
      "[Training Epoch 8] Batch 2719, Loss 0.2565966844558716\n",
      "[Training Epoch 8] Batch 2720, Loss 0.2463245540857315\n",
      "[Training Epoch 8] Batch 2721, Loss 0.24207428097724915\n",
      "[Training Epoch 8] Batch 2722, Loss 0.2457510381937027\n",
      "[Training Epoch 8] Batch 2723, Loss 0.23323799669742584\n",
      "[Training Epoch 8] Batch 2724, Loss 0.25236624479293823\n",
      "[Training Epoch 8] Batch 2725, Loss 0.23138773441314697\n",
      "[Training Epoch 8] Batch 2726, Loss 0.26427552103996277\n",
      "[Training Epoch 8] Batch 2727, Loss 0.2512611150741577\n",
      "[Training Epoch 8] Batch 2728, Loss 0.24697357416152954\n",
      "[Training Epoch 8] Batch 2729, Loss 0.2588610053062439\n",
      "[Training Epoch 8] Batch 2730, Loss 0.2502225637435913\n",
      "[Training Epoch 8] Batch 2731, Loss 0.27471470832824707\n",
      "[Training Epoch 8] Batch 2732, Loss 0.26552814245224\n",
      "[Training Epoch 8] Batch 2733, Loss 0.26298201084136963\n",
      "[Training Epoch 8] Batch 2734, Loss 0.24715016782283783\n",
      "[Training Epoch 8] Batch 2735, Loss 0.2889825105667114\n",
      "[Training Epoch 8] Batch 2736, Loss 0.24477767944335938\n",
      "[Training Epoch 8] Batch 2737, Loss 0.2832554578781128\n",
      "[Training Epoch 8] Batch 2738, Loss 0.274335652589798\n",
      "[Training Epoch 8] Batch 2739, Loss 0.259178102016449\n",
      "[Training Epoch 8] Batch 2740, Loss 0.25251197814941406\n",
      "[Training Epoch 8] Batch 2741, Loss 0.26231682300567627\n",
      "[Training Epoch 8] Batch 2742, Loss 0.2237054407596588\n",
      "[Training Epoch 8] Batch 2743, Loss 0.2766111493110657\n",
      "[Training Epoch 8] Batch 2744, Loss 0.25713279843330383\n",
      "[Training Epoch 8] Batch 2745, Loss 0.24936744570732117\n",
      "[Training Epoch 8] Batch 2746, Loss 0.22589445114135742\n",
      "[Training Epoch 8] Batch 2747, Loss 0.2661730647087097\n",
      "[Training Epoch 8] Batch 2748, Loss 0.29368239641189575\n",
      "[Training Epoch 8] Batch 2749, Loss 0.2828854024410248\n",
      "[Training Epoch 8] Batch 2750, Loss 0.2514587342739105\n",
      "[Training Epoch 8] Batch 2751, Loss 0.249810591340065\n",
      "[Training Epoch 8] Batch 2752, Loss 0.24064576625823975\n",
      "[Training Epoch 8] Batch 2753, Loss 0.26529189944267273\n",
      "[Training Epoch 8] Batch 2754, Loss 0.24998049437999725\n",
      "[Training Epoch 8] Batch 2755, Loss 0.2547808587551117\n",
      "[Training Epoch 8] Batch 2756, Loss 0.2556819021701813\n",
      "[Training Epoch 8] Batch 2757, Loss 0.2576506733894348\n",
      "[Training Epoch 8] Batch 2758, Loss 0.2512449622154236\n",
      "[Training Epoch 8] Batch 2759, Loss 0.268598347902298\n",
      "[Training Epoch 8] Batch 2760, Loss 0.26740360260009766\n",
      "[Training Epoch 8] Batch 2761, Loss 0.27353811264038086\n",
      "[Training Epoch 8] Batch 2762, Loss 0.2753039002418518\n",
      "[Training Epoch 8] Batch 2763, Loss 0.26939696073532104\n",
      "[Training Epoch 8] Batch 2764, Loss 0.28561750054359436\n",
      "[Training Epoch 8] Batch 2765, Loss 0.23484861850738525\n",
      "[Training Epoch 8] Batch 2766, Loss 0.27533671259880066\n",
      "[Training Epoch 8] Batch 2767, Loss 0.23279300332069397\n",
      "[Training Epoch 8] Batch 2768, Loss 0.2573079466819763\n",
      "[Training Epoch 8] Batch 2769, Loss 0.2632823884487152\n",
      "[Training Epoch 8] Batch 2770, Loss 0.26645076274871826\n",
      "[Training Epoch 8] Batch 2771, Loss 0.22601889073848724\n",
      "[Training Epoch 8] Batch 2772, Loss 0.2691555321216583\n",
      "[Training Epoch 8] Batch 2773, Loss 0.248581200838089\n",
      "[Training Epoch 8] Batch 2774, Loss 0.2617546021938324\n",
      "[Training Epoch 8] Batch 2775, Loss 0.23245343565940857\n",
      "[Training Epoch 8] Batch 2776, Loss 0.25458747148513794\n",
      "[Training Epoch 8] Batch 2777, Loss 0.24708020687103271\n",
      "[Training Epoch 8] Batch 2778, Loss 0.26092827320098877\n",
      "[Training Epoch 8] Batch 2779, Loss 0.25316888093948364\n",
      "[Training Epoch 8] Batch 2780, Loss 0.2639607787132263\n",
      "[Training Epoch 8] Batch 2781, Loss 0.26004350185394287\n",
      "[Training Epoch 8] Batch 2782, Loss 0.2862032651901245\n",
      "[Training Epoch 8] Batch 2783, Loss 0.2121734619140625\n",
      "[Training Epoch 8] Batch 2784, Loss 0.2526138722896576\n",
      "[Training Epoch 8] Batch 2785, Loss 0.26996248960494995\n",
      "[Training Epoch 8] Batch 2786, Loss 0.26488804817199707\n",
      "[Training Epoch 8] Batch 2787, Loss 0.26312121748924255\n",
      "[Training Epoch 8] Batch 2788, Loss 0.2543821930885315\n",
      "[Training Epoch 8] Batch 2789, Loss 0.2484418898820877\n",
      "[Training Epoch 8] Batch 2790, Loss 0.22502171993255615\n",
      "[Training Epoch 8] Batch 2791, Loss 0.23613548278808594\n",
      "[Training Epoch 8] Batch 2792, Loss 0.25270333886146545\n",
      "[Training Epoch 8] Batch 2793, Loss 0.2561752200126648\n",
      "[Training Epoch 8] Batch 2794, Loss 0.274941623210907\n",
      "[Training Epoch 8] Batch 2795, Loss 0.2806517779827118\n",
      "[Training Epoch 8] Batch 2796, Loss 0.24315106868743896\n",
      "[Training Epoch 8] Batch 2797, Loss 0.2415083348751068\n",
      "[Training Epoch 8] Batch 2798, Loss 0.25641775131225586\n",
      "[Training Epoch 8] Batch 2799, Loss 0.2756323218345642\n",
      "[Training Epoch 8] Batch 2800, Loss 0.26191118359565735\n",
      "[Training Epoch 8] Batch 2801, Loss 0.24315877258777618\n",
      "[Training Epoch 8] Batch 2802, Loss 0.24248585104942322\n",
      "[Training Epoch 8] Batch 2803, Loss 0.26228201389312744\n",
      "[Training Epoch 8] Batch 2804, Loss 0.2674824595451355\n",
      "[Training Epoch 8] Batch 2805, Loss 0.26480036973953247\n",
      "[Training Epoch 8] Batch 2806, Loss 0.27301570773124695\n",
      "[Training Epoch 8] Batch 2807, Loss 0.260226845741272\n",
      "[Training Epoch 8] Batch 2808, Loss 0.24021124839782715\n",
      "[Training Epoch 8] Batch 2809, Loss 0.2678418755531311\n",
      "[Training Epoch 8] Batch 2810, Loss 0.2283746898174286\n",
      "[Training Epoch 8] Batch 2811, Loss 0.2750350832939148\n",
      "[Training Epoch 8] Batch 2812, Loss 0.28697165846824646\n",
      "[Training Epoch 8] Batch 2813, Loss 0.26466822624206543\n",
      "[Training Epoch 8] Batch 2814, Loss 0.28234583139419556\n",
      "[Training Epoch 8] Batch 2815, Loss 0.2716534733772278\n",
      "[Training Epoch 8] Batch 2816, Loss 0.27105438709259033\n",
      "[Training Epoch 8] Batch 2817, Loss 0.28502827882766724\n",
      "[Training Epoch 8] Batch 2818, Loss 0.2469302862882614\n",
      "[Training Epoch 8] Batch 2819, Loss 0.2806917726993561\n",
      "[Training Epoch 8] Batch 2820, Loss 0.21771693229675293\n",
      "[Training Epoch 8] Batch 2821, Loss 0.282495379447937\n",
      "[Training Epoch 8] Batch 2822, Loss 0.2718115746974945\n",
      "[Training Epoch 8] Batch 2823, Loss 0.25492432713508606\n",
      "[Training Epoch 8] Batch 2824, Loss 0.24989767372608185\n",
      "[Training Epoch 8] Batch 2825, Loss 0.25980478525161743\n",
      "[Training Epoch 8] Batch 2826, Loss 0.26262232661247253\n",
      "[Training Epoch 8] Batch 2827, Loss 0.257689505815506\n",
      "[Training Epoch 8] Batch 2828, Loss 0.2693413496017456\n",
      "[Training Epoch 8] Batch 2829, Loss 0.24946464598178864\n",
      "[Training Epoch 8] Batch 2830, Loss 0.2996094822883606\n",
      "[Training Epoch 8] Batch 2831, Loss 0.2749307155609131\n",
      "[Training Epoch 8] Batch 2832, Loss 0.2741352617740631\n",
      "[Training Epoch 8] Batch 2833, Loss 0.24407902359962463\n",
      "[Training Epoch 8] Batch 2834, Loss 0.24886424839496613\n",
      "[Training Epoch 8] Batch 2835, Loss 0.25276392698287964\n",
      "[Training Epoch 8] Batch 2836, Loss 0.26333528757095337\n",
      "[Training Epoch 8] Batch 2837, Loss 0.26619046926498413\n",
      "[Training Epoch 8] Batch 2838, Loss 0.2411670684814453\n",
      "[Training Epoch 8] Batch 2839, Loss 0.25582942366600037\n",
      "[Training Epoch 8] Batch 2840, Loss 0.2185685634613037\n",
      "[Training Epoch 8] Batch 2841, Loss 0.25356656312942505\n",
      "[Training Epoch 8] Batch 2842, Loss 0.27255862951278687\n",
      "[Training Epoch 8] Batch 2843, Loss 0.22542545199394226\n",
      "[Training Epoch 8] Batch 2844, Loss 0.24194979667663574\n",
      "[Training Epoch 8] Batch 2845, Loss 0.2609984874725342\n",
      "[Training Epoch 8] Batch 2846, Loss 0.25932392477989197\n",
      "[Training Epoch 8] Batch 2847, Loss 0.2421742081642151\n",
      "[Training Epoch 8] Batch 2848, Loss 0.251090943813324\n",
      "[Training Epoch 8] Batch 2849, Loss 0.23410086333751678\n",
      "[Training Epoch 8] Batch 2850, Loss 0.26221442222595215\n",
      "[Training Epoch 8] Batch 2851, Loss 0.2599526643753052\n",
      "[Training Epoch 8] Batch 2852, Loss 0.22424370050430298\n",
      "[Training Epoch 8] Batch 2853, Loss 0.2554309368133545\n",
      "[Training Epoch 8] Batch 2854, Loss 0.28246021270751953\n",
      "[Training Epoch 8] Batch 2855, Loss 0.22803692519664764\n",
      "[Training Epoch 8] Batch 2856, Loss 0.24453768134117126\n",
      "[Training Epoch 8] Batch 2857, Loss 0.27968165278434753\n",
      "[Training Epoch 8] Batch 2858, Loss 0.23773802816867828\n",
      "[Training Epoch 8] Batch 2859, Loss 0.2690959870815277\n",
      "[Training Epoch 8] Batch 2860, Loss 0.2613908052444458\n",
      "[Training Epoch 8] Batch 2861, Loss 0.2371290773153305\n",
      "[Training Epoch 8] Batch 2862, Loss 0.2515075206756592\n",
      "[Training Epoch 8] Batch 2863, Loss 0.25564146041870117\n",
      "[Training Epoch 8] Batch 2864, Loss 0.2462809681892395\n",
      "[Training Epoch 8] Batch 2865, Loss 0.25445330142974854\n",
      "[Training Epoch 8] Batch 2866, Loss 0.24838724732398987\n",
      "[Training Epoch 8] Batch 2867, Loss 0.26394861936569214\n",
      "[Training Epoch 8] Batch 2868, Loss 0.22788219153881073\n",
      "[Training Epoch 8] Batch 2869, Loss 0.24945369362831116\n",
      "[Training Epoch 8] Batch 2870, Loss 0.24353234469890594\n",
      "[Training Epoch 8] Batch 2871, Loss 0.25839337706565857\n",
      "[Training Epoch 8] Batch 2872, Loss 0.2718690037727356\n",
      "[Training Epoch 8] Batch 2873, Loss 0.2879142761230469\n",
      "[Training Epoch 8] Batch 2874, Loss 0.2639262080192566\n",
      "[Training Epoch 8] Batch 2875, Loss 0.2551085352897644\n",
      "[Training Epoch 8] Batch 2876, Loss 0.2476826310157776\n",
      "[Training Epoch 8] Batch 2877, Loss 0.2615263760089874\n",
      "[Training Epoch 8] Batch 2878, Loss 0.2760878801345825\n",
      "[Training Epoch 8] Batch 2879, Loss 0.2983807921409607\n",
      "[Training Epoch 8] Batch 2880, Loss 0.2537814974784851\n",
      "[Training Epoch 8] Batch 2881, Loss 0.252604603767395\n",
      "[Training Epoch 8] Batch 2882, Loss 0.2413674294948578\n",
      "[Training Epoch 8] Batch 2883, Loss 0.24241141974925995\n",
      "[Training Epoch 8] Batch 2884, Loss 0.2651703953742981\n",
      "[Training Epoch 8] Batch 2885, Loss 0.22152811288833618\n",
      "[Training Epoch 8] Batch 2886, Loss 0.27258121967315674\n",
      "[Training Epoch 8] Batch 2887, Loss 0.27474677562713623\n",
      "[Training Epoch 8] Batch 2888, Loss 0.2494504302740097\n",
      "[Training Epoch 8] Batch 2889, Loss 0.2549779415130615\n",
      "[Training Epoch 8] Batch 2890, Loss 0.2574409246444702\n",
      "[Training Epoch 8] Batch 2891, Loss 0.2321670651435852\n",
      "[Training Epoch 8] Batch 2892, Loss 0.2336626946926117\n",
      "[Training Epoch 8] Batch 2893, Loss 0.26315876841545105\n",
      "[Training Epoch 8] Batch 2894, Loss 0.2501674294471741\n",
      "[Training Epoch 8] Batch 2895, Loss 0.27400004863739014\n",
      "[Training Epoch 8] Batch 2896, Loss 0.2892301678657532\n",
      "[Training Epoch 8] Batch 2897, Loss 0.26348328590393066\n",
      "[Training Epoch 8] Batch 2898, Loss 0.25458985567092896\n",
      "[Training Epoch 8] Batch 2899, Loss 0.23238612711429596\n",
      "[Training Epoch 8] Batch 2900, Loss 0.26064956188201904\n",
      "[Training Epoch 8] Batch 2901, Loss 0.2445451319217682\n",
      "[Training Epoch 8] Batch 2902, Loss 0.26459160447120667\n",
      "[Training Epoch 8] Batch 2903, Loss 0.24757421016693115\n",
      "[Training Epoch 8] Batch 2904, Loss 0.24839289486408234\n",
      "[Training Epoch 8] Batch 2905, Loss 0.2556682527065277\n",
      "[Training Epoch 8] Batch 2906, Loss 0.28042763471603394\n",
      "[Training Epoch 8] Batch 2907, Loss 0.250301331281662\n",
      "[Training Epoch 8] Batch 2908, Loss 0.2487066686153412\n",
      "[Training Epoch 8] Batch 2909, Loss 0.237894669175148\n",
      "[Training Epoch 8] Batch 2910, Loss 0.2730102837085724\n",
      "[Training Epoch 8] Batch 2911, Loss 0.24569380283355713\n",
      "[Training Epoch 8] Batch 2912, Loss 0.27567368745803833\n",
      "[Training Epoch 8] Batch 2913, Loss 0.2559613585472107\n",
      "[Training Epoch 8] Batch 2914, Loss 0.24371886253356934\n",
      "[Training Epoch 8] Batch 2915, Loss 0.22286933660507202\n",
      "[Training Epoch 8] Batch 2916, Loss 0.2900276780128479\n",
      "[Training Epoch 8] Batch 2917, Loss 0.27352046966552734\n",
      "[Training Epoch 8] Batch 2918, Loss 0.2477906495332718\n",
      "[Training Epoch 8] Batch 2919, Loss 0.2632545828819275\n",
      "[Training Epoch 8] Batch 2920, Loss 0.2527371942996979\n",
      "[Training Epoch 8] Batch 2921, Loss 0.22847697138786316\n",
      "[Training Epoch 8] Batch 2922, Loss 0.25440913438796997\n",
      "[Training Epoch 8] Batch 2923, Loss 0.2709764838218689\n",
      "[Training Epoch 8] Batch 2924, Loss 0.2612400949001312\n",
      "[Training Epoch 8] Batch 2925, Loss 0.2519080340862274\n",
      "[Training Epoch 8] Batch 2926, Loss 0.271961510181427\n",
      "[Training Epoch 8] Batch 2927, Loss 0.2699347734451294\n",
      "[Training Epoch 8] Batch 2928, Loss 0.24965502321720123\n",
      "[Training Epoch 8] Batch 2929, Loss 0.2555171251296997\n",
      "[Training Epoch 8] Batch 2930, Loss 0.26742276549339294\n",
      "[Training Epoch 8] Batch 2931, Loss 0.2788662016391754\n",
      "[Training Epoch 8] Batch 2932, Loss 0.24427801370620728\n",
      "[Training Epoch 8] Batch 2933, Loss 0.24885722994804382\n",
      "[Training Epoch 8] Batch 2934, Loss 0.2634166181087494\n",
      "[Training Epoch 8] Batch 2935, Loss 0.24855442345142365\n",
      "[Training Epoch 8] Batch 2936, Loss 0.2473827302455902\n",
      "[Training Epoch 8] Batch 2937, Loss 0.2718600332736969\n",
      "[Training Epoch 8] Batch 2938, Loss 0.2690747380256653\n",
      "[Training Epoch 8] Batch 2939, Loss 0.23352201282978058\n",
      "[Training Epoch 8] Batch 2940, Loss 0.25026941299438477\n",
      "[Training Epoch 8] Batch 2941, Loss 0.25383561849594116\n",
      "[Training Epoch 8] Batch 2942, Loss 0.25915077328681946\n",
      "[Training Epoch 8] Batch 2943, Loss 0.2731249928474426\n",
      "[Training Epoch 8] Batch 2944, Loss 0.25155511498451233\n",
      "[Training Epoch 8] Batch 2945, Loss 0.26400724053382874\n",
      "[Training Epoch 8] Batch 2946, Loss 0.24908959865570068\n",
      "[Training Epoch 8] Batch 2947, Loss 0.24995779991149902\n",
      "[Training Epoch 8] Batch 2948, Loss 0.2630727291107178\n",
      "[Training Epoch 8] Batch 2949, Loss 0.2630803883075714\n",
      "[Training Epoch 8] Batch 2950, Loss 0.2336060106754303\n",
      "[Training Epoch 8] Batch 2951, Loss 0.2636736035346985\n",
      "[Training Epoch 8] Batch 2952, Loss 0.25473958253860474\n",
      "[Training Epoch 8] Batch 2953, Loss 0.24818868935108185\n",
      "[Training Epoch 8] Batch 2954, Loss 0.23784740269184113\n",
      "[Training Epoch 8] Batch 2955, Loss 0.2602478265762329\n",
      "[Training Epoch 8] Batch 2956, Loss 0.28073787689208984\n",
      "[Training Epoch 8] Batch 2957, Loss 0.2651921212673187\n",
      "[Training Epoch 8] Batch 2958, Loss 0.25841397047042847\n",
      "[Training Epoch 8] Batch 2959, Loss 0.26483553647994995\n",
      "[Training Epoch 8] Batch 2960, Loss 0.2831602692604065\n",
      "[Training Epoch 8] Batch 2961, Loss 0.27329474687576294\n",
      "[Training Epoch 8] Batch 2962, Loss 0.2625696659088135\n",
      "[Training Epoch 8] Batch 2963, Loss 0.2561604678630829\n",
      "[Training Epoch 8] Batch 2964, Loss 0.2251783311367035\n",
      "[Training Epoch 8] Batch 2965, Loss 0.2204761803150177\n",
      "[Training Epoch 8] Batch 2966, Loss 0.27603960037231445\n",
      "[Training Epoch 8] Batch 2967, Loss 0.2222563475370407\n",
      "[Training Epoch 8] Batch 2968, Loss 0.2612828016281128\n",
      "[Training Epoch 8] Batch 2969, Loss 0.26780498027801514\n",
      "[Training Epoch 8] Batch 2970, Loss 0.2601206302642822\n",
      "[Training Epoch 8] Batch 2971, Loss 0.2653675675392151\n",
      "[Training Epoch 8] Batch 2972, Loss 0.2524721026420593\n",
      "[Training Epoch 8] Batch 2973, Loss 0.2357676476240158\n",
      "[Training Epoch 8] Batch 2974, Loss 0.27186205983161926\n",
      "[Training Epoch 8] Batch 2975, Loss 0.25503820180892944\n",
      "[Training Epoch 8] Batch 2976, Loss 0.2804334759712219\n",
      "[Training Epoch 8] Batch 2977, Loss 0.2506874203681946\n",
      "[Training Epoch 8] Batch 2978, Loss 0.27436551451683044\n",
      "[Training Epoch 8] Batch 2979, Loss 0.24061869084835052\n",
      "[Training Epoch 8] Batch 2980, Loss 0.2333158552646637\n",
      "[Training Epoch 8] Batch 2981, Loss 0.2614419162273407\n",
      "[Training Epoch 8] Batch 2982, Loss 0.28141361474990845\n",
      "[Training Epoch 8] Batch 2983, Loss 0.24551409482955933\n",
      "[Training Epoch 8] Batch 2984, Loss 0.2286970168352127\n",
      "[Training Epoch 8] Batch 2985, Loss 0.2563246190547943\n",
      "[Training Epoch 8] Batch 2986, Loss 0.25224992632865906\n",
      "[Training Epoch 8] Batch 2987, Loss 0.24062372744083405\n",
      "[Training Epoch 8] Batch 2988, Loss 0.2561032474040985\n",
      "[Training Epoch 8] Batch 2989, Loss 0.2586892247200012\n",
      "[Training Epoch 8] Batch 2990, Loss 0.25087642669677734\n",
      "[Training Epoch 8] Batch 2991, Loss 0.2443562150001526\n",
      "[Training Epoch 8] Batch 2992, Loss 0.26031389832496643\n",
      "[Training Epoch 8] Batch 2993, Loss 0.27307838201522827\n",
      "[Training Epoch 8] Batch 2994, Loss 0.2651667892932892\n",
      "[Training Epoch 8] Batch 2995, Loss 0.2331092655658722\n",
      "[Training Epoch 8] Batch 2996, Loss 0.24909494817256927\n",
      "[Training Epoch 8] Batch 2997, Loss 0.2715161442756653\n",
      "[Training Epoch 8] Batch 2998, Loss 0.2701849341392517\n",
      "[Training Epoch 8] Batch 2999, Loss 0.25894567370414734\n",
      "[Training Epoch 8] Batch 3000, Loss 0.24326223134994507\n",
      "[Training Epoch 8] Batch 3001, Loss 0.26416486501693726\n",
      "[Training Epoch 8] Batch 3002, Loss 0.249019056558609\n",
      "[Training Epoch 8] Batch 3003, Loss 0.2755947411060333\n",
      "[Training Epoch 8] Batch 3004, Loss 0.2511787414550781\n",
      "[Training Epoch 8] Batch 3005, Loss 0.2568601965904236\n",
      "[Training Epoch 8] Batch 3006, Loss 0.2675926983356476\n",
      "[Training Epoch 8] Batch 3007, Loss 0.24532699584960938\n",
      "[Training Epoch 8] Batch 3008, Loss 0.2689615488052368\n",
      "[Training Epoch 8] Batch 3009, Loss 0.231089785695076\n",
      "[Training Epoch 8] Batch 3010, Loss 0.24913375079631805\n",
      "[Training Epoch 8] Batch 3011, Loss 0.24999013543128967\n",
      "[Training Epoch 8] Batch 3012, Loss 0.25219783186912537\n",
      "[Training Epoch 8] Batch 3013, Loss 0.23494520783424377\n",
      "[Training Epoch 8] Batch 3014, Loss 0.22895248234272003\n",
      "[Training Epoch 8] Batch 3015, Loss 0.2815093994140625\n",
      "[Training Epoch 8] Batch 3016, Loss 0.2648158073425293\n",
      "[Training Epoch 8] Batch 3017, Loss 0.2829490005970001\n",
      "[Training Epoch 8] Batch 3018, Loss 0.22780679166316986\n",
      "[Training Epoch 8] Batch 3019, Loss 0.25036299228668213\n",
      "[Training Epoch 8] Batch 3020, Loss 0.26183971762657166\n",
      "[Training Epoch 8] Batch 3021, Loss 0.2855945825576782\n",
      "[Training Epoch 8] Batch 3022, Loss 0.2420293390750885\n",
      "[Training Epoch 8] Batch 3023, Loss 0.2716718018054962\n",
      "[Training Epoch 8] Batch 3024, Loss 0.26292145252227783\n",
      "[Training Epoch 8] Batch 3025, Loss 0.24376198649406433\n",
      "[Training Epoch 8] Batch 3026, Loss 0.2513435482978821\n",
      "[Training Epoch 8] Batch 3027, Loss 0.23451542854309082\n",
      "[Training Epoch 8] Batch 3028, Loss 0.2706858515739441\n",
      "[Training Epoch 8] Batch 3029, Loss 0.2725192904472351\n",
      "[Training Epoch 8] Batch 3030, Loss 0.22697842121124268\n",
      "[Training Epoch 8] Batch 3031, Loss 0.24617691338062286\n",
      "[Training Epoch 8] Batch 3032, Loss 0.20845815539360046\n",
      "[Training Epoch 8] Batch 3033, Loss 0.2529720067977905\n",
      "[Training Epoch 8] Batch 3034, Loss 0.24281790852546692\n",
      "[Training Epoch 8] Batch 3035, Loss 0.25191566348075867\n",
      "[Training Epoch 8] Batch 3036, Loss 0.2509097158908844\n",
      "[Training Epoch 8] Batch 3037, Loss 0.24278905987739563\n",
      "[Training Epoch 8] Batch 3038, Loss 0.27430278062820435\n",
      "[Training Epoch 8] Batch 3039, Loss 0.2786862552165985\n",
      "[Training Epoch 8] Batch 3040, Loss 0.2806330919265747\n",
      "[Training Epoch 8] Batch 3041, Loss 0.2479386329650879\n",
      "[Training Epoch 8] Batch 3042, Loss 0.263576477766037\n",
      "[Training Epoch 8] Batch 3043, Loss 0.2755451202392578\n",
      "[Training Epoch 8] Batch 3044, Loss 0.23896773159503937\n",
      "[Training Epoch 8] Batch 3045, Loss 0.25565075874328613\n",
      "[Training Epoch 8] Batch 3046, Loss 0.27722492814064026\n",
      "[Training Epoch 8] Batch 3047, Loss 0.24523450434207916\n",
      "[Training Epoch 8] Batch 3048, Loss 0.27065810561180115\n",
      "[Training Epoch 8] Batch 3049, Loss 0.24897009134292603\n",
      "[Training Epoch 8] Batch 3050, Loss 0.22204191982746124\n",
      "[Training Epoch 8] Batch 3051, Loss 0.26079824566841125\n",
      "[Training Epoch 8] Batch 3052, Loss 0.25729477405548096\n",
      "[Training Epoch 8] Batch 3053, Loss 0.23609736561775208\n",
      "[Training Epoch 8] Batch 3054, Loss 0.23956885933876038\n",
      "[Training Epoch 8] Batch 3055, Loss 0.24771399796009064\n",
      "[Training Epoch 8] Batch 3056, Loss 0.30060136318206787\n",
      "[Training Epoch 8] Batch 3057, Loss 0.25696462392807007\n",
      "[Training Epoch 8] Batch 3058, Loss 0.24497833847999573\n",
      "[Training Epoch 8] Batch 3059, Loss 0.26583215594291687\n",
      "[Training Epoch 8] Batch 3060, Loss 0.27006882429122925\n",
      "[Training Epoch 8] Batch 3061, Loss 0.2594648599624634\n",
      "[Training Epoch 8] Batch 3062, Loss 0.24659153819084167\n",
      "[Training Epoch 8] Batch 3063, Loss 0.2638949751853943\n",
      "[Training Epoch 8] Batch 3064, Loss 0.27051645517349243\n",
      "[Training Epoch 8] Batch 3065, Loss 0.25660985708236694\n",
      "[Training Epoch 8] Batch 3066, Loss 0.2517024278640747\n",
      "[Training Epoch 8] Batch 3067, Loss 0.24643567204475403\n",
      "[Training Epoch 8] Batch 3068, Loss 0.25683847069740295\n",
      "[Training Epoch 8] Batch 3069, Loss 0.2516489624977112\n",
      "[Training Epoch 8] Batch 3070, Loss 0.23329177498817444\n",
      "[Training Epoch 8] Batch 3071, Loss 0.2368679940700531\n",
      "[Training Epoch 8] Batch 3072, Loss 0.2525610029697418\n",
      "[Training Epoch 8] Batch 3073, Loss 0.27339261770248413\n",
      "[Training Epoch 8] Batch 3074, Loss 0.24946923553943634\n",
      "[Training Epoch 8] Batch 3075, Loss 0.24574267864227295\n",
      "[Training Epoch 8] Batch 3076, Loss 0.24782013893127441\n",
      "[Training Epoch 8] Batch 3077, Loss 0.26966726779937744\n",
      "[Training Epoch 8] Batch 3078, Loss 0.256432443857193\n",
      "[Training Epoch 8] Batch 3079, Loss 0.27300000190734863\n",
      "[Training Epoch 8] Batch 3080, Loss 0.2516629099845886\n",
      "[Training Epoch 8] Batch 3081, Loss 0.28713324666023254\n",
      "[Training Epoch 8] Batch 3082, Loss 0.2616027593612671\n",
      "[Training Epoch 8] Batch 3083, Loss 0.2911123037338257\n",
      "[Training Epoch 8] Batch 3084, Loss 0.2634686231613159\n",
      "[Training Epoch 8] Batch 3085, Loss 0.2376660406589508\n",
      "[Training Epoch 8] Batch 3086, Loss 0.2533808648586273\n",
      "[Training Epoch 8] Batch 3087, Loss 0.2342962920665741\n",
      "[Training Epoch 8] Batch 3088, Loss 0.24170778691768646\n",
      "[Training Epoch 8] Batch 3089, Loss 0.2566658854484558\n",
      "[Training Epoch 8] Batch 3090, Loss 0.2573590576648712\n",
      "[Training Epoch 8] Batch 3091, Loss 0.25152140855789185\n",
      "[Training Epoch 8] Batch 3092, Loss 0.25159189105033875\n",
      "[Training Epoch 8] Batch 3093, Loss 0.2640140652656555\n",
      "[Training Epoch 8] Batch 3094, Loss 0.25983870029449463\n",
      "[Training Epoch 8] Batch 3095, Loss 0.26268333196640015\n",
      "[Training Epoch 8] Batch 3096, Loss 0.23597058653831482\n",
      "[Training Epoch 8] Batch 3097, Loss 0.24879838526248932\n",
      "[Training Epoch 8] Batch 3098, Loss 0.2523994445800781\n",
      "[Training Epoch 8] Batch 3099, Loss 0.25700175762176514\n",
      "[Training Epoch 8] Batch 3100, Loss 0.23547300696372986\n",
      "[Training Epoch 8] Batch 3101, Loss 0.26968297362327576\n",
      "[Training Epoch 8] Batch 3102, Loss 0.2372630536556244\n",
      "[Training Epoch 8] Batch 3103, Loss 0.2581859230995178\n",
      "[Training Epoch 8] Batch 3104, Loss 0.23882341384887695\n",
      "[Training Epoch 8] Batch 3105, Loss 0.2396022230386734\n",
      "[Training Epoch 8] Batch 3106, Loss 0.24660146236419678\n",
      "[Training Epoch 8] Batch 3107, Loss 0.26110273599624634\n",
      "[Training Epoch 8] Batch 3108, Loss 0.26560068130493164\n",
      "[Training Epoch 8] Batch 3109, Loss 0.2599782943725586\n",
      "[Training Epoch 8] Batch 3110, Loss 0.2527643144130707\n",
      "[Training Epoch 8] Batch 3111, Loss 0.24013639986515045\n",
      "[Training Epoch 8] Batch 3112, Loss 0.2752842307090759\n",
      "[Training Epoch 8] Batch 3113, Loss 0.27404099702835083\n",
      "[Training Epoch 8] Batch 3114, Loss 0.2530030608177185\n",
      "[Training Epoch 8] Batch 3115, Loss 0.25659075379371643\n",
      "[Training Epoch 8] Batch 3116, Loss 0.260267049074173\n",
      "[Training Epoch 8] Batch 3117, Loss 0.27147752046585083\n",
      "[Training Epoch 8] Batch 3118, Loss 0.27047377824783325\n",
      "[Training Epoch 8] Batch 3119, Loss 0.27081748843193054\n",
      "[Training Epoch 8] Batch 3120, Loss 0.25150299072265625\n",
      "[Training Epoch 8] Batch 3121, Loss 0.22134225070476532\n",
      "[Training Epoch 8] Batch 3122, Loss 0.25410565733909607\n",
      "[Training Epoch 8] Batch 3123, Loss 0.24992208182811737\n",
      "[Training Epoch 8] Batch 3124, Loss 0.2713683843612671\n",
      "[Training Epoch 8] Batch 3125, Loss 0.2504006624221802\n",
      "[Training Epoch 8] Batch 3126, Loss 0.25781089067459106\n",
      "[Training Epoch 8] Batch 3127, Loss 0.2306433618068695\n",
      "[Training Epoch 8] Batch 3128, Loss 0.2512372136116028\n",
      "[Training Epoch 8] Batch 3129, Loss 0.2729061245918274\n",
      "[Training Epoch 8] Batch 3130, Loss 0.2742500901222229\n",
      "[Training Epoch 8] Batch 3131, Loss 0.25852224230766296\n",
      "[Training Epoch 8] Batch 3132, Loss 0.24830202758312225\n",
      "[Training Epoch 8] Batch 3133, Loss 0.2598922848701477\n",
      "[Training Epoch 8] Batch 3134, Loss 0.2705989480018616\n",
      "[Training Epoch 8] Batch 3135, Loss 0.2790772616863251\n",
      "[Training Epoch 8] Batch 3136, Loss 0.24700599908828735\n",
      "[Training Epoch 8] Batch 3137, Loss 0.26768723130226135\n",
      "[Training Epoch 8] Batch 3138, Loss 0.24675871431827545\n",
      "[Training Epoch 8] Batch 3139, Loss 0.2665506601333618\n",
      "[Training Epoch 8] Batch 3140, Loss 0.2660104036331177\n",
      "[Training Epoch 8] Batch 3141, Loss 0.24922585487365723\n",
      "[Training Epoch 8] Batch 3142, Loss 0.2781929075717926\n",
      "[Training Epoch 8] Batch 3143, Loss 0.2579003870487213\n",
      "[Training Epoch 8] Batch 3144, Loss 0.28272193670272827\n",
      "[Training Epoch 8] Batch 3145, Loss 0.26166555285453796\n",
      "[Training Epoch 8] Batch 3146, Loss 0.2514982223510742\n",
      "[Training Epoch 8] Batch 3147, Loss 0.2617679238319397\n",
      "[Training Epoch 8] Batch 3148, Loss 0.2304321825504303\n",
      "[Training Epoch 8] Batch 3149, Loss 0.24219612777233124\n",
      "[Training Epoch 8] Batch 3150, Loss 0.28254202008247375\n",
      "[Training Epoch 8] Batch 3151, Loss 0.2238631397485733\n",
      "[Training Epoch 8] Batch 3152, Loss 0.27566665410995483\n",
      "[Training Epoch 8] Batch 3153, Loss 0.28355807065963745\n",
      "[Training Epoch 8] Batch 3154, Loss 0.24132144451141357\n",
      "[Training Epoch 8] Batch 3155, Loss 0.2615414261817932\n",
      "[Training Epoch 8] Batch 3156, Loss 0.2534419000148773\n",
      "[Training Epoch 8] Batch 3157, Loss 0.2616943120956421\n",
      "[Training Epoch 8] Batch 3158, Loss 0.26378464698791504\n",
      "[Training Epoch 8] Batch 3159, Loss 0.27399277687072754\n",
      "[Training Epoch 8] Batch 3160, Loss 0.26829850673675537\n",
      "[Training Epoch 8] Batch 3161, Loss 0.2556564211845398\n",
      "[Training Epoch 8] Batch 3162, Loss 0.26490849256515503\n",
      "[Training Epoch 8] Batch 3163, Loss 0.2804587185382843\n",
      "[Training Epoch 8] Batch 3164, Loss 0.28330713510513306\n",
      "[Training Epoch 8] Batch 3165, Loss 0.2728431224822998\n",
      "[Training Epoch 8] Batch 3166, Loss 0.2580827474594116\n",
      "[Training Epoch 8] Batch 3167, Loss 0.2631992697715759\n",
      "[Training Epoch 8] Batch 3168, Loss 0.26368236541748047\n",
      "[Training Epoch 8] Batch 3169, Loss 0.26258042454719543\n",
      "[Training Epoch 8] Batch 3170, Loss 0.28911611437797546\n",
      "[Training Epoch 8] Batch 3171, Loss 0.2522786259651184\n",
      "[Training Epoch 8] Batch 3172, Loss 0.26419001817703247\n",
      "[Training Epoch 8] Batch 3173, Loss 0.2817528247833252\n",
      "[Training Epoch 8] Batch 3174, Loss 0.24260248243808746\n",
      "[Training Epoch 8] Batch 3175, Loss 0.22769592702388763\n",
      "[Training Epoch 8] Batch 3176, Loss 0.24068251252174377\n",
      "[Training Epoch 8] Batch 3177, Loss 0.2639329433441162\n",
      "[Training Epoch 8] Batch 3178, Loss 0.24567802250385284\n",
      "[Training Epoch 8] Batch 3179, Loss 0.25837305188179016\n",
      "[Training Epoch 8] Batch 3180, Loss 0.27233201265335083\n",
      "[Training Epoch 8] Batch 3181, Loss 0.2662712633609772\n",
      "[Training Epoch 8] Batch 3182, Loss 0.250683069229126\n",
      "[Training Epoch 8] Batch 3183, Loss 0.27295809984207153\n",
      "[Training Epoch 8] Batch 3184, Loss 0.25934624671936035\n",
      "[Training Epoch 8] Batch 3185, Loss 0.2513965368270874\n",
      "[Training Epoch 8] Batch 3186, Loss 0.2454184889793396\n",
      "[Training Epoch 8] Batch 3187, Loss 0.2734677493572235\n",
      "[Training Epoch 8] Batch 3188, Loss 0.2891620099544525\n",
      "[Training Epoch 8] Batch 3189, Loss 0.2192874699831009\n",
      "[Training Epoch 8] Batch 3190, Loss 0.26831644773483276\n",
      "[Training Epoch 8] Batch 3191, Loss 0.2468588650226593\n",
      "[Training Epoch 8] Batch 3192, Loss 0.2583059072494507\n",
      "[Training Epoch 8] Batch 3193, Loss 0.25868093967437744\n",
      "[Training Epoch 8] Batch 3194, Loss 0.24483735859394073\n",
      "[Training Epoch 8] Batch 3195, Loss 0.26639431715011597\n",
      "[Training Epoch 8] Batch 3196, Loss 0.23090018332004547\n",
      "[Training Epoch 8] Batch 3197, Loss 0.24959613382816315\n",
      "[Training Epoch 8] Batch 3198, Loss 0.23619139194488525\n",
      "[Training Epoch 8] Batch 3199, Loss 0.23682637512683868\n",
      "[Training Epoch 8] Batch 3200, Loss 0.25072741508483887\n",
      "[Training Epoch 8] Batch 3201, Loss 0.2580379247665405\n",
      "[Training Epoch 8] Batch 3202, Loss 0.24341917037963867\n",
      "[Training Epoch 8] Batch 3203, Loss 0.27828168869018555\n",
      "[Training Epoch 8] Batch 3204, Loss 0.24307192862033844\n",
      "[Training Epoch 8] Batch 3205, Loss 0.24890467524528503\n",
      "[Training Epoch 8] Batch 3206, Loss 0.2753846049308777\n",
      "[Training Epoch 8] Batch 3207, Loss 0.23810316622257233\n",
      "[Training Epoch 8] Batch 3208, Loss 0.26131898164749146\n",
      "[Training Epoch 8] Batch 3209, Loss 0.28099748492240906\n",
      "[Training Epoch 8] Batch 3210, Loss 0.24757450819015503\n",
      "[Training Epoch 8] Batch 3211, Loss 0.22770874202251434\n",
      "[Training Epoch 8] Batch 3212, Loss 0.25190550088882446\n",
      "[Training Epoch 8] Batch 3213, Loss 0.2632056772708893\n",
      "[Training Epoch 8] Batch 3214, Loss 0.2431812733411789\n",
      "[Training Epoch 8] Batch 3215, Loss 0.24431061744689941\n",
      "[Training Epoch 8] Batch 3216, Loss 0.2646768093109131\n",
      "[Training Epoch 8] Batch 3217, Loss 0.26724082231521606\n",
      "[Training Epoch 8] Batch 3218, Loss 0.269997775554657\n",
      "[Training Epoch 8] Batch 3219, Loss 0.22815710306167603\n",
      "[Training Epoch 8] Batch 3220, Loss 0.27757376432418823\n",
      "[Training Epoch 8] Batch 3221, Loss 0.25556185841560364\n",
      "[Training Epoch 8] Batch 3222, Loss 0.2550533413887024\n",
      "[Training Epoch 8] Batch 3223, Loss 0.26571711897850037\n",
      "[Training Epoch 8] Batch 3224, Loss 0.25097978115081787\n",
      "[Training Epoch 8] Batch 3225, Loss 0.27303487062454224\n",
      "[Training Epoch 8] Batch 3226, Loss 0.21865317225456238\n",
      "[Training Epoch 8] Batch 3227, Loss 0.24814236164093018\n",
      "[Training Epoch 8] Batch 3228, Loss 0.2702327072620392\n",
      "[Training Epoch 8] Batch 3229, Loss 0.2469489574432373\n",
      "[Training Epoch 8] Batch 3230, Loss 0.26710015535354614\n",
      "[Training Epoch 8] Batch 3231, Loss 0.2807629108428955\n",
      "[Training Epoch 8] Batch 3232, Loss 0.25322890281677246\n",
      "[Training Epoch 8] Batch 3233, Loss 0.25391021370887756\n",
      "[Training Epoch 8] Batch 3234, Loss 0.2320176213979721\n",
      "[Training Epoch 8] Batch 3235, Loss 0.2523554861545563\n",
      "[Training Epoch 8] Batch 3236, Loss 0.2721008360385895\n",
      "[Training Epoch 8] Batch 3237, Loss 0.2550145983695984\n",
      "[Training Epoch 8] Batch 3238, Loss 0.2229899913072586\n",
      "[Training Epoch 8] Batch 3239, Loss 0.27417612075805664\n",
      "[Training Epoch 8] Batch 3240, Loss 0.2689031660556793\n",
      "[Training Epoch 8] Batch 3241, Loss 0.2374926209449768\n",
      "[Training Epoch 8] Batch 3242, Loss 0.25859734416007996\n",
      "[Training Epoch 8] Batch 3243, Loss 0.2551923394203186\n",
      "[Training Epoch 8] Batch 3244, Loss 0.23862522840499878\n",
      "[Training Epoch 8] Batch 3245, Loss 0.28483474254608154\n",
      "[Training Epoch 8] Batch 3246, Loss 0.24619773030281067\n",
      "[Training Epoch 8] Batch 3247, Loss 0.2499176263809204\n",
      "[Training Epoch 8] Batch 3248, Loss 0.26825258135795593\n",
      "[Training Epoch 8] Batch 3249, Loss 0.23582929372787476\n",
      "[Training Epoch 8] Batch 3250, Loss 0.2427520900964737\n",
      "[Training Epoch 8] Batch 3251, Loss 0.2719486355781555\n",
      "[Training Epoch 8] Batch 3252, Loss 0.27628809213638306\n",
      "[Training Epoch 8] Batch 3253, Loss 0.2714160978794098\n",
      "[Training Epoch 8] Batch 3254, Loss 0.27213969826698303\n",
      "[Training Epoch 8] Batch 3255, Loss 0.2516527771949768\n",
      "[Training Epoch 8] Batch 3256, Loss 0.27491092681884766\n",
      "[Training Epoch 8] Batch 3257, Loss 0.28385356068611145\n",
      "[Training Epoch 8] Batch 3258, Loss 0.23108822107315063\n",
      "[Training Epoch 8] Batch 3259, Loss 0.2413274347782135\n",
      "[Training Epoch 8] Batch 3260, Loss 0.24621552228927612\n",
      "[Training Epoch 8] Batch 3261, Loss 0.23599082231521606\n",
      "[Training Epoch 8] Batch 3262, Loss 0.27184730768203735\n",
      "[Training Epoch 8] Batch 3263, Loss 0.247587189078331\n",
      "[Training Epoch 8] Batch 3264, Loss 0.28705403208732605\n",
      "[Training Epoch 8] Batch 3265, Loss 0.2741805911064148\n",
      "[Training Epoch 8] Batch 3266, Loss 0.27203285694122314\n",
      "[Training Epoch 8] Batch 3267, Loss 0.2674541175365448\n",
      "[Training Epoch 8] Batch 3268, Loss 0.22849714756011963\n",
      "[Training Epoch 8] Batch 3269, Loss 0.2872908413410187\n",
      "[Training Epoch 8] Batch 3270, Loss 0.2523233890533447\n",
      "[Training Epoch 8] Batch 3271, Loss 0.26478132605552673\n",
      "[Training Epoch 8] Batch 3272, Loss 0.26708880066871643\n",
      "[Training Epoch 8] Batch 3273, Loss 0.2529982924461365\n",
      "[Training Epoch 8] Batch 3274, Loss 0.26185283064842224\n",
      "[Training Epoch 8] Batch 3275, Loss 0.2629649043083191\n",
      "[Training Epoch 8] Batch 3276, Loss 0.2649247944355011\n",
      "[Training Epoch 8] Batch 3277, Loss 0.24388627707958221\n",
      "[Training Epoch 8] Batch 3278, Loss 0.25410112738609314\n",
      "[Training Epoch 8] Batch 3279, Loss 0.22973987460136414\n",
      "[Training Epoch 8] Batch 3280, Loss 0.2502594292163849\n",
      "[Training Epoch 8] Batch 3281, Loss 0.2455742061138153\n",
      "[Training Epoch 8] Batch 3282, Loss 0.24184772372245789\n",
      "[Training Epoch 8] Batch 3283, Loss 0.25988060235977173\n",
      "[Training Epoch 8] Batch 3284, Loss 0.2671165466308594\n",
      "[Training Epoch 8] Batch 3285, Loss 0.2772376537322998\n",
      "[Training Epoch 8] Batch 3286, Loss 0.2565597891807556\n",
      "[Training Epoch 8] Batch 3287, Loss 0.2827000021934509\n",
      "[Training Epoch 8] Batch 3288, Loss 0.24342849850654602\n",
      "[Training Epoch 8] Batch 3289, Loss 0.2670671045780182\n",
      "[Training Epoch 8] Batch 3290, Loss 0.2508161664009094\n",
      "[Training Epoch 8] Batch 3291, Loss 0.28226926922798157\n",
      "[Training Epoch 8] Batch 3292, Loss 0.22342488169670105\n",
      "[Training Epoch 8] Batch 3293, Loss 0.26628434658050537\n",
      "[Training Epoch 8] Batch 3294, Loss 0.23324167728424072\n",
      "[Training Epoch 8] Batch 3295, Loss 0.26806920766830444\n",
      "[Training Epoch 8] Batch 3296, Loss 0.24692299962043762\n",
      "[Training Epoch 8] Batch 3297, Loss 0.2689473330974579\n",
      "[Training Epoch 8] Batch 3298, Loss 0.22970420122146606\n",
      "[Training Epoch 8] Batch 3299, Loss 0.25045713782310486\n",
      "[Training Epoch 8] Batch 3300, Loss 0.2479589879512787\n",
      "[Training Epoch 8] Batch 3301, Loss 0.2654861807823181\n",
      "[Training Epoch 8] Batch 3302, Loss 0.25920626521110535\n",
      "[Training Epoch 8] Batch 3303, Loss 0.23308952152729034\n",
      "[Training Epoch 8] Batch 3304, Loss 0.27069291472435\n",
      "[Training Epoch 8] Batch 3305, Loss 0.26198288798332214\n",
      "[Training Epoch 8] Batch 3306, Loss 0.2389979362487793\n",
      "[Training Epoch 8] Batch 3307, Loss 0.25192615389823914\n",
      "[Training Epoch 8] Batch 3308, Loss 0.23866213858127594\n",
      "[Training Epoch 8] Batch 3309, Loss 0.2559055685997009\n",
      "[Training Epoch 8] Batch 3310, Loss 0.2543331980705261\n",
      "[Training Epoch 8] Batch 3311, Loss 0.2581004798412323\n",
      "[Training Epoch 8] Batch 3312, Loss 0.2588713467121124\n",
      "[Training Epoch 8] Batch 3313, Loss 0.26666152477264404\n",
      "[Training Epoch 8] Batch 3314, Loss 0.25232672691345215\n",
      "[Training Epoch 8] Batch 3315, Loss 0.27622267603874207\n",
      "[Training Epoch 8] Batch 3316, Loss 0.2493000030517578\n",
      "[Training Epoch 8] Batch 3317, Loss 0.2683115005493164\n",
      "[Training Epoch 8] Batch 3318, Loss 0.2238788902759552\n",
      "[Training Epoch 8] Batch 3319, Loss 0.27961936593055725\n",
      "[Training Epoch 8] Batch 3320, Loss 0.27400636672973633\n",
      "[Training Epoch 8] Batch 3321, Loss 0.24399594962596893\n",
      "[Training Epoch 8] Batch 3322, Loss 0.21789968013763428\n",
      "[Training Epoch 8] Batch 3323, Loss 0.2544637620449066\n",
      "[Training Epoch 8] Batch 3324, Loss 0.2412203848361969\n",
      "[Training Epoch 8] Batch 3325, Loss 0.25376859307289124\n",
      "[Training Epoch 8] Batch 3326, Loss 0.25148308277130127\n",
      "[Training Epoch 8] Batch 3327, Loss 0.26814067363739014\n",
      "[Training Epoch 8] Batch 3328, Loss 0.2667759656906128\n",
      "[Training Epoch 8] Batch 3329, Loss 0.2843303382396698\n",
      "[Training Epoch 8] Batch 3330, Loss 0.25975894927978516\n",
      "[Training Epoch 8] Batch 3331, Loss 0.22961246967315674\n",
      "[Training Epoch 8] Batch 3332, Loss 0.23041629791259766\n",
      "[Training Epoch 8] Batch 3333, Loss 0.27371203899383545\n",
      "[Training Epoch 8] Batch 3334, Loss 0.2489193081855774\n",
      "[Training Epoch 8] Batch 3335, Loss 0.27566826343536377\n",
      "[Training Epoch 8] Batch 3336, Loss 0.2684021592140198\n",
      "[Training Epoch 8] Batch 3337, Loss 0.28333231806755066\n",
      "[Training Epoch 8] Batch 3338, Loss 0.2609494924545288\n",
      "[Training Epoch 8] Batch 3339, Loss 0.2451513111591339\n",
      "[Training Epoch 8] Batch 3340, Loss 0.2442345917224884\n",
      "[Training Epoch 8] Batch 3341, Loss 0.25004175305366516\n",
      "[Training Epoch 8] Batch 3342, Loss 0.24839000403881073\n",
      "[Training Epoch 8] Batch 3343, Loss 0.2523680031299591\n",
      "[Training Epoch 8] Batch 3344, Loss 0.21829119324684143\n",
      "[Training Epoch 8] Batch 3345, Loss 0.26372063159942627\n",
      "[Training Epoch 8] Batch 3346, Loss 0.2757328748703003\n",
      "[Training Epoch 8] Batch 3347, Loss 0.24534209072589874\n",
      "[Training Epoch 8] Batch 3348, Loss 0.2531430721282959\n",
      "[Training Epoch 8] Batch 3349, Loss 0.2674126923084259\n",
      "[Training Epoch 8] Batch 3350, Loss 0.25427889823913574\n",
      "[Training Epoch 8] Batch 3351, Loss 0.25285160541534424\n",
      "[Training Epoch 8] Batch 3352, Loss 0.2580031752586365\n",
      "[Training Epoch 8] Batch 3353, Loss 0.25554946064949036\n",
      "[Training Epoch 8] Batch 3354, Loss 0.26232948899269104\n",
      "[Training Epoch 8] Batch 3355, Loss 0.24437467753887177\n",
      "[Training Epoch 8] Batch 3356, Loss 0.24274805188179016\n",
      "[Training Epoch 8] Batch 3357, Loss 0.28314071893692017\n",
      "[Training Epoch 8] Batch 3358, Loss 0.2512279748916626\n",
      "[Training Epoch 8] Batch 3359, Loss 0.2540075182914734\n",
      "[Training Epoch 8] Batch 3360, Loss 0.22953706979751587\n",
      "[Training Epoch 8] Batch 3361, Loss 0.2513723075389862\n",
      "[Training Epoch 8] Batch 3362, Loss 0.24628129601478577\n",
      "[Training Epoch 8] Batch 3363, Loss 0.27462026476860046\n",
      "[Training Epoch 8] Batch 3364, Loss 0.2774233818054199\n",
      "[Training Epoch 8] Batch 3365, Loss 0.2456299066543579\n",
      "[Training Epoch 8] Batch 3366, Loss 0.2573636472225189\n",
      "[Training Epoch 8] Batch 3367, Loss 0.23417013883590698\n",
      "[Training Epoch 8] Batch 3368, Loss 0.262106716632843\n",
      "[Training Epoch 8] Batch 3369, Loss 0.2433149218559265\n",
      "[Training Epoch 8] Batch 3370, Loss 0.24537673592567444\n",
      "[Training Epoch 8] Batch 3371, Loss 0.265033096075058\n",
      "[Training Epoch 8] Batch 3372, Loss 0.2558354139328003\n",
      "[Training Epoch 8] Batch 3373, Loss 0.2576824128627777\n",
      "[Training Epoch 8] Batch 3374, Loss 0.2547362446784973\n",
      "[Training Epoch 8] Batch 3375, Loss 0.24900326132774353\n",
      "[Training Epoch 8] Batch 3376, Loss 0.26273077726364136\n",
      "[Training Epoch 8] Batch 3377, Loss 0.25488996505737305\n",
      "[Training Epoch 8] Batch 3378, Loss 0.2626194953918457\n",
      "[Training Epoch 8] Batch 3379, Loss 0.2544343173503876\n",
      "[Training Epoch 8] Batch 3380, Loss 0.23615561425685883\n",
      "[Training Epoch 8] Batch 3381, Loss 0.2509913444519043\n",
      "[Training Epoch 8] Batch 3382, Loss 0.25675585865974426\n",
      "[Training Epoch 8] Batch 3383, Loss 0.24542444944381714\n",
      "[Training Epoch 8] Batch 3384, Loss 0.2682056427001953\n",
      "[Training Epoch 8] Batch 3385, Loss 0.27224037051200867\n",
      "[Training Epoch 8] Batch 3386, Loss 0.24656596779823303\n",
      "[Training Epoch 8] Batch 3387, Loss 0.2798477113246918\n",
      "[Training Epoch 8] Batch 3388, Loss 0.2320367693901062\n",
      "[Training Epoch 8] Batch 3389, Loss 0.2597132921218872\n",
      "[Training Epoch 8] Batch 3390, Loss 0.2640264332294464\n",
      "[Training Epoch 8] Batch 3391, Loss 0.2709783911705017\n",
      "[Training Epoch 8] Batch 3392, Loss 0.25445255637168884\n",
      "[Training Epoch 8] Batch 3393, Loss 0.24695231020450592\n",
      "[Training Epoch 8] Batch 3394, Loss 0.26052409410476685\n",
      "[Training Epoch 8] Batch 3395, Loss 0.27542245388031006\n",
      "[Training Epoch 8] Batch 3396, Loss 0.25224196910858154\n",
      "[Training Epoch 8] Batch 3397, Loss 0.2686399817466736\n",
      "[Training Epoch 8] Batch 3398, Loss 0.2612501382827759\n",
      "[Training Epoch 8] Batch 3399, Loss 0.2649345397949219\n",
      "[Training Epoch 8] Batch 3400, Loss 0.25400274991989136\n",
      "[Training Epoch 8] Batch 3401, Loss 0.2529970109462738\n",
      "[Training Epoch 8] Batch 3402, Loss 0.2655317783355713\n",
      "[Training Epoch 8] Batch 3403, Loss 0.2536073923110962\n",
      "[Training Epoch 8] Batch 3404, Loss 0.2788633704185486\n",
      "[Training Epoch 8] Batch 3405, Loss 0.24249286949634552\n",
      "[Training Epoch 8] Batch 3406, Loss 0.27129024267196655\n",
      "[Training Epoch 8] Batch 3407, Loss 0.27100732922554016\n",
      "[Training Epoch 8] Batch 3408, Loss 0.2645229995250702\n",
      "[Training Epoch 8] Batch 3409, Loss 0.23244988918304443\n",
      "[Training Epoch 8] Batch 3410, Loss 0.2568802833557129\n",
      "[Training Epoch 8] Batch 3411, Loss 0.2441462278366089\n",
      "[Training Epoch 8] Batch 3412, Loss 0.24609652161598206\n",
      "[Training Epoch 8] Batch 3413, Loss 0.2474154233932495\n",
      "[Training Epoch 8] Batch 3414, Loss 0.2760418951511383\n",
      "[Training Epoch 8] Batch 3415, Loss 0.25239598751068115\n",
      "[Training Epoch 8] Batch 3416, Loss 0.2721012234687805\n",
      "[Training Epoch 8] Batch 3417, Loss 0.2624782919883728\n",
      "[Training Epoch 8] Batch 3418, Loss 0.2307659387588501\n",
      "[Training Epoch 8] Batch 3419, Loss 0.24182452261447906\n",
      "[Training Epoch 8] Batch 3420, Loss 0.29791319370269775\n",
      "[Training Epoch 8] Batch 3421, Loss 0.2601681649684906\n",
      "[Training Epoch 8] Batch 3422, Loss 0.2710745334625244\n",
      "[Training Epoch 8] Batch 3423, Loss 0.24536268413066864\n",
      "[Training Epoch 8] Batch 3424, Loss 0.23571017384529114\n",
      "[Training Epoch 8] Batch 3425, Loss 0.2780027389526367\n",
      "[Training Epoch 8] Batch 3426, Loss 0.23975512385368347\n",
      "[Training Epoch 8] Batch 3427, Loss 0.2660316228866577\n",
      "[Training Epoch 8] Batch 3428, Loss 0.25422704219818115\n",
      "[Training Epoch 8] Batch 3429, Loss 0.259878009557724\n",
      "[Training Epoch 8] Batch 3430, Loss 0.2379450798034668\n",
      "[Training Epoch 8] Batch 3431, Loss 0.2593922019004822\n",
      "[Training Epoch 8] Batch 3432, Loss 0.26185816526412964\n",
      "[Training Epoch 8] Batch 3433, Loss 0.26643121242523193\n",
      "[Training Epoch 8] Batch 3434, Loss 0.2346843183040619\n",
      "[Training Epoch 8] Batch 3435, Loss 0.2664390206336975\n",
      "[Training Epoch 8] Batch 3436, Loss 0.2633926570415497\n",
      "[Training Epoch 8] Batch 3437, Loss 0.24004033207893372\n",
      "[Training Epoch 8] Batch 3438, Loss 0.2881366014480591\n",
      "[Training Epoch 8] Batch 3439, Loss 0.24227841198444366\n",
      "[Training Epoch 8] Batch 3440, Loss 0.27384939789772034\n",
      "[Training Epoch 8] Batch 3441, Loss 0.27181440591812134\n",
      "[Training Epoch 8] Batch 3442, Loss 0.25403356552124023\n",
      "[Training Epoch 8] Batch 3443, Loss 0.2464434802532196\n",
      "[Training Epoch 8] Batch 3444, Loss 0.24123966693878174\n",
      "[Training Epoch 8] Batch 3445, Loss 0.28687727451324463\n",
      "[Training Epoch 8] Batch 3446, Loss 0.26959705352783203\n",
      "[Training Epoch 8] Batch 3447, Loss 0.25678151845932007\n",
      "[Training Epoch 8] Batch 3448, Loss 0.25967732071876526\n",
      "[Training Epoch 8] Batch 3449, Loss 0.24970392882823944\n",
      "[Training Epoch 8] Batch 3450, Loss 0.25689059495925903\n",
      "[Training Epoch 8] Batch 3451, Loss 0.23519684374332428\n",
      "[Training Epoch 8] Batch 3452, Loss 0.2933120131492615\n",
      "[Training Epoch 8] Batch 3453, Loss 0.26572585105895996\n",
      "[Training Epoch 8] Batch 3454, Loss 0.254804402589798\n",
      "[Training Epoch 8] Batch 3455, Loss 0.24992240965366364\n",
      "[Training Epoch 8] Batch 3456, Loss 0.2484285980463028\n",
      "[Training Epoch 8] Batch 3457, Loss 0.2689265012741089\n",
      "[Training Epoch 8] Batch 3458, Loss 0.26262980699539185\n",
      "[Training Epoch 8] Batch 3459, Loss 0.24283578991889954\n",
      "[Training Epoch 8] Batch 3460, Loss 0.2785229980945587\n",
      "[Training Epoch 8] Batch 3461, Loss 0.22234627604484558\n",
      "[Training Epoch 8] Batch 3462, Loss 0.28840357065200806\n",
      "[Training Epoch 8] Batch 3463, Loss 0.24910998344421387\n",
      "[Training Epoch 8] Batch 3464, Loss 0.26776862144470215\n",
      "[Training Epoch 8] Batch 3465, Loss 0.27122873067855835\n",
      "[Training Epoch 8] Batch 3466, Loss 0.26698556542396545\n",
      "[Training Epoch 8] Batch 3467, Loss 0.2505718767642975\n",
      "[Training Epoch 8] Batch 3468, Loss 0.25888335704803467\n",
      "[Training Epoch 8] Batch 3469, Loss 0.250887930393219\n",
      "[Training Epoch 8] Batch 3470, Loss 0.2487657070159912\n",
      "[Training Epoch 8] Batch 3471, Loss 0.2483159601688385\n",
      "[Training Epoch 8] Batch 3472, Loss 0.2893395721912384\n",
      "[Training Epoch 8] Batch 3473, Loss 0.24407784640789032\n",
      "[Training Epoch 8] Batch 3474, Loss 0.2641235888004303\n",
      "[Training Epoch 8] Batch 3475, Loss 0.24874022603034973\n",
      "[Training Epoch 8] Batch 3476, Loss 0.2602628469467163\n",
      "[Training Epoch 8] Batch 3477, Loss 0.22931356728076935\n",
      "[Training Epoch 8] Batch 3478, Loss 0.25163012742996216\n",
      "[Training Epoch 8] Batch 3479, Loss 0.27362924814224243\n",
      "[Training Epoch 8] Batch 3480, Loss 0.264471173286438\n",
      "[Training Epoch 8] Batch 3481, Loss 0.24992528557777405\n",
      "[Training Epoch 8] Batch 3482, Loss 0.26298293471336365\n",
      "[Training Epoch 8] Batch 3483, Loss 0.2466174066066742\n",
      "[Training Epoch 8] Batch 3484, Loss 0.25479936599731445\n",
      "[Training Epoch 8] Batch 3485, Loss 0.239770770072937\n",
      "[Training Epoch 8] Batch 3486, Loss 0.2692197561264038\n",
      "[Training Epoch 8] Batch 3487, Loss 0.26536300778388977\n",
      "[Training Epoch 8] Batch 3488, Loss 0.28082650899887085\n",
      "[Training Epoch 8] Batch 3489, Loss 0.2792963683605194\n",
      "[Training Epoch 8] Batch 3490, Loss 0.27445775270462036\n",
      "[Training Epoch 8] Batch 3491, Loss 0.2587347626686096\n",
      "[Training Epoch 8] Batch 3492, Loss 0.2252483069896698\n",
      "[Training Epoch 8] Batch 3493, Loss 0.2452591210603714\n",
      "[Training Epoch 8] Batch 3494, Loss 0.26468420028686523\n",
      "[Training Epoch 8] Batch 3495, Loss 0.24398629367351532\n",
      "[Training Epoch 8] Batch 3496, Loss 0.26029330492019653\n",
      "[Training Epoch 8] Batch 3497, Loss 0.24719497561454773\n",
      "[Training Epoch 8] Batch 3498, Loss 0.24704653024673462\n",
      "[Training Epoch 8] Batch 3499, Loss 0.26494699716567993\n",
      "[Training Epoch 8] Batch 3500, Loss 0.26485833525657654\n",
      "[Training Epoch 8] Batch 3501, Loss 0.25848808884620667\n",
      "[Training Epoch 8] Batch 3502, Loss 0.2745390832424164\n",
      "[Training Epoch 8] Batch 3503, Loss 0.22473399341106415\n",
      "[Training Epoch 8] Batch 3504, Loss 0.23795633018016815\n",
      "[Training Epoch 8] Batch 3505, Loss 0.2612643837928772\n",
      "[Training Epoch 8] Batch 3506, Loss 0.2784425616264343\n",
      "[Training Epoch 8] Batch 3507, Loss 0.28818202018737793\n",
      "[Training Epoch 8] Batch 3508, Loss 0.252002477645874\n",
      "[Training Epoch 8] Batch 3509, Loss 0.2624286115169525\n",
      "[Training Epoch 8] Batch 3510, Loss 0.24526527523994446\n",
      "[Training Epoch 8] Batch 3511, Loss 0.24808740615844727\n",
      "[Training Epoch 8] Batch 3512, Loss 0.26592376828193665\n",
      "[Training Epoch 8] Batch 3513, Loss 0.2725391983985901\n",
      "[Training Epoch 8] Batch 3514, Loss 0.22421860694885254\n",
      "[Training Epoch 8] Batch 3515, Loss 0.2556401789188385\n",
      "[Training Epoch 8] Batch 3516, Loss 0.2450161576271057\n",
      "[Training Epoch 8] Batch 3517, Loss 0.2723131477832794\n",
      "[Training Epoch 8] Batch 3518, Loss 0.24295316636562347\n",
      "[Training Epoch 8] Batch 3519, Loss 0.22662249207496643\n",
      "[Training Epoch 8] Batch 3520, Loss 0.25224095582962036\n",
      "[Training Epoch 8] Batch 3521, Loss 0.23773503303527832\n",
      "[Training Epoch 8] Batch 3522, Loss 0.23965421319007874\n",
      "[Training Epoch 8] Batch 3523, Loss 0.2424216866493225\n",
      "[Training Epoch 8] Batch 3524, Loss 0.2723035514354706\n",
      "[Training Epoch 8] Batch 3525, Loss 0.23367251455783844\n",
      "[Training Epoch 8] Batch 3526, Loss 0.226230651140213\n",
      "[Training Epoch 8] Batch 3527, Loss 0.27937811613082886\n",
      "[Training Epoch 8] Batch 3528, Loss 0.25379490852355957\n",
      "[Training Epoch 8] Batch 3529, Loss 0.28697118163108826\n",
      "[Training Epoch 8] Batch 3530, Loss 0.2900260090827942\n",
      "[Training Epoch 8] Batch 3531, Loss 0.2396429479122162\n",
      "[Training Epoch 8] Batch 3532, Loss 0.25223711133003235\n",
      "[Training Epoch 8] Batch 3533, Loss 0.263010710477829\n",
      "[Training Epoch 8] Batch 3534, Loss 0.2510879635810852\n",
      "[Training Epoch 8] Batch 3535, Loss 0.26380860805511475\n",
      "[Training Epoch 8] Batch 3536, Loss 0.24033021926879883\n",
      "[Training Epoch 8] Batch 3537, Loss 0.25870293378829956\n",
      "[Training Epoch 8] Batch 3538, Loss 0.23130744695663452\n",
      "[Training Epoch 8] Batch 3539, Loss 0.23723793029785156\n",
      "[Training Epoch 8] Batch 3540, Loss 0.26610898971557617\n",
      "[Training Epoch 8] Batch 3541, Loss 0.2493477165699005\n",
      "[Training Epoch 8] Batch 3542, Loss 0.26178574562072754\n",
      "[Training Epoch 8] Batch 3543, Loss 0.2580803632736206\n",
      "[Training Epoch 8] Batch 3544, Loss 0.2326994389295578\n",
      "[Training Epoch 8] Batch 3545, Loss 0.2493513971567154\n",
      "[Training Epoch 8] Batch 3546, Loss 0.28060901165008545\n",
      "[Training Epoch 8] Batch 3547, Loss 0.27021360397338867\n",
      "[Training Epoch 8] Batch 3548, Loss 0.27340152859687805\n",
      "[Training Epoch 8] Batch 3549, Loss 0.25714346766471863\n",
      "[Training Epoch 8] Batch 3550, Loss 0.23443695902824402\n",
      "[Training Epoch 8] Batch 3551, Loss 0.269231379032135\n",
      "[Training Epoch 8] Batch 3552, Loss 0.23060357570648193\n",
      "[Training Epoch 8] Batch 3553, Loss 0.2487444132566452\n",
      "[Training Epoch 8] Batch 3554, Loss 0.26914113759994507\n",
      "[Training Epoch 8] Batch 3555, Loss 0.2688663899898529\n",
      "[Training Epoch 8] Batch 3556, Loss 0.258502334356308\n",
      "[Training Epoch 8] Batch 3557, Loss 0.2546039819717407\n",
      "[Training Epoch 8] Batch 3558, Loss 0.25254762172698975\n",
      "[Training Epoch 8] Batch 3559, Loss 0.24170224368572235\n",
      "[Training Epoch 8] Batch 3560, Loss 0.25133103132247925\n",
      "[Training Epoch 8] Batch 3561, Loss 0.2485482096672058\n",
      "[Training Epoch 8] Batch 3562, Loss 0.24339953064918518\n",
      "[Training Epoch 8] Batch 3563, Loss 0.22736814618110657\n",
      "[Training Epoch 8] Batch 3564, Loss 0.2390805184841156\n",
      "[Training Epoch 8] Batch 3565, Loss 0.279613196849823\n",
      "[Training Epoch 8] Batch 3566, Loss 0.2706507444381714\n",
      "[Training Epoch 8] Batch 3567, Loss 0.25238439440727234\n",
      "[Training Epoch 8] Batch 3568, Loss 0.2549068331718445\n",
      "[Training Epoch 8] Batch 3569, Loss 0.28090566396713257\n",
      "[Training Epoch 8] Batch 3570, Loss 0.2546669840812683\n",
      "[Training Epoch 8] Batch 3571, Loss 0.28595784306526184\n",
      "[Training Epoch 8] Batch 3572, Loss 0.2709577679634094\n",
      "[Training Epoch 8] Batch 3573, Loss 0.2494010627269745\n",
      "[Training Epoch 8] Batch 3574, Loss 0.2516995966434479\n",
      "[Training Epoch 8] Batch 3575, Loss 0.2526853084564209\n",
      "[Training Epoch 8] Batch 3576, Loss 0.25721168518066406\n",
      "[Training Epoch 8] Batch 3577, Loss 0.2382657825946808\n",
      "[Training Epoch 8] Batch 3578, Loss 0.2653930187225342\n",
      "[Training Epoch 8] Batch 3579, Loss 0.26341861486434937\n",
      "[Training Epoch 8] Batch 3580, Loss 0.26200759410858154\n",
      "[Training Epoch 8] Batch 3581, Loss 0.25437459349632263\n",
      "[Training Epoch 8] Batch 3582, Loss 0.2535737156867981\n",
      "[Training Epoch 8] Batch 3583, Loss 0.28190621733665466\n",
      "[Training Epoch 8] Batch 3584, Loss 0.25089409947395325\n",
      "[Training Epoch 8] Batch 3585, Loss 0.25271499156951904\n",
      "[Training Epoch 8] Batch 3586, Loss 0.2723005414009094\n",
      "[Training Epoch 8] Batch 3587, Loss 0.27928420901298523\n",
      "[Training Epoch 8] Batch 3588, Loss 0.26379892230033875\n",
      "[Training Epoch 8] Batch 3589, Loss 0.25794848799705505\n",
      "[Training Epoch 8] Batch 3590, Loss 0.24784430861473083\n",
      "[Training Epoch 8] Batch 3591, Loss 0.2710834741592407\n",
      "[Training Epoch 8] Batch 3592, Loss 0.2654225826263428\n",
      "[Training Epoch 8] Batch 3593, Loss 0.23311194777488708\n",
      "[Training Epoch 8] Batch 3594, Loss 0.29837748408317566\n",
      "[Training Epoch 8] Batch 3595, Loss 0.26801177859306335\n",
      "[Training Epoch 8] Batch 3596, Loss 0.23003500699996948\n",
      "[Training Epoch 8] Batch 3597, Loss 0.28778669238090515\n",
      "[Training Epoch 8] Batch 3598, Loss 0.27242398262023926\n",
      "[Training Epoch 8] Batch 3599, Loss 0.23284165561199188\n",
      "[Training Epoch 8] Batch 3600, Loss 0.24474048614501953\n",
      "[Training Epoch 8] Batch 3601, Loss 0.23912066221237183\n",
      "[Training Epoch 8] Batch 3602, Loss 0.25806593894958496\n",
      "[Training Epoch 8] Batch 3603, Loss 0.2314233034849167\n",
      "[Training Epoch 8] Batch 3604, Loss 0.26161760091781616\n",
      "[Training Epoch 8] Batch 3605, Loss 0.27801603078842163\n",
      "[Training Epoch 8] Batch 3606, Loss 0.2630402147769928\n",
      "[Training Epoch 8] Batch 3607, Loss 0.2646206021308899\n",
      "[Training Epoch 8] Batch 3608, Loss 0.240408793091774\n",
      "[Training Epoch 8] Batch 3609, Loss 0.28809696435928345\n",
      "[Training Epoch 8] Batch 3610, Loss 0.25677838921546936\n",
      "[Training Epoch 8] Batch 3611, Loss 0.2464216947555542\n",
      "[Training Epoch 8] Batch 3612, Loss 0.2622660994529724\n",
      "[Training Epoch 8] Batch 3613, Loss 0.21319571137428284\n",
      "[Training Epoch 8] Batch 3614, Loss 0.2427852749824524\n",
      "[Training Epoch 8] Batch 3615, Loss 0.2918337285518646\n",
      "[Training Epoch 8] Batch 3616, Loss 0.244899719953537\n",
      "[Training Epoch 8] Batch 3617, Loss 0.2536032795906067\n",
      "[Training Epoch 8] Batch 3618, Loss 0.2301238477230072\n",
      "[Training Epoch 8] Batch 3619, Loss 0.24242940545082092\n",
      "[Training Epoch 8] Batch 3620, Loss 0.2755957245826721\n",
      "[Training Epoch 8] Batch 3621, Loss 0.26677030324935913\n",
      "[Training Epoch 8] Batch 3622, Loss 0.25426530838012695\n",
      "[Training Epoch 8] Batch 3623, Loss 0.25355398654937744\n",
      "[Training Epoch 8] Batch 3624, Loss 0.246252179145813\n",
      "[Training Epoch 8] Batch 3625, Loss 0.24681341648101807\n",
      "[Training Epoch 8] Batch 3626, Loss 0.23974609375\n",
      "[Training Epoch 8] Batch 3627, Loss 0.2598625719547272\n",
      "[Training Epoch 8] Batch 3628, Loss 0.26686733961105347\n",
      "[Training Epoch 8] Batch 3629, Loss 0.2490658313035965\n",
      "[Training Epoch 8] Batch 3630, Loss 0.2440778613090515\n",
      "[Training Epoch 8] Batch 3631, Loss 0.2623967230319977\n",
      "[Training Epoch 8] Batch 3632, Loss 0.25519290566444397\n",
      "[Training Epoch 8] Batch 3633, Loss 0.24132131040096283\n",
      "[Training Epoch 8] Batch 3634, Loss 0.24667143821716309\n",
      "[Training Epoch 8] Batch 3635, Loss 0.2647938132286072\n",
      "[Training Epoch 8] Batch 3636, Loss 0.23822805285453796\n",
      "[Training Epoch 8] Batch 3637, Loss 0.24020282924175262\n",
      "[Training Epoch 8] Batch 3638, Loss 0.28124725818634033\n",
      "[Training Epoch 8] Batch 3639, Loss 0.2500297427177429\n",
      "[Training Epoch 8] Batch 3640, Loss 0.27506768703460693\n",
      "[Training Epoch 8] Batch 3641, Loss 0.2595860958099365\n",
      "[Training Epoch 8] Batch 3642, Loss 0.2641579210758209\n",
      "[Training Epoch 8] Batch 3643, Loss 0.26511329412460327\n",
      "[Training Epoch 8] Batch 3644, Loss 0.2369481772184372\n",
      "[Training Epoch 8] Batch 3645, Loss 0.28605276346206665\n",
      "[Training Epoch 8] Batch 3646, Loss 0.2445763498544693\n",
      "[Training Epoch 8] Batch 3647, Loss 0.267852246761322\n",
      "[Training Epoch 8] Batch 3648, Loss 0.2623422145843506\n",
      "[Training Epoch 8] Batch 3649, Loss 0.25258979201316833\n",
      "[Training Epoch 8] Batch 3650, Loss 0.26027584075927734\n",
      "[Training Epoch 8] Batch 3651, Loss 0.2688313126564026\n",
      "[Training Epoch 8] Batch 3652, Loss 0.2743581533432007\n",
      "[Training Epoch 8] Batch 3653, Loss 0.2571365535259247\n",
      "[Training Epoch 8] Batch 3654, Loss 0.27530622482299805\n",
      "[Training Epoch 8] Batch 3655, Loss 0.2525191307067871\n",
      "[Training Epoch 8] Batch 3656, Loss 0.25011926889419556\n",
      "[Training Epoch 8] Batch 3657, Loss 0.25177788734436035\n",
      "[Training Epoch 8] Batch 3658, Loss 0.2524281442165375\n",
      "[Training Epoch 8] Batch 3659, Loss 0.2521216869354248\n",
      "[Training Epoch 8] Batch 3660, Loss 0.23532047867774963\n",
      "[Training Epoch 8] Batch 3661, Loss 0.22851046919822693\n",
      "[Training Epoch 8] Batch 3662, Loss 0.28087687492370605\n",
      "[Training Epoch 8] Batch 3663, Loss 0.2651821970939636\n",
      "[Training Epoch 8] Batch 3664, Loss 0.2657952308654785\n",
      "[Training Epoch 8] Batch 3665, Loss 0.2574855089187622\n",
      "[Training Epoch 8] Batch 3666, Loss 0.26842451095581055\n",
      "[Training Epoch 8] Batch 3667, Loss 0.2470986694097519\n",
      "[Training Epoch 8] Batch 3668, Loss 0.2558448910713196\n",
      "[Training Epoch 8] Batch 3669, Loss 0.23163942992687225\n",
      "[Training Epoch 8] Batch 3670, Loss 0.25764089822769165\n",
      "[Training Epoch 8] Batch 3671, Loss 0.2600564658641815\n",
      "[Training Epoch 8] Batch 3672, Loss 0.2615145444869995\n",
      "[Training Epoch 8] Batch 3673, Loss 0.25786739587783813\n",
      "[Training Epoch 8] Batch 3674, Loss 0.24401868879795074\n",
      "[Training Epoch 8] Batch 3675, Loss 0.2575520873069763\n",
      "[Training Epoch 8] Batch 3676, Loss 0.2748105525970459\n",
      "[Training Epoch 8] Batch 3677, Loss 0.2613103985786438\n",
      "[Training Epoch 8] Batch 3678, Loss 0.24582348763942719\n",
      "[Training Epoch 8] Batch 3679, Loss 0.2668079435825348\n",
      "[Training Epoch 8] Batch 3680, Loss 0.2649278938770294\n",
      "[Training Epoch 8] Batch 3681, Loss 0.26592540740966797\n",
      "[Training Epoch 8] Batch 3682, Loss 0.24545693397521973\n",
      "[Training Epoch 8] Batch 3683, Loss 0.24152332544326782\n",
      "[Training Epoch 8] Batch 3684, Loss 0.2610720098018646\n",
      "[Training Epoch 8] Batch 3685, Loss 0.25724542140960693\n",
      "[Training Epoch 8] Batch 3686, Loss 0.2662632465362549\n",
      "[Training Epoch 8] Batch 3687, Loss 0.2695559561252594\n",
      "[Training Epoch 8] Batch 3688, Loss 0.2504602074623108\n",
      "[Training Epoch 8] Batch 3689, Loss 0.24768516421318054\n",
      "[Training Epoch 8] Batch 3690, Loss 0.2671070992946625\n",
      "[Training Epoch 8] Batch 3691, Loss 0.25549429655075073\n",
      "[Training Epoch 8] Batch 3692, Loss 0.25520241260528564\n",
      "[Training Epoch 8] Batch 3693, Loss 0.2728569507598877\n",
      "[Training Epoch 8] Batch 3694, Loss 0.25695183873176575\n",
      "[Training Epoch 8] Batch 3695, Loss 0.25074973702430725\n",
      "[Training Epoch 8] Batch 3696, Loss 0.27140846848487854\n",
      "[Training Epoch 8] Batch 3697, Loss 0.2511674165725708\n",
      "[Training Epoch 8] Batch 3698, Loss 0.24386000633239746\n",
      "[Training Epoch 8] Batch 3699, Loss 0.26716071367263794\n",
      "[Training Epoch 8] Batch 3700, Loss 0.2818242311477661\n",
      "[Training Epoch 8] Batch 3701, Loss 0.22348365187644958\n",
      "[Training Epoch 8] Batch 3702, Loss 0.2711665630340576\n",
      "[Training Epoch 8] Batch 3703, Loss 0.256855845451355\n",
      "[Training Epoch 8] Batch 3704, Loss 0.2536768317222595\n",
      "[Training Epoch 8] Batch 3705, Loss 0.24213607609272003\n",
      "[Training Epoch 8] Batch 3706, Loss 0.25175169110298157\n",
      "[Training Epoch 8] Batch 3707, Loss 0.25432541966438293\n",
      "[Training Epoch 8] Batch 3708, Loss 0.2487965226173401\n",
      "[Training Epoch 8] Batch 3709, Loss 0.24427922070026398\n",
      "[Training Epoch 8] Batch 3710, Loss 0.2634314000606537\n",
      "[Training Epoch 8] Batch 3711, Loss 0.23489931225776672\n",
      "[Training Epoch 8] Batch 3712, Loss 0.27047160267829895\n",
      "[Training Epoch 8] Batch 3713, Loss 0.24327926337718964\n",
      "[Training Epoch 8] Batch 3714, Loss 0.208977609872818\n",
      "[Training Epoch 8] Batch 3715, Loss 0.2907884120941162\n",
      "[Training Epoch 8] Batch 3716, Loss 0.2533166706562042\n",
      "[Training Epoch 8] Batch 3717, Loss 0.21933399140834808\n",
      "[Training Epoch 8] Batch 3718, Loss 0.22283977270126343\n",
      "[Training Epoch 8] Batch 3719, Loss 0.241411030292511\n",
      "[Training Epoch 8] Batch 3720, Loss 0.2539284825325012\n",
      "[Training Epoch 8] Batch 3721, Loss 0.23628129065036774\n",
      "[Training Epoch 8] Batch 3722, Loss 0.27935609221458435\n",
      "[Training Epoch 8] Batch 3723, Loss 0.24946463108062744\n",
      "[Training Epoch 8] Batch 3724, Loss 0.26770663261413574\n",
      "[Training Epoch 8] Batch 3725, Loss 0.2635248601436615\n",
      "[Training Epoch 8] Batch 3726, Loss 0.2568569779396057\n",
      "[Training Epoch 8] Batch 3727, Loss 0.2664330005645752\n",
      "[Training Epoch 8] Batch 3728, Loss 0.27915772795677185\n",
      "[Training Epoch 8] Batch 3729, Loss 0.2543368935585022\n",
      "[Training Epoch 8] Batch 3730, Loss 0.23815611004829407\n",
      "[Training Epoch 8] Batch 3731, Loss 0.27060168981552124\n",
      "[Training Epoch 8] Batch 3732, Loss 0.2771555781364441\n",
      "[Training Epoch 8] Batch 3733, Loss 0.25556692481040955\n",
      "[Training Epoch 8] Batch 3734, Loss 0.23387670516967773\n",
      "[Training Epoch 8] Batch 3735, Loss 0.28728538751602173\n",
      "[Training Epoch 8] Batch 3736, Loss 0.2691017985343933\n",
      "[Training Epoch 8] Batch 3737, Loss 0.2791128158569336\n",
      "[Training Epoch 8] Batch 3738, Loss 0.2668765187263489\n",
      "[Training Epoch 8] Batch 3739, Loss 0.2594406306743622\n",
      "[Training Epoch 8] Batch 3740, Loss 0.24802881479263306\n",
      "[Training Epoch 8] Batch 3741, Loss 0.2426767200231552\n",
      "[Training Epoch 8] Batch 3742, Loss 0.26126760244369507\n",
      "[Training Epoch 8] Batch 3743, Loss 0.24079272150993347\n",
      "[Training Epoch 8] Batch 3744, Loss 0.2613334059715271\n",
      "[Training Epoch 8] Batch 3745, Loss 0.24232915043830872\n",
      "[Training Epoch 8] Batch 3746, Loss 0.2720949351787567\n",
      "[Training Epoch 8] Batch 3747, Loss 0.24334149062633514\n",
      "[Training Epoch 8] Batch 3748, Loss 0.21572628617286682\n",
      "[Training Epoch 8] Batch 3749, Loss 0.23141197860240936\n",
      "[Training Epoch 8] Batch 3750, Loss 0.25533005595207214\n",
      "[Training Epoch 8] Batch 3751, Loss 0.25560423731803894\n",
      "[Training Epoch 8] Batch 3752, Loss 0.22850029170513153\n",
      "[Training Epoch 8] Batch 3753, Loss 0.2174396812915802\n",
      "[Training Epoch 8] Batch 3754, Loss 0.25887587666511536\n",
      "[Training Epoch 8] Batch 3755, Loss 0.2462862730026245\n",
      "[Training Epoch 8] Batch 3756, Loss 0.26279115676879883\n",
      "[Training Epoch 8] Batch 3757, Loss 0.2607153654098511\n",
      "[Training Epoch 8] Batch 3758, Loss 0.2790370285511017\n",
      "[Training Epoch 8] Batch 3759, Loss 0.24251285195350647\n",
      "[Training Epoch 8] Batch 3760, Loss 0.2722588777542114\n",
      "[Training Epoch 8] Batch 3761, Loss 0.2584156394004822\n",
      "[Training Epoch 8] Batch 3762, Loss 0.24616965651512146\n",
      "[Training Epoch 8] Batch 3763, Loss 0.22983169555664062\n",
      "[Training Epoch 8] Batch 3764, Loss 0.2612966299057007\n",
      "[Training Epoch 8] Batch 3765, Loss 0.2761727571487427\n",
      "[Training Epoch 8] Batch 3766, Loss 0.2854427695274353\n",
      "[Training Epoch 8] Batch 3767, Loss 0.2537878155708313\n",
      "[Training Epoch 8] Batch 3768, Loss 0.26970624923706055\n",
      "[Training Epoch 8] Batch 3769, Loss 0.2331995666027069\n",
      "[Training Epoch 8] Batch 3770, Loss 0.2602654993534088\n",
      "[Training Epoch 8] Batch 3771, Loss 0.24445627629756927\n",
      "[Training Epoch 8] Batch 3772, Loss 0.25867635011672974\n",
      "[Training Epoch 8] Batch 3773, Loss 0.28258347511291504\n",
      "[Training Epoch 8] Batch 3774, Loss 0.2516705393791199\n",
      "[Training Epoch 8] Batch 3775, Loss 0.25312143564224243\n",
      "[Training Epoch 8] Batch 3776, Loss 0.2938085198402405\n",
      "[Training Epoch 8] Batch 3777, Loss 0.27875638008117676\n",
      "[Training Epoch 8] Batch 3778, Loss 0.23762208223342896\n",
      "[Training Epoch 8] Batch 3779, Loss 0.2587878704071045\n",
      "[Training Epoch 8] Batch 3780, Loss 0.25546711683273315\n",
      "[Training Epoch 8] Batch 3781, Loss 0.25441810488700867\n",
      "[Training Epoch 8] Batch 3782, Loss 0.25226813554763794\n",
      "[Training Epoch 8] Batch 3783, Loss 0.2659814953804016\n",
      "[Training Epoch 8] Batch 3784, Loss 0.2640315294265747\n",
      "[Training Epoch 8] Batch 3785, Loss 0.28075459599494934\n",
      "[Training Epoch 8] Batch 3786, Loss 0.26777103543281555\n",
      "[Training Epoch 8] Batch 3787, Loss 0.23587733507156372\n",
      "[Training Epoch 8] Batch 3788, Loss 0.2757621109485626\n",
      "[Training Epoch 8] Batch 3789, Loss 0.2672992944717407\n",
      "[Training Epoch 8] Batch 3790, Loss 0.2526184022426605\n",
      "[Training Epoch 8] Batch 3791, Loss 0.23755979537963867\n",
      "[Training Epoch 8] Batch 3792, Loss 0.24663634598255157\n",
      "[Training Epoch 8] Batch 3793, Loss 0.25049707293510437\n",
      "[Training Epoch 8] Batch 3794, Loss 0.26874494552612305\n",
      "[Training Epoch 8] Batch 3795, Loss 0.3071099519729614\n",
      "[Training Epoch 8] Batch 3796, Loss 0.24280178546905518\n",
      "[Training Epoch 8] Batch 3797, Loss 0.2520008087158203\n",
      "[Training Epoch 8] Batch 3798, Loss 0.2286771535873413\n",
      "[Training Epoch 8] Batch 3799, Loss 0.2914808988571167\n",
      "[Training Epoch 8] Batch 3800, Loss 0.24746792018413544\n",
      "[Training Epoch 8] Batch 3801, Loss 0.22862377762794495\n",
      "[Training Epoch 8] Batch 3802, Loss 0.2834344804286957\n",
      "[Training Epoch 8] Batch 3803, Loss 0.2647942006587982\n",
      "[Training Epoch 8] Batch 3804, Loss 0.2927856743335724\n",
      "[Training Epoch 8] Batch 3805, Loss 0.26791155338287354\n",
      "[Training Epoch 8] Batch 3806, Loss 0.24254970252513885\n",
      "[Training Epoch 8] Batch 3807, Loss 0.2659069299697876\n",
      "[Training Epoch 8] Batch 3808, Loss 0.23459261655807495\n",
      "[Training Epoch 8] Batch 3809, Loss 0.24943961203098297\n",
      "[Training Epoch 8] Batch 3810, Loss 0.28192758560180664\n",
      "[Training Epoch 8] Batch 3811, Loss 0.3026962876319885\n",
      "[Training Epoch 8] Batch 3812, Loss 0.24595291912555695\n",
      "[Training Epoch 8] Batch 3813, Loss 0.23234346508979797\n",
      "[Training Epoch 8] Batch 3814, Loss 0.25625312328338623\n",
      "[Training Epoch 8] Batch 3815, Loss 0.22052524983882904\n",
      "[Training Epoch 8] Batch 3816, Loss 0.2664347290992737\n",
      "[Training Epoch 8] Batch 3817, Loss 0.2576178014278412\n",
      "[Training Epoch 8] Batch 3818, Loss 0.22956424951553345\n",
      "[Training Epoch 8] Batch 3819, Loss 0.27189716696739197\n",
      "[Training Epoch 8] Batch 3820, Loss 0.2254912108182907\n",
      "[Training Epoch 8] Batch 3821, Loss 0.26725560426712036\n",
      "[Training Epoch 8] Batch 3822, Loss 0.2714044153690338\n",
      "[Training Epoch 8] Batch 3823, Loss 0.2541518807411194\n",
      "[Training Epoch 8] Batch 3824, Loss 0.2650870680809021\n",
      "[Training Epoch 8] Batch 3825, Loss 0.2864810824394226\n",
      "[Training Epoch 8] Batch 3826, Loss 0.27572867274284363\n",
      "[Training Epoch 8] Batch 3827, Loss 0.2647934854030609\n",
      "[Training Epoch 8] Batch 3828, Loss 0.22939956188201904\n",
      "[Training Epoch 8] Batch 3829, Loss 0.26151755452156067\n",
      "[Training Epoch 8] Batch 3830, Loss 0.24336828291416168\n",
      "[Training Epoch 8] Batch 3831, Loss 0.26771414279937744\n",
      "[Training Epoch 8] Batch 3832, Loss 0.2426021844148636\n",
      "[Training Epoch 8] Batch 3833, Loss 0.2778075337409973\n",
      "[Training Epoch 8] Batch 3834, Loss 0.24684129655361176\n",
      "[Training Epoch 8] Batch 3835, Loss 0.2491758167743683\n",
      "[Training Epoch 8] Batch 3836, Loss 0.29220467805862427\n",
      "[Training Epoch 8] Batch 3837, Loss 0.27629634737968445\n",
      "[Training Epoch 8] Batch 3838, Loss 0.27887922525405884\n",
      "[Training Epoch 8] Batch 3839, Loss 0.2506692409515381\n",
      "[Training Epoch 8] Batch 3840, Loss 0.2737862765789032\n",
      "[Training Epoch 8] Batch 3841, Loss 0.26881811022758484\n",
      "[Training Epoch 8] Batch 3842, Loss 0.2839477062225342\n",
      "[Training Epoch 8] Batch 3843, Loss 0.2538272738456726\n",
      "[Training Epoch 8] Batch 3844, Loss 0.2509152889251709\n",
      "[Training Epoch 8] Batch 3845, Loss 0.26514631509780884\n",
      "[Training Epoch 8] Batch 3846, Loss 0.25224238634109497\n",
      "[Training Epoch 8] Batch 3847, Loss 0.2573171854019165\n",
      "[Training Epoch 8] Batch 3848, Loss 0.25482141971588135\n",
      "[Training Epoch 8] Batch 3849, Loss 0.27363842725753784\n",
      "[Training Epoch 8] Batch 3850, Loss 0.24140730500221252\n",
      "[Training Epoch 8] Batch 3851, Loss 0.26477810740470886\n",
      "[Training Epoch 8] Batch 3852, Loss 0.23848922550678253\n",
      "[Training Epoch 8] Batch 3853, Loss 0.2553520202636719\n",
      "[Training Epoch 8] Batch 3854, Loss 0.24396595358848572\n",
      "[Training Epoch 8] Batch 3855, Loss 0.24255189299583435\n",
      "[Training Epoch 8] Batch 3856, Loss 0.2533149719238281\n",
      "[Training Epoch 8] Batch 3857, Loss 0.25299423933029175\n",
      "[Training Epoch 8] Batch 3858, Loss 0.2565958499908447\n",
      "[Training Epoch 8] Batch 3859, Loss 0.2697557508945465\n",
      "[Training Epoch 8] Batch 3860, Loss 0.2723228931427002\n",
      "[Training Epoch 8] Batch 3861, Loss 0.2409239411354065\n",
      "[Training Epoch 8] Batch 3862, Loss 0.27798885107040405\n",
      "[Training Epoch 8] Batch 3863, Loss 0.24653078615665436\n",
      "[Training Epoch 8] Batch 3864, Loss 0.24925100803375244\n",
      "[Training Epoch 8] Batch 3865, Loss 0.27491751313209534\n",
      "[Training Epoch 8] Batch 3866, Loss 0.2438974678516388\n",
      "[Training Epoch 8] Batch 3867, Loss 0.25759992003440857\n",
      "[Training Epoch 8] Batch 3868, Loss 0.25087738037109375\n",
      "[Training Epoch 8] Batch 3869, Loss 0.25750163197517395\n",
      "[Training Epoch 8] Batch 3870, Loss 0.23382872343063354\n",
      "[Training Epoch 8] Batch 3871, Loss 0.24641001224517822\n",
      "[Training Epoch 8] Batch 3872, Loss 0.24228337407112122\n",
      "[Training Epoch 8] Batch 3873, Loss 0.24440796673297882\n",
      "[Training Epoch 8] Batch 3874, Loss 0.2527048885822296\n",
      "[Training Epoch 8] Batch 3875, Loss 0.25053849816322327\n",
      "[Training Epoch 8] Batch 3876, Loss 0.25295326113700867\n",
      "[Training Epoch 8] Batch 3877, Loss 0.2551904022693634\n",
      "[Training Epoch 8] Batch 3878, Loss 0.26165658235549927\n",
      "[Training Epoch 8] Batch 3879, Loss 0.21861925721168518\n",
      "[Training Epoch 8] Batch 3880, Loss 0.27237844467163086\n",
      "[Training Epoch 8] Batch 3881, Loss 0.2836627960205078\n",
      "[Training Epoch 8] Batch 3882, Loss 0.2948843836784363\n",
      "[Training Epoch 8] Batch 3883, Loss 0.24226438999176025\n",
      "[Training Epoch 8] Batch 3884, Loss 0.2708527743816376\n",
      "[Training Epoch 8] Batch 3885, Loss 0.265400767326355\n",
      "[Training Epoch 8] Batch 3886, Loss 0.24510923027992249\n",
      "[Training Epoch 8] Batch 3887, Loss 0.27224189043045044\n",
      "[Training Epoch 8] Batch 3888, Loss 0.27253323793411255\n",
      "[Training Epoch 8] Batch 3889, Loss 0.22975698113441467\n",
      "[Training Epoch 8] Batch 3890, Loss 0.2646133303642273\n",
      "[Training Epoch 8] Batch 3891, Loss 0.249817356467247\n",
      "[Training Epoch 8] Batch 3892, Loss 0.25502321124076843\n",
      "[Training Epoch 8] Batch 3893, Loss 0.24924075603485107\n",
      "[Training Epoch 8] Batch 3894, Loss 0.2795395255088806\n",
      "[Training Epoch 8] Batch 3895, Loss 0.2539616823196411\n",
      "[Training Epoch 8] Batch 3896, Loss 0.24795988202095032\n",
      "[Training Epoch 8] Batch 3897, Loss 0.253595769405365\n",
      "[Training Epoch 8] Batch 3898, Loss 0.24428993463516235\n",
      "[Training Epoch 8] Batch 3899, Loss 0.268655002117157\n",
      "[Training Epoch 8] Batch 3900, Loss 0.2718418836593628\n",
      "[Training Epoch 8] Batch 3901, Loss 0.24497491121292114\n",
      "[Training Epoch 8] Batch 3902, Loss 0.2522544264793396\n",
      "[Training Epoch 8] Batch 3903, Loss 0.25384941697120667\n",
      "[Training Epoch 8] Batch 3904, Loss 0.28911346197128296\n",
      "[Training Epoch 8] Batch 3905, Loss 0.2550807595252991\n",
      "[Training Epoch 8] Batch 3906, Loss 0.26745447516441345\n",
      "[Training Epoch 8] Batch 3907, Loss 0.26622700691223145\n",
      "[Training Epoch 8] Batch 3908, Loss 0.268174409866333\n",
      "[Training Epoch 8] Batch 3909, Loss 0.24826538562774658\n",
      "[Training Epoch 8] Batch 3910, Loss 0.26822513341903687\n",
      "[Training Epoch 8] Batch 3911, Loss 0.24482852220535278\n",
      "[Training Epoch 8] Batch 3912, Loss 0.25690019130706787\n",
      "[Training Epoch 8] Batch 3913, Loss 0.25071462988853455\n",
      "[Training Epoch 8] Batch 3914, Loss 0.2358098030090332\n",
      "[Training Epoch 8] Batch 3915, Loss 0.25786685943603516\n",
      "[Training Epoch 8] Batch 3916, Loss 0.28698477149009705\n",
      "[Training Epoch 8] Batch 3917, Loss 0.24870339035987854\n",
      "[Training Epoch 8] Batch 3918, Loss 0.2785142958164215\n",
      "[Training Epoch 8] Batch 3919, Loss 0.26962578296661377\n",
      "[Training Epoch 8] Batch 3920, Loss 0.26680582761764526\n",
      "[Training Epoch 8] Batch 3921, Loss 0.2622579336166382\n",
      "[Training Epoch 8] Batch 3922, Loss 0.2546203136444092\n",
      "[Training Epoch 8] Batch 3923, Loss 0.24866461753845215\n",
      "[Training Epoch 8] Batch 3924, Loss 0.23379483819007874\n",
      "[Training Epoch 8] Batch 3925, Loss 0.2588052451610565\n",
      "[Training Epoch 8] Batch 3926, Loss 0.2441868931055069\n",
      "[Training Epoch 8] Batch 3927, Loss 0.22738951444625854\n",
      "[Training Epoch 8] Batch 3928, Loss 0.25616586208343506\n",
      "[Training Epoch 8] Batch 3929, Loss 0.23996862769126892\n",
      "[Training Epoch 8] Batch 3930, Loss 0.24406760931015015\n",
      "[Training Epoch 8] Batch 3931, Loss 0.2734118402004242\n",
      "[Training Epoch 8] Batch 3932, Loss 0.2596411108970642\n",
      "[Training Epoch 8] Batch 3933, Loss 0.2513187825679779\n",
      "[Training Epoch 8] Batch 3934, Loss 0.2620495557785034\n",
      "[Training Epoch 8] Batch 3935, Loss 0.2529175877571106\n",
      "[Training Epoch 8] Batch 3936, Loss 0.230258047580719\n",
      "[Training Epoch 8] Batch 3937, Loss 0.24245011806488037\n",
      "[Training Epoch 8] Batch 3938, Loss 0.2585260272026062\n",
      "[Training Epoch 8] Batch 3939, Loss 0.2736383080482483\n",
      "[Training Epoch 8] Batch 3940, Loss 0.25229793787002563\n",
      "[Training Epoch 8] Batch 3941, Loss 0.25125399231910706\n",
      "[Training Epoch 8] Batch 3942, Loss 0.23681879043579102\n",
      "[Training Epoch 8] Batch 3943, Loss 0.24117279052734375\n",
      "[Training Epoch 8] Batch 3944, Loss 0.21394595503807068\n",
      "[Training Epoch 8] Batch 3945, Loss 0.2696192264556885\n",
      "[Training Epoch 8] Batch 3946, Loss 0.23976771533489227\n",
      "[Training Epoch 8] Batch 3947, Loss 0.22709017992019653\n",
      "[Training Epoch 8] Batch 3948, Loss 0.22703658044338226\n",
      "[Training Epoch 8] Batch 3949, Loss 0.23555423319339752\n",
      "[Training Epoch 8] Batch 3950, Loss 0.2471207082271576\n",
      "[Training Epoch 8] Batch 3951, Loss 0.24289357662200928\n",
      "[Training Epoch 8] Batch 3952, Loss 0.2438344955444336\n",
      "[Training Epoch 8] Batch 3953, Loss 0.2527264356613159\n",
      "[Training Epoch 8] Batch 3954, Loss 0.2790944278240204\n",
      "[Training Epoch 8] Batch 3955, Loss 0.2752750813961029\n",
      "[Training Epoch 8] Batch 3956, Loss 0.26486486196517944\n",
      "[Training Epoch 8] Batch 3957, Loss 0.24622368812561035\n",
      "[Training Epoch 8] Batch 3958, Loss 0.26412636041641235\n",
      "[Training Epoch 8] Batch 3959, Loss 0.2544175982475281\n",
      "[Training Epoch 8] Batch 3960, Loss 0.25133782625198364\n",
      "[Training Epoch 8] Batch 3961, Loss 0.2789041996002197\n",
      "[Training Epoch 8] Batch 3962, Loss 0.25722241401672363\n",
      "[Training Epoch 8] Batch 3963, Loss 0.24739420413970947\n",
      "[Training Epoch 8] Batch 3964, Loss 0.2343519628047943\n",
      "[Training Epoch 8] Batch 3965, Loss 0.26361095905303955\n",
      "[Training Epoch 8] Batch 3966, Loss 0.2533521354198456\n",
      "[Training Epoch 8] Batch 3967, Loss 0.2650328278541565\n",
      "[Training Epoch 8] Batch 3968, Loss 0.27553877234458923\n",
      "[Training Epoch 8] Batch 3969, Loss 0.25247591733932495\n",
      "[Training Epoch 8] Batch 3970, Loss 0.25527670979499817\n",
      "[Training Epoch 8] Batch 3971, Loss 0.2489696443080902\n",
      "[Training Epoch 8] Batch 3972, Loss 0.24662239849567413\n",
      "[Training Epoch 8] Batch 3973, Loss 0.2687442898750305\n",
      "[Training Epoch 8] Batch 3974, Loss 0.25968825817108154\n",
      "[Training Epoch 8] Batch 3975, Loss 0.2489345371723175\n",
      "[Training Epoch 8] Batch 3976, Loss 0.2727692127227783\n",
      "[Training Epoch 8] Batch 3977, Loss 0.23255929350852966\n",
      "[Training Epoch 8] Batch 3978, Loss 0.26982206106185913\n",
      "[Training Epoch 8] Batch 3979, Loss 0.24530483782291412\n",
      "[Training Epoch 8] Batch 3980, Loss 0.25504833459854126\n",
      "[Training Epoch 8] Batch 3981, Loss 0.27883780002593994\n",
      "[Training Epoch 8] Batch 3982, Loss 0.24237093329429626\n",
      "[Training Epoch 8] Batch 3983, Loss 0.24681633710861206\n",
      "[Training Epoch 8] Batch 3984, Loss 0.24792394042015076\n",
      "[Training Epoch 8] Batch 3985, Loss 0.21626915037631989\n",
      "[Training Epoch 8] Batch 3986, Loss 0.2524770498275757\n",
      "[Training Epoch 8] Batch 3987, Loss 0.2677517533302307\n",
      "[Training Epoch 8] Batch 3988, Loss 0.23410260677337646\n",
      "[Training Epoch 8] Batch 3989, Loss 0.27981284260749817\n",
      "[Training Epoch 8] Batch 3990, Loss 0.23495814204216003\n",
      "[Training Epoch 8] Batch 3991, Loss 0.2428676187992096\n",
      "[Training Epoch 8] Batch 3992, Loss 0.255743145942688\n",
      "[Training Epoch 8] Batch 3993, Loss 0.265378475189209\n",
      "[Training Epoch 8] Batch 3994, Loss 0.21465592086315155\n",
      "[Training Epoch 8] Batch 3995, Loss 0.27636396884918213\n",
      "[Training Epoch 8] Batch 3996, Loss 0.2699510455131531\n",
      "[Training Epoch 8] Batch 3997, Loss 0.25396066904067993\n",
      "[Training Epoch 8] Batch 3998, Loss 0.24254636466503143\n",
      "[Training Epoch 8] Batch 3999, Loss 0.26361367106437683\n",
      "[Training Epoch 8] Batch 4000, Loss 0.24298855662345886\n",
      "[Training Epoch 8] Batch 4001, Loss 0.2588016390800476\n",
      "[Training Epoch 8] Batch 4002, Loss 0.27027222514152527\n",
      "[Training Epoch 8] Batch 4003, Loss 0.2687690258026123\n",
      "[Training Epoch 8] Batch 4004, Loss 0.2542509436607361\n",
      "[Training Epoch 8] Batch 4005, Loss 0.27360114455223083\n",
      "[Training Epoch 8] Batch 4006, Loss 0.24298271536827087\n",
      "[Training Epoch 8] Batch 4007, Loss 0.2406531721353531\n",
      "[Training Epoch 8] Batch 4008, Loss 0.2568606734275818\n",
      "[Training Epoch 8] Batch 4009, Loss 0.27093857526779175\n",
      "[Training Epoch 8] Batch 4010, Loss 0.25628212094306946\n",
      "[Training Epoch 8] Batch 4011, Loss 0.2689465880393982\n",
      "[Training Epoch 8] Batch 4012, Loss 0.26440173387527466\n",
      "[Training Epoch 8] Batch 4013, Loss 0.2505429685115814\n",
      "[Training Epoch 8] Batch 4014, Loss 0.26861658692359924\n",
      "[Training Epoch 8] Batch 4015, Loss 0.2483549267053604\n",
      "[Training Epoch 8] Batch 4016, Loss 0.269535094499588\n",
      "[Training Epoch 8] Batch 4017, Loss 0.23915505409240723\n",
      "[Training Epoch 8] Batch 4018, Loss 0.259057879447937\n",
      "[Training Epoch 8] Batch 4019, Loss 0.2398476004600525\n",
      "[Training Epoch 8] Batch 4020, Loss 0.2335379719734192\n",
      "[Training Epoch 8] Batch 4021, Loss 0.2601616382598877\n",
      "[Training Epoch 8] Batch 4022, Loss 0.22529618442058563\n",
      "[Training Epoch 8] Batch 4023, Loss 0.2562818229198456\n",
      "[Training Epoch 8] Batch 4024, Loss 0.2528369426727295\n",
      "[Training Epoch 8] Batch 4025, Loss 0.26089295744895935\n",
      "[Training Epoch 8] Batch 4026, Loss 0.2364211529493332\n",
      "[Training Epoch 8] Batch 4027, Loss 0.2647210657596588\n",
      "[Training Epoch 8] Batch 4028, Loss 0.2465636134147644\n",
      "[Training Epoch 8] Batch 4029, Loss 0.2854539752006531\n",
      "[Training Epoch 8] Batch 4030, Loss 0.23381337523460388\n",
      "[Training Epoch 8] Batch 4031, Loss 0.24432514607906342\n",
      "[Training Epoch 8] Batch 4032, Loss 0.2846295237541199\n",
      "[Training Epoch 8] Batch 4033, Loss 0.23714211583137512\n",
      "[Training Epoch 8] Batch 4034, Loss 0.26579591631889343\n",
      "[Training Epoch 8] Batch 4035, Loss 0.2280939817428589\n",
      "[Training Epoch 8] Batch 4036, Loss 0.2695883810520172\n",
      "[Training Epoch 8] Batch 4037, Loss 0.2791481018066406\n",
      "[Training Epoch 8] Batch 4038, Loss 0.2596045136451721\n",
      "[Training Epoch 8] Batch 4039, Loss 0.25797581672668457\n",
      "[Training Epoch 8] Batch 4040, Loss 0.28666937351226807\n",
      "[Training Epoch 8] Batch 4041, Loss 0.24744421243667603\n",
      "[Training Epoch 8] Batch 4042, Loss 0.2537280321121216\n",
      "[Training Epoch 8] Batch 4043, Loss 0.28207361698150635\n",
      "[Training Epoch 8] Batch 4044, Loss 0.24347150325775146\n",
      "[Training Epoch 8] Batch 4045, Loss 0.27150654792785645\n",
      "[Training Epoch 8] Batch 4046, Loss 0.26217612624168396\n",
      "[Training Epoch 8] Batch 4047, Loss 0.271325945854187\n",
      "[Training Epoch 8] Batch 4048, Loss 0.25012996792793274\n",
      "[Training Epoch 8] Batch 4049, Loss 0.2679356336593628\n",
      "[Training Epoch 8] Batch 4050, Loss 0.2676234245300293\n",
      "[Training Epoch 8] Batch 4051, Loss 0.271598756313324\n",
      "[Training Epoch 8] Batch 4052, Loss 0.23286083340644836\n",
      "[Training Epoch 8] Batch 4053, Loss 0.24492119252681732\n",
      "[Training Epoch 8] Batch 4054, Loss 0.2685958445072174\n",
      "[Training Epoch 8] Batch 4055, Loss 0.25385379791259766\n",
      "[Training Epoch 8] Batch 4056, Loss 0.24909254908561707\n",
      "[Training Epoch 8] Batch 4057, Loss 0.25645503401756287\n",
      "[Training Epoch 8] Batch 4058, Loss 0.2601744830608368\n",
      "[Training Epoch 8] Batch 4059, Loss 0.2651060223579407\n",
      "[Training Epoch 8] Batch 4060, Loss 0.250549852848053\n",
      "[Training Epoch 8] Batch 4061, Loss 0.29997366666793823\n",
      "[Training Epoch 8] Batch 4062, Loss 0.24265629053115845\n",
      "[Training Epoch 8] Batch 4063, Loss 0.26350194215774536\n",
      "[Training Epoch 8] Batch 4064, Loss 0.24153932929039001\n",
      "[Training Epoch 8] Batch 4065, Loss 0.2577938437461853\n",
      "[Training Epoch 8] Batch 4066, Loss 0.26124894618988037\n",
      "[Training Epoch 8] Batch 4067, Loss 0.2373097836971283\n",
      "[Training Epoch 8] Batch 4068, Loss 0.23851336538791656\n",
      "[Training Epoch 8] Batch 4069, Loss 0.23670698702335358\n",
      "[Training Epoch 8] Batch 4070, Loss 0.2635813355445862\n",
      "[Training Epoch 8] Batch 4071, Loss 0.27102208137512207\n",
      "[Training Epoch 8] Batch 4072, Loss 0.2586039900779724\n",
      "[Training Epoch 8] Batch 4073, Loss 0.26790857315063477\n",
      "[Training Epoch 8] Batch 4074, Loss 0.278465211391449\n",
      "[Training Epoch 8] Batch 4075, Loss 0.2492726445198059\n",
      "[Training Epoch 8] Batch 4076, Loss 0.24146172404289246\n",
      "[Training Epoch 8] Batch 4077, Loss 0.2717331051826477\n",
      "[Training Epoch 8] Batch 4078, Loss 0.2999451160430908\n",
      "[Training Epoch 8] Batch 4079, Loss 0.28313153982162476\n",
      "[Training Epoch 8] Batch 4080, Loss 0.26555925607681274\n",
      "[Training Epoch 8] Batch 4081, Loss 0.26801222562789917\n",
      "[Training Epoch 8] Batch 4082, Loss 0.24042649567127228\n",
      "[Training Epoch 8] Batch 4083, Loss 0.23794995248317719\n",
      "[Training Epoch 8] Batch 4084, Loss 0.2687855660915375\n",
      "[Training Epoch 8] Batch 4085, Loss 0.2563663125038147\n",
      "[Training Epoch 8] Batch 4086, Loss 0.2658851146697998\n",
      "[Training Epoch 8] Batch 4087, Loss 0.2508348822593689\n",
      "[Training Epoch 8] Batch 4088, Loss 0.24413996934890747\n",
      "[Training Epoch 8] Batch 4089, Loss 0.281362384557724\n",
      "[Training Epoch 8] Batch 4090, Loss 0.2631300687789917\n",
      "[Training Epoch 8] Batch 4091, Loss 0.2386128455400467\n",
      "[Training Epoch 8] Batch 4092, Loss 0.28152668476104736\n",
      "[Training Epoch 8] Batch 4093, Loss 0.23875246942043304\n",
      "[Training Epoch 8] Batch 4094, Loss 0.2532646954059601\n",
      "[Training Epoch 8] Batch 4095, Loss 0.24944689869880676\n",
      "[Training Epoch 8] Batch 4096, Loss 0.23015685379505157\n",
      "[Training Epoch 8] Batch 4097, Loss 0.23610541224479675\n",
      "[Training Epoch 8] Batch 4098, Loss 0.2775121033191681\n",
      "[Training Epoch 8] Batch 4099, Loss 0.23244640231132507\n",
      "[Training Epoch 8] Batch 4100, Loss 0.23827943205833435\n",
      "[Training Epoch 8] Batch 4101, Loss 0.2569819986820221\n",
      "[Training Epoch 8] Batch 4102, Loss 0.2629319727420807\n",
      "[Training Epoch 8] Batch 4103, Loss 0.2701669931411743\n",
      "[Training Epoch 8] Batch 4104, Loss 0.2697753310203552\n",
      "[Training Epoch 8] Batch 4105, Loss 0.21349823474884033\n",
      "[Training Epoch 8] Batch 4106, Loss 0.2439488023519516\n",
      "[Training Epoch 8] Batch 4107, Loss 0.23427346348762512\n",
      "[Training Epoch 8] Batch 4108, Loss 0.23504984378814697\n",
      "[Training Epoch 8] Batch 4109, Loss 0.24909065663814545\n",
      "[Training Epoch 8] Batch 4110, Loss 0.24355371296405792\n",
      "[Training Epoch 8] Batch 4111, Loss 0.22531723976135254\n",
      "[Training Epoch 8] Batch 4112, Loss 0.2478460967540741\n",
      "[Training Epoch 8] Batch 4113, Loss 0.25928133726119995\n",
      "[Training Epoch 8] Batch 4114, Loss 0.22693681716918945\n",
      "[Training Epoch 8] Batch 4115, Loss 0.2492038607597351\n",
      "[Training Epoch 8] Batch 4116, Loss 0.2567417621612549\n",
      "[Training Epoch 8] Batch 4117, Loss 0.2642042636871338\n",
      "[Training Epoch 8] Batch 4118, Loss 0.2599147856235504\n",
      "[Training Epoch 8] Batch 4119, Loss 0.24400317668914795\n",
      "[Training Epoch 8] Batch 4120, Loss 0.2727717161178589\n",
      "[Training Epoch 8] Batch 4121, Loss 0.29738372564315796\n",
      "[Training Epoch 8] Batch 4122, Loss 0.2303682416677475\n",
      "[Training Epoch 8] Batch 4123, Loss 0.2572914958000183\n",
      "[Training Epoch 8] Batch 4124, Loss 0.2715109586715698\n",
      "[Training Epoch 8] Batch 4125, Loss 0.26155465841293335\n",
      "[Training Epoch 8] Batch 4126, Loss 0.2694687247276306\n",
      "[Training Epoch 8] Batch 4127, Loss 0.2632830739021301\n",
      "[Training Epoch 8] Batch 4128, Loss 0.2386644184589386\n",
      "[Training Epoch 8] Batch 4129, Loss 0.27279695868492126\n",
      "[Training Epoch 8] Batch 4130, Loss 0.2610047459602356\n",
      "[Training Epoch 8] Batch 4131, Loss 0.23714108765125275\n",
      "[Training Epoch 8] Batch 4132, Loss 0.23674041032791138\n",
      "[Training Epoch 8] Batch 4133, Loss 0.23963861167430878\n",
      "[Training Epoch 8] Batch 4134, Loss 0.2426873743534088\n",
      "[Training Epoch 8] Batch 4135, Loss 0.24681530892848969\n",
      "[Training Epoch 8] Batch 4136, Loss 0.28048980236053467\n",
      "[Training Epoch 8] Batch 4137, Loss 0.24161817133426666\n",
      "[Training Epoch 8] Batch 4138, Loss 0.2665436267852783\n",
      "[Training Epoch 8] Batch 4139, Loss 0.26020166277885437\n",
      "[Training Epoch 8] Batch 4140, Loss 0.26823872327804565\n",
      "[Training Epoch 8] Batch 4141, Loss 0.2700093388557434\n",
      "[Training Epoch 8] Batch 4142, Loss 0.2587432861328125\n",
      "[Training Epoch 8] Batch 4143, Loss 0.2517106831073761\n",
      "[Training Epoch 8] Batch 4144, Loss 0.22490881383419037\n",
      "[Training Epoch 8] Batch 4145, Loss 0.2815271019935608\n",
      "[Training Epoch 8] Batch 4146, Loss 0.27060818672180176\n",
      "[Training Epoch 8] Batch 4147, Loss 0.25333136320114136\n",
      "[Training Epoch 8] Batch 4148, Loss 0.2580452859401703\n",
      "[Training Epoch 8] Batch 4149, Loss 0.27074646949768066\n",
      "[Training Epoch 8] Batch 4150, Loss 0.2380644977092743\n",
      "[Training Epoch 8] Batch 4151, Loss 0.2572387754917145\n",
      "[Training Epoch 8] Batch 4152, Loss 0.2673557996749878\n",
      "[Training Epoch 8] Batch 4153, Loss 0.24738988280296326\n",
      "[Training Epoch 8] Batch 4154, Loss 0.2832210659980774\n",
      "[Training Epoch 8] Batch 4155, Loss 0.2559323310852051\n",
      "[Training Epoch 8] Batch 4156, Loss 0.2510852813720703\n",
      "[Training Epoch 8] Batch 4157, Loss 0.28156501054763794\n",
      "[Training Epoch 8] Batch 4158, Loss 0.26001161336898804\n",
      "[Training Epoch 8] Batch 4159, Loss 0.2384326159954071\n",
      "[Training Epoch 8] Batch 4160, Loss 0.22722166776657104\n",
      "[Training Epoch 8] Batch 4161, Loss 0.2548595666885376\n",
      "[Training Epoch 8] Batch 4162, Loss 0.23390603065490723\n",
      "[Training Epoch 8] Batch 4163, Loss 0.24863123893737793\n",
      "[Training Epoch 8] Batch 4164, Loss 0.2156858742237091\n",
      "[Training Epoch 8] Batch 4165, Loss 0.24919018149375916\n",
      "[Training Epoch 8] Batch 4166, Loss 0.2714596390724182\n",
      "[Training Epoch 8] Batch 4167, Loss 0.24703943729400635\n",
      "[Training Epoch 8] Batch 4168, Loss 0.2616814076900482\n",
      "[Training Epoch 8] Batch 4169, Loss 0.23920002579689026\n",
      "[Training Epoch 8] Batch 4170, Loss 0.25660693645477295\n",
      "[Training Epoch 8] Batch 4171, Loss 0.2669137120246887\n",
      "[Training Epoch 8] Batch 4172, Loss 0.2393205761909485\n",
      "[Training Epoch 8] Batch 4173, Loss 0.25065624713897705\n",
      "[Training Epoch 8] Batch 4174, Loss 0.2534002661705017\n",
      "[Training Epoch 8] Batch 4175, Loss 0.2350596934556961\n",
      "[Training Epoch 8] Batch 4176, Loss 0.26310163736343384\n",
      "[Training Epoch 8] Batch 4177, Loss 0.23071151971817017\n",
      "[Training Epoch 8] Batch 4178, Loss 0.26456472277641296\n",
      "[Training Epoch 8] Batch 4179, Loss 0.26570454239845276\n",
      "[Training Epoch 8] Batch 4180, Loss 0.2407989799976349\n",
      "[Training Epoch 8] Batch 4181, Loss 0.2846946716308594\n",
      "[Training Epoch 8] Batch 4182, Loss 0.27036094665527344\n",
      "[Training Epoch 8] Batch 4183, Loss 0.24987848103046417\n",
      "[Training Epoch 8] Batch 4184, Loss 0.25828635692596436\n",
      "[Training Epoch 8] Batch 4185, Loss 0.24265508353710175\n",
      "[Training Epoch 8] Batch 4186, Loss 0.23549428582191467\n",
      "[Training Epoch 8] Batch 4187, Loss 0.2520628571510315\n",
      "[Training Epoch 8] Batch 4188, Loss 0.28309452533721924\n",
      "[Training Epoch 8] Batch 4189, Loss 0.269890159368515\n",
      "[Training Epoch 8] Batch 4190, Loss 0.2634134292602539\n",
      "[Training Epoch 8] Batch 4191, Loss 0.2621915936470032\n",
      "[Training Epoch 8] Batch 4192, Loss 0.23661474883556366\n",
      "[Training Epoch 8] Batch 4193, Loss 0.21119040250778198\n",
      "[Training Epoch 8] Batch 4194, Loss 0.2611820697784424\n",
      "[Training Epoch 8] Batch 4195, Loss 0.23128986358642578\n",
      "[Training Epoch 8] Batch 4196, Loss 0.28050899505615234\n",
      "[Training Epoch 8] Batch 4197, Loss 0.27822113037109375\n",
      "[Training Epoch 8] Batch 4198, Loss 0.25520992279052734\n",
      "[Training Epoch 8] Batch 4199, Loss 0.2588089108467102\n",
      "[Training Epoch 8] Batch 4200, Loss 0.245382621884346\n",
      "[Training Epoch 8] Batch 4201, Loss 0.24908563494682312\n",
      "[Training Epoch 8] Batch 4202, Loss 0.2877991497516632\n",
      "[Training Epoch 8] Batch 4203, Loss 0.25816184282302856\n",
      "[Training Epoch 8] Batch 4204, Loss 0.26093244552612305\n",
      "[Training Epoch 8] Batch 4205, Loss 0.26723790168762207\n",
      "[Training Epoch 8] Batch 4206, Loss 0.27745985984802246\n",
      "[Training Epoch 8] Batch 4207, Loss 0.2685254216194153\n",
      "[Training Epoch 8] Batch 4208, Loss 0.2842598557472229\n",
      "[Training Epoch 8] Batch 4209, Loss 0.2737725079059601\n",
      "[Training Epoch 8] Batch 4210, Loss 0.2584560513496399\n",
      "[Training Epoch 8] Batch 4211, Loss 0.25365182757377625\n",
      "[Training Epoch 8] Batch 4212, Loss 0.22871942818164825\n",
      "[Training Epoch 8] Batch 4213, Loss 0.2516770362854004\n",
      "[Training Epoch 8] Batch 4214, Loss 0.2536521553993225\n",
      "[Training Epoch 8] Batch 4215, Loss 0.23331785202026367\n",
      "[Training Epoch 8] Batch 4216, Loss 0.2620396018028259\n",
      "[Training Epoch 8] Batch 4217, Loss 0.24818429350852966\n",
      "[Training Epoch 8] Batch 4218, Loss 0.26152661442756653\n",
      "[Training Epoch 8] Batch 4219, Loss 0.2665673792362213\n",
      "[Training Epoch 8] Batch 4220, Loss 0.2500905394554138\n",
      "[Training Epoch 8] Batch 4221, Loss 0.2763337194919586\n",
      "[Training Epoch 8] Batch 4222, Loss 0.2612457573413849\n",
      "[Training Epoch 8] Batch 4223, Loss 0.2331905961036682\n",
      "[Training Epoch 8] Batch 4224, Loss 0.2459595501422882\n",
      "[Training Epoch 8] Batch 4225, Loss 0.26359665393829346\n",
      "[Training Epoch 8] Batch 4226, Loss 0.26721012592315674\n",
      "[Training Epoch 8] Batch 4227, Loss 0.281233549118042\n",
      "[Training Epoch 8] Batch 4228, Loss 0.2664433717727661\n",
      "[Training Epoch 8] Batch 4229, Loss 0.28942781686782837\n",
      "[Training Epoch 8] Batch 4230, Loss 0.2461138814687729\n",
      "[Training Epoch 8] Batch 4231, Loss 0.25174635648727417\n",
      "[Training Epoch 8] Batch 4232, Loss 0.2277117669582367\n",
      "[Training Epoch 8] Batch 4233, Loss 0.2813892960548401\n",
      "[Training Epoch 8] Batch 4234, Loss 0.265915185213089\n",
      "[Training Epoch 8] Batch 4235, Loss 0.26147785782814026\n",
      "[Training Epoch 8] Batch 4236, Loss 0.2637344002723694\n",
      "[Training Epoch 8] Batch 4237, Loss 0.23174160718917847\n",
      "[Training Epoch 8] Batch 4238, Loss 0.24417611956596375\n",
      "[Training Epoch 8] Batch 4239, Loss 0.2672264277935028\n",
      "[Training Epoch 8] Batch 4240, Loss 0.21011708676815033\n",
      "[Training Epoch 8] Batch 4241, Loss 0.28126060962677\n",
      "[Training Epoch 8] Batch 4242, Loss 0.2351675033569336\n",
      "[Training Epoch 8] Batch 4243, Loss 0.24172773957252502\n",
      "[Training Epoch 8] Batch 4244, Loss 0.25587671995162964\n",
      "[Training Epoch 8] Batch 4245, Loss 0.2258901298046112\n",
      "[Training Epoch 8] Batch 4246, Loss 0.2527700960636139\n",
      "[Training Epoch 8] Batch 4247, Loss 0.25271689891815186\n",
      "[Training Epoch 8] Batch 4248, Loss 0.27193018794059753\n",
      "[Training Epoch 8] Batch 4249, Loss 0.23178310692310333\n",
      "[Training Epoch 8] Batch 4250, Loss 0.2325935959815979\n",
      "[Training Epoch 8] Batch 4251, Loss 0.231521338224411\n",
      "[Training Epoch 8] Batch 4252, Loss 0.2275163233280182\n",
      "[Training Epoch 8] Batch 4253, Loss 0.2731577157974243\n",
      "[Training Epoch 8] Batch 4254, Loss 0.2460561841726303\n",
      "[Training Epoch 8] Batch 4255, Loss 0.25075310468673706\n",
      "[Training Epoch 8] Batch 4256, Loss 0.25761377811431885\n",
      "[Training Epoch 8] Batch 4257, Loss 0.3027854561805725\n",
      "[Training Epoch 8] Batch 4258, Loss 0.2326534390449524\n",
      "[Training Epoch 8] Batch 4259, Loss 0.24623118340969086\n",
      "[Training Epoch 8] Batch 4260, Loss 0.23813776671886444\n",
      "[Training Epoch 8] Batch 4261, Loss 0.29419180750846863\n",
      "[Training Epoch 8] Batch 4262, Loss 0.23865273594856262\n",
      "[Training Epoch 8] Batch 4263, Loss 0.23672610521316528\n",
      "[Training Epoch 8] Batch 4264, Loss 0.2354971319437027\n",
      "[Training Epoch 8] Batch 4265, Loss 0.23457026481628418\n",
      "[Training Epoch 8] Batch 4266, Loss 0.2259274125099182\n",
      "[Training Epoch 8] Batch 4267, Loss 0.26117363572120667\n",
      "[Training Epoch 8] Batch 4268, Loss 0.26573890447616577\n",
      "[Training Epoch 8] Batch 4269, Loss 0.24834227561950684\n",
      "[Training Epoch 8] Batch 4270, Loss 0.2682112455368042\n",
      "[Training Epoch 8] Batch 4271, Loss 0.29539474844932556\n",
      "[Training Epoch 8] Batch 4272, Loss 0.23851719498634338\n",
      "[Training Epoch 8] Batch 4273, Loss 0.24496638774871826\n",
      "[Training Epoch 8] Batch 4274, Loss 0.2533930838108063\n",
      "[Training Epoch 8] Batch 4275, Loss 0.24125821888446808\n",
      "[Training Epoch 8] Batch 4276, Loss 0.2329695075750351\n",
      "[Training Epoch 8] Batch 4277, Loss 0.26021474599838257\n",
      "[Training Epoch 8] Batch 4278, Loss 0.2527857720851898\n",
      "[Training Epoch 8] Batch 4279, Loss 0.2671862244606018\n",
      "[Training Epoch 8] Batch 4280, Loss 0.25690925121307373\n",
      "[Training Epoch 8] Batch 4281, Loss 0.27097874879837036\n",
      "[Training Epoch 8] Batch 4282, Loss 0.24804039299488068\n",
      "[Training Epoch 8] Batch 4283, Loss 0.2553715705871582\n",
      "[Training Epoch 8] Batch 4284, Loss 0.24613548815250397\n",
      "[Training Epoch 8] Batch 4285, Loss 0.26790136098861694\n",
      "[Training Epoch 8] Batch 4286, Loss 0.25836142897605896\n",
      "[Training Epoch 8] Batch 4287, Loss 0.24171973764896393\n",
      "[Training Epoch 8] Batch 4288, Loss 0.23088940978050232\n",
      "[Training Epoch 8] Batch 4289, Loss 0.27157866954803467\n",
      "[Training Epoch 8] Batch 4290, Loss 0.2688829302787781\n",
      "[Training Epoch 8] Batch 4291, Loss 0.24366246163845062\n",
      "[Training Epoch 8] Batch 4292, Loss 0.29817453026771545\n",
      "[Training Epoch 8] Batch 4293, Loss 0.23118895292282104\n",
      "[Training Epoch 8] Batch 4294, Loss 0.23669542372226715\n",
      "[Training Epoch 8] Batch 4295, Loss 0.25315558910369873\n",
      "[Training Epoch 8] Batch 4296, Loss 0.2821336090564728\n",
      "[Training Epoch 8] Batch 4297, Loss 0.24817107617855072\n",
      "[Training Epoch 8] Batch 4298, Loss 0.2566099762916565\n",
      "[Training Epoch 8] Batch 4299, Loss 0.29821038246154785\n",
      "[Training Epoch 8] Batch 4300, Loss 0.2701401710510254\n",
      "[Training Epoch 8] Batch 4301, Loss 0.28949445486068726\n",
      "[Training Epoch 8] Batch 4302, Loss 0.2598462998867035\n",
      "[Training Epoch 8] Batch 4303, Loss 0.2424216866493225\n",
      "[Training Epoch 8] Batch 4304, Loss 0.2552754580974579\n",
      "[Training Epoch 8] Batch 4305, Loss 0.2596304416656494\n",
      "[Training Epoch 8] Batch 4306, Loss 0.25481441617012024\n",
      "[Training Epoch 8] Batch 4307, Loss 0.249044731259346\n",
      "[Training Epoch 8] Batch 4308, Loss 0.2642010450363159\n",
      "[Training Epoch 8] Batch 4309, Loss 0.23471972346305847\n",
      "[Training Epoch 8] Batch 4310, Loss 0.26200056076049805\n",
      "[Training Epoch 8] Batch 4311, Loss 0.26037779450416565\n",
      "[Training Epoch 8] Batch 4312, Loss 0.27380475401878357\n",
      "[Training Epoch 8] Batch 4313, Loss 0.24990314245224\n",
      "[Training Epoch 8] Batch 4314, Loss 0.2589203715324402\n",
      "[Training Epoch 8] Batch 4315, Loss 0.28426647186279297\n",
      "[Training Epoch 8] Batch 4316, Loss 0.24153593182563782\n",
      "[Training Epoch 8] Batch 4317, Loss 0.24044491350650787\n",
      "[Training Epoch 8] Batch 4318, Loss 0.25839459896087646\n",
      "[Training Epoch 8] Batch 4319, Loss 0.25297433137893677\n",
      "[Training Epoch 8] Batch 4320, Loss 0.23903273046016693\n",
      "[Training Epoch 8] Batch 4321, Loss 0.23510940372943878\n",
      "[Training Epoch 8] Batch 4322, Loss 0.24908548593521118\n",
      "[Training Epoch 8] Batch 4323, Loss 0.24519677460193634\n",
      "[Training Epoch 8] Batch 4324, Loss 0.27025723457336426\n",
      "[Training Epoch 8] Batch 4325, Loss 0.23897750675678253\n",
      "[Training Epoch 8] Batch 4326, Loss 0.2640400528907776\n",
      "[Training Epoch 8] Batch 4327, Loss 0.25203973054885864\n",
      "[Training Epoch 8] Batch 4328, Loss 0.22847118973731995\n",
      "[Training Epoch 8] Batch 4329, Loss 0.29391688108444214\n",
      "[Training Epoch 8] Batch 4330, Loss 0.2658699154853821\n",
      "[Training Epoch 8] Batch 4331, Loss 0.27741026878356934\n",
      "[Training Epoch 8] Batch 4332, Loss 0.2574613392353058\n",
      "[Training Epoch 8] Batch 4333, Loss 0.2780100703239441\n",
      "[Training Epoch 8] Batch 4334, Loss 0.2603956162929535\n",
      "[Training Epoch 8] Batch 4335, Loss 0.27641430497169495\n",
      "[Training Epoch 8] Batch 4336, Loss 0.2705209255218506\n",
      "[Training Epoch 8] Batch 4337, Loss 0.24760125577449799\n",
      "[Training Epoch 8] Batch 4338, Loss 0.2580239176750183\n",
      "[Training Epoch 8] Batch 4339, Loss 0.2645403742790222\n",
      "[Training Epoch 8] Batch 4340, Loss 0.2578393816947937\n",
      "[Training Epoch 8] Batch 4341, Loss 0.27183640003204346\n",
      "[Training Epoch 8] Batch 4342, Loss 0.2719646692276001\n",
      "[Training Epoch 8] Batch 4343, Loss 0.2814094126224518\n",
      "[Training Epoch 8] Batch 4344, Loss 0.2522585988044739\n",
      "[Training Epoch 8] Batch 4345, Loss 0.26516515016555786\n",
      "[Training Epoch 8] Batch 4346, Loss 0.26863062381744385\n",
      "[Training Epoch 8] Batch 4347, Loss 0.2740611433982849\n",
      "[Training Epoch 8] Batch 4348, Loss 0.23724430799484253\n",
      "[Training Epoch 8] Batch 4349, Loss 0.2367398887872696\n",
      "[Training Epoch 8] Batch 4350, Loss 0.26740193367004395\n",
      "[Training Epoch 8] Batch 4351, Loss 0.29353493452072144\n",
      "[Training Epoch 8] Batch 4352, Loss 0.27654561400413513\n",
      "[Training Epoch 8] Batch 4353, Loss 0.24663163721561432\n",
      "[Training Epoch 8] Batch 4354, Loss 0.25645190477371216\n",
      "[Training Epoch 8] Batch 4355, Loss 0.26719051599502563\n",
      "[Training Epoch 8] Batch 4356, Loss 0.24247196316719055\n",
      "[Training Epoch 8] Batch 4357, Loss 0.25476914644241333\n",
      "[Training Epoch 8] Batch 4358, Loss 0.2688937187194824\n",
      "[Training Epoch 8] Batch 4359, Loss 0.25820392370224\n",
      "[Training Epoch 8] Batch 4360, Loss 0.26871615648269653\n",
      "[Training Epoch 8] Batch 4361, Loss 0.2392682135105133\n",
      "[Training Epoch 8] Batch 4362, Loss 0.2514784336090088\n",
      "[Training Epoch 8] Batch 4363, Loss 0.26268380880355835\n",
      "[Training Epoch 8] Batch 4364, Loss 0.2432868629693985\n",
      "[Training Epoch 8] Batch 4365, Loss 0.2916676998138428\n",
      "[Training Epoch 8] Batch 4366, Loss 0.2736436724662781\n",
      "[Training Epoch 8] Batch 4367, Loss 0.25422585010528564\n",
      "[Training Epoch 8] Batch 4368, Loss 0.24054916203022003\n",
      "[Training Epoch 8] Batch 4369, Loss 0.25336626172065735\n",
      "[Training Epoch 8] Batch 4370, Loss 0.2950176000595093\n",
      "[Training Epoch 8] Batch 4371, Loss 0.2346978485584259\n",
      "[Training Epoch 8] Batch 4372, Loss 0.24226617813110352\n",
      "[Training Epoch 8] Batch 4373, Loss 0.27966997027397156\n",
      "[Training Epoch 8] Batch 4374, Loss 0.26562488079071045\n",
      "[Training Epoch 8] Batch 4375, Loss 0.24535280466079712\n",
      "[Training Epoch 8] Batch 4376, Loss 0.24511222541332245\n",
      "[Training Epoch 8] Batch 4377, Loss 0.25704970955848694\n",
      "[Training Epoch 8] Batch 4378, Loss 0.24669620394706726\n",
      "[Training Epoch 8] Batch 4379, Loss 0.26483994722366333\n",
      "[Training Epoch 8] Batch 4380, Loss 0.26537004113197327\n",
      "[Training Epoch 8] Batch 4381, Loss 0.25073564052581787\n",
      "[Training Epoch 8] Batch 4382, Loss 0.2899876832962036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2221.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 8] Precision = 0.2622, Recall = 0.7758\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.23814447224140167\n",
      "[Training Epoch 9] Batch 1, Loss 0.23979857563972473\n",
      "[Training Epoch 9] Batch 2, Loss 0.24079301953315735\n",
      "[Training Epoch 9] Batch 3, Loss 0.21342912316322327\n",
      "[Training Epoch 9] Batch 4, Loss 0.2770538628101349\n",
      "[Training Epoch 9] Batch 5, Loss 0.2356378734111786\n",
      "[Training Epoch 9] Batch 6, Loss 0.2509317994117737\n",
      "[Training Epoch 9] Batch 7, Loss 0.26008325815200806\n",
      "[Training Epoch 9] Batch 8, Loss 0.22588370740413666\n",
      "[Training Epoch 9] Batch 9, Loss 0.25613105297088623\n",
      "[Training Epoch 9] Batch 10, Loss 0.25647470355033875\n",
      "[Training Epoch 9] Batch 11, Loss 0.22597840428352356\n",
      "[Training Epoch 9] Batch 12, Loss 0.24643301963806152\n",
      "[Training Epoch 9] Batch 13, Loss 0.24360990524291992\n",
      "[Training Epoch 9] Batch 14, Loss 0.2541370689868927\n",
      "[Training Epoch 9] Batch 15, Loss 0.23924437165260315\n",
      "[Training Epoch 9] Batch 16, Loss 0.24870789051055908\n",
      "[Training Epoch 9] Batch 17, Loss 0.26724711060523987\n",
      "[Training Epoch 9] Batch 18, Loss 0.2517162561416626\n",
      "[Training Epoch 9] Batch 19, Loss 0.24975880980491638\n",
      "[Training Epoch 9] Batch 20, Loss 0.25028878450393677\n",
      "[Training Epoch 9] Batch 21, Loss 0.2138652503490448\n",
      "[Training Epoch 9] Batch 22, Loss 0.24480020999908447\n",
      "[Training Epoch 9] Batch 23, Loss 0.24583575129508972\n",
      "[Training Epoch 9] Batch 24, Loss 0.236634761095047\n",
      "[Training Epoch 9] Batch 25, Loss 0.2668299078941345\n",
      "[Training Epoch 9] Batch 26, Loss 0.26509663462638855\n",
      "[Training Epoch 9] Batch 27, Loss 0.27401041984558105\n",
      "[Training Epoch 9] Batch 28, Loss 0.29190611839294434\n",
      "[Training Epoch 9] Batch 29, Loss 0.2742255926132202\n",
      "[Training Epoch 9] Batch 30, Loss 0.23076793551445007\n",
      "[Training Epoch 9] Batch 31, Loss 0.27219104766845703\n",
      "[Training Epoch 9] Batch 32, Loss 0.2635895311832428\n",
      "[Training Epoch 9] Batch 33, Loss 0.25266969203948975\n",
      "[Training Epoch 9] Batch 34, Loss 0.2678426206111908\n",
      "[Training Epoch 9] Batch 35, Loss 0.26210999488830566\n",
      "[Training Epoch 9] Batch 36, Loss 0.26680243015289307\n",
      "[Training Epoch 9] Batch 37, Loss 0.23777291178703308\n",
      "[Training Epoch 9] Batch 38, Loss 0.2500420808792114\n",
      "[Training Epoch 9] Batch 39, Loss 0.2544633746147156\n",
      "[Training Epoch 9] Batch 40, Loss 0.23317521810531616\n",
      "[Training Epoch 9] Batch 41, Loss 0.23563633859157562\n",
      "[Training Epoch 9] Batch 42, Loss 0.24087801575660706\n",
      "[Training Epoch 9] Batch 43, Loss 0.21742135286331177\n",
      "[Training Epoch 9] Batch 44, Loss 0.24119386076927185\n",
      "[Training Epoch 9] Batch 45, Loss 0.22377148270606995\n",
      "[Training Epoch 9] Batch 46, Loss 0.24938419461250305\n",
      "[Training Epoch 9] Batch 47, Loss 0.27511805295944214\n",
      "[Training Epoch 9] Batch 48, Loss 0.23562966287136078\n",
      "[Training Epoch 9] Batch 49, Loss 0.26441657543182373\n",
      "[Training Epoch 9] Batch 50, Loss 0.24455475807189941\n",
      "[Training Epoch 9] Batch 51, Loss 0.22952857613563538\n",
      "[Training Epoch 9] Batch 52, Loss 0.23334507644176483\n",
      "[Training Epoch 9] Batch 53, Loss 0.21681581437587738\n",
      "[Training Epoch 9] Batch 54, Loss 0.22894994914531708\n",
      "[Training Epoch 9] Batch 55, Loss 0.2584281265735626\n",
      "[Training Epoch 9] Batch 56, Loss 0.25303366780281067\n",
      "[Training Epoch 9] Batch 57, Loss 0.23826614022254944\n",
      "[Training Epoch 9] Batch 58, Loss 0.21118351817131042\n",
      "[Training Epoch 9] Batch 59, Loss 0.24299496412277222\n",
      "[Training Epoch 9] Batch 60, Loss 0.23765051364898682\n",
      "[Training Epoch 9] Batch 61, Loss 0.2523877024650574\n",
      "[Training Epoch 9] Batch 62, Loss 0.27192580699920654\n",
      "[Training Epoch 9] Batch 63, Loss 0.24789336323738098\n",
      "[Training Epoch 9] Batch 64, Loss 0.23259413242340088\n",
      "[Training Epoch 9] Batch 65, Loss 0.2566736340522766\n",
      "[Training Epoch 9] Batch 66, Loss 0.24795544147491455\n",
      "[Training Epoch 9] Batch 67, Loss 0.2594689130783081\n",
      "[Training Epoch 9] Batch 68, Loss 0.2462746500968933\n",
      "[Training Epoch 9] Batch 69, Loss 0.24488520622253418\n",
      "[Training Epoch 9] Batch 70, Loss 0.24121728539466858\n",
      "[Training Epoch 9] Batch 71, Loss 0.24826565384864807\n",
      "[Training Epoch 9] Batch 72, Loss 0.2444523721933365\n",
      "[Training Epoch 9] Batch 73, Loss 0.23256570100784302\n",
      "[Training Epoch 9] Batch 74, Loss 0.2614184617996216\n",
      "[Training Epoch 9] Batch 75, Loss 0.24979428946971893\n",
      "[Training Epoch 9] Batch 76, Loss 0.23898327350616455\n",
      "[Training Epoch 9] Batch 77, Loss 0.24558225274085999\n",
      "[Training Epoch 9] Batch 78, Loss 0.2360185980796814\n",
      "[Training Epoch 9] Batch 79, Loss 0.2584834396839142\n",
      "[Training Epoch 9] Batch 80, Loss 0.24185499548912048\n",
      "[Training Epoch 9] Batch 81, Loss 0.22362634539604187\n",
      "[Training Epoch 9] Batch 82, Loss 0.22403451800346375\n",
      "[Training Epoch 9] Batch 83, Loss 0.19931292533874512\n",
      "[Training Epoch 9] Batch 84, Loss 0.24031800031661987\n",
      "[Training Epoch 9] Batch 85, Loss 0.2528591752052307\n",
      "[Training Epoch 9] Batch 86, Loss 0.2425440549850464\n",
      "[Training Epoch 9] Batch 87, Loss 0.23809736967086792\n",
      "[Training Epoch 9] Batch 88, Loss 0.2589099407196045\n",
      "[Training Epoch 9] Batch 89, Loss 0.2615198493003845\n",
      "[Training Epoch 9] Batch 90, Loss 0.2684232294559479\n",
      "[Training Epoch 9] Batch 91, Loss 0.24323488771915436\n",
      "[Training Epoch 9] Batch 92, Loss 0.25550031661987305\n",
      "[Training Epoch 9] Batch 93, Loss 0.25668659806251526\n",
      "[Training Epoch 9] Batch 94, Loss 0.2641907334327698\n",
      "[Training Epoch 9] Batch 95, Loss 0.24027207493782043\n",
      "[Training Epoch 9] Batch 96, Loss 0.26795732975006104\n",
      "[Training Epoch 9] Batch 97, Loss 0.26199275255203247\n",
      "[Training Epoch 9] Batch 98, Loss 0.2597871422767639\n",
      "[Training Epoch 9] Batch 99, Loss 0.26113033294677734\n",
      "[Training Epoch 9] Batch 100, Loss 0.24069376289844513\n",
      "[Training Epoch 9] Batch 101, Loss 0.2686816453933716\n",
      "[Training Epoch 9] Batch 102, Loss 0.24329234659671783\n",
      "[Training Epoch 9] Batch 103, Loss 0.24950170516967773\n",
      "[Training Epoch 9] Batch 104, Loss 0.2566492259502411\n",
      "[Training Epoch 9] Batch 105, Loss 0.24082151055335999\n",
      "[Training Epoch 9] Batch 106, Loss 0.25846749544143677\n",
      "[Training Epoch 9] Batch 107, Loss 0.2667810916900635\n",
      "[Training Epoch 9] Batch 108, Loss 0.26818615198135376\n",
      "[Training Epoch 9] Batch 109, Loss 0.2556608319282532\n",
      "[Training Epoch 9] Batch 110, Loss 0.2539846897125244\n",
      "[Training Epoch 9] Batch 111, Loss 0.23543038964271545\n",
      "[Training Epoch 9] Batch 112, Loss 0.25197872519493103\n",
      "[Training Epoch 9] Batch 113, Loss 0.26115167140960693\n",
      "[Training Epoch 9] Batch 114, Loss 0.26210975646972656\n",
      "[Training Epoch 9] Batch 115, Loss 0.2486283779144287\n",
      "[Training Epoch 9] Batch 116, Loss 0.25771564245224\n",
      "[Training Epoch 9] Batch 117, Loss 0.2531464099884033\n",
      "[Training Epoch 9] Batch 118, Loss 0.24514004588127136\n",
      "[Training Epoch 9] Batch 119, Loss 0.25468021631240845\n",
      "[Training Epoch 9] Batch 120, Loss 0.22764375805854797\n",
      "[Training Epoch 9] Batch 121, Loss 0.2680261433124542\n",
      "[Training Epoch 9] Batch 122, Loss 0.23874899744987488\n",
      "[Training Epoch 9] Batch 123, Loss 0.27375322580337524\n",
      "[Training Epoch 9] Batch 124, Loss 0.2555145025253296\n",
      "[Training Epoch 9] Batch 125, Loss 0.26553887128829956\n",
      "[Training Epoch 9] Batch 126, Loss 0.2596880793571472\n",
      "[Training Epoch 9] Batch 127, Loss 0.2567519247531891\n",
      "[Training Epoch 9] Batch 128, Loss 0.23266276717185974\n",
      "[Training Epoch 9] Batch 129, Loss 0.2497703582048416\n",
      "[Training Epoch 9] Batch 130, Loss 0.2495986670255661\n",
      "[Training Epoch 9] Batch 131, Loss 0.26574116945266724\n",
      "[Training Epoch 9] Batch 132, Loss 0.24788245558738708\n",
      "[Training Epoch 9] Batch 133, Loss 0.2536419928073883\n",
      "[Training Epoch 9] Batch 134, Loss 0.22533291578292847\n",
      "[Training Epoch 9] Batch 135, Loss 0.2771339416503906\n",
      "[Training Epoch 9] Batch 136, Loss 0.2340751439332962\n",
      "[Training Epoch 9] Batch 137, Loss 0.23497489094734192\n",
      "[Training Epoch 9] Batch 138, Loss 0.25106292963027954\n",
      "[Training Epoch 9] Batch 139, Loss 0.23051618039608002\n",
      "[Training Epoch 9] Batch 140, Loss 0.23737844824790955\n",
      "[Training Epoch 9] Batch 141, Loss 0.2513512372970581\n",
      "[Training Epoch 9] Batch 142, Loss 0.25405561923980713\n",
      "[Training Epoch 9] Batch 143, Loss 0.2266254425048828\n",
      "[Training Epoch 9] Batch 144, Loss 0.24465787410736084\n",
      "[Training Epoch 9] Batch 145, Loss 0.2658308744430542\n",
      "[Training Epoch 9] Batch 146, Loss 0.22912290692329407\n",
      "[Training Epoch 9] Batch 147, Loss 0.2758975625038147\n",
      "[Training Epoch 9] Batch 148, Loss 0.24459439516067505\n",
      "[Training Epoch 9] Batch 149, Loss 0.24742618203163147\n",
      "[Training Epoch 9] Batch 150, Loss 0.23743727803230286\n",
      "[Training Epoch 9] Batch 151, Loss 0.23557618260383606\n",
      "[Training Epoch 9] Batch 152, Loss 0.25249725580215454\n",
      "[Training Epoch 9] Batch 153, Loss 0.2636069059371948\n",
      "[Training Epoch 9] Batch 154, Loss 0.26104092597961426\n",
      "[Training Epoch 9] Batch 155, Loss 0.2631458044052124\n",
      "[Training Epoch 9] Batch 156, Loss 0.23920899629592896\n",
      "[Training Epoch 9] Batch 157, Loss 0.2358013391494751\n",
      "[Training Epoch 9] Batch 158, Loss 0.23869292438030243\n",
      "[Training Epoch 9] Batch 159, Loss 0.25966617465019226\n",
      "[Training Epoch 9] Batch 160, Loss 0.24828305840492249\n",
      "[Training Epoch 9] Batch 161, Loss 0.24535787105560303\n",
      "[Training Epoch 9] Batch 162, Loss 0.2433425486087799\n",
      "[Training Epoch 9] Batch 163, Loss 0.2546193599700928\n",
      "[Training Epoch 9] Batch 164, Loss 0.2521292567253113\n",
      "[Training Epoch 9] Batch 165, Loss 0.26543399691581726\n",
      "[Training Epoch 9] Batch 166, Loss 0.2390129417181015\n",
      "[Training Epoch 9] Batch 167, Loss 0.2842075526714325\n",
      "[Training Epoch 9] Batch 168, Loss 0.24214844405651093\n",
      "[Training Epoch 9] Batch 169, Loss 0.2743253707885742\n",
      "[Training Epoch 9] Batch 170, Loss 0.24988088011741638\n",
      "[Training Epoch 9] Batch 171, Loss 0.24316145479679108\n",
      "[Training Epoch 9] Batch 172, Loss 0.26589781045913696\n",
      "[Training Epoch 9] Batch 173, Loss 0.2596757411956787\n",
      "[Training Epoch 9] Batch 174, Loss 0.2496437281370163\n",
      "[Training Epoch 9] Batch 175, Loss 0.2892507314682007\n",
      "[Training Epoch 9] Batch 176, Loss 0.2746935486793518\n",
      "[Training Epoch 9] Batch 177, Loss 0.2466004341840744\n",
      "[Training Epoch 9] Batch 178, Loss 0.24841229617595673\n",
      "[Training Epoch 9] Batch 179, Loss 0.2354118824005127\n",
      "[Training Epoch 9] Batch 180, Loss 0.27340906858444214\n",
      "[Training Epoch 9] Batch 181, Loss 0.2345777451992035\n",
      "[Training Epoch 9] Batch 182, Loss 0.24241036176681519\n",
      "[Training Epoch 9] Batch 183, Loss 0.25654613971710205\n",
      "[Training Epoch 9] Batch 184, Loss 0.2626354694366455\n",
      "[Training Epoch 9] Batch 185, Loss 0.24738526344299316\n",
      "[Training Epoch 9] Batch 186, Loss 0.2326439470052719\n",
      "[Training Epoch 9] Batch 187, Loss 0.2567211389541626\n",
      "[Training Epoch 9] Batch 188, Loss 0.2357185184955597\n",
      "[Training Epoch 9] Batch 189, Loss 0.24278151988983154\n",
      "[Training Epoch 9] Batch 190, Loss 0.2843988537788391\n",
      "[Training Epoch 9] Batch 191, Loss 0.2444099485874176\n",
      "[Training Epoch 9] Batch 192, Loss 0.2799171209335327\n",
      "[Training Epoch 9] Batch 193, Loss 0.2551148533821106\n",
      "[Training Epoch 9] Batch 194, Loss 0.2281055748462677\n",
      "[Training Epoch 9] Batch 195, Loss 0.24152275919914246\n",
      "[Training Epoch 9] Batch 196, Loss 0.25837773084640503\n",
      "[Training Epoch 9] Batch 197, Loss 0.2651863694190979\n",
      "[Training Epoch 9] Batch 198, Loss 0.2508174180984497\n",
      "[Training Epoch 9] Batch 199, Loss 0.23348096013069153\n",
      "[Training Epoch 9] Batch 200, Loss 0.26571500301361084\n",
      "[Training Epoch 9] Batch 201, Loss 0.2325129210948944\n",
      "[Training Epoch 9] Batch 202, Loss 0.2704121768474579\n",
      "[Training Epoch 9] Batch 203, Loss 0.2581019401550293\n",
      "[Training Epoch 9] Batch 204, Loss 0.25666725635528564\n",
      "[Training Epoch 9] Batch 205, Loss 0.22730937600135803\n",
      "[Training Epoch 9] Batch 206, Loss 0.26864367723464966\n",
      "[Training Epoch 9] Batch 207, Loss 0.23719032108783722\n",
      "[Training Epoch 9] Batch 208, Loss 0.23429498076438904\n",
      "[Training Epoch 9] Batch 209, Loss 0.2548869252204895\n",
      "[Training Epoch 9] Batch 210, Loss 0.23421011865139008\n",
      "[Training Epoch 9] Batch 211, Loss 0.24110928177833557\n",
      "[Training Epoch 9] Batch 212, Loss 0.2834157347679138\n",
      "[Training Epoch 9] Batch 213, Loss 0.23308295011520386\n",
      "[Training Epoch 9] Batch 214, Loss 0.23178169131278992\n",
      "[Training Epoch 9] Batch 215, Loss 0.260910302400589\n",
      "[Training Epoch 9] Batch 216, Loss 0.2478899359703064\n",
      "[Training Epoch 9] Batch 217, Loss 0.2568727135658264\n",
      "[Training Epoch 9] Batch 218, Loss 0.24662578105926514\n",
      "[Training Epoch 9] Batch 219, Loss 0.25183412432670593\n",
      "[Training Epoch 9] Batch 220, Loss 0.27845871448516846\n",
      "[Training Epoch 9] Batch 221, Loss 0.24667948484420776\n",
      "[Training Epoch 9] Batch 222, Loss 0.240431010723114\n",
      "[Training Epoch 9] Batch 223, Loss 0.26837092638015747\n",
      "[Training Epoch 9] Batch 224, Loss 0.2377793788909912\n",
      "[Training Epoch 9] Batch 225, Loss 0.2579578459262848\n",
      "[Training Epoch 9] Batch 226, Loss 0.25368067622184753\n",
      "[Training Epoch 9] Batch 227, Loss 0.2455075979232788\n",
      "[Training Epoch 9] Batch 228, Loss 0.26030075550079346\n",
      "[Training Epoch 9] Batch 229, Loss 0.27257388830184937\n",
      "[Training Epoch 9] Batch 230, Loss 0.2738538980484009\n",
      "[Training Epoch 9] Batch 231, Loss 0.259962797164917\n",
      "[Training Epoch 9] Batch 232, Loss 0.21837134659290314\n",
      "[Training Epoch 9] Batch 233, Loss 0.2452186495065689\n",
      "[Training Epoch 9] Batch 234, Loss 0.22998514771461487\n",
      "[Training Epoch 9] Batch 235, Loss 0.284981369972229\n",
      "[Training Epoch 9] Batch 236, Loss 0.24481333792209625\n",
      "[Training Epoch 9] Batch 237, Loss 0.24908636510372162\n",
      "[Training Epoch 9] Batch 238, Loss 0.24220627546310425\n",
      "[Training Epoch 9] Batch 239, Loss 0.27157992124557495\n",
      "[Training Epoch 9] Batch 240, Loss 0.2837398052215576\n",
      "[Training Epoch 9] Batch 241, Loss 0.2707803249359131\n",
      "[Training Epoch 9] Batch 242, Loss 0.2656894326210022\n",
      "[Training Epoch 9] Batch 243, Loss 0.24454264342784882\n",
      "[Training Epoch 9] Batch 244, Loss 0.2575613856315613\n",
      "[Training Epoch 9] Batch 245, Loss 0.25679704546928406\n",
      "[Training Epoch 9] Batch 246, Loss 0.20309978723526\n",
      "[Training Epoch 9] Batch 247, Loss 0.24657577276229858\n",
      "[Training Epoch 9] Batch 248, Loss 0.23406970500946045\n",
      "[Training Epoch 9] Batch 249, Loss 0.2680092751979828\n",
      "[Training Epoch 9] Batch 250, Loss 0.25941821932792664\n",
      "[Training Epoch 9] Batch 251, Loss 0.2413666546344757\n",
      "[Training Epoch 9] Batch 252, Loss 0.24841752648353577\n",
      "[Training Epoch 9] Batch 253, Loss 0.24526971578598022\n",
      "[Training Epoch 9] Batch 254, Loss 0.25389909744262695\n",
      "[Training Epoch 9] Batch 255, Loss 0.22961333394050598\n",
      "[Training Epoch 9] Batch 256, Loss 0.24686075747013092\n",
      "[Training Epoch 9] Batch 257, Loss 0.24046772718429565\n",
      "[Training Epoch 9] Batch 258, Loss 0.2626038193702698\n",
      "[Training Epoch 9] Batch 259, Loss 0.25060057640075684\n",
      "[Training Epoch 9] Batch 260, Loss 0.276459276676178\n",
      "[Training Epoch 9] Batch 261, Loss 0.24257338047027588\n",
      "[Training Epoch 9] Batch 262, Loss 0.23159629106521606\n",
      "[Training Epoch 9] Batch 263, Loss 0.2656221389770508\n",
      "[Training Epoch 9] Batch 264, Loss 0.2496194988489151\n",
      "[Training Epoch 9] Batch 265, Loss 0.27083268761634827\n",
      "[Training Epoch 9] Batch 266, Loss 0.2554675340652466\n",
      "[Training Epoch 9] Batch 267, Loss 0.25742441415786743\n",
      "[Training Epoch 9] Batch 268, Loss 0.22956693172454834\n",
      "[Training Epoch 9] Batch 269, Loss 0.22755487263202667\n",
      "[Training Epoch 9] Batch 270, Loss 0.22773441672325134\n",
      "[Training Epoch 9] Batch 271, Loss 0.23843292891979218\n",
      "[Training Epoch 9] Batch 272, Loss 0.24290667474269867\n",
      "[Training Epoch 9] Batch 273, Loss 0.249050572514534\n",
      "[Training Epoch 9] Batch 274, Loss 0.27703937888145447\n",
      "[Training Epoch 9] Batch 275, Loss 0.24492377042770386\n",
      "[Training Epoch 9] Batch 276, Loss 0.2491573691368103\n",
      "[Training Epoch 9] Batch 277, Loss 0.25031816959381104\n",
      "[Training Epoch 9] Batch 278, Loss 0.2561800181865692\n",
      "[Training Epoch 9] Batch 279, Loss 0.23512683808803558\n",
      "[Training Epoch 9] Batch 280, Loss 0.24626173079013824\n",
      "[Training Epoch 9] Batch 281, Loss 0.2524447441101074\n",
      "[Training Epoch 9] Batch 282, Loss 0.23706084489822388\n",
      "[Training Epoch 9] Batch 283, Loss 0.2543216943740845\n",
      "[Training Epoch 9] Batch 284, Loss 0.26840606331825256\n",
      "[Training Epoch 9] Batch 285, Loss 0.23702070116996765\n",
      "[Training Epoch 9] Batch 286, Loss 0.2208607792854309\n",
      "[Training Epoch 9] Batch 287, Loss 0.28564876317977905\n",
      "[Training Epoch 9] Batch 288, Loss 0.22984489798545837\n",
      "[Training Epoch 9] Batch 289, Loss 0.28972452878952026\n",
      "[Training Epoch 9] Batch 290, Loss 0.2550995945930481\n",
      "[Training Epoch 9] Batch 291, Loss 0.24882185459136963\n",
      "[Training Epoch 9] Batch 292, Loss 0.22126427292823792\n",
      "[Training Epoch 9] Batch 293, Loss 0.23060011863708496\n",
      "[Training Epoch 9] Batch 294, Loss 0.27738794684410095\n",
      "[Training Epoch 9] Batch 295, Loss 0.26815086603164673\n",
      "[Training Epoch 9] Batch 296, Loss 0.22886312007904053\n",
      "[Training Epoch 9] Batch 297, Loss 0.2565786838531494\n",
      "[Training Epoch 9] Batch 298, Loss 0.283191978931427\n",
      "[Training Epoch 9] Batch 299, Loss 0.2470117062330246\n",
      "[Training Epoch 9] Batch 300, Loss 0.2710183262825012\n",
      "[Training Epoch 9] Batch 301, Loss 0.24219834804534912\n",
      "[Training Epoch 9] Batch 302, Loss 0.23223134875297546\n",
      "[Training Epoch 9] Batch 303, Loss 0.2594127058982849\n",
      "[Training Epoch 9] Batch 304, Loss 0.2670260965824127\n",
      "[Training Epoch 9] Batch 305, Loss 0.25298595428466797\n",
      "[Training Epoch 9] Batch 306, Loss 0.2580341696739197\n",
      "[Training Epoch 9] Batch 307, Loss 0.2615051865577698\n",
      "[Training Epoch 9] Batch 308, Loss 0.2736700177192688\n",
      "[Training Epoch 9] Batch 309, Loss 0.2593565583229065\n",
      "[Training Epoch 9] Batch 310, Loss 0.24057473242282867\n",
      "[Training Epoch 9] Batch 311, Loss 0.2492489367723465\n",
      "[Training Epoch 9] Batch 312, Loss 0.21751022338867188\n",
      "[Training Epoch 9] Batch 313, Loss 0.26775336265563965\n",
      "[Training Epoch 9] Batch 314, Loss 0.25359025597572327\n",
      "[Training Epoch 9] Batch 315, Loss 0.24316662549972534\n",
      "[Training Epoch 9] Batch 316, Loss 0.26217758655548096\n",
      "[Training Epoch 9] Batch 317, Loss 0.2478710412979126\n",
      "[Training Epoch 9] Batch 318, Loss 0.2602577209472656\n",
      "[Training Epoch 9] Batch 319, Loss 0.2301001250743866\n",
      "[Training Epoch 9] Batch 320, Loss 0.28444403409957886\n",
      "[Training Epoch 9] Batch 321, Loss 0.278351753950119\n",
      "[Training Epoch 9] Batch 322, Loss 0.25542736053466797\n",
      "[Training Epoch 9] Batch 323, Loss 0.24796763062477112\n",
      "[Training Epoch 9] Batch 324, Loss 0.2475825548171997\n",
      "[Training Epoch 9] Batch 325, Loss 0.23144271969795227\n",
      "[Training Epoch 9] Batch 326, Loss 0.2564842104911804\n",
      "[Training Epoch 9] Batch 327, Loss 0.2564338445663452\n",
      "[Training Epoch 9] Batch 328, Loss 0.25664234161376953\n",
      "[Training Epoch 9] Batch 329, Loss 0.24272659420967102\n",
      "[Training Epoch 9] Batch 330, Loss 0.2464441955089569\n",
      "[Training Epoch 9] Batch 331, Loss 0.2432056963443756\n",
      "[Training Epoch 9] Batch 332, Loss 0.24617606401443481\n",
      "[Training Epoch 9] Batch 333, Loss 0.26805004477500916\n",
      "[Training Epoch 9] Batch 334, Loss 0.22780795395374298\n",
      "[Training Epoch 9] Batch 335, Loss 0.2567025423049927\n",
      "[Training Epoch 9] Batch 336, Loss 0.2268168032169342\n",
      "[Training Epoch 9] Batch 337, Loss 0.25704053044319153\n",
      "[Training Epoch 9] Batch 338, Loss 0.2541520893573761\n",
      "[Training Epoch 9] Batch 339, Loss 0.23669151961803436\n",
      "[Training Epoch 9] Batch 340, Loss 0.26458030939102173\n",
      "[Training Epoch 9] Batch 341, Loss 0.2668703496456146\n",
      "[Training Epoch 9] Batch 342, Loss 0.2534434497356415\n",
      "[Training Epoch 9] Batch 343, Loss 0.2887570261955261\n",
      "[Training Epoch 9] Batch 344, Loss 0.2597057819366455\n",
      "[Training Epoch 9] Batch 345, Loss 0.2677464485168457\n",
      "[Training Epoch 9] Batch 346, Loss 0.2559969425201416\n",
      "[Training Epoch 9] Batch 347, Loss 0.24986156821250916\n",
      "[Training Epoch 9] Batch 348, Loss 0.26724734902381897\n",
      "[Training Epoch 9] Batch 349, Loss 0.2553870677947998\n",
      "[Training Epoch 9] Batch 350, Loss 0.2632836401462555\n",
      "[Training Epoch 9] Batch 351, Loss 0.24672025442123413\n",
      "[Training Epoch 9] Batch 352, Loss 0.24436578154563904\n",
      "[Training Epoch 9] Batch 353, Loss 0.25631144642829895\n",
      "[Training Epoch 9] Batch 354, Loss 0.2438504546880722\n",
      "[Training Epoch 9] Batch 355, Loss 0.22813042998313904\n",
      "[Training Epoch 9] Batch 356, Loss 0.24852381646633148\n",
      "[Training Epoch 9] Batch 357, Loss 0.25105294585227966\n",
      "[Training Epoch 9] Batch 358, Loss 0.23703604936599731\n",
      "[Training Epoch 9] Batch 359, Loss 0.24180327355861664\n",
      "[Training Epoch 9] Batch 360, Loss 0.2633684277534485\n",
      "[Training Epoch 9] Batch 361, Loss 0.24698606133460999\n",
      "[Training Epoch 9] Batch 362, Loss 0.24530643224716187\n",
      "[Training Epoch 9] Batch 363, Loss 0.2468908429145813\n",
      "[Training Epoch 9] Batch 364, Loss 0.2551981210708618\n",
      "[Training Epoch 9] Batch 365, Loss 0.23189091682434082\n",
      "[Training Epoch 9] Batch 366, Loss 0.2394336462020874\n",
      "[Training Epoch 9] Batch 367, Loss 0.2666415572166443\n",
      "[Training Epoch 9] Batch 368, Loss 0.2435980588197708\n",
      "[Training Epoch 9] Batch 369, Loss 0.2682874798774719\n",
      "[Training Epoch 9] Batch 370, Loss 0.24971112608909607\n",
      "[Training Epoch 9] Batch 371, Loss 0.2457515001296997\n",
      "[Training Epoch 9] Batch 372, Loss 0.20001114904880524\n",
      "[Training Epoch 9] Batch 373, Loss 0.26598432660102844\n",
      "[Training Epoch 9] Batch 374, Loss 0.22797006368637085\n",
      "[Training Epoch 9] Batch 375, Loss 0.2527373433113098\n",
      "[Training Epoch 9] Batch 376, Loss 0.2505326271057129\n",
      "[Training Epoch 9] Batch 377, Loss 0.28171879053115845\n",
      "[Training Epoch 9] Batch 378, Loss 0.23868697881698608\n",
      "[Training Epoch 9] Batch 379, Loss 0.2557908594608307\n",
      "[Training Epoch 9] Batch 380, Loss 0.24756178259849548\n",
      "[Training Epoch 9] Batch 381, Loss 0.30717846751213074\n",
      "[Training Epoch 9] Batch 382, Loss 0.23764170706272125\n",
      "[Training Epoch 9] Batch 383, Loss 0.22846969962120056\n",
      "[Training Epoch 9] Batch 384, Loss 0.2651769518852234\n",
      "[Training Epoch 9] Batch 385, Loss 0.2500346601009369\n",
      "[Training Epoch 9] Batch 386, Loss 0.23503299057483673\n",
      "[Training Epoch 9] Batch 387, Loss 0.2432219237089157\n",
      "[Training Epoch 9] Batch 388, Loss 0.270504355430603\n",
      "[Training Epoch 9] Batch 389, Loss 0.24105605483055115\n",
      "[Training Epoch 9] Batch 390, Loss 0.26831039786338806\n",
      "[Training Epoch 9] Batch 391, Loss 0.2644597291946411\n",
      "[Training Epoch 9] Batch 392, Loss 0.23217564821243286\n",
      "[Training Epoch 9] Batch 393, Loss 0.22678932547569275\n",
      "[Training Epoch 9] Batch 394, Loss 0.2380952686071396\n",
      "[Training Epoch 9] Batch 395, Loss 0.2430446445941925\n",
      "[Training Epoch 9] Batch 396, Loss 0.28420719504356384\n",
      "[Training Epoch 9] Batch 397, Loss 0.26613134145736694\n",
      "[Training Epoch 9] Batch 398, Loss 0.27515798807144165\n",
      "[Training Epoch 9] Batch 399, Loss 0.25019463896751404\n",
      "[Training Epoch 9] Batch 400, Loss 0.25395262241363525\n",
      "[Training Epoch 9] Batch 401, Loss 0.2546347975730896\n",
      "[Training Epoch 9] Batch 402, Loss 0.2688559889793396\n",
      "[Training Epoch 9] Batch 403, Loss 0.2630325257778168\n",
      "[Training Epoch 9] Batch 404, Loss 0.24569517374038696\n",
      "[Training Epoch 9] Batch 405, Loss 0.26806551218032837\n",
      "[Training Epoch 9] Batch 406, Loss 0.2515089511871338\n",
      "[Training Epoch 9] Batch 407, Loss 0.2660270035266876\n",
      "[Training Epoch 9] Batch 408, Loss 0.2610967755317688\n",
      "[Training Epoch 9] Batch 409, Loss 0.24535098671913147\n",
      "[Training Epoch 9] Batch 410, Loss 0.25542646646499634\n",
      "[Training Epoch 9] Batch 411, Loss 0.27554142475128174\n",
      "[Training Epoch 9] Batch 412, Loss 0.2408462017774582\n",
      "[Training Epoch 9] Batch 413, Loss 0.25511109828948975\n",
      "[Training Epoch 9] Batch 414, Loss 0.25890132784843445\n",
      "[Training Epoch 9] Batch 415, Loss 0.2496764361858368\n",
      "[Training Epoch 9] Batch 416, Loss 0.24309207499027252\n",
      "[Training Epoch 9] Batch 417, Loss 0.22594089806079865\n",
      "[Training Epoch 9] Batch 418, Loss 0.2711154818534851\n",
      "[Training Epoch 9] Batch 419, Loss 0.2444804608821869\n",
      "[Training Epoch 9] Batch 420, Loss 0.26859816908836365\n",
      "[Training Epoch 9] Batch 421, Loss 0.24962106347084045\n",
      "[Training Epoch 9] Batch 422, Loss 0.27198219299316406\n",
      "[Training Epoch 9] Batch 423, Loss 0.2454949915409088\n",
      "[Training Epoch 9] Batch 424, Loss 0.28866463899612427\n",
      "[Training Epoch 9] Batch 425, Loss 0.2793993651866913\n",
      "[Training Epoch 9] Batch 426, Loss 0.25316494703292847\n",
      "[Training Epoch 9] Batch 427, Loss 0.22710704803466797\n",
      "[Training Epoch 9] Batch 428, Loss 0.2189217060804367\n",
      "[Training Epoch 9] Batch 429, Loss 0.22849062085151672\n",
      "[Training Epoch 9] Batch 430, Loss 0.2907143831253052\n",
      "[Training Epoch 9] Batch 431, Loss 0.24646565318107605\n",
      "[Training Epoch 9] Batch 432, Loss 0.25327467918395996\n",
      "[Training Epoch 9] Batch 433, Loss 0.2561829686164856\n",
      "[Training Epoch 9] Batch 434, Loss 0.24608545005321503\n",
      "[Training Epoch 9] Batch 435, Loss 0.2622528076171875\n",
      "[Training Epoch 9] Batch 436, Loss 0.22006213665008545\n",
      "[Training Epoch 9] Batch 437, Loss 0.2294398993253708\n",
      "[Training Epoch 9] Batch 438, Loss 0.22557567059993744\n",
      "[Training Epoch 9] Batch 439, Loss 0.25250476598739624\n",
      "[Training Epoch 9] Batch 440, Loss 0.22307777404785156\n",
      "[Training Epoch 9] Batch 441, Loss 0.2591504752635956\n",
      "[Training Epoch 9] Batch 442, Loss 0.24331052601337433\n",
      "[Training Epoch 9] Batch 443, Loss 0.2525046765804291\n",
      "[Training Epoch 9] Batch 444, Loss 0.24505433440208435\n",
      "[Training Epoch 9] Batch 445, Loss 0.27506470680236816\n",
      "[Training Epoch 9] Batch 446, Loss 0.2702181339263916\n",
      "[Training Epoch 9] Batch 447, Loss 0.26675063371658325\n",
      "[Training Epoch 9] Batch 448, Loss 0.26766300201416016\n",
      "[Training Epoch 9] Batch 449, Loss 0.2718747854232788\n",
      "[Training Epoch 9] Batch 450, Loss 0.26523470878601074\n",
      "[Training Epoch 9] Batch 451, Loss 0.24849829077720642\n",
      "[Training Epoch 9] Batch 452, Loss 0.25486159324645996\n",
      "[Training Epoch 9] Batch 453, Loss 0.2638872265815735\n",
      "[Training Epoch 9] Batch 454, Loss 0.2793086767196655\n",
      "[Training Epoch 9] Batch 455, Loss 0.2721306085586548\n",
      "[Training Epoch 9] Batch 456, Loss 0.2698041498661041\n",
      "[Training Epoch 9] Batch 457, Loss 0.22662411630153656\n",
      "[Training Epoch 9] Batch 458, Loss 0.2565634846687317\n",
      "[Training Epoch 9] Batch 459, Loss 0.2602980136871338\n",
      "[Training Epoch 9] Batch 460, Loss 0.2701413929462433\n",
      "[Training Epoch 9] Batch 461, Loss 0.23426328599452972\n",
      "[Training Epoch 9] Batch 462, Loss 0.2562708258628845\n",
      "[Training Epoch 9] Batch 463, Loss 0.2425408959388733\n",
      "[Training Epoch 9] Batch 464, Loss 0.299843966960907\n",
      "[Training Epoch 9] Batch 465, Loss 0.22968363761901855\n",
      "[Training Epoch 9] Batch 466, Loss 0.22779282927513123\n",
      "[Training Epoch 9] Batch 467, Loss 0.26149922609329224\n",
      "[Training Epoch 9] Batch 468, Loss 0.23608501255512238\n",
      "[Training Epoch 9] Batch 469, Loss 0.2586814761161804\n",
      "[Training Epoch 9] Batch 470, Loss 0.26369380950927734\n",
      "[Training Epoch 9] Batch 471, Loss 0.22284623980522156\n",
      "[Training Epoch 9] Batch 472, Loss 0.26472899317741394\n",
      "[Training Epoch 9] Batch 473, Loss 0.2907378375530243\n",
      "[Training Epoch 9] Batch 474, Loss 0.24830707907676697\n",
      "[Training Epoch 9] Batch 475, Loss 0.2536948025226593\n",
      "[Training Epoch 9] Batch 476, Loss 0.25683748722076416\n",
      "[Training Epoch 9] Batch 477, Loss 0.2724931836128235\n",
      "[Training Epoch 9] Batch 478, Loss 0.2554174065589905\n",
      "[Training Epoch 9] Batch 479, Loss 0.2202037125825882\n",
      "[Training Epoch 9] Batch 480, Loss 0.2221403270959854\n",
      "[Training Epoch 9] Batch 481, Loss 0.24672409892082214\n",
      "[Training Epoch 9] Batch 482, Loss 0.2664809226989746\n",
      "[Training Epoch 9] Batch 483, Loss 0.2655268907546997\n",
      "[Training Epoch 9] Batch 484, Loss 0.21704405546188354\n",
      "[Training Epoch 9] Batch 485, Loss 0.23698103427886963\n",
      "[Training Epoch 9] Batch 486, Loss 0.25848233699798584\n",
      "[Training Epoch 9] Batch 487, Loss 0.2659309506416321\n",
      "[Training Epoch 9] Batch 488, Loss 0.27224597334861755\n",
      "[Training Epoch 9] Batch 489, Loss 0.25360339879989624\n",
      "[Training Epoch 9] Batch 490, Loss 0.24643512070178986\n",
      "[Training Epoch 9] Batch 491, Loss 0.26671916246414185\n",
      "[Training Epoch 9] Batch 492, Loss 0.24270950257778168\n",
      "[Training Epoch 9] Batch 493, Loss 0.23548367619514465\n",
      "[Training Epoch 9] Batch 494, Loss 0.2633402347564697\n",
      "[Training Epoch 9] Batch 495, Loss 0.2513534426689148\n",
      "[Training Epoch 9] Batch 496, Loss 0.24871081113815308\n",
      "[Training Epoch 9] Batch 497, Loss 0.26177674531936646\n",
      "[Training Epoch 9] Batch 498, Loss 0.24363283812999725\n",
      "[Training Epoch 9] Batch 499, Loss 0.2755599021911621\n",
      "[Training Epoch 9] Batch 500, Loss 0.24354197084903717\n",
      "[Training Epoch 9] Batch 501, Loss 0.24036245048046112\n",
      "[Training Epoch 9] Batch 502, Loss 0.2587745189666748\n",
      "[Training Epoch 9] Batch 503, Loss 0.24105775356292725\n",
      "[Training Epoch 9] Batch 504, Loss 0.2713586688041687\n",
      "[Training Epoch 9] Batch 505, Loss 0.24935147166252136\n",
      "[Training Epoch 9] Batch 506, Loss 0.2681872248649597\n",
      "[Training Epoch 9] Batch 507, Loss 0.2213948667049408\n",
      "[Training Epoch 9] Batch 508, Loss 0.23730090260505676\n",
      "[Training Epoch 9] Batch 509, Loss 0.27290230989456177\n",
      "[Training Epoch 9] Batch 510, Loss 0.2923468053340912\n",
      "[Training Epoch 9] Batch 511, Loss 0.26227062940597534\n",
      "[Training Epoch 9] Batch 512, Loss 0.2577775716781616\n",
      "[Training Epoch 9] Batch 513, Loss 0.2644081115722656\n",
      "[Training Epoch 9] Batch 514, Loss 0.27089521288871765\n",
      "[Training Epoch 9] Batch 515, Loss 0.24299967288970947\n",
      "[Training Epoch 9] Batch 516, Loss 0.24873822927474976\n",
      "[Training Epoch 9] Batch 517, Loss 0.2699345350265503\n",
      "[Training Epoch 9] Batch 518, Loss 0.26344385743141174\n",
      "[Training Epoch 9] Batch 519, Loss 0.2597711682319641\n",
      "[Training Epoch 9] Batch 520, Loss 0.24487778544425964\n",
      "[Training Epoch 9] Batch 521, Loss 0.27662646770477295\n",
      "[Training Epoch 9] Batch 522, Loss 0.24715982377529144\n",
      "[Training Epoch 9] Batch 523, Loss 0.25416460633277893\n",
      "[Training Epoch 9] Batch 524, Loss 0.25263893604278564\n",
      "[Training Epoch 9] Batch 525, Loss 0.2621723413467407\n",
      "[Training Epoch 9] Batch 526, Loss 0.2575622498989105\n",
      "[Training Epoch 9] Batch 527, Loss 0.2426721155643463\n",
      "[Training Epoch 9] Batch 528, Loss 0.23365461826324463\n",
      "[Training Epoch 9] Batch 529, Loss 0.22635582089424133\n",
      "[Training Epoch 9] Batch 530, Loss 0.2578049302101135\n",
      "[Training Epoch 9] Batch 531, Loss 0.2482733130455017\n",
      "[Training Epoch 9] Batch 532, Loss 0.24398404359817505\n",
      "[Training Epoch 9] Batch 533, Loss 0.25058823823928833\n",
      "[Training Epoch 9] Batch 534, Loss 0.2563677430152893\n",
      "[Training Epoch 9] Batch 535, Loss 0.2375417947769165\n",
      "[Training Epoch 9] Batch 536, Loss 0.21596157550811768\n",
      "[Training Epoch 9] Batch 537, Loss 0.2447117567062378\n",
      "[Training Epoch 9] Batch 538, Loss 0.24635013937950134\n",
      "[Training Epoch 9] Batch 539, Loss 0.22393916547298431\n",
      "[Training Epoch 9] Batch 540, Loss 0.2562536299228668\n",
      "[Training Epoch 9] Batch 541, Loss 0.25179415941238403\n",
      "[Training Epoch 9] Batch 542, Loss 0.23853251338005066\n",
      "[Training Epoch 9] Batch 543, Loss 0.24962647259235382\n",
      "[Training Epoch 9] Batch 544, Loss 0.25121399760246277\n",
      "[Training Epoch 9] Batch 545, Loss 0.2400256097316742\n",
      "[Training Epoch 9] Batch 546, Loss 0.26316338777542114\n",
      "[Training Epoch 9] Batch 547, Loss 0.24523510038852692\n",
      "[Training Epoch 9] Batch 548, Loss 0.26758262515068054\n",
      "[Training Epoch 9] Batch 549, Loss 0.25384461879730225\n",
      "[Training Epoch 9] Batch 550, Loss 0.2414713054895401\n",
      "[Training Epoch 9] Batch 551, Loss 0.24943041801452637\n",
      "[Training Epoch 9] Batch 552, Loss 0.23432055115699768\n",
      "[Training Epoch 9] Batch 553, Loss 0.26753923296928406\n",
      "[Training Epoch 9] Batch 554, Loss 0.26377761363983154\n",
      "[Training Epoch 9] Batch 555, Loss 0.2211766541004181\n",
      "[Training Epoch 9] Batch 556, Loss 0.2252964973449707\n",
      "[Training Epoch 9] Batch 557, Loss 0.25986015796661377\n",
      "[Training Epoch 9] Batch 558, Loss 0.24828125536441803\n",
      "[Training Epoch 9] Batch 559, Loss 0.20792734622955322\n",
      "[Training Epoch 9] Batch 560, Loss 0.24343955516815186\n",
      "[Training Epoch 9] Batch 561, Loss 0.2793588638305664\n",
      "[Training Epoch 9] Batch 562, Loss 0.22471484541893005\n",
      "[Training Epoch 9] Batch 563, Loss 0.23624646663665771\n",
      "[Training Epoch 9] Batch 564, Loss 0.26145732402801514\n",
      "[Training Epoch 9] Batch 565, Loss 0.296735554933548\n",
      "[Training Epoch 9] Batch 566, Loss 0.22416934370994568\n",
      "[Training Epoch 9] Batch 567, Loss 0.25656694173812866\n",
      "[Training Epoch 9] Batch 568, Loss 0.2871689796447754\n",
      "[Training Epoch 9] Batch 569, Loss 0.26772522926330566\n",
      "[Training Epoch 9] Batch 570, Loss 0.2328961193561554\n",
      "[Training Epoch 9] Batch 571, Loss 0.2568427324295044\n",
      "[Training Epoch 9] Batch 572, Loss 0.2321343868970871\n",
      "[Training Epoch 9] Batch 573, Loss 0.26658013463020325\n",
      "[Training Epoch 9] Batch 574, Loss 0.22900599241256714\n",
      "[Training Epoch 9] Batch 575, Loss 0.24202510714530945\n",
      "[Training Epoch 9] Batch 576, Loss 0.23201331496238708\n",
      "[Training Epoch 9] Batch 577, Loss 0.23486968874931335\n",
      "[Training Epoch 9] Batch 578, Loss 0.2803509831428528\n",
      "[Training Epoch 9] Batch 579, Loss 0.23849676549434662\n",
      "[Training Epoch 9] Batch 580, Loss 0.26160192489624023\n",
      "[Training Epoch 9] Batch 581, Loss 0.22859999537467957\n",
      "[Training Epoch 9] Batch 582, Loss 0.26803842186927795\n",
      "[Training Epoch 9] Batch 583, Loss 0.23237860202789307\n",
      "[Training Epoch 9] Batch 584, Loss 0.23615770041942596\n",
      "[Training Epoch 9] Batch 585, Loss 0.27187660336494446\n",
      "[Training Epoch 9] Batch 586, Loss 0.2739768624305725\n",
      "[Training Epoch 9] Batch 587, Loss 0.24524900317192078\n",
      "[Training Epoch 9] Batch 588, Loss 0.24142763018608093\n",
      "[Training Epoch 9] Batch 589, Loss 0.23144224286079407\n",
      "[Training Epoch 9] Batch 590, Loss 0.2485235035419464\n",
      "[Training Epoch 9] Batch 591, Loss 0.23402683436870575\n",
      "[Training Epoch 9] Batch 592, Loss 0.25248897075653076\n",
      "[Training Epoch 9] Batch 593, Loss 0.25098302960395813\n",
      "[Training Epoch 9] Batch 594, Loss 0.27181947231292725\n",
      "[Training Epoch 9] Batch 595, Loss 0.24648985266685486\n",
      "[Training Epoch 9] Batch 596, Loss 0.23468026518821716\n",
      "[Training Epoch 9] Batch 597, Loss 0.24736009538173676\n",
      "[Training Epoch 9] Batch 598, Loss 0.2297740876674652\n",
      "[Training Epoch 9] Batch 599, Loss 0.23800383508205414\n",
      "[Training Epoch 9] Batch 600, Loss 0.27068179845809937\n",
      "[Training Epoch 9] Batch 601, Loss 0.22608402371406555\n",
      "[Training Epoch 9] Batch 602, Loss 0.22769391536712646\n",
      "[Training Epoch 9] Batch 603, Loss 0.253268837928772\n",
      "[Training Epoch 9] Batch 604, Loss 0.25995948910713196\n",
      "[Training Epoch 9] Batch 605, Loss 0.27585816383361816\n",
      "[Training Epoch 9] Batch 606, Loss 0.26189446449279785\n",
      "[Training Epoch 9] Batch 607, Loss 0.2443644255399704\n",
      "[Training Epoch 9] Batch 608, Loss 0.25738948583602905\n",
      "[Training Epoch 9] Batch 609, Loss 0.26900559663772583\n",
      "[Training Epoch 9] Batch 610, Loss 0.26397377252578735\n",
      "[Training Epoch 9] Batch 611, Loss 0.24063740670681\n",
      "[Training Epoch 9] Batch 612, Loss 0.26372429728507996\n",
      "[Training Epoch 9] Batch 613, Loss 0.2555481791496277\n",
      "[Training Epoch 9] Batch 614, Loss 0.26294898986816406\n",
      "[Training Epoch 9] Batch 615, Loss 0.23758921027183533\n",
      "[Training Epoch 9] Batch 616, Loss 0.2372892051935196\n",
      "[Training Epoch 9] Batch 617, Loss 0.26508671045303345\n",
      "[Training Epoch 9] Batch 618, Loss 0.24738676846027374\n",
      "[Training Epoch 9] Batch 619, Loss 0.25412076711654663\n",
      "[Training Epoch 9] Batch 620, Loss 0.2152741253376007\n",
      "[Training Epoch 9] Batch 621, Loss 0.27352413535118103\n",
      "[Training Epoch 9] Batch 622, Loss 0.2573252320289612\n",
      "[Training Epoch 9] Batch 623, Loss 0.23707988858222961\n",
      "[Training Epoch 9] Batch 624, Loss 0.22135424613952637\n",
      "[Training Epoch 9] Batch 625, Loss 0.25693562626838684\n",
      "[Training Epoch 9] Batch 626, Loss 0.29279065132141113\n",
      "[Training Epoch 9] Batch 627, Loss 0.28904592990875244\n",
      "[Training Epoch 9] Batch 628, Loss 0.2488948106765747\n",
      "[Training Epoch 9] Batch 629, Loss 0.2751164436340332\n",
      "[Training Epoch 9] Batch 630, Loss 0.23970085382461548\n",
      "[Training Epoch 9] Batch 631, Loss 0.2600829005241394\n",
      "[Training Epoch 9] Batch 632, Loss 0.2782856822013855\n",
      "[Training Epoch 9] Batch 633, Loss 0.2649548649787903\n",
      "[Training Epoch 9] Batch 634, Loss 0.26695021986961365\n",
      "[Training Epoch 9] Batch 635, Loss 0.25267988443374634\n",
      "[Training Epoch 9] Batch 636, Loss 0.28329744935035706\n",
      "[Training Epoch 9] Batch 637, Loss 0.25969427824020386\n",
      "[Training Epoch 9] Batch 638, Loss 0.2774951457977295\n",
      "[Training Epoch 9] Batch 639, Loss 0.22970730066299438\n",
      "[Training Epoch 9] Batch 640, Loss 0.25740131735801697\n",
      "[Training Epoch 9] Batch 641, Loss 0.2395927608013153\n",
      "[Training Epoch 9] Batch 642, Loss 0.25166505575180054\n",
      "[Training Epoch 9] Batch 643, Loss 0.23300521075725555\n",
      "[Training Epoch 9] Batch 644, Loss 0.2403210997581482\n",
      "[Training Epoch 9] Batch 645, Loss 0.26561713218688965\n",
      "[Training Epoch 9] Batch 646, Loss 0.27416425943374634\n",
      "[Training Epoch 9] Batch 647, Loss 0.22333143651485443\n",
      "[Training Epoch 9] Batch 648, Loss 0.2611851096153259\n",
      "[Training Epoch 9] Batch 649, Loss 0.2814372181892395\n",
      "[Training Epoch 9] Batch 650, Loss 0.2436019778251648\n",
      "[Training Epoch 9] Batch 651, Loss 0.23051589727401733\n",
      "[Training Epoch 9] Batch 652, Loss 0.2889014184474945\n",
      "[Training Epoch 9] Batch 653, Loss 0.2448730766773224\n",
      "[Training Epoch 9] Batch 654, Loss 0.2549116015434265\n",
      "[Training Epoch 9] Batch 655, Loss 0.23182770609855652\n",
      "[Training Epoch 9] Batch 656, Loss 0.23922333121299744\n",
      "[Training Epoch 9] Batch 657, Loss 0.22649885714054108\n",
      "[Training Epoch 9] Batch 658, Loss 0.2440737932920456\n",
      "[Training Epoch 9] Batch 659, Loss 0.26542356610298157\n",
      "[Training Epoch 9] Batch 660, Loss 0.26355236768722534\n",
      "[Training Epoch 9] Batch 661, Loss 0.24125263094902039\n",
      "[Training Epoch 9] Batch 662, Loss 0.24858251214027405\n",
      "[Training Epoch 9] Batch 663, Loss 0.2501029372215271\n",
      "[Training Epoch 9] Batch 664, Loss 0.2470114529132843\n",
      "[Training Epoch 9] Batch 665, Loss 0.23872758448123932\n",
      "[Training Epoch 9] Batch 666, Loss 0.2915748655796051\n",
      "[Training Epoch 9] Batch 667, Loss 0.27344387769699097\n",
      "[Training Epoch 9] Batch 668, Loss 0.2565137445926666\n",
      "[Training Epoch 9] Batch 669, Loss 0.24571344256401062\n",
      "[Training Epoch 9] Batch 670, Loss 0.24476248025894165\n",
      "[Training Epoch 9] Batch 671, Loss 0.22557233273983002\n",
      "[Training Epoch 9] Batch 672, Loss 0.26026880741119385\n",
      "[Training Epoch 9] Batch 673, Loss 0.2399841696023941\n",
      "[Training Epoch 9] Batch 674, Loss 0.2675231099128723\n",
      "[Training Epoch 9] Batch 675, Loss 0.2515147030353546\n",
      "[Training Epoch 9] Batch 676, Loss 0.2550867199897766\n",
      "[Training Epoch 9] Batch 677, Loss 0.2634170651435852\n",
      "[Training Epoch 9] Batch 678, Loss 0.26416218280792236\n",
      "[Training Epoch 9] Batch 679, Loss 0.22144562005996704\n",
      "[Training Epoch 9] Batch 680, Loss 0.22896075248718262\n",
      "[Training Epoch 9] Batch 681, Loss 0.25269144773483276\n",
      "[Training Epoch 9] Batch 682, Loss 0.2592596709728241\n",
      "[Training Epoch 9] Batch 683, Loss 0.2694051265716553\n",
      "[Training Epoch 9] Batch 684, Loss 0.25396019220352173\n",
      "[Training Epoch 9] Batch 685, Loss 0.25398534536361694\n",
      "[Training Epoch 9] Batch 686, Loss 0.25443798303604126\n",
      "[Training Epoch 9] Batch 687, Loss 0.26149362325668335\n",
      "[Training Epoch 9] Batch 688, Loss 0.24482284486293793\n",
      "[Training Epoch 9] Batch 689, Loss 0.25314784049987793\n",
      "[Training Epoch 9] Batch 690, Loss 0.2556053400039673\n",
      "[Training Epoch 9] Batch 691, Loss 0.24043488502502441\n",
      "[Training Epoch 9] Batch 692, Loss 0.2414003610610962\n",
      "[Training Epoch 9] Batch 693, Loss 0.26401287317276\n",
      "[Training Epoch 9] Batch 694, Loss 0.21177412569522858\n",
      "[Training Epoch 9] Batch 695, Loss 0.2905164957046509\n",
      "[Training Epoch 9] Batch 696, Loss 0.27100253105163574\n",
      "[Training Epoch 9] Batch 697, Loss 0.28102532029151917\n",
      "[Training Epoch 9] Batch 698, Loss 0.24939918518066406\n",
      "[Training Epoch 9] Batch 699, Loss 0.2642541229724884\n",
      "[Training Epoch 9] Batch 700, Loss 0.2341064065694809\n",
      "[Training Epoch 9] Batch 701, Loss 0.24241244792938232\n",
      "[Training Epoch 9] Batch 702, Loss 0.22914007306098938\n",
      "[Training Epoch 9] Batch 703, Loss 0.2680167555809021\n",
      "[Training Epoch 9] Batch 704, Loss 0.23413878679275513\n",
      "[Training Epoch 9] Batch 705, Loss 0.2637553811073303\n",
      "[Training Epoch 9] Batch 706, Loss 0.2561972439289093\n",
      "[Training Epoch 9] Batch 707, Loss 0.2650381922721863\n",
      "[Training Epoch 9] Batch 708, Loss 0.27610450983047485\n",
      "[Training Epoch 9] Batch 709, Loss 0.25629037618637085\n",
      "[Training Epoch 9] Batch 710, Loss 0.25794345140457153\n",
      "[Training Epoch 9] Batch 711, Loss 0.23878532648086548\n",
      "[Training Epoch 9] Batch 712, Loss 0.23049041628837585\n",
      "[Training Epoch 9] Batch 713, Loss 0.2697293758392334\n",
      "[Training Epoch 9] Batch 714, Loss 0.2493727207183838\n",
      "[Training Epoch 9] Batch 715, Loss 0.26977935433387756\n",
      "[Training Epoch 9] Batch 716, Loss 0.22949160635471344\n",
      "[Training Epoch 9] Batch 717, Loss 0.25058814883232117\n",
      "[Training Epoch 9] Batch 718, Loss 0.25702452659606934\n",
      "[Training Epoch 9] Batch 719, Loss 0.2467779964208603\n",
      "[Training Epoch 9] Batch 720, Loss 0.23619405925273895\n",
      "[Training Epoch 9] Batch 721, Loss 0.26654690504074097\n",
      "[Training Epoch 9] Batch 722, Loss 0.25642281770706177\n",
      "[Training Epoch 9] Batch 723, Loss 0.2501715421676636\n",
      "[Training Epoch 9] Batch 724, Loss 0.28319674730300903\n",
      "[Training Epoch 9] Batch 725, Loss 0.2360026240348816\n",
      "[Training Epoch 9] Batch 726, Loss 0.2753888964653015\n",
      "[Training Epoch 9] Batch 727, Loss 0.25325530767440796\n",
      "[Training Epoch 9] Batch 728, Loss 0.26065611839294434\n",
      "[Training Epoch 9] Batch 729, Loss 0.2548863887786865\n",
      "[Training Epoch 9] Batch 730, Loss 0.2449711412191391\n",
      "[Training Epoch 9] Batch 731, Loss 0.24574023485183716\n",
      "[Training Epoch 9] Batch 732, Loss 0.2422676533460617\n",
      "[Training Epoch 9] Batch 733, Loss 0.25727736949920654\n",
      "[Training Epoch 9] Batch 734, Loss 0.26674067974090576\n",
      "[Training Epoch 9] Batch 735, Loss 0.26387327909469604\n",
      "[Training Epoch 9] Batch 736, Loss 0.27124807238578796\n",
      "[Training Epoch 9] Batch 737, Loss 0.29515206813812256\n",
      "[Training Epoch 9] Batch 738, Loss 0.23283901810646057\n",
      "[Training Epoch 9] Batch 739, Loss 0.27764439582824707\n",
      "[Training Epoch 9] Batch 740, Loss 0.2368260622024536\n",
      "[Training Epoch 9] Batch 741, Loss 0.23865747451782227\n",
      "[Training Epoch 9] Batch 742, Loss 0.271986186504364\n",
      "[Training Epoch 9] Batch 743, Loss 0.2331191748380661\n",
      "[Training Epoch 9] Batch 744, Loss 0.22851967811584473\n",
      "[Training Epoch 9] Batch 745, Loss 0.270413339138031\n",
      "[Training Epoch 9] Batch 746, Loss 0.24090969562530518\n",
      "[Training Epoch 9] Batch 747, Loss 0.26029467582702637\n",
      "[Training Epoch 9] Batch 748, Loss 0.23755644261837006\n",
      "[Training Epoch 9] Batch 749, Loss 0.2775033712387085\n",
      "[Training Epoch 9] Batch 750, Loss 0.2726004123687744\n",
      "[Training Epoch 9] Batch 751, Loss 0.25293195247650146\n",
      "[Training Epoch 9] Batch 752, Loss 0.25011318922042847\n",
      "[Training Epoch 9] Batch 753, Loss 0.22483371198177338\n",
      "[Training Epoch 9] Batch 754, Loss 0.24890011548995972\n",
      "[Training Epoch 9] Batch 755, Loss 0.25321197509765625\n",
      "[Training Epoch 9] Batch 756, Loss 0.2445833832025528\n",
      "[Training Epoch 9] Batch 757, Loss 0.24176019430160522\n",
      "[Training Epoch 9] Batch 758, Loss 0.26122725009918213\n",
      "[Training Epoch 9] Batch 759, Loss 0.25180479884147644\n",
      "[Training Epoch 9] Batch 760, Loss 0.23294948041439056\n",
      "[Training Epoch 9] Batch 761, Loss 0.2764979302883148\n",
      "[Training Epoch 9] Batch 762, Loss 0.2652854025363922\n",
      "[Training Epoch 9] Batch 763, Loss 0.22178563475608826\n",
      "[Training Epoch 9] Batch 764, Loss 0.25441187620162964\n",
      "[Training Epoch 9] Batch 765, Loss 0.2376631498336792\n",
      "[Training Epoch 9] Batch 766, Loss 0.2532987892627716\n",
      "[Training Epoch 9] Batch 767, Loss 0.26071304082870483\n",
      "[Training Epoch 9] Batch 768, Loss 0.22705614566802979\n",
      "[Training Epoch 9] Batch 769, Loss 0.2459617704153061\n",
      "[Training Epoch 9] Batch 770, Loss 0.21973848342895508\n",
      "[Training Epoch 9] Batch 771, Loss 0.25434112548828125\n",
      "[Training Epoch 9] Batch 772, Loss 0.23892781138420105\n",
      "[Training Epoch 9] Batch 773, Loss 0.2226320505142212\n",
      "[Training Epoch 9] Batch 774, Loss 0.2340470254421234\n",
      "[Training Epoch 9] Batch 775, Loss 0.2413386106491089\n",
      "[Training Epoch 9] Batch 776, Loss 0.2506907284259796\n",
      "[Training Epoch 9] Batch 777, Loss 0.23969927430152893\n",
      "[Training Epoch 9] Batch 778, Loss 0.2970905303955078\n",
      "[Training Epoch 9] Batch 779, Loss 0.24842523038387299\n",
      "[Training Epoch 9] Batch 780, Loss 0.24285969138145447\n",
      "[Training Epoch 9] Batch 781, Loss 0.24213117361068726\n",
      "[Training Epoch 9] Batch 782, Loss 0.25829750299453735\n",
      "[Training Epoch 9] Batch 783, Loss 0.2526357173919678\n",
      "[Training Epoch 9] Batch 784, Loss 0.2575264573097229\n",
      "[Training Epoch 9] Batch 785, Loss 0.23109662532806396\n",
      "[Training Epoch 9] Batch 786, Loss 0.2864018678665161\n",
      "[Training Epoch 9] Batch 787, Loss 0.23404057323932648\n",
      "[Training Epoch 9] Batch 788, Loss 0.2541511058807373\n",
      "[Training Epoch 9] Batch 789, Loss 0.2602425515651703\n",
      "[Training Epoch 9] Batch 790, Loss 0.26043492555618286\n",
      "[Training Epoch 9] Batch 791, Loss 0.2734341323375702\n",
      "[Training Epoch 9] Batch 792, Loss 0.2605592906475067\n",
      "[Training Epoch 9] Batch 793, Loss 0.24674038589000702\n",
      "[Training Epoch 9] Batch 794, Loss 0.25056126713752747\n",
      "[Training Epoch 9] Batch 795, Loss 0.24287977814674377\n",
      "[Training Epoch 9] Batch 796, Loss 0.2733900547027588\n",
      "[Training Epoch 9] Batch 797, Loss 0.22345402836799622\n",
      "[Training Epoch 9] Batch 798, Loss 0.23688240349292755\n",
      "[Training Epoch 9] Batch 799, Loss 0.24901935458183289\n",
      "[Training Epoch 9] Batch 800, Loss 0.2841777503490448\n",
      "[Training Epoch 9] Batch 801, Loss 0.24890586733818054\n",
      "[Training Epoch 9] Batch 802, Loss 0.2748083472251892\n",
      "[Training Epoch 9] Batch 803, Loss 0.2524843215942383\n",
      "[Training Epoch 9] Batch 804, Loss 0.28765571117401123\n",
      "[Training Epoch 9] Batch 805, Loss 0.27694928646087646\n",
      "[Training Epoch 9] Batch 806, Loss 0.24494647979736328\n",
      "[Training Epoch 9] Batch 807, Loss 0.23543891310691833\n",
      "[Training Epoch 9] Batch 808, Loss 0.23748597502708435\n",
      "[Training Epoch 9] Batch 809, Loss 0.243742436170578\n",
      "[Training Epoch 9] Batch 810, Loss 0.2647032141685486\n",
      "[Training Epoch 9] Batch 811, Loss 0.27798670530319214\n",
      "[Training Epoch 9] Batch 812, Loss 0.24384355545043945\n",
      "[Training Epoch 9] Batch 813, Loss 0.2571653127670288\n",
      "[Training Epoch 9] Batch 814, Loss 0.24532240629196167\n",
      "[Training Epoch 9] Batch 815, Loss 0.23554012179374695\n",
      "[Training Epoch 9] Batch 816, Loss 0.2504942715167999\n",
      "[Training Epoch 9] Batch 817, Loss 0.2329428493976593\n",
      "[Training Epoch 9] Batch 818, Loss 0.2570747137069702\n",
      "[Training Epoch 9] Batch 819, Loss 0.22935283184051514\n",
      "[Training Epoch 9] Batch 820, Loss 0.22007694840431213\n",
      "[Training Epoch 9] Batch 821, Loss 0.255726158618927\n",
      "[Training Epoch 9] Batch 822, Loss 0.24949517846107483\n",
      "[Training Epoch 9] Batch 823, Loss 0.21932029724121094\n",
      "[Training Epoch 9] Batch 824, Loss 0.25435715913772583\n",
      "[Training Epoch 9] Batch 825, Loss 0.24873608350753784\n",
      "[Training Epoch 9] Batch 826, Loss 0.2597868740558624\n",
      "[Training Epoch 9] Batch 827, Loss 0.2627509534358978\n",
      "[Training Epoch 9] Batch 828, Loss 0.2601241171360016\n",
      "[Training Epoch 9] Batch 829, Loss 0.2598533630371094\n",
      "[Training Epoch 9] Batch 830, Loss 0.22803768515586853\n",
      "[Training Epoch 9] Batch 831, Loss 0.2688637971878052\n",
      "[Training Epoch 9] Batch 832, Loss 0.24933871626853943\n",
      "[Training Epoch 9] Batch 833, Loss 0.2578723728656769\n",
      "[Training Epoch 9] Batch 834, Loss 0.2972579896450043\n",
      "[Training Epoch 9] Batch 835, Loss 0.2455328106880188\n",
      "[Training Epoch 9] Batch 836, Loss 0.23575255274772644\n",
      "[Training Epoch 9] Batch 837, Loss 0.26245585083961487\n",
      "[Training Epoch 9] Batch 838, Loss 0.26805803179740906\n",
      "[Training Epoch 9] Batch 839, Loss 0.24784260988235474\n",
      "[Training Epoch 9] Batch 840, Loss 0.2806497812271118\n",
      "[Training Epoch 9] Batch 841, Loss 0.253470778465271\n",
      "[Training Epoch 9] Batch 842, Loss 0.2445410192012787\n",
      "[Training Epoch 9] Batch 843, Loss 0.29433661699295044\n",
      "[Training Epoch 9] Batch 844, Loss 0.2519117295742035\n",
      "[Training Epoch 9] Batch 845, Loss 0.2338506430387497\n",
      "[Training Epoch 9] Batch 846, Loss 0.28816279768943787\n",
      "[Training Epoch 9] Batch 847, Loss 0.2588014006614685\n",
      "[Training Epoch 9] Batch 848, Loss 0.23957866430282593\n",
      "[Training Epoch 9] Batch 849, Loss 0.24560165405273438\n",
      "[Training Epoch 9] Batch 850, Loss 0.23932072520256042\n",
      "[Training Epoch 9] Batch 851, Loss 0.27255624532699585\n",
      "[Training Epoch 9] Batch 852, Loss 0.24828317761421204\n",
      "[Training Epoch 9] Batch 853, Loss 0.24810490012168884\n",
      "[Training Epoch 9] Batch 854, Loss 0.23946020007133484\n",
      "[Training Epoch 9] Batch 855, Loss 0.26599186658859253\n",
      "[Training Epoch 9] Batch 856, Loss 0.2534993588924408\n",
      "[Training Epoch 9] Batch 857, Loss 0.284894734621048\n",
      "[Training Epoch 9] Batch 858, Loss 0.26393598318099976\n",
      "[Training Epoch 9] Batch 859, Loss 0.2260100245475769\n",
      "[Training Epoch 9] Batch 860, Loss 0.23995015025138855\n",
      "[Training Epoch 9] Batch 861, Loss 0.26581811904907227\n",
      "[Training Epoch 9] Batch 862, Loss 0.24077187478542328\n",
      "[Training Epoch 9] Batch 863, Loss 0.2541061043739319\n",
      "[Training Epoch 9] Batch 864, Loss 0.24130035936832428\n",
      "[Training Epoch 9] Batch 865, Loss 0.2499287724494934\n",
      "[Training Epoch 9] Batch 866, Loss 0.24428275227546692\n",
      "[Training Epoch 9] Batch 867, Loss 0.28127601742744446\n",
      "[Training Epoch 9] Batch 868, Loss 0.291412889957428\n",
      "[Training Epoch 9] Batch 869, Loss 0.2865591049194336\n",
      "[Training Epoch 9] Batch 870, Loss 0.25627124309539795\n",
      "[Training Epoch 9] Batch 871, Loss 0.24697339534759521\n",
      "[Training Epoch 9] Batch 872, Loss 0.25345104932785034\n",
      "[Training Epoch 9] Batch 873, Loss 0.21881476044654846\n",
      "[Training Epoch 9] Batch 874, Loss 0.27152490615844727\n",
      "[Training Epoch 9] Batch 875, Loss 0.2457822859287262\n",
      "[Training Epoch 9] Batch 876, Loss 0.2386959344148636\n",
      "[Training Epoch 9] Batch 877, Loss 0.2597523629665375\n",
      "[Training Epoch 9] Batch 878, Loss 0.26554301381111145\n",
      "[Training Epoch 9] Batch 879, Loss 0.2486051321029663\n",
      "[Training Epoch 9] Batch 880, Loss 0.24661768972873688\n",
      "[Training Epoch 9] Batch 881, Loss 0.2625299096107483\n",
      "[Training Epoch 9] Batch 882, Loss 0.2958186864852905\n",
      "[Training Epoch 9] Batch 883, Loss 0.22092202305793762\n",
      "[Training Epoch 9] Batch 884, Loss 0.23831039667129517\n",
      "[Training Epoch 9] Batch 885, Loss 0.2306136190891266\n",
      "[Training Epoch 9] Batch 886, Loss 0.26749199628829956\n",
      "[Training Epoch 9] Batch 887, Loss 0.28614938259124756\n",
      "[Training Epoch 9] Batch 888, Loss 0.26320236921310425\n",
      "[Training Epoch 9] Batch 889, Loss 0.22708702087402344\n",
      "[Training Epoch 9] Batch 890, Loss 0.23705771565437317\n",
      "[Training Epoch 9] Batch 891, Loss 0.25056490302085876\n",
      "[Training Epoch 9] Batch 892, Loss 0.293661892414093\n",
      "[Training Epoch 9] Batch 893, Loss 0.2658727765083313\n",
      "[Training Epoch 9] Batch 894, Loss 0.266855388879776\n",
      "[Training Epoch 9] Batch 895, Loss 0.25157302618026733\n",
      "[Training Epoch 9] Batch 896, Loss 0.24864348769187927\n",
      "[Training Epoch 9] Batch 897, Loss 0.2618388235569\n",
      "[Training Epoch 9] Batch 898, Loss 0.25732481479644775\n",
      "[Training Epoch 9] Batch 899, Loss 0.2432645559310913\n",
      "[Training Epoch 9] Batch 900, Loss 0.26976847648620605\n",
      "[Training Epoch 9] Batch 901, Loss 0.27022525668144226\n",
      "[Training Epoch 9] Batch 902, Loss 0.23854076862335205\n",
      "[Training Epoch 9] Batch 903, Loss 0.2760108709335327\n",
      "[Training Epoch 9] Batch 904, Loss 0.24091312289237976\n",
      "[Training Epoch 9] Batch 905, Loss 0.26121097803115845\n",
      "[Training Epoch 9] Batch 906, Loss 0.25303661823272705\n",
      "[Training Epoch 9] Batch 907, Loss 0.24134719371795654\n",
      "[Training Epoch 9] Batch 908, Loss 0.2570396661758423\n",
      "[Training Epoch 9] Batch 909, Loss 0.23466554284095764\n",
      "[Training Epoch 9] Batch 910, Loss 0.2397126853466034\n",
      "[Training Epoch 9] Batch 911, Loss 0.22549690306186676\n",
      "[Training Epoch 9] Batch 912, Loss 0.2571871876716614\n",
      "[Training Epoch 9] Batch 913, Loss 0.23397906124591827\n",
      "[Training Epoch 9] Batch 914, Loss 0.2358274757862091\n",
      "[Training Epoch 9] Batch 915, Loss 0.24116046726703644\n",
      "[Training Epoch 9] Batch 916, Loss 0.2689252495765686\n",
      "[Training Epoch 9] Batch 917, Loss 0.2859281003475189\n",
      "[Training Epoch 9] Batch 918, Loss 0.270403653383255\n",
      "[Training Epoch 9] Batch 919, Loss 0.27912765741348267\n",
      "[Training Epoch 9] Batch 920, Loss 0.23879459500312805\n",
      "[Training Epoch 9] Batch 921, Loss 0.23953332006931305\n",
      "[Training Epoch 9] Batch 922, Loss 0.23480376601219177\n",
      "[Training Epoch 9] Batch 923, Loss 0.25914883613586426\n",
      "[Training Epoch 9] Batch 924, Loss 0.25924962759017944\n",
      "[Training Epoch 9] Batch 925, Loss 0.27027255296707153\n",
      "[Training Epoch 9] Batch 926, Loss 0.2451815903186798\n",
      "[Training Epoch 9] Batch 927, Loss 0.26613837480545044\n",
      "[Training Epoch 9] Batch 928, Loss 0.24392667412757874\n",
      "[Training Epoch 9] Batch 929, Loss 0.25948643684387207\n",
      "[Training Epoch 9] Batch 930, Loss 0.2298349142074585\n",
      "[Training Epoch 9] Batch 931, Loss 0.26699498295783997\n",
      "[Training Epoch 9] Batch 932, Loss 0.2534928023815155\n",
      "[Training Epoch 9] Batch 933, Loss 0.23058351874351501\n",
      "[Training Epoch 9] Batch 934, Loss 0.2816374897956848\n",
      "[Training Epoch 9] Batch 935, Loss 0.24971362948417664\n",
      "[Training Epoch 9] Batch 936, Loss 0.24945439398288727\n",
      "[Training Epoch 9] Batch 937, Loss 0.2621060609817505\n",
      "[Training Epoch 9] Batch 938, Loss 0.24718469381332397\n",
      "[Training Epoch 9] Batch 939, Loss 0.26432591676712036\n",
      "[Training Epoch 9] Batch 940, Loss 0.23948773741722107\n",
      "[Training Epoch 9] Batch 941, Loss 0.25728631019592285\n",
      "[Training Epoch 9] Batch 942, Loss 0.2646028697490692\n",
      "[Training Epoch 9] Batch 943, Loss 0.2564729154109955\n",
      "[Training Epoch 9] Batch 944, Loss 0.23903656005859375\n",
      "[Training Epoch 9] Batch 945, Loss 0.2690051794052124\n",
      "[Training Epoch 9] Batch 946, Loss 0.2530510425567627\n",
      "[Training Epoch 9] Batch 947, Loss 0.26315876841545105\n",
      "[Training Epoch 9] Batch 948, Loss 0.26299574971199036\n",
      "[Training Epoch 9] Batch 949, Loss 0.2439427673816681\n",
      "[Training Epoch 9] Batch 950, Loss 0.2804313600063324\n",
      "[Training Epoch 9] Batch 951, Loss 0.26483315229415894\n",
      "[Training Epoch 9] Batch 952, Loss 0.24816924333572388\n",
      "[Training Epoch 9] Batch 953, Loss 0.2589614689350128\n",
      "[Training Epoch 9] Batch 954, Loss 0.24917471408843994\n",
      "[Training Epoch 9] Batch 955, Loss 0.26273655891418457\n",
      "[Training Epoch 9] Batch 956, Loss 0.2550845146179199\n",
      "[Training Epoch 9] Batch 957, Loss 0.25407832860946655\n",
      "[Training Epoch 9] Batch 958, Loss 0.2553477883338928\n",
      "[Training Epoch 9] Batch 959, Loss 0.27752023935317993\n",
      "[Training Epoch 9] Batch 960, Loss 0.25206679105758667\n",
      "[Training Epoch 9] Batch 961, Loss 0.2640174925327301\n",
      "[Training Epoch 9] Batch 962, Loss 0.24710969626903534\n",
      "[Training Epoch 9] Batch 963, Loss 0.2434729039669037\n",
      "[Training Epoch 9] Batch 964, Loss 0.2629495859146118\n",
      "[Training Epoch 9] Batch 965, Loss 0.25518617033958435\n",
      "[Training Epoch 9] Batch 966, Loss 0.23384541273117065\n",
      "[Training Epoch 9] Batch 967, Loss 0.2470814734697342\n",
      "[Training Epoch 9] Batch 968, Loss 0.25758644938468933\n",
      "[Training Epoch 9] Batch 969, Loss 0.23503804206848145\n",
      "[Training Epoch 9] Batch 970, Loss 0.26232630014419556\n",
      "[Training Epoch 9] Batch 971, Loss 0.2685762643814087\n",
      "[Training Epoch 9] Batch 972, Loss 0.2467055320739746\n",
      "[Training Epoch 9] Batch 973, Loss 0.29265567660331726\n",
      "[Training Epoch 9] Batch 974, Loss 0.25892022252082825\n",
      "[Training Epoch 9] Batch 975, Loss 0.27363765239715576\n",
      "[Training Epoch 9] Batch 976, Loss 0.250478059053421\n",
      "[Training Epoch 9] Batch 977, Loss 0.25807487964630127\n",
      "[Training Epoch 9] Batch 978, Loss 0.25176236033439636\n",
      "[Training Epoch 9] Batch 979, Loss 0.25781354308128357\n",
      "[Training Epoch 9] Batch 980, Loss 0.2400493621826172\n",
      "[Training Epoch 9] Batch 981, Loss 0.24072912335395813\n",
      "[Training Epoch 9] Batch 982, Loss 0.23531940579414368\n",
      "[Training Epoch 9] Batch 983, Loss 0.22960028052330017\n",
      "[Training Epoch 9] Batch 984, Loss 0.2524591088294983\n",
      "[Training Epoch 9] Batch 985, Loss 0.23623120784759521\n",
      "[Training Epoch 9] Batch 986, Loss 0.22882643342018127\n",
      "[Training Epoch 9] Batch 987, Loss 0.24827125668525696\n",
      "[Training Epoch 9] Batch 988, Loss 0.2622146010398865\n",
      "[Training Epoch 9] Batch 989, Loss 0.25253981351852417\n",
      "[Training Epoch 9] Batch 990, Loss 0.2616010308265686\n",
      "[Training Epoch 9] Batch 991, Loss 0.24705010652542114\n",
      "[Training Epoch 9] Batch 992, Loss 0.2399836927652359\n",
      "[Training Epoch 9] Batch 993, Loss 0.24679231643676758\n",
      "[Training Epoch 9] Batch 994, Loss 0.2503195106983185\n",
      "[Training Epoch 9] Batch 995, Loss 0.23564866185188293\n",
      "[Training Epoch 9] Batch 996, Loss 0.25874271988868713\n",
      "[Training Epoch 9] Batch 997, Loss 0.25051677227020264\n",
      "[Training Epoch 9] Batch 998, Loss 0.2469770610332489\n",
      "[Training Epoch 9] Batch 999, Loss 0.23211677372455597\n",
      "[Training Epoch 9] Batch 1000, Loss 0.23356054723262787\n",
      "[Training Epoch 9] Batch 1001, Loss 0.23056592047214508\n",
      "[Training Epoch 9] Batch 1002, Loss 0.26114368438720703\n",
      "[Training Epoch 9] Batch 1003, Loss 0.28327757120132446\n",
      "[Training Epoch 9] Batch 1004, Loss 0.2683165669441223\n",
      "[Training Epoch 9] Batch 1005, Loss 0.24561884999275208\n",
      "[Training Epoch 9] Batch 1006, Loss 0.2516610622406006\n",
      "[Training Epoch 9] Batch 1007, Loss 0.2636820077896118\n",
      "[Training Epoch 9] Batch 1008, Loss 0.235617533326149\n",
      "[Training Epoch 9] Batch 1009, Loss 0.23859769105911255\n",
      "[Training Epoch 9] Batch 1010, Loss 0.2744673788547516\n",
      "[Training Epoch 9] Batch 1011, Loss 0.26819494366645813\n",
      "[Training Epoch 9] Batch 1012, Loss 0.24581100046634674\n",
      "[Training Epoch 9] Batch 1013, Loss 0.23256534337997437\n",
      "[Training Epoch 9] Batch 1014, Loss 0.2483445703983307\n",
      "[Training Epoch 9] Batch 1015, Loss 0.2465323507785797\n",
      "[Training Epoch 9] Batch 1016, Loss 0.2399514615535736\n",
      "[Training Epoch 9] Batch 1017, Loss 0.25466015934944153\n",
      "[Training Epoch 9] Batch 1018, Loss 0.23511472344398499\n",
      "[Training Epoch 9] Batch 1019, Loss 0.2640119194984436\n",
      "[Training Epoch 9] Batch 1020, Loss 0.23371654748916626\n",
      "[Training Epoch 9] Batch 1021, Loss 0.24398855865001678\n",
      "[Training Epoch 9] Batch 1022, Loss 0.24398162961006165\n",
      "[Training Epoch 9] Batch 1023, Loss 0.27315783500671387\n",
      "[Training Epoch 9] Batch 1024, Loss 0.24761854112148285\n",
      "[Training Epoch 9] Batch 1025, Loss 0.30713948607444763\n",
      "[Training Epoch 9] Batch 1026, Loss 0.2641768753528595\n",
      "[Training Epoch 9] Batch 1027, Loss 0.243229478597641\n",
      "[Training Epoch 9] Batch 1028, Loss 0.26196759939193726\n",
      "[Training Epoch 9] Batch 1029, Loss 0.24047255516052246\n",
      "[Training Epoch 9] Batch 1030, Loss 0.25646859407424927\n",
      "[Training Epoch 9] Batch 1031, Loss 0.2515870928764343\n",
      "[Training Epoch 9] Batch 1032, Loss 0.24551048874855042\n",
      "[Training Epoch 9] Batch 1033, Loss 0.23876211047172546\n",
      "[Training Epoch 9] Batch 1034, Loss 0.24997064471244812\n",
      "[Training Epoch 9] Batch 1035, Loss 0.22954419255256653\n",
      "[Training Epoch 9] Batch 1036, Loss 0.2766910195350647\n",
      "[Training Epoch 9] Batch 1037, Loss 0.2419019639492035\n",
      "[Training Epoch 9] Batch 1038, Loss 0.248627170920372\n",
      "[Training Epoch 9] Batch 1039, Loss 0.23900337517261505\n",
      "[Training Epoch 9] Batch 1040, Loss 0.24259303510189056\n",
      "[Training Epoch 9] Batch 1041, Loss 0.2529638111591339\n",
      "[Training Epoch 9] Batch 1042, Loss 0.24854274094104767\n",
      "[Training Epoch 9] Batch 1043, Loss 0.2490994930267334\n",
      "[Training Epoch 9] Batch 1044, Loss 0.24929551780223846\n",
      "[Training Epoch 9] Batch 1045, Loss 0.24561046063899994\n",
      "[Training Epoch 9] Batch 1046, Loss 0.26662659645080566\n",
      "[Training Epoch 9] Batch 1047, Loss 0.25640928745269775\n",
      "[Training Epoch 9] Batch 1048, Loss 0.2760017216205597\n",
      "[Training Epoch 9] Batch 1049, Loss 0.24319911003112793\n",
      "[Training Epoch 9] Batch 1050, Loss 0.26087403297424316\n",
      "[Training Epoch 9] Batch 1051, Loss 0.24314948916435242\n",
      "[Training Epoch 9] Batch 1052, Loss 0.26448678970336914\n",
      "[Training Epoch 9] Batch 1053, Loss 0.2523782253265381\n",
      "[Training Epoch 9] Batch 1054, Loss 0.2750200927257538\n",
      "[Training Epoch 9] Batch 1055, Loss 0.24072593450546265\n",
      "[Training Epoch 9] Batch 1056, Loss 0.23451104760169983\n",
      "[Training Epoch 9] Batch 1057, Loss 0.2796834111213684\n",
      "[Training Epoch 9] Batch 1058, Loss 0.2593706548213959\n",
      "[Training Epoch 9] Batch 1059, Loss 0.25876933336257935\n",
      "[Training Epoch 9] Batch 1060, Loss 0.25841736793518066\n",
      "[Training Epoch 9] Batch 1061, Loss 0.2697521448135376\n",
      "[Training Epoch 9] Batch 1062, Loss 0.25441133975982666\n",
      "[Training Epoch 9] Batch 1063, Loss 0.2670475244522095\n",
      "[Training Epoch 9] Batch 1064, Loss 0.27909111976623535\n",
      "[Training Epoch 9] Batch 1065, Loss 0.22568698227405548\n",
      "[Training Epoch 9] Batch 1066, Loss 0.23435579240322113\n",
      "[Training Epoch 9] Batch 1067, Loss 0.259972482919693\n",
      "[Training Epoch 9] Batch 1068, Loss 0.26050543785095215\n",
      "[Training Epoch 9] Batch 1069, Loss 0.22647705674171448\n",
      "[Training Epoch 9] Batch 1070, Loss 0.2331925928592682\n",
      "[Training Epoch 9] Batch 1071, Loss 0.26664552092552185\n",
      "[Training Epoch 9] Batch 1072, Loss 0.2635979950428009\n",
      "[Training Epoch 9] Batch 1073, Loss 0.25664594769477844\n",
      "[Training Epoch 9] Batch 1074, Loss 0.2418011724948883\n",
      "[Training Epoch 9] Batch 1075, Loss 0.27658379077911377\n",
      "[Training Epoch 9] Batch 1076, Loss 0.2600107192993164\n",
      "[Training Epoch 9] Batch 1077, Loss 0.28264427185058594\n",
      "[Training Epoch 9] Batch 1078, Loss 0.24325397610664368\n",
      "[Training Epoch 9] Batch 1079, Loss 0.2642276883125305\n",
      "[Training Epoch 9] Batch 1080, Loss 0.2475440949201584\n",
      "[Training Epoch 9] Batch 1081, Loss 0.2524262070655823\n",
      "[Training Epoch 9] Batch 1082, Loss 0.22802597284317017\n",
      "[Training Epoch 9] Batch 1083, Loss 0.26057669520378113\n",
      "[Training Epoch 9] Batch 1084, Loss 0.25788676738739014\n",
      "[Training Epoch 9] Batch 1085, Loss 0.2621571719646454\n",
      "[Training Epoch 9] Batch 1086, Loss 0.2555258870124817\n",
      "[Training Epoch 9] Batch 1087, Loss 0.25114381313323975\n",
      "[Training Epoch 9] Batch 1088, Loss 0.24657639861106873\n",
      "[Training Epoch 9] Batch 1089, Loss 0.2638767957687378\n",
      "[Training Epoch 9] Batch 1090, Loss 0.2479339987039566\n",
      "[Training Epoch 9] Batch 1091, Loss 0.28299856185913086\n",
      "[Training Epoch 9] Batch 1092, Loss 0.2668948471546173\n",
      "[Training Epoch 9] Batch 1093, Loss 0.26610103249549866\n",
      "[Training Epoch 9] Batch 1094, Loss 0.24882642924785614\n",
      "[Training Epoch 9] Batch 1095, Loss 0.2660916745662689\n",
      "[Training Epoch 9] Batch 1096, Loss 0.27445995807647705\n",
      "[Training Epoch 9] Batch 1097, Loss 0.22955819964408875\n",
      "[Training Epoch 9] Batch 1098, Loss 0.26854366064071655\n",
      "[Training Epoch 9] Batch 1099, Loss 0.23062558472156525\n",
      "[Training Epoch 9] Batch 1100, Loss 0.22693821787834167\n",
      "[Training Epoch 9] Batch 1101, Loss 0.24095535278320312\n",
      "[Training Epoch 9] Batch 1102, Loss 0.25880762934684753\n",
      "[Training Epoch 9] Batch 1103, Loss 0.23850873112678528\n",
      "[Training Epoch 9] Batch 1104, Loss 0.23059822618961334\n",
      "[Training Epoch 9] Batch 1105, Loss 0.2380983978509903\n",
      "[Training Epoch 9] Batch 1106, Loss 0.26154133677482605\n",
      "[Training Epoch 9] Batch 1107, Loss 0.2294078767299652\n",
      "[Training Epoch 9] Batch 1108, Loss 0.2551645338535309\n",
      "[Training Epoch 9] Batch 1109, Loss 0.24165694415569305\n",
      "[Training Epoch 9] Batch 1110, Loss 0.249195396900177\n",
      "[Training Epoch 9] Batch 1111, Loss 0.2560326159000397\n",
      "[Training Epoch 9] Batch 1112, Loss 0.28169727325439453\n",
      "[Training Epoch 9] Batch 1113, Loss 0.24443039298057556\n",
      "[Training Epoch 9] Batch 1114, Loss 0.24979069828987122\n",
      "[Training Epoch 9] Batch 1115, Loss 0.22576627135276794\n",
      "[Training Epoch 9] Batch 1116, Loss 0.2962265610694885\n",
      "[Training Epoch 9] Batch 1117, Loss 0.24718761444091797\n",
      "[Training Epoch 9] Batch 1118, Loss 0.2554813623428345\n",
      "[Training Epoch 9] Batch 1119, Loss 0.276870459318161\n",
      "[Training Epoch 9] Batch 1120, Loss 0.2673277258872986\n",
      "[Training Epoch 9] Batch 1121, Loss 0.2392033338546753\n",
      "[Training Epoch 9] Batch 1122, Loss 0.2523612380027771\n",
      "[Training Epoch 9] Batch 1123, Loss 0.24723783135414124\n",
      "[Training Epoch 9] Batch 1124, Loss 0.2606736421585083\n",
      "[Training Epoch 9] Batch 1125, Loss 0.22919660806655884\n",
      "[Training Epoch 9] Batch 1126, Loss 0.23660169541835785\n",
      "[Training Epoch 9] Batch 1127, Loss 0.22743523120880127\n",
      "[Training Epoch 9] Batch 1128, Loss 0.24253976345062256\n",
      "[Training Epoch 9] Batch 1129, Loss 0.22931942343711853\n",
      "[Training Epoch 9] Batch 1130, Loss 0.25298386812210083\n",
      "[Training Epoch 9] Batch 1131, Loss 0.27467936277389526\n",
      "[Training Epoch 9] Batch 1132, Loss 0.24701012670993805\n",
      "[Training Epoch 9] Batch 1133, Loss 0.23635287582874298\n",
      "[Training Epoch 9] Batch 1134, Loss 0.2626679837703705\n",
      "[Training Epoch 9] Batch 1135, Loss 0.2491917908191681\n",
      "[Training Epoch 9] Batch 1136, Loss 0.261148065328598\n",
      "[Training Epoch 9] Batch 1137, Loss 0.22142799198627472\n",
      "[Training Epoch 9] Batch 1138, Loss 0.25214675068855286\n",
      "[Training Epoch 9] Batch 1139, Loss 0.2542274296283722\n",
      "[Training Epoch 9] Batch 1140, Loss 0.2345951795578003\n",
      "[Training Epoch 9] Batch 1141, Loss 0.2504039406776428\n",
      "[Training Epoch 9] Batch 1142, Loss 0.24293065071105957\n",
      "[Training Epoch 9] Batch 1143, Loss 0.2670978307723999\n",
      "[Training Epoch 9] Batch 1144, Loss 0.27514854073524475\n",
      "[Training Epoch 9] Batch 1145, Loss 0.2503315806388855\n",
      "[Training Epoch 9] Batch 1146, Loss 0.26755258440971375\n",
      "[Training Epoch 9] Batch 1147, Loss 0.2605990171432495\n",
      "[Training Epoch 9] Batch 1148, Loss 0.2454124093055725\n",
      "[Training Epoch 9] Batch 1149, Loss 0.259063720703125\n",
      "[Training Epoch 9] Batch 1150, Loss 0.2688308656215668\n",
      "[Training Epoch 9] Batch 1151, Loss 0.1897057741880417\n",
      "[Training Epoch 9] Batch 1152, Loss 0.2564566731452942\n",
      "[Training Epoch 9] Batch 1153, Loss 0.24950970709323883\n",
      "[Training Epoch 9] Batch 1154, Loss 0.2552080750465393\n",
      "[Training Epoch 9] Batch 1155, Loss 0.2638077437877655\n",
      "[Training Epoch 9] Batch 1156, Loss 0.2621021866798401\n",
      "[Training Epoch 9] Batch 1157, Loss 0.2656456530094147\n",
      "[Training Epoch 9] Batch 1158, Loss 0.258402556180954\n",
      "[Training Epoch 9] Batch 1159, Loss 0.2658911645412445\n",
      "[Training Epoch 9] Batch 1160, Loss 0.2605109214782715\n",
      "[Training Epoch 9] Batch 1161, Loss 0.2668609321117401\n",
      "[Training Epoch 9] Batch 1162, Loss 0.26868584752082825\n",
      "[Training Epoch 9] Batch 1163, Loss 0.2594555616378784\n",
      "[Training Epoch 9] Batch 1164, Loss 0.24517345428466797\n",
      "[Training Epoch 9] Batch 1165, Loss 0.24571874737739563\n",
      "[Training Epoch 9] Batch 1166, Loss 0.31168681383132935\n",
      "[Training Epoch 9] Batch 1167, Loss 0.2533551752567291\n",
      "[Training Epoch 9] Batch 1168, Loss 0.25265344977378845\n",
      "[Training Epoch 9] Batch 1169, Loss 0.25515350699424744\n",
      "[Training Epoch 9] Batch 1170, Loss 0.254790335893631\n",
      "[Training Epoch 9] Batch 1171, Loss 0.2539268732070923\n",
      "[Training Epoch 9] Batch 1172, Loss 0.25311315059661865\n",
      "[Training Epoch 9] Batch 1173, Loss 0.23929256200790405\n",
      "[Training Epoch 9] Batch 1174, Loss 0.23827460408210754\n",
      "[Training Epoch 9] Batch 1175, Loss 0.24006301164627075\n",
      "[Training Epoch 9] Batch 1176, Loss 0.2521565556526184\n",
      "[Training Epoch 9] Batch 1177, Loss 0.2833316922187805\n",
      "[Training Epoch 9] Batch 1178, Loss 0.2265365719795227\n",
      "[Training Epoch 9] Batch 1179, Loss 0.2477281093597412\n",
      "[Training Epoch 9] Batch 1180, Loss 0.25468528270721436\n",
      "[Training Epoch 9] Batch 1181, Loss 0.2420414686203003\n",
      "[Training Epoch 9] Batch 1182, Loss 0.2477893829345703\n",
      "[Training Epoch 9] Batch 1183, Loss 0.24572408199310303\n",
      "[Training Epoch 9] Batch 1184, Loss 0.2633734941482544\n",
      "[Training Epoch 9] Batch 1185, Loss 0.2505374550819397\n",
      "[Training Epoch 9] Batch 1186, Loss 0.2627912163734436\n",
      "[Training Epoch 9] Batch 1187, Loss 0.25927525758743286\n",
      "[Training Epoch 9] Batch 1188, Loss 0.2455759197473526\n",
      "[Training Epoch 9] Batch 1189, Loss 0.24217988550662994\n",
      "[Training Epoch 9] Batch 1190, Loss 0.2669154405593872\n",
      "[Training Epoch 9] Batch 1191, Loss 0.28224992752075195\n",
      "[Training Epoch 9] Batch 1192, Loss 0.24577002227306366\n",
      "[Training Epoch 9] Batch 1193, Loss 0.23395775258541107\n",
      "[Training Epoch 9] Batch 1194, Loss 0.25514358282089233\n",
      "[Training Epoch 9] Batch 1195, Loss 0.28373944759368896\n",
      "[Training Epoch 9] Batch 1196, Loss 0.24339288473129272\n",
      "[Training Epoch 9] Batch 1197, Loss 0.2619674503803253\n",
      "[Training Epoch 9] Batch 1198, Loss 0.2732371687889099\n",
      "[Training Epoch 9] Batch 1199, Loss 0.23977848887443542\n",
      "[Training Epoch 9] Batch 1200, Loss 0.24666889011859894\n",
      "[Training Epoch 9] Batch 1201, Loss 0.25463175773620605\n",
      "[Training Epoch 9] Batch 1202, Loss 0.26525768637657166\n",
      "[Training Epoch 9] Batch 1203, Loss 0.27239692211151123\n",
      "[Training Epoch 9] Batch 1204, Loss 0.2673007845878601\n",
      "[Training Epoch 9] Batch 1205, Loss 0.2378130853176117\n",
      "[Training Epoch 9] Batch 1206, Loss 0.23073944449424744\n",
      "[Training Epoch 9] Batch 1207, Loss 0.27842792868614197\n",
      "[Training Epoch 9] Batch 1208, Loss 0.2499413788318634\n",
      "[Training Epoch 9] Batch 1209, Loss 0.23457011580467224\n",
      "[Training Epoch 9] Batch 1210, Loss 0.26117992401123047\n",
      "[Training Epoch 9] Batch 1211, Loss 0.24973152577877045\n",
      "[Training Epoch 9] Batch 1212, Loss 0.2797253131866455\n",
      "[Training Epoch 9] Batch 1213, Loss 0.26596713066101074\n",
      "[Training Epoch 9] Batch 1214, Loss 0.23488467931747437\n",
      "[Training Epoch 9] Batch 1215, Loss 0.24578610062599182\n",
      "[Training Epoch 9] Batch 1216, Loss 0.2542954683303833\n",
      "[Training Epoch 9] Batch 1217, Loss 0.2658436894416809\n",
      "[Training Epoch 9] Batch 1218, Loss 0.25325870513916016\n",
      "[Training Epoch 9] Batch 1219, Loss 0.25015178322792053\n",
      "[Training Epoch 9] Batch 1220, Loss 0.2330707311630249\n",
      "[Training Epoch 9] Batch 1221, Loss 0.2554485499858856\n",
      "[Training Epoch 9] Batch 1222, Loss 0.25792738795280457\n",
      "[Training Epoch 9] Batch 1223, Loss 0.23763751983642578\n",
      "[Training Epoch 9] Batch 1224, Loss 0.24154923856258392\n",
      "[Training Epoch 9] Batch 1225, Loss 0.21322928369045258\n",
      "[Training Epoch 9] Batch 1226, Loss 0.248019278049469\n",
      "[Training Epoch 9] Batch 1227, Loss 0.28904014825820923\n",
      "[Training Epoch 9] Batch 1228, Loss 0.2527933120727539\n",
      "[Training Epoch 9] Batch 1229, Loss 0.2372114211320877\n",
      "[Training Epoch 9] Batch 1230, Loss 0.2650308907032013\n",
      "[Training Epoch 9] Batch 1231, Loss 0.24261757731437683\n",
      "[Training Epoch 9] Batch 1232, Loss 0.27302032709121704\n",
      "[Training Epoch 9] Batch 1233, Loss 0.24636976420879364\n",
      "[Training Epoch 9] Batch 1234, Loss 0.23073112964630127\n",
      "[Training Epoch 9] Batch 1235, Loss 0.23673337697982788\n",
      "[Training Epoch 9] Batch 1236, Loss 0.2738061547279358\n",
      "[Training Epoch 9] Batch 1237, Loss 0.24929022789001465\n",
      "[Training Epoch 9] Batch 1238, Loss 0.23239131271839142\n",
      "[Training Epoch 9] Batch 1239, Loss 0.27663654088974\n",
      "[Training Epoch 9] Batch 1240, Loss 0.25131815671920776\n",
      "[Training Epoch 9] Batch 1241, Loss 0.2616313695907593\n",
      "[Training Epoch 9] Batch 1242, Loss 0.24682192504405975\n",
      "[Training Epoch 9] Batch 1243, Loss 0.2297401875257492\n",
      "[Training Epoch 9] Batch 1244, Loss 0.28328025341033936\n",
      "[Training Epoch 9] Batch 1245, Loss 0.27852946519851685\n",
      "[Training Epoch 9] Batch 1246, Loss 0.2540695071220398\n",
      "[Training Epoch 9] Batch 1247, Loss 0.2959134578704834\n",
      "[Training Epoch 9] Batch 1248, Loss 0.25314241647720337\n",
      "[Training Epoch 9] Batch 1249, Loss 0.2613432705402374\n",
      "[Training Epoch 9] Batch 1250, Loss 0.24617226421833038\n",
      "[Training Epoch 9] Batch 1251, Loss 0.27036041021347046\n",
      "[Training Epoch 9] Batch 1252, Loss 0.23179155588150024\n",
      "[Training Epoch 9] Batch 1253, Loss 0.2685808539390564\n",
      "[Training Epoch 9] Batch 1254, Loss 0.24447669088840485\n",
      "[Training Epoch 9] Batch 1255, Loss 0.2618846297264099\n",
      "[Training Epoch 9] Batch 1256, Loss 0.28690895438194275\n",
      "[Training Epoch 9] Batch 1257, Loss 0.2640570402145386\n",
      "[Training Epoch 9] Batch 1258, Loss 0.2440299093723297\n",
      "[Training Epoch 9] Batch 1259, Loss 0.23869696259498596\n",
      "[Training Epoch 9] Batch 1260, Loss 0.2284618467092514\n",
      "[Training Epoch 9] Batch 1261, Loss 0.2609815001487732\n",
      "[Training Epoch 9] Batch 1262, Loss 0.28583356738090515\n",
      "[Training Epoch 9] Batch 1263, Loss 0.24893894791603088\n",
      "[Training Epoch 9] Batch 1264, Loss 0.26154381036758423\n",
      "[Training Epoch 9] Batch 1265, Loss 0.25077128410339355\n",
      "[Training Epoch 9] Batch 1266, Loss 0.2821873724460602\n",
      "[Training Epoch 9] Batch 1267, Loss 0.26973217725753784\n",
      "[Training Epoch 9] Batch 1268, Loss 0.2560313045978546\n",
      "[Training Epoch 9] Batch 1269, Loss 0.2718527317047119\n",
      "[Training Epoch 9] Batch 1270, Loss 0.24268946051597595\n",
      "[Training Epoch 9] Batch 1271, Loss 0.2664850354194641\n",
      "[Training Epoch 9] Batch 1272, Loss 0.2534254789352417\n",
      "[Training Epoch 9] Batch 1273, Loss 0.2660641074180603\n",
      "[Training Epoch 9] Batch 1274, Loss 0.2658664584159851\n",
      "[Training Epoch 9] Batch 1275, Loss 0.23615801334381104\n",
      "[Training Epoch 9] Batch 1276, Loss 0.2383417934179306\n",
      "[Training Epoch 9] Batch 1277, Loss 0.23375163972377777\n",
      "[Training Epoch 9] Batch 1278, Loss 0.2576773762702942\n",
      "[Training Epoch 9] Batch 1279, Loss 0.25096815824508667\n",
      "[Training Epoch 9] Batch 1280, Loss 0.2453303039073944\n",
      "[Training Epoch 9] Batch 1281, Loss 0.24259856343269348\n",
      "[Training Epoch 9] Batch 1282, Loss 0.2607547342777252\n",
      "[Training Epoch 9] Batch 1283, Loss 0.26041167974472046\n",
      "[Training Epoch 9] Batch 1284, Loss 0.2715741991996765\n",
      "[Training Epoch 9] Batch 1285, Loss 0.2387288361787796\n",
      "[Training Epoch 9] Batch 1286, Loss 0.2674834132194519\n",
      "[Training Epoch 9] Batch 1287, Loss 0.27482014894485474\n",
      "[Training Epoch 9] Batch 1288, Loss 0.2591524124145508\n",
      "[Training Epoch 9] Batch 1289, Loss 0.19524255394935608\n",
      "[Training Epoch 9] Batch 1290, Loss 0.23417764902114868\n",
      "[Training Epoch 9] Batch 1291, Loss 0.28815674781799316\n",
      "[Training Epoch 9] Batch 1292, Loss 0.24773946404457092\n",
      "[Training Epoch 9] Batch 1293, Loss 0.2780337333679199\n",
      "[Training Epoch 9] Batch 1294, Loss 0.2531172037124634\n",
      "[Training Epoch 9] Batch 1295, Loss 0.2453528344631195\n",
      "[Training Epoch 9] Batch 1296, Loss 0.28670454025268555\n",
      "[Training Epoch 9] Batch 1297, Loss 0.2497643530368805\n",
      "[Training Epoch 9] Batch 1298, Loss 0.2604904770851135\n",
      "[Training Epoch 9] Batch 1299, Loss 0.24771207571029663\n",
      "[Training Epoch 9] Batch 1300, Loss 0.2799723148345947\n",
      "[Training Epoch 9] Batch 1301, Loss 0.25942784547805786\n",
      "[Training Epoch 9] Batch 1302, Loss 0.25819310545921326\n",
      "[Training Epoch 9] Batch 1303, Loss 0.26607346534729004\n",
      "[Training Epoch 9] Batch 1304, Loss 0.2226504534482956\n",
      "[Training Epoch 9] Batch 1305, Loss 0.28230953216552734\n",
      "[Training Epoch 9] Batch 1306, Loss 0.274652898311615\n",
      "[Training Epoch 9] Batch 1307, Loss 0.2544656991958618\n",
      "[Training Epoch 9] Batch 1308, Loss 0.2281164824962616\n",
      "[Training Epoch 9] Batch 1309, Loss 0.22176897525787354\n",
      "[Training Epoch 9] Batch 1310, Loss 0.29105067253112793\n",
      "[Training Epoch 9] Batch 1311, Loss 0.2564402222633362\n",
      "[Training Epoch 9] Batch 1312, Loss 0.24783840775489807\n",
      "[Training Epoch 9] Batch 1313, Loss 0.23967862129211426\n",
      "[Training Epoch 9] Batch 1314, Loss 0.292664498090744\n",
      "[Training Epoch 9] Batch 1315, Loss 0.2455638349056244\n",
      "[Training Epoch 9] Batch 1316, Loss 0.22855021059513092\n",
      "[Training Epoch 9] Batch 1317, Loss 0.23786744475364685\n",
      "[Training Epoch 9] Batch 1318, Loss 0.2551960051059723\n",
      "[Training Epoch 9] Batch 1319, Loss 0.23511740565299988\n",
      "[Training Epoch 9] Batch 1320, Loss 0.24668289721012115\n",
      "[Training Epoch 9] Batch 1321, Loss 0.2622653841972351\n",
      "[Training Epoch 9] Batch 1322, Loss 0.2546541392803192\n",
      "[Training Epoch 9] Batch 1323, Loss 0.2445494681596756\n",
      "[Training Epoch 9] Batch 1324, Loss 0.26614126563072205\n",
      "[Training Epoch 9] Batch 1325, Loss 0.27345260977745056\n",
      "[Training Epoch 9] Batch 1326, Loss 0.27833864092826843\n",
      "[Training Epoch 9] Batch 1327, Loss 0.26329776644706726\n",
      "[Training Epoch 9] Batch 1328, Loss 0.252556174993515\n",
      "[Training Epoch 9] Batch 1329, Loss 0.23455384373664856\n",
      "[Training Epoch 9] Batch 1330, Loss 0.27108415961265564\n",
      "[Training Epoch 9] Batch 1331, Loss 0.23114857077598572\n",
      "[Training Epoch 9] Batch 1332, Loss 0.24253660440444946\n",
      "[Training Epoch 9] Batch 1333, Loss 0.2525903582572937\n",
      "[Training Epoch 9] Batch 1334, Loss 0.2505526840686798\n",
      "[Training Epoch 9] Batch 1335, Loss 0.2835932970046997\n",
      "[Training Epoch 9] Batch 1336, Loss 0.2604965269565582\n",
      "[Training Epoch 9] Batch 1337, Loss 0.2511495351791382\n",
      "[Training Epoch 9] Batch 1338, Loss 0.24601289629936218\n",
      "[Training Epoch 9] Batch 1339, Loss 0.24983732402324677\n",
      "[Training Epoch 9] Batch 1340, Loss 0.2435305118560791\n",
      "[Training Epoch 9] Batch 1341, Loss 0.27151572704315186\n",
      "[Training Epoch 9] Batch 1342, Loss 0.26040685176849365\n",
      "[Training Epoch 9] Batch 1343, Loss 0.2642821669578552\n",
      "[Training Epoch 9] Batch 1344, Loss 0.27103689312934875\n",
      "[Training Epoch 9] Batch 1345, Loss 0.26725900173187256\n",
      "[Training Epoch 9] Batch 1346, Loss 0.26270154118537903\n",
      "[Training Epoch 9] Batch 1347, Loss 0.2114461362361908\n",
      "[Training Epoch 9] Batch 1348, Loss 0.22483348846435547\n",
      "[Training Epoch 9] Batch 1349, Loss 0.2657608985900879\n",
      "[Training Epoch 9] Batch 1350, Loss 0.2867588996887207\n",
      "[Training Epoch 9] Batch 1351, Loss 0.2750079333782196\n",
      "[Training Epoch 9] Batch 1352, Loss 0.2578987181186676\n",
      "[Training Epoch 9] Batch 1353, Loss 0.2464076429605484\n",
      "[Training Epoch 9] Batch 1354, Loss 0.23075193166732788\n",
      "[Training Epoch 9] Batch 1355, Loss 0.2427597939968109\n",
      "[Training Epoch 9] Batch 1356, Loss 0.2760133445262909\n",
      "[Training Epoch 9] Batch 1357, Loss 0.2242073118686676\n",
      "[Training Epoch 9] Batch 1358, Loss 0.2511363923549652\n",
      "[Training Epoch 9] Batch 1359, Loss 0.2661539912223816\n",
      "[Training Epoch 9] Batch 1360, Loss 0.23875539004802704\n",
      "[Training Epoch 9] Batch 1361, Loss 0.26018989086151123\n",
      "[Training Epoch 9] Batch 1362, Loss 0.23732300102710724\n",
      "[Training Epoch 9] Batch 1363, Loss 0.2569172978401184\n",
      "[Training Epoch 9] Batch 1364, Loss 0.2627064287662506\n",
      "[Training Epoch 9] Batch 1365, Loss 0.2491447627544403\n",
      "[Training Epoch 9] Batch 1366, Loss 0.2730526328086853\n",
      "[Training Epoch 9] Batch 1367, Loss 0.26310649514198303\n",
      "[Training Epoch 9] Batch 1368, Loss 0.2558491826057434\n",
      "[Training Epoch 9] Batch 1369, Loss 0.23395270109176636\n",
      "[Training Epoch 9] Batch 1370, Loss 0.2358439564704895\n",
      "[Training Epoch 9] Batch 1371, Loss 0.23017112910747528\n",
      "[Training Epoch 9] Batch 1372, Loss 0.2500673532485962\n",
      "[Training Epoch 9] Batch 1373, Loss 0.25629156827926636\n",
      "[Training Epoch 9] Batch 1374, Loss 0.28494876623153687\n",
      "[Training Epoch 9] Batch 1375, Loss 0.2728073298931122\n",
      "[Training Epoch 9] Batch 1376, Loss 0.280728816986084\n",
      "[Training Epoch 9] Batch 1377, Loss 0.2456507384777069\n",
      "[Training Epoch 9] Batch 1378, Loss 0.26353657245635986\n",
      "[Training Epoch 9] Batch 1379, Loss 0.2447969913482666\n",
      "[Training Epoch 9] Batch 1380, Loss 0.2147534191608429\n",
      "[Training Epoch 9] Batch 1381, Loss 0.25042855739593506\n",
      "[Training Epoch 9] Batch 1382, Loss 0.2572755813598633\n",
      "[Training Epoch 9] Batch 1383, Loss 0.2589191496372223\n",
      "[Training Epoch 9] Batch 1384, Loss 0.24844880402088165\n",
      "[Training Epoch 9] Batch 1385, Loss 0.2545456886291504\n",
      "[Training Epoch 9] Batch 1386, Loss 0.24814441800117493\n",
      "[Training Epoch 9] Batch 1387, Loss 0.2572600245475769\n",
      "[Training Epoch 9] Batch 1388, Loss 0.23705914616584778\n",
      "[Training Epoch 9] Batch 1389, Loss 0.21564368903636932\n",
      "[Training Epoch 9] Batch 1390, Loss 0.24313221871852875\n",
      "[Training Epoch 9] Batch 1391, Loss 0.25249361991882324\n",
      "[Training Epoch 9] Batch 1392, Loss 0.2408403903245926\n",
      "[Training Epoch 9] Batch 1393, Loss 0.3108174800872803\n",
      "[Training Epoch 9] Batch 1394, Loss 0.2945951223373413\n",
      "[Training Epoch 9] Batch 1395, Loss 0.2904045283794403\n",
      "[Training Epoch 9] Batch 1396, Loss 0.2546674311161041\n",
      "[Training Epoch 9] Batch 1397, Loss 0.259936660528183\n",
      "[Training Epoch 9] Batch 1398, Loss 0.24549588561058044\n",
      "[Training Epoch 9] Batch 1399, Loss 0.26195380091667175\n",
      "[Training Epoch 9] Batch 1400, Loss 0.20402230322360992\n",
      "[Training Epoch 9] Batch 1401, Loss 0.2267788201570511\n",
      "[Training Epoch 9] Batch 1402, Loss 0.2270500808954239\n",
      "[Training Epoch 9] Batch 1403, Loss 0.2600853443145752\n",
      "[Training Epoch 9] Batch 1404, Loss 0.26281893253326416\n",
      "[Training Epoch 9] Batch 1405, Loss 0.24871905148029327\n",
      "[Training Epoch 9] Batch 1406, Loss 0.2548878788948059\n",
      "[Training Epoch 9] Batch 1407, Loss 0.2782432436943054\n",
      "[Training Epoch 9] Batch 1408, Loss 0.2521924376487732\n",
      "[Training Epoch 9] Batch 1409, Loss 0.23966431617736816\n",
      "[Training Epoch 9] Batch 1410, Loss 0.2619668245315552\n",
      "[Training Epoch 9] Batch 1411, Loss 0.24295315146446228\n",
      "[Training Epoch 9] Batch 1412, Loss 0.23858210444450378\n",
      "[Training Epoch 9] Batch 1413, Loss 0.2533973157405853\n",
      "[Training Epoch 9] Batch 1414, Loss 0.2469799518585205\n",
      "[Training Epoch 9] Batch 1415, Loss 0.25428465008735657\n",
      "[Training Epoch 9] Batch 1416, Loss 0.27231311798095703\n",
      "[Training Epoch 9] Batch 1417, Loss 0.2506769895553589\n",
      "[Training Epoch 9] Batch 1418, Loss 0.2894245684146881\n",
      "[Training Epoch 9] Batch 1419, Loss 0.23614346981048584\n",
      "[Training Epoch 9] Batch 1420, Loss 0.22751837968826294\n",
      "[Training Epoch 9] Batch 1421, Loss 0.26455584168434143\n",
      "[Training Epoch 9] Batch 1422, Loss 0.25857728719711304\n",
      "[Training Epoch 9] Batch 1423, Loss 0.2437799572944641\n",
      "[Training Epoch 9] Batch 1424, Loss 0.25609368085861206\n",
      "[Training Epoch 9] Batch 1425, Loss 0.24178019165992737\n",
      "[Training Epoch 9] Batch 1426, Loss 0.23315151035785675\n",
      "[Training Epoch 9] Batch 1427, Loss 0.24230718612670898\n",
      "[Training Epoch 9] Batch 1428, Loss 0.2779921591281891\n",
      "[Training Epoch 9] Batch 1429, Loss 0.2553488314151764\n",
      "[Training Epoch 9] Batch 1430, Loss 0.27774518728256226\n",
      "[Training Epoch 9] Batch 1431, Loss 0.24756354093551636\n",
      "[Training Epoch 9] Batch 1432, Loss 0.26437872648239136\n",
      "[Training Epoch 9] Batch 1433, Loss 0.2615754008293152\n",
      "[Training Epoch 9] Batch 1434, Loss 0.20151835680007935\n",
      "[Training Epoch 9] Batch 1435, Loss 0.268437922000885\n",
      "[Training Epoch 9] Batch 1436, Loss 0.252675324678421\n",
      "[Training Epoch 9] Batch 1437, Loss 0.23999512195587158\n",
      "[Training Epoch 9] Batch 1438, Loss 0.2712264955043793\n",
      "[Training Epoch 9] Batch 1439, Loss 0.2702767252922058\n",
      "[Training Epoch 9] Batch 1440, Loss 0.24488720297813416\n",
      "[Training Epoch 9] Batch 1441, Loss 0.2493952065706253\n",
      "[Training Epoch 9] Batch 1442, Loss 0.2698194086551666\n",
      "[Training Epoch 9] Batch 1443, Loss 0.24966280162334442\n",
      "[Training Epoch 9] Batch 1444, Loss 0.2552974224090576\n",
      "[Training Epoch 9] Batch 1445, Loss 0.23282930254936218\n",
      "[Training Epoch 9] Batch 1446, Loss 0.25762051343917847\n",
      "[Training Epoch 9] Batch 1447, Loss 0.2721966505050659\n",
      "[Training Epoch 9] Batch 1448, Loss 0.2705782651901245\n",
      "[Training Epoch 9] Batch 1449, Loss 0.25563275814056396\n",
      "[Training Epoch 9] Batch 1450, Loss 0.24283477663993835\n",
      "[Training Epoch 9] Batch 1451, Loss 0.2387424111366272\n",
      "[Training Epoch 9] Batch 1452, Loss 0.2464125156402588\n",
      "[Training Epoch 9] Batch 1453, Loss 0.2877674996852875\n",
      "[Training Epoch 9] Batch 1454, Loss 0.26266056299209595\n",
      "[Training Epoch 9] Batch 1455, Loss 0.26991701126098633\n",
      "[Training Epoch 9] Batch 1456, Loss 0.22190776467323303\n",
      "[Training Epoch 9] Batch 1457, Loss 0.2361280918121338\n",
      "[Training Epoch 9] Batch 1458, Loss 0.2543007731437683\n",
      "[Training Epoch 9] Batch 1459, Loss 0.28211504220962524\n",
      "[Training Epoch 9] Batch 1460, Loss 0.24600455164909363\n",
      "[Training Epoch 9] Batch 1461, Loss 0.26531413197517395\n",
      "[Training Epoch 9] Batch 1462, Loss 0.23747017979621887\n",
      "[Training Epoch 9] Batch 1463, Loss 0.2326975166797638\n",
      "[Training Epoch 9] Batch 1464, Loss 0.23723234236240387\n",
      "[Training Epoch 9] Batch 1465, Loss 0.252908319234848\n",
      "[Training Epoch 9] Batch 1466, Loss 0.25196972489356995\n",
      "[Training Epoch 9] Batch 1467, Loss 0.2610359787940979\n",
      "[Training Epoch 9] Batch 1468, Loss 0.2582350969314575\n",
      "[Training Epoch 9] Batch 1469, Loss 0.2714317739009857\n",
      "[Training Epoch 9] Batch 1470, Loss 0.23585818707942963\n",
      "[Training Epoch 9] Batch 1471, Loss 0.2655387818813324\n",
      "[Training Epoch 9] Batch 1472, Loss 0.2592019736766815\n",
      "[Training Epoch 9] Batch 1473, Loss 0.25587403774261475\n",
      "[Training Epoch 9] Batch 1474, Loss 0.27443188428878784\n",
      "[Training Epoch 9] Batch 1475, Loss 0.23559734225273132\n",
      "[Training Epoch 9] Batch 1476, Loss 0.25673210620880127\n",
      "[Training Epoch 9] Batch 1477, Loss 0.26403284072875977\n",
      "[Training Epoch 9] Batch 1478, Loss 0.25042402744293213\n",
      "[Training Epoch 9] Batch 1479, Loss 0.2470196783542633\n",
      "[Training Epoch 9] Batch 1480, Loss 0.24269059300422668\n",
      "[Training Epoch 9] Batch 1481, Loss 0.2633538842201233\n",
      "[Training Epoch 9] Batch 1482, Loss 0.24291366338729858\n",
      "[Training Epoch 9] Batch 1483, Loss 0.2593740224838257\n",
      "[Training Epoch 9] Batch 1484, Loss 0.26519933342933655\n",
      "[Training Epoch 9] Batch 1485, Loss 0.24457062780857086\n",
      "[Training Epoch 9] Batch 1486, Loss 0.24911059439182281\n",
      "[Training Epoch 9] Batch 1487, Loss 0.2323458045721054\n",
      "[Training Epoch 9] Batch 1488, Loss 0.2790425419807434\n",
      "[Training Epoch 9] Batch 1489, Loss 0.25899797677993774\n",
      "[Training Epoch 9] Batch 1490, Loss 0.23710542917251587\n",
      "[Training Epoch 9] Batch 1491, Loss 0.2652355432510376\n",
      "[Training Epoch 9] Batch 1492, Loss 0.22431981563568115\n",
      "[Training Epoch 9] Batch 1493, Loss 0.2333485335111618\n",
      "[Training Epoch 9] Batch 1494, Loss 0.2537035346031189\n",
      "[Training Epoch 9] Batch 1495, Loss 0.28092867136001587\n",
      "[Training Epoch 9] Batch 1496, Loss 0.26004546880722046\n",
      "[Training Epoch 9] Batch 1497, Loss 0.2330883890390396\n",
      "[Training Epoch 9] Batch 1498, Loss 0.23345035314559937\n",
      "[Training Epoch 9] Batch 1499, Loss 0.2472171187400818\n",
      "[Training Epoch 9] Batch 1500, Loss 0.2500525414943695\n",
      "[Training Epoch 9] Batch 1501, Loss 0.25399869680404663\n",
      "[Training Epoch 9] Batch 1502, Loss 0.26082170009613037\n",
      "[Training Epoch 9] Batch 1503, Loss 0.240973562002182\n",
      "[Training Epoch 9] Batch 1504, Loss 0.25744926929473877\n",
      "[Training Epoch 9] Batch 1505, Loss 0.2762751579284668\n",
      "[Training Epoch 9] Batch 1506, Loss 0.2653273344039917\n",
      "[Training Epoch 9] Batch 1507, Loss 0.21975217759609222\n",
      "[Training Epoch 9] Batch 1508, Loss 0.2823052704334259\n",
      "[Training Epoch 9] Batch 1509, Loss 0.2674686312675476\n",
      "[Training Epoch 9] Batch 1510, Loss 0.2568092942237854\n",
      "[Training Epoch 9] Batch 1511, Loss 0.24716311693191528\n",
      "[Training Epoch 9] Batch 1512, Loss 0.2611112892627716\n",
      "[Training Epoch 9] Batch 1513, Loss 0.28005215525627136\n",
      "[Training Epoch 9] Batch 1514, Loss 0.21071921288967133\n",
      "[Training Epoch 9] Batch 1515, Loss 0.26321130990982056\n",
      "[Training Epoch 9] Batch 1516, Loss 0.24399463832378387\n",
      "[Training Epoch 9] Batch 1517, Loss 0.23796014487743378\n",
      "[Training Epoch 9] Batch 1518, Loss 0.2616938352584839\n",
      "[Training Epoch 9] Batch 1519, Loss 0.282706618309021\n",
      "[Training Epoch 9] Batch 1520, Loss 0.2365347445011139\n",
      "[Training Epoch 9] Batch 1521, Loss 0.23563139140605927\n",
      "[Training Epoch 9] Batch 1522, Loss 0.23904424905776978\n",
      "[Training Epoch 9] Batch 1523, Loss 0.24967466294765472\n",
      "[Training Epoch 9] Batch 1524, Loss 0.24552080035209656\n",
      "[Training Epoch 9] Batch 1525, Loss 0.2552779018878937\n",
      "[Training Epoch 9] Batch 1526, Loss 0.27926257252693176\n",
      "[Training Epoch 9] Batch 1527, Loss 0.25572776794433594\n",
      "[Training Epoch 9] Batch 1528, Loss 0.25775137543678284\n",
      "[Training Epoch 9] Batch 1529, Loss 0.25562530755996704\n",
      "[Training Epoch 9] Batch 1530, Loss 0.24073974788188934\n",
      "[Training Epoch 9] Batch 1531, Loss 0.26512688398361206\n",
      "[Training Epoch 9] Batch 1532, Loss 0.25754308700561523\n",
      "[Training Epoch 9] Batch 1533, Loss 0.2307136058807373\n",
      "[Training Epoch 9] Batch 1534, Loss 0.2196698635816574\n",
      "[Training Epoch 9] Batch 1535, Loss 0.21763254702091217\n",
      "[Training Epoch 9] Batch 1536, Loss 0.26973843574523926\n",
      "[Training Epoch 9] Batch 1537, Loss 0.23484504222869873\n",
      "[Training Epoch 9] Batch 1538, Loss 0.27715161442756653\n",
      "[Training Epoch 9] Batch 1539, Loss 0.23351624608039856\n",
      "[Training Epoch 9] Batch 1540, Loss 0.25119835138320923\n",
      "[Training Epoch 9] Batch 1541, Loss 0.25154271721839905\n",
      "[Training Epoch 9] Batch 1542, Loss 0.24917802214622498\n",
      "[Training Epoch 9] Batch 1543, Loss 0.2589080035686493\n",
      "[Training Epoch 9] Batch 1544, Loss 0.279593288898468\n",
      "[Training Epoch 9] Batch 1545, Loss 0.237066850066185\n",
      "[Training Epoch 9] Batch 1546, Loss 0.2594984769821167\n",
      "[Training Epoch 9] Batch 1547, Loss 0.25491923093795776\n",
      "[Training Epoch 9] Batch 1548, Loss 0.23743703961372375\n",
      "[Training Epoch 9] Batch 1549, Loss 0.23779498040676117\n",
      "[Training Epoch 9] Batch 1550, Loss 0.26167815923690796\n",
      "[Training Epoch 9] Batch 1551, Loss 0.2661174535751343\n",
      "[Training Epoch 9] Batch 1552, Loss 0.2600923180580139\n",
      "[Training Epoch 9] Batch 1553, Loss 0.25294923782348633\n",
      "[Training Epoch 9] Batch 1554, Loss 0.23767398297786713\n",
      "[Training Epoch 9] Batch 1555, Loss 0.26450037956237793\n",
      "[Training Epoch 9] Batch 1556, Loss 0.2740600109100342\n",
      "[Training Epoch 9] Batch 1557, Loss 0.2626838684082031\n",
      "[Training Epoch 9] Batch 1558, Loss 0.23025351762771606\n",
      "[Training Epoch 9] Batch 1559, Loss 0.23307080566883087\n",
      "[Training Epoch 9] Batch 1560, Loss 0.2610958218574524\n",
      "[Training Epoch 9] Batch 1561, Loss 0.2540401220321655\n",
      "[Training Epoch 9] Batch 1562, Loss 0.22567078471183777\n",
      "[Training Epoch 9] Batch 1563, Loss 0.2483101785182953\n",
      "[Training Epoch 9] Batch 1564, Loss 0.24502494931221008\n",
      "[Training Epoch 9] Batch 1565, Loss 0.22127318382263184\n",
      "[Training Epoch 9] Batch 1566, Loss 0.27933359146118164\n",
      "[Training Epoch 9] Batch 1567, Loss 0.26592886447906494\n",
      "[Training Epoch 9] Batch 1568, Loss 0.3047035336494446\n",
      "[Training Epoch 9] Batch 1569, Loss 0.25708550214767456\n",
      "[Training Epoch 9] Batch 1570, Loss 0.23003627359867096\n",
      "[Training Epoch 9] Batch 1571, Loss 0.26041388511657715\n",
      "[Training Epoch 9] Batch 1572, Loss 0.22990401089191437\n",
      "[Training Epoch 9] Batch 1573, Loss 0.25010350346565247\n",
      "[Training Epoch 9] Batch 1574, Loss 0.2413157820701599\n",
      "[Training Epoch 9] Batch 1575, Loss 0.27449706196784973\n",
      "[Training Epoch 9] Batch 1576, Loss 0.279887855052948\n",
      "[Training Epoch 9] Batch 1577, Loss 0.22359105944633484\n",
      "[Training Epoch 9] Batch 1578, Loss 0.26763367652893066\n",
      "[Training Epoch 9] Batch 1579, Loss 0.2630632221698761\n",
      "[Training Epoch 9] Batch 1580, Loss 0.23270754516124725\n",
      "[Training Epoch 9] Batch 1581, Loss 0.2666476368904114\n",
      "[Training Epoch 9] Batch 1582, Loss 0.272111713886261\n",
      "[Training Epoch 9] Batch 1583, Loss 0.2625817060470581\n",
      "[Training Epoch 9] Batch 1584, Loss 0.24870403110980988\n",
      "[Training Epoch 9] Batch 1585, Loss 0.2529478073120117\n",
      "[Training Epoch 9] Batch 1586, Loss 0.2441677302122116\n",
      "[Training Epoch 9] Batch 1587, Loss 0.27545875310897827\n",
      "[Training Epoch 9] Batch 1588, Loss 0.23988807201385498\n",
      "[Training Epoch 9] Batch 1589, Loss 0.2445298731327057\n",
      "[Training Epoch 9] Batch 1590, Loss 0.26386743783950806\n",
      "[Training Epoch 9] Batch 1591, Loss 0.24621173739433289\n",
      "[Training Epoch 9] Batch 1592, Loss 0.2512223720550537\n",
      "[Training Epoch 9] Batch 1593, Loss 0.2609100043773651\n",
      "[Training Epoch 9] Batch 1594, Loss 0.24960437417030334\n",
      "[Training Epoch 9] Batch 1595, Loss 0.2670241892337799\n",
      "[Training Epoch 9] Batch 1596, Loss 0.2412586510181427\n",
      "[Training Epoch 9] Batch 1597, Loss 0.26164305210113525\n",
      "[Training Epoch 9] Batch 1598, Loss 0.25282275676727295\n",
      "[Training Epoch 9] Batch 1599, Loss 0.2540457844734192\n",
      "[Training Epoch 9] Batch 1600, Loss 0.2719043493270874\n",
      "[Training Epoch 9] Batch 1601, Loss 0.23071157932281494\n",
      "[Training Epoch 9] Batch 1602, Loss 0.2812514305114746\n",
      "[Training Epoch 9] Batch 1603, Loss 0.2613229751586914\n",
      "[Training Epoch 9] Batch 1604, Loss 0.25132834911346436\n",
      "[Training Epoch 9] Batch 1605, Loss 0.2585165500640869\n",
      "[Training Epoch 9] Batch 1606, Loss 0.24976053833961487\n",
      "[Training Epoch 9] Batch 1607, Loss 0.24808691442012787\n",
      "[Training Epoch 9] Batch 1608, Loss 0.24130922555923462\n",
      "[Training Epoch 9] Batch 1609, Loss 0.24729996919631958\n",
      "[Training Epoch 9] Batch 1610, Loss 0.25920742750167847\n",
      "[Training Epoch 9] Batch 1611, Loss 0.2573118805885315\n",
      "[Training Epoch 9] Batch 1612, Loss 0.2609495520591736\n",
      "[Training Epoch 9] Batch 1613, Loss 0.2326151430606842\n",
      "[Training Epoch 9] Batch 1614, Loss 0.2550000548362732\n",
      "[Training Epoch 9] Batch 1615, Loss 0.2356002777814865\n",
      "[Training Epoch 9] Batch 1616, Loss 0.2670968174934387\n",
      "[Training Epoch 9] Batch 1617, Loss 0.228509783744812\n",
      "[Training Epoch 9] Batch 1618, Loss 0.2560592591762543\n",
      "[Training Epoch 9] Batch 1619, Loss 0.23677954077720642\n",
      "[Training Epoch 9] Batch 1620, Loss 0.2607918977737427\n",
      "[Training Epoch 9] Batch 1621, Loss 0.25506168603897095\n",
      "[Training Epoch 9] Batch 1622, Loss 0.2470836490392685\n",
      "[Training Epoch 9] Batch 1623, Loss 0.25862258672714233\n",
      "[Training Epoch 9] Batch 1624, Loss 0.2587134838104248\n",
      "[Training Epoch 9] Batch 1625, Loss 0.25546059012413025\n",
      "[Training Epoch 9] Batch 1626, Loss 0.25089016556739807\n",
      "[Training Epoch 9] Batch 1627, Loss 0.2704097628593445\n",
      "[Training Epoch 9] Batch 1628, Loss 0.26055628061294556\n",
      "[Training Epoch 9] Batch 1629, Loss 0.25759345293045044\n",
      "[Training Epoch 9] Batch 1630, Loss 0.2494341880083084\n",
      "[Training Epoch 9] Batch 1631, Loss 0.24257147312164307\n",
      "[Training Epoch 9] Batch 1632, Loss 0.2592425048351288\n",
      "[Training Epoch 9] Batch 1633, Loss 0.24618002772331238\n",
      "[Training Epoch 9] Batch 1634, Loss 0.2592426538467407\n",
      "[Training Epoch 9] Batch 1635, Loss 0.2349022924900055\n",
      "[Training Epoch 9] Batch 1636, Loss 0.25637125968933105\n",
      "[Training Epoch 9] Batch 1637, Loss 0.2546929121017456\n",
      "[Training Epoch 9] Batch 1638, Loss 0.26449495553970337\n",
      "[Training Epoch 9] Batch 1639, Loss 0.25458547472953796\n",
      "[Training Epoch 9] Batch 1640, Loss 0.2682664692401886\n",
      "[Training Epoch 9] Batch 1641, Loss 0.25276902318000793\n",
      "[Training Epoch 9] Batch 1642, Loss 0.26386725902557373\n",
      "[Training Epoch 9] Batch 1643, Loss 0.23977413773536682\n",
      "[Training Epoch 9] Batch 1644, Loss 0.24757012724876404\n",
      "[Training Epoch 9] Batch 1645, Loss 0.253018856048584\n",
      "[Training Epoch 9] Batch 1646, Loss 0.2878454625606537\n",
      "[Training Epoch 9] Batch 1647, Loss 0.24575820565223694\n",
      "[Training Epoch 9] Batch 1648, Loss 0.28175872564315796\n",
      "[Training Epoch 9] Batch 1649, Loss 0.24617820978164673\n",
      "[Training Epoch 9] Batch 1650, Loss 0.29399919509887695\n",
      "[Training Epoch 9] Batch 1651, Loss 0.26035189628601074\n",
      "[Training Epoch 9] Batch 1652, Loss 0.23786307871341705\n",
      "[Training Epoch 9] Batch 1653, Loss 0.25708115100860596\n",
      "[Training Epoch 9] Batch 1654, Loss 0.2833714187145233\n",
      "[Training Epoch 9] Batch 1655, Loss 0.28757429122924805\n",
      "[Training Epoch 9] Batch 1656, Loss 0.22564560174942017\n",
      "[Training Epoch 9] Batch 1657, Loss 0.26092854142189026\n",
      "[Training Epoch 9] Batch 1658, Loss 0.2766314744949341\n",
      "[Training Epoch 9] Batch 1659, Loss 0.24211743474006653\n",
      "[Training Epoch 9] Batch 1660, Loss 0.24640828371047974\n",
      "[Training Epoch 9] Batch 1661, Loss 0.2819315493106842\n",
      "[Training Epoch 9] Batch 1662, Loss 0.24547791481018066\n",
      "[Training Epoch 9] Batch 1663, Loss 0.23615816235542297\n",
      "[Training Epoch 9] Batch 1664, Loss 0.2636291980743408\n",
      "[Training Epoch 9] Batch 1665, Loss 0.2658305764198303\n",
      "[Training Epoch 9] Batch 1666, Loss 0.2480282187461853\n",
      "[Training Epoch 9] Batch 1667, Loss 0.2596072256565094\n",
      "[Training Epoch 9] Batch 1668, Loss 0.2374020218849182\n",
      "[Training Epoch 9] Batch 1669, Loss 0.2357865869998932\n",
      "[Training Epoch 9] Batch 1670, Loss 0.2226414680480957\n",
      "[Training Epoch 9] Batch 1671, Loss 0.24705740809440613\n",
      "[Training Epoch 9] Batch 1672, Loss 0.2427748739719391\n",
      "[Training Epoch 9] Batch 1673, Loss 0.2741297483444214\n",
      "[Training Epoch 9] Batch 1674, Loss 0.2196417599916458\n",
      "[Training Epoch 9] Batch 1675, Loss 0.24588100612163544\n",
      "[Training Epoch 9] Batch 1676, Loss 0.2561420798301697\n",
      "[Training Epoch 9] Batch 1677, Loss 0.27433133125305176\n",
      "[Training Epoch 9] Batch 1678, Loss 0.2711910009384155\n",
      "[Training Epoch 9] Batch 1679, Loss 0.22905941307544708\n",
      "[Training Epoch 9] Batch 1680, Loss 0.267291396856308\n",
      "[Training Epoch 9] Batch 1681, Loss 0.23756302893161774\n",
      "[Training Epoch 9] Batch 1682, Loss 0.2399836927652359\n",
      "[Training Epoch 9] Batch 1683, Loss 0.24251145124435425\n",
      "[Training Epoch 9] Batch 1684, Loss 0.2543362081050873\n",
      "[Training Epoch 9] Batch 1685, Loss 0.2522415816783905\n",
      "[Training Epoch 9] Batch 1686, Loss 0.23186475038528442\n",
      "[Training Epoch 9] Batch 1687, Loss 0.24400775134563446\n",
      "[Training Epoch 9] Batch 1688, Loss 0.2308761477470398\n",
      "[Training Epoch 9] Batch 1689, Loss 0.27082982659339905\n",
      "[Training Epoch 9] Batch 1690, Loss 0.2384093701839447\n",
      "[Training Epoch 9] Batch 1691, Loss 0.2448262721300125\n",
      "[Training Epoch 9] Batch 1692, Loss 0.23850327730178833\n",
      "[Training Epoch 9] Batch 1693, Loss 0.2475845068693161\n",
      "[Training Epoch 9] Batch 1694, Loss 0.2400221824645996\n",
      "[Training Epoch 9] Batch 1695, Loss 0.26159632205963135\n",
      "[Training Epoch 9] Batch 1696, Loss 0.2743528485298157\n",
      "[Training Epoch 9] Batch 1697, Loss 0.2496677041053772\n",
      "[Training Epoch 9] Batch 1698, Loss 0.25287288427352905\n",
      "[Training Epoch 9] Batch 1699, Loss 0.23317289352416992\n",
      "[Training Epoch 9] Batch 1700, Loss 0.25380945205688477\n",
      "[Training Epoch 9] Batch 1701, Loss 0.24483124911785126\n",
      "[Training Epoch 9] Batch 1702, Loss 0.28905948996543884\n",
      "[Training Epoch 9] Batch 1703, Loss 0.23955529928207397\n",
      "[Training Epoch 9] Batch 1704, Loss 0.2779289484024048\n",
      "[Training Epoch 9] Batch 1705, Loss 0.2777993977069855\n",
      "[Training Epoch 9] Batch 1706, Loss 0.24619336426258087\n",
      "[Training Epoch 9] Batch 1707, Loss 0.2808910608291626\n",
      "[Training Epoch 9] Batch 1708, Loss 0.26581698656082153\n",
      "[Training Epoch 9] Batch 1709, Loss 0.24579937756061554\n",
      "[Training Epoch 9] Batch 1710, Loss 0.251354843378067\n",
      "[Training Epoch 9] Batch 1711, Loss 0.2502966821193695\n",
      "[Training Epoch 9] Batch 1712, Loss 0.25321298837661743\n",
      "[Training Epoch 9] Batch 1713, Loss 0.2448059618473053\n",
      "[Training Epoch 9] Batch 1714, Loss 0.27375370264053345\n",
      "[Training Epoch 9] Batch 1715, Loss 0.2467690408229828\n",
      "[Training Epoch 9] Batch 1716, Loss 0.2720531225204468\n",
      "[Training Epoch 9] Batch 1717, Loss 0.2346782237291336\n",
      "[Training Epoch 9] Batch 1718, Loss 0.25404614210128784\n",
      "[Training Epoch 9] Batch 1719, Loss 0.24410861730575562\n",
      "[Training Epoch 9] Batch 1720, Loss 0.24207571148872375\n",
      "[Training Epoch 9] Batch 1721, Loss 0.2789801061153412\n",
      "[Training Epoch 9] Batch 1722, Loss 0.24973803758621216\n",
      "[Training Epoch 9] Batch 1723, Loss 0.27176111936569214\n",
      "[Training Epoch 9] Batch 1724, Loss 0.291238397359848\n",
      "[Training Epoch 9] Batch 1725, Loss 0.24872225522994995\n",
      "[Training Epoch 9] Batch 1726, Loss 0.26036131381988525\n",
      "[Training Epoch 9] Batch 1727, Loss 0.24200397729873657\n",
      "[Training Epoch 9] Batch 1728, Loss 0.2634451389312744\n",
      "[Training Epoch 9] Batch 1729, Loss 0.24821899831295013\n",
      "[Training Epoch 9] Batch 1730, Loss 0.23940299451351166\n",
      "[Training Epoch 9] Batch 1731, Loss 0.2585127353668213\n",
      "[Training Epoch 9] Batch 1732, Loss 0.286251038312912\n",
      "[Training Epoch 9] Batch 1733, Loss 0.26046210527420044\n",
      "[Training Epoch 9] Batch 1734, Loss 0.24214208126068115\n",
      "[Training Epoch 9] Batch 1735, Loss 0.26917240023612976\n",
      "[Training Epoch 9] Batch 1736, Loss 0.24680963158607483\n",
      "[Training Epoch 9] Batch 1737, Loss 0.2570398151874542\n",
      "[Training Epoch 9] Batch 1738, Loss 0.22302421927452087\n",
      "[Training Epoch 9] Batch 1739, Loss 0.2424311637878418\n",
      "[Training Epoch 9] Batch 1740, Loss 0.2470787912607193\n",
      "[Training Epoch 9] Batch 1741, Loss 0.24869418144226074\n",
      "[Training Epoch 9] Batch 1742, Loss 0.242727592587471\n",
      "[Training Epoch 9] Batch 1743, Loss 0.2895404100418091\n",
      "[Training Epoch 9] Batch 1744, Loss 0.25250324606895447\n",
      "[Training Epoch 9] Batch 1745, Loss 0.2920047342777252\n",
      "[Training Epoch 9] Batch 1746, Loss 0.2486991137266159\n",
      "[Training Epoch 9] Batch 1747, Loss 0.22950926423072815\n",
      "[Training Epoch 9] Batch 1748, Loss 0.24910402297973633\n",
      "[Training Epoch 9] Batch 1749, Loss 0.2619779109954834\n",
      "[Training Epoch 9] Batch 1750, Loss 0.2320694625377655\n",
      "[Training Epoch 9] Batch 1751, Loss 0.22962364554405212\n",
      "[Training Epoch 9] Batch 1752, Loss 0.2507791817188263\n",
      "[Training Epoch 9] Batch 1753, Loss 0.24795091152191162\n",
      "[Training Epoch 9] Batch 1754, Loss 0.27303093671798706\n",
      "[Training Epoch 9] Batch 1755, Loss 0.27456408739089966\n",
      "[Training Epoch 9] Batch 1756, Loss 0.2627485990524292\n",
      "[Training Epoch 9] Batch 1757, Loss 0.2602161765098572\n",
      "[Training Epoch 9] Batch 1758, Loss 0.279995858669281\n",
      "[Training Epoch 9] Batch 1759, Loss 0.2395864874124527\n",
      "[Training Epoch 9] Batch 1760, Loss 0.25444141030311584\n",
      "[Training Epoch 9] Batch 1761, Loss 0.2546772360801697\n",
      "[Training Epoch 9] Batch 1762, Loss 0.25283294916152954\n",
      "[Training Epoch 9] Batch 1763, Loss 0.2339339256286621\n",
      "[Training Epoch 9] Batch 1764, Loss 0.2617676258087158\n",
      "[Training Epoch 9] Batch 1765, Loss 0.2105153650045395\n",
      "[Training Epoch 9] Batch 1766, Loss 0.2567443549633026\n",
      "[Training Epoch 9] Batch 1767, Loss 0.2565256953239441\n",
      "[Training Epoch 9] Batch 1768, Loss 0.2920553684234619\n",
      "[Training Epoch 9] Batch 1769, Loss 0.2369743138551712\n",
      "[Training Epoch 9] Batch 1770, Loss 0.25478002429008484\n",
      "[Training Epoch 9] Batch 1771, Loss 0.23704299330711365\n",
      "[Training Epoch 9] Batch 1772, Loss 0.242278054356575\n",
      "[Training Epoch 9] Batch 1773, Loss 0.2540925145149231\n",
      "[Training Epoch 9] Batch 1774, Loss 0.26077237725257874\n",
      "[Training Epoch 9] Batch 1775, Loss 0.2459881752729416\n",
      "[Training Epoch 9] Batch 1776, Loss 0.27166318893432617\n",
      "[Training Epoch 9] Batch 1777, Loss 0.2375226616859436\n",
      "[Training Epoch 9] Batch 1778, Loss 0.2542710304260254\n",
      "[Training Epoch 9] Batch 1779, Loss 0.2239946573972702\n",
      "[Training Epoch 9] Batch 1780, Loss 0.2826443910598755\n",
      "[Training Epoch 9] Batch 1781, Loss 0.22131749987602234\n",
      "[Training Epoch 9] Batch 1782, Loss 0.254423588514328\n",
      "[Training Epoch 9] Batch 1783, Loss 0.24155914783477783\n",
      "[Training Epoch 9] Batch 1784, Loss 0.23756825923919678\n",
      "[Training Epoch 9] Batch 1785, Loss 0.2607012987136841\n",
      "[Training Epoch 9] Batch 1786, Loss 0.26212257146835327\n",
      "[Training Epoch 9] Batch 1787, Loss 0.2620292603969574\n",
      "[Training Epoch 9] Batch 1788, Loss 0.23900750279426575\n",
      "[Training Epoch 9] Batch 1789, Loss 0.23027947545051575\n",
      "[Training Epoch 9] Batch 1790, Loss 0.2674548625946045\n",
      "[Training Epoch 9] Batch 1791, Loss 0.2825324535369873\n",
      "[Training Epoch 9] Batch 1792, Loss 0.22943933308124542\n",
      "[Training Epoch 9] Batch 1793, Loss 0.2674027681350708\n",
      "[Training Epoch 9] Batch 1794, Loss 0.25844478607177734\n",
      "[Training Epoch 9] Batch 1795, Loss 0.25134021043777466\n",
      "[Training Epoch 9] Batch 1796, Loss 0.23030716180801392\n",
      "[Training Epoch 9] Batch 1797, Loss 0.24184399843215942\n",
      "[Training Epoch 9] Batch 1798, Loss 0.25478965044021606\n",
      "[Training Epoch 9] Batch 1799, Loss 0.2276356965303421\n",
      "[Training Epoch 9] Batch 1800, Loss 0.2340262234210968\n",
      "[Training Epoch 9] Batch 1801, Loss 0.22291827201843262\n",
      "[Training Epoch 9] Batch 1802, Loss 0.2786478102207184\n",
      "[Training Epoch 9] Batch 1803, Loss 0.2611980140209198\n",
      "[Training Epoch 9] Batch 1804, Loss 0.24799604713916779\n",
      "[Training Epoch 9] Batch 1805, Loss 0.26887965202331543\n",
      "[Training Epoch 9] Batch 1806, Loss 0.28145405650138855\n",
      "[Training Epoch 9] Batch 1807, Loss 0.2420623004436493\n",
      "[Training Epoch 9] Batch 1808, Loss 0.2703966200351715\n",
      "[Training Epoch 9] Batch 1809, Loss 0.28526216745376587\n",
      "[Training Epoch 9] Batch 1810, Loss 0.24416981637477875\n",
      "[Training Epoch 9] Batch 1811, Loss 0.23059436678886414\n",
      "[Training Epoch 9] Batch 1812, Loss 0.2651425004005432\n",
      "[Training Epoch 9] Batch 1813, Loss 0.2206561118364334\n",
      "[Training Epoch 9] Batch 1814, Loss 0.24396269023418427\n",
      "[Training Epoch 9] Batch 1815, Loss 0.24990345537662506\n",
      "[Training Epoch 9] Batch 1816, Loss 0.21749691665172577\n",
      "[Training Epoch 9] Batch 1817, Loss 0.2621208429336548\n",
      "[Training Epoch 9] Batch 1818, Loss 0.25343894958496094\n",
      "[Training Epoch 9] Batch 1819, Loss 0.22202911972999573\n",
      "[Training Epoch 9] Batch 1820, Loss 0.2267685830593109\n",
      "[Training Epoch 9] Batch 1821, Loss 0.2557145357131958\n",
      "[Training Epoch 9] Batch 1822, Loss 0.2491896152496338\n",
      "[Training Epoch 9] Batch 1823, Loss 0.22898027300834656\n",
      "[Training Epoch 9] Batch 1824, Loss 0.2800005078315735\n",
      "[Training Epoch 9] Batch 1825, Loss 0.24572668969631195\n",
      "[Training Epoch 9] Batch 1826, Loss 0.2494238168001175\n",
      "[Training Epoch 9] Batch 1827, Loss 0.24695053696632385\n",
      "[Training Epoch 9] Batch 1828, Loss 0.2547025680541992\n",
      "[Training Epoch 9] Batch 1829, Loss 0.258368581533432\n",
      "[Training Epoch 9] Batch 1830, Loss 0.25755396485328674\n",
      "[Training Epoch 9] Batch 1831, Loss 0.2565780282020569\n",
      "[Training Epoch 9] Batch 1832, Loss 0.22607816755771637\n",
      "[Training Epoch 9] Batch 1833, Loss 0.2685593366622925\n",
      "[Training Epoch 9] Batch 1834, Loss 0.2444177269935608\n",
      "[Training Epoch 9] Batch 1835, Loss 0.27086302638053894\n",
      "[Training Epoch 9] Batch 1836, Loss 0.27073633670806885\n",
      "[Training Epoch 9] Batch 1837, Loss 0.24121788144111633\n",
      "[Training Epoch 9] Batch 1838, Loss 0.2486206293106079\n",
      "[Training Epoch 9] Batch 1839, Loss 0.24813832342624664\n",
      "[Training Epoch 9] Batch 1840, Loss 0.27062761783599854\n",
      "[Training Epoch 9] Batch 1841, Loss 0.2649437487125397\n",
      "[Training Epoch 9] Batch 1842, Loss 0.25773781538009644\n",
      "[Training Epoch 9] Batch 1843, Loss 0.25298014283180237\n",
      "[Training Epoch 9] Batch 1844, Loss 0.24568769335746765\n",
      "[Training Epoch 9] Batch 1845, Loss 0.25842517614364624\n",
      "[Training Epoch 9] Batch 1846, Loss 0.2592657208442688\n",
      "[Training Epoch 9] Batch 1847, Loss 0.22817209362983704\n",
      "[Training Epoch 9] Batch 1848, Loss 0.28959035873413086\n",
      "[Training Epoch 9] Batch 1849, Loss 0.2387010157108307\n",
      "[Training Epoch 9] Batch 1850, Loss 0.23370859026908875\n",
      "[Training Epoch 9] Batch 1851, Loss 0.2498544156551361\n",
      "[Training Epoch 9] Batch 1852, Loss 0.2504929304122925\n",
      "[Training Epoch 9] Batch 1853, Loss 0.2307470142841339\n",
      "[Training Epoch 9] Batch 1854, Loss 0.2550102472305298\n",
      "[Training Epoch 9] Batch 1855, Loss 0.2658225893974304\n",
      "[Training Epoch 9] Batch 1856, Loss 0.2609960436820984\n",
      "[Training Epoch 9] Batch 1857, Loss 0.23906013369560242\n",
      "[Training Epoch 9] Batch 1858, Loss 0.23103073239326477\n",
      "[Training Epoch 9] Batch 1859, Loss 0.23707352578639984\n",
      "[Training Epoch 9] Batch 1860, Loss 0.2557990550994873\n",
      "[Training Epoch 9] Batch 1861, Loss 0.26729851961135864\n",
      "[Training Epoch 9] Batch 1862, Loss 0.25860291719436646\n",
      "[Training Epoch 9] Batch 1863, Loss 0.23343470692634583\n",
      "[Training Epoch 9] Batch 1864, Loss 0.27625685930252075\n",
      "[Training Epoch 9] Batch 1865, Loss 0.25280240178108215\n",
      "[Training Epoch 9] Batch 1866, Loss 0.25627630949020386\n",
      "[Training Epoch 9] Batch 1867, Loss 0.23158729076385498\n",
      "[Training Epoch 9] Batch 1868, Loss 0.21784044802188873\n",
      "[Training Epoch 9] Batch 1869, Loss 0.2811402380466461\n",
      "[Training Epoch 9] Batch 1870, Loss 0.23930105566978455\n",
      "[Training Epoch 9] Batch 1871, Loss 0.24946606159210205\n",
      "[Training Epoch 9] Batch 1872, Loss 0.2634386420249939\n",
      "[Training Epoch 9] Batch 1873, Loss 0.25628790259361267\n",
      "[Training Epoch 9] Batch 1874, Loss 0.25049054622650146\n",
      "[Training Epoch 9] Batch 1875, Loss 0.24411174654960632\n",
      "[Training Epoch 9] Batch 1876, Loss 0.2684806287288666\n",
      "[Training Epoch 9] Batch 1877, Loss 0.22201070189476013\n",
      "[Training Epoch 9] Batch 1878, Loss 0.2398260235786438\n",
      "[Training Epoch 9] Batch 1879, Loss 0.27492213249206543\n",
      "[Training Epoch 9] Batch 1880, Loss 0.2787262201309204\n",
      "[Training Epoch 9] Batch 1881, Loss 0.2061934769153595\n",
      "[Training Epoch 9] Batch 1882, Loss 0.2522006034851074\n",
      "[Training Epoch 9] Batch 1883, Loss 0.24411793053150177\n",
      "[Training Epoch 9] Batch 1884, Loss 0.26816684007644653\n",
      "[Training Epoch 9] Batch 1885, Loss 0.23897787928581238\n",
      "[Training Epoch 9] Batch 1886, Loss 0.27402979135513306\n",
      "[Training Epoch 9] Batch 1887, Loss 0.22859108448028564\n",
      "[Training Epoch 9] Batch 1888, Loss 0.25041863322257996\n",
      "[Training Epoch 9] Batch 1889, Loss 0.22256679832935333\n",
      "[Training Epoch 9] Batch 1890, Loss 0.22716432809829712\n",
      "[Training Epoch 9] Batch 1891, Loss 0.2581871747970581\n",
      "[Training Epoch 9] Batch 1892, Loss 0.27151066064834595\n",
      "[Training Epoch 9] Batch 1893, Loss 0.2159702032804489\n",
      "[Training Epoch 9] Batch 1894, Loss 0.24407365918159485\n",
      "[Training Epoch 9] Batch 1895, Loss 0.28715944290161133\n",
      "[Training Epoch 9] Batch 1896, Loss 0.2739475667476654\n",
      "[Training Epoch 9] Batch 1897, Loss 0.25142669677734375\n",
      "[Training Epoch 9] Batch 1898, Loss 0.24915164709091187\n",
      "[Training Epoch 9] Batch 1899, Loss 0.24584780633449554\n",
      "[Training Epoch 9] Batch 1900, Loss 0.23541221022605896\n",
      "[Training Epoch 9] Batch 1901, Loss 0.24545729160308838\n",
      "[Training Epoch 9] Batch 1902, Loss 0.22991272807121277\n",
      "[Training Epoch 9] Batch 1903, Loss 0.2805531620979309\n",
      "[Training Epoch 9] Batch 1904, Loss 0.2810945510864258\n",
      "[Training Epoch 9] Batch 1905, Loss 0.2274385690689087\n",
      "[Training Epoch 9] Batch 1906, Loss 0.2755882740020752\n",
      "[Training Epoch 9] Batch 1907, Loss 0.25491228699684143\n",
      "[Training Epoch 9] Batch 1908, Loss 0.24030543863773346\n",
      "[Training Epoch 9] Batch 1909, Loss 0.2845804691314697\n",
      "[Training Epoch 9] Batch 1910, Loss 0.2563716769218445\n",
      "[Training Epoch 9] Batch 1911, Loss 0.24859604239463806\n",
      "[Training Epoch 9] Batch 1912, Loss 0.25565728545188904\n",
      "[Training Epoch 9] Batch 1913, Loss 0.25025105476379395\n",
      "[Training Epoch 9] Batch 1914, Loss 0.2757355570793152\n",
      "[Training Epoch 9] Batch 1915, Loss 0.23467689752578735\n",
      "[Training Epoch 9] Batch 1916, Loss 0.2581162750720978\n",
      "[Training Epoch 9] Batch 1917, Loss 0.2587279677391052\n",
      "[Training Epoch 9] Batch 1918, Loss 0.24985569715499878\n",
      "[Training Epoch 9] Batch 1919, Loss 0.26480117440223694\n",
      "[Training Epoch 9] Batch 1920, Loss 0.24403643608093262\n",
      "[Training Epoch 9] Batch 1921, Loss 0.26880186796188354\n",
      "[Training Epoch 9] Batch 1922, Loss 0.24429838359355927\n",
      "[Training Epoch 9] Batch 1923, Loss 0.2276960015296936\n",
      "[Training Epoch 9] Batch 1924, Loss 0.23822161555290222\n",
      "[Training Epoch 9] Batch 1925, Loss 0.23496779799461365\n",
      "[Training Epoch 9] Batch 1926, Loss 0.2657664120197296\n",
      "[Training Epoch 9] Batch 1927, Loss 0.2272244691848755\n",
      "[Training Epoch 9] Batch 1928, Loss 0.2852171063423157\n",
      "[Training Epoch 9] Batch 1929, Loss 0.2429126799106598\n",
      "[Training Epoch 9] Batch 1930, Loss 0.2660192847251892\n",
      "[Training Epoch 9] Batch 1931, Loss 0.23270444571971893\n",
      "[Training Epoch 9] Batch 1932, Loss 0.2923998236656189\n",
      "[Training Epoch 9] Batch 1933, Loss 0.23656661808490753\n",
      "[Training Epoch 9] Batch 1934, Loss 0.24575713276863098\n",
      "[Training Epoch 9] Batch 1935, Loss 0.24777448177337646\n",
      "[Training Epoch 9] Batch 1936, Loss 0.240385040640831\n",
      "[Training Epoch 9] Batch 1937, Loss 0.22753047943115234\n",
      "[Training Epoch 9] Batch 1938, Loss 0.24706204235553741\n",
      "[Training Epoch 9] Batch 1939, Loss 0.2744925022125244\n",
      "[Training Epoch 9] Batch 1940, Loss 0.2509881556034088\n",
      "[Training Epoch 9] Batch 1941, Loss 0.23410671949386597\n",
      "[Training Epoch 9] Batch 1942, Loss 0.25994205474853516\n",
      "[Training Epoch 9] Batch 1943, Loss 0.2850826680660248\n",
      "[Training Epoch 9] Batch 1944, Loss 0.29534441232681274\n",
      "[Training Epoch 9] Batch 1945, Loss 0.2597963511943817\n",
      "[Training Epoch 9] Batch 1946, Loss 0.24967283010482788\n",
      "[Training Epoch 9] Batch 1947, Loss 0.2781779170036316\n",
      "[Training Epoch 9] Batch 1948, Loss 0.23393526673316956\n",
      "[Training Epoch 9] Batch 1949, Loss 0.2473694533109665\n",
      "[Training Epoch 9] Batch 1950, Loss 0.24404168128967285\n",
      "[Training Epoch 9] Batch 1951, Loss 0.26400500535964966\n",
      "[Training Epoch 9] Batch 1952, Loss 0.26279908418655396\n",
      "[Training Epoch 9] Batch 1953, Loss 0.22429437935352325\n",
      "[Training Epoch 9] Batch 1954, Loss 0.27916979789733887\n",
      "[Training Epoch 9] Batch 1955, Loss 0.24678316712379456\n",
      "[Training Epoch 9] Batch 1956, Loss 0.28806272149086\n",
      "[Training Epoch 9] Batch 1957, Loss 0.24686884880065918\n",
      "[Training Epoch 9] Batch 1958, Loss 0.23576891422271729\n",
      "[Training Epoch 9] Batch 1959, Loss 0.23799265921115875\n",
      "[Training Epoch 9] Batch 1960, Loss 0.2704242467880249\n",
      "[Training Epoch 9] Batch 1961, Loss 0.2836189270019531\n",
      "[Training Epoch 9] Batch 1962, Loss 0.25438040494918823\n",
      "[Training Epoch 9] Batch 1963, Loss 0.2489931285381317\n",
      "[Training Epoch 9] Batch 1964, Loss 0.26192089915275574\n",
      "[Training Epoch 9] Batch 1965, Loss 0.23605214059352875\n",
      "[Training Epoch 9] Batch 1966, Loss 0.26553523540496826\n",
      "[Training Epoch 9] Batch 1967, Loss 0.28110408782958984\n",
      "[Training Epoch 9] Batch 1968, Loss 0.28150105476379395\n",
      "[Training Epoch 9] Batch 1969, Loss 0.24935977160930634\n",
      "[Training Epoch 9] Batch 1970, Loss 0.2534686028957367\n",
      "[Training Epoch 9] Batch 1971, Loss 0.25121474266052246\n",
      "[Training Epoch 9] Batch 1972, Loss 0.22462402284145355\n",
      "[Training Epoch 9] Batch 1973, Loss 0.25723063945770264\n",
      "[Training Epoch 9] Batch 1974, Loss 0.2793111503124237\n",
      "[Training Epoch 9] Batch 1975, Loss 0.24476182460784912\n",
      "[Training Epoch 9] Batch 1976, Loss 0.2164083570241928\n",
      "[Training Epoch 9] Batch 1977, Loss 0.2664329707622528\n",
      "[Training Epoch 9] Batch 1978, Loss 0.24852821230888367\n",
      "[Training Epoch 9] Batch 1979, Loss 0.2529699206352234\n",
      "[Training Epoch 9] Batch 1980, Loss 0.2591239809989929\n",
      "[Training Epoch 9] Batch 1981, Loss 0.23688340187072754\n",
      "[Training Epoch 9] Batch 1982, Loss 0.26560473442077637\n",
      "[Training Epoch 9] Batch 1983, Loss 0.23298820853233337\n",
      "[Training Epoch 9] Batch 1984, Loss 0.30419790744781494\n",
      "[Training Epoch 9] Batch 1985, Loss 0.23904918134212494\n",
      "[Training Epoch 9] Batch 1986, Loss 0.24447417259216309\n",
      "[Training Epoch 9] Batch 1987, Loss 0.25082093477249146\n",
      "[Training Epoch 9] Batch 1988, Loss 0.25259196758270264\n",
      "[Training Epoch 9] Batch 1989, Loss 0.24707196652889252\n",
      "[Training Epoch 9] Batch 1990, Loss 0.24921345710754395\n",
      "[Training Epoch 9] Batch 1991, Loss 0.22998102009296417\n",
      "[Training Epoch 9] Batch 1992, Loss 0.23644672334194183\n",
      "[Training Epoch 9] Batch 1993, Loss 0.2514086961746216\n",
      "[Training Epoch 9] Batch 1994, Loss 0.2474011927843094\n",
      "[Training Epoch 9] Batch 1995, Loss 0.2538999617099762\n",
      "[Training Epoch 9] Batch 1996, Loss 0.25130441784858704\n",
      "[Training Epoch 9] Batch 1997, Loss 0.21730250120162964\n",
      "[Training Epoch 9] Batch 1998, Loss 0.28668707609176636\n",
      "[Training Epoch 9] Batch 1999, Loss 0.29778245091438293\n",
      "[Training Epoch 9] Batch 2000, Loss 0.24159866571426392\n",
      "[Training Epoch 9] Batch 2001, Loss 0.23721809685230255\n",
      "[Training Epoch 9] Batch 2002, Loss 0.2663133442401886\n",
      "[Training Epoch 9] Batch 2003, Loss 0.24256856739521027\n",
      "[Training Epoch 9] Batch 2004, Loss 0.23806282877922058\n",
      "[Training Epoch 9] Batch 2005, Loss 0.26004087924957275\n",
      "[Training Epoch 9] Batch 2006, Loss 0.2586495280265808\n",
      "[Training Epoch 9] Batch 2007, Loss 0.2460043728351593\n",
      "[Training Epoch 9] Batch 2008, Loss 0.23997627198696136\n",
      "[Training Epoch 9] Batch 2009, Loss 0.25090956687927246\n",
      "[Training Epoch 9] Batch 2010, Loss 0.2606605291366577\n",
      "[Training Epoch 9] Batch 2011, Loss 0.2487928867340088\n",
      "[Training Epoch 9] Batch 2012, Loss 0.24917638301849365\n",
      "[Training Epoch 9] Batch 2013, Loss 0.26978108286857605\n",
      "[Training Epoch 9] Batch 2014, Loss 0.28844720125198364\n",
      "[Training Epoch 9] Batch 2015, Loss 0.2802284061908722\n",
      "[Training Epoch 9] Batch 2016, Loss 0.2538643777370453\n",
      "[Training Epoch 9] Batch 2017, Loss 0.24490520358085632\n",
      "[Training Epoch 9] Batch 2018, Loss 0.2791011333465576\n",
      "[Training Epoch 9] Batch 2019, Loss 0.24118484556674957\n",
      "[Training Epoch 9] Batch 2020, Loss 0.25181281566619873\n",
      "[Training Epoch 9] Batch 2021, Loss 0.2464641034603119\n",
      "[Training Epoch 9] Batch 2022, Loss 0.23667773604393005\n",
      "[Training Epoch 9] Batch 2023, Loss 0.2504776120185852\n",
      "[Training Epoch 9] Batch 2024, Loss 0.21802958846092224\n",
      "[Training Epoch 9] Batch 2025, Loss 0.25367969274520874\n",
      "[Training Epoch 9] Batch 2026, Loss 0.2501766085624695\n",
      "[Training Epoch 9] Batch 2027, Loss 0.24017104506492615\n",
      "[Training Epoch 9] Batch 2028, Loss 0.24559149146080017\n",
      "[Training Epoch 9] Batch 2029, Loss 0.2710190713405609\n",
      "[Training Epoch 9] Batch 2030, Loss 0.23935313522815704\n",
      "[Training Epoch 9] Batch 2031, Loss 0.22956889867782593\n",
      "[Training Epoch 9] Batch 2032, Loss 0.237137109041214\n",
      "[Training Epoch 9] Batch 2033, Loss 0.2662336826324463\n",
      "[Training Epoch 9] Batch 2034, Loss 0.23437169194221497\n",
      "[Training Epoch 9] Batch 2035, Loss 0.27070844173431396\n",
      "[Training Epoch 9] Batch 2036, Loss 0.26169952750205994\n",
      "[Training Epoch 9] Batch 2037, Loss 0.27411454916000366\n",
      "[Training Epoch 9] Batch 2038, Loss 0.26341086626052856\n",
      "[Training Epoch 9] Batch 2039, Loss 0.22488529980182648\n",
      "[Training Epoch 9] Batch 2040, Loss 0.29493939876556396\n",
      "[Training Epoch 9] Batch 2041, Loss 0.24457235634326935\n",
      "[Training Epoch 9] Batch 2042, Loss 0.2659760117530823\n",
      "[Training Epoch 9] Batch 2043, Loss 0.21652817726135254\n",
      "[Training Epoch 9] Batch 2044, Loss 0.23994657397270203\n",
      "[Training Epoch 9] Batch 2045, Loss 0.25134599208831787\n",
      "[Training Epoch 9] Batch 2046, Loss 0.24901297688484192\n",
      "[Training Epoch 9] Batch 2047, Loss 0.23001547157764435\n",
      "[Training Epoch 9] Batch 2048, Loss 0.2862524390220642\n",
      "[Training Epoch 9] Batch 2049, Loss 0.2343119978904724\n",
      "[Training Epoch 9] Batch 2050, Loss 0.29141056537628174\n",
      "[Training Epoch 9] Batch 2051, Loss 0.22703558206558228\n",
      "[Training Epoch 9] Batch 2052, Loss 0.25304579734802246\n",
      "[Training Epoch 9] Batch 2053, Loss 0.2761511206626892\n",
      "[Training Epoch 9] Batch 2054, Loss 0.2829248011112213\n",
      "[Training Epoch 9] Batch 2055, Loss 0.2320086807012558\n",
      "[Training Epoch 9] Batch 2056, Loss 0.2382688671350479\n",
      "[Training Epoch 9] Batch 2057, Loss 0.2504296898841858\n",
      "[Training Epoch 9] Batch 2058, Loss 0.26500847935676575\n",
      "[Training Epoch 9] Batch 2059, Loss 0.26752156019210815\n",
      "[Training Epoch 9] Batch 2060, Loss 0.26597100496292114\n",
      "[Training Epoch 9] Batch 2061, Loss 0.2638764977455139\n",
      "[Training Epoch 9] Batch 2062, Loss 0.23144319653511047\n",
      "[Training Epoch 9] Batch 2063, Loss 0.23757611215114594\n",
      "[Training Epoch 9] Batch 2064, Loss 0.22810760140419006\n",
      "[Training Epoch 9] Batch 2065, Loss 0.2547765374183655\n",
      "[Training Epoch 9] Batch 2066, Loss 0.24544094502925873\n",
      "[Training Epoch 9] Batch 2067, Loss 0.2531359791755676\n",
      "[Training Epoch 9] Batch 2068, Loss 0.24236801266670227\n",
      "[Training Epoch 9] Batch 2069, Loss 0.24278011918067932\n",
      "[Training Epoch 9] Batch 2070, Loss 0.27020639181137085\n",
      "[Training Epoch 9] Batch 2071, Loss 0.2478327453136444\n",
      "[Training Epoch 9] Batch 2072, Loss 0.2975820302963257\n",
      "[Training Epoch 9] Batch 2073, Loss 0.2533745765686035\n",
      "[Training Epoch 9] Batch 2074, Loss 0.2412855625152588\n",
      "[Training Epoch 9] Batch 2075, Loss 0.25183284282684326\n",
      "[Training Epoch 9] Batch 2076, Loss 0.26750296354293823\n",
      "[Training Epoch 9] Batch 2077, Loss 0.24649524688720703\n",
      "[Training Epoch 9] Batch 2078, Loss 0.27063238620758057\n",
      "[Training Epoch 9] Batch 2079, Loss 0.27754706144332886\n",
      "[Training Epoch 9] Batch 2080, Loss 0.25454944372177124\n",
      "[Training Epoch 9] Batch 2081, Loss 0.2409469485282898\n",
      "[Training Epoch 9] Batch 2082, Loss 0.2560001015663147\n",
      "[Training Epoch 9] Batch 2083, Loss 0.24799829721450806\n",
      "[Training Epoch 9] Batch 2084, Loss 0.2600415349006653\n",
      "[Training Epoch 9] Batch 2085, Loss 0.2488076537847519\n",
      "[Training Epoch 9] Batch 2086, Loss 0.26452481746673584\n",
      "[Training Epoch 9] Batch 2087, Loss 0.27823516726493835\n",
      "[Training Epoch 9] Batch 2088, Loss 0.25230729579925537\n",
      "[Training Epoch 9] Batch 2089, Loss 0.24106362462043762\n",
      "[Training Epoch 9] Batch 2090, Loss 0.2584141194820404\n",
      "[Training Epoch 9] Batch 2091, Loss 0.23582293093204498\n",
      "[Training Epoch 9] Batch 2092, Loss 0.2270522117614746\n",
      "[Training Epoch 9] Batch 2093, Loss 0.2735917568206787\n",
      "[Training Epoch 9] Batch 2094, Loss 0.2648031711578369\n",
      "[Training Epoch 9] Batch 2095, Loss 0.24011953175067902\n",
      "[Training Epoch 9] Batch 2096, Loss 0.22862350940704346\n",
      "[Training Epoch 9] Batch 2097, Loss 0.2600482702255249\n",
      "[Training Epoch 9] Batch 2098, Loss 0.2769220471382141\n",
      "[Training Epoch 9] Batch 2099, Loss 0.24497705698013306\n",
      "[Training Epoch 9] Batch 2100, Loss 0.20931756496429443\n",
      "[Training Epoch 9] Batch 2101, Loss 0.2522418797016144\n",
      "[Training Epoch 9] Batch 2102, Loss 0.2620803713798523\n",
      "[Training Epoch 9] Batch 2103, Loss 0.22696194052696228\n",
      "[Training Epoch 9] Batch 2104, Loss 0.27164775133132935\n",
      "[Training Epoch 9] Batch 2105, Loss 0.27959123253822327\n",
      "[Training Epoch 9] Batch 2106, Loss 0.2520187497138977\n",
      "[Training Epoch 9] Batch 2107, Loss 0.24023933708667755\n",
      "[Training Epoch 9] Batch 2108, Loss 0.23531346023082733\n",
      "[Training Epoch 9] Batch 2109, Loss 0.23385143280029297\n",
      "[Training Epoch 9] Batch 2110, Loss 0.2706117033958435\n",
      "[Training Epoch 9] Batch 2111, Loss 0.26433882117271423\n",
      "[Training Epoch 9] Batch 2112, Loss 0.24656838178634644\n",
      "[Training Epoch 9] Batch 2113, Loss 0.24750521779060364\n",
      "[Training Epoch 9] Batch 2114, Loss 0.2613222002983093\n",
      "[Training Epoch 9] Batch 2115, Loss 0.242059126496315\n",
      "[Training Epoch 9] Batch 2116, Loss 0.23637643456459045\n",
      "[Training Epoch 9] Batch 2117, Loss 0.24838212132453918\n",
      "[Training Epoch 9] Batch 2118, Loss 0.2545604109764099\n",
      "[Training Epoch 9] Batch 2119, Loss 0.25067710876464844\n",
      "[Training Epoch 9] Batch 2120, Loss 0.24707837402820587\n",
      "[Training Epoch 9] Batch 2121, Loss 0.2382974922657013\n",
      "[Training Epoch 9] Batch 2122, Loss 0.256451815366745\n",
      "[Training Epoch 9] Batch 2123, Loss 0.2610098719596863\n",
      "[Training Epoch 9] Batch 2124, Loss 0.25625115633010864\n",
      "[Training Epoch 9] Batch 2125, Loss 0.2277907431125641\n",
      "[Training Epoch 9] Batch 2126, Loss 0.22992826998233795\n",
      "[Training Epoch 9] Batch 2127, Loss 0.23465266823768616\n",
      "[Training Epoch 9] Batch 2128, Loss 0.24186715483665466\n",
      "[Training Epoch 9] Batch 2129, Loss 0.23798048496246338\n",
      "[Training Epoch 9] Batch 2130, Loss 0.2568133473396301\n",
      "[Training Epoch 9] Batch 2131, Loss 0.27511656284332275\n",
      "[Training Epoch 9] Batch 2132, Loss 0.26472198963165283\n",
      "[Training Epoch 9] Batch 2133, Loss 0.22221799194812775\n",
      "[Training Epoch 9] Batch 2134, Loss 0.26934272050857544\n",
      "[Training Epoch 9] Batch 2135, Loss 0.2360542118549347\n",
      "[Training Epoch 9] Batch 2136, Loss 0.25311392545700073\n",
      "[Training Epoch 9] Batch 2137, Loss 0.2663757801055908\n",
      "[Training Epoch 9] Batch 2138, Loss 0.2737644910812378\n",
      "[Training Epoch 9] Batch 2139, Loss 0.2627003788948059\n",
      "[Training Epoch 9] Batch 2140, Loss 0.2521519660949707\n",
      "[Training Epoch 9] Batch 2141, Loss 0.2804228663444519\n",
      "[Training Epoch 9] Batch 2142, Loss 0.2779105305671692\n",
      "[Training Epoch 9] Batch 2143, Loss 0.2587321400642395\n",
      "[Training Epoch 9] Batch 2144, Loss 0.2464764416217804\n",
      "[Training Epoch 9] Batch 2145, Loss 0.24782416224479675\n",
      "[Training Epoch 9] Batch 2146, Loss 0.2606930434703827\n",
      "[Training Epoch 9] Batch 2147, Loss 0.2635473608970642\n",
      "[Training Epoch 9] Batch 2148, Loss 0.2503606379032135\n",
      "[Training Epoch 9] Batch 2149, Loss 0.27053719758987427\n",
      "[Training Epoch 9] Batch 2150, Loss 0.2735186219215393\n",
      "[Training Epoch 9] Batch 2151, Loss 0.27151089906692505\n",
      "[Training Epoch 9] Batch 2152, Loss 0.2538563311100006\n",
      "[Training Epoch 9] Batch 2153, Loss 0.21887175738811493\n",
      "[Training Epoch 9] Batch 2154, Loss 0.28658849000930786\n",
      "[Training Epoch 9] Batch 2155, Loss 0.24501337110996246\n",
      "[Training Epoch 9] Batch 2156, Loss 0.22913824021816254\n",
      "[Training Epoch 9] Batch 2157, Loss 0.2623993158340454\n",
      "[Training Epoch 9] Batch 2158, Loss 0.255472332239151\n",
      "[Training Epoch 9] Batch 2159, Loss 0.26716238260269165\n",
      "[Training Epoch 9] Batch 2160, Loss 0.24506241083145142\n",
      "[Training Epoch 9] Batch 2161, Loss 0.25505056977272034\n",
      "[Training Epoch 9] Batch 2162, Loss 0.2528703510761261\n",
      "[Training Epoch 9] Batch 2163, Loss 0.24703755974769592\n",
      "[Training Epoch 9] Batch 2164, Loss 0.24794666469097137\n",
      "[Training Epoch 9] Batch 2165, Loss 0.22962084412574768\n",
      "[Training Epoch 9] Batch 2166, Loss 0.24174922704696655\n",
      "[Training Epoch 9] Batch 2167, Loss 0.2719053030014038\n",
      "[Training Epoch 9] Batch 2168, Loss 0.23797857761383057\n",
      "[Training Epoch 9] Batch 2169, Loss 0.2816908359527588\n",
      "[Training Epoch 9] Batch 2170, Loss 0.2559284269809723\n",
      "[Training Epoch 9] Batch 2171, Loss 0.2355048954486847\n",
      "[Training Epoch 9] Batch 2172, Loss 0.21307078003883362\n",
      "[Training Epoch 9] Batch 2173, Loss 0.2864534258842468\n",
      "[Training Epoch 9] Batch 2174, Loss 0.2732238173484802\n",
      "[Training Epoch 9] Batch 2175, Loss 0.2575297951698303\n",
      "[Training Epoch 9] Batch 2176, Loss 0.26177364587783813\n",
      "[Training Epoch 9] Batch 2177, Loss 0.26545262336730957\n",
      "[Training Epoch 9] Batch 2178, Loss 0.2560781240463257\n",
      "[Training Epoch 9] Batch 2179, Loss 0.2718147039413452\n",
      "[Training Epoch 9] Batch 2180, Loss 0.24958306550979614\n",
      "[Training Epoch 9] Batch 2181, Loss 0.2341846376657486\n",
      "[Training Epoch 9] Batch 2182, Loss 0.22510260343551636\n",
      "[Training Epoch 9] Batch 2183, Loss 0.2476276159286499\n",
      "[Training Epoch 9] Batch 2184, Loss 0.25855135917663574\n",
      "[Training Epoch 9] Batch 2185, Loss 0.25657814741134644\n",
      "[Training Epoch 9] Batch 2186, Loss 0.2694743275642395\n",
      "[Training Epoch 9] Batch 2187, Loss 0.23768427968025208\n",
      "[Training Epoch 9] Batch 2188, Loss 0.25579237937927246\n",
      "[Training Epoch 9] Batch 2189, Loss 0.2669781744480133\n",
      "[Training Epoch 9] Batch 2190, Loss 0.22676335275173187\n",
      "[Training Epoch 9] Batch 2191, Loss 0.26047009229660034\n",
      "[Training Epoch 9] Batch 2192, Loss 0.26750728487968445\n",
      "[Training Epoch 9] Batch 2193, Loss 0.22659270465373993\n",
      "[Training Epoch 9] Batch 2194, Loss 0.25419723987579346\n",
      "[Training Epoch 9] Batch 2195, Loss 0.26341307163238525\n",
      "[Training Epoch 9] Batch 2196, Loss 0.24122118949890137\n",
      "[Training Epoch 9] Batch 2197, Loss 0.2540673613548279\n",
      "[Training Epoch 9] Batch 2198, Loss 0.2613275945186615\n",
      "[Training Epoch 9] Batch 2199, Loss 0.24390381574630737\n",
      "[Training Epoch 9] Batch 2200, Loss 0.24285121262073517\n",
      "[Training Epoch 9] Batch 2201, Loss 0.2642492651939392\n",
      "[Training Epoch 9] Batch 2202, Loss 0.26146674156188965\n",
      "[Training Epoch 9] Batch 2203, Loss 0.23506617546081543\n",
      "[Training Epoch 9] Batch 2204, Loss 0.2484414279460907\n",
      "[Training Epoch 9] Batch 2205, Loss 0.24879080057144165\n",
      "[Training Epoch 9] Batch 2206, Loss 0.2326974868774414\n",
      "[Training Epoch 9] Batch 2207, Loss 0.2642366290092468\n",
      "[Training Epoch 9] Batch 2208, Loss 0.27249351143836975\n",
      "[Training Epoch 9] Batch 2209, Loss 0.25417032837867737\n",
      "[Training Epoch 9] Batch 2210, Loss 0.2452247589826584\n",
      "[Training Epoch 9] Batch 2211, Loss 0.25519081950187683\n",
      "[Training Epoch 9] Batch 2212, Loss 0.2687852382659912\n",
      "[Training Epoch 9] Batch 2213, Loss 0.24862882494926453\n",
      "[Training Epoch 9] Batch 2214, Loss 0.25195175409317017\n",
      "[Training Epoch 9] Batch 2215, Loss 0.2624145448207855\n",
      "[Training Epoch 9] Batch 2216, Loss 0.2489444613456726\n",
      "[Training Epoch 9] Batch 2217, Loss 0.2761693000793457\n",
      "[Training Epoch 9] Batch 2218, Loss 0.24608692526817322\n",
      "[Training Epoch 9] Batch 2219, Loss 0.23384049534797668\n",
      "[Training Epoch 9] Batch 2220, Loss 0.26787522435188293\n",
      "[Training Epoch 9] Batch 2221, Loss 0.2736179828643799\n",
      "[Training Epoch 9] Batch 2222, Loss 0.26878973841667175\n",
      "[Training Epoch 9] Batch 2223, Loss 0.2692490816116333\n",
      "[Training Epoch 9] Batch 2224, Loss 0.26399093866348267\n",
      "[Training Epoch 9] Batch 2225, Loss 0.24995680153369904\n",
      "[Training Epoch 9] Batch 2226, Loss 0.21924054622650146\n",
      "[Training Epoch 9] Batch 2227, Loss 0.2767871022224426\n",
      "[Training Epoch 9] Batch 2228, Loss 0.2636566758155823\n",
      "[Training Epoch 9] Batch 2229, Loss 0.25362735986709595\n",
      "[Training Epoch 9] Batch 2230, Loss 0.26181066036224365\n",
      "[Training Epoch 9] Batch 2231, Loss 0.22319766879081726\n",
      "[Training Epoch 9] Batch 2232, Loss 0.25381141901016235\n",
      "[Training Epoch 9] Batch 2233, Loss 0.2664676904678345\n",
      "[Training Epoch 9] Batch 2234, Loss 0.2702621519565582\n",
      "[Training Epoch 9] Batch 2235, Loss 0.26306596398353577\n",
      "[Training Epoch 9] Batch 2236, Loss 0.24170717597007751\n",
      "[Training Epoch 9] Batch 2237, Loss 0.26542553305625916\n",
      "[Training Epoch 9] Batch 2238, Loss 0.2484133541584015\n",
      "[Training Epoch 9] Batch 2239, Loss 0.24879640340805054\n",
      "[Training Epoch 9] Batch 2240, Loss 0.27870726585388184\n",
      "[Training Epoch 9] Batch 2241, Loss 0.24410004913806915\n",
      "[Training Epoch 9] Batch 2242, Loss 0.25848785042762756\n",
      "[Training Epoch 9] Batch 2243, Loss 0.24485492706298828\n",
      "[Training Epoch 9] Batch 2244, Loss 0.21558928489685059\n",
      "[Training Epoch 9] Batch 2245, Loss 0.2452700436115265\n",
      "[Training Epoch 9] Batch 2246, Loss 0.28067320585250854\n",
      "[Training Epoch 9] Batch 2247, Loss 0.2833494544029236\n",
      "[Training Epoch 9] Batch 2248, Loss 0.2506338357925415\n",
      "[Training Epoch 9] Batch 2249, Loss 0.2546527683734894\n",
      "[Training Epoch 9] Batch 2250, Loss 0.2571569085121155\n",
      "[Training Epoch 9] Batch 2251, Loss 0.25988346338272095\n",
      "[Training Epoch 9] Batch 2252, Loss 0.2604832351207733\n",
      "[Training Epoch 9] Batch 2253, Loss 0.2533659338951111\n",
      "[Training Epoch 9] Batch 2254, Loss 0.2505141496658325\n",
      "[Training Epoch 9] Batch 2255, Loss 0.29001545906066895\n",
      "[Training Epoch 9] Batch 2256, Loss 0.27506691217422485\n",
      "[Training Epoch 9] Batch 2257, Loss 0.2589544355869293\n",
      "[Training Epoch 9] Batch 2258, Loss 0.2535538673400879\n",
      "[Training Epoch 9] Batch 2259, Loss 0.2579765319824219\n",
      "[Training Epoch 9] Batch 2260, Loss 0.2826595902442932\n",
      "[Training Epoch 9] Batch 2261, Loss 0.23574954271316528\n",
      "[Training Epoch 9] Batch 2262, Loss 0.24436289072036743\n",
      "[Training Epoch 9] Batch 2263, Loss 0.26390257477760315\n",
      "[Training Epoch 9] Batch 2264, Loss 0.26382800936698914\n",
      "[Training Epoch 9] Batch 2265, Loss 0.25187474489212036\n",
      "[Training Epoch 9] Batch 2266, Loss 0.24087664484977722\n",
      "[Training Epoch 9] Batch 2267, Loss 0.2627391219139099\n",
      "[Training Epoch 9] Batch 2268, Loss 0.26963385939598083\n",
      "[Training Epoch 9] Batch 2269, Loss 0.25869932770729065\n",
      "[Training Epoch 9] Batch 2270, Loss 0.2518671751022339\n",
      "[Training Epoch 9] Batch 2271, Loss 0.23038242757320404\n",
      "[Training Epoch 9] Batch 2272, Loss 0.29887694120407104\n",
      "[Training Epoch 9] Batch 2273, Loss 0.2617979049682617\n",
      "[Training Epoch 9] Batch 2274, Loss 0.2545713186264038\n",
      "[Training Epoch 9] Batch 2275, Loss 0.2536534070968628\n",
      "[Training Epoch 9] Batch 2276, Loss 0.24518710374832153\n",
      "[Training Epoch 9] Batch 2277, Loss 0.24480251967906952\n",
      "[Training Epoch 9] Batch 2278, Loss 0.2526610791683197\n",
      "[Training Epoch 9] Batch 2279, Loss 0.23605619370937347\n",
      "[Training Epoch 9] Batch 2280, Loss 0.2319008708000183\n",
      "[Training Epoch 9] Batch 2281, Loss 0.24995683133602142\n",
      "[Training Epoch 9] Batch 2282, Loss 0.24906043708324432\n",
      "[Training Epoch 9] Batch 2283, Loss 0.2530500292778015\n",
      "[Training Epoch 9] Batch 2284, Loss 0.24679666757583618\n",
      "[Training Epoch 9] Batch 2285, Loss 0.25355562567710876\n",
      "[Training Epoch 9] Batch 2286, Loss 0.2310437262058258\n",
      "[Training Epoch 9] Batch 2287, Loss 0.2541613280773163\n",
      "[Training Epoch 9] Batch 2288, Loss 0.24362121522426605\n",
      "[Training Epoch 9] Batch 2289, Loss 0.27564382553100586\n",
      "[Training Epoch 9] Batch 2290, Loss 0.2329777181148529\n",
      "[Training Epoch 9] Batch 2291, Loss 0.252094030380249\n",
      "[Training Epoch 9] Batch 2292, Loss 0.264676034450531\n",
      "[Training Epoch 9] Batch 2293, Loss 0.2362249791622162\n",
      "[Training Epoch 9] Batch 2294, Loss 0.25720274448394775\n",
      "[Training Epoch 9] Batch 2295, Loss 0.2608916163444519\n",
      "[Training Epoch 9] Batch 2296, Loss 0.23963671922683716\n",
      "[Training Epoch 9] Batch 2297, Loss 0.2616448402404785\n",
      "[Training Epoch 9] Batch 2298, Loss 0.2645159959793091\n",
      "[Training Epoch 9] Batch 2299, Loss 0.2751893997192383\n",
      "[Training Epoch 9] Batch 2300, Loss 0.25437578558921814\n",
      "[Training Epoch 9] Batch 2301, Loss 0.2395811825990677\n",
      "[Training Epoch 9] Batch 2302, Loss 0.283342182636261\n",
      "[Training Epoch 9] Batch 2303, Loss 0.2595711946487427\n",
      "[Training Epoch 9] Batch 2304, Loss 0.24224427342414856\n",
      "[Training Epoch 9] Batch 2305, Loss 0.27734190225601196\n",
      "[Training Epoch 9] Batch 2306, Loss 0.22138884663581848\n",
      "[Training Epoch 9] Batch 2307, Loss 0.29148969054222107\n",
      "[Training Epoch 9] Batch 2308, Loss 0.24865630269050598\n",
      "[Training Epoch 9] Batch 2309, Loss 0.23716998100280762\n",
      "[Training Epoch 9] Batch 2310, Loss 0.26229536533355713\n",
      "[Training Epoch 9] Batch 2311, Loss 0.26417863368988037\n",
      "[Training Epoch 9] Batch 2312, Loss 0.24923649430274963\n",
      "[Training Epoch 9] Batch 2313, Loss 0.28746938705444336\n",
      "[Training Epoch 9] Batch 2314, Loss 0.2362363487482071\n",
      "[Training Epoch 9] Batch 2315, Loss 0.26245129108428955\n",
      "[Training Epoch 9] Batch 2316, Loss 0.2523829936981201\n",
      "[Training Epoch 9] Batch 2317, Loss 0.2611149549484253\n",
      "[Training Epoch 9] Batch 2318, Loss 0.2658579349517822\n",
      "[Training Epoch 9] Batch 2319, Loss 0.2384893000125885\n",
      "[Training Epoch 9] Batch 2320, Loss 0.26419705152511597\n",
      "[Training Epoch 9] Batch 2321, Loss 0.22557896375656128\n",
      "[Training Epoch 9] Batch 2322, Loss 0.2443343997001648\n",
      "[Training Epoch 9] Batch 2323, Loss 0.24625131487846375\n",
      "[Training Epoch 9] Batch 2324, Loss 0.23930639028549194\n",
      "[Training Epoch 9] Batch 2325, Loss 0.24334657192230225\n",
      "[Training Epoch 9] Batch 2326, Loss 0.28412288427352905\n",
      "[Training Epoch 9] Batch 2327, Loss 0.25490087270736694\n",
      "[Training Epoch 9] Batch 2328, Loss 0.24709567427635193\n",
      "[Training Epoch 9] Batch 2329, Loss 0.27502548694610596\n",
      "[Training Epoch 9] Batch 2330, Loss 0.23701807856559753\n",
      "[Training Epoch 9] Batch 2331, Loss 0.24445000290870667\n",
      "[Training Epoch 9] Batch 2332, Loss 0.26055434346199036\n",
      "[Training Epoch 9] Batch 2333, Loss 0.2360037863254547\n",
      "[Training Epoch 9] Batch 2334, Loss 0.2771933674812317\n",
      "[Training Epoch 9] Batch 2335, Loss 0.26029422879219055\n",
      "[Training Epoch 9] Batch 2336, Loss 0.23658505082130432\n",
      "[Training Epoch 9] Batch 2337, Loss 0.24125319719314575\n",
      "[Training Epoch 9] Batch 2338, Loss 0.2829541563987732\n",
      "[Training Epoch 9] Batch 2339, Loss 0.2585442066192627\n",
      "[Training Epoch 9] Batch 2340, Loss 0.24362477660179138\n",
      "[Training Epoch 9] Batch 2341, Loss 0.2583719789981842\n",
      "[Training Epoch 9] Batch 2342, Loss 0.2715132534503937\n",
      "[Training Epoch 9] Batch 2343, Loss 0.24691906571388245\n",
      "[Training Epoch 9] Batch 2344, Loss 0.2468854933977127\n",
      "[Training Epoch 9] Batch 2345, Loss 0.23751626908779144\n",
      "[Training Epoch 9] Batch 2346, Loss 0.25233083963394165\n",
      "[Training Epoch 9] Batch 2347, Loss 0.23601344227790833\n",
      "[Training Epoch 9] Batch 2348, Loss 0.23799006640911102\n",
      "[Training Epoch 9] Batch 2349, Loss 0.23819705843925476\n",
      "[Training Epoch 9] Batch 2350, Loss 0.27616462111473083\n",
      "[Training Epoch 9] Batch 2351, Loss 0.22657173871994019\n",
      "[Training Epoch 9] Batch 2352, Loss 0.2526971995830536\n",
      "[Training Epoch 9] Batch 2353, Loss 0.2739447355270386\n",
      "[Training Epoch 9] Batch 2354, Loss 0.2458343654870987\n",
      "[Training Epoch 9] Batch 2355, Loss 0.2375313639640808\n",
      "[Training Epoch 9] Batch 2356, Loss 0.26877516508102417\n",
      "[Training Epoch 9] Batch 2357, Loss 0.2978495657444\n",
      "[Training Epoch 9] Batch 2358, Loss 0.2259310632944107\n",
      "[Training Epoch 9] Batch 2359, Loss 0.24393709003925323\n",
      "[Training Epoch 9] Batch 2360, Loss 0.2628704309463501\n",
      "[Training Epoch 9] Batch 2361, Loss 0.27368733286857605\n",
      "[Training Epoch 9] Batch 2362, Loss 0.26457327604293823\n",
      "[Training Epoch 9] Batch 2363, Loss 0.2775065302848816\n",
      "[Training Epoch 9] Batch 2364, Loss 0.25139960646629333\n",
      "[Training Epoch 9] Batch 2365, Loss 0.2461114227771759\n",
      "[Training Epoch 9] Batch 2366, Loss 0.26738178730010986\n",
      "[Training Epoch 9] Batch 2367, Loss 0.23642593622207642\n",
      "[Training Epoch 9] Batch 2368, Loss 0.25304850935935974\n",
      "[Training Epoch 9] Batch 2369, Loss 0.2540169358253479\n",
      "[Training Epoch 9] Batch 2370, Loss 0.2450530230998993\n",
      "[Training Epoch 9] Batch 2371, Loss 0.2570418417453766\n",
      "[Training Epoch 9] Batch 2372, Loss 0.22412747144699097\n",
      "[Training Epoch 9] Batch 2373, Loss 0.2777350842952728\n",
      "[Training Epoch 9] Batch 2374, Loss 0.28014469146728516\n",
      "[Training Epoch 9] Batch 2375, Loss 0.24994178116321564\n",
      "[Training Epoch 9] Batch 2376, Loss 0.24648508429527283\n",
      "[Training Epoch 9] Batch 2377, Loss 0.2334352731704712\n",
      "[Training Epoch 9] Batch 2378, Loss 0.2546827495098114\n",
      "[Training Epoch 9] Batch 2379, Loss 0.23972775042057037\n",
      "[Training Epoch 9] Batch 2380, Loss 0.2571616470813751\n",
      "[Training Epoch 9] Batch 2381, Loss 0.27629876136779785\n",
      "[Training Epoch 9] Batch 2382, Loss 0.2785413861274719\n",
      "[Training Epoch 9] Batch 2383, Loss 0.25011342763900757\n",
      "[Training Epoch 9] Batch 2384, Loss 0.2536783516407013\n",
      "[Training Epoch 9] Batch 2385, Loss 0.24694763123989105\n",
      "[Training Epoch 9] Batch 2386, Loss 0.2569332718849182\n",
      "[Training Epoch 9] Batch 2387, Loss 0.26405060291290283\n",
      "[Training Epoch 9] Batch 2388, Loss 0.2691798210144043\n",
      "[Training Epoch 9] Batch 2389, Loss 0.23541784286499023\n",
      "[Training Epoch 9] Batch 2390, Loss 0.2551417946815491\n",
      "[Training Epoch 9] Batch 2391, Loss 0.276736319065094\n",
      "[Training Epoch 9] Batch 2392, Loss 0.25507402420043945\n",
      "[Training Epoch 9] Batch 2393, Loss 0.28990721702575684\n",
      "[Training Epoch 9] Batch 2394, Loss 0.25365468859672546\n",
      "[Training Epoch 9] Batch 2395, Loss 0.2389196753501892\n",
      "[Training Epoch 9] Batch 2396, Loss 0.23919552564620972\n",
      "[Training Epoch 9] Batch 2397, Loss 0.26482778787612915\n",
      "[Training Epoch 9] Batch 2398, Loss 0.2170383483171463\n",
      "[Training Epoch 9] Batch 2399, Loss 0.2632392644882202\n",
      "[Training Epoch 9] Batch 2400, Loss 0.23555810749530792\n",
      "[Training Epoch 9] Batch 2401, Loss 0.2571590542793274\n",
      "[Training Epoch 9] Batch 2402, Loss 0.3016727566719055\n",
      "[Training Epoch 9] Batch 2403, Loss 0.24352048337459564\n",
      "[Training Epoch 9] Batch 2404, Loss 0.2506406009197235\n",
      "[Training Epoch 9] Batch 2405, Loss 0.2592296004295349\n",
      "[Training Epoch 9] Batch 2406, Loss 0.25341400504112244\n",
      "[Training Epoch 9] Batch 2407, Loss 0.24883809685707092\n",
      "[Training Epoch 9] Batch 2408, Loss 0.25136446952819824\n",
      "[Training Epoch 9] Batch 2409, Loss 0.26012951135635376\n",
      "[Training Epoch 9] Batch 2410, Loss 0.2498924881219864\n",
      "[Training Epoch 9] Batch 2411, Loss 0.23602047562599182\n",
      "[Training Epoch 9] Batch 2412, Loss 0.26562631130218506\n",
      "[Training Epoch 9] Batch 2413, Loss 0.2511960566043854\n",
      "[Training Epoch 9] Batch 2414, Loss 0.2519427239894867\n",
      "[Training Epoch 9] Batch 2415, Loss 0.2761003077030182\n",
      "[Training Epoch 9] Batch 2416, Loss 0.24847213923931122\n",
      "[Training Epoch 9] Batch 2417, Loss 0.2714926302433014\n",
      "[Training Epoch 9] Batch 2418, Loss 0.24793806672096252\n",
      "[Training Epoch 9] Batch 2419, Loss 0.23562350869178772\n",
      "[Training Epoch 9] Batch 2420, Loss 0.2610704004764557\n",
      "[Training Epoch 9] Batch 2421, Loss 0.24802464246749878\n",
      "[Training Epoch 9] Batch 2422, Loss 0.2724985182285309\n",
      "[Training Epoch 9] Batch 2423, Loss 0.2676677107810974\n",
      "[Training Epoch 9] Batch 2424, Loss 0.26659613847732544\n",
      "[Training Epoch 9] Batch 2425, Loss 0.23258453607559204\n",
      "[Training Epoch 9] Batch 2426, Loss 0.2708684802055359\n",
      "[Training Epoch 9] Batch 2427, Loss 0.24648816883563995\n",
      "[Training Epoch 9] Batch 2428, Loss 0.2475435584783554\n",
      "[Training Epoch 9] Batch 2429, Loss 0.2411639392375946\n",
      "[Training Epoch 9] Batch 2430, Loss 0.25128960609436035\n",
      "[Training Epoch 9] Batch 2431, Loss 0.25762253999710083\n",
      "[Training Epoch 9] Batch 2432, Loss 0.2738259732723236\n",
      "[Training Epoch 9] Batch 2433, Loss 0.2627534866333008\n",
      "[Training Epoch 9] Batch 2434, Loss 0.27252066135406494\n",
      "[Training Epoch 9] Batch 2435, Loss 0.2520284652709961\n",
      "[Training Epoch 9] Batch 2436, Loss 0.23920567333698273\n",
      "[Training Epoch 9] Batch 2437, Loss 0.23002144694328308\n",
      "[Training Epoch 9] Batch 2438, Loss 0.24899886548519135\n",
      "[Training Epoch 9] Batch 2439, Loss 0.23895972967147827\n",
      "[Training Epoch 9] Batch 2440, Loss 0.25159937143325806\n",
      "[Training Epoch 9] Batch 2441, Loss 0.2718583047389984\n",
      "[Training Epoch 9] Batch 2442, Loss 0.23060208559036255\n",
      "[Training Epoch 9] Batch 2443, Loss 0.24437165260314941\n",
      "[Training Epoch 9] Batch 2444, Loss 0.2549061179161072\n",
      "[Training Epoch 9] Batch 2445, Loss 0.24704724550247192\n",
      "[Training Epoch 9] Batch 2446, Loss 0.24356496334075928\n",
      "[Training Epoch 9] Batch 2447, Loss 0.26234742999076843\n",
      "[Training Epoch 9] Batch 2448, Loss 0.2647046744823456\n",
      "[Training Epoch 9] Batch 2449, Loss 0.24993491172790527\n",
      "[Training Epoch 9] Batch 2450, Loss 0.2783740162849426\n",
      "[Training Epoch 9] Batch 2451, Loss 0.23804906010627747\n",
      "[Training Epoch 9] Batch 2452, Loss 0.2521297335624695\n",
      "[Training Epoch 9] Batch 2453, Loss 0.25365328788757324\n",
      "[Training Epoch 9] Batch 2454, Loss 0.2471301108598709\n",
      "[Training Epoch 9] Batch 2455, Loss 0.24507367610931396\n",
      "[Training Epoch 9] Batch 2456, Loss 0.22997543215751648\n",
      "[Training Epoch 9] Batch 2457, Loss 0.2543194890022278\n",
      "[Training Epoch 9] Batch 2458, Loss 0.24639108777046204\n",
      "[Training Epoch 9] Batch 2459, Loss 0.2785613238811493\n",
      "[Training Epoch 9] Batch 2460, Loss 0.24424850940704346\n",
      "[Training Epoch 9] Batch 2461, Loss 0.27814996242523193\n",
      "[Training Epoch 9] Batch 2462, Loss 0.24121958017349243\n",
      "[Training Epoch 9] Batch 2463, Loss 0.2544167935848236\n",
      "[Training Epoch 9] Batch 2464, Loss 0.233097106218338\n",
      "[Training Epoch 9] Batch 2465, Loss 0.2599315047264099\n",
      "[Training Epoch 9] Batch 2466, Loss 0.2671982944011688\n",
      "[Training Epoch 9] Batch 2467, Loss 0.2510252892971039\n",
      "[Training Epoch 9] Batch 2468, Loss 0.2705196738243103\n",
      "[Training Epoch 9] Batch 2469, Loss 0.2785123586654663\n",
      "[Training Epoch 9] Batch 2470, Loss 0.2506248652935028\n",
      "[Training Epoch 9] Batch 2471, Loss 0.2664910852909088\n",
      "[Training Epoch 9] Batch 2472, Loss 0.24351580440998077\n",
      "[Training Epoch 9] Batch 2473, Loss 0.24390557408332825\n",
      "[Training Epoch 9] Batch 2474, Loss 0.23739489912986755\n",
      "[Training Epoch 9] Batch 2475, Loss 0.24407221376895905\n",
      "[Training Epoch 9] Batch 2476, Loss 0.2638004422187805\n",
      "[Training Epoch 9] Batch 2477, Loss 0.24628332257270813\n",
      "[Training Epoch 9] Batch 2478, Loss 0.23936998844146729\n",
      "[Training Epoch 9] Batch 2479, Loss 0.2966250777244568\n",
      "[Training Epoch 9] Batch 2480, Loss 0.265493243932724\n",
      "[Training Epoch 9] Batch 2481, Loss 0.21284446120262146\n",
      "[Training Epoch 9] Batch 2482, Loss 0.2550106942653656\n",
      "[Training Epoch 9] Batch 2483, Loss 0.2602585554122925\n",
      "[Training Epoch 9] Batch 2484, Loss 0.22848081588745117\n",
      "[Training Epoch 9] Batch 2485, Loss 0.2662370800971985\n",
      "[Training Epoch 9] Batch 2486, Loss 0.2573089003562927\n",
      "[Training Epoch 9] Batch 2487, Loss 0.2555064558982849\n",
      "[Training Epoch 9] Batch 2488, Loss 0.270596981048584\n",
      "[Training Epoch 9] Batch 2489, Loss 0.2295272946357727\n",
      "[Training Epoch 9] Batch 2490, Loss 0.25054579973220825\n",
      "[Training Epoch 9] Batch 2491, Loss 0.24407172203063965\n",
      "[Training Epoch 9] Batch 2492, Loss 0.26039350032806396\n",
      "[Training Epoch 9] Batch 2493, Loss 0.2769436836242676\n",
      "[Training Epoch 9] Batch 2494, Loss 0.2809510827064514\n",
      "[Training Epoch 9] Batch 2495, Loss 0.25230589509010315\n",
      "[Training Epoch 9] Batch 2496, Loss 0.24319136142730713\n",
      "[Training Epoch 9] Batch 2497, Loss 0.2851254343986511\n",
      "[Training Epoch 9] Batch 2498, Loss 0.2307974398136139\n",
      "[Training Epoch 9] Batch 2499, Loss 0.2454656958580017\n",
      "[Training Epoch 9] Batch 2500, Loss 0.2390025109052658\n",
      "[Training Epoch 9] Batch 2501, Loss 0.28144508600234985\n",
      "[Training Epoch 9] Batch 2502, Loss 0.261067271232605\n",
      "[Training Epoch 9] Batch 2503, Loss 0.2502047121524811\n",
      "[Training Epoch 9] Batch 2504, Loss 0.2422187477350235\n",
      "[Training Epoch 9] Batch 2505, Loss 0.28720396757125854\n",
      "[Training Epoch 9] Batch 2506, Loss 0.23647718131542206\n",
      "[Training Epoch 9] Batch 2507, Loss 0.23857252299785614\n",
      "[Training Epoch 9] Batch 2508, Loss 0.2549412250518799\n",
      "[Training Epoch 9] Batch 2509, Loss 0.2805078625679016\n",
      "[Training Epoch 9] Batch 2510, Loss 0.26518717408180237\n",
      "[Training Epoch 9] Batch 2511, Loss 0.25738272070884705\n",
      "[Training Epoch 9] Batch 2512, Loss 0.28868532180786133\n",
      "[Training Epoch 9] Batch 2513, Loss 0.2572544515132904\n",
      "[Training Epoch 9] Batch 2514, Loss 0.26263514161109924\n",
      "[Training Epoch 9] Batch 2515, Loss 0.2611691951751709\n",
      "[Training Epoch 9] Batch 2516, Loss 0.2807857394218445\n",
      "[Training Epoch 9] Batch 2517, Loss 0.26336029171943665\n",
      "[Training Epoch 9] Batch 2518, Loss 0.24926109611988068\n",
      "[Training Epoch 9] Batch 2519, Loss 0.25367555022239685\n",
      "[Training Epoch 9] Batch 2520, Loss 0.23180729150772095\n",
      "[Training Epoch 9] Batch 2521, Loss 0.2545996606349945\n",
      "[Training Epoch 9] Batch 2522, Loss 0.2628380060195923\n",
      "[Training Epoch 9] Batch 2523, Loss 0.2703285217285156\n",
      "[Training Epoch 9] Batch 2524, Loss 0.26106464862823486\n",
      "[Training Epoch 9] Batch 2525, Loss 0.245416522026062\n",
      "[Training Epoch 9] Batch 2526, Loss 0.2551429569721222\n",
      "[Training Epoch 9] Batch 2527, Loss 0.25623947381973267\n",
      "[Training Epoch 9] Batch 2528, Loss 0.25851529836654663\n",
      "[Training Epoch 9] Batch 2529, Loss 0.24592238664627075\n",
      "[Training Epoch 9] Batch 2530, Loss 0.26944854855537415\n",
      "[Training Epoch 9] Batch 2531, Loss 0.2366606593132019\n",
      "[Training Epoch 9] Batch 2532, Loss 0.2778598964214325\n",
      "[Training Epoch 9] Batch 2533, Loss 0.2751643657684326\n",
      "[Training Epoch 9] Batch 2534, Loss 0.2692588269710541\n",
      "[Training Epoch 9] Batch 2535, Loss 0.22765228152275085\n",
      "[Training Epoch 9] Batch 2536, Loss 0.2473044991493225\n",
      "[Training Epoch 9] Batch 2537, Loss 0.25830012559890747\n",
      "[Training Epoch 9] Batch 2538, Loss 0.23051206767559052\n",
      "[Training Epoch 9] Batch 2539, Loss 0.25016939640045166\n",
      "[Training Epoch 9] Batch 2540, Loss 0.24041110277175903\n",
      "[Training Epoch 9] Batch 2541, Loss 0.23865742981433868\n",
      "[Training Epoch 9] Batch 2542, Loss 0.22771617770195007\n",
      "[Training Epoch 9] Batch 2543, Loss 0.24139505624771118\n",
      "[Training Epoch 9] Batch 2544, Loss 0.25094836950302124\n",
      "[Training Epoch 9] Batch 2545, Loss 0.2684367895126343\n",
      "[Training Epoch 9] Batch 2546, Loss 0.22733339667320251\n",
      "[Training Epoch 9] Batch 2547, Loss 0.26538121700286865\n",
      "[Training Epoch 9] Batch 2548, Loss 0.2504130005836487\n",
      "[Training Epoch 9] Batch 2549, Loss 0.24430865049362183\n",
      "[Training Epoch 9] Batch 2550, Loss 0.2492659091949463\n",
      "[Training Epoch 9] Batch 2551, Loss 0.2584574520587921\n",
      "[Training Epoch 9] Batch 2552, Loss 0.2689864933490753\n",
      "[Training Epoch 9] Batch 2553, Loss 0.2703801989555359\n",
      "[Training Epoch 9] Batch 2554, Loss 0.2449176013469696\n",
      "[Training Epoch 9] Batch 2555, Loss 0.23011897504329681\n",
      "[Training Epoch 9] Batch 2556, Loss 0.2428535670042038\n",
      "[Training Epoch 9] Batch 2557, Loss 0.2547144293785095\n",
      "[Training Epoch 9] Batch 2558, Loss 0.247905433177948\n",
      "[Training Epoch 9] Batch 2559, Loss 0.2527291774749756\n",
      "[Training Epoch 9] Batch 2560, Loss 0.2879769206047058\n",
      "[Training Epoch 9] Batch 2561, Loss 0.26303431391716003\n",
      "[Training Epoch 9] Batch 2562, Loss 0.2532484233379364\n",
      "[Training Epoch 9] Batch 2563, Loss 0.260353684425354\n",
      "[Training Epoch 9] Batch 2564, Loss 0.2273578643798828\n",
      "[Training Epoch 9] Batch 2565, Loss 0.22640357911586761\n",
      "[Training Epoch 9] Batch 2566, Loss 0.2529591917991638\n",
      "[Training Epoch 9] Batch 2567, Loss 0.2622288465499878\n",
      "[Training Epoch 9] Batch 2568, Loss 0.26675117015838623\n",
      "[Training Epoch 9] Batch 2569, Loss 0.23405160009860992\n",
      "[Training Epoch 9] Batch 2570, Loss 0.2599685788154602\n",
      "[Training Epoch 9] Batch 2571, Loss 0.24782702326774597\n",
      "[Training Epoch 9] Batch 2572, Loss 0.25225868821144104\n",
      "[Training Epoch 9] Batch 2573, Loss 0.27178943157196045\n",
      "[Training Epoch 9] Batch 2574, Loss 0.24382388591766357\n",
      "[Training Epoch 9] Batch 2575, Loss 0.2646141052246094\n",
      "[Training Epoch 9] Batch 2576, Loss 0.2727595567703247\n",
      "[Training Epoch 9] Batch 2577, Loss 0.26810649037361145\n",
      "[Training Epoch 9] Batch 2578, Loss 0.22293302416801453\n",
      "[Training Epoch 9] Batch 2579, Loss 0.26150214672088623\n",
      "[Training Epoch 9] Batch 2580, Loss 0.23596987128257751\n",
      "[Training Epoch 9] Batch 2581, Loss 0.23632565140724182\n",
      "[Training Epoch 9] Batch 2582, Loss 0.2408522367477417\n",
      "[Training Epoch 9] Batch 2583, Loss 0.23570483922958374\n",
      "[Training Epoch 9] Batch 2584, Loss 0.22694338858127594\n",
      "[Training Epoch 9] Batch 2585, Loss 0.2543511390686035\n",
      "[Training Epoch 9] Batch 2586, Loss 0.2477213591337204\n",
      "[Training Epoch 9] Batch 2587, Loss 0.24613922834396362\n",
      "[Training Epoch 9] Batch 2588, Loss 0.2530745267868042\n",
      "[Training Epoch 9] Batch 2589, Loss 0.2633342742919922\n",
      "[Training Epoch 9] Batch 2590, Loss 0.2632913589477539\n",
      "[Training Epoch 9] Batch 2591, Loss 0.25413814187049866\n",
      "[Training Epoch 9] Batch 2592, Loss 0.23239469528198242\n",
      "[Training Epoch 9] Batch 2593, Loss 0.22792094945907593\n",
      "[Training Epoch 9] Batch 2594, Loss 0.2561195492744446\n",
      "[Training Epoch 9] Batch 2595, Loss 0.24313940107822418\n",
      "[Training Epoch 9] Batch 2596, Loss 0.25880351662635803\n",
      "[Training Epoch 9] Batch 2597, Loss 0.26334354281425476\n",
      "[Training Epoch 9] Batch 2598, Loss 0.23244965076446533\n",
      "[Training Epoch 9] Batch 2599, Loss 0.21949274837970734\n",
      "[Training Epoch 9] Batch 2600, Loss 0.2464933544397354\n",
      "[Training Epoch 9] Batch 2601, Loss 0.26977694034576416\n",
      "[Training Epoch 9] Batch 2602, Loss 0.2667773365974426\n",
      "[Training Epoch 9] Batch 2603, Loss 0.2763538956642151\n",
      "[Training Epoch 9] Batch 2604, Loss 0.2676822245121002\n",
      "[Training Epoch 9] Batch 2605, Loss 0.23156306147575378\n",
      "[Training Epoch 9] Batch 2606, Loss 0.26742660999298096\n",
      "[Training Epoch 9] Batch 2607, Loss 0.3037610948085785\n",
      "[Training Epoch 9] Batch 2608, Loss 0.2522745728492737\n",
      "[Training Epoch 9] Batch 2609, Loss 0.26377636194229126\n",
      "[Training Epoch 9] Batch 2610, Loss 0.2652454376220703\n",
      "[Training Epoch 9] Batch 2611, Loss 0.25300315022468567\n",
      "[Training Epoch 9] Batch 2612, Loss 0.23955902457237244\n",
      "[Training Epoch 9] Batch 2613, Loss 0.2283935248851776\n",
      "[Training Epoch 9] Batch 2614, Loss 0.2632533013820648\n",
      "[Training Epoch 9] Batch 2615, Loss 0.25527793169021606\n",
      "[Training Epoch 9] Batch 2616, Loss 0.2578955590724945\n",
      "[Training Epoch 9] Batch 2617, Loss 0.24898561835289001\n",
      "[Training Epoch 9] Batch 2618, Loss 0.2427968829870224\n",
      "[Training Epoch 9] Batch 2619, Loss 0.24323034286499023\n",
      "[Training Epoch 9] Batch 2620, Loss 0.2647964656352997\n",
      "[Training Epoch 9] Batch 2621, Loss 0.289362370967865\n",
      "[Training Epoch 9] Batch 2622, Loss 0.25963491201400757\n",
      "[Training Epoch 9] Batch 2623, Loss 0.2581966519355774\n",
      "[Training Epoch 9] Batch 2624, Loss 0.23785234987735748\n",
      "[Training Epoch 9] Batch 2625, Loss 0.24114012718200684\n",
      "[Training Epoch 9] Batch 2626, Loss 0.25205084681510925\n",
      "[Training Epoch 9] Batch 2627, Loss 0.2632181942462921\n",
      "[Training Epoch 9] Batch 2628, Loss 0.24414712190628052\n",
      "[Training Epoch 9] Batch 2629, Loss 0.25534722208976746\n",
      "[Training Epoch 9] Batch 2630, Loss 0.25532644987106323\n",
      "[Training Epoch 9] Batch 2631, Loss 0.24559463560581207\n",
      "[Training Epoch 9] Batch 2632, Loss 0.26520898938179016\n",
      "[Training Epoch 9] Batch 2633, Loss 0.2578970491886139\n",
      "[Training Epoch 9] Batch 2634, Loss 0.2560472786426544\n",
      "[Training Epoch 9] Batch 2635, Loss 0.2685340642929077\n",
      "[Training Epoch 9] Batch 2636, Loss 0.24962636828422546\n",
      "[Training Epoch 9] Batch 2637, Loss 0.24203696846961975\n",
      "[Training Epoch 9] Batch 2638, Loss 0.24152204394340515\n",
      "[Training Epoch 9] Batch 2639, Loss 0.2427673488855362\n",
      "[Training Epoch 9] Batch 2640, Loss 0.24202600121498108\n",
      "[Training Epoch 9] Batch 2641, Loss 0.2787155508995056\n",
      "[Training Epoch 9] Batch 2642, Loss 0.2520936131477356\n",
      "[Training Epoch 9] Batch 2643, Loss 0.24799928069114685\n",
      "[Training Epoch 9] Batch 2644, Loss 0.2502345144748688\n",
      "[Training Epoch 9] Batch 2645, Loss 0.2566460371017456\n",
      "[Training Epoch 9] Batch 2646, Loss 0.2583562135696411\n",
      "[Training Epoch 9] Batch 2647, Loss 0.2582521438598633\n",
      "[Training Epoch 9] Batch 2648, Loss 0.2430274486541748\n",
      "[Training Epoch 9] Batch 2649, Loss 0.23122447729110718\n",
      "[Training Epoch 9] Batch 2650, Loss 0.2270422726869583\n",
      "[Training Epoch 9] Batch 2651, Loss 0.2557060718536377\n",
      "[Training Epoch 9] Batch 2652, Loss 0.27936500310897827\n",
      "[Training Epoch 9] Batch 2653, Loss 0.2415475994348526\n",
      "[Training Epoch 9] Batch 2654, Loss 0.2371438890695572\n",
      "[Training Epoch 9] Batch 2655, Loss 0.2803184688091278\n",
      "[Training Epoch 9] Batch 2656, Loss 0.2397286593914032\n",
      "[Training Epoch 9] Batch 2657, Loss 0.2610187530517578\n",
      "[Training Epoch 9] Batch 2658, Loss 0.2370554655790329\n",
      "[Training Epoch 9] Batch 2659, Loss 0.270068883895874\n",
      "[Training Epoch 9] Batch 2660, Loss 0.25997811555862427\n",
      "[Training Epoch 9] Batch 2661, Loss 0.25064629316329956\n",
      "[Training Epoch 9] Batch 2662, Loss 0.2483830600976944\n",
      "[Training Epoch 9] Batch 2663, Loss 0.2651914060115814\n",
      "[Training Epoch 9] Batch 2664, Loss 0.25974616408348083\n",
      "[Training Epoch 9] Batch 2665, Loss 0.249286487698555\n",
      "[Training Epoch 9] Batch 2666, Loss 0.25585246086120605\n",
      "[Training Epoch 9] Batch 2667, Loss 0.23318517208099365\n",
      "[Training Epoch 9] Batch 2668, Loss 0.23792527616024017\n",
      "[Training Epoch 9] Batch 2669, Loss 0.27350425720214844\n",
      "[Training Epoch 9] Batch 2670, Loss 0.28232884407043457\n",
      "[Training Epoch 9] Batch 2671, Loss 0.2641093134880066\n",
      "[Training Epoch 9] Batch 2672, Loss 0.2337208390235901\n",
      "[Training Epoch 9] Batch 2673, Loss 0.2887459397315979\n",
      "[Training Epoch 9] Batch 2674, Loss 0.26516368985176086\n",
      "[Training Epoch 9] Batch 2675, Loss 0.22532208263874054\n",
      "[Training Epoch 9] Batch 2676, Loss 0.2439771294593811\n",
      "[Training Epoch 9] Batch 2677, Loss 0.26966142654418945\n",
      "[Training Epoch 9] Batch 2678, Loss 0.2543495297431946\n",
      "[Training Epoch 9] Batch 2679, Loss 0.2545117437839508\n",
      "[Training Epoch 9] Batch 2680, Loss 0.259186327457428\n",
      "[Training Epoch 9] Batch 2681, Loss 0.24711379408836365\n",
      "[Training Epoch 9] Batch 2682, Loss 0.2542894184589386\n",
      "[Training Epoch 9] Batch 2683, Loss 0.24044087529182434\n",
      "[Training Epoch 9] Batch 2684, Loss 0.26200270652770996\n",
      "[Training Epoch 9] Batch 2685, Loss 0.255531907081604\n",
      "[Training Epoch 9] Batch 2686, Loss 0.2626355290412903\n",
      "[Training Epoch 9] Batch 2687, Loss 0.2604699730873108\n",
      "[Training Epoch 9] Batch 2688, Loss 0.2411424219608307\n",
      "[Training Epoch 9] Batch 2689, Loss 0.26739513874053955\n",
      "[Training Epoch 9] Batch 2690, Loss 0.2751382291316986\n",
      "[Training Epoch 9] Batch 2691, Loss 0.23043420910835266\n",
      "[Training Epoch 9] Batch 2692, Loss 0.26150673627853394\n",
      "[Training Epoch 9] Batch 2693, Loss 0.2541217505931854\n",
      "[Training Epoch 9] Batch 2694, Loss 0.2747092545032501\n",
      "[Training Epoch 9] Batch 2695, Loss 0.26958703994750977\n",
      "[Training Epoch 9] Batch 2696, Loss 0.28034788370132446\n",
      "[Training Epoch 9] Batch 2697, Loss 0.25201958417892456\n",
      "[Training Epoch 9] Batch 2698, Loss 0.251841276884079\n",
      "[Training Epoch 9] Batch 2699, Loss 0.24211208522319794\n",
      "[Training Epoch 9] Batch 2700, Loss 0.24614562094211578\n",
      "[Training Epoch 9] Batch 2701, Loss 0.218973308801651\n",
      "[Training Epoch 9] Batch 2702, Loss 0.24487248063087463\n",
      "[Training Epoch 9] Batch 2703, Loss 0.2662765681743622\n",
      "[Training Epoch 9] Batch 2704, Loss 0.24444414675235748\n",
      "[Training Epoch 9] Batch 2705, Loss 0.24028298258781433\n",
      "[Training Epoch 9] Batch 2706, Loss 0.24208533763885498\n",
      "[Training Epoch 9] Batch 2707, Loss 0.29558447003364563\n",
      "[Training Epoch 9] Batch 2708, Loss 0.23110035061836243\n",
      "[Training Epoch 9] Batch 2709, Loss 0.24074508249759674\n",
      "[Training Epoch 9] Batch 2710, Loss 0.23828262090682983\n",
      "[Training Epoch 9] Batch 2711, Loss 0.25366097688674927\n",
      "[Training Epoch 9] Batch 2712, Loss 0.2553229331970215\n",
      "[Training Epoch 9] Batch 2713, Loss 0.23779460787773132\n",
      "[Training Epoch 9] Batch 2714, Loss 0.2567044496536255\n",
      "[Training Epoch 9] Batch 2715, Loss 0.23762434720993042\n",
      "[Training Epoch 9] Batch 2716, Loss 0.2548055052757263\n",
      "[Training Epoch 9] Batch 2717, Loss 0.23546937108039856\n",
      "[Training Epoch 9] Batch 2718, Loss 0.25794321298599243\n",
      "[Training Epoch 9] Batch 2719, Loss 0.2575831413269043\n",
      "[Training Epoch 9] Batch 2720, Loss 0.2254101037979126\n",
      "[Training Epoch 9] Batch 2721, Loss 0.2542945146560669\n",
      "[Training Epoch 9] Batch 2722, Loss 0.24888773262500763\n",
      "[Training Epoch 9] Batch 2723, Loss 0.252951443195343\n",
      "[Training Epoch 9] Batch 2724, Loss 0.24592620134353638\n",
      "[Training Epoch 9] Batch 2725, Loss 0.27185165882110596\n",
      "[Training Epoch 9] Batch 2726, Loss 0.218496173620224\n",
      "[Training Epoch 9] Batch 2727, Loss 0.2777497172355652\n",
      "[Training Epoch 9] Batch 2728, Loss 0.24748660624027252\n",
      "[Training Epoch 9] Batch 2729, Loss 0.24649649858474731\n",
      "[Training Epoch 9] Batch 2730, Loss 0.23336949944496155\n",
      "[Training Epoch 9] Batch 2731, Loss 0.23401038348674774\n",
      "[Training Epoch 9] Batch 2732, Loss 0.25123435258865356\n",
      "[Training Epoch 9] Batch 2733, Loss 0.2542549967765808\n",
      "[Training Epoch 9] Batch 2734, Loss 0.25707152485847473\n",
      "[Training Epoch 9] Batch 2735, Loss 0.27082788944244385\n",
      "[Training Epoch 9] Batch 2736, Loss 0.24271896481513977\n",
      "[Training Epoch 9] Batch 2737, Loss 0.24267393350601196\n",
      "[Training Epoch 9] Batch 2738, Loss 0.23948650062084198\n",
      "[Training Epoch 9] Batch 2739, Loss 0.23052245378494263\n",
      "[Training Epoch 9] Batch 2740, Loss 0.23159345984458923\n",
      "[Training Epoch 9] Batch 2741, Loss 0.2630797326564789\n",
      "[Training Epoch 9] Batch 2742, Loss 0.2754285931587219\n",
      "[Training Epoch 9] Batch 2743, Loss 0.25478219985961914\n",
      "[Training Epoch 9] Batch 2744, Loss 0.24724897742271423\n",
      "[Training Epoch 9] Batch 2745, Loss 0.26819121837615967\n",
      "[Training Epoch 9] Batch 2746, Loss 0.2560616135597229\n",
      "[Training Epoch 9] Batch 2747, Loss 0.26236987113952637\n",
      "[Training Epoch 9] Batch 2748, Loss 0.24049948155879974\n",
      "[Training Epoch 9] Batch 2749, Loss 0.2908009886741638\n",
      "[Training Epoch 9] Batch 2750, Loss 0.2453884780406952\n",
      "[Training Epoch 9] Batch 2751, Loss 0.23667968809604645\n",
      "[Training Epoch 9] Batch 2752, Loss 0.2588256597518921\n",
      "[Training Epoch 9] Batch 2753, Loss 0.25011324882507324\n",
      "[Training Epoch 9] Batch 2754, Loss 0.23927779495716095\n",
      "[Training Epoch 9] Batch 2755, Loss 0.24285434186458588\n",
      "[Training Epoch 9] Batch 2756, Loss 0.26161956787109375\n",
      "[Training Epoch 9] Batch 2757, Loss 0.25436949729919434\n",
      "[Training Epoch 9] Batch 2758, Loss 0.23909147083759308\n",
      "[Training Epoch 9] Batch 2759, Loss 0.22783523797988892\n",
      "[Training Epoch 9] Batch 2760, Loss 0.21729110181331635\n",
      "[Training Epoch 9] Batch 2761, Loss 0.24633881449699402\n",
      "[Training Epoch 9] Batch 2762, Loss 0.25193026661872864\n",
      "[Training Epoch 9] Batch 2763, Loss 0.25185519456863403\n",
      "[Training Epoch 9] Batch 2764, Loss 0.25208544731140137\n",
      "[Training Epoch 9] Batch 2765, Loss 0.25365275144577026\n",
      "[Training Epoch 9] Batch 2766, Loss 0.2389482855796814\n",
      "[Training Epoch 9] Batch 2767, Loss 0.23049631714820862\n",
      "[Training Epoch 9] Batch 2768, Loss 0.24065199494361877\n",
      "[Training Epoch 9] Batch 2769, Loss 0.2617010474205017\n",
      "[Training Epoch 9] Batch 2770, Loss 0.22588786482810974\n",
      "[Training Epoch 9] Batch 2771, Loss 0.23610475659370422\n",
      "[Training Epoch 9] Batch 2772, Loss 0.26080071926116943\n",
      "[Training Epoch 9] Batch 2773, Loss 0.2852187752723694\n",
      "[Training Epoch 9] Batch 2774, Loss 0.2508426010608673\n",
      "[Training Epoch 9] Batch 2775, Loss 0.2554594874382019\n",
      "[Training Epoch 9] Batch 2776, Loss 0.2616298794746399\n",
      "[Training Epoch 9] Batch 2777, Loss 0.2593654990196228\n",
      "[Training Epoch 9] Batch 2778, Loss 0.24369962513446808\n",
      "[Training Epoch 9] Batch 2779, Loss 0.249528706073761\n",
      "[Training Epoch 9] Batch 2780, Loss 0.26667946577072144\n",
      "[Training Epoch 9] Batch 2781, Loss 0.2573358714580536\n",
      "[Training Epoch 9] Batch 2782, Loss 0.2622124254703522\n",
      "[Training Epoch 9] Batch 2783, Loss 0.23542340099811554\n",
      "[Training Epoch 9] Batch 2784, Loss 0.268806129693985\n",
      "[Training Epoch 9] Batch 2785, Loss 0.26875266432762146\n",
      "[Training Epoch 9] Batch 2786, Loss 0.2656496465206146\n",
      "[Training Epoch 9] Batch 2787, Loss 0.24580413103103638\n",
      "[Training Epoch 9] Batch 2788, Loss 0.23911088705062866\n",
      "[Training Epoch 9] Batch 2789, Loss 0.23735086619853973\n",
      "[Training Epoch 9] Batch 2790, Loss 0.27496305108070374\n",
      "[Training Epoch 9] Batch 2791, Loss 0.2611957788467407\n",
      "[Training Epoch 9] Batch 2792, Loss 0.2637093961238861\n",
      "[Training Epoch 9] Batch 2793, Loss 0.2810676097869873\n",
      "[Training Epoch 9] Batch 2794, Loss 0.2565518319606781\n",
      "[Training Epoch 9] Batch 2795, Loss 0.26943713426589966\n",
      "[Training Epoch 9] Batch 2796, Loss 0.2601167559623718\n",
      "[Training Epoch 9] Batch 2797, Loss 0.22969435155391693\n",
      "[Training Epoch 9] Batch 2798, Loss 0.2550920844078064\n",
      "[Training Epoch 9] Batch 2799, Loss 0.25229620933532715\n",
      "[Training Epoch 9] Batch 2800, Loss 0.2184678316116333\n",
      "[Training Epoch 9] Batch 2801, Loss 0.25482240319252014\n",
      "[Training Epoch 9] Batch 2802, Loss 0.2722089886665344\n",
      "[Training Epoch 9] Batch 2803, Loss 0.2526472508907318\n",
      "[Training Epoch 9] Batch 2804, Loss 0.2513774633407593\n",
      "[Training Epoch 9] Batch 2805, Loss 0.2405748963356018\n",
      "[Training Epoch 9] Batch 2806, Loss 0.26288342475891113\n",
      "[Training Epoch 9] Batch 2807, Loss 0.28229206800460815\n",
      "[Training Epoch 9] Batch 2808, Loss 0.2628045082092285\n",
      "[Training Epoch 9] Batch 2809, Loss 0.22202621400356293\n",
      "[Training Epoch 9] Batch 2810, Loss 0.2595152258872986\n",
      "[Training Epoch 9] Batch 2811, Loss 0.26508837938308716\n",
      "[Training Epoch 9] Batch 2812, Loss 0.2439018040895462\n",
      "[Training Epoch 9] Batch 2813, Loss 0.26614752411842346\n",
      "[Training Epoch 9] Batch 2814, Loss 0.24405130743980408\n",
      "[Training Epoch 9] Batch 2815, Loss 0.25915253162384033\n",
      "[Training Epoch 9] Batch 2816, Loss 0.22757117450237274\n",
      "[Training Epoch 9] Batch 2817, Loss 0.25953012704849243\n",
      "[Training Epoch 9] Batch 2818, Loss 0.2530936002731323\n",
      "[Training Epoch 9] Batch 2819, Loss 0.255157470703125\n",
      "[Training Epoch 9] Batch 2820, Loss 0.23030205070972443\n",
      "[Training Epoch 9] Batch 2821, Loss 0.26339191198349\n",
      "[Training Epoch 9] Batch 2822, Loss 0.24329912662506104\n",
      "[Training Epoch 9] Batch 2823, Loss 0.28420141339302063\n",
      "[Training Epoch 9] Batch 2824, Loss 0.2400166094303131\n",
      "[Training Epoch 9] Batch 2825, Loss 0.26121944189071655\n",
      "[Training Epoch 9] Batch 2826, Loss 0.27461355924606323\n",
      "[Training Epoch 9] Batch 2827, Loss 0.2755581736564636\n",
      "[Training Epoch 9] Batch 2828, Loss 0.2556469738483429\n",
      "[Training Epoch 9] Batch 2829, Loss 0.22991755604743958\n",
      "[Training Epoch 9] Batch 2830, Loss 0.2748870849609375\n",
      "[Training Epoch 9] Batch 2831, Loss 0.23929178714752197\n",
      "[Training Epoch 9] Batch 2832, Loss 0.2968333959579468\n",
      "[Training Epoch 9] Batch 2833, Loss 0.2573646605014801\n",
      "[Training Epoch 9] Batch 2834, Loss 0.2738223075866699\n",
      "[Training Epoch 9] Batch 2835, Loss 0.24952135980129242\n",
      "[Training Epoch 9] Batch 2836, Loss 0.2862468361854553\n",
      "[Training Epoch 9] Batch 2837, Loss 0.23257282376289368\n",
      "[Training Epoch 9] Batch 2838, Loss 0.28643929958343506\n",
      "[Training Epoch 9] Batch 2839, Loss 0.27667373418807983\n",
      "[Training Epoch 9] Batch 2840, Loss 0.2360302060842514\n",
      "[Training Epoch 9] Batch 2841, Loss 0.25992637872695923\n",
      "[Training Epoch 9] Batch 2842, Loss 0.24735933542251587\n",
      "[Training Epoch 9] Batch 2843, Loss 0.22798213362693787\n",
      "[Training Epoch 9] Batch 2844, Loss 0.2681620121002197\n",
      "[Training Epoch 9] Batch 2845, Loss 0.2667257487773895\n",
      "[Training Epoch 9] Batch 2846, Loss 0.22147898375988007\n",
      "[Training Epoch 9] Batch 2847, Loss 0.25718531012535095\n",
      "[Training Epoch 9] Batch 2848, Loss 0.27473512291908264\n",
      "[Training Epoch 9] Batch 2849, Loss 0.27468961477279663\n",
      "[Training Epoch 9] Batch 2850, Loss 0.2578008472919464\n",
      "[Training Epoch 9] Batch 2851, Loss 0.24319565296173096\n",
      "[Training Epoch 9] Batch 2852, Loss 0.26107877492904663\n",
      "[Training Epoch 9] Batch 2853, Loss 0.2371026575565338\n",
      "[Training Epoch 9] Batch 2854, Loss 0.2601562738418579\n",
      "[Training Epoch 9] Batch 2855, Loss 0.2518056631088257\n",
      "[Training Epoch 9] Batch 2856, Loss 0.2283150851726532\n",
      "[Training Epoch 9] Batch 2857, Loss 0.2362109124660492\n",
      "[Training Epoch 9] Batch 2858, Loss 0.25718721747398376\n",
      "[Training Epoch 9] Batch 2859, Loss 0.26838743686676025\n",
      "[Training Epoch 9] Batch 2860, Loss 0.24644812941551208\n",
      "[Training Epoch 9] Batch 2861, Loss 0.2708037793636322\n",
      "[Training Epoch 9] Batch 2862, Loss 0.2707272171974182\n",
      "[Training Epoch 9] Batch 2863, Loss 0.26539552211761475\n",
      "[Training Epoch 9] Batch 2864, Loss 0.2525149881839752\n",
      "[Training Epoch 9] Batch 2865, Loss 0.2401811182498932\n",
      "[Training Epoch 9] Batch 2866, Loss 0.24205543100833893\n",
      "[Training Epoch 9] Batch 2867, Loss 0.26155468821525574\n",
      "[Training Epoch 9] Batch 2868, Loss 0.26177552342414856\n",
      "[Training Epoch 9] Batch 2869, Loss 0.27114641666412354\n",
      "[Training Epoch 9] Batch 2870, Loss 0.21916994452476501\n",
      "[Training Epoch 9] Batch 2871, Loss 0.2309567630290985\n",
      "[Training Epoch 9] Batch 2872, Loss 0.2588135004043579\n",
      "[Training Epoch 9] Batch 2873, Loss 0.266094446182251\n",
      "[Training Epoch 9] Batch 2874, Loss 0.2583981156349182\n",
      "[Training Epoch 9] Batch 2875, Loss 0.24517637491226196\n",
      "[Training Epoch 9] Batch 2876, Loss 0.2598983347415924\n",
      "[Training Epoch 9] Batch 2877, Loss 0.27284520864486694\n",
      "[Training Epoch 9] Batch 2878, Loss 0.28064197301864624\n",
      "[Training Epoch 9] Batch 2879, Loss 0.25086256861686707\n",
      "[Training Epoch 9] Batch 2880, Loss 0.25434815883636475\n",
      "[Training Epoch 9] Batch 2881, Loss 0.288808137178421\n",
      "[Training Epoch 9] Batch 2882, Loss 0.28116559982299805\n",
      "[Training Epoch 9] Batch 2883, Loss 0.24799948930740356\n",
      "[Training Epoch 9] Batch 2884, Loss 0.26346930861473083\n",
      "[Training Epoch 9] Batch 2885, Loss 0.2529125213623047\n",
      "[Training Epoch 9] Batch 2886, Loss 0.2405012547969818\n",
      "[Training Epoch 9] Batch 2887, Loss 0.2364414632320404\n",
      "[Training Epoch 9] Batch 2888, Loss 0.2349396049976349\n",
      "[Training Epoch 9] Batch 2889, Loss 0.2706698477268219\n",
      "[Training Epoch 9] Batch 2890, Loss 0.26078668236732483\n",
      "[Training Epoch 9] Batch 2891, Loss 0.23946517705917358\n",
      "[Training Epoch 9] Batch 2892, Loss 0.21894890069961548\n",
      "[Training Epoch 9] Batch 2893, Loss 0.2677972912788391\n",
      "[Training Epoch 9] Batch 2894, Loss 0.25314825773239136\n",
      "[Training Epoch 9] Batch 2895, Loss 0.2237035632133484\n",
      "[Training Epoch 9] Batch 2896, Loss 0.24457259476184845\n",
      "[Training Epoch 9] Batch 2897, Loss 0.27585044503211975\n",
      "[Training Epoch 9] Batch 2898, Loss 0.23218439519405365\n",
      "[Training Epoch 9] Batch 2899, Loss 0.26668843626976013\n",
      "[Training Epoch 9] Batch 2900, Loss 0.24912738800048828\n",
      "[Training Epoch 9] Batch 2901, Loss 0.27047258615493774\n",
      "[Training Epoch 9] Batch 2902, Loss 0.23039616644382477\n",
      "[Training Epoch 9] Batch 2903, Loss 0.27919846773147583\n",
      "[Training Epoch 9] Batch 2904, Loss 0.25262752175331116\n",
      "[Training Epoch 9] Batch 2905, Loss 0.2506464123725891\n",
      "[Training Epoch 9] Batch 2906, Loss 0.27559101581573486\n",
      "[Training Epoch 9] Batch 2907, Loss 0.2523246705532074\n",
      "[Training Epoch 9] Batch 2908, Loss 0.2553486227989197\n",
      "[Training Epoch 9] Batch 2909, Loss 0.24570365250110626\n",
      "[Training Epoch 9] Batch 2910, Loss 0.2553127408027649\n",
      "[Training Epoch 9] Batch 2911, Loss 0.2555393576622009\n",
      "[Training Epoch 9] Batch 2912, Loss 0.2594382166862488\n",
      "[Training Epoch 9] Batch 2913, Loss 0.2635766267776489\n",
      "[Training Epoch 9] Batch 2914, Loss 0.23629873991012573\n",
      "[Training Epoch 9] Batch 2915, Loss 0.26652026176452637\n",
      "[Training Epoch 9] Batch 2916, Loss 0.22368213534355164\n",
      "[Training Epoch 9] Batch 2917, Loss 0.24220989644527435\n",
      "[Training Epoch 9] Batch 2918, Loss 0.23695750534534454\n",
      "[Training Epoch 9] Batch 2919, Loss 0.264931857585907\n",
      "[Training Epoch 9] Batch 2920, Loss 0.2622225880622864\n",
      "[Training Epoch 9] Batch 2921, Loss 0.2472015917301178\n",
      "[Training Epoch 9] Batch 2922, Loss 0.2362392395734787\n",
      "[Training Epoch 9] Batch 2923, Loss 0.2569499611854553\n",
      "[Training Epoch 9] Batch 2924, Loss 0.2372635006904602\n",
      "[Training Epoch 9] Batch 2925, Loss 0.2341606169939041\n",
      "[Training Epoch 9] Batch 2926, Loss 0.26634058356285095\n",
      "[Training Epoch 9] Batch 2927, Loss 0.26204103231430054\n",
      "[Training Epoch 9] Batch 2928, Loss 0.220646470785141\n",
      "[Training Epoch 9] Batch 2929, Loss 0.23382890224456787\n",
      "[Training Epoch 9] Batch 2930, Loss 0.25532835721969604\n",
      "[Training Epoch 9] Batch 2931, Loss 0.279089093208313\n",
      "[Training Epoch 9] Batch 2932, Loss 0.22776804864406586\n",
      "[Training Epoch 9] Batch 2933, Loss 0.2670135498046875\n",
      "[Training Epoch 9] Batch 2934, Loss 0.24797523021697998\n",
      "[Training Epoch 9] Batch 2935, Loss 0.26018375158309937\n",
      "[Training Epoch 9] Batch 2936, Loss 0.2947482168674469\n",
      "[Training Epoch 9] Batch 2937, Loss 0.26702582836151123\n",
      "[Training Epoch 9] Batch 2938, Loss 0.22374175488948822\n",
      "[Training Epoch 9] Batch 2939, Loss 0.26706576347351074\n",
      "[Training Epoch 9] Batch 2940, Loss 0.24134708940982819\n",
      "[Training Epoch 9] Batch 2941, Loss 0.2599920630455017\n",
      "[Training Epoch 9] Batch 2942, Loss 0.25580358505249023\n",
      "[Training Epoch 9] Batch 2943, Loss 0.27817055583000183\n",
      "[Training Epoch 9] Batch 2944, Loss 0.23835667967796326\n",
      "[Training Epoch 9] Batch 2945, Loss 0.2712998688220978\n",
      "[Training Epoch 9] Batch 2946, Loss 0.2295456826686859\n",
      "[Training Epoch 9] Batch 2947, Loss 0.2671026587486267\n",
      "[Training Epoch 9] Batch 2948, Loss 0.2323116958141327\n",
      "[Training Epoch 9] Batch 2949, Loss 0.2624135911464691\n",
      "[Training Epoch 9] Batch 2950, Loss 0.24576810002326965\n",
      "[Training Epoch 9] Batch 2951, Loss 0.24528026580810547\n",
      "[Training Epoch 9] Batch 2952, Loss 0.25308090448379517\n",
      "[Training Epoch 9] Batch 2953, Loss 0.27360647916793823\n",
      "[Training Epoch 9] Batch 2954, Loss 0.25068676471710205\n",
      "[Training Epoch 9] Batch 2955, Loss 0.25731614232063293\n",
      "[Training Epoch 9] Batch 2956, Loss 0.2710793912410736\n",
      "[Training Epoch 9] Batch 2957, Loss 0.24370545148849487\n",
      "[Training Epoch 9] Batch 2958, Loss 0.25228843092918396\n",
      "[Training Epoch 9] Batch 2959, Loss 0.2490328848361969\n",
      "[Training Epoch 9] Batch 2960, Loss 0.2728567123413086\n",
      "[Training Epoch 9] Batch 2961, Loss 0.2575896084308624\n",
      "[Training Epoch 9] Batch 2962, Loss 0.2750343084335327\n",
      "[Training Epoch 9] Batch 2963, Loss 0.25699564814567566\n",
      "[Training Epoch 9] Batch 2964, Loss 0.26796483993530273\n",
      "[Training Epoch 9] Batch 2965, Loss 0.25983288884162903\n",
      "[Training Epoch 9] Batch 2966, Loss 0.255607008934021\n",
      "[Training Epoch 9] Batch 2967, Loss 0.22624817490577698\n",
      "[Training Epoch 9] Batch 2968, Loss 0.24815122783184052\n",
      "[Training Epoch 9] Batch 2969, Loss 0.2762805223464966\n",
      "[Training Epoch 9] Batch 2970, Loss 0.2642022669315338\n",
      "[Training Epoch 9] Batch 2971, Loss 0.2576560974121094\n",
      "[Training Epoch 9] Batch 2972, Loss 0.2717873752117157\n",
      "[Training Epoch 9] Batch 2973, Loss 0.22953982651233673\n",
      "[Training Epoch 9] Batch 2974, Loss 0.2220340073108673\n",
      "[Training Epoch 9] Batch 2975, Loss 0.2393188327550888\n",
      "[Training Epoch 9] Batch 2976, Loss 0.2660299241542816\n",
      "[Training Epoch 9] Batch 2977, Loss 0.24225006997585297\n",
      "[Training Epoch 9] Batch 2978, Loss 0.2390928566455841\n",
      "[Training Epoch 9] Batch 2979, Loss 0.2607305645942688\n",
      "[Training Epoch 9] Batch 2980, Loss 0.27722591161727905\n",
      "[Training Epoch 9] Batch 2981, Loss 0.2710754871368408\n",
      "[Training Epoch 9] Batch 2982, Loss 0.2474583238363266\n",
      "[Training Epoch 9] Batch 2983, Loss 0.25506722927093506\n",
      "[Training Epoch 9] Batch 2984, Loss 0.2788480520248413\n",
      "[Training Epoch 9] Batch 2985, Loss 0.256799578666687\n",
      "[Training Epoch 9] Batch 2986, Loss 0.25882667303085327\n",
      "[Training Epoch 9] Batch 2987, Loss 0.2904203236103058\n",
      "[Training Epoch 9] Batch 2988, Loss 0.23180004954338074\n",
      "[Training Epoch 9] Batch 2989, Loss 0.27620431780815125\n",
      "[Training Epoch 9] Batch 2990, Loss 0.2534944415092468\n",
      "[Training Epoch 9] Batch 2991, Loss 0.2677617073059082\n",
      "[Training Epoch 9] Batch 2992, Loss 0.2666064500808716\n",
      "[Training Epoch 9] Batch 2993, Loss 0.24691858887672424\n",
      "[Training Epoch 9] Batch 2994, Loss 0.2807410955429077\n",
      "[Training Epoch 9] Batch 2995, Loss 0.25955715775489807\n",
      "[Training Epoch 9] Batch 2996, Loss 0.2767108678817749\n",
      "[Training Epoch 9] Batch 2997, Loss 0.25498294830322266\n",
      "[Training Epoch 9] Batch 2998, Loss 0.2639877498149872\n",
      "[Training Epoch 9] Batch 2999, Loss 0.2706875801086426\n",
      "[Training Epoch 9] Batch 3000, Loss 0.23108404874801636\n",
      "[Training Epoch 9] Batch 3001, Loss 0.2648366689682007\n",
      "[Training Epoch 9] Batch 3002, Loss 0.25989192724227905\n",
      "[Training Epoch 9] Batch 3003, Loss 0.2669268846511841\n",
      "[Training Epoch 9] Batch 3004, Loss 0.2820286154747009\n",
      "[Training Epoch 9] Batch 3005, Loss 0.22569593787193298\n",
      "[Training Epoch 9] Batch 3006, Loss 0.27256256341934204\n",
      "[Training Epoch 9] Batch 3007, Loss 0.26289013028144836\n",
      "[Training Epoch 9] Batch 3008, Loss 0.29209938645362854\n",
      "[Training Epoch 9] Batch 3009, Loss 0.2563610374927521\n",
      "[Training Epoch 9] Batch 3010, Loss 0.26394230127334595\n",
      "[Training Epoch 9] Batch 3011, Loss 0.2763921618461609\n",
      "[Training Epoch 9] Batch 3012, Loss 0.29602187871932983\n",
      "[Training Epoch 9] Batch 3013, Loss 0.2310088574886322\n",
      "[Training Epoch 9] Batch 3014, Loss 0.2635747492313385\n",
      "[Training Epoch 9] Batch 3015, Loss 0.2404751032590866\n",
      "[Training Epoch 9] Batch 3016, Loss 0.24980567395687103\n",
      "[Training Epoch 9] Batch 3017, Loss 0.2214580625295639\n",
      "[Training Epoch 9] Batch 3018, Loss 0.2550759017467499\n",
      "[Training Epoch 9] Batch 3019, Loss 0.2748280167579651\n",
      "[Training Epoch 9] Batch 3020, Loss 0.2865262031555176\n",
      "[Training Epoch 9] Batch 3021, Loss 0.2665884494781494\n",
      "[Training Epoch 9] Batch 3022, Loss 0.24890509247779846\n",
      "[Training Epoch 9] Batch 3023, Loss 0.2631867527961731\n",
      "[Training Epoch 9] Batch 3024, Loss 0.24984602630138397\n",
      "[Training Epoch 9] Batch 3025, Loss 0.24693545699119568\n",
      "[Training Epoch 9] Batch 3026, Loss 0.2733544707298279\n",
      "[Training Epoch 9] Batch 3027, Loss 0.2394125759601593\n",
      "[Training Epoch 9] Batch 3028, Loss 0.2538021206855774\n",
      "[Training Epoch 9] Batch 3029, Loss 0.256602942943573\n",
      "[Training Epoch 9] Batch 3030, Loss 0.2783772945404053\n",
      "[Training Epoch 9] Batch 3031, Loss 0.23474690318107605\n",
      "[Training Epoch 9] Batch 3032, Loss 0.2707286775112152\n",
      "[Training Epoch 9] Batch 3033, Loss 0.24871423840522766\n",
      "[Training Epoch 9] Batch 3034, Loss 0.264434278011322\n",
      "[Training Epoch 9] Batch 3035, Loss 0.2518270015716553\n",
      "[Training Epoch 9] Batch 3036, Loss 0.2655561566352844\n",
      "[Training Epoch 9] Batch 3037, Loss 0.2439749836921692\n",
      "[Training Epoch 9] Batch 3038, Loss 0.2701409161090851\n",
      "[Training Epoch 9] Batch 3039, Loss 0.2744091749191284\n",
      "[Training Epoch 9] Batch 3040, Loss 0.27195388078689575\n",
      "[Training Epoch 9] Batch 3041, Loss 0.23601004481315613\n",
      "[Training Epoch 9] Batch 3042, Loss 0.2536045014858246\n",
      "[Training Epoch 9] Batch 3043, Loss 0.28222113847732544\n",
      "[Training Epoch 9] Batch 3044, Loss 0.2770957052707672\n",
      "[Training Epoch 9] Batch 3045, Loss 0.2440406084060669\n",
      "[Training Epoch 9] Batch 3046, Loss 0.25328975915908813\n",
      "[Training Epoch 9] Batch 3047, Loss 0.2369118332862854\n",
      "[Training Epoch 9] Batch 3048, Loss 0.23274314403533936\n",
      "[Training Epoch 9] Batch 3049, Loss 0.27359920740127563\n",
      "[Training Epoch 9] Batch 3050, Loss 0.22784215211868286\n",
      "[Training Epoch 9] Batch 3051, Loss 0.22516843676567078\n",
      "[Training Epoch 9] Batch 3052, Loss 0.2647322714328766\n",
      "[Training Epoch 9] Batch 3053, Loss 0.23369313776493073\n",
      "[Training Epoch 9] Batch 3054, Loss 0.24246260523796082\n",
      "[Training Epoch 9] Batch 3055, Loss 0.2562364637851715\n",
      "[Training Epoch 9] Batch 3056, Loss 0.2729995846748352\n",
      "[Training Epoch 9] Batch 3057, Loss 0.24953128397464752\n",
      "[Training Epoch 9] Batch 3058, Loss 0.25710511207580566\n",
      "[Training Epoch 9] Batch 3059, Loss 0.2504843473434448\n",
      "[Training Epoch 9] Batch 3060, Loss 0.27605119347572327\n",
      "[Training Epoch 9] Batch 3061, Loss 0.2782685160636902\n",
      "[Training Epoch 9] Batch 3062, Loss 0.28116145730018616\n",
      "[Training Epoch 9] Batch 3063, Loss 0.26670998334884644\n",
      "[Training Epoch 9] Batch 3064, Loss 0.22187095880508423\n",
      "[Training Epoch 9] Batch 3065, Loss 0.23809665441513062\n",
      "[Training Epoch 9] Batch 3066, Loss 0.24032360315322876\n",
      "[Training Epoch 9] Batch 3067, Loss 0.2724428176879883\n",
      "[Training Epoch 9] Batch 3068, Loss 0.2448158860206604\n",
      "[Training Epoch 9] Batch 3069, Loss 0.23599475622177124\n",
      "[Training Epoch 9] Batch 3070, Loss 0.2756843566894531\n",
      "[Training Epoch 9] Batch 3071, Loss 0.2611173987388611\n",
      "[Training Epoch 9] Batch 3072, Loss 0.25115838646888733\n",
      "[Training Epoch 9] Batch 3073, Loss 0.2621496617794037\n",
      "[Training Epoch 9] Batch 3074, Loss 0.250324010848999\n",
      "[Training Epoch 9] Batch 3075, Loss 0.23157772421836853\n",
      "[Training Epoch 9] Batch 3076, Loss 0.27421027421951294\n",
      "[Training Epoch 9] Batch 3077, Loss 0.26168203353881836\n",
      "[Training Epoch 9] Batch 3078, Loss 0.2556312084197998\n",
      "[Training Epoch 9] Batch 3079, Loss 0.2434026002883911\n",
      "[Training Epoch 9] Batch 3080, Loss 0.24370379745960236\n",
      "[Training Epoch 9] Batch 3081, Loss 0.21877774596214294\n",
      "[Training Epoch 9] Batch 3082, Loss 0.2535005509853363\n",
      "[Training Epoch 9] Batch 3083, Loss 0.25336867570877075\n",
      "[Training Epoch 9] Batch 3084, Loss 0.21291367709636688\n",
      "[Training Epoch 9] Batch 3085, Loss 0.2307550609111786\n",
      "[Training Epoch 9] Batch 3086, Loss 0.2576806843280792\n",
      "[Training Epoch 9] Batch 3087, Loss 0.21324004232883453\n",
      "[Training Epoch 9] Batch 3088, Loss 0.2798581123352051\n",
      "[Training Epoch 9] Batch 3089, Loss 0.23843008279800415\n",
      "[Training Epoch 9] Batch 3090, Loss 0.24949893355369568\n",
      "[Training Epoch 9] Batch 3091, Loss 0.2342178225517273\n",
      "[Training Epoch 9] Batch 3092, Loss 0.2664417624473572\n",
      "[Training Epoch 9] Batch 3093, Loss 0.2678458094596863\n",
      "[Training Epoch 9] Batch 3094, Loss 0.23857349157333374\n",
      "[Training Epoch 9] Batch 3095, Loss 0.275142103433609\n",
      "[Training Epoch 9] Batch 3096, Loss 0.24458250403404236\n",
      "[Training Epoch 9] Batch 3097, Loss 0.22813308238983154\n",
      "[Training Epoch 9] Batch 3098, Loss 0.26161521673202515\n",
      "[Training Epoch 9] Batch 3099, Loss 0.2551560401916504\n",
      "[Training Epoch 9] Batch 3100, Loss 0.2719881534576416\n",
      "[Training Epoch 9] Batch 3101, Loss 0.25636452436447144\n",
      "[Training Epoch 9] Batch 3102, Loss 0.2531293034553528\n",
      "[Training Epoch 9] Batch 3103, Loss 0.27774879336357117\n",
      "[Training Epoch 9] Batch 3104, Loss 0.23050761222839355\n",
      "[Training Epoch 9] Batch 3105, Loss 0.23314836621284485\n",
      "[Training Epoch 9] Batch 3106, Loss 0.2780190706253052\n",
      "[Training Epoch 9] Batch 3107, Loss 0.2626941204071045\n",
      "[Training Epoch 9] Batch 3108, Loss 0.24150070548057556\n",
      "[Training Epoch 9] Batch 3109, Loss 0.25426045060157776\n",
      "[Training Epoch 9] Batch 3110, Loss 0.2720831632614136\n",
      "[Training Epoch 9] Batch 3111, Loss 0.2903844714164734\n",
      "[Training Epoch 9] Batch 3112, Loss 0.2676636576652527\n",
      "[Training Epoch 9] Batch 3113, Loss 0.24994319677352905\n",
      "[Training Epoch 9] Batch 3114, Loss 0.2639700770378113\n",
      "[Training Epoch 9] Batch 3115, Loss 0.23288802802562714\n",
      "[Training Epoch 9] Batch 3116, Loss 0.2487720251083374\n",
      "[Training Epoch 9] Batch 3117, Loss 0.2480981945991516\n",
      "[Training Epoch 9] Batch 3118, Loss 0.25482702255249023\n",
      "[Training Epoch 9] Batch 3119, Loss 0.24438577890396118\n",
      "[Training Epoch 9] Batch 3120, Loss 0.26408645510673523\n",
      "[Training Epoch 9] Batch 3121, Loss 0.24204519391059875\n",
      "[Training Epoch 9] Batch 3122, Loss 0.25808802247047424\n",
      "[Training Epoch 9] Batch 3123, Loss 0.25289541482925415\n",
      "[Training Epoch 9] Batch 3124, Loss 0.23959535360336304\n",
      "[Training Epoch 9] Batch 3125, Loss 0.25727659463882446\n",
      "[Training Epoch 9] Batch 3126, Loss 0.2511809766292572\n",
      "[Training Epoch 9] Batch 3127, Loss 0.2651444673538208\n",
      "[Training Epoch 9] Batch 3128, Loss 0.25008824467658997\n",
      "[Training Epoch 9] Batch 3129, Loss 0.2823032736778259\n",
      "[Training Epoch 9] Batch 3130, Loss 0.24801495671272278\n",
      "[Training Epoch 9] Batch 3131, Loss 0.2722073495388031\n",
      "[Training Epoch 9] Batch 3132, Loss 0.2378721833229065\n",
      "[Training Epoch 9] Batch 3133, Loss 0.28136155009269714\n",
      "[Training Epoch 9] Batch 3134, Loss 0.2586413621902466\n",
      "[Training Epoch 9] Batch 3135, Loss 0.22263649106025696\n",
      "[Training Epoch 9] Batch 3136, Loss 0.2508387863636017\n",
      "[Training Epoch 9] Batch 3137, Loss 0.24230718612670898\n",
      "[Training Epoch 9] Batch 3138, Loss 0.24395859241485596\n",
      "[Training Epoch 9] Batch 3139, Loss 0.2264283299446106\n",
      "[Training Epoch 9] Batch 3140, Loss 0.26935622096061707\n",
      "[Training Epoch 9] Batch 3141, Loss 0.23408348858356476\n",
      "[Training Epoch 9] Batch 3142, Loss 0.2569355368614197\n",
      "[Training Epoch 9] Batch 3143, Loss 0.2560834288597107\n",
      "[Training Epoch 9] Batch 3144, Loss 0.2648504078388214\n",
      "[Training Epoch 9] Batch 3145, Loss 0.24621227383613586\n",
      "[Training Epoch 9] Batch 3146, Loss 0.2310357540845871\n",
      "[Training Epoch 9] Batch 3147, Loss 0.21195489168167114\n",
      "[Training Epoch 9] Batch 3148, Loss 0.24541951715946198\n",
      "[Training Epoch 9] Batch 3149, Loss 0.25215739011764526\n",
      "[Training Epoch 9] Batch 3150, Loss 0.2702082395553589\n",
      "[Training Epoch 9] Batch 3151, Loss 0.24540802836418152\n",
      "[Training Epoch 9] Batch 3152, Loss 0.2876187562942505\n",
      "[Training Epoch 9] Batch 3153, Loss 0.24120023846626282\n",
      "[Training Epoch 9] Batch 3154, Loss 0.25062859058380127\n",
      "[Training Epoch 9] Batch 3155, Loss 0.29517000913619995\n",
      "[Training Epoch 9] Batch 3156, Loss 0.25691115856170654\n",
      "[Training Epoch 9] Batch 3157, Loss 0.24428850412368774\n",
      "[Training Epoch 9] Batch 3158, Loss 0.26914411783218384\n",
      "[Training Epoch 9] Batch 3159, Loss 0.25468677282333374\n",
      "[Training Epoch 9] Batch 3160, Loss 0.2690284252166748\n",
      "[Training Epoch 9] Batch 3161, Loss 0.2481151670217514\n",
      "[Training Epoch 9] Batch 3162, Loss 0.24690191447734833\n",
      "[Training Epoch 9] Batch 3163, Loss 0.22511640191078186\n",
      "[Training Epoch 9] Batch 3164, Loss 0.2439250648021698\n",
      "[Training Epoch 9] Batch 3165, Loss 0.2441660314798355\n",
      "[Training Epoch 9] Batch 3166, Loss 0.21501633524894714\n",
      "[Training Epoch 9] Batch 3167, Loss 0.2765299379825592\n",
      "[Training Epoch 9] Batch 3168, Loss 0.2723691165447235\n",
      "[Training Epoch 9] Batch 3169, Loss 0.24495404958724976\n",
      "[Training Epoch 9] Batch 3170, Loss 0.24894030392169952\n",
      "[Training Epoch 9] Batch 3171, Loss 0.2604494094848633\n",
      "[Training Epoch 9] Batch 3172, Loss 0.2232922464609146\n",
      "[Training Epoch 9] Batch 3173, Loss 0.2658739686012268\n",
      "[Training Epoch 9] Batch 3174, Loss 0.27228137850761414\n",
      "[Training Epoch 9] Batch 3175, Loss 0.24038802087306976\n",
      "[Training Epoch 9] Batch 3176, Loss 0.23906704783439636\n",
      "[Training Epoch 9] Batch 3177, Loss 0.2551984488964081\n",
      "[Training Epoch 9] Batch 3178, Loss 0.23702430725097656\n",
      "[Training Epoch 9] Batch 3179, Loss 0.22627472877502441\n",
      "[Training Epoch 9] Batch 3180, Loss 0.2736930251121521\n",
      "[Training Epoch 9] Batch 3181, Loss 0.25129780173301697\n",
      "[Training Epoch 9] Batch 3182, Loss 0.24384737014770508\n",
      "[Training Epoch 9] Batch 3183, Loss 0.2557906210422516\n",
      "[Training Epoch 9] Batch 3184, Loss 0.2501014769077301\n",
      "[Training Epoch 9] Batch 3185, Loss 0.23897230625152588\n",
      "[Training Epoch 9] Batch 3186, Loss 0.2550920248031616\n",
      "[Training Epoch 9] Batch 3187, Loss 0.24362267553806305\n",
      "[Training Epoch 9] Batch 3188, Loss 0.24816802144050598\n",
      "[Training Epoch 9] Batch 3189, Loss 0.2602425813674927\n",
      "[Training Epoch 9] Batch 3190, Loss 0.21945545077323914\n",
      "[Training Epoch 9] Batch 3191, Loss 0.25827786326408386\n",
      "[Training Epoch 9] Batch 3192, Loss 0.2343149185180664\n",
      "[Training Epoch 9] Batch 3193, Loss 0.24370437860488892\n",
      "[Training Epoch 9] Batch 3194, Loss 0.250004380941391\n",
      "[Training Epoch 9] Batch 3195, Loss 0.2781512439250946\n",
      "[Training Epoch 9] Batch 3196, Loss 0.22592556476593018\n",
      "[Training Epoch 9] Batch 3197, Loss 0.2549549341201782\n",
      "[Training Epoch 9] Batch 3198, Loss 0.2564845681190491\n",
      "[Training Epoch 9] Batch 3199, Loss 0.27012553811073303\n",
      "[Training Epoch 9] Batch 3200, Loss 0.2337188422679901\n",
      "[Training Epoch 9] Batch 3201, Loss 0.2503935694694519\n",
      "[Training Epoch 9] Batch 3202, Loss 0.23855701088905334\n",
      "[Training Epoch 9] Batch 3203, Loss 0.2768286466598511\n",
      "[Training Epoch 9] Batch 3204, Loss 0.25641781091690063\n",
      "[Training Epoch 9] Batch 3205, Loss 0.22815969586372375\n",
      "[Training Epoch 9] Batch 3206, Loss 0.2848111689090729\n",
      "[Training Epoch 9] Batch 3207, Loss 0.2815448045730591\n",
      "[Training Epoch 9] Batch 3208, Loss 0.2283225655555725\n",
      "[Training Epoch 9] Batch 3209, Loss 0.25915592908859253\n",
      "[Training Epoch 9] Batch 3210, Loss 0.2533831000328064\n",
      "[Training Epoch 9] Batch 3211, Loss 0.24466323852539062\n",
      "[Training Epoch 9] Batch 3212, Loss 0.21855510771274567\n",
      "[Training Epoch 9] Batch 3213, Loss 0.2633823752403259\n",
      "[Training Epoch 9] Batch 3214, Loss 0.24685126543045044\n",
      "[Training Epoch 9] Batch 3215, Loss 0.27814561128616333\n",
      "[Training Epoch 9] Batch 3216, Loss 0.24639227986335754\n",
      "[Training Epoch 9] Batch 3217, Loss 0.23060554265975952\n",
      "[Training Epoch 9] Batch 3218, Loss 0.2643413543701172\n",
      "[Training Epoch 9] Batch 3219, Loss 0.21656599640846252\n",
      "[Training Epoch 9] Batch 3220, Loss 0.2786013185977936\n",
      "[Training Epoch 9] Batch 3221, Loss 0.26689866185188293\n",
      "[Training Epoch 9] Batch 3222, Loss 0.2509796619415283\n",
      "[Training Epoch 9] Batch 3223, Loss 0.22435396909713745\n",
      "[Training Epoch 9] Batch 3224, Loss 0.29868197441101074\n",
      "[Training Epoch 9] Batch 3225, Loss 0.26435545086860657\n",
      "[Training Epoch 9] Batch 3226, Loss 0.24613085389137268\n",
      "[Training Epoch 9] Batch 3227, Loss 0.27756330370903015\n",
      "[Training Epoch 9] Batch 3228, Loss 0.30782458186149597\n",
      "[Training Epoch 9] Batch 3229, Loss 0.24777603149414062\n",
      "[Training Epoch 9] Batch 3230, Loss 0.24698036909103394\n",
      "[Training Epoch 9] Batch 3231, Loss 0.2519252300262451\n",
      "[Training Epoch 9] Batch 3232, Loss 0.25704753398895264\n",
      "[Training Epoch 9] Batch 3233, Loss 0.2630574703216553\n",
      "[Training Epoch 9] Batch 3234, Loss 0.23623421788215637\n",
      "[Training Epoch 9] Batch 3235, Loss 0.24627217650413513\n",
      "[Training Epoch 9] Batch 3236, Loss 0.2469959855079651\n",
      "[Training Epoch 9] Batch 3237, Loss 0.24095207452774048\n",
      "[Training Epoch 9] Batch 3238, Loss 0.2593739330768585\n",
      "[Training Epoch 9] Batch 3239, Loss 0.2517533004283905\n",
      "[Training Epoch 9] Batch 3240, Loss 0.26709234714508057\n",
      "[Training Epoch 9] Batch 3241, Loss 0.25267940759658813\n",
      "[Training Epoch 9] Batch 3242, Loss 0.25536394119262695\n",
      "[Training Epoch 9] Batch 3243, Loss 0.2532740831375122\n",
      "[Training Epoch 9] Batch 3244, Loss 0.26298293471336365\n",
      "[Training Epoch 9] Batch 3245, Loss 0.2511622905731201\n",
      "[Training Epoch 9] Batch 3246, Loss 0.22292017936706543\n",
      "[Training Epoch 9] Batch 3247, Loss 0.2781054973602295\n",
      "[Training Epoch 9] Batch 3248, Loss 0.2745688259601593\n",
      "[Training Epoch 9] Batch 3249, Loss 0.2483978569507599\n",
      "[Training Epoch 9] Batch 3250, Loss 0.265152245759964\n",
      "[Training Epoch 9] Batch 3251, Loss 0.2643295228481293\n",
      "[Training Epoch 9] Batch 3252, Loss 0.2545875608921051\n",
      "[Training Epoch 9] Batch 3253, Loss 0.26967138051986694\n",
      "[Training Epoch 9] Batch 3254, Loss 0.2660757601261139\n",
      "[Training Epoch 9] Batch 3255, Loss 0.28816694021224976\n",
      "[Training Epoch 9] Batch 3256, Loss 0.27394798398017883\n",
      "[Training Epoch 9] Batch 3257, Loss 0.23628747463226318\n",
      "[Training Epoch 9] Batch 3258, Loss 0.2562425434589386\n",
      "[Training Epoch 9] Batch 3259, Loss 0.2659282684326172\n",
      "[Training Epoch 9] Batch 3260, Loss 0.2543000876903534\n",
      "[Training Epoch 9] Batch 3261, Loss 0.24361979961395264\n",
      "[Training Epoch 9] Batch 3262, Loss 0.24488860368728638\n",
      "[Training Epoch 9] Batch 3263, Loss 0.2440885603427887\n",
      "[Training Epoch 9] Batch 3264, Loss 0.2711936831474304\n",
      "[Training Epoch 9] Batch 3265, Loss 0.2263321578502655\n",
      "[Training Epoch 9] Batch 3266, Loss 0.24100501835346222\n",
      "[Training Epoch 9] Batch 3267, Loss 0.24655455350875854\n",
      "[Training Epoch 9] Batch 3268, Loss 0.2776508331298828\n",
      "[Training Epoch 9] Batch 3269, Loss 0.26224544644355774\n",
      "[Training Epoch 9] Batch 3270, Loss 0.26723718643188477\n",
      "[Training Epoch 9] Batch 3271, Loss 0.2484622746706009\n",
      "[Training Epoch 9] Batch 3272, Loss 0.25342506170272827\n",
      "[Training Epoch 9] Batch 3273, Loss 0.2592821717262268\n",
      "[Training Epoch 9] Batch 3274, Loss 0.2835536599159241\n",
      "[Training Epoch 9] Batch 3275, Loss 0.26388832926750183\n",
      "[Training Epoch 9] Batch 3276, Loss 0.2732184827327728\n",
      "[Training Epoch 9] Batch 3277, Loss 0.25546759366989136\n",
      "[Training Epoch 9] Batch 3278, Loss 0.21605968475341797\n",
      "[Training Epoch 9] Batch 3279, Loss 0.28127434849739075\n",
      "[Training Epoch 9] Batch 3280, Loss 0.2602837085723877\n",
      "[Training Epoch 9] Batch 3281, Loss 0.29701077938079834\n",
      "[Training Epoch 9] Batch 3282, Loss 0.24317103624343872\n",
      "[Training Epoch 9] Batch 3283, Loss 0.25616127252578735\n",
      "[Training Epoch 9] Batch 3284, Loss 0.270900696516037\n",
      "[Training Epoch 9] Batch 3285, Loss 0.2474300116300583\n",
      "[Training Epoch 9] Batch 3286, Loss 0.24967479705810547\n",
      "[Training Epoch 9] Batch 3287, Loss 0.23443171381950378\n",
      "[Training Epoch 9] Batch 3288, Loss 0.2572019100189209\n",
      "[Training Epoch 9] Batch 3289, Loss 0.259350448846817\n",
      "[Training Epoch 9] Batch 3290, Loss 0.2570875287055969\n",
      "[Training Epoch 9] Batch 3291, Loss 0.27484557032585144\n",
      "[Training Epoch 9] Batch 3292, Loss 0.2556196451187134\n",
      "[Training Epoch 9] Batch 3293, Loss 0.25137513875961304\n",
      "[Training Epoch 9] Batch 3294, Loss 0.2511783242225647\n",
      "[Training Epoch 9] Batch 3295, Loss 0.25612878799438477\n",
      "[Training Epoch 9] Batch 3296, Loss 0.2801613211631775\n",
      "[Training Epoch 9] Batch 3297, Loss 0.26559001207351685\n",
      "[Training Epoch 9] Batch 3298, Loss 0.2495981603860855\n",
      "[Training Epoch 9] Batch 3299, Loss 0.2686910629272461\n",
      "[Training Epoch 9] Batch 3300, Loss 0.2625058591365814\n",
      "[Training Epoch 9] Batch 3301, Loss 0.26572588086128235\n",
      "[Training Epoch 9] Batch 3302, Loss 0.2500019073486328\n",
      "[Training Epoch 9] Batch 3303, Loss 0.26747408509254456\n",
      "[Training Epoch 9] Batch 3304, Loss 0.27597707509994507\n",
      "[Training Epoch 9] Batch 3305, Loss 0.26631438732147217\n",
      "[Training Epoch 9] Batch 3306, Loss 0.27606579661369324\n",
      "[Training Epoch 9] Batch 3307, Loss 0.23565536737442017\n",
      "[Training Epoch 9] Batch 3308, Loss 0.22724978625774384\n",
      "[Training Epoch 9] Batch 3309, Loss 0.26403892040252686\n",
      "[Training Epoch 9] Batch 3310, Loss 0.2625795602798462\n",
      "[Training Epoch 9] Batch 3311, Loss 0.22956225275993347\n",
      "[Training Epoch 9] Batch 3312, Loss 0.26513588428497314\n",
      "[Training Epoch 9] Batch 3313, Loss 0.25182968378067017\n",
      "[Training Epoch 9] Batch 3314, Loss 0.25309181213378906\n",
      "[Training Epoch 9] Batch 3315, Loss 0.25562697649002075\n",
      "[Training Epoch 9] Batch 3316, Loss 0.2308061271905899\n",
      "[Training Epoch 9] Batch 3317, Loss 0.23922157287597656\n",
      "[Training Epoch 9] Batch 3318, Loss 0.2577396631240845\n",
      "[Training Epoch 9] Batch 3319, Loss 0.22633861005306244\n",
      "[Training Epoch 9] Batch 3320, Loss 0.25369787216186523\n",
      "[Training Epoch 9] Batch 3321, Loss 0.26674312353134155\n",
      "[Training Epoch 9] Batch 3322, Loss 0.24975912272930145\n",
      "[Training Epoch 9] Batch 3323, Loss 0.26587143540382385\n",
      "[Training Epoch 9] Batch 3324, Loss 0.286186158657074\n",
      "[Training Epoch 9] Batch 3325, Loss 0.25569725036621094\n",
      "[Training Epoch 9] Batch 3326, Loss 0.25384917855262756\n",
      "[Training Epoch 9] Batch 3327, Loss 0.21451227366924286\n",
      "[Training Epoch 9] Batch 3328, Loss 0.2552235722541809\n",
      "[Training Epoch 9] Batch 3329, Loss 0.25592201948165894\n",
      "[Training Epoch 9] Batch 3330, Loss 0.2495279610157013\n",
      "[Training Epoch 9] Batch 3331, Loss 0.2747536301612854\n",
      "[Training Epoch 9] Batch 3332, Loss 0.24817350506782532\n",
      "[Training Epoch 9] Batch 3333, Loss 0.24413131177425385\n",
      "[Training Epoch 9] Batch 3334, Loss 0.2351367473602295\n",
      "[Training Epoch 9] Batch 3335, Loss 0.2593505382537842\n",
      "[Training Epoch 9] Batch 3336, Loss 0.2709289789199829\n",
      "[Training Epoch 9] Batch 3337, Loss 0.2724926471710205\n",
      "[Training Epoch 9] Batch 3338, Loss 0.23765996098518372\n",
      "[Training Epoch 9] Batch 3339, Loss 0.26374849677085876\n",
      "[Training Epoch 9] Batch 3340, Loss 0.24925574660301208\n",
      "[Training Epoch 9] Batch 3341, Loss 0.2714909315109253\n",
      "[Training Epoch 9] Batch 3342, Loss 0.2846965193748474\n",
      "[Training Epoch 9] Batch 3343, Loss 0.2646855115890503\n",
      "[Training Epoch 9] Batch 3344, Loss 0.26207244396209717\n",
      "[Training Epoch 9] Batch 3345, Loss 0.24797499179840088\n",
      "[Training Epoch 9] Batch 3346, Loss 0.27106422185897827\n",
      "[Training Epoch 9] Batch 3347, Loss 0.26192259788513184\n",
      "[Training Epoch 9] Batch 3348, Loss 0.251878947019577\n",
      "[Training Epoch 9] Batch 3349, Loss 0.22412124276161194\n",
      "[Training Epoch 9] Batch 3350, Loss 0.25440603494644165\n",
      "[Training Epoch 9] Batch 3351, Loss 0.27208709716796875\n",
      "[Training Epoch 9] Batch 3352, Loss 0.2824253439903259\n",
      "[Training Epoch 9] Batch 3353, Loss 0.2408677339553833\n",
      "[Training Epoch 9] Batch 3354, Loss 0.24024873971939087\n",
      "[Training Epoch 9] Batch 3355, Loss 0.2570638954639435\n",
      "[Training Epoch 9] Batch 3356, Loss 0.23742742836475372\n",
      "[Training Epoch 9] Batch 3357, Loss 0.25523141026496887\n",
      "[Training Epoch 9] Batch 3358, Loss 0.2569116950035095\n",
      "[Training Epoch 9] Batch 3359, Loss 0.26193374395370483\n",
      "[Training Epoch 9] Batch 3360, Loss 0.26325154304504395\n",
      "[Training Epoch 9] Batch 3361, Loss 0.26137015223503113\n",
      "[Training Epoch 9] Batch 3362, Loss 0.23659589886665344\n",
      "[Training Epoch 9] Batch 3363, Loss 0.25289613008499146\n",
      "[Training Epoch 9] Batch 3364, Loss 0.26542234420776367\n",
      "[Training Epoch 9] Batch 3365, Loss 0.25562751293182373\n",
      "[Training Epoch 9] Batch 3366, Loss 0.25452959537506104\n",
      "[Training Epoch 9] Batch 3367, Loss 0.2732778787612915\n",
      "[Training Epoch 9] Batch 3368, Loss 0.2686232626438141\n",
      "[Training Epoch 9] Batch 3369, Loss 0.2514394521713257\n",
      "[Training Epoch 9] Batch 3370, Loss 0.2556186318397522\n",
      "[Training Epoch 9] Batch 3371, Loss 0.2557450234889984\n",
      "[Training Epoch 9] Batch 3372, Loss 0.2776871919631958\n",
      "[Training Epoch 9] Batch 3373, Loss 0.22820821404457092\n",
      "[Training Epoch 9] Batch 3374, Loss 0.2563142478466034\n",
      "[Training Epoch 9] Batch 3375, Loss 0.24873802065849304\n",
      "[Training Epoch 9] Batch 3376, Loss 0.2721569836139679\n",
      "[Training Epoch 9] Batch 3377, Loss 0.27506023645401\n",
      "[Training Epoch 9] Batch 3378, Loss 0.23362381756305695\n",
      "[Training Epoch 9] Batch 3379, Loss 0.2405383288860321\n",
      "[Training Epoch 9] Batch 3380, Loss 0.2831736207008362\n",
      "[Training Epoch 9] Batch 3381, Loss 0.24805231392383575\n",
      "[Training Epoch 9] Batch 3382, Loss 0.22579428553581238\n",
      "[Training Epoch 9] Batch 3383, Loss 0.2653101682662964\n",
      "[Training Epoch 9] Batch 3384, Loss 0.2493029236793518\n",
      "[Training Epoch 9] Batch 3385, Loss 0.265611857175827\n",
      "[Training Epoch 9] Batch 3386, Loss 0.27262184023857117\n",
      "[Training Epoch 9] Batch 3387, Loss 0.27446603775024414\n",
      "[Training Epoch 9] Batch 3388, Loss 0.23926344513893127\n",
      "[Training Epoch 9] Batch 3389, Loss 0.2688104212284088\n",
      "[Training Epoch 9] Batch 3390, Loss 0.23378175497055054\n",
      "[Training Epoch 9] Batch 3391, Loss 0.28063446283340454\n",
      "[Training Epoch 9] Batch 3392, Loss 0.24071507155895233\n",
      "[Training Epoch 9] Batch 3393, Loss 0.24750939011573792\n",
      "[Training Epoch 9] Batch 3394, Loss 0.2478303611278534\n",
      "[Training Epoch 9] Batch 3395, Loss 0.2399120330810547\n",
      "[Training Epoch 9] Batch 3396, Loss 0.2486737072467804\n",
      "[Training Epoch 9] Batch 3397, Loss 0.2534652054309845\n",
      "[Training Epoch 9] Batch 3398, Loss 0.24566471576690674\n",
      "[Training Epoch 9] Batch 3399, Loss 0.24257634580135345\n",
      "[Training Epoch 9] Batch 3400, Loss 0.23856493830680847\n",
      "[Training Epoch 9] Batch 3401, Loss 0.25544223189353943\n",
      "[Training Epoch 9] Batch 3402, Loss 0.2620691657066345\n",
      "[Training Epoch 9] Batch 3403, Loss 0.2632681727409363\n",
      "[Training Epoch 9] Batch 3404, Loss 0.24096618592739105\n",
      "[Training Epoch 9] Batch 3405, Loss 0.24507087469100952\n",
      "[Training Epoch 9] Batch 3406, Loss 0.2261047214269638\n",
      "[Training Epoch 9] Batch 3407, Loss 0.229941263794899\n",
      "[Training Epoch 9] Batch 3408, Loss 0.2384284883737564\n",
      "[Training Epoch 9] Batch 3409, Loss 0.24173542857170105\n",
      "[Training Epoch 9] Batch 3410, Loss 0.27702322602272034\n",
      "[Training Epoch 9] Batch 3411, Loss 0.25032907724380493\n",
      "[Training Epoch 9] Batch 3412, Loss 0.2820386290550232\n",
      "[Training Epoch 9] Batch 3413, Loss 0.25023379921913147\n",
      "[Training Epoch 9] Batch 3414, Loss 0.2774657607078552\n",
      "[Training Epoch 9] Batch 3415, Loss 0.2438732385635376\n",
      "[Training Epoch 9] Batch 3416, Loss 0.231456458568573\n",
      "[Training Epoch 9] Batch 3417, Loss 0.2641765773296356\n",
      "[Training Epoch 9] Batch 3418, Loss 0.2594876289367676\n",
      "[Training Epoch 9] Batch 3419, Loss 0.26021039485931396\n",
      "[Training Epoch 9] Batch 3420, Loss 0.23193460702896118\n",
      "[Training Epoch 9] Batch 3421, Loss 0.2515888214111328\n",
      "[Training Epoch 9] Batch 3422, Loss 0.2538177967071533\n",
      "[Training Epoch 9] Batch 3423, Loss 0.21943436563014984\n",
      "[Training Epoch 9] Batch 3424, Loss 0.259214848279953\n",
      "[Training Epoch 9] Batch 3425, Loss 0.26689839363098145\n",
      "[Training Epoch 9] Batch 3426, Loss 0.23274412751197815\n",
      "[Training Epoch 9] Batch 3427, Loss 0.25669044256210327\n",
      "[Training Epoch 9] Batch 3428, Loss 0.26328009366989136\n",
      "[Training Epoch 9] Batch 3429, Loss 0.23593398928642273\n",
      "[Training Epoch 9] Batch 3430, Loss 0.24854996800422668\n",
      "[Training Epoch 9] Batch 3431, Loss 0.2478669136762619\n",
      "[Training Epoch 9] Batch 3432, Loss 0.2281079888343811\n",
      "[Training Epoch 9] Batch 3433, Loss 0.25472667813301086\n",
      "[Training Epoch 9] Batch 3434, Loss 0.24566954374313354\n",
      "[Training Epoch 9] Batch 3435, Loss 0.2589137852191925\n",
      "[Training Epoch 9] Batch 3436, Loss 0.23622752726078033\n",
      "[Training Epoch 9] Batch 3437, Loss 0.2662467956542969\n",
      "[Training Epoch 9] Batch 3438, Loss 0.23105168342590332\n",
      "[Training Epoch 9] Batch 3439, Loss 0.23978376388549805\n",
      "[Training Epoch 9] Batch 3440, Loss 0.2702065408229828\n",
      "[Training Epoch 9] Batch 3441, Loss 0.21880248188972473\n",
      "[Training Epoch 9] Batch 3442, Loss 0.24524611234664917\n",
      "[Training Epoch 9] Batch 3443, Loss 0.28821244835853577\n",
      "[Training Epoch 9] Batch 3444, Loss 0.2595808804035187\n",
      "[Training Epoch 9] Batch 3445, Loss 0.23470908403396606\n",
      "[Training Epoch 9] Batch 3446, Loss 0.2266322374343872\n",
      "[Training Epoch 9] Batch 3447, Loss 0.2496001422405243\n",
      "[Training Epoch 9] Batch 3448, Loss 0.2669427990913391\n",
      "[Training Epoch 9] Batch 3449, Loss 0.24811214208602905\n",
      "[Training Epoch 9] Batch 3450, Loss 0.2874884605407715\n",
      "[Training Epoch 9] Batch 3451, Loss 0.2553652822971344\n",
      "[Training Epoch 9] Batch 3452, Loss 0.2587521970272064\n",
      "[Training Epoch 9] Batch 3453, Loss 0.2296932190656662\n",
      "[Training Epoch 9] Batch 3454, Loss 0.2529822289943695\n",
      "[Training Epoch 9] Batch 3455, Loss 0.24278925359249115\n",
      "[Training Epoch 9] Batch 3456, Loss 0.2610497772693634\n",
      "[Training Epoch 9] Batch 3457, Loss 0.2782517075538635\n",
      "[Training Epoch 9] Batch 3458, Loss 0.24281643331050873\n",
      "[Training Epoch 9] Batch 3459, Loss 0.26792705059051514\n",
      "[Training Epoch 9] Batch 3460, Loss 0.258628249168396\n",
      "[Training Epoch 9] Batch 3461, Loss 0.23054122924804688\n",
      "[Training Epoch 9] Batch 3462, Loss 0.2837669849395752\n",
      "[Training Epoch 9] Batch 3463, Loss 0.25906902551651\n",
      "[Training Epoch 9] Batch 3464, Loss 0.27611345052719116\n",
      "[Training Epoch 9] Batch 3465, Loss 0.25233834981918335\n",
      "[Training Epoch 9] Batch 3466, Loss 0.19403059780597687\n",
      "[Training Epoch 9] Batch 3467, Loss 0.25546249747276306\n",
      "[Training Epoch 9] Batch 3468, Loss 0.2423447072505951\n",
      "[Training Epoch 9] Batch 3469, Loss 0.2442571371793747\n",
      "[Training Epoch 9] Batch 3470, Loss 0.25217294692993164\n",
      "[Training Epoch 9] Batch 3471, Loss 0.24715416133403778\n",
      "[Training Epoch 9] Batch 3472, Loss 0.26519161462783813\n",
      "[Training Epoch 9] Batch 3473, Loss 0.23962976038455963\n",
      "[Training Epoch 9] Batch 3474, Loss 0.2532224655151367\n",
      "[Training Epoch 9] Batch 3475, Loss 0.2767806947231293\n",
      "[Training Epoch 9] Batch 3476, Loss 0.2441115826368332\n",
      "[Training Epoch 9] Batch 3477, Loss 0.27634069323539734\n",
      "[Training Epoch 9] Batch 3478, Loss 0.2564929723739624\n",
      "[Training Epoch 9] Batch 3479, Loss 0.2550159692764282\n",
      "[Training Epoch 9] Batch 3480, Loss 0.28956958651542664\n",
      "[Training Epoch 9] Batch 3481, Loss 0.26601743698120117\n",
      "[Training Epoch 9] Batch 3482, Loss 0.2515381872653961\n",
      "[Training Epoch 9] Batch 3483, Loss 0.25239917635917664\n",
      "[Training Epoch 9] Batch 3484, Loss 0.24914845824241638\n",
      "[Training Epoch 9] Batch 3485, Loss 0.26495522260665894\n",
      "[Training Epoch 9] Batch 3486, Loss 0.24124157428741455\n",
      "[Training Epoch 9] Batch 3487, Loss 0.24886533617973328\n",
      "[Training Epoch 9] Batch 3488, Loss 0.26805800199508667\n",
      "[Training Epoch 9] Batch 3489, Loss 0.2882568836212158\n",
      "[Training Epoch 9] Batch 3490, Loss 0.26418203115463257\n",
      "[Training Epoch 9] Batch 3491, Loss 0.25966352224349976\n",
      "[Training Epoch 9] Batch 3492, Loss 0.2760399281978607\n",
      "[Training Epoch 9] Batch 3493, Loss 0.2759999632835388\n",
      "[Training Epoch 9] Batch 3494, Loss 0.2743059992790222\n",
      "[Training Epoch 9] Batch 3495, Loss 0.23885583877563477\n",
      "[Training Epoch 9] Batch 3496, Loss 0.24924635887145996\n",
      "[Training Epoch 9] Batch 3497, Loss 0.2631584703922272\n",
      "[Training Epoch 9] Batch 3498, Loss 0.2540532052516937\n",
      "[Training Epoch 9] Batch 3499, Loss 0.24349448084831238\n",
      "[Training Epoch 9] Batch 3500, Loss 0.24626320600509644\n",
      "[Training Epoch 9] Batch 3501, Loss 0.2428462952375412\n",
      "[Training Epoch 9] Batch 3502, Loss 0.2529073655605316\n",
      "[Training Epoch 9] Batch 3503, Loss 0.25606438517570496\n",
      "[Training Epoch 9] Batch 3504, Loss 0.24800249934196472\n",
      "[Training Epoch 9] Batch 3505, Loss 0.27514901757240295\n",
      "[Training Epoch 9] Batch 3506, Loss 0.2613667845726013\n",
      "[Training Epoch 9] Batch 3507, Loss 0.24484340846538544\n",
      "[Training Epoch 9] Batch 3508, Loss 0.24713541567325592\n",
      "[Training Epoch 9] Batch 3509, Loss 0.2660359740257263\n",
      "[Training Epoch 9] Batch 3510, Loss 0.2621787190437317\n",
      "[Training Epoch 9] Batch 3511, Loss 0.24420863389968872\n",
      "[Training Epoch 9] Batch 3512, Loss 0.23969990015029907\n",
      "[Training Epoch 9] Batch 3513, Loss 0.2885710597038269\n",
      "[Training Epoch 9] Batch 3514, Loss 0.27513033151626587\n",
      "[Training Epoch 9] Batch 3515, Loss 0.2314985692501068\n",
      "[Training Epoch 9] Batch 3516, Loss 0.23005855083465576\n",
      "[Training Epoch 9] Batch 3517, Loss 0.25568169355392456\n",
      "[Training Epoch 9] Batch 3518, Loss 0.25611311197280884\n",
      "[Training Epoch 9] Batch 3519, Loss 0.2550939917564392\n",
      "[Training Epoch 9] Batch 3520, Loss 0.23842310905456543\n",
      "[Training Epoch 9] Batch 3521, Loss 0.24194321036338806\n",
      "[Training Epoch 9] Batch 3522, Loss 0.26568010449409485\n",
      "[Training Epoch 9] Batch 3523, Loss 0.2590520679950714\n",
      "[Training Epoch 9] Batch 3524, Loss 0.2856479287147522\n",
      "[Training Epoch 9] Batch 3525, Loss 0.25911495089530945\n",
      "[Training Epoch 9] Batch 3526, Loss 0.24371019005775452\n",
      "[Training Epoch 9] Batch 3527, Loss 0.24314460158348083\n",
      "[Training Epoch 9] Batch 3528, Loss 0.24957120418548584\n",
      "[Training Epoch 9] Batch 3529, Loss 0.2685970067977905\n",
      "[Training Epoch 9] Batch 3530, Loss 0.2778369188308716\n",
      "[Training Epoch 9] Batch 3531, Loss 0.2581077218055725\n",
      "[Training Epoch 9] Batch 3532, Loss 0.25803548097610474\n",
      "[Training Epoch 9] Batch 3533, Loss 0.25430580973625183\n",
      "[Training Epoch 9] Batch 3534, Loss 0.2626824975013733\n",
      "[Training Epoch 9] Batch 3535, Loss 0.2571563720703125\n",
      "[Training Epoch 9] Batch 3536, Loss 0.22976990044116974\n",
      "[Training Epoch 9] Batch 3537, Loss 0.2580471336841583\n",
      "[Training Epoch 9] Batch 3538, Loss 0.23576897382736206\n",
      "[Training Epoch 9] Batch 3539, Loss 0.2694896459579468\n",
      "[Training Epoch 9] Batch 3540, Loss 0.2601356506347656\n",
      "[Training Epoch 9] Batch 3541, Loss 0.23722970485687256\n",
      "[Training Epoch 9] Batch 3542, Loss 0.2633816599845886\n",
      "[Training Epoch 9] Batch 3543, Loss 0.2396324872970581\n",
      "[Training Epoch 9] Batch 3544, Loss 0.23203106224536896\n",
      "[Training Epoch 9] Batch 3545, Loss 0.2501085698604584\n",
      "[Training Epoch 9] Batch 3546, Loss 0.2904192805290222\n",
      "[Training Epoch 9] Batch 3547, Loss 0.23799538612365723\n",
      "[Training Epoch 9] Batch 3548, Loss 0.2424945831298828\n",
      "[Training Epoch 9] Batch 3549, Loss 0.24460487067699432\n",
      "[Training Epoch 9] Batch 3550, Loss 0.2552371025085449\n",
      "[Training Epoch 9] Batch 3551, Loss 0.27440670132637024\n",
      "[Training Epoch 9] Batch 3552, Loss 0.22768095135688782\n",
      "[Training Epoch 9] Batch 3553, Loss 0.25023505091667175\n",
      "[Training Epoch 9] Batch 3554, Loss 0.22859087586402893\n",
      "[Training Epoch 9] Batch 3555, Loss 0.2582310438156128\n",
      "[Training Epoch 9] Batch 3556, Loss 0.27339720726013184\n",
      "[Training Epoch 9] Batch 3557, Loss 0.25180989503860474\n",
      "[Training Epoch 9] Batch 3558, Loss 0.24432283639907837\n",
      "[Training Epoch 9] Batch 3559, Loss 0.24167582392692566\n",
      "[Training Epoch 9] Batch 3560, Loss 0.2616122364997864\n",
      "[Training Epoch 9] Batch 3561, Loss 0.23007504642009735\n",
      "[Training Epoch 9] Batch 3562, Loss 0.24106119573116302\n",
      "[Training Epoch 9] Batch 3563, Loss 0.23968377709388733\n",
      "[Training Epoch 9] Batch 3564, Loss 0.241807758808136\n",
      "[Training Epoch 9] Batch 3565, Loss 0.2749965786933899\n",
      "[Training Epoch 9] Batch 3566, Loss 0.25678956508636475\n",
      "[Training Epoch 9] Batch 3567, Loss 0.2391686737537384\n",
      "[Training Epoch 9] Batch 3568, Loss 0.28406018018722534\n",
      "[Training Epoch 9] Batch 3569, Loss 0.24549110233783722\n",
      "[Training Epoch 9] Batch 3570, Loss 0.27011698484420776\n",
      "[Training Epoch 9] Batch 3571, Loss 0.26046860218048096\n",
      "[Training Epoch 9] Batch 3572, Loss 0.2762961983680725\n",
      "[Training Epoch 9] Batch 3573, Loss 0.26225975155830383\n",
      "[Training Epoch 9] Batch 3574, Loss 0.2764142155647278\n",
      "[Training Epoch 9] Batch 3575, Loss 0.28164762258529663\n",
      "[Training Epoch 9] Batch 3576, Loss 0.2530791163444519\n",
      "[Training Epoch 9] Batch 3577, Loss 0.2730114459991455\n",
      "[Training Epoch 9] Batch 3578, Loss 0.2484477460384369\n",
      "[Training Epoch 9] Batch 3579, Loss 0.23062187433242798\n",
      "[Training Epoch 9] Batch 3580, Loss 0.25782325863838196\n",
      "[Training Epoch 9] Batch 3581, Loss 0.26252281665802\n",
      "[Training Epoch 9] Batch 3582, Loss 0.20936867594718933\n",
      "[Training Epoch 9] Batch 3583, Loss 0.2608198821544647\n",
      "[Training Epoch 9] Batch 3584, Loss 0.25583627820014954\n",
      "[Training Epoch 9] Batch 3585, Loss 0.25295567512512207\n",
      "[Training Epoch 9] Batch 3586, Loss 0.2658310532569885\n",
      "[Training Epoch 9] Batch 3587, Loss 0.2564545273780823\n",
      "[Training Epoch 9] Batch 3588, Loss 0.26265662908554077\n",
      "[Training Epoch 9] Batch 3589, Loss 0.2737833261489868\n",
      "[Training Epoch 9] Batch 3590, Loss 0.24835437536239624\n",
      "[Training Epoch 9] Batch 3591, Loss 0.25445982813835144\n",
      "[Training Epoch 9] Batch 3592, Loss 0.23519651591777802\n",
      "[Training Epoch 9] Batch 3593, Loss 0.26874876022338867\n",
      "[Training Epoch 9] Batch 3594, Loss 0.25917473435401917\n",
      "[Training Epoch 9] Batch 3595, Loss 0.2588918209075928\n",
      "[Training Epoch 9] Batch 3596, Loss 0.23935356736183167\n",
      "[Training Epoch 9] Batch 3597, Loss 0.25492608547210693\n",
      "[Training Epoch 9] Batch 3598, Loss 0.24060633778572083\n",
      "[Training Epoch 9] Batch 3599, Loss 0.2351214438676834\n",
      "[Training Epoch 9] Batch 3600, Loss 0.22771725058555603\n",
      "[Training Epoch 9] Batch 3601, Loss 0.24941833317279816\n",
      "[Training Epoch 9] Batch 3602, Loss 0.24675020575523376\n",
      "[Training Epoch 9] Batch 3603, Loss 0.25071072578430176\n",
      "[Training Epoch 9] Batch 3604, Loss 0.2859100103378296\n",
      "[Training Epoch 9] Batch 3605, Loss 0.26796066761016846\n",
      "[Training Epoch 9] Batch 3606, Loss 0.2718285918235779\n",
      "[Training Epoch 9] Batch 3607, Loss 0.24270419776439667\n",
      "[Training Epoch 9] Batch 3608, Loss 0.2779160141944885\n",
      "[Training Epoch 9] Batch 3609, Loss 0.2468143254518509\n",
      "[Training Epoch 9] Batch 3610, Loss 0.2794173061847687\n",
      "[Training Epoch 9] Batch 3611, Loss 0.2754420042037964\n",
      "[Training Epoch 9] Batch 3612, Loss 0.2438351958990097\n",
      "[Training Epoch 9] Batch 3613, Loss 0.26204174757003784\n",
      "[Training Epoch 9] Batch 3614, Loss 0.2529224753379822\n",
      "[Training Epoch 9] Batch 3615, Loss 0.26480406522750854\n",
      "[Training Epoch 9] Batch 3616, Loss 0.2745625376701355\n",
      "[Training Epoch 9] Batch 3617, Loss 0.2676869034767151\n",
      "[Training Epoch 9] Batch 3618, Loss 0.26412200927734375\n",
      "[Training Epoch 9] Batch 3619, Loss 0.2723276615142822\n",
      "[Training Epoch 9] Batch 3620, Loss 0.2417088747024536\n",
      "[Training Epoch 9] Batch 3621, Loss 0.2363230139017105\n",
      "[Training Epoch 9] Batch 3622, Loss 0.23782336711883545\n",
      "[Training Epoch 9] Batch 3623, Loss 0.2650812268257141\n",
      "[Training Epoch 9] Batch 3624, Loss 0.2477520853281021\n",
      "[Training Epoch 9] Batch 3625, Loss 0.2559814751148224\n",
      "[Training Epoch 9] Batch 3626, Loss 0.2466108798980713\n",
      "[Training Epoch 9] Batch 3627, Loss 0.2504849135875702\n",
      "[Training Epoch 9] Batch 3628, Loss 0.26991763710975647\n",
      "[Training Epoch 9] Batch 3629, Loss 0.24081577360630035\n",
      "[Training Epoch 9] Batch 3630, Loss 0.2480541467666626\n",
      "[Training Epoch 9] Batch 3631, Loss 0.2312682569026947\n",
      "[Training Epoch 9] Batch 3632, Loss 0.2717815339565277\n",
      "[Training Epoch 9] Batch 3633, Loss 0.2658212184906006\n",
      "[Training Epoch 9] Batch 3634, Loss 0.2707919180393219\n",
      "[Training Epoch 9] Batch 3635, Loss 0.23189646005630493\n",
      "[Training Epoch 9] Batch 3636, Loss 0.23310640454292297\n",
      "[Training Epoch 9] Batch 3637, Loss 0.28060275316238403\n",
      "[Training Epoch 9] Batch 3638, Loss 0.24414022266864777\n",
      "[Training Epoch 9] Batch 3639, Loss 0.24636989831924438\n",
      "[Training Epoch 9] Batch 3640, Loss 0.27890080213546753\n",
      "[Training Epoch 9] Batch 3641, Loss 0.26593488454818726\n",
      "[Training Epoch 9] Batch 3642, Loss 0.25926217436790466\n",
      "[Training Epoch 9] Batch 3643, Loss 0.26633554697036743\n",
      "[Training Epoch 9] Batch 3644, Loss 0.2652394771575928\n",
      "[Training Epoch 9] Batch 3645, Loss 0.23223528265953064\n",
      "[Training Epoch 9] Batch 3646, Loss 0.24993985891342163\n",
      "[Training Epoch 9] Batch 3647, Loss 0.24497435986995697\n",
      "[Training Epoch 9] Batch 3648, Loss 0.26810023188591003\n",
      "[Training Epoch 9] Batch 3649, Loss 0.250818133354187\n",
      "[Training Epoch 9] Batch 3650, Loss 0.2576400339603424\n",
      "[Training Epoch 9] Batch 3651, Loss 0.2697402238845825\n",
      "[Training Epoch 9] Batch 3652, Loss 0.2662690281867981\n",
      "[Training Epoch 9] Batch 3653, Loss 0.248238667845726\n",
      "[Training Epoch 9] Batch 3654, Loss 0.24865630269050598\n",
      "[Training Epoch 9] Batch 3655, Loss 0.29798126220703125\n",
      "[Training Epoch 9] Batch 3656, Loss 0.24447882175445557\n",
      "[Training Epoch 9] Batch 3657, Loss 0.2664181590080261\n",
      "[Training Epoch 9] Batch 3658, Loss 0.24262221157550812\n",
      "[Training Epoch 9] Batch 3659, Loss 0.22877970337867737\n",
      "[Training Epoch 9] Batch 3660, Loss 0.272828608751297\n",
      "[Training Epoch 9] Batch 3661, Loss 0.26381832361221313\n",
      "[Training Epoch 9] Batch 3662, Loss 0.27337855100631714\n",
      "[Training Epoch 9] Batch 3663, Loss 0.2713176906108856\n",
      "[Training Epoch 9] Batch 3664, Loss 0.2652529776096344\n",
      "[Training Epoch 9] Batch 3665, Loss 0.22540652751922607\n",
      "[Training Epoch 9] Batch 3666, Loss 0.2592216730117798\n",
      "[Training Epoch 9] Batch 3667, Loss 0.2471873164176941\n",
      "[Training Epoch 9] Batch 3668, Loss 0.2444787621498108\n",
      "[Training Epoch 9] Batch 3669, Loss 0.2761750817298889\n",
      "[Training Epoch 9] Batch 3670, Loss 0.2480284869670868\n",
      "[Training Epoch 9] Batch 3671, Loss 0.23629963397979736\n",
      "[Training Epoch 9] Batch 3672, Loss 0.27346086502075195\n",
      "[Training Epoch 9] Batch 3673, Loss 0.23569980263710022\n",
      "[Training Epoch 9] Batch 3674, Loss 0.292100191116333\n",
      "[Training Epoch 9] Batch 3675, Loss 0.25355711579322815\n",
      "[Training Epoch 9] Batch 3676, Loss 0.2532229423522949\n",
      "[Training Epoch 9] Batch 3677, Loss 0.2614579200744629\n",
      "[Training Epoch 9] Batch 3678, Loss 0.25982803106307983\n",
      "[Training Epoch 9] Batch 3679, Loss 0.2406841516494751\n",
      "[Training Epoch 9] Batch 3680, Loss 0.22087697684764862\n",
      "[Training Epoch 9] Batch 3681, Loss 0.2684440016746521\n",
      "[Training Epoch 9] Batch 3682, Loss 0.25648126006126404\n",
      "[Training Epoch 9] Batch 3683, Loss 0.23483014106750488\n",
      "[Training Epoch 9] Batch 3684, Loss 0.270596444606781\n",
      "[Training Epoch 9] Batch 3685, Loss 0.2261262685060501\n",
      "[Training Epoch 9] Batch 3686, Loss 0.2575605511665344\n",
      "[Training Epoch 9] Batch 3687, Loss 0.2834087014198303\n",
      "[Training Epoch 9] Batch 3688, Loss 0.2844775319099426\n",
      "[Training Epoch 9] Batch 3689, Loss 0.23422479629516602\n",
      "[Training Epoch 9] Batch 3690, Loss 0.28129228949546814\n",
      "[Training Epoch 9] Batch 3691, Loss 0.2585827112197876\n",
      "[Training Epoch 9] Batch 3692, Loss 0.2580350637435913\n",
      "[Training Epoch 9] Batch 3693, Loss 0.2790873944759369\n",
      "[Training Epoch 9] Batch 3694, Loss 0.2555493712425232\n",
      "[Training Epoch 9] Batch 3695, Loss 0.2692607641220093\n",
      "[Training Epoch 9] Batch 3696, Loss 0.24029575288295746\n",
      "[Training Epoch 9] Batch 3697, Loss 0.2420409619808197\n",
      "[Training Epoch 9] Batch 3698, Loss 0.24983331561088562\n",
      "[Training Epoch 9] Batch 3699, Loss 0.2679690718650818\n",
      "[Training Epoch 9] Batch 3700, Loss 0.2986826002597809\n",
      "[Training Epoch 9] Batch 3701, Loss 0.22908954322338104\n",
      "[Training Epoch 9] Batch 3702, Loss 0.24214699864387512\n",
      "[Training Epoch 9] Batch 3703, Loss 0.2456788271665573\n",
      "[Training Epoch 9] Batch 3704, Loss 0.26834967732429504\n",
      "[Training Epoch 9] Batch 3705, Loss 0.26362836360931396\n",
      "[Training Epoch 9] Batch 3706, Loss 0.2552638053894043\n",
      "[Training Epoch 9] Batch 3707, Loss 0.23423460125923157\n",
      "[Training Epoch 9] Batch 3708, Loss 0.25583773851394653\n",
      "[Training Epoch 9] Batch 3709, Loss 0.2865941524505615\n",
      "[Training Epoch 9] Batch 3710, Loss 0.25418534874916077\n",
      "[Training Epoch 9] Batch 3711, Loss 0.2341020703315735\n",
      "[Training Epoch 9] Batch 3712, Loss 0.24002858996391296\n",
      "[Training Epoch 9] Batch 3713, Loss 0.25240612030029297\n",
      "[Training Epoch 9] Batch 3714, Loss 0.23568013310432434\n",
      "[Training Epoch 9] Batch 3715, Loss 0.2610927224159241\n",
      "[Training Epoch 9] Batch 3716, Loss 0.2669007182121277\n",
      "[Training Epoch 9] Batch 3717, Loss 0.26174667477607727\n",
      "[Training Epoch 9] Batch 3718, Loss 0.2833322286605835\n",
      "[Training Epoch 9] Batch 3719, Loss 0.24908027052879333\n",
      "[Training Epoch 9] Batch 3720, Loss 0.23787568509578705\n",
      "[Training Epoch 9] Batch 3721, Loss 0.255053848028183\n",
      "[Training Epoch 9] Batch 3722, Loss 0.24248209595680237\n",
      "[Training Epoch 9] Batch 3723, Loss 0.27213072776794434\n",
      "[Training Epoch 9] Batch 3724, Loss 0.2334010899066925\n",
      "[Training Epoch 9] Batch 3725, Loss 0.22549565136432648\n",
      "[Training Epoch 9] Batch 3726, Loss 0.24436244368553162\n",
      "[Training Epoch 9] Batch 3727, Loss 0.26090359687805176\n",
      "[Training Epoch 9] Batch 3728, Loss 0.25699615478515625\n",
      "[Training Epoch 9] Batch 3729, Loss 0.30273061990737915\n",
      "[Training Epoch 9] Batch 3730, Loss 0.29971903562545776\n",
      "[Training Epoch 9] Batch 3731, Loss 0.24445182085037231\n",
      "[Training Epoch 9] Batch 3732, Loss 0.21653157472610474\n",
      "[Training Epoch 9] Batch 3733, Loss 0.24095264077186584\n",
      "[Training Epoch 9] Batch 3734, Loss 0.20068007707595825\n",
      "[Training Epoch 9] Batch 3735, Loss 0.23482686281204224\n",
      "[Training Epoch 9] Batch 3736, Loss 0.2796139717102051\n",
      "[Training Epoch 9] Batch 3737, Loss 0.2799927592277527\n",
      "[Training Epoch 9] Batch 3738, Loss 0.2785843312740326\n",
      "[Training Epoch 9] Batch 3739, Loss 0.2401248812675476\n",
      "[Training Epoch 9] Batch 3740, Loss 0.25208497047424316\n",
      "[Training Epoch 9] Batch 3741, Loss 0.24922221899032593\n",
      "[Training Epoch 9] Batch 3742, Loss 0.26442950963974\n",
      "[Training Epoch 9] Batch 3743, Loss 0.26891547441482544\n",
      "[Training Epoch 9] Batch 3744, Loss 0.2254846692085266\n",
      "[Training Epoch 9] Batch 3745, Loss 0.26440393924713135\n",
      "[Training Epoch 9] Batch 3746, Loss 0.23788613080978394\n",
      "[Training Epoch 9] Batch 3747, Loss 0.29056626558303833\n",
      "[Training Epoch 9] Batch 3748, Loss 0.25099414587020874\n",
      "[Training Epoch 9] Batch 3749, Loss 0.29191821813583374\n",
      "[Training Epoch 9] Batch 3750, Loss 0.24308709800243378\n",
      "[Training Epoch 9] Batch 3751, Loss 0.2736440896987915\n",
      "[Training Epoch 9] Batch 3752, Loss 0.24449419975280762\n",
      "[Training Epoch 9] Batch 3753, Loss 0.2333751767873764\n",
      "[Training Epoch 9] Batch 3754, Loss 0.2412605732679367\n",
      "[Training Epoch 9] Batch 3755, Loss 0.21852251887321472\n",
      "[Training Epoch 9] Batch 3756, Loss 0.23196059465408325\n",
      "[Training Epoch 9] Batch 3757, Loss 0.25584733486175537\n",
      "[Training Epoch 9] Batch 3758, Loss 0.24962212145328522\n",
      "[Training Epoch 9] Batch 3759, Loss 0.24987158179283142\n",
      "[Training Epoch 9] Batch 3760, Loss 0.2855725884437561\n",
      "[Training Epoch 9] Batch 3761, Loss 0.2682499885559082\n",
      "[Training Epoch 9] Batch 3762, Loss 0.289459764957428\n",
      "[Training Epoch 9] Batch 3763, Loss 0.24482059478759766\n",
      "[Training Epoch 9] Batch 3764, Loss 0.258672297000885\n",
      "[Training Epoch 9] Batch 3765, Loss 0.23503604531288147\n",
      "[Training Epoch 9] Batch 3766, Loss 0.24582833051681519\n",
      "[Training Epoch 9] Batch 3767, Loss 0.24299389123916626\n",
      "[Training Epoch 9] Batch 3768, Loss 0.2577518820762634\n",
      "[Training Epoch 9] Batch 3769, Loss 0.2585401237010956\n",
      "[Training Epoch 9] Batch 3770, Loss 0.2447110116481781\n",
      "[Training Epoch 9] Batch 3771, Loss 0.24617916345596313\n",
      "[Training Epoch 9] Batch 3772, Loss 0.28596439957618713\n",
      "[Training Epoch 9] Batch 3773, Loss 0.26799994707107544\n",
      "[Training Epoch 9] Batch 3774, Loss 0.26722416281700134\n",
      "[Training Epoch 9] Batch 3775, Loss 0.26211220026016235\n",
      "[Training Epoch 9] Batch 3776, Loss 0.26736241579055786\n",
      "[Training Epoch 9] Batch 3777, Loss 0.2639058828353882\n",
      "[Training Epoch 9] Batch 3778, Loss 0.26396918296813965\n",
      "[Training Epoch 9] Batch 3779, Loss 0.2713179588317871\n",
      "[Training Epoch 9] Batch 3780, Loss 0.2851507365703583\n",
      "[Training Epoch 9] Batch 3781, Loss 0.2459753304719925\n",
      "[Training Epoch 9] Batch 3782, Loss 0.2417096495628357\n",
      "[Training Epoch 9] Batch 3783, Loss 0.26181477308273315\n",
      "[Training Epoch 9] Batch 3784, Loss 0.26212310791015625\n",
      "[Training Epoch 9] Batch 3785, Loss 0.2664984464645386\n",
      "[Training Epoch 9] Batch 3786, Loss 0.24053728580474854\n",
      "[Training Epoch 9] Batch 3787, Loss 0.2659691572189331\n",
      "[Training Epoch 9] Batch 3788, Loss 0.24342510104179382\n",
      "[Training Epoch 9] Batch 3789, Loss 0.28217947483062744\n",
      "[Training Epoch 9] Batch 3790, Loss 0.24020916223526\n",
      "[Training Epoch 9] Batch 3791, Loss 0.2622387409210205\n",
      "[Training Epoch 9] Batch 3792, Loss 0.2566145360469818\n",
      "[Training Epoch 9] Batch 3793, Loss 0.2406298667192459\n",
      "[Training Epoch 9] Batch 3794, Loss 0.2387847900390625\n",
      "[Training Epoch 9] Batch 3795, Loss 0.2671878933906555\n",
      "[Training Epoch 9] Batch 3796, Loss 0.23467396199703217\n",
      "[Training Epoch 9] Batch 3797, Loss 0.22040501236915588\n",
      "[Training Epoch 9] Batch 3798, Loss 0.23433759808540344\n",
      "[Training Epoch 9] Batch 3799, Loss 0.26073259115219116\n",
      "[Training Epoch 9] Batch 3800, Loss 0.25461509823799133\n",
      "[Training Epoch 9] Batch 3801, Loss 0.25214290618896484\n",
      "[Training Epoch 9] Batch 3802, Loss 0.24683548510074615\n",
      "[Training Epoch 9] Batch 3803, Loss 0.24323028326034546\n",
      "[Training Epoch 9] Batch 3804, Loss 0.23555587232112885\n",
      "[Training Epoch 9] Batch 3805, Loss 0.24295537173748016\n",
      "[Training Epoch 9] Batch 3806, Loss 0.2615082859992981\n",
      "[Training Epoch 9] Batch 3807, Loss 0.2515590190887451\n",
      "[Training Epoch 9] Batch 3808, Loss 0.27329856157302856\n",
      "[Training Epoch 9] Batch 3809, Loss 0.2920600175857544\n",
      "[Training Epoch 9] Batch 3810, Loss 0.23065999150276184\n",
      "[Training Epoch 9] Batch 3811, Loss 0.25130730867385864\n",
      "[Training Epoch 9] Batch 3812, Loss 0.2637042701244354\n",
      "[Training Epoch 9] Batch 3813, Loss 0.2440953403711319\n",
      "[Training Epoch 9] Batch 3814, Loss 0.265052855014801\n",
      "[Training Epoch 9] Batch 3815, Loss 0.26613056659698486\n",
      "[Training Epoch 9] Batch 3816, Loss 0.2388520985841751\n",
      "[Training Epoch 9] Batch 3817, Loss 0.2466801106929779\n",
      "[Training Epoch 9] Batch 3818, Loss 0.25286567211151123\n",
      "[Training Epoch 9] Batch 3819, Loss 0.2375139594078064\n",
      "[Training Epoch 9] Batch 3820, Loss 0.25245532393455505\n",
      "[Training Epoch 9] Batch 3821, Loss 0.2862510085105896\n",
      "[Training Epoch 9] Batch 3822, Loss 0.2547530233860016\n",
      "[Training Epoch 9] Batch 3823, Loss 0.28174158930778503\n",
      "[Training Epoch 9] Batch 3824, Loss 0.24358190596103668\n",
      "[Training Epoch 9] Batch 3825, Loss 0.27571552991867065\n",
      "[Training Epoch 9] Batch 3826, Loss 0.22761449217796326\n",
      "[Training Epoch 9] Batch 3827, Loss 0.24060186743736267\n",
      "[Training Epoch 9] Batch 3828, Loss 0.2816263735294342\n",
      "[Training Epoch 9] Batch 3829, Loss 0.26542210578918457\n",
      "[Training Epoch 9] Batch 3830, Loss 0.24056001007556915\n",
      "[Training Epoch 9] Batch 3831, Loss 0.2493690550327301\n",
      "[Training Epoch 9] Batch 3832, Loss 0.24347707629203796\n",
      "[Training Epoch 9] Batch 3833, Loss 0.217690110206604\n",
      "[Training Epoch 9] Batch 3834, Loss 0.2474382221698761\n",
      "[Training Epoch 9] Batch 3835, Loss 0.2806382477283478\n",
      "[Training Epoch 9] Batch 3836, Loss 0.2744758725166321\n",
      "[Training Epoch 9] Batch 3837, Loss 0.2808854579925537\n",
      "[Training Epoch 9] Batch 3838, Loss 0.2584439516067505\n",
      "[Training Epoch 9] Batch 3839, Loss 0.2488953322172165\n",
      "[Training Epoch 9] Batch 3840, Loss 0.2501301169395447\n",
      "[Training Epoch 9] Batch 3841, Loss 0.23746582865715027\n",
      "[Training Epoch 9] Batch 3842, Loss 0.26267510652542114\n",
      "[Training Epoch 9] Batch 3843, Loss 0.21436309814453125\n",
      "[Training Epoch 9] Batch 3844, Loss 0.2547035813331604\n",
      "[Training Epoch 9] Batch 3845, Loss 0.2308148592710495\n",
      "[Training Epoch 9] Batch 3846, Loss 0.26604288816452026\n",
      "[Training Epoch 9] Batch 3847, Loss 0.2633863091468811\n",
      "[Training Epoch 9] Batch 3848, Loss 0.2855944037437439\n",
      "[Training Epoch 9] Batch 3849, Loss 0.23276616632938385\n",
      "[Training Epoch 9] Batch 3850, Loss 0.295356810092926\n",
      "[Training Epoch 9] Batch 3851, Loss 0.23376482725143433\n",
      "[Training Epoch 9] Batch 3852, Loss 0.25144246220588684\n",
      "[Training Epoch 9] Batch 3853, Loss 0.24447503685951233\n",
      "[Training Epoch 9] Batch 3854, Loss 0.25253310799598694\n",
      "[Training Epoch 9] Batch 3855, Loss 0.24772775173187256\n",
      "[Training Epoch 9] Batch 3856, Loss 0.24500320851802826\n",
      "[Training Epoch 9] Batch 3857, Loss 0.2872096300125122\n",
      "[Training Epoch 9] Batch 3858, Loss 0.26638275384902954\n",
      "[Training Epoch 9] Batch 3859, Loss 0.2766602635383606\n",
      "[Training Epoch 9] Batch 3860, Loss 0.26866137981414795\n",
      "[Training Epoch 9] Batch 3861, Loss 0.2124272882938385\n",
      "[Training Epoch 9] Batch 3862, Loss 0.25482842326164246\n",
      "[Training Epoch 9] Batch 3863, Loss 0.28239744901657104\n",
      "[Training Epoch 9] Batch 3864, Loss 0.24128638207912445\n",
      "[Training Epoch 9] Batch 3865, Loss 0.2589769959449768\n",
      "[Training Epoch 9] Batch 3866, Loss 0.25188782811164856\n",
      "[Training Epoch 9] Batch 3867, Loss 0.26930227875709534\n",
      "[Training Epoch 9] Batch 3868, Loss 0.25502637028694153\n",
      "[Training Epoch 9] Batch 3869, Loss 0.2643042206764221\n",
      "[Training Epoch 9] Batch 3870, Loss 0.26738208532333374\n",
      "[Training Epoch 9] Batch 3871, Loss 0.230220764875412\n",
      "[Training Epoch 9] Batch 3872, Loss 0.26893240213394165\n",
      "[Training Epoch 9] Batch 3873, Loss 0.21643471717834473\n",
      "[Training Epoch 9] Batch 3874, Loss 0.29619500041007996\n",
      "[Training Epoch 9] Batch 3875, Loss 0.26963791251182556\n",
      "[Training Epoch 9] Batch 3876, Loss 0.3105364441871643\n",
      "[Training Epoch 9] Batch 3877, Loss 0.25432878732681274\n",
      "[Training Epoch 9] Batch 3878, Loss 0.2640342712402344\n",
      "[Training Epoch 9] Batch 3879, Loss 0.25851544737815857\n",
      "[Training Epoch 9] Batch 3880, Loss 0.2649710774421692\n",
      "[Training Epoch 9] Batch 3881, Loss 0.2334904968738556\n",
      "[Training Epoch 9] Batch 3882, Loss 0.2604449689388275\n",
      "[Training Epoch 9] Batch 3883, Loss 0.2811313271522522\n",
      "[Training Epoch 9] Batch 3884, Loss 0.26307550072669983\n",
      "[Training Epoch 9] Batch 3885, Loss 0.2259942591190338\n",
      "[Training Epoch 9] Batch 3886, Loss 0.25978511571884155\n",
      "[Training Epoch 9] Batch 3887, Loss 0.24948260188102722\n",
      "[Training Epoch 9] Batch 3888, Loss 0.22164520621299744\n",
      "[Training Epoch 9] Batch 3889, Loss 0.23878057301044464\n",
      "[Training Epoch 9] Batch 3890, Loss 0.2771369516849518\n",
      "[Training Epoch 9] Batch 3891, Loss 0.232456237077713\n",
      "[Training Epoch 9] Batch 3892, Loss 0.2672632038593292\n",
      "[Training Epoch 9] Batch 3893, Loss 0.2782880961894989\n",
      "[Training Epoch 9] Batch 3894, Loss 0.2613109350204468\n",
      "[Training Epoch 9] Batch 3895, Loss 0.259390652179718\n",
      "[Training Epoch 9] Batch 3896, Loss 0.24704857170581818\n",
      "[Training Epoch 9] Batch 3897, Loss 0.2586163282394409\n",
      "[Training Epoch 9] Batch 3898, Loss 0.2923678755760193\n",
      "[Training Epoch 9] Batch 3899, Loss 0.2475314438343048\n",
      "[Training Epoch 9] Batch 3900, Loss 0.2458365261554718\n",
      "[Training Epoch 9] Batch 3901, Loss 0.24787160754203796\n",
      "[Training Epoch 9] Batch 3902, Loss 0.26419690251350403\n",
      "[Training Epoch 9] Batch 3903, Loss 0.25050777196884155\n",
      "[Training Epoch 9] Batch 3904, Loss 0.24611160159111023\n",
      "[Training Epoch 9] Batch 3905, Loss 0.27160024642944336\n",
      "[Training Epoch 9] Batch 3906, Loss 0.2718660235404968\n",
      "[Training Epoch 9] Batch 3907, Loss 0.24580058455467224\n",
      "[Training Epoch 9] Batch 3908, Loss 0.25293803215026855\n",
      "[Training Epoch 9] Batch 3909, Loss 0.2697579562664032\n",
      "[Training Epoch 9] Batch 3910, Loss 0.2693973481655121\n",
      "[Training Epoch 9] Batch 3911, Loss 0.28263363242149353\n",
      "[Training Epoch 9] Batch 3912, Loss 0.20967456698417664\n",
      "[Training Epoch 9] Batch 3913, Loss 0.25356537103652954\n",
      "[Training Epoch 9] Batch 3914, Loss 0.2509284019470215\n",
      "[Training Epoch 9] Batch 3915, Loss 0.2345040738582611\n",
      "[Training Epoch 9] Batch 3916, Loss 0.2775803208351135\n",
      "[Training Epoch 9] Batch 3917, Loss 0.2801961600780487\n",
      "[Training Epoch 9] Batch 3918, Loss 0.24916182458400726\n",
      "[Training Epoch 9] Batch 3919, Loss 0.25632554292678833\n",
      "[Training Epoch 9] Batch 3920, Loss 0.24374660849571228\n",
      "[Training Epoch 9] Batch 3921, Loss 0.2340819388628006\n",
      "[Training Epoch 9] Batch 3922, Loss 0.28221672773361206\n",
      "[Training Epoch 9] Batch 3923, Loss 0.29248902201652527\n",
      "[Training Epoch 9] Batch 3924, Loss 0.2561723291873932\n",
      "[Training Epoch 9] Batch 3925, Loss 0.2620035409927368\n",
      "[Training Epoch 9] Batch 3926, Loss 0.2764754295349121\n",
      "[Training Epoch 9] Batch 3927, Loss 0.23498286306858063\n",
      "[Training Epoch 9] Batch 3928, Loss 0.26746469736099243\n",
      "[Training Epoch 9] Batch 3929, Loss 0.21617776155471802\n",
      "[Training Epoch 9] Batch 3930, Loss 0.2780004143714905\n",
      "[Training Epoch 9] Batch 3931, Loss 0.2724655866622925\n",
      "[Training Epoch 9] Batch 3932, Loss 0.24906086921691895\n",
      "[Training Epoch 9] Batch 3933, Loss 0.26680871844291687\n",
      "[Training Epoch 9] Batch 3934, Loss 0.2665162682533264\n",
      "[Training Epoch 9] Batch 3935, Loss 0.24507096409797668\n",
      "[Training Epoch 9] Batch 3936, Loss 0.24810799956321716\n",
      "[Training Epoch 9] Batch 3937, Loss 0.2542152404785156\n",
      "[Training Epoch 9] Batch 3938, Loss 0.2723085880279541\n",
      "[Training Epoch 9] Batch 3939, Loss 0.2207653671503067\n",
      "[Training Epoch 9] Batch 3940, Loss 0.24705836176872253\n",
      "[Training Epoch 9] Batch 3941, Loss 0.23696519434452057\n",
      "[Training Epoch 9] Batch 3942, Loss 0.22553876042366028\n",
      "[Training Epoch 9] Batch 3943, Loss 0.2561989426612854\n",
      "[Training Epoch 9] Batch 3944, Loss 0.24418865144252777\n",
      "[Training Epoch 9] Batch 3945, Loss 0.26221632957458496\n",
      "[Training Epoch 9] Batch 3946, Loss 0.2522965371608734\n",
      "[Training Epoch 9] Batch 3947, Loss 0.2403574138879776\n",
      "[Training Epoch 9] Batch 3948, Loss 0.2839467525482178\n",
      "[Training Epoch 9] Batch 3949, Loss 0.24110299348831177\n",
      "[Training Epoch 9] Batch 3950, Loss 0.28474730253219604\n",
      "[Training Epoch 9] Batch 3951, Loss 0.28825515508651733\n",
      "[Training Epoch 9] Batch 3952, Loss 0.267160564661026\n",
      "[Training Epoch 9] Batch 3953, Loss 0.24677994847297668\n",
      "[Training Epoch 9] Batch 3954, Loss 0.24790427088737488\n",
      "[Training Epoch 9] Batch 3955, Loss 0.2542145550251007\n",
      "[Training Epoch 9] Batch 3956, Loss 0.22367700934410095\n",
      "[Training Epoch 9] Batch 3957, Loss 0.26071077585220337\n",
      "[Training Epoch 9] Batch 3958, Loss 0.23378844559192657\n",
      "[Training Epoch 9] Batch 3959, Loss 0.23589840531349182\n",
      "[Training Epoch 9] Batch 3960, Loss 0.2671319842338562\n",
      "[Training Epoch 9] Batch 3961, Loss 0.25246143341064453\n",
      "[Training Epoch 9] Batch 3962, Loss 0.27248504757881165\n",
      "[Training Epoch 9] Batch 3963, Loss 0.2803124785423279\n",
      "[Training Epoch 9] Batch 3964, Loss 0.2614593505859375\n",
      "[Training Epoch 9] Batch 3965, Loss 0.23621052503585815\n",
      "[Training Epoch 9] Batch 3966, Loss 0.2577795386314392\n",
      "[Training Epoch 9] Batch 3967, Loss 0.24112260341644287\n",
      "[Training Epoch 9] Batch 3968, Loss 0.270713210105896\n",
      "[Training Epoch 9] Batch 3969, Loss 0.25380170345306396\n",
      "[Training Epoch 9] Batch 3970, Loss 0.26905396580696106\n",
      "[Training Epoch 9] Batch 3971, Loss 0.25084275007247925\n",
      "[Training Epoch 9] Batch 3972, Loss 0.25653886795043945\n",
      "[Training Epoch 9] Batch 3973, Loss 0.2622174620628357\n",
      "[Training Epoch 9] Batch 3974, Loss 0.2554562985897064\n",
      "[Training Epoch 9] Batch 3975, Loss 0.2648311257362366\n",
      "[Training Epoch 9] Batch 3976, Loss 0.24545636773109436\n",
      "[Training Epoch 9] Batch 3977, Loss 0.23906759917736053\n",
      "[Training Epoch 9] Batch 3978, Loss 0.25072723627090454\n",
      "[Training Epoch 9] Batch 3979, Loss 0.27816295623779297\n",
      "[Training Epoch 9] Batch 3980, Loss 0.25288623571395874\n",
      "[Training Epoch 9] Batch 3981, Loss 0.27035582065582275\n",
      "[Training Epoch 9] Batch 3982, Loss 0.27918291091918945\n",
      "[Training Epoch 9] Batch 3983, Loss 0.26138895750045776\n",
      "[Training Epoch 9] Batch 3984, Loss 0.26734524965286255\n",
      "[Training Epoch 9] Batch 3985, Loss 0.2563810348510742\n",
      "[Training Epoch 9] Batch 3986, Loss 0.26517555117607117\n",
      "[Training Epoch 9] Batch 3987, Loss 0.23529434204101562\n",
      "[Training Epoch 9] Batch 3988, Loss 0.2693455219268799\n",
      "[Training Epoch 9] Batch 3989, Loss 0.2422785758972168\n",
      "[Training Epoch 9] Batch 3990, Loss 0.24869124591350555\n",
      "[Training Epoch 9] Batch 3991, Loss 0.2691306471824646\n",
      "[Training Epoch 9] Batch 3992, Loss 0.2588224709033966\n",
      "[Training Epoch 9] Batch 3993, Loss 0.26584818959236145\n",
      "[Training Epoch 9] Batch 3994, Loss 0.2591233253479004\n",
      "[Training Epoch 9] Batch 3995, Loss 0.23760806024074554\n",
      "[Training Epoch 9] Batch 3996, Loss 0.24266594648361206\n",
      "[Training Epoch 9] Batch 3997, Loss 0.25903722643852234\n",
      "[Training Epoch 9] Batch 3998, Loss 0.26767289638519287\n",
      "[Training Epoch 9] Batch 3999, Loss 0.25740331411361694\n",
      "[Training Epoch 9] Batch 4000, Loss 0.2283904105424881\n",
      "[Training Epoch 9] Batch 4001, Loss 0.21820054948329926\n",
      "[Training Epoch 9] Batch 4002, Loss 0.2543753981590271\n",
      "[Training Epoch 9] Batch 4003, Loss 0.26448363065719604\n",
      "[Training Epoch 9] Batch 4004, Loss 0.25228482484817505\n",
      "[Training Epoch 9] Batch 4005, Loss 0.22476837038993835\n",
      "[Training Epoch 9] Batch 4006, Loss 0.2572593092918396\n",
      "[Training Epoch 9] Batch 4007, Loss 0.25962144136428833\n",
      "[Training Epoch 9] Batch 4008, Loss 0.2435218095779419\n",
      "[Training Epoch 9] Batch 4009, Loss 0.23145553469657898\n",
      "[Training Epoch 9] Batch 4010, Loss 0.2342059463262558\n",
      "[Training Epoch 9] Batch 4011, Loss 0.25487834215164185\n",
      "[Training Epoch 9] Batch 4012, Loss 0.25008538365364075\n",
      "[Training Epoch 9] Batch 4013, Loss 0.249127596616745\n",
      "[Training Epoch 9] Batch 4014, Loss 0.26346737146377563\n",
      "[Training Epoch 9] Batch 4015, Loss 0.2837716341018677\n",
      "[Training Epoch 9] Batch 4016, Loss 0.26547709107398987\n",
      "[Training Epoch 9] Batch 4017, Loss 0.2379261702299118\n",
      "[Training Epoch 9] Batch 4018, Loss 0.2864420413970947\n",
      "[Training Epoch 9] Batch 4019, Loss 0.23172301054000854\n",
      "[Training Epoch 9] Batch 4020, Loss 0.2600032687187195\n",
      "[Training Epoch 9] Batch 4021, Loss 0.24918216466903687\n",
      "[Training Epoch 9] Batch 4022, Loss 0.23580384254455566\n",
      "[Training Epoch 9] Batch 4023, Loss 0.2649327218532562\n",
      "[Training Epoch 9] Batch 4024, Loss 0.266130268573761\n",
      "[Training Epoch 9] Batch 4025, Loss 0.23344287276268005\n",
      "[Training Epoch 9] Batch 4026, Loss 0.23826974630355835\n",
      "[Training Epoch 9] Batch 4027, Loss 0.2635681629180908\n",
      "[Training Epoch 9] Batch 4028, Loss 0.25096839666366577\n",
      "[Training Epoch 9] Batch 4029, Loss 0.265333354473114\n",
      "[Training Epoch 9] Batch 4030, Loss 0.25647374987602234\n",
      "[Training Epoch 9] Batch 4031, Loss 0.2615242600440979\n",
      "[Training Epoch 9] Batch 4032, Loss 0.2593098282814026\n",
      "[Training Epoch 9] Batch 4033, Loss 0.24708321690559387\n",
      "[Training Epoch 9] Batch 4034, Loss 0.25849175453186035\n",
      "[Training Epoch 9] Batch 4035, Loss 0.22943463921546936\n",
      "[Training Epoch 9] Batch 4036, Loss 0.2742826044559479\n",
      "[Training Epoch 9] Batch 4037, Loss 0.26876500248908997\n",
      "[Training Epoch 9] Batch 4038, Loss 0.246420219540596\n",
      "[Training Epoch 9] Batch 4039, Loss 0.2486390620470047\n",
      "[Training Epoch 9] Batch 4040, Loss 0.24289079010486603\n",
      "[Training Epoch 9] Batch 4041, Loss 0.2606179714202881\n",
      "[Training Epoch 9] Batch 4042, Loss 0.20434211194515228\n",
      "[Training Epoch 9] Batch 4043, Loss 0.27558624744415283\n",
      "[Training Epoch 9] Batch 4044, Loss 0.24561217427253723\n",
      "[Training Epoch 9] Batch 4045, Loss 0.26940953731536865\n",
      "[Training Epoch 9] Batch 4046, Loss 0.24542835354804993\n",
      "[Training Epoch 9] Batch 4047, Loss 0.22663553059101105\n",
      "[Training Epoch 9] Batch 4048, Loss 0.263944149017334\n",
      "[Training Epoch 9] Batch 4049, Loss 0.24846914410591125\n",
      "[Training Epoch 9] Batch 4050, Loss 0.25219035148620605\n",
      "[Training Epoch 9] Batch 4051, Loss 0.2350589781999588\n",
      "[Training Epoch 9] Batch 4052, Loss 0.24867750704288483\n",
      "[Training Epoch 9] Batch 4053, Loss 0.2812459468841553\n",
      "[Training Epoch 9] Batch 4054, Loss 0.26909711956977844\n",
      "[Training Epoch 9] Batch 4055, Loss 0.25512832403182983\n",
      "[Training Epoch 9] Batch 4056, Loss 0.26638248562812805\n",
      "[Training Epoch 9] Batch 4057, Loss 0.2782132625579834\n",
      "[Training Epoch 9] Batch 4058, Loss 0.26000046730041504\n",
      "[Training Epoch 9] Batch 4059, Loss 0.24820715188980103\n",
      "[Training Epoch 9] Batch 4060, Loss 0.2464791238307953\n",
      "[Training Epoch 9] Batch 4061, Loss 0.24199432134628296\n",
      "[Training Epoch 9] Batch 4062, Loss 0.2476675808429718\n",
      "[Training Epoch 9] Batch 4063, Loss 0.2706594467163086\n",
      "[Training Epoch 9] Batch 4064, Loss 0.28770747780799866\n",
      "[Training Epoch 9] Batch 4065, Loss 0.29660648107528687\n",
      "[Training Epoch 9] Batch 4066, Loss 0.2501027286052704\n",
      "[Training Epoch 9] Batch 4067, Loss 0.21705816686153412\n",
      "[Training Epoch 9] Batch 4068, Loss 0.25156131386756897\n",
      "[Training Epoch 9] Batch 4069, Loss 0.23770180344581604\n",
      "[Training Epoch 9] Batch 4070, Loss 0.27416014671325684\n",
      "[Training Epoch 9] Batch 4071, Loss 0.2612350285053253\n",
      "[Training Epoch 9] Batch 4072, Loss 0.24835249781608582\n",
      "[Training Epoch 9] Batch 4073, Loss 0.24717828631401062\n",
      "[Training Epoch 9] Batch 4074, Loss 0.2524036169052124\n",
      "[Training Epoch 9] Batch 4075, Loss 0.25117599964141846\n",
      "[Training Epoch 9] Batch 4076, Loss 0.23841845989227295\n",
      "[Training Epoch 9] Batch 4077, Loss 0.2591511607170105\n",
      "[Training Epoch 9] Batch 4078, Loss 0.25083744525909424\n",
      "[Training Epoch 9] Batch 4079, Loss 0.20765796303749084\n",
      "[Training Epoch 9] Batch 4080, Loss 0.27239054441452026\n",
      "[Training Epoch 9] Batch 4081, Loss 0.27334970235824585\n",
      "[Training Epoch 9] Batch 4082, Loss 0.2341049462556839\n",
      "[Training Epoch 9] Batch 4083, Loss 0.27966874837875366\n",
      "[Training Epoch 9] Batch 4084, Loss 0.25130921602249146\n",
      "[Training Epoch 9] Batch 4085, Loss 0.26167768239974976\n",
      "[Training Epoch 9] Batch 4086, Loss 0.24219100177288055\n",
      "[Training Epoch 9] Batch 4087, Loss 0.27160191535949707\n",
      "[Training Epoch 9] Batch 4088, Loss 0.2367834597826004\n",
      "[Training Epoch 9] Batch 4089, Loss 0.2913358807563782\n",
      "[Training Epoch 9] Batch 4090, Loss 0.24455784261226654\n",
      "[Training Epoch 9] Batch 4091, Loss 0.23617948591709137\n",
      "[Training Epoch 9] Batch 4092, Loss 0.23950958251953125\n",
      "[Training Epoch 9] Batch 4093, Loss 0.26768726110458374\n",
      "[Training Epoch 9] Batch 4094, Loss 0.2340855896472931\n",
      "[Training Epoch 9] Batch 4095, Loss 0.2489486038684845\n",
      "[Training Epoch 9] Batch 4096, Loss 0.21486115455627441\n",
      "[Training Epoch 9] Batch 4097, Loss 0.23985415697097778\n",
      "[Training Epoch 9] Batch 4098, Loss 0.22462449967861176\n",
      "[Training Epoch 9] Batch 4099, Loss 0.24495209753513336\n",
      "[Training Epoch 9] Batch 4100, Loss 0.2630593776702881\n",
      "[Training Epoch 9] Batch 4101, Loss 0.2632526755332947\n",
      "[Training Epoch 9] Batch 4102, Loss 0.29290664196014404\n",
      "[Training Epoch 9] Batch 4103, Loss 0.23878484964370728\n",
      "[Training Epoch 9] Batch 4104, Loss 0.2786509096622467\n",
      "[Training Epoch 9] Batch 4105, Loss 0.2758520543575287\n",
      "[Training Epoch 9] Batch 4106, Loss 0.25260961055755615\n",
      "[Training Epoch 9] Batch 4107, Loss 0.23741433024406433\n",
      "[Training Epoch 9] Batch 4108, Loss 0.25745612382888794\n",
      "[Training Epoch 9] Batch 4109, Loss 0.26208221912384033\n",
      "[Training Epoch 9] Batch 4110, Loss 0.2656695246696472\n",
      "[Training Epoch 9] Batch 4111, Loss 0.25772055983543396\n",
      "[Training Epoch 9] Batch 4112, Loss 0.2736542224884033\n",
      "[Training Epoch 9] Batch 4113, Loss 0.24747920036315918\n",
      "[Training Epoch 9] Batch 4114, Loss 0.25659096240997314\n",
      "[Training Epoch 9] Batch 4115, Loss 0.2762604355812073\n",
      "[Training Epoch 9] Batch 4116, Loss 0.23494936525821686\n",
      "[Training Epoch 9] Batch 4117, Loss 0.2689201235771179\n",
      "[Training Epoch 9] Batch 4118, Loss 0.28398776054382324\n",
      "[Training Epoch 9] Batch 4119, Loss 0.22904863953590393\n",
      "[Training Epoch 9] Batch 4120, Loss 0.2556946873664856\n",
      "[Training Epoch 9] Batch 4121, Loss 0.25588229298591614\n",
      "[Training Epoch 9] Batch 4122, Loss 0.258323073387146\n",
      "[Training Epoch 9] Batch 4123, Loss 0.22858762741088867\n",
      "[Training Epoch 9] Batch 4124, Loss 0.28394776582717896\n",
      "[Training Epoch 9] Batch 4125, Loss 0.2557985484600067\n",
      "[Training Epoch 9] Batch 4126, Loss 0.2557726502418518\n",
      "[Training Epoch 9] Batch 4127, Loss 0.22990114986896515\n",
      "[Training Epoch 9] Batch 4128, Loss 0.23296134173870087\n",
      "[Training Epoch 9] Batch 4129, Loss 0.2583416700363159\n",
      "[Training Epoch 9] Batch 4130, Loss 0.28054797649383545\n",
      "[Training Epoch 9] Batch 4131, Loss 0.24500560760498047\n",
      "[Training Epoch 9] Batch 4132, Loss 0.24385963380336761\n",
      "[Training Epoch 9] Batch 4133, Loss 0.2953086793422699\n",
      "[Training Epoch 9] Batch 4134, Loss 0.2538336515426636\n",
      "[Training Epoch 9] Batch 4135, Loss 0.24510133266448975\n",
      "[Training Epoch 9] Batch 4136, Loss 0.2723921835422516\n",
      "[Training Epoch 9] Batch 4137, Loss 0.2895432114601135\n",
      "[Training Epoch 9] Batch 4138, Loss 0.2614867687225342\n",
      "[Training Epoch 9] Batch 4139, Loss 0.2587442696094513\n",
      "[Training Epoch 9] Batch 4140, Loss 0.2669685482978821\n",
      "[Training Epoch 9] Batch 4141, Loss 0.23831772804260254\n",
      "[Training Epoch 9] Batch 4142, Loss 0.2686399817466736\n",
      "[Training Epoch 9] Batch 4143, Loss 0.24193036556243896\n",
      "[Training Epoch 9] Batch 4144, Loss 0.22879745066165924\n",
      "[Training Epoch 9] Batch 4145, Loss 0.2455253303050995\n",
      "[Training Epoch 9] Batch 4146, Loss 0.2605736553668976\n",
      "[Training Epoch 9] Batch 4147, Loss 0.23970647156238556\n",
      "[Training Epoch 9] Batch 4148, Loss 0.26012977957725525\n",
      "[Training Epoch 9] Batch 4149, Loss 0.26386553049087524\n",
      "[Training Epoch 9] Batch 4150, Loss 0.2538575828075409\n",
      "[Training Epoch 9] Batch 4151, Loss 0.2555660605430603\n",
      "[Training Epoch 9] Batch 4152, Loss 0.2500133216381073\n",
      "[Training Epoch 9] Batch 4153, Loss 0.28828495740890503\n",
      "[Training Epoch 9] Batch 4154, Loss 0.23795902729034424\n",
      "[Training Epoch 9] Batch 4155, Loss 0.277739018201828\n",
      "[Training Epoch 9] Batch 4156, Loss 0.2544814646244049\n",
      "[Training Epoch 9] Batch 4157, Loss 0.26989656686782837\n",
      "[Training Epoch 9] Batch 4158, Loss 0.2713541090488434\n",
      "[Training Epoch 9] Batch 4159, Loss 0.25201690196990967\n",
      "[Training Epoch 9] Batch 4160, Loss 0.25513529777526855\n",
      "[Training Epoch 9] Batch 4161, Loss 0.2683245837688446\n",
      "[Training Epoch 9] Batch 4162, Loss 0.2429719865322113\n",
      "[Training Epoch 9] Batch 4163, Loss 0.2477080374956131\n",
      "[Training Epoch 9] Batch 4164, Loss 0.23980945348739624\n",
      "[Training Epoch 9] Batch 4165, Loss 0.22140324115753174\n",
      "[Training Epoch 9] Batch 4166, Loss 0.27932775020599365\n",
      "[Training Epoch 9] Batch 4167, Loss 0.2763899564743042\n",
      "[Training Epoch 9] Batch 4168, Loss 0.2610231637954712\n",
      "[Training Epoch 9] Batch 4169, Loss 0.2727561593055725\n",
      "[Training Epoch 9] Batch 4170, Loss 0.2927301824092865\n",
      "[Training Epoch 9] Batch 4171, Loss 0.2609815299510956\n",
      "[Training Epoch 9] Batch 4172, Loss 0.23819495737552643\n",
      "[Training Epoch 9] Batch 4173, Loss 0.24938219785690308\n",
      "[Training Epoch 9] Batch 4174, Loss 0.27687424421310425\n",
      "[Training Epoch 9] Batch 4175, Loss 0.23139280080795288\n",
      "[Training Epoch 9] Batch 4176, Loss 0.2382475882768631\n",
      "[Training Epoch 9] Batch 4177, Loss 0.2982640862464905\n",
      "[Training Epoch 9] Batch 4178, Loss 0.2652958035469055\n",
      "[Training Epoch 9] Batch 4179, Loss 0.25713050365448\n",
      "[Training Epoch 9] Batch 4180, Loss 0.2663916349411011\n",
      "[Training Epoch 9] Batch 4181, Loss 0.2620970606803894\n",
      "[Training Epoch 9] Batch 4182, Loss 0.25561586022377014\n",
      "[Training Epoch 9] Batch 4183, Loss 0.25493595004081726\n",
      "[Training Epoch 9] Batch 4184, Loss 0.2475118786096573\n",
      "[Training Epoch 9] Batch 4185, Loss 0.26063042879104614\n",
      "[Training Epoch 9] Batch 4186, Loss 0.242397278547287\n",
      "[Training Epoch 9] Batch 4187, Loss 0.27294233441352844\n",
      "[Training Epoch 9] Batch 4188, Loss 0.2669386863708496\n",
      "[Training Epoch 9] Batch 4189, Loss 0.24694675207138062\n",
      "[Training Epoch 9] Batch 4190, Loss 0.26418089866638184\n",
      "[Training Epoch 9] Batch 4191, Loss 0.25732988119125366\n",
      "[Training Epoch 9] Batch 4192, Loss 0.2575927972793579\n",
      "[Training Epoch 9] Batch 4193, Loss 0.2637948989868164\n",
      "[Training Epoch 9] Batch 4194, Loss 0.2362731695175171\n",
      "[Training Epoch 9] Batch 4195, Loss 0.247336745262146\n",
      "[Training Epoch 9] Batch 4196, Loss 0.2672000527381897\n",
      "[Training Epoch 9] Batch 4197, Loss 0.2801739573478699\n",
      "[Training Epoch 9] Batch 4198, Loss 0.25847136974334717\n",
      "[Training Epoch 9] Batch 4199, Loss 0.2663959562778473\n",
      "[Training Epoch 9] Batch 4200, Loss 0.25986799597740173\n",
      "[Training Epoch 9] Batch 4201, Loss 0.2464999407529831\n",
      "[Training Epoch 9] Batch 4202, Loss 0.26125437021255493\n",
      "[Training Epoch 9] Batch 4203, Loss 0.25395017862319946\n",
      "[Training Epoch 9] Batch 4204, Loss 0.2728121876716614\n",
      "[Training Epoch 9] Batch 4205, Loss 0.2431630939245224\n",
      "[Training Epoch 9] Batch 4206, Loss 0.2283724844455719\n",
      "[Training Epoch 9] Batch 4207, Loss 0.21883556246757507\n",
      "[Training Epoch 9] Batch 4208, Loss 0.24697697162628174\n",
      "[Training Epoch 9] Batch 4209, Loss 0.25032609701156616\n",
      "[Training Epoch 9] Batch 4210, Loss 0.26028934121131897\n",
      "[Training Epoch 9] Batch 4211, Loss 0.27126333117485046\n",
      "[Training Epoch 9] Batch 4212, Loss 0.250114768743515\n",
      "[Training Epoch 9] Batch 4213, Loss 0.2502105236053467\n",
      "[Training Epoch 9] Batch 4214, Loss 0.22899284958839417\n",
      "[Training Epoch 9] Batch 4215, Loss 0.24131742119789124\n",
      "[Training Epoch 9] Batch 4216, Loss 0.2676851451396942\n",
      "[Training Epoch 9] Batch 4217, Loss 0.2556909918785095\n",
      "[Training Epoch 9] Batch 4218, Loss 0.2832523584365845\n",
      "[Training Epoch 9] Batch 4219, Loss 0.2521013617515564\n",
      "[Training Epoch 9] Batch 4220, Loss 0.247287780046463\n",
      "[Training Epoch 9] Batch 4221, Loss 0.27512866258621216\n",
      "[Training Epoch 9] Batch 4222, Loss 0.24563466012477875\n",
      "[Training Epoch 9] Batch 4223, Loss 0.2534441351890564\n",
      "[Training Epoch 9] Batch 4224, Loss 0.2556748390197754\n",
      "[Training Epoch 9] Batch 4225, Loss 0.27050918340682983\n",
      "[Training Epoch 9] Batch 4226, Loss 0.24244394898414612\n",
      "[Training Epoch 9] Batch 4227, Loss 0.29034942388534546\n",
      "[Training Epoch 9] Batch 4228, Loss 0.23147757351398468\n",
      "[Training Epoch 9] Batch 4229, Loss 0.27252331376075745\n",
      "[Training Epoch 9] Batch 4230, Loss 0.23387925326824188\n",
      "[Training Epoch 9] Batch 4231, Loss 0.2286541610956192\n",
      "[Training Epoch 9] Batch 4232, Loss 0.25633201003074646\n",
      "[Training Epoch 9] Batch 4233, Loss 0.23576395213603973\n",
      "[Training Epoch 9] Batch 4234, Loss 0.2559615969657898\n",
      "[Training Epoch 9] Batch 4235, Loss 0.2658689618110657\n",
      "[Training Epoch 9] Batch 4236, Loss 0.266776442527771\n",
      "[Training Epoch 9] Batch 4237, Loss 0.25355005264282227\n",
      "[Training Epoch 9] Batch 4238, Loss 0.2565094232559204\n",
      "[Training Epoch 9] Batch 4239, Loss 0.2618473768234253\n",
      "[Training Epoch 9] Batch 4240, Loss 0.2911437451839447\n",
      "[Training Epoch 9] Batch 4241, Loss 0.2615732252597809\n",
      "[Training Epoch 9] Batch 4242, Loss 0.27748042345046997\n",
      "[Training Epoch 9] Batch 4243, Loss 0.24837827682495117\n",
      "[Training Epoch 9] Batch 4244, Loss 0.24386240541934967\n",
      "[Training Epoch 9] Batch 4245, Loss 0.28456979990005493\n",
      "[Training Epoch 9] Batch 4246, Loss 0.25546592473983765\n",
      "[Training Epoch 9] Batch 4247, Loss 0.24431708455085754\n",
      "[Training Epoch 9] Batch 4248, Loss 0.2616459131240845\n",
      "[Training Epoch 9] Batch 4249, Loss 0.2552452087402344\n",
      "[Training Epoch 9] Batch 4250, Loss 0.24440771341323853\n",
      "[Training Epoch 9] Batch 4251, Loss 0.26503533124923706\n",
      "[Training Epoch 9] Batch 4252, Loss 0.26801782846450806\n",
      "[Training Epoch 9] Batch 4253, Loss 0.22625823318958282\n",
      "[Training Epoch 9] Batch 4254, Loss 0.23108355700969696\n",
      "[Training Epoch 9] Batch 4255, Loss 0.25530239939689636\n",
      "[Training Epoch 9] Batch 4256, Loss 0.2574285864830017\n",
      "[Training Epoch 9] Batch 4257, Loss 0.2664998173713684\n",
      "[Training Epoch 9] Batch 4258, Loss 0.24172796308994293\n",
      "[Training Epoch 9] Batch 4259, Loss 0.23710192739963531\n",
      "[Training Epoch 9] Batch 4260, Loss 0.23364561796188354\n",
      "[Training Epoch 9] Batch 4261, Loss 0.2771355211734772\n",
      "[Training Epoch 9] Batch 4262, Loss 0.27124905586242676\n",
      "[Training Epoch 9] Batch 4263, Loss 0.23020777106285095\n",
      "[Training Epoch 9] Batch 4264, Loss 0.24605676531791687\n",
      "[Training Epoch 9] Batch 4265, Loss 0.2691432237625122\n",
      "[Training Epoch 9] Batch 4266, Loss 0.27126431465148926\n",
      "[Training Epoch 9] Batch 4267, Loss 0.25633496046066284\n",
      "[Training Epoch 9] Batch 4268, Loss 0.25654512643814087\n",
      "[Training Epoch 9] Batch 4269, Loss 0.24981459975242615\n",
      "[Training Epoch 9] Batch 4270, Loss 0.2717815637588501\n",
      "[Training Epoch 9] Batch 4271, Loss 0.2340177297592163\n",
      "[Training Epoch 9] Batch 4272, Loss 0.2543819546699524\n",
      "[Training Epoch 9] Batch 4273, Loss 0.263827383518219\n",
      "[Training Epoch 9] Batch 4274, Loss 0.23758339881896973\n",
      "[Training Epoch 9] Batch 4275, Loss 0.26961153745651245\n",
      "[Training Epoch 9] Batch 4276, Loss 0.2448832094669342\n",
      "[Training Epoch 9] Batch 4277, Loss 0.22382766008377075\n",
      "[Training Epoch 9] Batch 4278, Loss 0.23768135905265808\n",
      "[Training Epoch 9] Batch 4279, Loss 0.23192457854747772\n",
      "[Training Epoch 9] Batch 4280, Loss 0.2317775934934616\n",
      "[Training Epoch 9] Batch 4281, Loss 0.2656252980232239\n",
      "[Training Epoch 9] Batch 4282, Loss 0.2802278697490692\n",
      "[Training Epoch 9] Batch 4283, Loss 0.2590683102607727\n",
      "[Training Epoch 9] Batch 4284, Loss 0.24340927600860596\n",
      "[Training Epoch 9] Batch 4285, Loss 0.23146170377731323\n",
      "[Training Epoch 9] Batch 4286, Loss 0.23539063334465027\n",
      "[Training Epoch 9] Batch 4287, Loss 0.25680047273635864\n",
      "[Training Epoch 9] Batch 4288, Loss 0.27721643447875977\n",
      "[Training Epoch 9] Batch 4289, Loss 0.26420724391937256\n",
      "[Training Epoch 9] Batch 4290, Loss 0.2722189724445343\n",
      "[Training Epoch 9] Batch 4291, Loss 0.24512457847595215\n",
      "[Training Epoch 9] Batch 4292, Loss 0.26903843879699707\n",
      "[Training Epoch 9] Batch 4293, Loss 0.2350369095802307\n",
      "[Training Epoch 9] Batch 4294, Loss 0.2629835605621338\n",
      "[Training Epoch 9] Batch 4295, Loss 0.2558135688304901\n",
      "[Training Epoch 9] Batch 4296, Loss 0.25665998458862305\n",
      "[Training Epoch 9] Batch 4297, Loss 0.2658986449241638\n",
      "[Training Epoch 9] Batch 4298, Loss 0.2531995177268982\n",
      "[Training Epoch 9] Batch 4299, Loss 0.2609867453575134\n",
      "[Training Epoch 9] Batch 4300, Loss 0.2532559931278229\n",
      "[Training Epoch 9] Batch 4301, Loss 0.23284181952476501\n",
      "[Training Epoch 9] Batch 4302, Loss 0.2519969940185547\n",
      "[Training Epoch 9] Batch 4303, Loss 0.2522091269493103\n",
      "[Training Epoch 9] Batch 4304, Loss 0.26957452297210693\n",
      "[Training Epoch 9] Batch 4305, Loss 0.2917376458644867\n",
      "[Training Epoch 9] Batch 4306, Loss 0.23476222157478333\n",
      "[Training Epoch 9] Batch 4307, Loss 0.23831665515899658\n",
      "[Training Epoch 9] Batch 4308, Loss 0.26911723613739014\n",
      "[Training Epoch 9] Batch 4309, Loss 0.25645577907562256\n",
      "[Training Epoch 9] Batch 4310, Loss 0.2442481368780136\n",
      "[Training Epoch 9] Batch 4311, Loss 0.24105793237686157\n",
      "[Training Epoch 9] Batch 4312, Loss 0.255797803401947\n",
      "[Training Epoch 9] Batch 4313, Loss 0.24345487356185913\n",
      "[Training Epoch 9] Batch 4314, Loss 0.25068652629852295\n",
      "[Training Epoch 9] Batch 4315, Loss 0.23821310698986053\n",
      "[Training Epoch 9] Batch 4316, Loss 0.24973516166210175\n",
      "[Training Epoch 9] Batch 4317, Loss 0.2637651562690735\n",
      "[Training Epoch 9] Batch 4318, Loss 0.25431790947914124\n",
      "[Training Epoch 9] Batch 4319, Loss 0.2619146704673767\n",
      "[Training Epoch 9] Batch 4320, Loss 0.28310367465019226\n",
      "[Training Epoch 9] Batch 4321, Loss 0.2446283996105194\n",
      "[Training Epoch 9] Batch 4322, Loss 0.27031442523002625\n",
      "[Training Epoch 9] Batch 4323, Loss 0.26911240816116333\n",
      "[Training Epoch 9] Batch 4324, Loss 0.26707687973976135\n",
      "[Training Epoch 9] Batch 4325, Loss 0.26841795444488525\n",
      "[Training Epoch 9] Batch 4326, Loss 0.25699418783187866\n",
      "[Training Epoch 9] Batch 4327, Loss 0.25207003951072693\n",
      "[Training Epoch 9] Batch 4328, Loss 0.2897689938545227\n",
      "[Training Epoch 9] Batch 4329, Loss 0.27204954624176025\n",
      "[Training Epoch 9] Batch 4330, Loss 0.2566879689693451\n",
      "[Training Epoch 9] Batch 4331, Loss 0.2624710202217102\n",
      "[Training Epoch 9] Batch 4332, Loss 0.2561473250389099\n",
      "[Training Epoch 9] Batch 4333, Loss 0.24396395683288574\n",
      "[Training Epoch 9] Batch 4334, Loss 0.2549622654914856\n",
      "[Training Epoch 9] Batch 4335, Loss 0.25382471084594727\n",
      "[Training Epoch 9] Batch 4336, Loss 0.2590663433074951\n",
      "[Training Epoch 9] Batch 4337, Loss 0.23699524998664856\n",
      "[Training Epoch 9] Batch 4338, Loss 0.22716131806373596\n",
      "[Training Epoch 9] Batch 4339, Loss 0.2542542815208435\n",
      "[Training Epoch 9] Batch 4340, Loss 0.28225862979888916\n",
      "[Training Epoch 9] Batch 4341, Loss 0.2617315351963043\n",
      "[Training Epoch 9] Batch 4342, Loss 0.2735329568386078\n",
      "[Training Epoch 9] Batch 4343, Loss 0.24479123950004578\n",
      "[Training Epoch 9] Batch 4344, Loss 0.25729310512542725\n",
      "[Training Epoch 9] Batch 4345, Loss 0.2481808066368103\n",
      "[Training Epoch 9] Batch 4346, Loss 0.22931739687919617\n",
      "[Training Epoch 9] Batch 4347, Loss 0.2523694634437561\n",
      "[Training Epoch 9] Batch 4348, Loss 0.28493160009384155\n",
      "[Training Epoch 9] Batch 4349, Loss 0.25581470131874084\n",
      "[Training Epoch 9] Batch 4350, Loss 0.29386407136917114\n",
      "[Training Epoch 9] Batch 4351, Loss 0.257842481136322\n",
      "[Training Epoch 9] Batch 4352, Loss 0.25473248958587646\n",
      "[Training Epoch 9] Batch 4353, Loss 0.23454207181930542\n",
      "[Training Epoch 9] Batch 4354, Loss 0.26869258284568787\n",
      "[Training Epoch 9] Batch 4355, Loss 0.2356862723827362\n",
      "[Training Epoch 9] Batch 4356, Loss 0.25149691104888916\n",
      "[Training Epoch 9] Batch 4357, Loss 0.2595100998878479\n",
      "[Training Epoch 9] Batch 4358, Loss 0.2521595358848572\n",
      "[Training Epoch 9] Batch 4359, Loss 0.26774993538856506\n",
      "[Training Epoch 9] Batch 4360, Loss 0.25134915113449097\n",
      "[Training Epoch 9] Batch 4361, Loss 0.24588434398174286\n",
      "[Training Epoch 9] Batch 4362, Loss 0.2601502537727356\n",
      "[Training Epoch 9] Batch 4363, Loss 0.2500079274177551\n",
      "[Training Epoch 9] Batch 4364, Loss 0.2562265396118164\n",
      "[Training Epoch 9] Batch 4365, Loss 0.26250800490379333\n",
      "[Training Epoch 9] Batch 4366, Loss 0.24980928003787994\n",
      "[Training Epoch 9] Batch 4367, Loss 0.2507941722869873\n",
      "[Training Epoch 9] Batch 4368, Loss 0.26820290088653564\n",
      "[Training Epoch 9] Batch 4369, Loss 0.2718084156513214\n",
      "[Training Epoch 9] Batch 4370, Loss 0.2583141326904297\n",
      "[Training Epoch 9] Batch 4371, Loss 0.2201615422964096\n",
      "[Training Epoch 9] Batch 4372, Loss 0.26205408573150635\n",
      "[Training Epoch 9] Batch 4373, Loss 0.27000516653060913\n",
      "[Training Epoch 9] Batch 4374, Loss 0.2519568204879761\n",
      "[Training Epoch 9] Batch 4375, Loss 0.2697286009788513\n",
      "[Training Epoch 9] Batch 4376, Loss 0.2372514009475708\n",
      "[Training Epoch 9] Batch 4377, Loss 0.26442044973373413\n",
      "[Training Epoch 9] Batch 4378, Loss 0.2528582215309143\n",
      "[Training Epoch 9] Batch 4379, Loss 0.2725628614425659\n",
      "[Training Epoch 9] Batch 4380, Loss 0.2292136400938034\n",
      "[Training Epoch 9] Batch 4381, Loss 0.24576757848262787\n",
      "[Training Epoch 9] Batch 4382, Loss 0.23551440238952637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9935/9935 [00:04<00:00, 2257.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 102759\n",
      "Length of test_items: 102759\n",
      "Length of test_preds: 102759\n",
      "[Evaluating Epoch 9] Precision = 0.2612, Recall = 0.7725\n"
     ]
    }
   ],
   "source": [
    "# Reindex\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n",
    "# DataLoader for training \n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "\n",
    "config = neumf_config\n",
    "engine = NeuMFEngine(config)\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    precision, recall = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    engine.save(config['alias'], epoch, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodbooks_dir = 'goodbooks-10k/ratings.csv'\n",
    "goodbooks_rating = pd.read_csv(goodbooks_dir)   \n",
    "\n",
    "user_ids = goodbooks_rating['uid'].sample(n=1000, random_state=42).unique()\n",
    "goodbooks_rating = goodbooks_rating[goodbooks_rating['uid'].isin(user_ids)]\n",
    "\n",
    "# Filter out books with less than 100 ratings\n",
    "book_counts = goodbooks_rating['mid'].value_counts()\n",
    "book_ids = (book_counts[book_counts >= 30].index)\n",
    "goodbooks_rating = goodbooks_rating[goodbooks_rating['mid'].isin(book_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>mid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>55</td>\n",
       "      <td>212</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>55</td>\n",
       "      <td>1544</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>55</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>55</td>\n",
       "      <td>397</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>55</td>\n",
       "      <td>653</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047863</th>\n",
       "      <td>7747</td>\n",
       "      <td>366</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047867</th>\n",
       "      <td>7747</td>\n",
       "      <td>144</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047954</th>\n",
       "      <td>7747</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048028</th>\n",
       "      <td>7747</td>\n",
       "      <td>1641</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048032</th>\n",
       "      <td>7747</td>\n",
       "      <td>173</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56311 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid   mid  rating  timestamp\n",
       "622        55   212       5          0\n",
       "624        55  1544       5          0\n",
       "643        55   264       5          0\n",
       "644        55   397       2          0\n",
       "645        55   653       5          0\n",
       "...       ...   ...     ...        ...\n",
       "1047863  7747   366       5          0\n",
       "1047867  7747   144       5          0\n",
       "1047954  7747   179       3          0\n",
       "1048028  7747  1641       5          0\n",
       "1048032  7747   173       3          0\n",
       "\n",
       "[56311 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodbooks_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_config = {'alias': 'neumf_first_try',\n",
    "                'num_epoch': 10,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 944,\n",
    "                'num_items': 693,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16, 64, 32, 16, 8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "                'l2_regularization': 0.0000001,\n",
    "                'weight_init_gaussian': True,\n",
    "                'use_cuda': True,\n",
    "                'use_bachify_eval': True,\n",
    "                'device_id': 0,\n",
    "                'pretrain': False,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_precision0.6391_recall0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_precision0.5606_recall0.2463.model'),\n",
    "                'model_dir': 'checkpoints/{}_Epoch{}_precision{:.4f}_recall{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 943]\n",
      "Range of itemId is [0, 692]\n",
      "Index(['userId', 'itemId', 'rating', 'real_score', 'negative_samples'], dtype='object')\n",
      "Embedding(944, 8)\n",
      "Embedding(693, 8)\n",
      "Embedding(944, 8)\n",
      "Embedding(693, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(944, 8)\n",
      "  (embedding_item_mlp): Embedding(693, 8)\n",
      "  (embedding_user_mf): Embedding(944, 8)\n",
      "  (embedding_item_mf): Embedding(693, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6620096564292908\n",
      "[Training Epoch 0] Batch 1, Loss 0.6601542234420776\n",
      "[Training Epoch 0] Batch 2, Loss 0.6597121953964233\n",
      "[Training Epoch 0] Batch 3, Loss 0.658929705619812\n",
      "[Training Epoch 0] Batch 4, Loss 0.6575688719749451\n",
      "[Training Epoch 0] Batch 5, Loss 0.6596298217773438\n",
      "[Training Epoch 0] Batch 6, Loss 0.6597685217857361\n",
      "[Training Epoch 0] Batch 7, Loss 0.660853922367096\n",
      "[Training Epoch 0] Batch 8, Loss 0.6598381996154785\n",
      "[Training Epoch 0] Batch 9, Loss 0.6584498286247253\n",
      "[Training Epoch 0] Batch 10, Loss 0.6555910110473633\n",
      "[Training Epoch 0] Batch 11, Loss 0.656092643737793\n",
      "[Training Epoch 0] Batch 12, Loss 0.6543940305709839\n",
      "[Training Epoch 0] Batch 13, Loss 0.6566436290740967\n",
      "[Training Epoch 0] Batch 14, Loss 0.6556805372238159\n",
      "[Training Epoch 0] Batch 15, Loss 0.6541975736618042\n",
      "[Training Epoch 0] Batch 16, Loss 0.6581917405128479\n",
      "[Training Epoch 0] Batch 17, Loss 0.655553936958313\n",
      "[Training Epoch 0] Batch 18, Loss 0.6536532044410706\n",
      "[Training Epoch 0] Batch 19, Loss 0.6573801040649414\n",
      "[Training Epoch 0] Batch 20, Loss 0.6560028195381165\n",
      "[Training Epoch 0] Batch 21, Loss 0.6528618931770325\n",
      "[Training Epoch 0] Batch 22, Loss 0.6580568552017212\n",
      "[Training Epoch 0] Batch 23, Loss 0.6535203456878662\n",
      "[Training Epoch 0] Batch 24, Loss 0.651522696018219\n",
      "[Training Epoch 0] Batch 25, Loss 0.6538037061691284\n",
      "[Training Epoch 0] Batch 26, Loss 0.6500933766365051\n",
      "[Training Epoch 0] Batch 27, Loss 0.6526936292648315\n",
      "[Training Epoch 0] Batch 28, Loss 0.654344916343689\n",
      "[Training Epoch 0] Batch 29, Loss 0.6504013538360596\n",
      "[Training Epoch 0] Batch 30, Loss 0.6558574438095093\n",
      "[Training Epoch 0] Batch 31, Loss 0.6470085382461548\n",
      "[Training Epoch 0] Batch 32, Loss 0.6512044668197632\n",
      "[Training Epoch 0] Batch 33, Loss 0.6496387720108032\n",
      "[Training Epoch 0] Batch 34, Loss 0.6491034626960754\n",
      "[Training Epoch 0] Batch 35, Loss 0.6535980701446533\n",
      "[Training Epoch 0] Batch 36, Loss 0.6463128328323364\n",
      "[Training Epoch 0] Batch 37, Loss 0.6505590081214905\n",
      "[Training Epoch 0] Batch 38, Loss 0.6500176191329956\n",
      "[Training Epoch 0] Batch 39, Loss 0.647403359413147\n",
      "[Training Epoch 0] Batch 40, Loss 0.6429728269577026\n",
      "[Training Epoch 0] Batch 41, Loss 0.647354245185852\n",
      "[Training Epoch 0] Batch 42, Loss 0.644463062286377\n",
      "[Training Epoch 0] Batch 43, Loss 0.6454761624336243\n",
      "[Training Epoch 0] Batch 44, Loss 0.6471766233444214\n",
      "[Training Epoch 0] Batch 45, Loss 0.6404675245285034\n",
      "[Training Epoch 0] Batch 46, Loss 0.6436796188354492\n",
      "[Training Epoch 0] Batch 47, Loss 0.6412601470947266\n",
      "[Training Epoch 0] Batch 48, Loss 0.6438454389572144\n",
      "[Training Epoch 0] Batch 49, Loss 0.637647271156311\n",
      "[Training Epoch 0] Batch 50, Loss 0.6420272588729858\n",
      "[Training Epoch 0] Batch 51, Loss 0.6432493925094604\n",
      "[Training Epoch 0] Batch 52, Loss 0.6428452134132385\n",
      "[Training Epoch 0] Batch 53, Loss 0.640564501285553\n",
      "[Training Epoch 0] Batch 54, Loss 0.6406652927398682\n",
      "[Training Epoch 0] Batch 55, Loss 0.6333039999008179\n",
      "[Training Epoch 0] Batch 56, Loss 0.6422023773193359\n",
      "[Training Epoch 0] Batch 57, Loss 0.6367884874343872\n",
      "[Training Epoch 0] Batch 58, Loss 0.6431889533996582\n",
      "[Training Epoch 0] Batch 59, Loss 0.6398447751998901\n",
      "[Training Epoch 0] Batch 60, Loss 0.6367861032485962\n",
      "[Training Epoch 0] Batch 61, Loss 0.6346572041511536\n",
      "[Training Epoch 0] Batch 62, Loss 0.6335188150405884\n",
      "[Training Epoch 0] Batch 63, Loss 0.6333938837051392\n",
      "[Training Epoch 0] Batch 64, Loss 0.6332545280456543\n",
      "[Training Epoch 0] Batch 65, Loss 0.6340205073356628\n",
      "[Training Epoch 0] Batch 66, Loss 0.6313002109527588\n",
      "[Training Epoch 0] Batch 67, Loss 0.6256750822067261\n",
      "[Training Epoch 0] Batch 68, Loss 0.6267232894897461\n",
      "[Training Epoch 0] Batch 69, Loss 0.6297761797904968\n",
      "[Training Epoch 0] Batch 70, Loss 0.6244055032730103\n",
      "[Training Epoch 0] Batch 71, Loss 0.6229085326194763\n",
      "[Training Epoch 0] Batch 72, Loss 0.62526535987854\n",
      "[Training Epoch 0] Batch 73, Loss 0.6215119361877441\n",
      "[Training Epoch 0] Batch 74, Loss 0.6164050698280334\n",
      "[Training Epoch 0] Batch 75, Loss 0.6193938255310059\n",
      "[Training Epoch 0] Batch 76, Loss 0.6142334342002869\n",
      "[Training Epoch 0] Batch 77, Loss 0.610254168510437\n",
      "[Training Epoch 0] Batch 78, Loss 0.6067088842391968\n",
      "[Training Epoch 0] Batch 79, Loss 0.6056230068206787\n",
      "[Training Epoch 0] Batch 80, Loss 0.6033742427825928\n",
      "[Training Epoch 0] Batch 81, Loss 0.6006596684455872\n",
      "[Training Epoch 0] Batch 82, Loss 0.600359320640564\n",
      "[Training Epoch 0] Batch 83, Loss 0.5928846597671509\n",
      "[Training Epoch 0] Batch 84, Loss 0.5880783200263977\n",
      "[Training Epoch 0] Batch 85, Loss 0.6020807027816772\n",
      "[Training Epoch 0] Batch 86, Loss 0.5808814764022827\n",
      "[Training Epoch 0] Batch 87, Loss 0.5715855360031128\n",
      "[Training Epoch 0] Batch 88, Loss 0.5758783221244812\n",
      "[Training Epoch 0] Batch 89, Loss 0.5670948624610901\n",
      "[Training Epoch 0] Batch 90, Loss 0.565187931060791\n",
      "[Training Epoch 0] Batch 91, Loss 0.5678890347480774\n",
      "[Training Epoch 0] Batch 92, Loss 0.5712727308273315\n",
      "[Training Epoch 0] Batch 93, Loss 0.5633835792541504\n",
      "[Training Epoch 0] Batch 94, Loss 0.5597878694534302\n",
      "[Training Epoch 0] Batch 95, Loss 0.5387358069419861\n",
      "[Training Epoch 0] Batch 96, Loss 0.5567739009857178\n",
      "[Training Epoch 0] Batch 97, Loss 0.5356452465057373\n",
      "[Training Epoch 0] Batch 98, Loss 0.5434064865112305\n",
      "[Training Epoch 0] Batch 99, Loss 0.5226225852966309\n",
      "[Training Epoch 0] Batch 100, Loss 0.5228375196456909\n",
      "[Training Epoch 0] Batch 101, Loss 0.5108523368835449\n",
      "[Training Epoch 0] Batch 102, Loss 0.5347068309783936\n",
      "[Training Epoch 0] Batch 103, Loss 0.504830539226532\n",
      "[Training Epoch 0] Batch 104, Loss 0.49947741627693176\n",
      "[Training Epoch 0] Batch 105, Loss 0.5110355615615845\n",
      "[Training Epoch 0] Batch 106, Loss 0.48813989758491516\n",
      "[Training Epoch 0] Batch 107, Loss 0.4951755106449127\n",
      "[Training Epoch 0] Batch 108, Loss 0.47249993681907654\n",
      "[Training Epoch 0] Batch 109, Loss 0.46283891797065735\n",
      "[Training Epoch 0] Batch 110, Loss 0.49536705017089844\n",
      "[Training Epoch 0] Batch 111, Loss 0.48891329765319824\n",
      "[Training Epoch 0] Batch 112, Loss 0.4505859315395355\n",
      "[Training Epoch 0] Batch 113, Loss 0.5047544240951538\n",
      "[Training Epoch 0] Batch 114, Loss 0.47904127836227417\n",
      "[Training Epoch 0] Batch 115, Loss 0.49503618478775024\n",
      "[Training Epoch 0] Batch 116, Loss 0.48502397537231445\n",
      "[Training Epoch 0] Batch 117, Loss 0.5011794567108154\n",
      "[Training Epoch 0] Batch 118, Loss 0.4863862991333008\n",
      "[Training Epoch 0] Batch 119, Loss 0.4757862091064453\n",
      "[Training Epoch 0] Batch 120, Loss 0.4954632520675659\n",
      "[Training Epoch 0] Batch 121, Loss 0.4838384985923767\n",
      "[Training Epoch 0] Batch 122, Loss 0.4917958378791809\n",
      "[Training Epoch 0] Batch 123, Loss 0.48125675320625305\n",
      "[Training Epoch 0] Batch 124, Loss 0.48062849044799805\n",
      "[Training Epoch 0] Batch 125, Loss 0.4969871938228607\n",
      "[Training Epoch 0] Batch 126, Loss 0.5043689012527466\n",
      "[Training Epoch 0] Batch 127, Loss 0.4879066050052643\n",
      "[Training Epoch 0] Batch 128, Loss 0.5017937421798706\n",
      "[Training Epoch 0] Batch 129, Loss 0.48523709177970886\n",
      "[Training Epoch 0] Batch 130, Loss 0.49892184138298035\n",
      "[Training Epoch 0] Batch 131, Loss 0.5180706977844238\n",
      "[Training Epoch 0] Batch 132, Loss 0.4446328282356262\n",
      "[Training Epoch 0] Batch 133, Loss 0.4917590022087097\n",
      "[Training Epoch 0] Batch 134, Loss 0.4982281029224396\n",
      "[Training Epoch 0] Batch 135, Loss 0.49272972345352173\n",
      "[Training Epoch 0] Batch 136, Loss 0.4874778091907501\n",
      "[Training Epoch 0] Batch 137, Loss 0.48321977257728577\n",
      "[Training Epoch 0] Batch 138, Loss 0.502020537853241\n",
      "[Training Epoch 0] Batch 139, Loss 0.4860404431819916\n",
      "[Training Epoch 0] Batch 140, Loss 0.47083956003189087\n",
      "[Training Epoch 0] Batch 141, Loss 0.47798800468444824\n",
      "[Training Epoch 0] Batch 142, Loss 0.48774653673171997\n",
      "[Training Epoch 0] Batch 143, Loss 0.4654316306114197\n",
      "[Training Epoch 0] Batch 144, Loss 0.4792432188987732\n",
      "[Training Epoch 0] Batch 145, Loss 0.4682351350784302\n",
      "[Training Epoch 0] Batch 146, Loss 0.4794107675552368\n",
      "[Training Epoch 0] Batch 147, Loss 0.4634852111339569\n",
      "[Training Epoch 0] Batch 148, Loss 0.45975521206855774\n",
      "[Training Epoch 0] Batch 149, Loss 0.47645217180252075\n",
      "[Training Epoch 0] Batch 150, Loss 0.4766422510147095\n",
      "[Training Epoch 0] Batch 151, Loss 0.4824991226196289\n",
      "[Training Epoch 0] Batch 152, Loss 0.47327131032943726\n",
      "[Training Epoch 0] Batch 153, Loss 0.46320217847824097\n",
      "[Training Epoch 0] Batch 154, Loss 0.49268439412117004\n",
      "[Training Epoch 0] Batch 155, Loss 0.4632716774940491\n",
      "[Training Epoch 0] Batch 156, Loss 0.45995843410491943\n",
      "[Training Epoch 0] Batch 157, Loss 0.5067176222801208\n",
      "[Training Epoch 0] Batch 158, Loss 0.48977160453796387\n",
      "[Training Epoch 0] Batch 159, Loss 0.46243688464164734\n",
      "[Training Epoch 0] Batch 160, Loss 0.4548594355583191\n",
      "[Training Epoch 0] Batch 161, Loss 0.47518569231033325\n",
      "[Training Epoch 0] Batch 162, Loss 0.44665634632110596\n",
      "[Training Epoch 0] Batch 163, Loss 0.4903275668621063\n",
      "[Training Epoch 0] Batch 164, Loss 0.483770489692688\n",
      "[Training Epoch 0] Batch 165, Loss 0.48957470059394836\n",
      "[Training Epoch 0] Batch 166, Loss 0.45190197229385376\n",
      "[Training Epoch 0] Batch 167, Loss 0.44177961349487305\n",
      "[Training Epoch 0] Batch 168, Loss 0.47903239727020264\n",
      "[Training Epoch 0] Batch 169, Loss 0.4521844685077667\n",
      "[Training Epoch 0] Batch 170, Loss 0.4470151364803314\n",
      "[Training Epoch 0] Batch 171, Loss 0.4682484269142151\n",
      "[Training Epoch 0] Batch 172, Loss 0.43908315896987915\n",
      "[Training Epoch 0] Batch 173, Loss 0.4652317762374878\n",
      "[Training Epoch 0] Batch 174, Loss 0.4687938988208771\n",
      "[Training Epoch 0] Batch 175, Loss 0.4650396704673767\n",
      "[Training Epoch 0] Batch 176, Loss 0.4646286368370056\n",
      "[Training Epoch 0] Batch 177, Loss 0.4402942955493927\n",
      "[Training Epoch 0] Batch 178, Loss 0.4884673058986664\n",
      "[Training Epoch 0] Batch 179, Loss 0.47667115926742554\n",
      "[Training Epoch 0] Batch 180, Loss 0.5034996867179871\n",
      "[Training Epoch 0] Batch 181, Loss 0.48092252016067505\n",
      "[Training Epoch 0] Batch 182, Loss 0.4727676212787628\n",
      "[Training Epoch 0] Batch 183, Loss 0.45763304829597473\n",
      "[Training Epoch 0] Batch 184, Loss 0.4838753938674927\n",
      "[Training Epoch 0] Batch 185, Loss 0.4780980348587036\n",
      "[Training Epoch 0] Batch 186, Loss 0.4833030104637146\n",
      "[Training Epoch 0] Batch 187, Loss 0.4370303750038147\n",
      "[Training Epoch 0] Batch 188, Loss 0.464300274848938\n",
      "[Training Epoch 0] Batch 189, Loss 0.46797797083854675\n",
      "[Training Epoch 0] Batch 190, Loss 0.469431608915329\n",
      "[Training Epoch 0] Batch 191, Loss 0.4622543454170227\n",
      "[Training Epoch 0] Batch 192, Loss 0.4577016234397888\n",
      "[Training Epoch 0] Batch 193, Loss 0.4795204699039459\n",
      "[Training Epoch 0] Batch 194, Loss 0.4949629604816437\n",
      "[Training Epoch 0] Batch 195, Loss 0.4673733711242676\n",
      "[Training Epoch 0] Batch 196, Loss 0.4638919532299042\n",
      "[Training Epoch 0] Batch 197, Loss 0.44020944833755493\n",
      "[Training Epoch 0] Batch 198, Loss 0.49562835693359375\n",
      "[Training Epoch 0] Batch 199, Loss 0.47688862681388855\n",
      "[Training Epoch 0] Batch 200, Loss 0.46639367938041687\n",
      "[Training Epoch 0] Batch 201, Loss 0.5048527717590332\n",
      "[Training Epoch 0] Batch 202, Loss 0.46808212995529175\n",
      "[Training Epoch 0] Batch 203, Loss 0.44405514001846313\n",
      "[Training Epoch 0] Batch 204, Loss 0.4605047106742859\n",
      "[Training Epoch 0] Batch 205, Loss 0.44701772928237915\n",
      "[Training Epoch 0] Batch 206, Loss 0.48138242959976196\n",
      "[Training Epoch 0] Batch 207, Loss 0.45423272252082825\n",
      "[Training Epoch 0] Batch 208, Loss 0.4801023602485657\n",
      "[Training Epoch 0] Batch 209, Loss 0.4740596413612366\n",
      "[Training Epoch 0] Batch 210, Loss 0.4634941816329956\n",
      "[Training Epoch 0] Batch 211, Loss 0.46575772762298584\n",
      "[Training Epoch 0] Batch 212, Loss 0.48597025871276855\n",
      "[Training Epoch 0] Batch 213, Loss 0.4770604372024536\n",
      "[Training Epoch 0] Batch 214, Loss 0.49409395456314087\n",
      "[Training Epoch 0] Batch 215, Loss 0.45484650135040283\n",
      "[Training Epoch 0] Batch 216, Loss 0.48474785685539246\n",
      "[Training Epoch 0] Batch 217, Loss 0.44127845764160156\n",
      "[Training Epoch 0] Batch 218, Loss 0.4671604037284851\n",
      "[Training Epoch 0] Batch 219, Loss 0.49377939105033875\n",
      "[Training Epoch 0] Batch 220, Loss 0.43455880880355835\n",
      "[Training Epoch 0] Batch 221, Loss 0.4906804859638214\n",
      "[Training Epoch 0] Batch 222, Loss 0.4471740126609802\n",
      "[Training Epoch 0] Batch 223, Loss 0.4652765989303589\n",
      "[Training Epoch 0] Batch 224, Loss 0.43364623188972473\n",
      "[Training Epoch 0] Batch 225, Loss 0.427885502576828\n",
      "[Training Epoch 0] Batch 226, Loss 0.4536551833152771\n",
      "[Training Epoch 0] Batch 227, Loss 0.4442388415336609\n",
      "[Training Epoch 0] Batch 228, Loss 0.5004006624221802\n",
      "[Training Epoch 0] Batch 229, Loss 0.46850013732910156\n",
      "[Training Epoch 0] Batch 230, Loss 0.4153163433074951\n",
      "[Training Epoch 0] Batch 231, Loss 0.46784117817878723\n",
      "[Training Epoch 0] Batch 232, Loss 0.4613867402076721\n",
      "[Training Epoch 0] Batch 233, Loss 0.4414217472076416\n",
      "[Training Epoch 0] Batch 234, Loss 0.4562826156616211\n",
      "[Training Epoch 0] Batch 235, Loss 0.43145352602005005\n",
      "[Training Epoch 0] Batch 236, Loss 0.4487013816833496\n",
      "[Training Epoch 0] Batch 237, Loss 0.4208017587661743\n",
      "[Training Epoch 0] Batch 238, Loss 0.4391545355319977\n",
      "[Training Epoch 0] Batch 239, Loss 0.4281533658504486\n",
      "[Training Epoch 0] Batch 240, Loss 0.45483389496803284\n",
      "[Training Epoch 0] Batch 241, Loss 0.499454528093338\n",
      "[Training Epoch 0] Batch 242, Loss 0.4604944586753845\n",
      "[Training Epoch 0] Batch 243, Loss 0.4765568673610687\n",
      "[Training Epoch 0] Batch 244, Loss 0.4726127088069916\n",
      "[Training Epoch 0] Batch 245, Loss 0.41715604066848755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2066.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2237.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 0] Precision = 0.3617, Recall = 0.9837\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.4970695972442627\n",
      "[Training Epoch 1] Batch 1, Loss 0.48441773653030396\n",
      "[Training Epoch 1] Batch 2, Loss 0.4508233964443207\n",
      "[Training Epoch 1] Batch 3, Loss 0.4651326835155487\n",
      "[Training Epoch 1] Batch 4, Loss 0.44454848766326904\n",
      "[Training Epoch 1] Batch 5, Loss 0.4608386158943176\n",
      "[Training Epoch 1] Batch 6, Loss 0.466101735830307\n",
      "[Training Epoch 1] Batch 7, Loss 0.43562230467796326\n",
      "[Training Epoch 1] Batch 8, Loss 0.5028026700019836\n",
      "[Training Epoch 1] Batch 9, Loss 0.4771023392677307\n",
      "[Training Epoch 1] Batch 10, Loss 0.4553016424179077\n",
      "[Training Epoch 1] Batch 11, Loss 0.43584930896759033\n",
      "[Training Epoch 1] Batch 12, Loss 0.436941921710968\n",
      "[Training Epoch 1] Batch 13, Loss 0.43030601739883423\n",
      "[Training Epoch 1] Batch 14, Loss 0.43099546432495117\n",
      "[Training Epoch 1] Batch 15, Loss 0.48936814069747925\n",
      "[Training Epoch 1] Batch 16, Loss 0.4632083773612976\n",
      "[Training Epoch 1] Batch 17, Loss 0.4921671748161316\n",
      "[Training Epoch 1] Batch 18, Loss 0.4892731010913849\n",
      "[Training Epoch 1] Batch 19, Loss 0.4632752537727356\n",
      "[Training Epoch 1] Batch 20, Loss 0.48502305150032043\n",
      "[Training Epoch 1] Batch 21, Loss 0.4514680802822113\n",
      "[Training Epoch 1] Batch 22, Loss 0.4460168480873108\n",
      "[Training Epoch 1] Batch 23, Loss 0.49200505018234253\n",
      "[Training Epoch 1] Batch 24, Loss 0.4684305489063263\n",
      "[Training Epoch 1] Batch 25, Loss 0.4802999496459961\n",
      "[Training Epoch 1] Batch 26, Loss 0.4712153673171997\n",
      "[Training Epoch 1] Batch 27, Loss 0.4306528568267822\n",
      "[Training Epoch 1] Batch 28, Loss 0.43506574630737305\n",
      "[Training Epoch 1] Batch 29, Loss 0.44685113430023193\n",
      "[Training Epoch 1] Batch 30, Loss 0.4438343644142151\n",
      "[Training Epoch 1] Batch 31, Loss 0.4636409282684326\n",
      "[Training Epoch 1] Batch 32, Loss 0.47036078572273254\n",
      "[Training Epoch 1] Batch 33, Loss 0.4559440016746521\n",
      "[Training Epoch 1] Batch 34, Loss 0.4595305621623993\n",
      "[Training Epoch 1] Batch 35, Loss 0.48242074251174927\n",
      "[Training Epoch 1] Batch 36, Loss 0.4655952453613281\n",
      "[Training Epoch 1] Batch 37, Loss 0.46924686431884766\n",
      "[Training Epoch 1] Batch 38, Loss 0.4682171940803528\n",
      "[Training Epoch 1] Batch 39, Loss 0.4504334032535553\n",
      "[Training Epoch 1] Batch 40, Loss 0.4449602961540222\n",
      "[Training Epoch 1] Batch 41, Loss 0.4232616424560547\n",
      "[Training Epoch 1] Batch 42, Loss 0.4455569386482239\n",
      "[Training Epoch 1] Batch 43, Loss 0.40397030115127563\n",
      "[Training Epoch 1] Batch 44, Loss 0.4702855944633484\n",
      "[Training Epoch 1] Batch 45, Loss 0.43877583742141724\n",
      "[Training Epoch 1] Batch 46, Loss 0.4757826030254364\n",
      "[Training Epoch 1] Batch 47, Loss 0.45904430747032166\n",
      "[Training Epoch 1] Batch 48, Loss 0.46535176038742065\n",
      "[Training Epoch 1] Batch 49, Loss 0.4669913649559021\n",
      "[Training Epoch 1] Batch 50, Loss 0.44523268938064575\n",
      "[Training Epoch 1] Batch 51, Loss 0.4659650921821594\n",
      "[Training Epoch 1] Batch 52, Loss 0.44435596466064453\n",
      "[Training Epoch 1] Batch 53, Loss 0.4543094038963318\n",
      "[Training Epoch 1] Batch 54, Loss 0.423839271068573\n",
      "[Training Epoch 1] Batch 55, Loss 0.4603998363018036\n",
      "[Training Epoch 1] Batch 56, Loss 0.46982258558273315\n",
      "[Training Epoch 1] Batch 57, Loss 0.4643287658691406\n",
      "[Training Epoch 1] Batch 58, Loss 0.48404887318611145\n",
      "[Training Epoch 1] Batch 59, Loss 0.45384278893470764\n",
      "[Training Epoch 1] Batch 60, Loss 0.4487612843513489\n",
      "[Training Epoch 1] Batch 61, Loss 0.4581507444381714\n",
      "[Training Epoch 1] Batch 62, Loss 0.4522801339626312\n",
      "[Training Epoch 1] Batch 63, Loss 0.4593692719936371\n",
      "[Training Epoch 1] Batch 64, Loss 0.4687783718109131\n",
      "[Training Epoch 1] Batch 65, Loss 0.45520052313804626\n",
      "[Training Epoch 1] Batch 66, Loss 0.4339151382446289\n",
      "[Training Epoch 1] Batch 67, Loss 0.47365379333496094\n",
      "[Training Epoch 1] Batch 68, Loss 0.4650743007659912\n",
      "[Training Epoch 1] Batch 69, Loss 0.4553523659706116\n",
      "[Training Epoch 1] Batch 70, Loss 0.4446578621864319\n",
      "[Training Epoch 1] Batch 71, Loss 0.4524219036102295\n",
      "[Training Epoch 1] Batch 72, Loss 0.4433230757713318\n",
      "[Training Epoch 1] Batch 73, Loss 0.4681110978126526\n",
      "[Training Epoch 1] Batch 74, Loss 0.5267684459686279\n",
      "[Training Epoch 1] Batch 75, Loss 0.4543241858482361\n",
      "[Training Epoch 1] Batch 76, Loss 0.46096521615982056\n",
      "[Training Epoch 1] Batch 77, Loss 0.4682125747203827\n",
      "[Training Epoch 1] Batch 78, Loss 0.44929778575897217\n",
      "[Training Epoch 1] Batch 79, Loss 0.4558142125606537\n",
      "[Training Epoch 1] Batch 80, Loss 0.45518332719802856\n",
      "[Training Epoch 1] Batch 81, Loss 0.4569961428642273\n",
      "[Training Epoch 1] Batch 82, Loss 0.47229230403900146\n",
      "[Training Epoch 1] Batch 83, Loss 0.45634084939956665\n",
      "[Training Epoch 1] Batch 84, Loss 0.4862307906150818\n",
      "[Training Epoch 1] Batch 85, Loss 0.44178271293640137\n",
      "[Training Epoch 1] Batch 86, Loss 0.44799894094467163\n",
      "[Training Epoch 1] Batch 87, Loss 0.4468761682510376\n",
      "[Training Epoch 1] Batch 88, Loss 0.4586717486381531\n",
      "[Training Epoch 1] Batch 89, Loss 0.46312177181243896\n",
      "[Training Epoch 1] Batch 90, Loss 0.4328349232673645\n",
      "[Training Epoch 1] Batch 91, Loss 0.48906415700912476\n",
      "[Training Epoch 1] Batch 92, Loss 0.45030272006988525\n",
      "[Training Epoch 1] Batch 93, Loss 0.4667539596557617\n",
      "[Training Epoch 1] Batch 94, Loss 0.44477057456970215\n",
      "[Training Epoch 1] Batch 95, Loss 0.4776384234428406\n",
      "[Training Epoch 1] Batch 96, Loss 0.4841318726539612\n",
      "[Training Epoch 1] Batch 97, Loss 0.43329665064811707\n",
      "[Training Epoch 1] Batch 98, Loss 0.4541473984718323\n",
      "[Training Epoch 1] Batch 99, Loss 0.4114447236061096\n",
      "[Training Epoch 1] Batch 100, Loss 0.4312880039215088\n",
      "[Training Epoch 1] Batch 101, Loss 0.48844629526138306\n",
      "[Training Epoch 1] Batch 102, Loss 0.4519304037094116\n",
      "[Training Epoch 1] Batch 103, Loss 0.46546366810798645\n",
      "[Training Epoch 1] Batch 104, Loss 0.47167134284973145\n",
      "[Training Epoch 1] Batch 105, Loss 0.4503280520439148\n",
      "[Training Epoch 1] Batch 106, Loss 0.4317401647567749\n",
      "[Training Epoch 1] Batch 107, Loss 0.4568125605583191\n",
      "[Training Epoch 1] Batch 108, Loss 0.41814160346984863\n",
      "[Training Epoch 1] Batch 109, Loss 0.4731970727443695\n",
      "[Training Epoch 1] Batch 110, Loss 0.44088488817214966\n",
      "[Training Epoch 1] Batch 111, Loss 0.46963930130004883\n",
      "[Training Epoch 1] Batch 112, Loss 0.44515901803970337\n",
      "[Training Epoch 1] Batch 113, Loss 0.46799367666244507\n",
      "[Training Epoch 1] Batch 114, Loss 0.4414243996143341\n",
      "[Training Epoch 1] Batch 115, Loss 0.45466935634613037\n",
      "[Training Epoch 1] Batch 116, Loss 0.4519802927970886\n",
      "[Training Epoch 1] Batch 117, Loss 0.43541020154953003\n",
      "[Training Epoch 1] Batch 118, Loss 0.4492681920528412\n",
      "[Training Epoch 1] Batch 119, Loss 0.4532073140144348\n",
      "[Training Epoch 1] Batch 120, Loss 0.4625879228115082\n",
      "[Training Epoch 1] Batch 121, Loss 0.47105351090431213\n",
      "[Training Epoch 1] Batch 122, Loss 0.47227486968040466\n",
      "[Training Epoch 1] Batch 123, Loss 0.46364647150039673\n",
      "[Training Epoch 1] Batch 124, Loss 0.4658030569553375\n",
      "[Training Epoch 1] Batch 125, Loss 0.4730773866176605\n",
      "[Training Epoch 1] Batch 126, Loss 0.4230162799358368\n",
      "[Training Epoch 1] Batch 127, Loss 0.45150619745254517\n",
      "[Training Epoch 1] Batch 128, Loss 0.451585590839386\n",
      "[Training Epoch 1] Batch 129, Loss 0.4668370485305786\n",
      "[Training Epoch 1] Batch 130, Loss 0.46205151081085205\n",
      "[Training Epoch 1] Batch 131, Loss 0.42636793851852417\n",
      "[Training Epoch 1] Batch 132, Loss 0.45120081305503845\n",
      "[Training Epoch 1] Batch 133, Loss 0.45239073038101196\n",
      "[Training Epoch 1] Batch 134, Loss 0.4448060691356659\n",
      "[Training Epoch 1] Batch 135, Loss 0.4452770948410034\n",
      "[Training Epoch 1] Batch 136, Loss 0.4688986837863922\n",
      "[Training Epoch 1] Batch 137, Loss 0.4591275453567505\n",
      "[Training Epoch 1] Batch 138, Loss 0.4670635461807251\n",
      "[Training Epoch 1] Batch 139, Loss 0.4391743540763855\n",
      "[Training Epoch 1] Batch 140, Loss 0.4707752466201782\n",
      "[Training Epoch 1] Batch 141, Loss 0.4380530118942261\n",
      "[Training Epoch 1] Batch 142, Loss 0.46716028451919556\n",
      "[Training Epoch 1] Batch 143, Loss 0.4531710743904114\n",
      "[Training Epoch 1] Batch 144, Loss 0.45274052023887634\n",
      "[Training Epoch 1] Batch 145, Loss 0.47836607694625854\n",
      "[Training Epoch 1] Batch 146, Loss 0.4605959951877594\n",
      "[Training Epoch 1] Batch 147, Loss 0.4490174651145935\n",
      "[Training Epoch 1] Batch 148, Loss 0.4544984996318817\n",
      "[Training Epoch 1] Batch 149, Loss 0.443013995885849\n",
      "[Training Epoch 1] Batch 150, Loss 0.4280141592025757\n",
      "[Training Epoch 1] Batch 151, Loss 0.46480435132980347\n",
      "[Training Epoch 1] Batch 152, Loss 0.4725724160671234\n",
      "[Training Epoch 1] Batch 153, Loss 0.4771936535835266\n",
      "[Training Epoch 1] Batch 154, Loss 0.4306114614009857\n",
      "[Training Epoch 1] Batch 155, Loss 0.4609784483909607\n",
      "[Training Epoch 1] Batch 156, Loss 0.41402673721313477\n",
      "[Training Epoch 1] Batch 157, Loss 0.4572167992591858\n",
      "[Training Epoch 1] Batch 158, Loss 0.43682393431663513\n",
      "[Training Epoch 1] Batch 159, Loss 0.46927011013031006\n",
      "[Training Epoch 1] Batch 160, Loss 0.4322766065597534\n",
      "[Training Epoch 1] Batch 161, Loss 0.45711275935173035\n",
      "[Training Epoch 1] Batch 162, Loss 0.4374270439147949\n",
      "[Training Epoch 1] Batch 163, Loss 0.45443183183670044\n",
      "[Training Epoch 1] Batch 164, Loss 0.44278573989868164\n",
      "[Training Epoch 1] Batch 165, Loss 0.4308416247367859\n",
      "[Training Epoch 1] Batch 166, Loss 0.43063968420028687\n",
      "[Training Epoch 1] Batch 167, Loss 0.46667006611824036\n",
      "[Training Epoch 1] Batch 168, Loss 0.42675358057022095\n",
      "[Training Epoch 1] Batch 169, Loss 0.4559299945831299\n",
      "[Training Epoch 1] Batch 170, Loss 0.3969563841819763\n",
      "[Training Epoch 1] Batch 171, Loss 0.44168829917907715\n",
      "[Training Epoch 1] Batch 172, Loss 0.44460493326187134\n",
      "[Training Epoch 1] Batch 173, Loss 0.45589834451675415\n",
      "[Training Epoch 1] Batch 174, Loss 0.44973015785217285\n",
      "[Training Epoch 1] Batch 175, Loss 0.45735764503479004\n",
      "[Training Epoch 1] Batch 176, Loss 0.4820355772972107\n",
      "[Training Epoch 1] Batch 177, Loss 0.4365774393081665\n",
      "[Training Epoch 1] Batch 178, Loss 0.43238821625709534\n",
      "[Training Epoch 1] Batch 179, Loss 0.4082840085029602\n",
      "[Training Epoch 1] Batch 180, Loss 0.46378013491630554\n",
      "[Training Epoch 1] Batch 181, Loss 0.43738657236099243\n",
      "[Training Epoch 1] Batch 182, Loss 0.46525734663009644\n",
      "[Training Epoch 1] Batch 183, Loss 0.45821312069892883\n",
      "[Training Epoch 1] Batch 184, Loss 0.4352530837059021\n",
      "[Training Epoch 1] Batch 185, Loss 0.43726804852485657\n",
      "[Training Epoch 1] Batch 186, Loss 0.4609410762786865\n",
      "[Training Epoch 1] Batch 187, Loss 0.48060843348503113\n",
      "[Training Epoch 1] Batch 188, Loss 0.4533379375934601\n",
      "[Training Epoch 1] Batch 189, Loss 0.4636160433292389\n",
      "[Training Epoch 1] Batch 190, Loss 0.43445488810539246\n",
      "[Training Epoch 1] Batch 191, Loss 0.45004451274871826\n",
      "[Training Epoch 1] Batch 192, Loss 0.4549083113670349\n",
      "[Training Epoch 1] Batch 193, Loss 0.45854508876800537\n",
      "[Training Epoch 1] Batch 194, Loss 0.44246906042099\n",
      "[Training Epoch 1] Batch 195, Loss 0.4283284544944763\n",
      "[Training Epoch 1] Batch 196, Loss 0.46338266134262085\n",
      "[Training Epoch 1] Batch 197, Loss 0.45709332823753357\n",
      "[Training Epoch 1] Batch 198, Loss 0.4087642729282379\n",
      "[Training Epoch 1] Batch 199, Loss 0.45278626680374146\n",
      "[Training Epoch 1] Batch 200, Loss 0.445490300655365\n",
      "[Training Epoch 1] Batch 201, Loss 0.45149481296539307\n",
      "[Training Epoch 1] Batch 202, Loss 0.4204390048980713\n",
      "[Training Epoch 1] Batch 203, Loss 0.43330302834510803\n",
      "[Training Epoch 1] Batch 204, Loss 0.4595949947834015\n",
      "[Training Epoch 1] Batch 205, Loss 0.4691869914531708\n",
      "[Training Epoch 1] Batch 206, Loss 0.46253204345703125\n",
      "[Training Epoch 1] Batch 207, Loss 0.4522862434387207\n",
      "[Training Epoch 1] Batch 208, Loss 0.45005762577056885\n",
      "[Training Epoch 1] Batch 209, Loss 0.4165472388267517\n",
      "[Training Epoch 1] Batch 210, Loss 0.44429993629455566\n",
      "[Training Epoch 1] Batch 211, Loss 0.4467856287956238\n",
      "[Training Epoch 1] Batch 212, Loss 0.4565032720565796\n",
      "[Training Epoch 1] Batch 213, Loss 0.4728849530220032\n",
      "[Training Epoch 1] Batch 214, Loss 0.4395977854728699\n",
      "[Training Epoch 1] Batch 215, Loss 0.438752144575119\n",
      "[Training Epoch 1] Batch 216, Loss 0.45359742641448975\n",
      "[Training Epoch 1] Batch 217, Loss 0.4582095146179199\n",
      "[Training Epoch 1] Batch 218, Loss 0.45603078603744507\n",
      "[Training Epoch 1] Batch 219, Loss 0.43723827600479126\n",
      "[Training Epoch 1] Batch 220, Loss 0.4407769739627838\n",
      "[Training Epoch 1] Batch 221, Loss 0.461157888174057\n",
      "[Training Epoch 1] Batch 222, Loss 0.4232364594936371\n",
      "[Training Epoch 1] Batch 223, Loss 0.4451274275779724\n",
      "[Training Epoch 1] Batch 224, Loss 0.47863051295280457\n",
      "[Training Epoch 1] Batch 225, Loss 0.4455850422382355\n",
      "[Training Epoch 1] Batch 226, Loss 0.45423632860183716\n",
      "[Training Epoch 1] Batch 227, Loss 0.43900489807128906\n",
      "[Training Epoch 1] Batch 228, Loss 0.45572781562805176\n",
      "[Training Epoch 1] Batch 229, Loss 0.4576725363731384\n",
      "[Training Epoch 1] Batch 230, Loss 0.4585287570953369\n",
      "[Training Epoch 1] Batch 231, Loss 0.4547007083892822\n",
      "[Training Epoch 1] Batch 232, Loss 0.43793466687202454\n",
      "[Training Epoch 1] Batch 233, Loss 0.44471681118011475\n",
      "[Training Epoch 1] Batch 234, Loss 0.4445423483848572\n",
      "[Training Epoch 1] Batch 235, Loss 0.4273303151130676\n",
      "[Training Epoch 1] Batch 236, Loss 0.44349876046180725\n",
      "[Training Epoch 1] Batch 237, Loss 0.47330135107040405\n",
      "[Training Epoch 1] Batch 238, Loss 0.44963544607162476\n",
      "[Training Epoch 1] Batch 239, Loss 0.47091639041900635\n",
      "[Training Epoch 1] Batch 240, Loss 0.45888739824295044\n",
      "[Training Epoch 1] Batch 241, Loss 0.43917959928512573\n",
      "[Training Epoch 1] Batch 242, Loss 0.45946866273880005\n",
      "[Training Epoch 1] Batch 243, Loss 0.49055349826812744\n",
      "[Training Epoch 1] Batch 244, Loss 0.44233113527297974\n",
      "[Training Epoch 1] Batch 245, Loss 0.4626704752445221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2069.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2380.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 1] Precision = 0.3615, Recall = 0.9833\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.4517146348953247\n",
      "[Training Epoch 2] Batch 1, Loss 0.4265211820602417\n",
      "[Training Epoch 2] Batch 2, Loss 0.48203080892562866\n",
      "[Training Epoch 2] Batch 3, Loss 0.46337759494781494\n",
      "[Training Epoch 2] Batch 4, Loss 0.4483823776245117\n",
      "[Training Epoch 2] Batch 5, Loss 0.44265466928482056\n",
      "[Training Epoch 2] Batch 6, Loss 0.4679260551929474\n",
      "[Training Epoch 2] Batch 7, Loss 0.4344148337841034\n",
      "[Training Epoch 2] Batch 8, Loss 0.41026103496551514\n",
      "[Training Epoch 2] Batch 9, Loss 0.4399736523628235\n",
      "[Training Epoch 2] Batch 10, Loss 0.44939401745796204\n",
      "[Training Epoch 2] Batch 11, Loss 0.45837461948394775\n",
      "[Training Epoch 2] Batch 12, Loss 0.463325560092926\n",
      "[Training Epoch 2] Batch 13, Loss 0.45059284567832947\n",
      "[Training Epoch 2] Batch 14, Loss 0.43469974398612976\n",
      "[Training Epoch 2] Batch 15, Loss 0.4636768698692322\n",
      "[Training Epoch 2] Batch 16, Loss 0.46079689264297485\n",
      "[Training Epoch 2] Batch 17, Loss 0.46058908104896545\n",
      "[Training Epoch 2] Batch 18, Loss 0.43697285652160645\n",
      "[Training Epoch 2] Batch 19, Loss 0.4468209147453308\n",
      "[Training Epoch 2] Batch 20, Loss 0.4365260601043701\n",
      "[Training Epoch 2] Batch 21, Loss 0.44846951961517334\n",
      "[Training Epoch 2] Batch 22, Loss 0.42484164237976074\n",
      "[Training Epoch 2] Batch 23, Loss 0.4433645009994507\n",
      "[Training Epoch 2] Batch 24, Loss 0.43357864022254944\n",
      "[Training Epoch 2] Batch 25, Loss 0.4550902247428894\n",
      "[Training Epoch 2] Batch 26, Loss 0.46572011709213257\n",
      "[Training Epoch 2] Batch 27, Loss 0.42521342635154724\n",
      "[Training Epoch 2] Batch 28, Loss 0.46561455726623535\n",
      "[Training Epoch 2] Batch 29, Loss 0.4773164391517639\n",
      "[Training Epoch 2] Batch 30, Loss 0.4636549949645996\n",
      "[Training Epoch 2] Batch 31, Loss 0.42288005352020264\n",
      "[Training Epoch 2] Batch 32, Loss 0.42474305629730225\n",
      "[Training Epoch 2] Batch 33, Loss 0.4427958130836487\n",
      "[Training Epoch 2] Batch 34, Loss 0.44453680515289307\n",
      "[Training Epoch 2] Batch 35, Loss 0.44898372888565063\n",
      "[Training Epoch 2] Batch 36, Loss 0.4277706444263458\n",
      "[Training Epoch 2] Batch 37, Loss 0.44309085607528687\n",
      "[Training Epoch 2] Batch 38, Loss 0.4424639940261841\n",
      "[Training Epoch 2] Batch 39, Loss 0.43389561772346497\n",
      "[Training Epoch 2] Batch 40, Loss 0.42887359857559204\n",
      "[Training Epoch 2] Batch 41, Loss 0.43744057416915894\n",
      "[Training Epoch 2] Batch 42, Loss 0.4710700511932373\n",
      "[Training Epoch 2] Batch 43, Loss 0.4350373148918152\n",
      "[Training Epoch 2] Batch 44, Loss 0.4380703270435333\n",
      "[Training Epoch 2] Batch 45, Loss 0.4665646255016327\n",
      "[Training Epoch 2] Batch 46, Loss 0.4697991907596588\n",
      "[Training Epoch 2] Batch 47, Loss 0.42624402046203613\n",
      "[Training Epoch 2] Batch 48, Loss 0.43062642216682434\n",
      "[Training Epoch 2] Batch 49, Loss 0.42276859283447266\n",
      "[Training Epoch 2] Batch 50, Loss 0.4175836443901062\n",
      "[Training Epoch 2] Batch 51, Loss 0.46835294365882874\n",
      "[Training Epoch 2] Batch 52, Loss 0.44786137342453003\n",
      "[Training Epoch 2] Batch 53, Loss 0.4411567151546478\n",
      "[Training Epoch 2] Batch 54, Loss 0.4535805284976959\n",
      "[Training Epoch 2] Batch 55, Loss 0.42449936270713806\n",
      "[Training Epoch 2] Batch 56, Loss 0.4299456775188446\n",
      "[Training Epoch 2] Batch 57, Loss 0.41604769229888916\n",
      "[Training Epoch 2] Batch 58, Loss 0.44351625442504883\n",
      "[Training Epoch 2] Batch 59, Loss 0.43314677476882935\n",
      "[Training Epoch 2] Batch 60, Loss 0.4445781707763672\n",
      "[Training Epoch 2] Batch 61, Loss 0.4410555362701416\n",
      "[Training Epoch 2] Batch 62, Loss 0.43273866176605225\n",
      "[Training Epoch 2] Batch 63, Loss 0.4316618740558624\n",
      "[Training Epoch 2] Batch 64, Loss 0.42771002650260925\n",
      "[Training Epoch 2] Batch 65, Loss 0.4531329572200775\n",
      "[Training Epoch 2] Batch 66, Loss 0.44429999589920044\n",
      "[Training Epoch 2] Batch 67, Loss 0.4510640501976013\n",
      "[Training Epoch 2] Batch 68, Loss 0.43904295563697815\n",
      "[Training Epoch 2] Batch 69, Loss 0.4727834463119507\n",
      "[Training Epoch 2] Batch 70, Loss 0.42876681685447693\n",
      "[Training Epoch 2] Batch 71, Loss 0.44013041257858276\n",
      "[Training Epoch 2] Batch 72, Loss 0.44805586338043213\n",
      "[Training Epoch 2] Batch 73, Loss 0.4612962603569031\n",
      "[Training Epoch 2] Batch 74, Loss 0.45505329966545105\n",
      "[Training Epoch 2] Batch 75, Loss 0.4486345052719116\n",
      "[Training Epoch 2] Batch 76, Loss 0.47582703828811646\n",
      "[Training Epoch 2] Batch 77, Loss 0.44382286071777344\n",
      "[Training Epoch 2] Batch 78, Loss 0.435994416475296\n",
      "[Training Epoch 2] Batch 79, Loss 0.45694294571876526\n",
      "[Training Epoch 2] Batch 80, Loss 0.39237451553344727\n",
      "[Training Epoch 2] Batch 81, Loss 0.4336475133895874\n",
      "[Training Epoch 2] Batch 82, Loss 0.4553144872188568\n",
      "[Training Epoch 2] Batch 83, Loss 0.4316774606704712\n",
      "[Training Epoch 2] Batch 84, Loss 0.4155091643333435\n",
      "[Training Epoch 2] Batch 85, Loss 0.42185837030410767\n",
      "[Training Epoch 2] Batch 86, Loss 0.44284141063690186\n",
      "[Training Epoch 2] Batch 87, Loss 0.4393264055252075\n",
      "[Training Epoch 2] Batch 88, Loss 0.4405590891838074\n",
      "[Training Epoch 2] Batch 89, Loss 0.4364858865737915\n",
      "[Training Epoch 2] Batch 90, Loss 0.4294465184211731\n",
      "[Training Epoch 2] Batch 91, Loss 0.42581474781036377\n",
      "[Training Epoch 2] Batch 92, Loss 0.4320412278175354\n",
      "[Training Epoch 2] Batch 93, Loss 0.4565884470939636\n",
      "[Training Epoch 2] Batch 94, Loss 0.4614803194999695\n",
      "[Training Epoch 2] Batch 95, Loss 0.4617260992527008\n",
      "[Training Epoch 2] Batch 96, Loss 0.44159048795700073\n",
      "[Training Epoch 2] Batch 97, Loss 0.4411933422088623\n",
      "[Training Epoch 2] Batch 98, Loss 0.46064236760139465\n",
      "[Training Epoch 2] Batch 99, Loss 0.44572776556015015\n",
      "[Training Epoch 2] Batch 100, Loss 0.4529423117637634\n",
      "[Training Epoch 2] Batch 101, Loss 0.44423413276672363\n",
      "[Training Epoch 2] Batch 102, Loss 0.4300262928009033\n",
      "[Training Epoch 2] Batch 103, Loss 0.43070337176322937\n",
      "[Training Epoch 2] Batch 104, Loss 0.4569171667098999\n",
      "[Training Epoch 2] Batch 105, Loss 0.4141015410423279\n",
      "[Training Epoch 2] Batch 106, Loss 0.4332178235054016\n",
      "[Training Epoch 2] Batch 107, Loss 0.4674990773200989\n",
      "[Training Epoch 2] Batch 108, Loss 0.42698290944099426\n",
      "[Training Epoch 2] Batch 109, Loss 0.4433166980743408\n",
      "[Training Epoch 2] Batch 110, Loss 0.41813230514526367\n",
      "[Training Epoch 2] Batch 111, Loss 0.4467470049858093\n",
      "[Training Epoch 2] Batch 112, Loss 0.4270918369293213\n",
      "[Training Epoch 2] Batch 113, Loss 0.46971598267555237\n",
      "[Training Epoch 2] Batch 114, Loss 0.4322440028190613\n",
      "[Training Epoch 2] Batch 115, Loss 0.4310447573661804\n",
      "[Training Epoch 2] Batch 116, Loss 0.4287550449371338\n",
      "[Training Epoch 2] Batch 117, Loss 0.42265331745147705\n",
      "[Training Epoch 2] Batch 118, Loss 0.4468635618686676\n",
      "[Training Epoch 2] Batch 119, Loss 0.47259384393692017\n",
      "[Training Epoch 2] Batch 120, Loss 0.4360259771347046\n",
      "[Training Epoch 2] Batch 121, Loss 0.41917046904563904\n",
      "[Training Epoch 2] Batch 122, Loss 0.45890945196151733\n",
      "[Training Epoch 2] Batch 123, Loss 0.45794564485549927\n",
      "[Training Epoch 2] Batch 124, Loss 0.42634934186935425\n",
      "[Training Epoch 2] Batch 125, Loss 0.451348215341568\n",
      "[Training Epoch 2] Batch 126, Loss 0.4081645905971527\n",
      "[Training Epoch 2] Batch 127, Loss 0.41145437955856323\n",
      "[Training Epoch 2] Batch 128, Loss 0.4536004066467285\n",
      "[Training Epoch 2] Batch 129, Loss 0.4543226361274719\n",
      "[Training Epoch 2] Batch 130, Loss 0.42629778385162354\n",
      "[Training Epoch 2] Batch 131, Loss 0.4281913638114929\n",
      "[Training Epoch 2] Batch 132, Loss 0.4491766691207886\n",
      "[Training Epoch 2] Batch 133, Loss 0.39554280042648315\n",
      "[Training Epoch 2] Batch 134, Loss 0.483701229095459\n",
      "[Training Epoch 2] Batch 135, Loss 0.45249924063682556\n",
      "[Training Epoch 2] Batch 136, Loss 0.4417397379875183\n",
      "[Training Epoch 2] Batch 137, Loss 0.45640239119529724\n",
      "[Training Epoch 2] Batch 138, Loss 0.4431670308113098\n",
      "[Training Epoch 2] Batch 139, Loss 0.4400125741958618\n",
      "[Training Epoch 2] Batch 140, Loss 0.4297958016395569\n",
      "[Training Epoch 2] Batch 141, Loss 0.4429582953453064\n",
      "[Training Epoch 2] Batch 142, Loss 0.4500191807746887\n",
      "[Training Epoch 2] Batch 143, Loss 0.4493628144264221\n",
      "[Training Epoch 2] Batch 144, Loss 0.430217981338501\n",
      "[Training Epoch 2] Batch 145, Loss 0.4495490491390228\n",
      "[Training Epoch 2] Batch 146, Loss 0.41845980286598206\n",
      "[Training Epoch 2] Batch 147, Loss 0.4238051772117615\n",
      "[Training Epoch 2] Batch 148, Loss 0.4358704090118408\n",
      "[Training Epoch 2] Batch 149, Loss 0.4148002862930298\n",
      "[Training Epoch 2] Batch 150, Loss 0.4507785737514496\n",
      "[Training Epoch 2] Batch 151, Loss 0.41968834400177\n",
      "[Training Epoch 2] Batch 152, Loss 0.43190962076187134\n",
      "[Training Epoch 2] Batch 153, Loss 0.4288763999938965\n",
      "[Training Epoch 2] Batch 154, Loss 0.4265897274017334\n",
      "[Training Epoch 2] Batch 155, Loss 0.4776707887649536\n",
      "[Training Epoch 2] Batch 156, Loss 0.4505201578140259\n",
      "[Training Epoch 2] Batch 157, Loss 0.422515869140625\n",
      "[Training Epoch 2] Batch 158, Loss 0.4293077290058136\n",
      "[Training Epoch 2] Batch 159, Loss 0.44493311643600464\n",
      "[Training Epoch 2] Batch 160, Loss 0.4258421063423157\n",
      "[Training Epoch 2] Batch 161, Loss 0.41393959522247314\n",
      "[Training Epoch 2] Batch 162, Loss 0.44335997104644775\n",
      "[Training Epoch 2] Batch 163, Loss 0.4670025110244751\n",
      "[Training Epoch 2] Batch 164, Loss 0.4205995202064514\n",
      "[Training Epoch 2] Batch 165, Loss 0.42596006393432617\n",
      "[Training Epoch 2] Batch 166, Loss 0.4249503016471863\n",
      "[Training Epoch 2] Batch 167, Loss 0.4224531054496765\n",
      "[Training Epoch 2] Batch 168, Loss 0.4455397129058838\n",
      "[Training Epoch 2] Batch 169, Loss 0.4169688820838928\n",
      "[Training Epoch 2] Batch 170, Loss 0.44090646505355835\n",
      "[Training Epoch 2] Batch 171, Loss 0.42467060685157776\n",
      "[Training Epoch 2] Batch 172, Loss 0.4488266706466675\n",
      "[Training Epoch 2] Batch 173, Loss 0.4236956536769867\n",
      "[Training Epoch 2] Batch 174, Loss 0.42888855934143066\n",
      "[Training Epoch 2] Batch 175, Loss 0.44704532623291016\n",
      "[Training Epoch 2] Batch 176, Loss 0.4294360876083374\n",
      "[Training Epoch 2] Batch 177, Loss 0.4216242730617523\n",
      "[Training Epoch 2] Batch 178, Loss 0.4316141605377197\n",
      "[Training Epoch 2] Batch 179, Loss 0.43921005725860596\n",
      "[Training Epoch 2] Batch 180, Loss 0.4405715763568878\n",
      "[Training Epoch 2] Batch 181, Loss 0.40989911556243896\n",
      "[Training Epoch 2] Batch 182, Loss 0.44383102655410767\n",
      "[Training Epoch 2] Batch 183, Loss 0.4677978754043579\n",
      "[Training Epoch 2] Batch 184, Loss 0.44060325622558594\n",
      "[Training Epoch 2] Batch 185, Loss 0.4655712842941284\n",
      "[Training Epoch 2] Batch 186, Loss 0.4431682825088501\n",
      "[Training Epoch 2] Batch 187, Loss 0.4697526693344116\n",
      "[Training Epoch 2] Batch 188, Loss 0.4174676537513733\n",
      "[Training Epoch 2] Batch 189, Loss 0.4389175772666931\n",
      "[Training Epoch 2] Batch 190, Loss 0.42634543776512146\n",
      "[Training Epoch 2] Batch 191, Loss 0.4240161180496216\n",
      "[Training Epoch 2] Batch 192, Loss 0.46782028675079346\n",
      "[Training Epoch 2] Batch 193, Loss 0.41861581802368164\n",
      "[Training Epoch 2] Batch 194, Loss 0.4099586606025696\n",
      "[Training Epoch 2] Batch 195, Loss 0.4291488528251648\n",
      "[Training Epoch 2] Batch 196, Loss 0.4476397931575775\n",
      "[Training Epoch 2] Batch 197, Loss 0.44169366359710693\n",
      "[Training Epoch 2] Batch 198, Loss 0.44919484853744507\n",
      "[Training Epoch 2] Batch 199, Loss 0.452678918838501\n",
      "[Training Epoch 2] Batch 200, Loss 0.43365395069122314\n",
      "[Training Epoch 2] Batch 201, Loss 0.43683239817619324\n",
      "[Training Epoch 2] Batch 202, Loss 0.44714808464050293\n",
      "[Training Epoch 2] Batch 203, Loss 0.43758758902549744\n",
      "[Training Epoch 2] Batch 204, Loss 0.4451555609703064\n",
      "[Training Epoch 2] Batch 205, Loss 0.43429964780807495\n",
      "[Training Epoch 2] Batch 206, Loss 0.4626978635787964\n",
      "[Training Epoch 2] Batch 207, Loss 0.44943901896476746\n",
      "[Training Epoch 2] Batch 208, Loss 0.42307981848716736\n",
      "[Training Epoch 2] Batch 209, Loss 0.4540838599205017\n",
      "[Training Epoch 2] Batch 210, Loss 0.41573551297187805\n",
      "[Training Epoch 2] Batch 211, Loss 0.4202452301979065\n",
      "[Training Epoch 2] Batch 212, Loss 0.4124124050140381\n",
      "[Training Epoch 2] Batch 213, Loss 0.4427211880683899\n",
      "[Training Epoch 2] Batch 214, Loss 0.4331969618797302\n",
      "[Training Epoch 2] Batch 215, Loss 0.41921648383140564\n",
      "[Training Epoch 2] Batch 216, Loss 0.4330636262893677\n",
      "[Training Epoch 2] Batch 217, Loss 0.40279829502105713\n",
      "[Training Epoch 2] Batch 218, Loss 0.44741734862327576\n",
      "[Training Epoch 2] Batch 219, Loss 0.42985039949417114\n",
      "[Training Epoch 2] Batch 220, Loss 0.42792922258377075\n",
      "[Training Epoch 2] Batch 221, Loss 0.42392703890800476\n",
      "[Training Epoch 2] Batch 222, Loss 0.44413328170776367\n",
      "[Training Epoch 2] Batch 223, Loss 0.43172913789749146\n",
      "[Training Epoch 2] Batch 224, Loss 0.42834165692329407\n",
      "[Training Epoch 2] Batch 225, Loss 0.44078347086906433\n",
      "[Training Epoch 2] Batch 226, Loss 0.4577663540840149\n",
      "[Training Epoch 2] Batch 227, Loss 0.41672325134277344\n",
      "[Training Epoch 2] Batch 228, Loss 0.42923504114151\n",
      "[Training Epoch 2] Batch 229, Loss 0.4442252814769745\n",
      "[Training Epoch 2] Batch 230, Loss 0.4324842691421509\n",
      "[Training Epoch 2] Batch 231, Loss 0.43846264481544495\n",
      "[Training Epoch 2] Batch 232, Loss 0.4243714213371277\n",
      "[Training Epoch 2] Batch 233, Loss 0.443382203578949\n",
      "[Training Epoch 2] Batch 234, Loss 0.40600571036338806\n",
      "[Training Epoch 2] Batch 235, Loss 0.4098976254463196\n",
      "[Training Epoch 2] Batch 236, Loss 0.445710688829422\n",
      "[Training Epoch 2] Batch 237, Loss 0.4025037884712219\n",
      "[Training Epoch 2] Batch 238, Loss 0.42525023221969604\n",
      "[Training Epoch 2] Batch 239, Loss 0.4406436085700989\n",
      "[Training Epoch 2] Batch 240, Loss 0.43537235260009766\n",
      "[Training Epoch 2] Batch 241, Loss 0.44020897150039673\n",
      "[Training Epoch 2] Batch 242, Loss 0.4238262176513672\n",
      "[Training Epoch 2] Batch 243, Loss 0.4374932646751404\n",
      "[Training Epoch 2] Batch 244, Loss 0.4182624816894531\n",
      "[Training Epoch 2] Batch 245, Loss 0.4318108558654785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2042.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2388.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 2] Precision = 0.3619, Recall = 0.9855\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.4340514540672302\n",
      "[Training Epoch 3] Batch 1, Loss 0.4098758101463318\n",
      "[Training Epoch 3] Batch 2, Loss 0.4743691086769104\n",
      "[Training Epoch 3] Batch 3, Loss 0.44991034269332886\n",
      "[Training Epoch 3] Batch 4, Loss 0.4640035927295685\n",
      "[Training Epoch 3] Batch 5, Loss 0.4318402409553528\n",
      "[Training Epoch 3] Batch 6, Loss 0.41437268257141113\n",
      "[Training Epoch 3] Batch 7, Loss 0.43158161640167236\n",
      "[Training Epoch 3] Batch 8, Loss 0.4288187623023987\n",
      "[Training Epoch 3] Batch 9, Loss 0.43300533294677734\n",
      "[Training Epoch 3] Batch 10, Loss 0.40700310468673706\n",
      "[Training Epoch 3] Batch 11, Loss 0.4207610487937927\n",
      "[Training Epoch 3] Batch 12, Loss 0.44484949111938477\n",
      "[Training Epoch 3] Batch 13, Loss 0.433523952960968\n",
      "[Training Epoch 3] Batch 14, Loss 0.4611623287200928\n",
      "[Training Epoch 3] Batch 15, Loss 0.4068785309791565\n",
      "[Training Epoch 3] Batch 16, Loss 0.4559307396411896\n",
      "[Training Epoch 3] Batch 17, Loss 0.4388781189918518\n",
      "[Training Epoch 3] Batch 18, Loss 0.43978697061538696\n",
      "[Training Epoch 3] Batch 19, Loss 0.418484628200531\n",
      "[Training Epoch 3] Batch 20, Loss 0.4433499276638031\n",
      "[Training Epoch 3] Batch 21, Loss 0.4184201955795288\n",
      "[Training Epoch 3] Batch 22, Loss 0.4269781708717346\n",
      "[Training Epoch 3] Batch 23, Loss 0.4290294349193573\n",
      "[Training Epoch 3] Batch 24, Loss 0.45974981784820557\n",
      "[Training Epoch 3] Batch 25, Loss 0.4523031711578369\n",
      "[Training Epoch 3] Batch 26, Loss 0.41668829321861267\n",
      "[Training Epoch 3] Batch 27, Loss 0.43485555052757263\n",
      "[Training Epoch 3] Batch 28, Loss 0.43075358867645264\n",
      "[Training Epoch 3] Batch 29, Loss 0.41827160120010376\n",
      "[Training Epoch 3] Batch 30, Loss 0.42803657054901123\n",
      "[Training Epoch 3] Batch 31, Loss 0.39966535568237305\n",
      "[Training Epoch 3] Batch 32, Loss 0.444169282913208\n",
      "[Training Epoch 3] Batch 33, Loss 0.40945422649383545\n",
      "[Training Epoch 3] Batch 34, Loss 0.41870424151420593\n",
      "[Training Epoch 3] Batch 35, Loss 0.43641093373298645\n",
      "[Training Epoch 3] Batch 36, Loss 0.4291244447231293\n",
      "[Training Epoch 3] Batch 37, Loss 0.4466511309146881\n",
      "[Training Epoch 3] Batch 38, Loss 0.419109046459198\n",
      "[Training Epoch 3] Batch 39, Loss 0.41825753450393677\n",
      "[Training Epoch 3] Batch 40, Loss 0.43646496534347534\n",
      "[Training Epoch 3] Batch 41, Loss 0.42872190475463867\n",
      "[Training Epoch 3] Batch 42, Loss 0.4094559848308563\n",
      "[Training Epoch 3] Batch 43, Loss 0.4244321882724762\n",
      "[Training Epoch 3] Batch 44, Loss 0.40804368257522583\n",
      "[Training Epoch 3] Batch 45, Loss 0.40100133419036865\n",
      "[Training Epoch 3] Batch 46, Loss 0.4217655658721924\n",
      "[Training Epoch 3] Batch 47, Loss 0.4290759563446045\n",
      "[Training Epoch 3] Batch 48, Loss 0.42029452323913574\n",
      "[Training Epoch 3] Batch 49, Loss 0.4233012795448303\n",
      "[Training Epoch 3] Batch 50, Loss 0.42496219277381897\n",
      "[Training Epoch 3] Batch 51, Loss 0.43213388323783875\n",
      "[Training Epoch 3] Batch 52, Loss 0.4273887574672699\n",
      "[Training Epoch 3] Batch 53, Loss 0.3985079526901245\n",
      "[Training Epoch 3] Batch 54, Loss 0.41858577728271484\n",
      "[Training Epoch 3] Batch 55, Loss 0.45353156328201294\n",
      "[Training Epoch 3] Batch 56, Loss 0.4146074056625366\n",
      "[Training Epoch 3] Batch 57, Loss 0.39055028557777405\n",
      "[Training Epoch 3] Batch 58, Loss 0.4106792211532593\n",
      "[Training Epoch 3] Batch 59, Loss 0.42935866117477417\n",
      "[Training Epoch 3] Batch 60, Loss 0.4166659712791443\n",
      "[Training Epoch 3] Batch 61, Loss 0.4344809055328369\n",
      "[Training Epoch 3] Batch 62, Loss 0.4140844941139221\n",
      "[Training Epoch 3] Batch 63, Loss 0.41736549139022827\n",
      "[Training Epoch 3] Batch 64, Loss 0.43896108865737915\n",
      "[Training Epoch 3] Batch 65, Loss 0.42200154066085815\n",
      "[Training Epoch 3] Batch 66, Loss 0.3987113833427429\n",
      "[Training Epoch 3] Batch 67, Loss 0.42038434743881226\n",
      "[Training Epoch 3] Batch 68, Loss 0.42208606004714966\n",
      "[Training Epoch 3] Batch 69, Loss 0.41141951084136963\n",
      "[Training Epoch 3] Batch 70, Loss 0.42074525356292725\n",
      "[Training Epoch 3] Batch 71, Loss 0.44634127616882324\n",
      "[Training Epoch 3] Batch 72, Loss 0.4638761281967163\n",
      "[Training Epoch 3] Batch 73, Loss 0.43782785534858704\n",
      "[Training Epoch 3] Batch 74, Loss 0.4056634306907654\n",
      "[Training Epoch 3] Batch 75, Loss 0.39044201374053955\n",
      "[Training Epoch 3] Batch 76, Loss 0.4153675436973572\n",
      "[Training Epoch 3] Batch 77, Loss 0.4071030020713806\n",
      "[Training Epoch 3] Batch 78, Loss 0.4360572397708893\n",
      "[Training Epoch 3] Batch 79, Loss 0.4225936233997345\n",
      "[Training Epoch 3] Batch 80, Loss 0.43944060802459717\n",
      "[Training Epoch 3] Batch 81, Loss 0.4194786548614502\n",
      "[Training Epoch 3] Batch 82, Loss 0.43398863077163696\n",
      "[Training Epoch 3] Batch 83, Loss 0.4062478244304657\n",
      "[Training Epoch 3] Batch 84, Loss 0.4401118755340576\n",
      "[Training Epoch 3] Batch 85, Loss 0.43110984563827515\n",
      "[Training Epoch 3] Batch 86, Loss 0.39252081513404846\n",
      "[Training Epoch 3] Batch 87, Loss 0.4313214421272278\n",
      "[Training Epoch 3] Batch 88, Loss 0.41531479358673096\n",
      "[Training Epoch 3] Batch 89, Loss 0.398090660572052\n",
      "[Training Epoch 3] Batch 90, Loss 0.42007723450660706\n",
      "[Training Epoch 3] Batch 91, Loss 0.433461993932724\n",
      "[Training Epoch 3] Batch 92, Loss 0.41525399684906006\n",
      "[Training Epoch 3] Batch 93, Loss 0.42070698738098145\n",
      "[Training Epoch 3] Batch 94, Loss 0.42509976029396057\n",
      "[Training Epoch 3] Batch 95, Loss 0.39091211557388306\n",
      "[Training Epoch 3] Batch 96, Loss 0.413962721824646\n",
      "[Training Epoch 3] Batch 97, Loss 0.4294692873954773\n",
      "[Training Epoch 3] Batch 98, Loss 0.4001859426498413\n",
      "[Training Epoch 3] Batch 99, Loss 0.4380696415901184\n",
      "[Training Epoch 3] Batch 100, Loss 0.4409448206424713\n",
      "[Training Epoch 3] Batch 101, Loss 0.41110938787460327\n",
      "[Training Epoch 3] Batch 102, Loss 0.39556175470352173\n",
      "[Training Epoch 3] Batch 103, Loss 0.43635278940200806\n",
      "[Training Epoch 3] Batch 104, Loss 0.4056854248046875\n",
      "[Training Epoch 3] Batch 105, Loss 0.429121732711792\n",
      "[Training Epoch 3] Batch 106, Loss 0.3982459008693695\n",
      "[Training Epoch 3] Batch 107, Loss 0.39430174231529236\n",
      "[Training Epoch 3] Batch 108, Loss 0.4269542396068573\n",
      "[Training Epoch 3] Batch 109, Loss 0.39165210723876953\n",
      "[Training Epoch 3] Batch 110, Loss 0.3969579339027405\n",
      "[Training Epoch 3] Batch 111, Loss 0.41573113203048706\n",
      "[Training Epoch 3] Batch 112, Loss 0.4363122582435608\n",
      "[Training Epoch 3] Batch 113, Loss 0.43518418073654175\n",
      "[Training Epoch 3] Batch 114, Loss 0.42547279596328735\n",
      "[Training Epoch 3] Batch 115, Loss 0.40878450870513916\n",
      "[Training Epoch 3] Batch 116, Loss 0.4382311701774597\n",
      "[Training Epoch 3] Batch 117, Loss 0.42312610149383545\n",
      "[Training Epoch 3] Batch 118, Loss 0.3982650935649872\n",
      "[Training Epoch 3] Batch 119, Loss 0.4097561240196228\n",
      "[Training Epoch 3] Batch 120, Loss 0.39451056718826294\n",
      "[Training Epoch 3] Batch 121, Loss 0.4342098832130432\n",
      "[Training Epoch 3] Batch 122, Loss 0.4327160120010376\n",
      "[Training Epoch 3] Batch 123, Loss 0.4416094124317169\n",
      "[Training Epoch 3] Batch 124, Loss 0.41405731439590454\n",
      "[Training Epoch 3] Batch 125, Loss 0.41716545820236206\n",
      "[Training Epoch 3] Batch 126, Loss 0.4331483542919159\n",
      "[Training Epoch 3] Batch 127, Loss 0.3878989815711975\n",
      "[Training Epoch 3] Batch 128, Loss 0.43056559562683105\n",
      "[Training Epoch 3] Batch 129, Loss 0.41077154874801636\n",
      "[Training Epoch 3] Batch 130, Loss 0.41087067127227783\n",
      "[Training Epoch 3] Batch 131, Loss 0.39559781551361084\n",
      "[Training Epoch 3] Batch 132, Loss 0.4363844394683838\n",
      "[Training Epoch 3] Batch 133, Loss 0.4136289358139038\n",
      "[Training Epoch 3] Batch 134, Loss 0.42860913276672363\n",
      "[Training Epoch 3] Batch 135, Loss 0.42738649249076843\n",
      "[Training Epoch 3] Batch 136, Loss 0.42607322335243225\n",
      "[Training Epoch 3] Batch 137, Loss 0.4105004072189331\n",
      "[Training Epoch 3] Batch 138, Loss 0.4170438349246979\n",
      "[Training Epoch 3] Batch 139, Loss 0.4095744788646698\n",
      "[Training Epoch 3] Batch 140, Loss 0.4117618501186371\n",
      "[Training Epoch 3] Batch 141, Loss 0.4262349605560303\n",
      "[Training Epoch 3] Batch 142, Loss 0.4145926833152771\n",
      "[Training Epoch 3] Batch 143, Loss 0.4203367829322815\n",
      "[Training Epoch 3] Batch 144, Loss 0.43762797117233276\n",
      "[Training Epoch 3] Batch 145, Loss 0.4082130193710327\n",
      "[Training Epoch 3] Batch 146, Loss 0.4485856890678406\n",
      "[Training Epoch 3] Batch 147, Loss 0.4286660850048065\n",
      "[Training Epoch 3] Batch 148, Loss 0.4159250855445862\n",
      "[Training Epoch 3] Batch 149, Loss 0.4365241527557373\n",
      "[Training Epoch 3] Batch 150, Loss 0.4194276034832001\n",
      "[Training Epoch 3] Batch 151, Loss 0.4091324210166931\n",
      "[Training Epoch 3] Batch 152, Loss 0.4038834869861603\n",
      "[Training Epoch 3] Batch 153, Loss 0.4303065240383148\n",
      "[Training Epoch 3] Batch 154, Loss 0.41723334789276123\n",
      "[Training Epoch 3] Batch 155, Loss 0.4406259059906006\n",
      "[Training Epoch 3] Batch 156, Loss 0.44299978017807007\n",
      "[Training Epoch 3] Batch 157, Loss 0.395951509475708\n",
      "[Training Epoch 3] Batch 158, Loss 0.42251327633857727\n",
      "[Training Epoch 3] Batch 159, Loss 0.39951467514038086\n",
      "[Training Epoch 3] Batch 160, Loss 0.3939839005470276\n",
      "[Training Epoch 3] Batch 161, Loss 0.4202214181423187\n",
      "[Training Epoch 3] Batch 162, Loss 0.41562190651893616\n",
      "[Training Epoch 3] Batch 163, Loss 0.38623037934303284\n",
      "[Training Epoch 3] Batch 164, Loss 0.41289222240448\n",
      "[Training Epoch 3] Batch 165, Loss 0.4244075119495392\n",
      "[Training Epoch 3] Batch 166, Loss 0.4503968060016632\n",
      "[Training Epoch 3] Batch 167, Loss 0.41040199995040894\n",
      "[Training Epoch 3] Batch 168, Loss 0.3970785140991211\n",
      "[Training Epoch 3] Batch 169, Loss 0.3930400013923645\n",
      "[Training Epoch 3] Batch 170, Loss 0.436756432056427\n",
      "[Training Epoch 3] Batch 171, Loss 0.4303671419620514\n",
      "[Training Epoch 3] Batch 172, Loss 0.4075600504875183\n",
      "[Training Epoch 3] Batch 173, Loss 0.4285874664783478\n",
      "[Training Epoch 3] Batch 174, Loss 0.44219934940338135\n",
      "[Training Epoch 3] Batch 175, Loss 0.4140150547027588\n",
      "[Training Epoch 3] Batch 176, Loss 0.44115009903907776\n",
      "[Training Epoch 3] Batch 177, Loss 0.3914104998111725\n",
      "[Training Epoch 3] Batch 178, Loss 0.4014439582824707\n",
      "[Training Epoch 3] Batch 179, Loss 0.4052070677280426\n",
      "[Training Epoch 3] Batch 180, Loss 0.4385414719581604\n",
      "[Training Epoch 3] Batch 181, Loss 0.406985342502594\n",
      "[Training Epoch 3] Batch 182, Loss 0.4173431396484375\n",
      "[Training Epoch 3] Batch 183, Loss 0.41297590732574463\n",
      "[Training Epoch 3] Batch 184, Loss 0.4079040288925171\n",
      "[Training Epoch 3] Batch 185, Loss 0.4334593415260315\n",
      "[Training Epoch 3] Batch 186, Loss 0.41212892532348633\n",
      "[Training Epoch 3] Batch 187, Loss 0.4172436594963074\n",
      "[Training Epoch 3] Batch 188, Loss 0.4087865352630615\n",
      "[Training Epoch 3] Batch 189, Loss 0.410976380109787\n",
      "[Training Epoch 3] Batch 190, Loss 0.429542601108551\n",
      "[Training Epoch 3] Batch 191, Loss 0.41684091091156006\n",
      "[Training Epoch 3] Batch 192, Loss 0.406563937664032\n",
      "[Training Epoch 3] Batch 193, Loss 0.404339075088501\n",
      "[Training Epoch 3] Batch 194, Loss 0.41645973920822144\n",
      "[Training Epoch 3] Batch 195, Loss 0.40363532304763794\n",
      "[Training Epoch 3] Batch 196, Loss 0.4379955530166626\n",
      "[Training Epoch 3] Batch 197, Loss 0.4241298735141754\n",
      "[Training Epoch 3] Batch 198, Loss 0.40245115756988525\n",
      "[Training Epoch 3] Batch 199, Loss 0.41226014494895935\n",
      "[Training Epoch 3] Batch 200, Loss 0.40139052271842957\n",
      "[Training Epoch 3] Batch 201, Loss 0.41422349214553833\n",
      "[Training Epoch 3] Batch 202, Loss 0.4096059799194336\n",
      "[Training Epoch 3] Batch 203, Loss 0.4267334043979645\n",
      "[Training Epoch 3] Batch 204, Loss 0.42292892932891846\n",
      "[Training Epoch 3] Batch 205, Loss 0.389489084482193\n",
      "[Training Epoch 3] Batch 206, Loss 0.41361555457115173\n",
      "[Training Epoch 3] Batch 207, Loss 0.4126911163330078\n",
      "[Training Epoch 3] Batch 208, Loss 0.40696051716804504\n",
      "[Training Epoch 3] Batch 209, Loss 0.42156803607940674\n",
      "[Training Epoch 3] Batch 210, Loss 0.41949135065078735\n",
      "[Training Epoch 3] Batch 211, Loss 0.4230548143386841\n",
      "[Training Epoch 3] Batch 212, Loss 0.431157648563385\n",
      "[Training Epoch 3] Batch 213, Loss 0.39334794878959656\n",
      "[Training Epoch 3] Batch 214, Loss 0.4275163412094116\n",
      "[Training Epoch 3] Batch 215, Loss 0.404818594455719\n",
      "[Training Epoch 3] Batch 216, Loss 0.42616355419158936\n",
      "[Training Epoch 3] Batch 217, Loss 0.40952253341674805\n",
      "[Training Epoch 3] Batch 218, Loss 0.41765525937080383\n",
      "[Training Epoch 3] Batch 219, Loss 0.421254426240921\n",
      "[Training Epoch 3] Batch 220, Loss 0.42613309621810913\n",
      "[Training Epoch 3] Batch 221, Loss 0.4120226204395294\n",
      "[Training Epoch 3] Batch 222, Loss 0.41632282733917236\n",
      "[Training Epoch 3] Batch 223, Loss 0.421562135219574\n",
      "[Training Epoch 3] Batch 224, Loss 0.45285987854003906\n",
      "[Training Epoch 3] Batch 225, Loss 0.4203990697860718\n",
      "[Training Epoch 3] Batch 226, Loss 0.4291229248046875\n",
      "[Training Epoch 3] Batch 227, Loss 0.4138919711112976\n",
      "[Training Epoch 3] Batch 228, Loss 0.39250871539115906\n",
      "[Training Epoch 3] Batch 229, Loss 0.4213879108428955\n",
      "[Training Epoch 3] Batch 230, Loss 0.4190044105052948\n",
      "[Training Epoch 3] Batch 231, Loss 0.3861393630504608\n",
      "[Training Epoch 3] Batch 232, Loss 0.4047508239746094\n",
      "[Training Epoch 3] Batch 233, Loss 0.40660473704338074\n",
      "[Training Epoch 3] Batch 234, Loss 0.4194068908691406\n",
      "[Training Epoch 3] Batch 235, Loss 0.4253925681114197\n",
      "[Training Epoch 3] Batch 236, Loss 0.38226237893104553\n",
      "[Training Epoch 3] Batch 237, Loss 0.39671990275382996\n",
      "[Training Epoch 3] Batch 238, Loss 0.42669403553009033\n",
      "[Training Epoch 3] Batch 239, Loss 0.41492271423339844\n",
      "[Training Epoch 3] Batch 240, Loss 0.39978766441345215\n",
      "[Training Epoch 3] Batch 241, Loss 0.3924259543418884\n",
      "[Training Epoch 3] Batch 242, Loss 0.4252846837043762\n",
      "[Training Epoch 3] Batch 243, Loss 0.41977083683013916\n",
      "[Training Epoch 3] Batch 244, Loss 0.4342769682407379\n",
      "[Training Epoch 3] Batch 245, Loss 0.4400472640991211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2079.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2384.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 3] Precision = 0.3622, Recall = 0.9859\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.4158279299736023\n",
      "[Training Epoch 4] Batch 1, Loss 0.40967512130737305\n",
      "[Training Epoch 4] Batch 2, Loss 0.4022253751754761\n",
      "[Training Epoch 4] Batch 3, Loss 0.40500810742378235\n",
      "[Training Epoch 4] Batch 4, Loss 0.40152132511138916\n",
      "[Training Epoch 4] Batch 5, Loss 0.39334020018577576\n",
      "[Training Epoch 4] Batch 6, Loss 0.4028382897377014\n",
      "[Training Epoch 4] Batch 7, Loss 0.45220479369163513\n",
      "[Training Epoch 4] Batch 8, Loss 0.42829227447509766\n",
      "[Training Epoch 4] Batch 9, Loss 0.42204344272613525\n",
      "[Training Epoch 4] Batch 10, Loss 0.4369966387748718\n",
      "[Training Epoch 4] Batch 11, Loss 0.4077003002166748\n",
      "[Training Epoch 4] Batch 12, Loss 0.38256198167800903\n",
      "[Training Epoch 4] Batch 13, Loss 0.39433395862579346\n",
      "[Training Epoch 4] Batch 14, Loss 0.3927844762802124\n",
      "[Training Epoch 4] Batch 15, Loss 0.42712610960006714\n",
      "[Training Epoch 4] Batch 16, Loss 0.42011964321136475\n",
      "[Training Epoch 4] Batch 17, Loss 0.4173775017261505\n",
      "[Training Epoch 4] Batch 18, Loss 0.408103883266449\n",
      "[Training Epoch 4] Batch 19, Loss 0.39963746070861816\n",
      "[Training Epoch 4] Batch 20, Loss 0.38117367029190063\n",
      "[Training Epoch 4] Batch 21, Loss 0.43172600865364075\n",
      "[Training Epoch 4] Batch 22, Loss 0.4234169125556946\n",
      "[Training Epoch 4] Batch 23, Loss 0.4175446033477783\n",
      "[Training Epoch 4] Batch 24, Loss 0.4014458656311035\n",
      "[Training Epoch 4] Batch 25, Loss 0.3667568266391754\n",
      "[Training Epoch 4] Batch 26, Loss 0.40548956394195557\n",
      "[Training Epoch 4] Batch 27, Loss 0.4032514691352844\n",
      "[Training Epoch 4] Batch 28, Loss 0.40890854597091675\n",
      "[Training Epoch 4] Batch 29, Loss 0.3921986222267151\n",
      "[Training Epoch 4] Batch 30, Loss 0.4108771085739136\n",
      "[Training Epoch 4] Batch 31, Loss 0.4020186960697174\n",
      "[Training Epoch 4] Batch 32, Loss 0.42250943183898926\n",
      "[Training Epoch 4] Batch 33, Loss 0.40491312742233276\n",
      "[Training Epoch 4] Batch 34, Loss 0.4337025284767151\n",
      "[Training Epoch 4] Batch 35, Loss 0.4044829308986664\n",
      "[Training Epoch 4] Batch 36, Loss 0.42308753728866577\n",
      "[Training Epoch 4] Batch 37, Loss 0.3962330222129822\n",
      "[Training Epoch 4] Batch 38, Loss 0.38994091749191284\n",
      "[Training Epoch 4] Batch 39, Loss 0.42156100273132324\n",
      "[Training Epoch 4] Batch 40, Loss 0.413673996925354\n",
      "[Training Epoch 4] Batch 41, Loss 0.4056324362754822\n",
      "[Training Epoch 4] Batch 42, Loss 0.38829153776168823\n",
      "[Training Epoch 4] Batch 43, Loss 0.38168731331825256\n",
      "[Training Epoch 4] Batch 44, Loss 0.4085160493850708\n",
      "[Training Epoch 4] Batch 45, Loss 0.43206214904785156\n",
      "[Training Epoch 4] Batch 46, Loss 0.4133290946483612\n",
      "[Training Epoch 4] Batch 47, Loss 0.41376206278800964\n",
      "[Training Epoch 4] Batch 48, Loss 0.3806246817111969\n",
      "[Training Epoch 4] Batch 49, Loss 0.42931467294692993\n",
      "[Training Epoch 4] Batch 50, Loss 0.39945265650749207\n",
      "[Training Epoch 4] Batch 51, Loss 0.421118825674057\n",
      "[Training Epoch 4] Batch 52, Loss 0.40246376395225525\n",
      "[Training Epoch 4] Batch 53, Loss 0.4021585285663605\n",
      "[Training Epoch 4] Batch 54, Loss 0.39247336983680725\n",
      "[Training Epoch 4] Batch 55, Loss 0.40361595153808594\n",
      "[Training Epoch 4] Batch 56, Loss 0.418510377407074\n",
      "[Training Epoch 4] Batch 57, Loss 0.39745622873306274\n",
      "[Training Epoch 4] Batch 58, Loss 0.40224552154541016\n",
      "[Training Epoch 4] Batch 59, Loss 0.4348071813583374\n",
      "[Training Epoch 4] Batch 60, Loss 0.39478829503059387\n",
      "[Training Epoch 4] Batch 61, Loss 0.41986554861068726\n",
      "[Training Epoch 4] Batch 62, Loss 0.41455861926078796\n",
      "[Training Epoch 4] Batch 63, Loss 0.3858014941215515\n",
      "[Training Epoch 4] Batch 64, Loss 0.41891175508499146\n",
      "[Training Epoch 4] Batch 65, Loss 0.4005536437034607\n",
      "[Training Epoch 4] Batch 66, Loss 0.41099685430526733\n",
      "[Training Epoch 4] Batch 67, Loss 0.3896231949329376\n",
      "[Training Epoch 4] Batch 68, Loss 0.3809778690338135\n",
      "[Training Epoch 4] Batch 69, Loss 0.4048413336277008\n",
      "[Training Epoch 4] Batch 70, Loss 0.42582517862319946\n",
      "[Training Epoch 4] Batch 71, Loss 0.38095957040786743\n",
      "[Training Epoch 4] Batch 72, Loss 0.43825763463974\n",
      "[Training Epoch 4] Batch 73, Loss 0.40891772508621216\n",
      "[Training Epoch 4] Batch 74, Loss 0.4089909791946411\n",
      "[Training Epoch 4] Batch 75, Loss 0.43565529584884644\n",
      "[Training Epoch 4] Batch 76, Loss 0.4297122359275818\n",
      "[Training Epoch 4] Batch 77, Loss 0.39319461584091187\n",
      "[Training Epoch 4] Batch 78, Loss 0.3982093334197998\n",
      "[Training Epoch 4] Batch 79, Loss 0.4165268540382385\n",
      "[Training Epoch 4] Batch 80, Loss 0.40121668577194214\n",
      "[Training Epoch 4] Batch 81, Loss 0.3882073760032654\n",
      "[Training Epoch 4] Batch 82, Loss 0.42300984263420105\n",
      "[Training Epoch 4] Batch 83, Loss 0.40465009212493896\n",
      "[Training Epoch 4] Batch 84, Loss 0.41564470529556274\n",
      "[Training Epoch 4] Batch 85, Loss 0.42308613657951355\n",
      "[Training Epoch 4] Batch 86, Loss 0.3778209686279297\n",
      "[Training Epoch 4] Batch 87, Loss 0.4353015720844269\n",
      "[Training Epoch 4] Batch 88, Loss 0.40891486406326294\n",
      "[Training Epoch 4] Batch 89, Loss 0.36855000257492065\n",
      "[Training Epoch 4] Batch 90, Loss 0.38610291481018066\n",
      "[Training Epoch 4] Batch 91, Loss 0.43339842557907104\n",
      "[Training Epoch 4] Batch 92, Loss 0.43671631813049316\n",
      "[Training Epoch 4] Batch 93, Loss 0.40799981355667114\n",
      "[Training Epoch 4] Batch 94, Loss 0.3811836838722229\n",
      "[Training Epoch 4] Batch 95, Loss 0.40392762422561646\n",
      "[Training Epoch 4] Batch 96, Loss 0.38755056262016296\n",
      "[Training Epoch 4] Batch 97, Loss 0.393972247838974\n",
      "[Training Epoch 4] Batch 98, Loss 0.40149015188217163\n",
      "[Training Epoch 4] Batch 99, Loss 0.39981788396835327\n",
      "[Training Epoch 4] Batch 100, Loss 0.4065294861793518\n",
      "[Training Epoch 4] Batch 101, Loss 0.42433327436447144\n",
      "[Training Epoch 4] Batch 102, Loss 0.38894930481910706\n",
      "[Training Epoch 4] Batch 103, Loss 0.4270433187484741\n",
      "[Training Epoch 4] Batch 104, Loss 0.387290358543396\n",
      "[Training Epoch 4] Batch 105, Loss 0.4134986400604248\n",
      "[Training Epoch 4] Batch 106, Loss 0.3769393563270569\n",
      "[Training Epoch 4] Batch 107, Loss 0.42788130044937134\n",
      "[Training Epoch 4] Batch 108, Loss 0.39252570271492004\n",
      "[Training Epoch 4] Batch 109, Loss 0.4086182117462158\n",
      "[Training Epoch 4] Batch 110, Loss 0.3893769681453705\n",
      "[Training Epoch 4] Batch 111, Loss 0.4047102928161621\n",
      "[Training Epoch 4] Batch 112, Loss 0.380428671836853\n",
      "[Training Epoch 4] Batch 113, Loss 0.39365291595458984\n",
      "[Training Epoch 4] Batch 114, Loss 0.417087197303772\n",
      "[Training Epoch 4] Batch 115, Loss 0.42203474044799805\n",
      "[Training Epoch 4] Batch 116, Loss 0.4061521887779236\n",
      "[Training Epoch 4] Batch 117, Loss 0.40860986709594727\n",
      "[Training Epoch 4] Batch 118, Loss 0.3974790573120117\n",
      "[Training Epoch 4] Batch 119, Loss 0.4045788049697876\n",
      "[Training Epoch 4] Batch 120, Loss 0.4084304869174957\n",
      "[Training Epoch 4] Batch 121, Loss 0.41915881633758545\n",
      "[Training Epoch 4] Batch 122, Loss 0.38829877972602844\n",
      "[Training Epoch 4] Batch 123, Loss 0.4230514466762543\n",
      "[Training Epoch 4] Batch 124, Loss 0.4097828269004822\n",
      "[Training Epoch 4] Batch 125, Loss 0.39721450209617615\n",
      "[Training Epoch 4] Batch 126, Loss 0.4239341616630554\n",
      "[Training Epoch 4] Batch 127, Loss 0.39627909660339355\n",
      "[Training Epoch 4] Batch 128, Loss 0.4183708429336548\n",
      "[Training Epoch 4] Batch 129, Loss 0.3884941339492798\n",
      "[Training Epoch 4] Batch 130, Loss 0.3923361003398895\n",
      "[Training Epoch 4] Batch 131, Loss 0.3964800238609314\n",
      "[Training Epoch 4] Batch 132, Loss 0.39280885457992554\n",
      "[Training Epoch 4] Batch 133, Loss 0.3887418210506439\n",
      "[Training Epoch 4] Batch 134, Loss 0.39927905797958374\n",
      "[Training Epoch 4] Batch 135, Loss 0.4085805416107178\n",
      "[Training Epoch 4] Batch 136, Loss 0.3934352993965149\n",
      "[Training Epoch 4] Batch 137, Loss 0.397493839263916\n",
      "[Training Epoch 4] Batch 138, Loss 0.4059486389160156\n",
      "[Training Epoch 4] Batch 139, Loss 0.4206872880458832\n",
      "[Training Epoch 4] Batch 140, Loss 0.4203870892524719\n",
      "[Training Epoch 4] Batch 141, Loss 0.4064076542854309\n",
      "[Training Epoch 4] Batch 142, Loss 0.41859740018844604\n",
      "[Training Epoch 4] Batch 143, Loss 0.38454610109329224\n",
      "[Training Epoch 4] Batch 144, Loss 0.4128568470478058\n",
      "[Training Epoch 4] Batch 145, Loss 0.4251859188079834\n",
      "[Training Epoch 4] Batch 146, Loss 0.4359586238861084\n",
      "[Training Epoch 4] Batch 147, Loss 0.3866654336452484\n",
      "[Training Epoch 4] Batch 148, Loss 0.3988337814807892\n",
      "[Training Epoch 4] Batch 149, Loss 0.4273121953010559\n",
      "[Training Epoch 4] Batch 150, Loss 0.3901543617248535\n",
      "[Training Epoch 4] Batch 151, Loss 0.39615362882614136\n",
      "[Training Epoch 4] Batch 152, Loss 0.41188129782676697\n",
      "[Training Epoch 4] Batch 153, Loss 0.3940484821796417\n",
      "[Training Epoch 4] Batch 154, Loss 0.39839625358581543\n",
      "[Training Epoch 4] Batch 155, Loss 0.37834709882736206\n",
      "[Training Epoch 4] Batch 156, Loss 0.399240642786026\n",
      "[Training Epoch 4] Batch 157, Loss 0.399372398853302\n",
      "[Training Epoch 4] Batch 158, Loss 0.39950883388519287\n",
      "[Training Epoch 4] Batch 159, Loss 0.4060004651546478\n",
      "[Training Epoch 4] Batch 160, Loss 0.39834314584732056\n",
      "[Training Epoch 4] Batch 161, Loss 0.349179744720459\n",
      "[Training Epoch 4] Batch 162, Loss 0.42908430099487305\n",
      "[Training Epoch 4] Batch 163, Loss 0.3860180079936981\n",
      "[Training Epoch 4] Batch 164, Loss 0.4016725420951843\n",
      "[Training Epoch 4] Batch 165, Loss 0.3740676939487457\n",
      "[Training Epoch 4] Batch 166, Loss 0.4117606580257416\n",
      "[Training Epoch 4] Batch 167, Loss 0.3925629258155823\n",
      "[Training Epoch 4] Batch 168, Loss 0.3969583511352539\n",
      "[Training Epoch 4] Batch 169, Loss 0.3572295904159546\n",
      "[Training Epoch 4] Batch 170, Loss 0.3862006366252899\n",
      "[Training Epoch 4] Batch 171, Loss 0.42396873235702515\n",
      "[Training Epoch 4] Batch 172, Loss 0.4215961694717407\n",
      "[Training Epoch 4] Batch 173, Loss 0.4196571111679077\n",
      "[Training Epoch 4] Batch 174, Loss 0.3956722021102905\n",
      "[Training Epoch 4] Batch 175, Loss 0.3861474394798279\n",
      "[Training Epoch 4] Batch 176, Loss 0.3890624940395355\n",
      "[Training Epoch 4] Batch 177, Loss 0.4093739986419678\n",
      "[Training Epoch 4] Batch 178, Loss 0.4044919013977051\n",
      "[Training Epoch 4] Batch 179, Loss 0.39662277698516846\n",
      "[Training Epoch 4] Batch 180, Loss 0.3778197169303894\n",
      "[Training Epoch 4] Batch 181, Loss 0.39794307947158813\n",
      "[Training Epoch 4] Batch 182, Loss 0.40237951278686523\n",
      "[Training Epoch 4] Batch 183, Loss 0.3833780586719513\n",
      "[Training Epoch 4] Batch 184, Loss 0.39085808396339417\n",
      "[Training Epoch 4] Batch 185, Loss 0.40353918075561523\n",
      "[Training Epoch 4] Batch 186, Loss 0.38474181294441223\n",
      "[Training Epoch 4] Batch 187, Loss 0.41433072090148926\n",
      "[Training Epoch 4] Batch 188, Loss 0.4334465563297272\n",
      "[Training Epoch 4] Batch 189, Loss 0.4052509665489197\n",
      "[Training Epoch 4] Batch 190, Loss 0.39214369654655457\n",
      "[Training Epoch 4] Batch 191, Loss 0.40327510237693787\n",
      "[Training Epoch 4] Batch 192, Loss 0.4243694841861725\n",
      "[Training Epoch 4] Batch 193, Loss 0.38817036151885986\n",
      "[Training Epoch 4] Batch 194, Loss 0.3836503028869629\n",
      "[Training Epoch 4] Batch 195, Loss 0.4436910152435303\n",
      "[Training Epoch 4] Batch 196, Loss 0.40243828296661377\n",
      "[Training Epoch 4] Batch 197, Loss 0.3887304961681366\n",
      "[Training Epoch 4] Batch 198, Loss 0.3980529308319092\n",
      "[Training Epoch 4] Batch 199, Loss 0.4079621434211731\n",
      "[Training Epoch 4] Batch 200, Loss 0.3740536868572235\n",
      "[Training Epoch 4] Batch 201, Loss 0.37757617235183716\n",
      "[Training Epoch 4] Batch 202, Loss 0.37226778268814087\n",
      "[Training Epoch 4] Batch 203, Loss 0.4040548801422119\n",
      "[Training Epoch 4] Batch 204, Loss 0.38494613766670227\n",
      "[Training Epoch 4] Batch 205, Loss 0.4098750054836273\n",
      "[Training Epoch 4] Batch 206, Loss 0.39753997325897217\n",
      "[Training Epoch 4] Batch 207, Loss 0.41751188039779663\n",
      "[Training Epoch 4] Batch 208, Loss 0.38898491859436035\n",
      "[Training Epoch 4] Batch 209, Loss 0.3968035876750946\n",
      "[Training Epoch 4] Batch 210, Loss 0.3827696442604065\n",
      "[Training Epoch 4] Batch 211, Loss 0.38761234283447266\n",
      "[Training Epoch 4] Batch 212, Loss 0.3817436695098877\n",
      "[Training Epoch 4] Batch 213, Loss 0.40025174617767334\n",
      "[Training Epoch 4] Batch 214, Loss 0.4217926263809204\n",
      "[Training Epoch 4] Batch 215, Loss 0.39426305890083313\n",
      "[Training Epoch 4] Batch 216, Loss 0.4244697093963623\n",
      "[Training Epoch 4] Batch 217, Loss 0.37847402691841125\n",
      "[Training Epoch 4] Batch 218, Loss 0.3859400749206543\n",
      "[Training Epoch 4] Batch 219, Loss 0.3735402226448059\n",
      "[Training Epoch 4] Batch 220, Loss 0.38886648416519165\n",
      "[Training Epoch 4] Batch 221, Loss 0.4097875952720642\n",
      "[Training Epoch 4] Batch 222, Loss 0.4059021472930908\n",
      "[Training Epoch 4] Batch 223, Loss 0.4206765294075012\n",
      "[Training Epoch 4] Batch 224, Loss 0.3727726340293884\n",
      "[Training Epoch 4] Batch 225, Loss 0.38833993673324585\n",
      "[Training Epoch 4] Batch 226, Loss 0.411210834980011\n",
      "[Training Epoch 4] Batch 227, Loss 0.37852463126182556\n",
      "[Training Epoch 4] Batch 228, Loss 0.40903669595718384\n",
      "[Training Epoch 4] Batch 229, Loss 0.3984212577342987\n",
      "[Training Epoch 4] Batch 230, Loss 0.392642617225647\n",
      "[Training Epoch 4] Batch 231, Loss 0.3778839111328125\n",
      "[Training Epoch 4] Batch 232, Loss 0.39590248465538025\n",
      "[Training Epoch 4] Batch 233, Loss 0.37956398725509644\n",
      "[Training Epoch 4] Batch 234, Loss 0.40177392959594727\n",
      "[Training Epoch 4] Batch 235, Loss 0.3907507658004761\n",
      "[Training Epoch 4] Batch 236, Loss 0.3910820484161377\n",
      "[Training Epoch 4] Batch 237, Loss 0.4195123314857483\n",
      "[Training Epoch 4] Batch 238, Loss 0.4042186737060547\n",
      "[Training Epoch 4] Batch 239, Loss 0.40397655963897705\n",
      "[Training Epoch 4] Batch 240, Loss 0.3733022212982178\n",
      "[Training Epoch 4] Batch 241, Loss 0.40768522024154663\n",
      "[Training Epoch 4] Batch 242, Loss 0.4067777693271637\n",
      "[Training Epoch 4] Batch 243, Loss 0.4137584865093231\n",
      "[Training Epoch 4] Batch 244, Loss 0.39481109380722046\n",
      "[Training Epoch 4] Batch 245, Loss 0.37796998023986816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2090.53it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2399.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 4] Precision = 0.3625, Recall = 0.9861\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.41044458746910095\n",
      "[Training Epoch 5] Batch 1, Loss 0.3979334533214569\n",
      "[Training Epoch 5] Batch 2, Loss 0.36481255292892456\n",
      "[Training Epoch 5] Batch 3, Loss 0.4084470570087433\n",
      "[Training Epoch 5] Batch 4, Loss 0.41040924191474915\n",
      "[Training Epoch 5] Batch 5, Loss 0.41572991013526917\n",
      "[Training Epoch 5] Batch 6, Loss 0.416847288608551\n",
      "[Training Epoch 5] Batch 7, Loss 0.3666227161884308\n",
      "[Training Epoch 5] Batch 8, Loss 0.3694532513618469\n",
      "[Training Epoch 5] Batch 9, Loss 0.41192036867141724\n",
      "[Training Epoch 5] Batch 10, Loss 0.39078330993652344\n",
      "[Training Epoch 5] Batch 11, Loss 0.38046926259994507\n",
      "[Training Epoch 5] Batch 12, Loss 0.3682614266872406\n",
      "[Training Epoch 5] Batch 13, Loss 0.38663095235824585\n",
      "[Training Epoch 5] Batch 14, Loss 0.3985219895839691\n",
      "[Training Epoch 5] Batch 15, Loss 0.400124192237854\n",
      "[Training Epoch 5] Batch 16, Loss 0.3816336989402771\n",
      "[Training Epoch 5] Batch 17, Loss 0.4100436270236969\n",
      "[Training Epoch 5] Batch 18, Loss 0.35780245065689087\n",
      "[Training Epoch 5] Batch 19, Loss 0.40703633427619934\n",
      "[Training Epoch 5] Batch 20, Loss 0.4110325276851654\n",
      "[Training Epoch 5] Batch 21, Loss 0.40790191292762756\n",
      "[Training Epoch 5] Batch 22, Loss 0.3619145154953003\n",
      "[Training Epoch 5] Batch 23, Loss 0.3971940279006958\n",
      "[Training Epoch 5] Batch 24, Loss 0.3898616433143616\n",
      "[Training Epoch 5] Batch 25, Loss 0.3985898494720459\n",
      "[Training Epoch 5] Batch 26, Loss 0.3648205101490021\n",
      "[Training Epoch 5] Batch 27, Loss 0.38210123777389526\n",
      "[Training Epoch 5] Batch 28, Loss 0.3830631971359253\n",
      "[Training Epoch 5] Batch 29, Loss 0.4071422815322876\n",
      "[Training Epoch 5] Batch 30, Loss 0.39209622144699097\n",
      "[Training Epoch 5] Batch 31, Loss 0.37144625186920166\n",
      "[Training Epoch 5] Batch 32, Loss 0.3744857907295227\n",
      "[Training Epoch 5] Batch 33, Loss 0.41979047656059265\n",
      "[Training Epoch 5] Batch 34, Loss 0.42726725339889526\n",
      "[Training Epoch 5] Batch 35, Loss 0.41329842805862427\n",
      "[Training Epoch 5] Batch 36, Loss 0.4109588861465454\n",
      "[Training Epoch 5] Batch 37, Loss 0.381804883480072\n",
      "[Training Epoch 5] Batch 38, Loss 0.3836303949356079\n",
      "[Training Epoch 5] Batch 39, Loss 0.405153751373291\n",
      "[Training Epoch 5] Batch 40, Loss 0.3957098126411438\n",
      "[Training Epoch 5] Batch 41, Loss 0.40489768981933594\n",
      "[Training Epoch 5] Batch 42, Loss 0.39713436365127563\n",
      "[Training Epoch 5] Batch 43, Loss 0.3720167279243469\n",
      "[Training Epoch 5] Batch 44, Loss 0.4003782868385315\n",
      "[Training Epoch 5] Batch 45, Loss 0.37939250469207764\n",
      "[Training Epoch 5] Batch 46, Loss 0.413478285074234\n",
      "[Training Epoch 5] Batch 47, Loss 0.36692261695861816\n",
      "[Training Epoch 5] Batch 48, Loss 0.4199802279472351\n",
      "[Training Epoch 5] Batch 49, Loss 0.406406044960022\n",
      "[Training Epoch 5] Batch 50, Loss 0.3888893723487854\n",
      "[Training Epoch 5] Batch 51, Loss 0.4254937171936035\n",
      "[Training Epoch 5] Batch 52, Loss 0.37096476554870605\n",
      "[Training Epoch 5] Batch 53, Loss 0.388491153717041\n",
      "[Training Epoch 5] Batch 54, Loss 0.38348931074142456\n",
      "[Training Epoch 5] Batch 55, Loss 0.37748610973358154\n",
      "[Training Epoch 5] Batch 56, Loss 0.4036220610141754\n",
      "[Training Epoch 5] Batch 57, Loss 0.38711100816726685\n",
      "[Training Epoch 5] Batch 58, Loss 0.40702974796295166\n",
      "[Training Epoch 5] Batch 59, Loss 0.41784048080444336\n",
      "[Training Epoch 5] Batch 60, Loss 0.41521981358528137\n",
      "[Training Epoch 5] Batch 61, Loss 0.4172184467315674\n",
      "[Training Epoch 5] Batch 62, Loss 0.4042010009288788\n",
      "[Training Epoch 5] Batch 63, Loss 0.3922686278820038\n",
      "[Training Epoch 5] Batch 64, Loss 0.39083659648895264\n",
      "[Training Epoch 5] Batch 65, Loss 0.3776596188545227\n",
      "[Training Epoch 5] Batch 66, Loss 0.403023362159729\n",
      "[Training Epoch 5] Batch 67, Loss 0.39290034770965576\n",
      "[Training Epoch 5] Batch 68, Loss 0.379438579082489\n",
      "[Training Epoch 5] Batch 69, Loss 0.388742059469223\n",
      "[Training Epoch 5] Batch 70, Loss 0.3925156593322754\n",
      "[Training Epoch 5] Batch 71, Loss 0.3903748095035553\n",
      "[Training Epoch 5] Batch 72, Loss 0.3896990418434143\n",
      "[Training Epoch 5] Batch 73, Loss 0.393036425113678\n",
      "[Training Epoch 5] Batch 74, Loss 0.3837147057056427\n",
      "[Training Epoch 5] Batch 75, Loss 0.40884822607040405\n",
      "[Training Epoch 5] Batch 76, Loss 0.3844781517982483\n",
      "[Training Epoch 5] Batch 77, Loss 0.422604501247406\n",
      "[Training Epoch 5] Batch 78, Loss 0.3893572688102722\n",
      "[Training Epoch 5] Batch 79, Loss 0.3973552882671356\n",
      "[Training Epoch 5] Batch 80, Loss 0.3895670771598816\n",
      "[Training Epoch 5] Batch 81, Loss 0.3748287856578827\n",
      "[Training Epoch 5] Batch 82, Loss 0.39054349064826965\n",
      "[Training Epoch 5] Batch 83, Loss 0.41727763414382935\n",
      "[Training Epoch 5] Batch 84, Loss 0.39990872144699097\n",
      "[Training Epoch 5] Batch 85, Loss 0.3881577253341675\n",
      "[Training Epoch 5] Batch 86, Loss 0.3808794915676117\n",
      "[Training Epoch 5] Batch 87, Loss 0.39239925146102905\n",
      "[Training Epoch 5] Batch 88, Loss 0.39044255018234253\n",
      "[Training Epoch 5] Batch 89, Loss 0.38089776039123535\n",
      "[Training Epoch 5] Batch 90, Loss 0.35791921615600586\n",
      "[Training Epoch 5] Batch 91, Loss 0.38881367444992065\n",
      "[Training Epoch 5] Batch 92, Loss 0.4029092788696289\n",
      "[Training Epoch 5] Batch 93, Loss 0.4092276692390442\n",
      "[Training Epoch 5] Batch 94, Loss 0.3968481719493866\n",
      "[Training Epoch 5] Batch 95, Loss 0.3800833523273468\n",
      "[Training Epoch 5] Batch 96, Loss 0.404636025428772\n",
      "[Training Epoch 5] Batch 97, Loss 0.39154648780822754\n",
      "[Training Epoch 5] Batch 98, Loss 0.41103053092956543\n",
      "[Training Epoch 5] Batch 99, Loss 0.3959859609603882\n",
      "[Training Epoch 5] Batch 100, Loss 0.3951006829738617\n",
      "[Training Epoch 5] Batch 101, Loss 0.39784127473831177\n",
      "[Training Epoch 5] Batch 102, Loss 0.3676379323005676\n",
      "[Training Epoch 5] Batch 103, Loss 0.3779629170894623\n",
      "[Training Epoch 5] Batch 104, Loss 0.4103929102420807\n",
      "[Training Epoch 5] Batch 105, Loss 0.3870283365249634\n",
      "[Training Epoch 5] Batch 106, Loss 0.36007750034332275\n",
      "[Training Epoch 5] Batch 107, Loss 0.39547473192214966\n",
      "[Training Epoch 5] Batch 108, Loss 0.3928845524787903\n",
      "[Training Epoch 5] Batch 109, Loss 0.3852015733718872\n",
      "[Training Epoch 5] Batch 110, Loss 0.3748573064804077\n",
      "[Training Epoch 5] Batch 111, Loss 0.3869706690311432\n",
      "[Training Epoch 5] Batch 112, Loss 0.37437623739242554\n",
      "[Training Epoch 5] Batch 113, Loss 0.39025384187698364\n",
      "[Training Epoch 5] Batch 114, Loss 0.3816309869289398\n",
      "[Training Epoch 5] Batch 115, Loss 0.35886436700820923\n",
      "[Training Epoch 5] Batch 116, Loss 0.35028237104415894\n",
      "[Training Epoch 5] Batch 117, Loss 0.3903445303440094\n",
      "[Training Epoch 5] Batch 118, Loss 0.37082868814468384\n",
      "[Training Epoch 5] Batch 119, Loss 0.345877468585968\n",
      "[Training Epoch 5] Batch 120, Loss 0.40041255950927734\n",
      "[Training Epoch 5] Batch 121, Loss 0.41023677587509155\n",
      "[Training Epoch 5] Batch 122, Loss 0.38613349199295044\n",
      "[Training Epoch 5] Batch 123, Loss 0.3800991177558899\n",
      "[Training Epoch 5] Batch 124, Loss 0.39837101101875305\n",
      "[Training Epoch 5] Batch 125, Loss 0.3984219431877136\n",
      "[Training Epoch 5] Batch 126, Loss 0.4406774044036865\n",
      "[Training Epoch 5] Batch 127, Loss 0.3874146044254303\n",
      "[Training Epoch 5] Batch 128, Loss 0.37549537420272827\n",
      "[Training Epoch 5] Batch 129, Loss 0.37205979228019714\n",
      "[Training Epoch 5] Batch 130, Loss 0.3956834673881531\n",
      "[Training Epoch 5] Batch 131, Loss 0.38962996006011963\n",
      "[Training Epoch 5] Batch 132, Loss 0.40107810497283936\n",
      "[Training Epoch 5] Batch 133, Loss 0.39666831493377686\n",
      "[Training Epoch 5] Batch 134, Loss 0.3817065954208374\n",
      "[Training Epoch 5] Batch 135, Loss 0.3934757113456726\n",
      "[Training Epoch 5] Batch 136, Loss 0.3687417507171631\n",
      "[Training Epoch 5] Batch 137, Loss 0.4259405732154846\n",
      "[Training Epoch 5] Batch 138, Loss 0.42026379704475403\n",
      "[Training Epoch 5] Batch 139, Loss 0.3659901022911072\n",
      "[Training Epoch 5] Batch 140, Loss 0.35940244793891907\n",
      "[Training Epoch 5] Batch 141, Loss 0.39658868312835693\n",
      "[Training Epoch 5] Batch 142, Loss 0.4233782887458801\n",
      "[Training Epoch 5] Batch 143, Loss 0.4160783588886261\n",
      "[Training Epoch 5] Batch 144, Loss 0.38779106736183167\n",
      "[Training Epoch 5] Batch 145, Loss 0.3801358938217163\n",
      "[Training Epoch 5] Batch 146, Loss 0.39881667494773865\n",
      "[Training Epoch 5] Batch 147, Loss 0.3595247268676758\n",
      "[Training Epoch 5] Batch 148, Loss 0.3716382682323456\n",
      "[Training Epoch 5] Batch 149, Loss 0.3643997311592102\n",
      "[Training Epoch 5] Batch 150, Loss 0.39034897089004517\n",
      "[Training Epoch 5] Batch 151, Loss 0.38964927196502686\n",
      "[Training Epoch 5] Batch 152, Loss 0.3757307827472687\n",
      "[Training Epoch 5] Batch 153, Loss 0.3953933119773865\n",
      "[Training Epoch 5] Batch 154, Loss 0.39790236949920654\n",
      "[Training Epoch 5] Batch 155, Loss 0.3834902346134186\n",
      "[Training Epoch 5] Batch 156, Loss 0.3706192970275879\n",
      "[Training Epoch 5] Batch 157, Loss 0.4067702293395996\n",
      "[Training Epoch 5] Batch 158, Loss 0.41919347643852234\n",
      "[Training Epoch 5] Batch 159, Loss 0.3779538571834564\n",
      "[Training Epoch 5] Batch 160, Loss 0.394747257232666\n",
      "[Training Epoch 5] Batch 161, Loss 0.3879033923149109\n",
      "[Training Epoch 5] Batch 162, Loss 0.3975624442100525\n",
      "[Training Epoch 5] Batch 163, Loss 0.372647225856781\n",
      "[Training Epoch 5] Batch 164, Loss 0.3931329846382141\n",
      "[Training Epoch 5] Batch 165, Loss 0.3915032744407654\n",
      "[Training Epoch 5] Batch 166, Loss 0.379840612411499\n",
      "[Training Epoch 5] Batch 167, Loss 0.39867234230041504\n",
      "[Training Epoch 5] Batch 168, Loss 0.3663078248500824\n",
      "[Training Epoch 5] Batch 169, Loss 0.3604804277420044\n",
      "[Training Epoch 5] Batch 170, Loss 0.39129164814949036\n",
      "[Training Epoch 5] Batch 171, Loss 0.39649659395217896\n",
      "[Training Epoch 5] Batch 172, Loss 0.3660926818847656\n",
      "[Training Epoch 5] Batch 173, Loss 0.3610571324825287\n",
      "[Training Epoch 5] Batch 174, Loss 0.38311767578125\n",
      "[Training Epoch 5] Batch 175, Loss 0.38713428378105164\n",
      "[Training Epoch 5] Batch 176, Loss 0.40164053440093994\n",
      "[Training Epoch 5] Batch 177, Loss 0.40012800693511963\n",
      "[Training Epoch 5] Batch 178, Loss 0.3703947961330414\n",
      "[Training Epoch 5] Batch 179, Loss 0.413802832365036\n",
      "[Training Epoch 5] Batch 180, Loss 0.35533422231674194\n",
      "[Training Epoch 5] Batch 181, Loss 0.38516533374786377\n",
      "[Training Epoch 5] Batch 182, Loss 0.35876134037971497\n",
      "[Training Epoch 5] Batch 183, Loss 0.3954618573188782\n",
      "[Training Epoch 5] Batch 184, Loss 0.4188482165336609\n",
      "[Training Epoch 5] Batch 185, Loss 0.4196966886520386\n",
      "[Training Epoch 5] Batch 186, Loss 0.38067424297332764\n",
      "[Training Epoch 5] Batch 187, Loss 0.3933751583099365\n",
      "[Training Epoch 5] Batch 188, Loss 0.39598917961120605\n",
      "[Training Epoch 5] Batch 189, Loss 0.38630878925323486\n",
      "[Training Epoch 5] Batch 190, Loss 0.36189860105514526\n",
      "[Training Epoch 5] Batch 191, Loss 0.37196141481399536\n",
      "[Training Epoch 5] Batch 192, Loss 0.39676904678344727\n",
      "[Training Epoch 5] Batch 193, Loss 0.39335763454437256\n",
      "[Training Epoch 5] Batch 194, Loss 0.36470046639442444\n",
      "[Training Epoch 5] Batch 195, Loss 0.42373985052108765\n",
      "[Training Epoch 5] Batch 196, Loss 0.3968011736869812\n",
      "[Training Epoch 5] Batch 197, Loss 0.405686616897583\n",
      "[Training Epoch 5] Batch 198, Loss 0.38604652881622314\n",
      "[Training Epoch 5] Batch 199, Loss 0.4159759283065796\n",
      "[Training Epoch 5] Batch 200, Loss 0.3704996109008789\n",
      "[Training Epoch 5] Batch 201, Loss 0.38413336873054504\n",
      "[Training Epoch 5] Batch 202, Loss 0.36986470222473145\n",
      "[Training Epoch 5] Batch 203, Loss 0.38720592856407166\n",
      "[Training Epoch 5] Batch 204, Loss 0.36109983921051025\n",
      "[Training Epoch 5] Batch 205, Loss 0.3780224919319153\n",
      "[Training Epoch 5] Batch 206, Loss 0.3839542269706726\n",
      "[Training Epoch 5] Batch 207, Loss 0.40862494707107544\n",
      "[Training Epoch 5] Batch 208, Loss 0.3730100095272064\n",
      "[Training Epoch 5] Batch 209, Loss 0.37066134810447693\n",
      "[Training Epoch 5] Batch 210, Loss 0.38528645038604736\n",
      "[Training Epoch 5] Batch 211, Loss 0.4034741222858429\n",
      "[Training Epoch 5] Batch 212, Loss 0.409733384847641\n",
      "[Training Epoch 5] Batch 213, Loss 0.39438432455062866\n",
      "[Training Epoch 5] Batch 214, Loss 0.3982267677783966\n",
      "[Training Epoch 5] Batch 215, Loss 0.4152577519416809\n",
      "[Training Epoch 5] Batch 216, Loss 0.37522122263908386\n",
      "[Training Epoch 5] Batch 217, Loss 0.37758755683898926\n",
      "[Training Epoch 5] Batch 218, Loss 0.37057018280029297\n",
      "[Training Epoch 5] Batch 219, Loss 0.3616792559623718\n",
      "[Training Epoch 5] Batch 220, Loss 0.3836626410484314\n",
      "[Training Epoch 5] Batch 221, Loss 0.374494731426239\n",
      "[Training Epoch 5] Batch 222, Loss 0.39517414569854736\n",
      "[Training Epoch 5] Batch 223, Loss 0.38246071338653564\n",
      "[Training Epoch 5] Batch 224, Loss 0.4027380049228668\n",
      "[Training Epoch 5] Batch 225, Loss 0.39851540327072144\n",
      "[Training Epoch 5] Batch 226, Loss 0.39711636304855347\n",
      "[Training Epoch 5] Batch 227, Loss 0.38970187306404114\n",
      "[Training Epoch 5] Batch 228, Loss 0.3872362971305847\n",
      "[Training Epoch 5] Batch 229, Loss 0.3931266963481903\n",
      "[Training Epoch 5] Batch 230, Loss 0.40414947271347046\n",
      "[Training Epoch 5] Batch 231, Loss 0.40943443775177\n",
      "[Training Epoch 5] Batch 232, Loss 0.41037121415138245\n",
      "[Training Epoch 5] Batch 233, Loss 0.4243791401386261\n",
      "[Training Epoch 5] Batch 234, Loss 0.39622777700424194\n",
      "[Training Epoch 5] Batch 235, Loss 0.3971351683139801\n",
      "[Training Epoch 5] Batch 236, Loss 0.39276599884033203\n",
      "[Training Epoch 5] Batch 237, Loss 0.4196416139602661\n",
      "[Training Epoch 5] Batch 238, Loss 0.3872019350528717\n",
      "[Training Epoch 5] Batch 239, Loss 0.35667121410369873\n",
      "[Training Epoch 5] Batch 240, Loss 0.38720279932022095\n",
      "[Training Epoch 5] Batch 241, Loss 0.38443723320961\n",
      "[Training Epoch 5] Batch 242, Loss 0.4126027524471283\n",
      "[Training Epoch 5] Batch 243, Loss 0.3881831765174866\n",
      "[Training Epoch 5] Batch 244, Loss 0.3978416919708252\n",
      "[Training Epoch 5] Batch 245, Loss 0.38555628061294556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2083.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2398.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 5] Precision = 0.3622, Recall = 0.9852\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3668864071369171\n",
      "[Training Epoch 6] Batch 1, Loss 0.389005571603775\n",
      "[Training Epoch 6] Batch 2, Loss 0.3718198835849762\n",
      "[Training Epoch 6] Batch 3, Loss 0.4059959053993225\n",
      "[Training Epoch 6] Batch 4, Loss 0.40494945645332336\n",
      "[Training Epoch 6] Batch 5, Loss 0.35683149099349976\n",
      "[Training Epoch 6] Batch 6, Loss 0.369960218667984\n",
      "[Training Epoch 6] Batch 7, Loss 0.3852355480194092\n",
      "[Training Epoch 6] Batch 8, Loss 0.40555670857429504\n",
      "[Training Epoch 6] Batch 9, Loss 0.399797260761261\n",
      "[Training Epoch 6] Batch 10, Loss 0.37157154083251953\n",
      "[Training Epoch 6] Batch 11, Loss 0.39688625931739807\n",
      "[Training Epoch 6] Batch 12, Loss 0.40227556228637695\n",
      "[Training Epoch 6] Batch 13, Loss 0.3881559669971466\n",
      "[Training Epoch 6] Batch 14, Loss 0.3546103239059448\n",
      "[Training Epoch 6] Batch 15, Loss 0.4008924663066864\n",
      "[Training Epoch 6] Batch 16, Loss 0.3830251693725586\n",
      "[Training Epoch 6] Batch 17, Loss 0.3743281364440918\n",
      "[Training Epoch 6] Batch 18, Loss 0.40214839577674866\n",
      "[Training Epoch 6] Batch 19, Loss 0.3798116147518158\n",
      "[Training Epoch 6] Batch 20, Loss 0.3804139494895935\n",
      "[Training Epoch 6] Batch 21, Loss 0.39314478635787964\n",
      "[Training Epoch 6] Batch 22, Loss 0.38093528151512146\n",
      "[Training Epoch 6] Batch 23, Loss 0.36403408646583557\n",
      "[Training Epoch 6] Batch 24, Loss 0.3889554440975189\n",
      "[Training Epoch 6] Batch 25, Loss 0.3571271300315857\n",
      "[Training Epoch 6] Batch 26, Loss 0.3826870620250702\n",
      "[Training Epoch 6] Batch 27, Loss 0.38970476388931274\n",
      "[Training Epoch 6] Batch 28, Loss 0.36339661478996277\n",
      "[Training Epoch 6] Batch 29, Loss 0.40478217601776123\n",
      "[Training Epoch 6] Batch 30, Loss 0.3777163624763489\n",
      "[Training Epoch 6] Batch 31, Loss 0.3768172860145569\n",
      "[Training Epoch 6] Batch 32, Loss 0.4085158705711365\n",
      "[Training Epoch 6] Batch 33, Loss 0.3632899522781372\n",
      "[Training Epoch 6] Batch 34, Loss 0.3712981343269348\n",
      "[Training Epoch 6] Batch 35, Loss 0.38719022274017334\n",
      "[Training Epoch 6] Batch 36, Loss 0.38558483123779297\n",
      "[Training Epoch 6] Batch 37, Loss 0.3977966010570526\n",
      "[Training Epoch 6] Batch 38, Loss 0.36388349533081055\n",
      "[Training Epoch 6] Batch 39, Loss 0.3463834524154663\n",
      "[Training Epoch 6] Batch 40, Loss 0.36859411001205444\n",
      "[Training Epoch 6] Batch 41, Loss 0.3953586220741272\n",
      "[Training Epoch 6] Batch 42, Loss 0.376128613948822\n",
      "[Training Epoch 6] Batch 43, Loss 0.4035475254058838\n",
      "[Training Epoch 6] Batch 44, Loss 0.38277941942214966\n",
      "[Training Epoch 6] Batch 45, Loss 0.3820417523384094\n",
      "[Training Epoch 6] Batch 46, Loss 0.36980679631233215\n",
      "[Training Epoch 6] Batch 47, Loss 0.38667839765548706\n",
      "[Training Epoch 6] Batch 48, Loss 0.38813111186027527\n",
      "[Training Epoch 6] Batch 49, Loss 0.38927119970321655\n",
      "[Training Epoch 6] Batch 50, Loss 0.38494032621383667\n",
      "[Training Epoch 6] Batch 51, Loss 0.3838654160499573\n",
      "[Training Epoch 6] Batch 52, Loss 0.3738759160041809\n",
      "[Training Epoch 6] Batch 53, Loss 0.3964013159275055\n",
      "[Training Epoch 6] Batch 54, Loss 0.39040881395339966\n",
      "[Training Epoch 6] Batch 55, Loss 0.370288610458374\n",
      "[Training Epoch 6] Batch 56, Loss 0.3783302903175354\n",
      "[Training Epoch 6] Batch 57, Loss 0.38074132800102234\n",
      "[Training Epoch 6] Batch 58, Loss 0.4025651812553406\n",
      "[Training Epoch 6] Batch 59, Loss 0.3751012682914734\n",
      "[Training Epoch 6] Batch 60, Loss 0.39381957054138184\n",
      "[Training Epoch 6] Batch 61, Loss 0.3643658757209778\n",
      "[Training Epoch 6] Batch 62, Loss 0.40801072120666504\n",
      "[Training Epoch 6] Batch 63, Loss 0.38602209091186523\n",
      "[Training Epoch 6] Batch 64, Loss 0.3805219233036041\n",
      "[Training Epoch 6] Batch 65, Loss 0.36207231879234314\n",
      "[Training Epoch 6] Batch 66, Loss 0.3803328275680542\n",
      "[Training Epoch 6] Batch 67, Loss 0.34263932704925537\n",
      "[Training Epoch 6] Batch 68, Loss 0.3889590799808502\n",
      "[Training Epoch 6] Batch 69, Loss 0.39526551961898804\n",
      "[Training Epoch 6] Batch 70, Loss 0.39016690850257874\n",
      "[Training Epoch 6] Batch 71, Loss 0.38513654470443726\n",
      "[Training Epoch 6] Batch 72, Loss 0.3841792643070221\n",
      "[Training Epoch 6] Batch 73, Loss 0.40657904744148254\n",
      "[Training Epoch 6] Batch 74, Loss 0.37939202785491943\n",
      "[Training Epoch 6] Batch 75, Loss 0.39749234914779663\n",
      "[Training Epoch 6] Batch 76, Loss 0.394970178604126\n",
      "[Training Epoch 6] Batch 77, Loss 0.37771186232566833\n",
      "[Training Epoch 6] Batch 78, Loss 0.4027469754219055\n",
      "[Training Epoch 6] Batch 79, Loss 0.35022562742233276\n",
      "[Training Epoch 6] Batch 80, Loss 0.40060681104660034\n",
      "[Training Epoch 6] Batch 81, Loss 0.38750123977661133\n",
      "[Training Epoch 6] Batch 82, Loss 0.38583850860595703\n",
      "[Training Epoch 6] Batch 83, Loss 0.40248388051986694\n",
      "[Training Epoch 6] Batch 84, Loss 0.362957626581192\n",
      "[Training Epoch 6] Batch 85, Loss 0.3613954484462738\n",
      "[Training Epoch 6] Batch 86, Loss 0.40407148003578186\n",
      "[Training Epoch 6] Batch 87, Loss 0.36661526560783386\n",
      "[Training Epoch 6] Batch 88, Loss 0.3770568370819092\n",
      "[Training Epoch 6] Batch 89, Loss 0.3748241662979126\n",
      "[Training Epoch 6] Batch 90, Loss 0.3743300139904022\n",
      "[Training Epoch 6] Batch 91, Loss 0.40257400274276733\n",
      "[Training Epoch 6] Batch 92, Loss 0.4156755208969116\n",
      "[Training Epoch 6] Batch 93, Loss 0.38776418566703796\n",
      "[Training Epoch 6] Batch 94, Loss 0.3876693844795227\n",
      "[Training Epoch 6] Batch 95, Loss 0.433313250541687\n",
      "[Training Epoch 6] Batch 96, Loss 0.4008893072605133\n",
      "[Training Epoch 6] Batch 97, Loss 0.37774577736854553\n",
      "[Training Epoch 6] Batch 98, Loss 0.38787344098091125\n",
      "[Training Epoch 6] Batch 99, Loss 0.3780226707458496\n",
      "[Training Epoch 6] Batch 100, Loss 0.3769971430301666\n",
      "[Training Epoch 6] Batch 101, Loss 0.3785621225833893\n",
      "[Training Epoch 6] Batch 102, Loss 0.3842390775680542\n",
      "[Training Epoch 6] Batch 103, Loss 0.3755243718624115\n",
      "[Training Epoch 6] Batch 104, Loss 0.3687601685523987\n",
      "[Training Epoch 6] Batch 105, Loss 0.37424421310424805\n",
      "[Training Epoch 6] Batch 106, Loss 0.3619728088378906\n",
      "[Training Epoch 6] Batch 107, Loss 0.3661777675151825\n",
      "[Training Epoch 6] Batch 108, Loss 0.37816929817199707\n",
      "[Training Epoch 6] Batch 109, Loss 0.37815845012664795\n",
      "[Training Epoch 6] Batch 110, Loss 0.3645344376564026\n",
      "[Training Epoch 6] Batch 111, Loss 0.39916500449180603\n",
      "[Training Epoch 6] Batch 112, Loss 0.41612640023231506\n",
      "[Training Epoch 6] Batch 113, Loss 0.3975096344947815\n",
      "[Training Epoch 6] Batch 114, Loss 0.3887712061405182\n",
      "[Training Epoch 6] Batch 115, Loss 0.39366739988327026\n",
      "[Training Epoch 6] Batch 116, Loss 0.3515160381793976\n",
      "[Training Epoch 6] Batch 117, Loss 0.4014289379119873\n",
      "[Training Epoch 6] Batch 118, Loss 0.360146701335907\n",
      "[Training Epoch 6] Batch 119, Loss 0.3440006375312805\n",
      "[Training Epoch 6] Batch 120, Loss 0.41358211636543274\n",
      "[Training Epoch 6] Batch 121, Loss 0.3873027563095093\n",
      "[Training Epoch 6] Batch 122, Loss 0.37193870544433594\n",
      "[Training Epoch 6] Batch 123, Loss 0.3890002369880676\n",
      "[Training Epoch 6] Batch 124, Loss 0.36078542470932007\n",
      "[Training Epoch 6] Batch 125, Loss 0.38376542925834656\n",
      "[Training Epoch 6] Batch 126, Loss 0.360167920589447\n",
      "[Training Epoch 6] Batch 127, Loss 0.3825572729110718\n",
      "[Training Epoch 6] Batch 128, Loss 0.39348936080932617\n",
      "[Training Epoch 6] Batch 129, Loss 0.3708341121673584\n",
      "[Training Epoch 6] Batch 130, Loss 0.3807275593280792\n",
      "[Training Epoch 6] Batch 131, Loss 0.416904091835022\n",
      "[Training Epoch 6] Batch 132, Loss 0.3708447515964508\n",
      "[Training Epoch 6] Batch 133, Loss 0.3880353569984436\n",
      "[Training Epoch 6] Batch 134, Loss 0.41059717535972595\n",
      "[Training Epoch 6] Batch 135, Loss 0.35585904121398926\n",
      "[Training Epoch 6] Batch 136, Loss 0.35258325934410095\n",
      "[Training Epoch 6] Batch 137, Loss 0.36266812682151794\n",
      "[Training Epoch 6] Batch 138, Loss 0.3982888460159302\n",
      "[Training Epoch 6] Batch 139, Loss 0.36622196435928345\n",
      "[Training Epoch 6] Batch 140, Loss 0.37732401490211487\n",
      "[Training Epoch 6] Batch 141, Loss 0.3933209776878357\n",
      "[Training Epoch 6] Batch 142, Loss 0.358523964881897\n",
      "[Training Epoch 6] Batch 143, Loss 0.39008402824401855\n",
      "[Training Epoch 6] Batch 144, Loss 0.4108206331729889\n",
      "[Training Epoch 6] Batch 145, Loss 0.3951890170574188\n",
      "[Training Epoch 6] Batch 146, Loss 0.363259881734848\n",
      "[Training Epoch 6] Batch 147, Loss 0.37886616587638855\n",
      "[Training Epoch 6] Batch 148, Loss 0.4058428406715393\n",
      "[Training Epoch 6] Batch 149, Loss 0.393217533826828\n",
      "[Training Epoch 6] Batch 150, Loss 0.37903648614883423\n",
      "[Training Epoch 6] Batch 151, Loss 0.38703489303588867\n",
      "[Training Epoch 6] Batch 152, Loss 0.3825864791870117\n",
      "[Training Epoch 6] Batch 153, Loss 0.3593469262123108\n",
      "[Training Epoch 6] Batch 154, Loss 0.3874778151512146\n",
      "[Training Epoch 6] Batch 155, Loss 0.36104264855384827\n",
      "[Training Epoch 6] Batch 156, Loss 0.3819712996482849\n",
      "[Training Epoch 6] Batch 157, Loss 0.36784905195236206\n",
      "[Training Epoch 6] Batch 158, Loss 0.369498610496521\n",
      "[Training Epoch 6] Batch 159, Loss 0.38337722420692444\n",
      "[Training Epoch 6] Batch 160, Loss 0.3944547772407532\n",
      "[Training Epoch 6] Batch 161, Loss 0.40154534578323364\n",
      "[Training Epoch 6] Batch 162, Loss 0.35693061351776123\n",
      "[Training Epoch 6] Batch 163, Loss 0.3964439630508423\n",
      "[Training Epoch 6] Batch 164, Loss 0.3803238272666931\n",
      "[Training Epoch 6] Batch 165, Loss 0.40511608123779297\n",
      "[Training Epoch 6] Batch 166, Loss 0.36812761425971985\n",
      "[Training Epoch 6] Batch 167, Loss 0.4124945402145386\n",
      "[Training Epoch 6] Batch 168, Loss 0.3655244708061218\n",
      "[Training Epoch 6] Batch 169, Loss 0.39012837409973145\n",
      "[Training Epoch 6] Batch 170, Loss 0.37956222891807556\n",
      "[Training Epoch 6] Batch 171, Loss 0.3863752782344818\n",
      "[Training Epoch 6] Batch 172, Loss 0.34154385328292847\n",
      "[Training Epoch 6] Batch 173, Loss 0.3835071921348572\n",
      "[Training Epoch 6] Batch 174, Loss 0.37142831087112427\n",
      "[Training Epoch 6] Batch 175, Loss 0.37219148874282837\n",
      "[Training Epoch 6] Batch 176, Loss 0.3698658347129822\n",
      "[Training Epoch 6] Batch 177, Loss 0.3911712169647217\n",
      "[Training Epoch 6] Batch 178, Loss 0.3532559275627136\n",
      "[Training Epoch 6] Batch 179, Loss 0.3862929940223694\n",
      "[Training Epoch 6] Batch 180, Loss 0.36726662516593933\n",
      "[Training Epoch 6] Batch 181, Loss 0.39807432889938354\n",
      "[Training Epoch 6] Batch 182, Loss 0.37574437260627747\n",
      "[Training Epoch 6] Batch 183, Loss 0.3961208164691925\n",
      "[Training Epoch 6] Batch 184, Loss 0.37913283705711365\n",
      "[Training Epoch 6] Batch 185, Loss 0.35075825452804565\n",
      "[Training Epoch 6] Batch 186, Loss 0.4001331925392151\n",
      "[Training Epoch 6] Batch 187, Loss 0.36810606718063354\n",
      "[Training Epoch 6] Batch 188, Loss 0.41434580087661743\n",
      "[Training Epoch 6] Batch 189, Loss 0.41494548320770264\n",
      "[Training Epoch 6] Batch 190, Loss 0.4099692106246948\n",
      "[Training Epoch 6] Batch 191, Loss 0.3698617219924927\n",
      "[Training Epoch 6] Batch 192, Loss 0.3828204870223999\n",
      "[Training Epoch 6] Batch 193, Loss 0.38763636350631714\n",
      "[Training Epoch 6] Batch 194, Loss 0.3916688561439514\n",
      "[Training Epoch 6] Batch 195, Loss 0.39207515120506287\n",
      "[Training Epoch 6] Batch 196, Loss 0.39581936597824097\n",
      "[Training Epoch 6] Batch 197, Loss 0.40749800205230713\n",
      "[Training Epoch 6] Batch 198, Loss 0.3779356777667999\n",
      "[Training Epoch 6] Batch 199, Loss 0.3799930214881897\n",
      "[Training Epoch 6] Batch 200, Loss 0.3714921474456787\n",
      "[Training Epoch 6] Batch 201, Loss 0.38322722911834717\n",
      "[Training Epoch 6] Batch 202, Loss 0.36739015579223633\n",
      "[Training Epoch 6] Batch 203, Loss 0.4195200800895691\n",
      "[Training Epoch 6] Batch 204, Loss 0.3684242367744446\n",
      "[Training Epoch 6] Batch 205, Loss 0.4178624451160431\n",
      "[Training Epoch 6] Batch 206, Loss 0.38430994749069214\n",
      "[Training Epoch 6] Batch 207, Loss 0.3860313892364502\n",
      "[Training Epoch 6] Batch 208, Loss 0.39707255363464355\n",
      "[Training Epoch 6] Batch 209, Loss 0.37888258695602417\n",
      "[Training Epoch 6] Batch 210, Loss 0.37631911039352417\n",
      "[Training Epoch 6] Batch 211, Loss 0.40289953351020813\n",
      "[Training Epoch 6] Batch 212, Loss 0.36703288555145264\n",
      "[Training Epoch 6] Batch 213, Loss 0.3866347074508667\n",
      "[Training Epoch 6] Batch 214, Loss 0.3805285096168518\n",
      "[Training Epoch 6] Batch 215, Loss 0.4055490493774414\n",
      "[Training Epoch 6] Batch 216, Loss 0.37118518352508545\n",
      "[Training Epoch 6] Batch 217, Loss 0.414272665977478\n",
      "[Training Epoch 6] Batch 218, Loss 0.42094704508781433\n",
      "[Training Epoch 6] Batch 219, Loss 0.37637272477149963\n",
      "[Training Epoch 6] Batch 220, Loss 0.3647996485233307\n",
      "[Training Epoch 6] Batch 221, Loss 0.36835145950317383\n",
      "[Training Epoch 6] Batch 222, Loss 0.4004850685596466\n",
      "[Training Epoch 6] Batch 223, Loss 0.38945674896240234\n",
      "[Training Epoch 6] Batch 224, Loss 0.3893548846244812\n",
      "[Training Epoch 6] Batch 225, Loss 0.4028581976890564\n",
      "[Training Epoch 6] Batch 226, Loss 0.39082878828048706\n",
      "[Training Epoch 6] Batch 227, Loss 0.3745768070220947\n",
      "[Training Epoch 6] Batch 228, Loss 0.37363356351852417\n",
      "[Training Epoch 6] Batch 229, Loss 0.3523446321487427\n",
      "[Training Epoch 6] Batch 230, Loss 0.3739458918571472\n",
      "[Training Epoch 6] Batch 231, Loss 0.36116915941238403\n",
      "[Training Epoch 6] Batch 232, Loss 0.3818944990634918\n",
      "[Training Epoch 6] Batch 233, Loss 0.36733126640319824\n",
      "[Training Epoch 6] Batch 234, Loss 0.3921149969100952\n",
      "[Training Epoch 6] Batch 235, Loss 0.38648882508277893\n",
      "[Training Epoch 6] Batch 236, Loss 0.3662314713001251\n",
      "[Training Epoch 6] Batch 237, Loss 0.37601998448371887\n",
      "[Training Epoch 6] Batch 238, Loss 0.37124162912368774\n",
      "[Training Epoch 6] Batch 239, Loss 0.397762656211853\n",
      "[Training Epoch 6] Batch 240, Loss 0.3818519711494446\n",
      "[Training Epoch 6] Batch 241, Loss 0.3928481936454773\n",
      "[Training Epoch 6] Batch 242, Loss 0.38455143570899963\n",
      "[Training Epoch 6] Batch 243, Loss 0.3831526041030884\n",
      "[Training Epoch 6] Batch 244, Loss 0.38358137011528015\n",
      "[Training Epoch 6] Batch 245, Loss 0.3758946359157562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2032.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2391.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 6] Precision = 0.3623, Recall = 0.9865\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.35956868529319763\n",
      "[Training Epoch 7] Batch 1, Loss 0.3952365517616272\n",
      "[Training Epoch 7] Batch 2, Loss 0.36510640382766724\n",
      "[Training Epoch 7] Batch 3, Loss 0.3571893572807312\n",
      "[Training Epoch 7] Batch 4, Loss 0.3786967992782593\n",
      "[Training Epoch 7] Batch 5, Loss 0.39059358835220337\n",
      "[Training Epoch 7] Batch 6, Loss 0.38881590962409973\n",
      "[Training Epoch 7] Batch 7, Loss 0.38552868366241455\n",
      "[Training Epoch 7] Batch 8, Loss 0.38961803913116455\n",
      "[Training Epoch 7] Batch 9, Loss 0.37549054622650146\n",
      "[Training Epoch 7] Batch 10, Loss 0.38997340202331543\n",
      "[Training Epoch 7] Batch 11, Loss 0.36988335847854614\n",
      "[Training Epoch 7] Batch 12, Loss 0.3857463002204895\n",
      "[Training Epoch 7] Batch 13, Loss 0.3757472336292267\n",
      "[Training Epoch 7] Batch 14, Loss 0.39818188548088074\n",
      "[Training Epoch 7] Batch 15, Loss 0.37507641315460205\n",
      "[Training Epoch 7] Batch 16, Loss 0.39272254705429077\n",
      "[Training Epoch 7] Batch 17, Loss 0.36124685406684875\n",
      "[Training Epoch 7] Batch 18, Loss 0.3966745138168335\n",
      "[Training Epoch 7] Batch 19, Loss 0.3734583854675293\n",
      "[Training Epoch 7] Batch 20, Loss 0.3833032250404358\n",
      "[Training Epoch 7] Batch 21, Loss 0.3887723684310913\n",
      "[Training Epoch 7] Batch 22, Loss 0.3641246557235718\n",
      "[Training Epoch 7] Batch 23, Loss 0.37756240367889404\n",
      "[Training Epoch 7] Batch 24, Loss 0.3761298656463623\n",
      "[Training Epoch 7] Batch 25, Loss 0.37065625190734863\n",
      "[Training Epoch 7] Batch 26, Loss 0.39037424325942993\n",
      "[Training Epoch 7] Batch 27, Loss 0.3513087034225464\n",
      "[Training Epoch 7] Batch 28, Loss 0.34908702969551086\n",
      "[Training Epoch 7] Batch 29, Loss 0.3695407509803772\n",
      "[Training Epoch 7] Batch 30, Loss 0.380618155002594\n",
      "[Training Epoch 7] Batch 31, Loss 0.3701649010181427\n",
      "[Training Epoch 7] Batch 32, Loss 0.36789441108703613\n",
      "[Training Epoch 7] Batch 33, Loss 0.3819422721862793\n",
      "[Training Epoch 7] Batch 34, Loss 0.3817892074584961\n",
      "[Training Epoch 7] Batch 35, Loss 0.3934762477874756\n",
      "[Training Epoch 7] Batch 36, Loss 0.38071876764297485\n",
      "[Training Epoch 7] Batch 37, Loss 0.374287486076355\n",
      "[Training Epoch 7] Batch 38, Loss 0.3547249436378479\n",
      "[Training Epoch 7] Batch 39, Loss 0.37799960374832153\n",
      "[Training Epoch 7] Batch 40, Loss 0.3792528808116913\n",
      "[Training Epoch 7] Batch 41, Loss 0.34412962198257446\n",
      "[Training Epoch 7] Batch 42, Loss 0.38475966453552246\n",
      "[Training Epoch 7] Batch 43, Loss 0.34830063581466675\n",
      "[Training Epoch 7] Batch 44, Loss 0.3936083912849426\n",
      "[Training Epoch 7] Batch 45, Loss 0.3630870580673218\n",
      "[Training Epoch 7] Batch 46, Loss 0.38593149185180664\n",
      "[Training Epoch 7] Batch 47, Loss 0.3787122368812561\n",
      "[Training Epoch 7] Batch 48, Loss 0.36800289154052734\n",
      "[Training Epoch 7] Batch 49, Loss 0.38814282417297363\n",
      "[Training Epoch 7] Batch 50, Loss 0.3575439453125\n",
      "[Training Epoch 7] Batch 51, Loss 0.376567542552948\n",
      "[Training Epoch 7] Batch 52, Loss 0.3742402493953705\n",
      "[Training Epoch 7] Batch 53, Loss 0.38039326667785645\n",
      "[Training Epoch 7] Batch 54, Loss 0.34489548206329346\n",
      "[Training Epoch 7] Batch 55, Loss 0.38086387515068054\n",
      "[Training Epoch 7] Batch 56, Loss 0.3866302967071533\n",
      "[Training Epoch 7] Batch 57, Loss 0.37383711338043213\n",
      "[Training Epoch 7] Batch 58, Loss 0.3654865026473999\n",
      "[Training Epoch 7] Batch 59, Loss 0.38288426399230957\n",
      "[Training Epoch 7] Batch 60, Loss 0.3925667703151703\n",
      "[Training Epoch 7] Batch 61, Loss 0.3818202614784241\n",
      "[Training Epoch 7] Batch 62, Loss 0.3713478744029999\n",
      "[Training Epoch 7] Batch 63, Loss 0.402154803276062\n",
      "[Training Epoch 7] Batch 64, Loss 0.37642979621887207\n",
      "[Training Epoch 7] Batch 65, Loss 0.36035609245300293\n",
      "[Training Epoch 7] Batch 66, Loss 0.35102811455726624\n",
      "[Training Epoch 7] Batch 67, Loss 0.3836854100227356\n",
      "[Training Epoch 7] Batch 68, Loss 0.3587222695350647\n",
      "[Training Epoch 7] Batch 69, Loss 0.38722875714302063\n",
      "[Training Epoch 7] Batch 70, Loss 0.3709670901298523\n",
      "[Training Epoch 7] Batch 71, Loss 0.38853636384010315\n",
      "[Training Epoch 7] Batch 72, Loss 0.37673643231391907\n",
      "[Training Epoch 7] Batch 73, Loss 0.3598402142524719\n",
      "[Training Epoch 7] Batch 74, Loss 0.34731051325798035\n",
      "[Training Epoch 7] Batch 75, Loss 0.370636522769928\n",
      "[Training Epoch 7] Batch 76, Loss 0.388528436422348\n",
      "[Training Epoch 7] Batch 77, Loss 0.37712913751602173\n",
      "[Training Epoch 7] Batch 78, Loss 0.34476447105407715\n",
      "[Training Epoch 7] Batch 79, Loss 0.3805079162120819\n",
      "[Training Epoch 7] Batch 80, Loss 0.39663511514663696\n",
      "[Training Epoch 7] Batch 81, Loss 0.37468063831329346\n",
      "[Training Epoch 7] Batch 82, Loss 0.3969396948814392\n",
      "[Training Epoch 7] Batch 83, Loss 0.37951481342315674\n",
      "[Training Epoch 7] Batch 84, Loss 0.42078858613967896\n",
      "[Training Epoch 7] Batch 85, Loss 0.38510239124298096\n",
      "[Training Epoch 7] Batch 86, Loss 0.39645057916641235\n",
      "[Training Epoch 7] Batch 87, Loss 0.3519380986690521\n",
      "[Training Epoch 7] Batch 88, Loss 0.3837552070617676\n",
      "[Training Epoch 7] Batch 89, Loss 0.3543164134025574\n",
      "[Training Epoch 7] Batch 90, Loss 0.3895605802536011\n",
      "[Training Epoch 7] Batch 91, Loss 0.3725391924381256\n",
      "[Training Epoch 7] Batch 92, Loss 0.36062997579574585\n",
      "[Training Epoch 7] Batch 93, Loss 0.38965779542922974\n",
      "[Training Epoch 7] Batch 94, Loss 0.34736111760139465\n",
      "[Training Epoch 7] Batch 95, Loss 0.3611038625240326\n",
      "[Training Epoch 7] Batch 96, Loss 0.39232051372528076\n",
      "[Training Epoch 7] Batch 97, Loss 0.36419087648391724\n",
      "[Training Epoch 7] Batch 98, Loss 0.3704913556575775\n",
      "[Training Epoch 7] Batch 99, Loss 0.37139153480529785\n",
      "[Training Epoch 7] Batch 100, Loss 0.3381633162498474\n",
      "[Training Epoch 7] Batch 101, Loss 0.40250077843666077\n",
      "[Training Epoch 7] Batch 102, Loss 0.3981698751449585\n",
      "[Training Epoch 7] Batch 103, Loss 0.36114490032196045\n",
      "[Training Epoch 7] Batch 104, Loss 0.3973988890647888\n",
      "[Training Epoch 7] Batch 105, Loss 0.365425705909729\n",
      "[Training Epoch 7] Batch 106, Loss 0.3800881505012512\n",
      "[Training Epoch 7] Batch 107, Loss 0.379535436630249\n",
      "[Training Epoch 7] Batch 108, Loss 0.36322665214538574\n",
      "[Training Epoch 7] Batch 109, Loss 0.3991541266441345\n",
      "[Training Epoch 7] Batch 110, Loss 0.3655928373336792\n",
      "[Training Epoch 7] Batch 111, Loss 0.3680077791213989\n",
      "[Training Epoch 7] Batch 112, Loss 0.39387089014053345\n",
      "[Training Epoch 7] Batch 113, Loss 0.36706575751304626\n",
      "[Training Epoch 7] Batch 114, Loss 0.40143078565597534\n",
      "[Training Epoch 7] Batch 115, Loss 0.36806613206863403\n",
      "[Training Epoch 7] Batch 116, Loss 0.36294057965278625\n",
      "[Training Epoch 7] Batch 117, Loss 0.3632490634918213\n",
      "[Training Epoch 7] Batch 118, Loss 0.390278160572052\n",
      "[Training Epoch 7] Batch 119, Loss 0.3794352412223816\n",
      "[Training Epoch 7] Batch 120, Loss 0.36979782581329346\n",
      "[Training Epoch 7] Batch 121, Loss 0.39042001962661743\n",
      "[Training Epoch 7] Batch 122, Loss 0.41603416204452515\n",
      "[Training Epoch 7] Batch 123, Loss 0.387643039226532\n",
      "[Training Epoch 7] Batch 124, Loss 0.3901582956314087\n",
      "[Training Epoch 7] Batch 125, Loss 0.4059023857116699\n",
      "[Training Epoch 7] Batch 126, Loss 0.39247453212738037\n",
      "[Training Epoch 7] Batch 127, Loss 0.36499959230422974\n",
      "[Training Epoch 7] Batch 128, Loss 0.3840242624282837\n",
      "[Training Epoch 7] Batch 129, Loss 0.3673522472381592\n",
      "[Training Epoch 7] Batch 130, Loss 0.41185668110847473\n",
      "[Training Epoch 7] Batch 131, Loss 0.35534870624542236\n",
      "[Training Epoch 7] Batch 132, Loss 0.36565542221069336\n",
      "[Training Epoch 7] Batch 133, Loss 0.38130396604537964\n",
      "[Training Epoch 7] Batch 134, Loss 0.3953382968902588\n",
      "[Training Epoch 7] Batch 135, Loss 0.3676404058933258\n",
      "[Training Epoch 7] Batch 136, Loss 0.3946418762207031\n",
      "[Training Epoch 7] Batch 137, Loss 0.3731473386287689\n",
      "[Training Epoch 7] Batch 138, Loss 0.40906909108161926\n",
      "[Training Epoch 7] Batch 139, Loss 0.3737781047821045\n",
      "[Training Epoch 7] Batch 140, Loss 0.3668975234031677\n",
      "[Training Epoch 7] Batch 141, Loss 0.3754711151123047\n",
      "[Training Epoch 7] Batch 142, Loss 0.3689378798007965\n",
      "[Training Epoch 7] Batch 143, Loss 0.42778563499450684\n",
      "[Training Epoch 7] Batch 144, Loss 0.39395785331726074\n",
      "[Training Epoch 7] Batch 145, Loss 0.3596152663230896\n",
      "[Training Epoch 7] Batch 146, Loss 0.33333784341812134\n",
      "[Training Epoch 7] Batch 147, Loss 0.3888593316078186\n",
      "[Training Epoch 7] Batch 148, Loss 0.3858354687690735\n",
      "[Training Epoch 7] Batch 149, Loss 0.359530508518219\n",
      "[Training Epoch 7] Batch 150, Loss 0.35944700241088867\n",
      "[Training Epoch 7] Batch 151, Loss 0.38061362504959106\n",
      "[Training Epoch 7] Batch 152, Loss 0.37859630584716797\n",
      "[Training Epoch 7] Batch 153, Loss 0.37728258967399597\n",
      "[Training Epoch 7] Batch 154, Loss 0.38229838013648987\n",
      "[Training Epoch 7] Batch 155, Loss 0.392467737197876\n",
      "[Training Epoch 7] Batch 156, Loss 0.35911089181900024\n",
      "[Training Epoch 7] Batch 157, Loss 0.3637400269508362\n",
      "[Training Epoch 7] Batch 158, Loss 0.3407217264175415\n",
      "[Training Epoch 7] Batch 159, Loss 0.3956582844257355\n",
      "[Training Epoch 7] Batch 160, Loss 0.3812640309333801\n",
      "[Training Epoch 7] Batch 161, Loss 0.37211525440216064\n",
      "[Training Epoch 7] Batch 162, Loss 0.3326150178909302\n",
      "[Training Epoch 7] Batch 163, Loss 0.37974414229393005\n",
      "[Training Epoch 7] Batch 164, Loss 0.3593128025531769\n",
      "[Training Epoch 7] Batch 165, Loss 0.3691433072090149\n",
      "[Training Epoch 7] Batch 166, Loss 0.36922115087509155\n",
      "[Training Epoch 7] Batch 167, Loss 0.36279749870300293\n",
      "[Training Epoch 7] Batch 168, Loss 0.36777618527412415\n",
      "[Training Epoch 7] Batch 169, Loss 0.36991196870803833\n",
      "[Training Epoch 7] Batch 170, Loss 0.3750748634338379\n",
      "[Training Epoch 7] Batch 171, Loss 0.37307628989219666\n",
      "[Training Epoch 7] Batch 172, Loss 0.37808850407600403\n",
      "[Training Epoch 7] Batch 173, Loss 0.3651062250137329\n",
      "[Training Epoch 7] Batch 174, Loss 0.37294313311576843\n",
      "[Training Epoch 7] Batch 175, Loss 0.36076587438583374\n",
      "[Training Epoch 7] Batch 176, Loss 0.3991730809211731\n",
      "[Training Epoch 7] Batch 177, Loss 0.39548733830451965\n",
      "[Training Epoch 7] Batch 178, Loss 0.3646084666252136\n",
      "[Training Epoch 7] Batch 179, Loss 0.38056421279907227\n",
      "[Training Epoch 7] Batch 180, Loss 0.42011040449142456\n",
      "[Training Epoch 7] Batch 181, Loss 0.41273534297943115\n",
      "[Training Epoch 7] Batch 182, Loss 0.39593708515167236\n",
      "[Training Epoch 7] Batch 183, Loss 0.3687959611415863\n",
      "[Training Epoch 7] Batch 184, Loss 0.3774467706680298\n",
      "[Training Epoch 7] Batch 185, Loss 0.3650932312011719\n",
      "[Training Epoch 7] Batch 186, Loss 0.3499322831630707\n",
      "[Training Epoch 7] Batch 187, Loss 0.3445757031440735\n",
      "[Training Epoch 7] Batch 188, Loss 0.40279412269592285\n",
      "[Training Epoch 7] Batch 189, Loss 0.3949176073074341\n",
      "[Training Epoch 7] Batch 190, Loss 0.3877893090248108\n",
      "[Training Epoch 7] Batch 191, Loss 0.3838191032409668\n",
      "[Training Epoch 7] Batch 192, Loss 0.3814179003238678\n",
      "[Training Epoch 7] Batch 193, Loss 0.3717895448207855\n",
      "[Training Epoch 7] Batch 194, Loss 0.3782994747161865\n",
      "[Training Epoch 7] Batch 195, Loss 0.3778756856918335\n",
      "[Training Epoch 7] Batch 196, Loss 0.3817538022994995\n",
      "[Training Epoch 7] Batch 197, Loss 0.366899311542511\n",
      "[Training Epoch 7] Batch 198, Loss 0.35749343037605286\n",
      "[Training Epoch 7] Batch 199, Loss 0.3715834617614746\n",
      "[Training Epoch 7] Batch 200, Loss 0.37803512811660767\n",
      "[Training Epoch 7] Batch 201, Loss 0.40278375148773193\n",
      "[Training Epoch 7] Batch 202, Loss 0.4236539304256439\n",
      "[Training Epoch 7] Batch 203, Loss 0.36380305886268616\n",
      "[Training Epoch 7] Batch 204, Loss 0.37974420189857483\n",
      "[Training Epoch 7] Batch 205, Loss 0.3734295964241028\n",
      "[Training Epoch 7] Batch 206, Loss 0.3649522066116333\n",
      "[Training Epoch 7] Batch 207, Loss 0.3482522666454315\n",
      "[Training Epoch 7] Batch 208, Loss 0.35536307096481323\n",
      "[Training Epoch 7] Batch 209, Loss 0.35096606612205505\n",
      "[Training Epoch 7] Batch 210, Loss 0.37218716740608215\n",
      "[Training Epoch 7] Batch 211, Loss 0.37568867206573486\n",
      "[Training Epoch 7] Batch 212, Loss 0.38780564069747925\n",
      "[Training Epoch 7] Batch 213, Loss 0.3467381000518799\n",
      "[Training Epoch 7] Batch 214, Loss 0.3917413353919983\n",
      "[Training Epoch 7] Batch 215, Loss 0.36997348070144653\n",
      "[Training Epoch 7] Batch 216, Loss 0.3852105140686035\n",
      "[Training Epoch 7] Batch 217, Loss 0.3761764168739319\n",
      "[Training Epoch 7] Batch 218, Loss 0.358370304107666\n",
      "[Training Epoch 7] Batch 219, Loss 0.356804221868515\n",
      "[Training Epoch 7] Batch 220, Loss 0.38956332206726074\n",
      "[Training Epoch 7] Batch 221, Loss 0.38303494453430176\n",
      "[Training Epoch 7] Batch 222, Loss 0.40440940856933594\n",
      "[Training Epoch 7] Batch 223, Loss 0.3793489336967468\n",
      "[Training Epoch 7] Batch 224, Loss 0.38937824964523315\n",
      "[Training Epoch 7] Batch 225, Loss 0.3662722110748291\n",
      "[Training Epoch 7] Batch 226, Loss 0.3598553538322449\n",
      "[Training Epoch 7] Batch 227, Loss 0.35812586545944214\n",
      "[Training Epoch 7] Batch 228, Loss 0.35688161849975586\n",
      "[Training Epoch 7] Batch 229, Loss 0.36330896615982056\n",
      "[Training Epoch 7] Batch 230, Loss 0.4004253149032593\n",
      "[Training Epoch 7] Batch 231, Loss 0.3762584328651428\n",
      "[Training Epoch 7] Batch 232, Loss 0.3958008885383606\n",
      "[Training Epoch 7] Batch 233, Loss 0.365494966506958\n",
      "[Training Epoch 7] Batch 234, Loss 0.3531588315963745\n",
      "[Training Epoch 7] Batch 235, Loss 0.38670626282691956\n",
      "[Training Epoch 7] Batch 236, Loss 0.3659629821777344\n",
      "[Training Epoch 7] Batch 237, Loss 0.363091379404068\n",
      "[Training Epoch 7] Batch 238, Loss 0.35632067918777466\n",
      "[Training Epoch 7] Batch 239, Loss 0.3924754858016968\n",
      "[Training Epoch 7] Batch 240, Loss 0.36654752492904663\n",
      "[Training Epoch 7] Batch 241, Loss 0.3500829339027405\n",
      "[Training Epoch 7] Batch 242, Loss 0.3726065754890442\n",
      "[Training Epoch 7] Batch 243, Loss 0.3773561716079712\n",
      "[Training Epoch 7] Batch 244, Loss 0.34983399510383606\n",
      "[Training Epoch 7] Batch 245, Loss 0.3738066554069519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2094.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2399.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 7] Precision = 0.3624, Recall = 0.9873\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.34746578335762024\n",
      "[Training Epoch 8] Batch 1, Loss 0.39401519298553467\n",
      "[Training Epoch 8] Batch 2, Loss 0.3811030089855194\n",
      "[Training Epoch 8] Batch 3, Loss 0.3784690499305725\n",
      "[Training Epoch 8] Batch 4, Loss 0.3756648302078247\n",
      "[Training Epoch 8] Batch 5, Loss 0.36912834644317627\n",
      "[Training Epoch 8] Batch 6, Loss 0.36934393644332886\n",
      "[Training Epoch 8] Batch 7, Loss 0.3802112340927124\n",
      "[Training Epoch 8] Batch 8, Loss 0.3592027425765991\n",
      "[Training Epoch 8] Batch 9, Loss 0.3800729513168335\n",
      "[Training Epoch 8] Batch 10, Loss 0.36840617656707764\n",
      "[Training Epoch 8] Batch 11, Loss 0.3807467818260193\n",
      "[Training Epoch 8] Batch 12, Loss 0.34802019596099854\n",
      "[Training Epoch 8] Batch 13, Loss 0.4003921151161194\n",
      "[Training Epoch 8] Batch 14, Loss 0.35217028856277466\n",
      "[Training Epoch 8] Batch 15, Loss 0.358401358127594\n",
      "[Training Epoch 8] Batch 16, Loss 0.36854320764541626\n",
      "[Training Epoch 8] Batch 17, Loss 0.36158251762390137\n",
      "[Training Epoch 8] Batch 18, Loss 0.3641512989997864\n",
      "[Training Epoch 8] Batch 19, Loss 0.3727305829524994\n",
      "[Training Epoch 8] Batch 20, Loss 0.38447463512420654\n",
      "[Training Epoch 8] Batch 21, Loss 0.3439982831478119\n",
      "[Training Epoch 8] Batch 22, Loss 0.3598259389400482\n",
      "[Training Epoch 8] Batch 23, Loss 0.3522270619869232\n",
      "[Training Epoch 8] Batch 24, Loss 0.35253244638442993\n",
      "[Training Epoch 8] Batch 25, Loss 0.35578668117523193\n",
      "[Training Epoch 8] Batch 26, Loss 0.3566754460334778\n",
      "[Training Epoch 8] Batch 27, Loss 0.37367820739746094\n",
      "[Training Epoch 8] Batch 28, Loss 0.38158491253852844\n",
      "[Training Epoch 8] Batch 29, Loss 0.3823501765727997\n",
      "[Training Epoch 8] Batch 30, Loss 0.3733188509941101\n",
      "[Training Epoch 8] Batch 31, Loss 0.3509752154350281\n",
      "[Training Epoch 8] Batch 32, Loss 0.3940410912036896\n",
      "[Training Epoch 8] Batch 33, Loss 0.37396663427352905\n",
      "[Training Epoch 8] Batch 34, Loss 0.3637291193008423\n",
      "[Training Epoch 8] Batch 35, Loss 0.3749546408653259\n",
      "[Training Epoch 8] Batch 36, Loss 0.37241631746292114\n",
      "[Training Epoch 8] Batch 37, Loss 0.3725441098213196\n",
      "[Training Epoch 8] Batch 38, Loss 0.3609699606895447\n",
      "[Training Epoch 8] Batch 39, Loss 0.3685430586338043\n",
      "[Training Epoch 8] Batch 40, Loss 0.3685993552207947\n",
      "[Training Epoch 8] Batch 41, Loss 0.38616234064102173\n",
      "[Training Epoch 8] Batch 42, Loss 0.38019227981567383\n",
      "[Training Epoch 8] Batch 43, Loss 0.3517744839191437\n",
      "[Training Epoch 8] Batch 44, Loss 0.36867743730545044\n",
      "[Training Epoch 8] Batch 45, Loss 0.3932666778564453\n",
      "[Training Epoch 8] Batch 46, Loss 0.3457818627357483\n",
      "[Training Epoch 8] Batch 47, Loss 0.3614024519920349\n",
      "[Training Epoch 8] Batch 48, Loss 0.35047483444213867\n",
      "[Training Epoch 8] Batch 49, Loss 0.3468344211578369\n",
      "[Training Epoch 8] Batch 50, Loss 0.3777625858783722\n",
      "[Training Epoch 8] Batch 51, Loss 0.3859674334526062\n",
      "[Training Epoch 8] Batch 52, Loss 0.37912797927856445\n",
      "[Training Epoch 8] Batch 53, Loss 0.3567904829978943\n",
      "[Training Epoch 8] Batch 54, Loss 0.36587369441986084\n",
      "[Training Epoch 8] Batch 55, Loss 0.37704724073410034\n",
      "[Training Epoch 8] Batch 56, Loss 0.35400840640068054\n",
      "[Training Epoch 8] Batch 57, Loss 0.3914591073989868\n",
      "[Training Epoch 8] Batch 58, Loss 0.33122754096984863\n",
      "[Training Epoch 8] Batch 59, Loss 0.3409436345100403\n",
      "[Training Epoch 8] Batch 60, Loss 0.35164862871170044\n",
      "[Training Epoch 8] Batch 61, Loss 0.39548641443252563\n",
      "[Training Epoch 8] Batch 62, Loss 0.4021798372268677\n",
      "[Training Epoch 8] Batch 63, Loss 0.395656555891037\n",
      "[Training Epoch 8] Batch 64, Loss 0.3659968972206116\n",
      "[Training Epoch 8] Batch 65, Loss 0.3542993664741516\n",
      "[Training Epoch 8] Batch 66, Loss 0.3832395672798157\n",
      "[Training Epoch 8] Batch 67, Loss 0.33995112776756287\n",
      "[Training Epoch 8] Batch 68, Loss 0.36442381143569946\n",
      "[Training Epoch 8] Batch 69, Loss 0.3788757920265198\n",
      "[Training Epoch 8] Batch 70, Loss 0.3481360673904419\n",
      "[Training Epoch 8] Batch 71, Loss 0.3710620403289795\n",
      "[Training Epoch 8] Batch 72, Loss 0.3853372037410736\n",
      "[Training Epoch 8] Batch 73, Loss 0.3827059864997864\n",
      "[Training Epoch 8] Batch 74, Loss 0.35310065746307373\n",
      "[Training Epoch 8] Batch 75, Loss 0.3744766414165497\n",
      "[Training Epoch 8] Batch 76, Loss 0.36897701025009155\n",
      "[Training Epoch 8] Batch 77, Loss 0.3556312620639801\n",
      "[Training Epoch 8] Batch 78, Loss 0.3647083342075348\n",
      "[Training Epoch 8] Batch 79, Loss 0.37921851873397827\n",
      "[Training Epoch 8] Batch 80, Loss 0.37975889444351196\n",
      "[Training Epoch 8] Batch 81, Loss 0.3744271397590637\n",
      "[Training Epoch 8] Batch 82, Loss 0.3733392357826233\n",
      "[Training Epoch 8] Batch 83, Loss 0.3645901679992676\n",
      "[Training Epoch 8] Batch 84, Loss 0.38061821460723877\n",
      "[Training Epoch 8] Batch 85, Loss 0.36473530530929565\n",
      "[Training Epoch 8] Batch 86, Loss 0.38361355662345886\n",
      "[Training Epoch 8] Batch 87, Loss 0.3334270715713501\n",
      "[Training Epoch 8] Batch 88, Loss 0.36319172382354736\n",
      "[Training Epoch 8] Batch 89, Loss 0.382549524307251\n",
      "[Training Epoch 8] Batch 90, Loss 0.36739665269851685\n",
      "[Training Epoch 8] Batch 91, Loss 0.3614749312400818\n",
      "[Training Epoch 8] Batch 92, Loss 0.3703553080558777\n",
      "[Training Epoch 8] Batch 93, Loss 0.3845216631889343\n",
      "[Training Epoch 8] Batch 94, Loss 0.3654343783855438\n",
      "[Training Epoch 8] Batch 95, Loss 0.37387076020240784\n",
      "[Training Epoch 8] Batch 96, Loss 0.38110294938087463\n",
      "[Training Epoch 8] Batch 97, Loss 0.384613037109375\n",
      "[Training Epoch 8] Batch 98, Loss 0.35123056173324585\n",
      "[Training Epoch 8] Batch 99, Loss 0.3966923952102661\n",
      "[Training Epoch 8] Batch 100, Loss 0.37818747758865356\n",
      "[Training Epoch 8] Batch 101, Loss 0.37467074394226074\n",
      "[Training Epoch 8] Batch 102, Loss 0.35340869426727295\n",
      "[Training Epoch 8] Batch 103, Loss 0.3654163181781769\n",
      "[Training Epoch 8] Batch 104, Loss 0.35854342579841614\n",
      "[Training Epoch 8] Batch 105, Loss 0.3770235776901245\n",
      "[Training Epoch 8] Batch 106, Loss 0.3586371839046478\n",
      "[Training Epoch 8] Batch 107, Loss 0.3790872395038605\n",
      "[Training Epoch 8] Batch 108, Loss 0.371965229511261\n",
      "[Training Epoch 8] Batch 109, Loss 0.38565564155578613\n",
      "[Training Epoch 8] Batch 110, Loss 0.3878272771835327\n",
      "[Training Epoch 8] Batch 111, Loss 0.3551834225654602\n",
      "[Training Epoch 8] Batch 112, Loss 0.40503013134002686\n",
      "[Training Epoch 8] Batch 113, Loss 0.38511887192726135\n",
      "[Training Epoch 8] Batch 114, Loss 0.36440229415893555\n",
      "[Training Epoch 8] Batch 115, Loss 0.3333422839641571\n",
      "[Training Epoch 8] Batch 116, Loss 0.3730070888996124\n",
      "[Training Epoch 8] Batch 117, Loss 0.3602912425994873\n",
      "[Training Epoch 8] Batch 118, Loss 0.3492512106895447\n",
      "[Training Epoch 8] Batch 119, Loss 0.35562288761138916\n",
      "[Training Epoch 8] Batch 120, Loss 0.41105079650878906\n",
      "[Training Epoch 8] Batch 121, Loss 0.3442193567752838\n",
      "[Training Epoch 8] Batch 122, Loss 0.39554333686828613\n",
      "[Training Epoch 8] Batch 123, Loss 0.39236223697662354\n",
      "[Training Epoch 8] Batch 124, Loss 0.3566599488258362\n",
      "[Training Epoch 8] Batch 125, Loss 0.36310479044914246\n",
      "[Training Epoch 8] Batch 126, Loss 0.4031439423561096\n",
      "[Training Epoch 8] Batch 127, Loss 0.34550392627716064\n",
      "[Training Epoch 8] Batch 128, Loss 0.36116570234298706\n",
      "[Training Epoch 8] Batch 129, Loss 0.386833131313324\n",
      "[Training Epoch 8] Batch 130, Loss 0.3725004196166992\n",
      "[Training Epoch 8] Batch 131, Loss 0.35637587308883667\n",
      "[Training Epoch 8] Batch 132, Loss 0.374851256608963\n",
      "[Training Epoch 8] Batch 133, Loss 0.3739360570907593\n",
      "[Training Epoch 8] Batch 134, Loss 0.36756405234336853\n",
      "[Training Epoch 8] Batch 135, Loss 0.3629199266433716\n",
      "[Training Epoch 8] Batch 136, Loss 0.3568853735923767\n",
      "[Training Epoch 8] Batch 137, Loss 0.37298011779785156\n",
      "[Training Epoch 8] Batch 138, Loss 0.38211148977279663\n",
      "[Training Epoch 8] Batch 139, Loss 0.34677159786224365\n",
      "[Training Epoch 8] Batch 140, Loss 0.37537699937820435\n",
      "[Training Epoch 8] Batch 141, Loss 0.404570072889328\n",
      "[Training Epoch 8] Batch 142, Loss 0.3655233681201935\n",
      "[Training Epoch 8] Batch 143, Loss 0.37002602219581604\n",
      "[Training Epoch 8] Batch 144, Loss 0.3870517909526825\n",
      "[Training Epoch 8] Batch 145, Loss 0.38892045617103577\n",
      "[Training Epoch 8] Batch 146, Loss 0.38195109367370605\n",
      "[Training Epoch 8] Batch 147, Loss 0.3608238101005554\n",
      "[Training Epoch 8] Batch 148, Loss 0.3588876724243164\n",
      "[Training Epoch 8] Batch 149, Loss 0.3612489104270935\n",
      "[Training Epoch 8] Batch 150, Loss 0.3296671509742737\n",
      "[Training Epoch 8] Batch 151, Loss 0.31908971071243286\n",
      "[Training Epoch 8] Batch 152, Loss 0.35975557565689087\n",
      "[Training Epoch 8] Batch 153, Loss 0.35302039980888367\n",
      "[Training Epoch 8] Batch 154, Loss 0.357410728931427\n",
      "[Training Epoch 8] Batch 155, Loss 0.3620273470878601\n",
      "[Training Epoch 8] Batch 156, Loss 0.3466637432575226\n",
      "[Training Epoch 8] Batch 157, Loss 0.35937440395355225\n",
      "[Training Epoch 8] Batch 158, Loss 0.3746371269226074\n",
      "[Training Epoch 8] Batch 159, Loss 0.34406691789627075\n",
      "[Training Epoch 8] Batch 160, Loss 0.3621174693107605\n",
      "[Training Epoch 8] Batch 161, Loss 0.36324378848075867\n",
      "[Training Epoch 8] Batch 162, Loss 0.3663182258605957\n",
      "[Training Epoch 8] Batch 163, Loss 0.35709255933761597\n",
      "[Training Epoch 8] Batch 164, Loss 0.39735108613967896\n",
      "[Training Epoch 8] Batch 165, Loss 0.3653491139411926\n",
      "[Training Epoch 8] Batch 166, Loss 0.3619462251663208\n",
      "[Training Epoch 8] Batch 167, Loss 0.3699679374694824\n",
      "[Training Epoch 8] Batch 168, Loss 0.39616256952285767\n",
      "[Training Epoch 8] Batch 169, Loss 0.3733057975769043\n",
      "[Training Epoch 8] Batch 170, Loss 0.39600810408592224\n",
      "[Training Epoch 8] Batch 171, Loss 0.3688947558403015\n",
      "[Training Epoch 8] Batch 172, Loss 0.33952954411506653\n",
      "[Training Epoch 8] Batch 173, Loss 0.3614341914653778\n",
      "[Training Epoch 8] Batch 174, Loss 0.36117029190063477\n",
      "[Training Epoch 8] Batch 175, Loss 0.36610084772109985\n",
      "[Training Epoch 8] Batch 176, Loss 0.3656885325908661\n",
      "[Training Epoch 8] Batch 177, Loss 0.35532617568969727\n",
      "[Training Epoch 8] Batch 178, Loss 0.3809249997138977\n",
      "[Training Epoch 8] Batch 179, Loss 0.3533971309661865\n",
      "[Training Epoch 8] Batch 180, Loss 0.3811458945274353\n",
      "[Training Epoch 8] Batch 181, Loss 0.3733646869659424\n",
      "[Training Epoch 8] Batch 182, Loss 0.35761758685112\n",
      "[Training Epoch 8] Batch 183, Loss 0.34101244807243347\n",
      "[Training Epoch 8] Batch 184, Loss 0.3680040240287781\n",
      "[Training Epoch 8] Batch 185, Loss 0.40415775775909424\n",
      "[Training Epoch 8] Batch 186, Loss 0.3589703440666199\n",
      "[Training Epoch 8] Batch 187, Loss 0.35893136262893677\n",
      "[Training Epoch 8] Batch 188, Loss 0.35694020986557007\n",
      "[Training Epoch 8] Batch 189, Loss 0.37107187509536743\n",
      "[Training Epoch 8] Batch 190, Loss 0.36552923917770386\n",
      "[Training Epoch 8] Batch 191, Loss 0.36572593450546265\n",
      "[Training Epoch 8] Batch 192, Loss 0.3796343207359314\n",
      "[Training Epoch 8] Batch 193, Loss 0.3634617328643799\n",
      "[Training Epoch 8] Batch 194, Loss 0.3569735884666443\n",
      "[Training Epoch 8] Batch 195, Loss 0.37352779507637024\n",
      "[Training Epoch 8] Batch 196, Loss 0.3818347752094269\n",
      "[Training Epoch 8] Batch 197, Loss 0.3592528700828552\n",
      "[Training Epoch 8] Batch 198, Loss 0.362851083278656\n",
      "[Training Epoch 8] Batch 199, Loss 0.36000826954841614\n",
      "[Training Epoch 8] Batch 200, Loss 0.36557504534721375\n",
      "[Training Epoch 8] Batch 201, Loss 0.36366140842437744\n",
      "[Training Epoch 8] Batch 202, Loss 0.38190656900405884\n",
      "[Training Epoch 8] Batch 203, Loss 0.37226325273513794\n",
      "[Training Epoch 8] Batch 204, Loss 0.3985068202018738\n",
      "[Training Epoch 8] Batch 205, Loss 0.34617477655410767\n",
      "[Training Epoch 8] Batch 206, Loss 0.3575509488582611\n",
      "[Training Epoch 8] Batch 207, Loss 0.3654637932777405\n",
      "[Training Epoch 8] Batch 208, Loss 0.41626596450805664\n",
      "[Training Epoch 8] Batch 209, Loss 0.40834853053092957\n",
      "[Training Epoch 8] Batch 210, Loss 0.38575899600982666\n",
      "[Training Epoch 8] Batch 211, Loss 0.40331393480300903\n",
      "[Training Epoch 8] Batch 212, Loss 0.38433992862701416\n",
      "[Training Epoch 8] Batch 213, Loss 0.3469874858856201\n",
      "[Training Epoch 8] Batch 214, Loss 0.399122416973114\n",
      "[Training Epoch 8] Batch 215, Loss 0.3574807643890381\n",
      "[Training Epoch 8] Batch 216, Loss 0.35009899735450745\n",
      "[Training Epoch 8] Batch 217, Loss 0.3716357350349426\n",
      "[Training Epoch 8] Batch 218, Loss 0.37544339895248413\n",
      "[Training Epoch 8] Batch 219, Loss 0.39389970898628235\n",
      "[Training Epoch 8] Batch 220, Loss 0.38171082735061646\n",
      "[Training Epoch 8] Batch 221, Loss 0.38244837522506714\n",
      "[Training Epoch 8] Batch 222, Loss 0.36805176734924316\n",
      "[Training Epoch 8] Batch 223, Loss 0.3948606550693512\n",
      "[Training Epoch 8] Batch 224, Loss 0.35495686531066895\n",
      "[Training Epoch 8] Batch 225, Loss 0.36650484800338745\n",
      "[Training Epoch 8] Batch 226, Loss 0.39807647466659546\n",
      "[Training Epoch 8] Batch 227, Loss 0.3849342167377472\n",
      "[Training Epoch 8] Batch 228, Loss 0.37665215134620667\n",
      "[Training Epoch 8] Batch 229, Loss 0.3787754476070404\n",
      "[Training Epoch 8] Batch 230, Loss 0.37495118379592896\n",
      "[Training Epoch 8] Batch 231, Loss 0.35479849576950073\n",
      "[Training Epoch 8] Batch 232, Loss 0.39815786480903625\n",
      "[Training Epoch 8] Batch 233, Loss 0.3892870545387268\n",
      "[Training Epoch 8] Batch 234, Loss 0.3406739830970764\n",
      "[Training Epoch 8] Batch 235, Loss 0.3649737238883972\n",
      "[Training Epoch 8] Batch 236, Loss 0.36928635835647583\n",
      "[Training Epoch 8] Batch 237, Loss 0.3682289123535156\n",
      "[Training Epoch 8] Batch 238, Loss 0.36833083629608154\n",
      "[Training Epoch 8] Batch 239, Loss 0.3666727840900421\n",
      "[Training Epoch 8] Batch 240, Loss 0.3584212064743042\n",
      "[Training Epoch 8] Batch 241, Loss 0.4187796711921692\n",
      "[Training Epoch 8] Batch 242, Loss 0.3695174753665924\n",
      "[Training Epoch 8] Batch 243, Loss 0.3562440276145935\n",
      "[Training Epoch 8] Batch 244, Loss 0.3955773711204529\n",
      "[Training Epoch 8] Batch 245, Loss 0.3658704459667206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 2124.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2397.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 8] Precision = 0.3623, Recall = 0.9873\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.42044681310653687\n",
      "[Training Epoch 9] Batch 1, Loss 0.37120527029037476\n",
      "[Training Epoch 9] Batch 2, Loss 0.3497869372367859\n",
      "[Training Epoch 9] Batch 3, Loss 0.38384002447128296\n",
      "[Training Epoch 9] Batch 4, Loss 0.39525216817855835\n",
      "[Training Epoch 9] Batch 5, Loss 0.34771254658699036\n",
      "[Training Epoch 9] Batch 6, Loss 0.3740326166152954\n",
      "[Training Epoch 9] Batch 7, Loss 0.3553900122642517\n",
      "[Training Epoch 9] Batch 8, Loss 0.3611702024936676\n",
      "[Training Epoch 9] Batch 9, Loss 0.3697684705257416\n",
      "[Training Epoch 9] Batch 10, Loss 0.3783802390098572\n",
      "[Training Epoch 9] Batch 11, Loss 0.36153730750083923\n",
      "[Training Epoch 9] Batch 12, Loss 0.3435439467430115\n",
      "[Training Epoch 9] Batch 13, Loss 0.3454408049583435\n",
      "[Training Epoch 9] Batch 14, Loss 0.34279611706733704\n",
      "[Training Epoch 9] Batch 15, Loss 0.3733781576156616\n",
      "[Training Epoch 9] Batch 16, Loss 0.36290985345840454\n",
      "[Training Epoch 9] Batch 17, Loss 0.36912283301353455\n",
      "[Training Epoch 9] Batch 18, Loss 0.3632529377937317\n",
      "[Training Epoch 9] Batch 19, Loss 0.3675943613052368\n",
      "[Training Epoch 9] Batch 20, Loss 0.38175371289253235\n",
      "[Training Epoch 9] Batch 21, Loss 0.37470605969429016\n",
      "[Training Epoch 9] Batch 22, Loss 0.37541988492012024\n",
      "[Training Epoch 9] Batch 23, Loss 0.3888920545578003\n",
      "[Training Epoch 9] Batch 24, Loss 0.36264151334762573\n",
      "[Training Epoch 9] Batch 25, Loss 0.37015026807785034\n",
      "[Training Epoch 9] Batch 26, Loss 0.39205458760261536\n",
      "[Training Epoch 9] Batch 27, Loss 0.3472345471382141\n",
      "[Training Epoch 9] Batch 28, Loss 0.35999560356140137\n",
      "[Training Epoch 9] Batch 29, Loss 0.356065034866333\n",
      "[Training Epoch 9] Batch 30, Loss 0.34046298265457153\n",
      "[Training Epoch 9] Batch 31, Loss 0.37549668550491333\n",
      "[Training Epoch 9] Batch 32, Loss 0.3925071954727173\n",
      "[Training Epoch 9] Batch 33, Loss 0.3622569739818573\n",
      "[Training Epoch 9] Batch 34, Loss 0.33387017250061035\n",
      "[Training Epoch 9] Batch 35, Loss 0.34956759214401245\n",
      "[Training Epoch 9] Batch 36, Loss 0.3819515109062195\n",
      "[Training Epoch 9] Batch 37, Loss 0.3801497519016266\n",
      "[Training Epoch 9] Batch 38, Loss 0.3652883768081665\n",
      "[Training Epoch 9] Batch 39, Loss 0.37015390396118164\n",
      "[Training Epoch 9] Batch 40, Loss 0.3551517724990845\n",
      "[Training Epoch 9] Batch 41, Loss 0.360837459564209\n",
      "[Training Epoch 9] Batch 42, Loss 0.3670535683631897\n",
      "[Training Epoch 9] Batch 43, Loss 0.3615345358848572\n",
      "[Training Epoch 9] Batch 44, Loss 0.3999604284763336\n",
      "[Training Epoch 9] Batch 45, Loss 0.3598296642303467\n",
      "[Training Epoch 9] Batch 46, Loss 0.3554815351963043\n",
      "[Training Epoch 9] Batch 47, Loss 0.34200841188430786\n",
      "[Training Epoch 9] Batch 48, Loss 0.37720030546188354\n",
      "[Training Epoch 9] Batch 49, Loss 0.398931086063385\n",
      "[Training Epoch 9] Batch 50, Loss 0.34527474641799927\n",
      "[Training Epoch 9] Batch 51, Loss 0.3587534725666046\n",
      "[Training Epoch 9] Batch 52, Loss 0.37326961755752563\n",
      "[Training Epoch 9] Batch 53, Loss 0.3568233847618103\n",
      "[Training Epoch 9] Batch 54, Loss 0.37658417224884033\n",
      "[Training Epoch 9] Batch 55, Loss 0.35590916872024536\n",
      "[Training Epoch 9] Batch 56, Loss 0.3530435860157013\n",
      "[Training Epoch 9] Batch 57, Loss 0.3503418266773224\n",
      "[Training Epoch 9] Batch 58, Loss 0.3485278785228729\n",
      "[Training Epoch 9] Batch 59, Loss 0.358022540807724\n",
      "[Training Epoch 9] Batch 60, Loss 0.3731679916381836\n",
      "[Training Epoch 9] Batch 61, Loss 0.37590277194976807\n",
      "[Training Epoch 9] Batch 62, Loss 0.3927798867225647\n",
      "[Training Epoch 9] Batch 63, Loss 0.3617478013038635\n",
      "[Training Epoch 9] Batch 64, Loss 0.3753480315208435\n",
      "[Training Epoch 9] Batch 65, Loss 0.39783865213394165\n",
      "[Training Epoch 9] Batch 66, Loss 0.3644408583641052\n",
      "[Training Epoch 9] Batch 67, Loss 0.3750627934932709\n",
      "[Training Epoch 9] Batch 68, Loss 0.35288602113723755\n",
      "[Training Epoch 9] Batch 69, Loss 0.3552408218383789\n",
      "[Training Epoch 9] Batch 70, Loss 0.3519427180290222\n",
      "[Training Epoch 9] Batch 71, Loss 0.3712296485900879\n",
      "[Training Epoch 9] Batch 72, Loss 0.3811156153678894\n",
      "[Training Epoch 9] Batch 73, Loss 0.35983872413635254\n",
      "[Training Epoch 9] Batch 74, Loss 0.3556862473487854\n",
      "[Training Epoch 9] Batch 75, Loss 0.35506385564804077\n",
      "[Training Epoch 9] Batch 76, Loss 0.3695070147514343\n",
      "[Training Epoch 9] Batch 77, Loss 0.38602936267852783\n",
      "[Training Epoch 9] Batch 78, Loss 0.3602062463760376\n",
      "[Training Epoch 9] Batch 79, Loss 0.3617749810218811\n",
      "[Training Epoch 9] Batch 80, Loss 0.3874345123767853\n",
      "[Training Epoch 9] Batch 81, Loss 0.3296703100204468\n",
      "[Training Epoch 9] Batch 82, Loss 0.3539135456085205\n",
      "[Training Epoch 9] Batch 83, Loss 0.36279237270355225\n",
      "[Training Epoch 9] Batch 84, Loss 0.33890223503112793\n",
      "[Training Epoch 9] Batch 85, Loss 0.3737049996852875\n",
      "[Training Epoch 9] Batch 86, Loss 0.3721860349178314\n",
      "[Training Epoch 9] Batch 87, Loss 0.363113671541214\n",
      "[Training Epoch 9] Batch 88, Loss 0.35037529468536377\n",
      "[Training Epoch 9] Batch 89, Loss 0.3512749671936035\n",
      "[Training Epoch 9] Batch 90, Loss 0.3863025903701782\n",
      "[Training Epoch 9] Batch 91, Loss 0.37745195627212524\n",
      "[Training Epoch 9] Batch 92, Loss 0.3907347321510315\n",
      "[Training Epoch 9] Batch 93, Loss 0.3980783224105835\n",
      "[Training Epoch 9] Batch 94, Loss 0.37851959466934204\n",
      "[Training Epoch 9] Batch 95, Loss 0.3155660629272461\n",
      "[Training Epoch 9] Batch 96, Loss 0.36158132553100586\n",
      "[Training Epoch 9] Batch 97, Loss 0.3526517152786255\n",
      "[Training Epoch 9] Batch 98, Loss 0.3480710983276367\n",
      "[Training Epoch 9] Batch 99, Loss 0.37929075956344604\n",
      "[Training Epoch 9] Batch 100, Loss 0.3651505708694458\n",
      "[Training Epoch 9] Batch 101, Loss 0.40096724033355713\n",
      "[Training Epoch 9] Batch 102, Loss 0.35781407356262207\n",
      "[Training Epoch 9] Batch 103, Loss 0.3573348820209503\n",
      "[Training Epoch 9] Batch 104, Loss 0.3409653306007385\n",
      "[Training Epoch 9] Batch 105, Loss 0.37895044684410095\n",
      "[Training Epoch 9] Batch 106, Loss 0.3578587770462036\n",
      "[Training Epoch 9] Batch 107, Loss 0.3589854836463928\n",
      "[Training Epoch 9] Batch 108, Loss 0.3732013702392578\n",
      "[Training Epoch 9] Batch 109, Loss 0.3723016381263733\n",
      "[Training Epoch 9] Batch 110, Loss 0.3692585825920105\n",
      "[Training Epoch 9] Batch 111, Loss 0.3738299012184143\n",
      "[Training Epoch 9] Batch 112, Loss 0.36568716168403625\n",
      "[Training Epoch 9] Batch 113, Loss 0.3610538840293884\n",
      "[Training Epoch 9] Batch 114, Loss 0.36199986934661865\n",
      "[Training Epoch 9] Batch 115, Loss 0.368434876203537\n",
      "[Training Epoch 9] Batch 116, Loss 0.35868197679519653\n",
      "[Training Epoch 9] Batch 117, Loss 0.3778884708881378\n",
      "[Training Epoch 9] Batch 118, Loss 0.3772532641887665\n",
      "[Training Epoch 9] Batch 119, Loss 0.356228232383728\n",
      "[Training Epoch 9] Batch 120, Loss 0.401388943195343\n",
      "[Training Epoch 9] Batch 121, Loss 0.3664745092391968\n",
      "[Training Epoch 9] Batch 122, Loss 0.37179338932037354\n",
      "[Training Epoch 9] Batch 123, Loss 0.37146279215812683\n",
      "[Training Epoch 9] Batch 124, Loss 0.3883128762245178\n",
      "[Training Epoch 9] Batch 125, Loss 0.3874494433403015\n",
      "[Training Epoch 9] Batch 126, Loss 0.3392678499221802\n",
      "[Training Epoch 9] Batch 127, Loss 0.3573671579360962\n",
      "[Training Epoch 9] Batch 128, Loss 0.36708441376686096\n",
      "[Training Epoch 9] Batch 129, Loss 0.3829047381877899\n",
      "[Training Epoch 9] Batch 130, Loss 0.36928510665893555\n",
      "[Training Epoch 9] Batch 131, Loss 0.3660861849784851\n",
      "[Training Epoch 9] Batch 132, Loss 0.3609488010406494\n",
      "[Training Epoch 9] Batch 133, Loss 0.3563354015350342\n",
      "[Training Epoch 9] Batch 134, Loss 0.35158252716064453\n",
      "[Training Epoch 9] Batch 135, Loss 0.3752044439315796\n",
      "[Training Epoch 9] Batch 136, Loss 0.3810312747955322\n",
      "[Training Epoch 9] Batch 137, Loss 0.367222398519516\n",
      "[Training Epoch 9] Batch 138, Loss 0.39749735593795776\n",
      "[Training Epoch 9] Batch 139, Loss 0.37079036235809326\n",
      "[Training Epoch 9] Batch 140, Loss 0.3697722256183624\n",
      "[Training Epoch 9] Batch 141, Loss 0.3799223303794861\n",
      "[Training Epoch 9] Batch 142, Loss 0.36163949966430664\n",
      "[Training Epoch 9] Batch 143, Loss 0.3702899217605591\n",
      "[Training Epoch 9] Batch 144, Loss 0.3565872013568878\n",
      "[Training Epoch 9] Batch 145, Loss 0.36612313985824585\n",
      "[Training Epoch 9] Batch 146, Loss 0.34279605746269226\n",
      "[Training Epoch 9] Batch 147, Loss 0.3662397265434265\n",
      "[Training Epoch 9] Batch 148, Loss 0.3465231657028198\n",
      "[Training Epoch 9] Batch 149, Loss 0.3596172332763672\n",
      "[Training Epoch 9] Batch 150, Loss 0.37308892607688904\n",
      "[Training Epoch 9] Batch 151, Loss 0.3647657036781311\n",
      "[Training Epoch 9] Batch 152, Loss 0.3519665598869324\n",
      "[Training Epoch 9] Batch 153, Loss 0.3550737202167511\n",
      "[Training Epoch 9] Batch 154, Loss 0.3797996938228607\n",
      "[Training Epoch 9] Batch 155, Loss 0.3436088562011719\n",
      "[Training Epoch 9] Batch 156, Loss 0.3685547709465027\n",
      "[Training Epoch 9] Batch 157, Loss 0.40829744935035706\n",
      "[Training Epoch 9] Batch 158, Loss 0.36892169713974\n",
      "[Training Epoch 9] Batch 159, Loss 0.3751886487007141\n",
      "[Training Epoch 9] Batch 160, Loss 0.3710988461971283\n",
      "[Training Epoch 9] Batch 161, Loss 0.3626202344894409\n",
      "[Training Epoch 9] Batch 162, Loss 0.360588401556015\n",
      "[Training Epoch 9] Batch 163, Loss 0.3773126006126404\n",
      "[Training Epoch 9] Batch 164, Loss 0.3714646100997925\n",
      "[Training Epoch 9] Batch 165, Loss 0.36057883501052856\n",
      "[Training Epoch 9] Batch 166, Loss 0.3526642322540283\n",
      "[Training Epoch 9] Batch 167, Loss 0.33465200662612915\n",
      "[Training Epoch 9] Batch 168, Loss 0.38854873180389404\n",
      "[Training Epoch 9] Batch 169, Loss 0.3609124720096588\n",
      "[Training Epoch 9] Batch 170, Loss 0.35853397846221924\n",
      "[Training Epoch 9] Batch 171, Loss 0.3626573085784912\n",
      "[Training Epoch 9] Batch 172, Loss 0.36464089155197144\n",
      "[Training Epoch 9] Batch 173, Loss 0.3478259742259979\n",
      "[Training Epoch 9] Batch 174, Loss 0.35890811681747437\n",
      "[Training Epoch 9] Batch 175, Loss 0.3640121817588806\n",
      "[Training Epoch 9] Batch 176, Loss 0.3628321886062622\n",
      "[Training Epoch 9] Batch 177, Loss 0.3724154829978943\n",
      "[Training Epoch 9] Batch 178, Loss 0.3819819986820221\n",
      "[Training Epoch 9] Batch 179, Loss 0.36517369747161865\n",
      "[Training Epoch 9] Batch 180, Loss 0.35405540466308594\n",
      "[Training Epoch 9] Batch 181, Loss 0.35363471508026123\n",
      "[Training Epoch 9] Batch 182, Loss 0.379015177488327\n",
      "[Training Epoch 9] Batch 183, Loss 0.36192142963409424\n",
      "[Training Epoch 9] Batch 184, Loss 0.3721392750740051\n",
      "[Training Epoch 9] Batch 185, Loss 0.3979552388191223\n",
      "[Training Epoch 9] Batch 186, Loss 0.35258546471595764\n",
      "[Training Epoch 9] Batch 187, Loss 0.37133195996284485\n",
      "[Training Epoch 9] Batch 188, Loss 0.35481005907058716\n",
      "[Training Epoch 9] Batch 189, Loss 0.37376868724823\n",
      "[Training Epoch 9] Batch 190, Loss 0.3846157193183899\n",
      "[Training Epoch 9] Batch 191, Loss 0.3604515492916107\n",
      "[Training Epoch 9] Batch 192, Loss 0.3814024329185486\n",
      "[Training Epoch 9] Batch 193, Loss 0.37269356846809387\n",
      "[Training Epoch 9] Batch 194, Loss 0.3875974416732788\n",
      "[Training Epoch 9] Batch 195, Loss 0.34203726053237915\n",
      "[Training Epoch 9] Batch 196, Loss 0.36211058497428894\n",
      "[Training Epoch 9] Batch 197, Loss 0.39014893770217896\n",
      "[Training Epoch 9] Batch 198, Loss 0.37322938442230225\n",
      "[Training Epoch 9] Batch 199, Loss 0.35940730571746826\n",
      "[Training Epoch 9] Batch 200, Loss 0.3720337748527527\n",
      "[Training Epoch 9] Batch 201, Loss 0.3439211845397949\n",
      "[Training Epoch 9] Batch 202, Loss 0.3694353699684143\n",
      "[Training Epoch 9] Batch 203, Loss 0.34111645817756653\n",
      "[Training Epoch 9] Batch 204, Loss 0.35171449184417725\n",
      "[Training Epoch 9] Batch 205, Loss 0.38015812635421753\n",
      "[Training Epoch 9] Batch 206, Loss 0.3586530089378357\n",
      "[Training Epoch 9] Batch 207, Loss 0.3510855734348297\n",
      "[Training Epoch 9] Batch 208, Loss 0.39779865741729736\n",
      "[Training Epoch 9] Batch 209, Loss 0.36257684230804443\n",
      "[Training Epoch 9] Batch 210, Loss 0.3471667170524597\n",
      "[Training Epoch 9] Batch 211, Loss 0.3316771388053894\n",
      "[Training Epoch 9] Batch 212, Loss 0.3602197766304016\n",
      "[Training Epoch 9] Batch 213, Loss 0.3679409623146057\n",
      "[Training Epoch 9] Batch 214, Loss 0.36041712760925293\n",
      "[Training Epoch 9] Batch 215, Loss 0.3810221552848816\n",
      "[Training Epoch 9] Batch 216, Loss 0.3857067823410034\n",
      "[Training Epoch 9] Batch 217, Loss 0.35869503021240234\n",
      "[Training Epoch 9] Batch 218, Loss 0.3637023866176605\n",
      "[Training Epoch 9] Batch 219, Loss 0.33553725481033325\n",
      "[Training Epoch 9] Batch 220, Loss 0.3622719347476959\n",
      "[Training Epoch 9] Batch 221, Loss 0.35710322856903076\n",
      "[Training Epoch 9] Batch 222, Loss 0.34404635429382324\n",
      "[Training Epoch 9] Batch 223, Loss 0.36699771881103516\n",
      "[Training Epoch 9] Batch 224, Loss 0.32443347573280334\n",
      "[Training Epoch 9] Batch 225, Loss 0.3735928535461426\n",
      "[Training Epoch 9] Batch 226, Loss 0.3538966774940491\n",
      "[Training Epoch 9] Batch 227, Loss 0.36370277404785156\n",
      "[Training Epoch 9] Batch 228, Loss 0.3798968195915222\n",
      "[Training Epoch 9] Batch 229, Loss 0.3527148962020874\n",
      "[Training Epoch 9] Batch 230, Loss 0.4023514688014984\n",
      "[Training Epoch 9] Batch 231, Loss 0.3924511671066284\n",
      "[Training Epoch 9] Batch 232, Loss 0.3764365315437317\n",
      "[Training Epoch 9] Batch 233, Loss 0.3563508987426758\n",
      "[Training Epoch 9] Batch 234, Loss 0.33794108033180237\n",
      "[Training Epoch 9] Batch 235, Loss 0.3768126368522644\n",
      "[Training Epoch 9] Batch 236, Loss 0.34217333793640137\n",
      "[Training Epoch 9] Batch 237, Loss 0.355334609746933\n",
      "[Training Epoch 9] Batch 238, Loss 0.38284528255462646\n",
      "[Training Epoch 9] Batch 239, Loss 0.3261101245880127\n",
      "[Training Epoch 9] Batch 240, Loss 0.3598370850086212\n",
      "[Training Epoch 9] Batch 241, Loss 0.3627607822418213\n",
      "[Training Epoch 9] Batch 242, Loss 0.3658744990825653\n",
      "[Training Epoch 9] Batch 243, Loss 0.35154345631599426\n",
      "[Training Epoch 9] Batch 244, Loss 0.359563410282135\n",
      "[Training Epoch 9] Batch 245, Loss 0.38096436858177185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 1983.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [00:00<00:00, 2406.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_users: 6045\n",
      "Length of test_items: 6045\n",
      "Length of test_preds: 6045\n",
      "[Evaluating Epoch 9] Precision = 0.3624, Recall = 0.9876\n"
     ]
    }
   ],
   "source": [
    "# Reindex\n",
    "user_id = goodbooks_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "goodbooks_rating = pd.merge(goodbooks_rating, user_id, on=['uid'], how='left')\n",
    "item_id = goodbooks_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "goodbooks_rating = pd.merge(goodbooks_rating, item_id, on=['mid'], how='left')\n",
    "goodbooks_rating = goodbooks_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(goodbooks_rating.userId.min(), goodbooks_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(goodbooks_rating.itemId.min(), goodbooks_rating.itemId.max()))\n",
    "# DataLoader for training \n",
    "sample_generator = SampleGenerator(ratings=goodbooks_rating)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "\n",
    "config = neumf_config\n",
    "engine = NeuMFEngine(config)\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    precision, recall = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    engine.save(config['alias'], epoch, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodbooks\n",
    "precision = [0.3617, 0.3615, 0.3619, 0.3622, 0.3625, 0.3622, 0.3623, 0.3624, 0.3623, 0.3624]\n",
    "recall = [0.9837, 0.9833, 0.9855, 0.9859, 0.9861, 0.9852, 0.9865, 0.9873, 0.9873, 0.9876]\n",
    "loss = [0.4626704752445221, 0.4318108558654785, 0.4400472640991211, 0.37796998023986816, 0.38555628061294556, 0.38528645038604736, 0.3758946359157562, 0.3738066554069519, 0.3658704459667206, 0.38096436858177185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = [0.2623, 0.2616, 0.2618, 0.2619, 0.2615, 0.2617, 0.2616, 0.2618, 0.2622, 0.2612]\n",
    "recall= [0.7758, 0.7744, 0.7739, 0.7748, 0.7742, 0.7735, 0.7742, 0.7740, 0.7758, 0.7725]\n",
    "loss = [0.38828298449516296, 0.2226588875055313, 0.3194698691368103, 0.31795328855514526, 0.1744556874036789, 0.1744556874036789, 0.22327680885791779, 0.22724725306034088, 0.2899876832962036, 0.23551440238952637]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Plot precision and recall values\n",
    "plt.plot(range(1, len(precision) + 1), precision, label='Precision@K', marker='o')\n",
    "plt.plot(range(1, len(recall) + 1), recall, label='Recall@K', marker='s')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Precision@K and Recall@K\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
