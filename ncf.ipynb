{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models_ncf.neumf import NeuMFEngine\n",
    "from models_ncf.data import SampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neumf_config = {'alias': 'neumf_first_try',\n",
    "                'num_epoch': 10,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 6040,\n",
    "                'num_items': 3706,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16, 64, 32, 16, 8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "                'l2_regularization': 0.0000001,\n",
    "                'weight_init_gaussian': True,\n",
    "                'use_cuda': True,\n",
    "                'use_bachify_eval': True,\n",
    "                'device_id': 0,\n",
    "                'pretrain': False,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_precision0.6391_recall0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_precision0.5606_recall0.2463.model'),\n",
    "                'model_dir': 'checkpoints/{}_Epoch{}_precision{:.4f}_recall{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0 - Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Print the device ID and its name for each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i} - Name: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n",
      "Index(['userId', 'itemId', 'rating', 'real_score', 'negative_samples'], dtype='object')\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6845279932022095\n",
      "[Training Epoch 0] Batch 1, Loss 0.6853315234184265\n",
      "[Training Epoch 0] Batch 2, Loss 0.6847243309020996\n",
      "[Training Epoch 0] Batch 3, Loss 0.6845508813858032\n",
      "[Training Epoch 0] Batch 4, Loss 0.6830440163612366\n",
      "[Training Epoch 0] Batch 5, Loss 0.6835166215896606\n",
      "[Training Epoch 0] Batch 6, Loss 0.6832441687583923\n",
      "[Training Epoch 0] Batch 7, Loss 0.6831282377243042\n",
      "[Training Epoch 0] Batch 8, Loss 0.681470513343811\n",
      "[Training Epoch 0] Batch 9, Loss 0.681921124458313\n",
      "[Training Epoch 0] Batch 10, Loss 0.6804699897766113\n",
      "[Training Epoch 0] Batch 11, Loss 0.6801059246063232\n",
      "[Training Epoch 0] Batch 12, Loss 0.6792280673980713\n",
      "[Training Epoch 0] Batch 13, Loss 0.677827000617981\n",
      "[Training Epoch 0] Batch 14, Loss 0.6793016195297241\n",
      "[Training Epoch 0] Batch 15, Loss 0.6781949996948242\n",
      "[Training Epoch 0] Batch 16, Loss 0.676649272441864\n",
      "[Training Epoch 0] Batch 17, Loss 0.6774559020996094\n",
      "[Training Epoch 0] Batch 18, Loss 0.6784948706626892\n",
      "[Training Epoch 0] Batch 19, Loss 0.6764549612998962\n",
      "[Training Epoch 0] Batch 20, Loss 0.6768009066581726\n",
      "[Training Epoch 0] Batch 21, Loss 0.6779859066009521\n",
      "[Training Epoch 0] Batch 22, Loss 0.6759854555130005\n",
      "[Training Epoch 0] Batch 23, Loss 0.6754534244537354\n",
      "[Training Epoch 0] Batch 24, Loss 0.675218939781189\n",
      "[Training Epoch 0] Batch 25, Loss 0.6747982501983643\n",
      "[Training Epoch 0] Batch 26, Loss 0.6745669841766357\n",
      "[Training Epoch 0] Batch 27, Loss 0.6731982231140137\n",
      "[Training Epoch 0] Batch 28, Loss 0.6732853651046753\n",
      "[Training Epoch 0] Batch 29, Loss 0.6716912984848022\n",
      "[Training Epoch 0] Batch 30, Loss 0.6730810403823853\n",
      "[Training Epoch 0] Batch 31, Loss 0.670595109462738\n",
      "[Training Epoch 0] Batch 32, Loss 0.6699048280715942\n",
      "[Training Epoch 0] Batch 33, Loss 0.66910719871521\n",
      "[Training Epoch 0] Batch 34, Loss 0.6680301427841187\n",
      "[Training Epoch 0] Batch 35, Loss 0.668996274471283\n",
      "[Training Epoch 0] Batch 36, Loss 0.6664919853210449\n",
      "[Training Epoch 0] Batch 37, Loss 0.664645791053772\n",
      "[Training Epoch 0] Batch 38, Loss 0.665169358253479\n",
      "[Training Epoch 0] Batch 39, Loss 0.6652639508247375\n",
      "[Training Epoch 0] Batch 40, Loss 0.6652790904045105\n",
      "[Training Epoch 0] Batch 41, Loss 0.6632893085479736\n",
      "[Training Epoch 0] Batch 42, Loss 0.6604458093643188\n",
      "[Training Epoch 0] Batch 43, Loss 0.6624892950057983\n",
      "[Training Epoch 0] Batch 44, Loss 0.6589901447296143\n",
      "[Training Epoch 0] Batch 45, Loss 0.6605799198150635\n",
      "[Training Epoch 0] Batch 46, Loss 0.659781813621521\n",
      "[Training Epoch 0] Batch 47, Loss 0.6575682163238525\n",
      "[Training Epoch 0] Batch 48, Loss 0.6536374092102051\n",
      "[Training Epoch 0] Batch 49, Loss 0.6548140048980713\n",
      "[Training Epoch 0] Batch 50, Loss 0.6511896252632141\n",
      "[Training Epoch 0] Batch 51, Loss 0.6512478590011597\n",
      "[Training Epoch 0] Batch 52, Loss 0.6476600170135498\n",
      "[Training Epoch 0] Batch 53, Loss 0.6477953791618347\n",
      "[Training Epoch 0] Batch 54, Loss 0.6459925174713135\n",
      "[Training Epoch 0] Batch 55, Loss 0.6412365436553955\n",
      "[Training Epoch 0] Batch 56, Loss 0.6412675976753235\n",
      "[Training Epoch 0] Batch 57, Loss 0.6386658549308777\n",
      "[Training Epoch 0] Batch 58, Loss 0.6389492750167847\n",
      "[Training Epoch 0] Batch 59, Loss 0.6303005218505859\n",
      "[Training Epoch 0] Batch 60, Loss 0.6319718360900879\n",
      "[Training Epoch 0] Batch 61, Loss 0.6304055452346802\n",
      "[Training Epoch 0] Batch 62, Loss 0.6173463463783264\n",
      "[Training Epoch 0] Batch 63, Loss 0.6196452975273132\n",
      "[Training Epoch 0] Batch 64, Loss 0.6127091646194458\n",
      "[Training Epoch 0] Batch 65, Loss 0.6064069867134094\n",
      "[Training Epoch 0] Batch 66, Loss 0.6038447618484497\n",
      "[Training Epoch 0] Batch 67, Loss 0.5955872535705566\n",
      "[Training Epoch 0] Batch 68, Loss 0.5973148345947266\n",
      "[Training Epoch 0] Batch 69, Loss 0.5996015071868896\n",
      "[Training Epoch 0] Batch 70, Loss 0.5784982442855835\n",
      "[Training Epoch 0] Batch 71, Loss 0.5924866199493408\n",
      "[Training Epoch 0] Batch 72, Loss 0.5814719200134277\n",
      "[Training Epoch 0] Batch 73, Loss 0.5724884271621704\n",
      "[Training Epoch 0] Batch 74, Loss 0.578741192817688\n",
      "[Training Epoch 0] Batch 75, Loss 0.5552012920379639\n",
      "[Training Epoch 0] Batch 76, Loss 0.5562052726745605\n",
      "[Training Epoch 0] Batch 77, Loss 0.5498431921005249\n",
      "[Training Epoch 0] Batch 78, Loss 0.5311091542243958\n",
      "[Training Epoch 0] Batch 79, Loss 0.5309584140777588\n",
      "[Training Epoch 0] Batch 80, Loss 0.5204615592956543\n",
      "[Training Epoch 0] Batch 81, Loss 0.5187721252441406\n",
      "[Training Epoch 0] Batch 82, Loss 0.5328678488731384\n",
      "[Training Epoch 0] Batch 83, Loss 0.5312258005142212\n",
      "[Training Epoch 0] Batch 84, Loss 0.4993169605731964\n",
      "[Training Epoch 0] Batch 85, Loss 0.5001010894775391\n",
      "[Training Epoch 0] Batch 86, Loss 0.5181894898414612\n",
      "[Training Epoch 0] Batch 87, Loss 0.4834859371185303\n",
      "[Training Epoch 0] Batch 88, Loss 0.5064678192138672\n",
      "[Training Epoch 0] Batch 89, Loss 0.47459492087364197\n",
      "[Training Epoch 0] Batch 90, Loss 0.4621604084968567\n",
      "[Training Epoch 0] Batch 91, Loss 0.4858775734901428\n",
      "[Training Epoch 0] Batch 92, Loss 0.46668368577957153\n",
      "[Training Epoch 0] Batch 93, Loss 0.4950915575027466\n",
      "[Training Epoch 0] Batch 94, Loss 0.5197644233703613\n",
      "[Training Epoch 0] Batch 95, Loss 0.49020975828170776\n",
      "[Training Epoch 0] Batch 96, Loss 0.48482048511505127\n",
      "[Training Epoch 0] Batch 97, Loss 0.46392497420310974\n",
      "[Training Epoch 0] Batch 98, Loss 0.5309537053108215\n",
      "[Training Epoch 0] Batch 99, Loss 0.5048482418060303\n",
      "[Training Epoch 0] Batch 100, Loss 0.4790008068084717\n",
      "[Training Epoch 0] Batch 101, Loss 0.4963039755821228\n",
      "[Training Epoch 0] Batch 102, Loss 0.47957906126976013\n",
      "[Training Epoch 0] Batch 103, Loss 0.49343547224998474\n",
      "[Training Epoch 0] Batch 104, Loss 0.48681405186653137\n",
      "[Training Epoch 0] Batch 105, Loss 0.5143330693244934\n",
      "[Training Epoch 0] Batch 106, Loss 0.4850115180015564\n",
      "[Training Epoch 0] Batch 107, Loss 0.49117010831832886\n",
      "[Training Epoch 0] Batch 108, Loss 0.4891688823699951\n",
      "[Training Epoch 0] Batch 109, Loss 0.462955117225647\n",
      "[Training Epoch 0] Batch 110, Loss 0.45924463868141174\n",
      "[Training Epoch 0] Batch 111, Loss 0.46980780363082886\n",
      "[Training Epoch 0] Batch 112, Loss 0.4530333876609802\n",
      "[Training Epoch 0] Batch 113, Loss 0.4859459102153778\n",
      "[Training Epoch 0] Batch 114, Loss 0.4769674241542816\n",
      "[Training Epoch 0] Batch 115, Loss 0.46697798371315\n",
      "[Training Epoch 0] Batch 116, Loss 0.47849029302597046\n",
      "[Training Epoch 0] Batch 117, Loss 0.47358012199401855\n",
      "[Training Epoch 0] Batch 118, Loss 0.459696888923645\n",
      "[Training Epoch 0] Batch 119, Loss 0.466344952583313\n",
      "[Training Epoch 0] Batch 120, Loss 0.4602452218532562\n",
      "[Training Epoch 0] Batch 121, Loss 0.47185206413269043\n",
      "[Training Epoch 0] Batch 122, Loss 0.486391544342041\n",
      "[Training Epoch 0] Batch 123, Loss 0.46968626976013184\n",
      "[Training Epoch 0] Batch 124, Loss 0.4856176972389221\n",
      "[Training Epoch 0] Batch 125, Loss 0.4638499617576599\n",
      "[Training Epoch 0] Batch 126, Loss 0.44807004928588867\n",
      "[Training Epoch 0] Batch 127, Loss 0.4577808082103729\n",
      "[Training Epoch 0] Batch 128, Loss 0.48134690523147583\n",
      "[Training Epoch 0] Batch 129, Loss 0.43954336643218994\n",
      "[Training Epoch 0] Batch 130, Loss 0.46844005584716797\n",
      "[Training Epoch 0] Batch 131, Loss 0.4523240327835083\n",
      "[Training Epoch 0] Batch 132, Loss 0.4491092562675476\n",
      "[Training Epoch 0] Batch 133, Loss 0.4493767023086548\n",
      "[Training Epoch 0] Batch 134, Loss 0.4411335587501526\n",
      "[Training Epoch 0] Batch 135, Loss 0.41801273822784424\n",
      "[Training Epoch 0] Batch 136, Loss 0.42989060282707214\n",
      "[Training Epoch 0] Batch 137, Loss 0.4575589895248413\n",
      "[Training Epoch 0] Batch 138, Loss 0.45009979605674744\n",
      "[Training Epoch 0] Batch 139, Loss 0.45862877368927\n",
      "[Training Epoch 0] Batch 140, Loss 0.4534134864807129\n",
      "[Training Epoch 0] Batch 141, Loss 0.4456061124801636\n",
      "[Training Epoch 0] Batch 142, Loss 0.45211413502693176\n",
      "[Training Epoch 0] Batch 143, Loss 0.4597066640853882\n",
      "[Training Epoch 0] Batch 144, Loss 0.4315710961818695\n",
      "[Training Epoch 0] Batch 145, Loss 0.4416409730911255\n",
      "[Training Epoch 0] Batch 146, Loss 0.46251076459884644\n",
      "[Training Epoch 0] Batch 147, Loss 0.44060441851615906\n",
      "[Training Epoch 0] Batch 148, Loss 0.4351755380630493\n",
      "[Training Epoch 0] Batch 149, Loss 0.45352765917778015\n",
      "[Training Epoch 0] Batch 150, Loss 0.3974948525428772\n",
      "[Training Epoch 0] Batch 151, Loss 0.41818767786026\n",
      "[Training Epoch 0] Batch 152, Loss 0.431491881608963\n",
      "[Training Epoch 0] Batch 153, Loss 0.43247318267822266\n",
      "[Training Epoch 0] Batch 154, Loss 0.4197727143764496\n",
      "[Training Epoch 0] Batch 155, Loss 0.4536796808242798\n",
      "[Training Epoch 0] Batch 156, Loss 0.4429060220718384\n",
      "[Training Epoch 0] Batch 157, Loss 0.4418041706085205\n",
      "[Training Epoch 0] Batch 158, Loss 0.37718528509140015\n",
      "[Training Epoch 0] Batch 159, Loss 0.39992624521255493\n",
      "[Training Epoch 0] Batch 160, Loss 0.4440596103668213\n",
      "[Training Epoch 0] Batch 161, Loss 0.4262555241584778\n",
      "[Training Epoch 0] Batch 162, Loss 0.44784605503082275\n",
      "[Training Epoch 0] Batch 163, Loss 0.421407550573349\n",
      "[Training Epoch 0] Batch 164, Loss 0.38559502363204956\n",
      "[Training Epoch 0] Batch 165, Loss 0.4289644658565521\n",
      "[Training Epoch 0] Batch 166, Loss 0.40427038073539734\n",
      "[Training Epoch 0] Batch 167, Loss 0.41140180826187134\n",
      "[Training Epoch 0] Batch 168, Loss 0.4049200415611267\n",
      "[Training Epoch 0] Batch 169, Loss 0.4101870656013489\n",
      "[Training Epoch 0] Batch 170, Loss 0.4141466021537781\n",
      "[Training Epoch 0] Batch 171, Loss 0.4221484959125519\n",
      "[Training Epoch 0] Batch 172, Loss 0.40463754534721375\n",
      "[Training Epoch 0] Batch 173, Loss 0.41236090660095215\n",
      "[Training Epoch 0] Batch 174, Loss 0.38732731342315674\n",
      "[Training Epoch 0] Batch 175, Loss 0.3951264023780823\n",
      "[Training Epoch 0] Batch 176, Loss 0.37591665983200073\n",
      "[Training Epoch 0] Batch 177, Loss 0.406850665807724\n",
      "[Training Epoch 0] Batch 178, Loss 0.3843648135662079\n",
      "[Training Epoch 0] Batch 179, Loss 0.3908425569534302\n",
      "[Training Epoch 0] Batch 180, Loss 0.38161855936050415\n",
      "[Training Epoch 0] Batch 181, Loss 0.3729099631309509\n",
      "[Training Epoch 0] Batch 182, Loss 0.3976588249206543\n",
      "[Training Epoch 0] Batch 183, Loss 0.4200013279914856\n",
      "[Training Epoch 0] Batch 184, Loss 0.3983052372932434\n",
      "[Training Epoch 0] Batch 185, Loss 0.4442571699619293\n",
      "[Training Epoch 0] Batch 186, Loss 0.400742769241333\n",
      "[Training Epoch 0] Batch 187, Loss 0.3984316289424896\n",
      "[Training Epoch 0] Batch 188, Loss 0.35878100991249084\n",
      "[Training Epoch 0] Batch 189, Loss 0.38873788714408875\n",
      "[Training Epoch 0] Batch 190, Loss 0.3755834102630615\n",
      "[Training Epoch 0] Batch 191, Loss 0.36588746309280396\n",
      "[Training Epoch 0] Batch 192, Loss 0.3939582407474518\n",
      "[Training Epoch 0] Batch 193, Loss 0.4084911644458771\n",
      "[Training Epoch 0] Batch 194, Loss 0.37557730078697205\n",
      "[Training Epoch 0] Batch 195, Loss 0.40612974762916565\n",
      "[Training Epoch 0] Batch 196, Loss 0.38874247670173645\n",
      "[Training Epoch 0] Batch 197, Loss 0.37710341811180115\n",
      "[Training Epoch 0] Batch 198, Loss 0.40883612632751465\n",
      "[Training Epoch 0] Batch 199, Loss 0.39909398555755615\n",
      "[Training Epoch 0] Batch 200, Loss 0.35818105936050415\n",
      "[Training Epoch 0] Batch 201, Loss 0.3709726333618164\n",
      "[Training Epoch 0] Batch 202, Loss 0.3869863748550415\n",
      "[Training Epoch 0] Batch 203, Loss 0.35926979780197144\n",
      "[Training Epoch 0] Batch 204, Loss 0.4122686982154846\n",
      "[Training Epoch 0] Batch 205, Loss 0.3668554723262787\n",
      "[Training Epoch 0] Batch 206, Loss 0.39185279607772827\n",
      "[Training Epoch 0] Batch 207, Loss 0.41909199953079224\n",
      "[Training Epoch 0] Batch 208, Loss 0.4009340703487396\n",
      "[Training Epoch 0] Batch 209, Loss 0.41039448976516724\n",
      "[Training Epoch 0] Batch 210, Loss 0.357857882976532\n",
      "[Training Epoch 0] Batch 211, Loss 0.3923449218273163\n",
      "[Training Epoch 0] Batch 212, Loss 0.3854272961616516\n",
      "[Training Epoch 0] Batch 213, Loss 0.36465853452682495\n",
      "[Training Epoch 0] Batch 214, Loss 0.38444972038269043\n",
      "[Training Epoch 0] Batch 215, Loss 0.3712940216064453\n",
      "[Training Epoch 0] Batch 216, Loss 0.3525238037109375\n",
      "[Training Epoch 0] Batch 217, Loss 0.3665364682674408\n",
      "[Training Epoch 0] Batch 218, Loss 0.3661005198955536\n",
      "[Training Epoch 0] Batch 219, Loss 0.38350629806518555\n",
      "[Training Epoch 0] Batch 220, Loss 0.356835275888443\n",
      "[Training Epoch 0] Batch 221, Loss 0.3854023218154907\n",
      "[Training Epoch 0] Batch 222, Loss 0.404348224401474\n",
      "[Training Epoch 0] Batch 223, Loss 0.3559993505477905\n",
      "[Training Epoch 0] Batch 224, Loss 0.4066557288169861\n",
      "[Training Epoch 0] Batch 225, Loss 0.37953388690948486\n",
      "[Training Epoch 0] Batch 226, Loss 0.38974446058273315\n",
      "[Training Epoch 0] Batch 227, Loss 0.42392900586128235\n",
      "[Training Epoch 0] Batch 228, Loss 0.40677982568740845\n",
      "[Training Epoch 0] Batch 229, Loss 0.38325074315071106\n",
      "[Training Epoch 0] Batch 230, Loss 0.3940916061401367\n",
      "[Training Epoch 0] Batch 231, Loss 0.369000107049942\n",
      "[Training Epoch 0] Batch 232, Loss 0.38164472579956055\n",
      "[Training Epoch 0] Batch 233, Loss 0.38634973764419556\n",
      "[Training Epoch 0] Batch 234, Loss 0.38510215282440186\n",
      "[Training Epoch 0] Batch 235, Loss 0.3715798854827881\n",
      "[Training Epoch 0] Batch 236, Loss 0.384297639131546\n",
      "[Training Epoch 0] Batch 237, Loss 0.40352126955986023\n",
      "[Training Epoch 0] Batch 238, Loss 0.3692864775657654\n",
      "[Training Epoch 0] Batch 239, Loss 0.36508363485336304\n",
      "[Training Epoch 0] Batch 240, Loss 0.36198991537094116\n",
      "[Training Epoch 0] Batch 241, Loss 0.37550243735313416\n",
      "[Training Epoch 0] Batch 242, Loss 0.35537803173065186\n",
      "[Training Epoch 0] Batch 243, Loss 0.39820241928100586\n",
      "[Training Epoch 0] Batch 244, Loss 0.3871874213218689\n",
      "[Training Epoch 0] Batch 245, Loss 0.39901620149612427\n",
      "[Training Epoch 0] Batch 246, Loss 0.3787800073623657\n",
      "[Training Epoch 0] Batch 247, Loss 0.38241928815841675\n",
      "[Training Epoch 0] Batch 248, Loss 0.371234267950058\n",
      "[Training Epoch 0] Batch 249, Loss 0.38230428099632263\n",
      "[Training Epoch 0] Batch 250, Loss 0.39660578966140747\n",
      "[Training Epoch 0] Batch 251, Loss 0.3734201192855835\n",
      "[Training Epoch 0] Batch 252, Loss 0.36676910519599915\n",
      "[Training Epoch 0] Batch 253, Loss 0.3414561152458191\n",
      "[Training Epoch 0] Batch 254, Loss 0.34542614221572876\n",
      "[Training Epoch 0] Batch 255, Loss 0.38215094804763794\n",
      "[Training Epoch 0] Batch 256, Loss 0.34813767671585083\n",
      "[Training Epoch 0] Batch 257, Loss 0.34971290826797485\n",
      "[Training Epoch 0] Batch 258, Loss 0.3832712769508362\n",
      "[Training Epoch 0] Batch 259, Loss 0.3668627142906189\n",
      "[Training Epoch 0] Batch 260, Loss 0.39617079496383667\n",
      "[Training Epoch 0] Batch 261, Loss 0.34565985202789307\n",
      "[Training Epoch 0] Batch 262, Loss 0.37348926067352295\n",
      "[Training Epoch 0] Batch 263, Loss 0.40140753984451294\n",
      "[Training Epoch 0] Batch 264, Loss 0.34105002880096436\n",
      "[Training Epoch 0] Batch 265, Loss 0.37410783767700195\n",
      "[Training Epoch 0] Batch 266, Loss 0.3601369857788086\n",
      "[Training Epoch 0] Batch 267, Loss 0.35267847776412964\n",
      "[Training Epoch 0] Batch 268, Loss 0.3927990198135376\n",
      "[Training Epoch 0] Batch 269, Loss 0.3945069909095764\n",
      "[Training Epoch 0] Batch 270, Loss 0.35460326075553894\n",
      "[Training Epoch 0] Batch 271, Loss 0.40605831146240234\n",
      "[Training Epoch 0] Batch 272, Loss 0.38159847259521484\n",
      "[Training Epoch 0] Batch 273, Loss 0.361343652009964\n",
      "[Training Epoch 0] Batch 274, Loss 0.37670284509658813\n",
      "[Training Epoch 0] Batch 275, Loss 0.3671075403690338\n",
      "[Training Epoch 0] Batch 276, Loss 0.39799365401268005\n",
      "[Training Epoch 0] Batch 277, Loss 0.333636999130249\n",
      "[Training Epoch 0] Batch 278, Loss 0.3712373971939087\n",
      "[Training Epoch 0] Batch 279, Loss 0.3689316511154175\n",
      "[Training Epoch 0] Batch 280, Loss 0.3706859350204468\n",
      "[Training Epoch 0] Batch 281, Loss 0.36525195837020874\n",
      "[Training Epoch 0] Batch 282, Loss 0.37823763489723206\n",
      "[Training Epoch 0] Batch 283, Loss 0.3786875605583191\n",
      "[Training Epoch 0] Batch 284, Loss 0.3823205828666687\n",
      "[Training Epoch 0] Batch 285, Loss 0.360774427652359\n",
      "[Training Epoch 0] Batch 286, Loss 0.3561016023159027\n",
      "[Training Epoch 0] Batch 287, Loss 0.3744867742061615\n",
      "[Training Epoch 0] Batch 288, Loss 0.37849846482276917\n",
      "[Training Epoch 0] Batch 289, Loss 0.35141104459762573\n",
      "[Training Epoch 0] Batch 290, Loss 0.3714824318885803\n",
      "[Training Epoch 0] Batch 291, Loss 0.37465137243270874\n",
      "[Training Epoch 0] Batch 292, Loss 0.3640706241130829\n",
      "[Training Epoch 0] Batch 293, Loss 0.379787802696228\n",
      "[Training Epoch 0] Batch 294, Loss 0.3682308793067932\n",
      "[Training Epoch 0] Batch 295, Loss 0.3377367854118347\n",
      "[Training Epoch 0] Batch 296, Loss 0.38312745094299316\n",
      "[Training Epoch 0] Batch 297, Loss 0.37152498960494995\n",
      "[Training Epoch 0] Batch 298, Loss 0.3472980856895447\n",
      "[Training Epoch 0] Batch 299, Loss 0.3675769567489624\n",
      "[Training Epoch 0] Batch 300, Loss 0.3623464107513428\n",
      "[Training Epoch 0] Batch 301, Loss 0.40171170234680176\n",
      "[Training Epoch 0] Batch 302, Loss 0.3759320378303528\n",
      "[Training Epoch 0] Batch 303, Loss 0.3614117503166199\n",
      "[Training Epoch 0] Batch 304, Loss 0.3898005783557892\n",
      "[Training Epoch 0] Batch 305, Loss 0.36454468965530396\n",
      "[Training Epoch 0] Batch 306, Loss 0.35174158215522766\n",
      "[Training Epoch 0] Batch 307, Loss 0.3688453435897827\n",
      "[Training Epoch 0] Batch 308, Loss 0.34561967849731445\n",
      "[Training Epoch 0] Batch 309, Loss 0.372152179479599\n",
      "[Training Epoch 0] Batch 310, Loss 0.39938873052597046\n",
      "[Training Epoch 0] Batch 311, Loss 0.3785093426704407\n",
      "[Training Epoch 0] Batch 312, Loss 0.35306066274642944\n",
      "[Training Epoch 0] Batch 313, Loss 0.3774743974208832\n",
      "[Training Epoch 0] Batch 314, Loss 0.35673320293426514\n",
      "[Training Epoch 0] Batch 315, Loss 0.35956403613090515\n",
      "[Training Epoch 0] Batch 316, Loss 0.4025305211544037\n",
      "[Training Epoch 0] Batch 317, Loss 0.3596155643463135\n",
      "[Training Epoch 0] Batch 318, Loss 0.3703525960445404\n",
      "[Training Epoch 0] Batch 319, Loss 0.3801465332508087\n",
      "[Training Epoch 0] Batch 320, Loss 0.3618561923503876\n",
      "[Training Epoch 0] Batch 321, Loss 0.3782651424407959\n",
      "[Training Epoch 0] Batch 322, Loss 0.3421776294708252\n",
      "[Training Epoch 0] Batch 323, Loss 0.3491664230823517\n",
      "[Training Epoch 0] Batch 324, Loss 0.3878430724143982\n",
      "[Training Epoch 0] Batch 325, Loss 0.36690554022789\n",
      "[Training Epoch 0] Batch 326, Loss 0.35984593629837036\n",
      "[Training Epoch 0] Batch 327, Loss 0.37263360619544983\n",
      "[Training Epoch 0] Batch 328, Loss 0.36596590280532837\n",
      "[Training Epoch 0] Batch 329, Loss 0.34691357612609863\n",
      "[Training Epoch 0] Batch 330, Loss 0.34644395112991333\n",
      "[Training Epoch 0] Batch 331, Loss 0.3966546356678009\n",
      "[Training Epoch 0] Batch 332, Loss 0.38434159755706787\n",
      "[Training Epoch 0] Batch 333, Loss 0.38773906230926514\n",
      "[Training Epoch 0] Batch 334, Loss 0.3769494295120239\n",
      "[Training Epoch 0] Batch 335, Loss 0.376977801322937\n",
      "[Training Epoch 0] Batch 336, Loss 0.37681710720062256\n",
      "[Training Epoch 0] Batch 337, Loss 0.33008715510368347\n",
      "[Training Epoch 0] Batch 338, Loss 0.3453637957572937\n",
      "[Training Epoch 0] Batch 339, Loss 0.3771899938583374\n",
      "[Training Epoch 0] Batch 340, Loss 0.37865275144577026\n",
      "[Training Epoch 0] Batch 341, Loss 0.3655185103416443\n",
      "[Training Epoch 0] Batch 342, Loss 0.3726515769958496\n",
      "[Training Epoch 0] Batch 343, Loss 0.39073532819747925\n",
      "[Training Epoch 0] Batch 344, Loss 0.35395222902297974\n",
      "[Training Epoch 0] Batch 345, Loss 0.38891661167144775\n",
      "[Training Epoch 0] Batch 346, Loss 0.38355743885040283\n",
      "[Training Epoch 0] Batch 347, Loss 0.3846210837364197\n",
      "[Training Epoch 0] Batch 348, Loss 0.38509294390678406\n",
      "[Training Epoch 0] Batch 349, Loss 0.36181241273880005\n",
      "[Training Epoch 0] Batch 350, Loss 0.3760918378829956\n",
      "[Training Epoch 0] Batch 351, Loss 0.34092819690704346\n",
      "[Training Epoch 0] Batch 352, Loss 0.34361809492111206\n",
      "[Training Epoch 0] Batch 353, Loss 0.3616653084754944\n",
      "[Training Epoch 0] Batch 354, Loss 0.36196649074554443\n",
      "[Training Epoch 0] Batch 355, Loss 0.344657301902771\n",
      "[Training Epoch 0] Batch 356, Loss 0.34460780024528503\n",
      "[Training Epoch 0] Batch 357, Loss 0.3534918427467346\n",
      "[Training Epoch 0] Batch 358, Loss 0.34856122732162476\n",
      "[Training Epoch 0] Batch 359, Loss 0.3640575706958771\n",
      "[Training Epoch 0] Batch 360, Loss 0.36230742931365967\n",
      "[Training Epoch 0] Batch 361, Loss 0.36782902479171753\n",
      "[Training Epoch 0] Batch 362, Loss 0.3246232867240906\n",
      "[Training Epoch 0] Batch 363, Loss 0.36155766248703003\n",
      "[Training Epoch 0] Batch 364, Loss 0.3695061504840851\n",
      "[Training Epoch 0] Batch 365, Loss 0.34763026237487793\n",
      "[Training Epoch 0] Batch 366, Loss 0.3563714921474457\n",
      "[Training Epoch 0] Batch 367, Loss 0.38530662655830383\n",
      "[Training Epoch 0] Batch 368, Loss 0.3745688199996948\n",
      "[Training Epoch 0] Batch 369, Loss 0.3758417069911957\n",
      "[Training Epoch 0] Batch 370, Loss 0.34202277660369873\n",
      "[Training Epoch 0] Batch 371, Loss 0.35433429479599\n",
      "[Training Epoch 0] Batch 372, Loss 0.3937242031097412\n",
      "[Training Epoch 0] Batch 373, Loss 0.3892499804496765\n",
      "[Training Epoch 0] Batch 374, Loss 0.3684271574020386\n",
      "[Training Epoch 0] Batch 375, Loss 0.36808088421821594\n",
      "[Training Epoch 0] Batch 376, Loss 0.36227935552597046\n",
      "[Training Epoch 0] Batch 377, Loss 0.34555065631866455\n",
      "[Training Epoch 0] Batch 378, Loss 0.4018028974533081\n",
      "[Training Epoch 0] Batch 379, Loss 0.38888412714004517\n",
      "[Training Epoch 0] Batch 380, Loss 0.33381444215774536\n",
      "[Training Epoch 0] Batch 381, Loss 0.3628140687942505\n",
      "[Training Epoch 0] Batch 382, Loss 0.3793545961380005\n",
      "[Training Epoch 0] Batch 383, Loss 0.36151450872421265\n",
      "[Training Epoch 0] Batch 384, Loss 0.36102294921875\n",
      "[Training Epoch 0] Batch 385, Loss 0.3437608480453491\n",
      "[Training Epoch 0] Batch 386, Loss 0.3392811417579651\n",
      "[Training Epoch 0] Batch 387, Loss 0.3592532277107239\n",
      "[Training Epoch 0] Batch 388, Loss 0.3562999665737152\n",
      "[Training Epoch 0] Batch 389, Loss 0.36102747917175293\n",
      "[Training Epoch 0] Batch 390, Loss 0.37896549701690674\n",
      "[Training Epoch 0] Batch 391, Loss 0.34687262773513794\n",
      "[Training Epoch 0] Batch 392, Loss 0.3754653334617615\n",
      "[Training Epoch 0] Batch 393, Loss 0.35244154930114746\n",
      "[Training Epoch 0] Batch 394, Loss 0.364729106426239\n",
      "[Training Epoch 0] Batch 395, Loss 0.3706364631652832\n",
      "[Training Epoch 0] Batch 396, Loss 0.38488033413887024\n",
      "[Training Epoch 0] Batch 397, Loss 0.3560773730278015\n",
      "[Training Epoch 0] Batch 398, Loss 0.4086851179599762\n",
      "[Training Epoch 0] Batch 399, Loss 0.36614829301834106\n",
      "[Training Epoch 0] Batch 400, Loss 0.36320728063583374\n",
      "[Training Epoch 0] Batch 401, Loss 0.3807750642299652\n",
      "[Training Epoch 0] Batch 402, Loss 0.35877907276153564\n",
      "[Training Epoch 0] Batch 403, Loss 0.3715612292289734\n",
      "[Training Epoch 0] Batch 404, Loss 0.36341893672943115\n",
      "[Training Epoch 0] Batch 405, Loss 0.3477001190185547\n",
      "[Training Epoch 0] Batch 406, Loss 0.3316566050052643\n",
      "[Training Epoch 0] Batch 407, Loss 0.3782635033130646\n",
      "[Training Epoch 0] Batch 408, Loss 0.3645656108856201\n",
      "[Training Epoch 0] Batch 409, Loss 0.36223435401916504\n",
      "[Training Epoch 0] Batch 410, Loss 0.39608442783355713\n",
      "[Training Epoch 0] Batch 411, Loss 0.37915951013565063\n",
      "[Training Epoch 0] Batch 412, Loss 0.3539793789386749\n",
      "[Training Epoch 0] Batch 413, Loss 0.352496862411499\n",
      "[Training Epoch 0] Batch 414, Loss 0.37563052773475647\n",
      "[Training Epoch 0] Batch 415, Loss 0.3897462487220764\n",
      "[Training Epoch 0] Batch 416, Loss 0.36967939138412476\n",
      "[Training Epoch 0] Batch 417, Loss 0.36720675230026245\n",
      "[Training Epoch 0] Batch 418, Loss 0.3667762875556946\n",
      "[Training Epoch 0] Batch 419, Loss 0.35923805832862854\n",
      "[Training Epoch 0] Batch 420, Loss 0.36065474152565\n",
      "[Training Epoch 0] Batch 421, Loss 0.35002514719963074\n",
      "[Training Epoch 0] Batch 422, Loss 0.3456202745437622\n",
      "[Training Epoch 0] Batch 423, Loss 0.356958270072937\n",
      "[Training Epoch 0] Batch 424, Loss 0.35025906562805176\n",
      "[Training Epoch 0] Batch 425, Loss 0.3734925091266632\n",
      "[Training Epoch 0] Batch 426, Loss 0.38437575101852417\n",
      "[Training Epoch 0] Batch 427, Loss 0.35975176095962524\n",
      "[Training Epoch 0] Batch 428, Loss 0.3635700047016144\n",
      "[Training Epoch 0] Batch 429, Loss 0.3803613483905792\n",
      "[Training Epoch 0] Batch 430, Loss 0.36924484372138977\n",
      "[Training Epoch 0] Batch 431, Loss 0.3824138045310974\n",
      "[Training Epoch 0] Batch 432, Loss 0.3805246651172638\n",
      "[Training Epoch 0] Batch 433, Loss 0.38758277893066406\n",
      "[Training Epoch 0] Batch 434, Loss 0.3607926070690155\n",
      "[Training Epoch 0] Batch 435, Loss 0.3650650382041931\n",
      "[Training Epoch 0] Batch 436, Loss 0.34746333956718445\n",
      "[Training Epoch 0] Batch 437, Loss 0.37241995334625244\n",
      "[Training Epoch 0] Batch 438, Loss 0.35911044478416443\n",
      "[Training Epoch 0] Batch 439, Loss 0.3692607283592224\n",
      "[Training Epoch 0] Batch 440, Loss 0.3929469585418701\n",
      "[Training Epoch 0] Batch 441, Loss 0.3339625298976898\n",
      "[Training Epoch 0] Batch 442, Loss 0.3581516146659851\n",
      "[Training Epoch 0] Batch 443, Loss 0.3495163321495056\n",
      "[Training Epoch 0] Batch 444, Loss 0.3631204664707184\n",
      "[Training Epoch 0] Batch 445, Loss 0.40086400508880615\n",
      "[Training Epoch 0] Batch 446, Loss 0.3659646511077881\n",
      "[Training Epoch 0] Batch 447, Loss 0.37030813097953796\n",
      "[Training Epoch 0] Batch 448, Loss 0.37613755464553833\n",
      "[Training Epoch 0] Batch 449, Loss 0.3573794662952423\n",
      "[Training Epoch 0] Batch 450, Loss 0.3480253219604492\n",
      "[Training Epoch 0] Batch 451, Loss 0.3758109211921692\n",
      "[Training Epoch 0] Batch 452, Loss 0.35731884837150574\n",
      "[Training Epoch 0] Batch 453, Loss 0.37008923292160034\n",
      "[Training Epoch 0] Batch 454, Loss 0.36577165126800537\n",
      "[Training Epoch 0] Batch 455, Loss 0.35420849919319153\n",
      "[Training Epoch 0] Batch 456, Loss 0.380499929189682\n",
      "[Training Epoch 0] Batch 457, Loss 0.3559581935405731\n",
      "[Training Epoch 0] Batch 458, Loss 0.385103702545166\n",
      "[Training Epoch 0] Batch 459, Loss 0.35285109281539917\n",
      "[Training Epoch 0] Batch 460, Loss 0.3367505669593811\n",
      "[Training Epoch 0] Batch 461, Loss 0.3740236759185791\n",
      "[Training Epoch 0] Batch 462, Loss 0.3531196415424347\n",
      "[Training Epoch 0] Batch 463, Loss 0.39109599590301514\n",
      "[Training Epoch 0] Batch 464, Loss 0.3676700294017792\n",
      "[Training Epoch 0] Batch 465, Loss 0.36681851744651794\n",
      "[Training Epoch 0] Batch 466, Loss 0.359045147895813\n",
      "[Training Epoch 0] Batch 467, Loss 0.36322134733200073\n",
      "[Training Epoch 0] Batch 468, Loss 0.3563178777694702\n",
      "[Training Epoch 0] Batch 469, Loss 0.3945108652114868\n",
      "[Training Epoch 0] Batch 470, Loss 0.3685010075569153\n",
      "[Training Epoch 0] Batch 471, Loss 0.37041178345680237\n",
      "[Training Epoch 0] Batch 472, Loss 0.3875126838684082\n",
      "[Training Epoch 0] Batch 473, Loss 0.3861309289932251\n",
      "[Training Epoch 0] Batch 474, Loss 0.36512768268585205\n",
      "[Training Epoch 0] Batch 475, Loss 0.36474379897117615\n",
      "[Training Epoch 0] Batch 476, Loss 0.3703584671020508\n",
      "[Training Epoch 0] Batch 477, Loss 0.3407864570617676\n",
      "[Training Epoch 0] Batch 478, Loss 0.37594473361968994\n",
      "[Training Epoch 0] Batch 479, Loss 0.35614264011383057\n",
      "[Training Epoch 0] Batch 480, Loss 0.387997031211853\n",
      "[Training Epoch 0] Batch 481, Loss 0.35754236578941345\n",
      "[Training Epoch 0] Batch 482, Loss 0.33400505781173706\n",
      "[Training Epoch 0] Batch 483, Loss 0.3484790325164795\n",
      "[Training Epoch 0] Batch 484, Loss 0.358781635761261\n",
      "[Training Epoch 0] Batch 485, Loss 0.36446672677993774\n",
      "[Training Epoch 0] Batch 486, Loss 0.37708061933517456\n",
      "[Training Epoch 0] Batch 487, Loss 0.3441806435585022\n",
      "[Training Epoch 0] Batch 488, Loss 0.3790024518966675\n",
      "[Training Epoch 0] Batch 489, Loss 0.3672967255115509\n",
      "[Training Epoch 0] Batch 490, Loss 0.3685360848903656\n",
      "[Training Epoch 0] Batch 491, Loss 0.34779125452041626\n",
      "[Training Epoch 0] Batch 492, Loss 0.35335710644721985\n",
      "[Training Epoch 0] Batch 493, Loss 0.36286407709121704\n",
      "[Training Epoch 0] Batch 494, Loss 0.4090961813926697\n",
      "[Training Epoch 0] Batch 495, Loss 0.3713403344154358\n",
      "[Training Epoch 0] Batch 496, Loss 0.36203208565711975\n",
      "[Training Epoch 0] Batch 497, Loss 0.3901965320110321\n",
      "[Training Epoch 0] Batch 498, Loss 0.35459381341934204\n",
      "[Training Epoch 0] Batch 499, Loss 0.3272581100463867\n",
      "[Training Epoch 0] Batch 500, Loss 0.3380635976791382\n",
      "[Training Epoch 0] Batch 501, Loss 0.35848310589790344\n",
      "[Training Epoch 0] Batch 502, Loss 0.3742067515850067\n",
      "[Training Epoch 0] Batch 503, Loss 0.37049853801727295\n",
      "[Training Epoch 0] Batch 504, Loss 0.32606637477874756\n",
      "[Training Epoch 0] Batch 505, Loss 0.37201201915740967\n",
      "[Training Epoch 0] Batch 506, Loss 0.37778228521347046\n",
      "[Training Epoch 0] Batch 507, Loss 0.3690456748008728\n",
      "[Training Epoch 0] Batch 508, Loss 0.32723790407180786\n",
      "[Training Epoch 0] Batch 509, Loss 0.40381956100463867\n",
      "[Training Epoch 0] Batch 510, Loss 0.3723808526992798\n",
      "[Training Epoch 0] Batch 511, Loss 0.3729296922683716\n",
      "[Training Epoch 0] Batch 512, Loss 0.39895349740982056\n",
      "[Training Epoch 0] Batch 513, Loss 0.3606158494949341\n",
      "[Training Epoch 0] Batch 514, Loss 0.3569570779800415\n",
      "[Training Epoch 0] Batch 515, Loss 0.35663676261901855\n",
      "[Training Epoch 0] Batch 516, Loss 0.38156044483184814\n",
      "[Training Epoch 0] Batch 517, Loss 0.36641380190849304\n",
      "[Training Epoch 0] Batch 518, Loss 0.36581623554229736\n",
      "[Training Epoch 0] Batch 519, Loss 0.3526330292224884\n",
      "[Training Epoch 0] Batch 520, Loss 0.4038582444190979\n",
      "[Training Epoch 0] Batch 521, Loss 0.3469417095184326\n",
      "[Training Epoch 0] Batch 522, Loss 0.3623158037662506\n",
      "[Training Epoch 0] Batch 523, Loss 0.3727814555168152\n",
      "[Training Epoch 0] Batch 524, Loss 0.3911899924278259\n",
      "[Training Epoch 0] Batch 525, Loss 0.36272501945495605\n",
      "[Training Epoch 0] Batch 526, Loss 0.38952410221099854\n",
      "[Training Epoch 0] Batch 527, Loss 0.3442670404911041\n",
      "[Training Epoch 0] Batch 528, Loss 0.4026947617530823\n",
      "[Training Epoch 0] Batch 529, Loss 0.3470066487789154\n",
      "[Training Epoch 0] Batch 530, Loss 0.3667752742767334\n",
      "[Training Epoch 0] Batch 531, Loss 0.3522931933403015\n",
      "[Training Epoch 0] Batch 532, Loss 0.36026597023010254\n",
      "[Training Epoch 0] Batch 533, Loss 0.3552291989326477\n",
      "[Training Epoch 0] Batch 534, Loss 0.35558605194091797\n",
      "[Training Epoch 0] Batch 535, Loss 0.3843660354614258\n",
      "[Training Epoch 0] Batch 536, Loss 0.3472987413406372\n",
      "[Training Epoch 0] Batch 537, Loss 0.37673068046569824\n",
      "[Training Epoch 0] Batch 538, Loss 0.3409229516983032\n",
      "[Training Epoch 0] Batch 539, Loss 0.3769058585166931\n",
      "[Training Epoch 0] Batch 540, Loss 0.3634938895702362\n",
      "[Training Epoch 0] Batch 541, Loss 0.36139383912086487\n",
      "[Training Epoch 0] Batch 542, Loss 0.36541855335235596\n",
      "[Training Epoch 0] Batch 543, Loss 0.3646526634693146\n",
      "[Training Epoch 0] Batch 544, Loss 0.36880922317504883\n",
      "[Training Epoch 0] Batch 545, Loss 0.34271925687789917\n",
      "[Training Epoch 0] Batch 546, Loss 0.35908061265945435\n",
      "[Training Epoch 0] Batch 547, Loss 0.3609420657157898\n",
      "[Training Epoch 0] Batch 548, Loss 0.3417806625366211\n",
      "[Training Epoch 0] Batch 549, Loss 0.3483704924583435\n",
      "[Training Epoch 0] Batch 550, Loss 0.355061799287796\n",
      "[Training Epoch 0] Batch 551, Loss 0.3406471610069275\n",
      "[Training Epoch 0] Batch 552, Loss 0.33703097701072693\n",
      "[Training Epoch 0] Batch 553, Loss 0.3350503742694855\n",
      "[Training Epoch 0] Batch 554, Loss 0.34020572900772095\n",
      "[Training Epoch 0] Batch 555, Loss 0.371769517660141\n",
      "[Training Epoch 0] Batch 556, Loss 0.38206952810287476\n",
      "[Training Epoch 0] Batch 557, Loss 0.3299190402030945\n",
      "[Training Epoch 0] Batch 558, Loss 0.3865725100040436\n",
      "[Training Epoch 0] Batch 559, Loss 0.4011309742927551\n",
      "[Training Epoch 0] Batch 560, Loss 0.3660387396812439\n",
      "[Training Epoch 0] Batch 561, Loss 0.3690292239189148\n",
      "[Training Epoch 0] Batch 562, Loss 0.32085227966308594\n",
      "[Training Epoch 0] Batch 563, Loss 0.3543349504470825\n",
      "[Training Epoch 0] Batch 564, Loss 0.37269383668899536\n",
      "[Training Epoch 0] Batch 565, Loss 0.3540681004524231\n",
      "[Training Epoch 0] Batch 566, Loss 0.34707826375961304\n",
      "[Training Epoch 0] Batch 567, Loss 0.3613346219062805\n",
      "[Training Epoch 0] Batch 568, Loss 0.3811279535293579\n",
      "[Training Epoch 0] Batch 569, Loss 0.3777487277984619\n",
      "[Training Epoch 0] Batch 570, Loss 0.3822726011276245\n",
      "[Training Epoch 0] Batch 571, Loss 0.34445059299468994\n",
      "[Training Epoch 0] Batch 572, Loss 0.3809049725532532\n",
      "[Training Epoch 0] Batch 573, Loss 0.36520737409591675\n",
      "[Training Epoch 0] Batch 574, Loss 0.3548445701599121\n",
      "[Training Epoch 0] Batch 575, Loss 0.3621053695678711\n",
      "[Training Epoch 0] Batch 576, Loss 0.3932526707649231\n",
      "[Training Epoch 0] Batch 577, Loss 0.3454930782318115\n",
      "[Training Epoch 0] Batch 578, Loss 0.388405442237854\n",
      "[Training Epoch 0] Batch 579, Loss 0.3470948338508606\n",
      "[Training Epoch 0] Batch 580, Loss 0.37380409240722656\n",
      "[Training Epoch 0] Batch 581, Loss 0.3461085557937622\n",
      "[Training Epoch 0] Batch 582, Loss 0.3614313006401062\n",
      "[Training Epoch 0] Batch 583, Loss 0.35670679807662964\n",
      "[Training Epoch 0] Batch 584, Loss 0.3814603090286255\n",
      "[Training Epoch 0] Batch 585, Loss 0.34883224964141846\n",
      "[Training Epoch 0] Batch 586, Loss 0.3677264451980591\n",
      "[Training Epoch 0] Batch 587, Loss 0.36494889855384827\n",
      "[Training Epoch 0] Batch 588, Loss 0.3575899600982666\n",
      "[Training Epoch 0] Batch 589, Loss 0.35286518931388855\n",
      "[Training Epoch 0] Batch 590, Loss 0.3396337628364563\n",
      "[Training Epoch 0] Batch 591, Loss 0.36371517181396484\n",
      "[Training Epoch 0] Batch 592, Loss 0.3450663685798645\n",
      "[Training Epoch 0] Batch 593, Loss 0.3964603543281555\n",
      "[Training Epoch 0] Batch 594, Loss 0.35609158873558044\n",
      "[Training Epoch 0] Batch 595, Loss 0.3563549518585205\n",
      "[Training Epoch 0] Batch 596, Loss 0.35624414682388306\n",
      "[Training Epoch 0] Batch 597, Loss 0.3428228497505188\n",
      "[Training Epoch 0] Batch 598, Loss 0.3546273708343506\n",
      "[Training Epoch 0] Batch 599, Loss 0.34540438652038574\n",
      "[Training Epoch 0] Batch 600, Loss 0.360154926776886\n",
      "[Training Epoch 0] Batch 601, Loss 0.3731003403663635\n",
      "[Training Epoch 0] Batch 602, Loss 0.3371385931968689\n",
      "[Training Epoch 0] Batch 603, Loss 0.32556480169296265\n",
      "[Training Epoch 0] Batch 604, Loss 0.366332471370697\n",
      "[Training Epoch 0] Batch 605, Loss 0.36768990755081177\n",
      "[Training Epoch 0] Batch 606, Loss 0.36895716190338135\n",
      "[Training Epoch 0] Batch 607, Loss 0.3680492639541626\n",
      "[Training Epoch 0] Batch 608, Loss 0.33855801820755005\n",
      "[Training Epoch 0] Batch 609, Loss 0.3566315472126007\n",
      "[Training Epoch 0] Batch 610, Loss 0.3753719925880432\n",
      "[Training Epoch 0] Batch 611, Loss 0.3443151116371155\n",
      "[Training Epoch 0] Batch 612, Loss 0.3507668972015381\n",
      "[Training Epoch 0] Batch 613, Loss 0.3701055645942688\n",
      "[Training Epoch 0] Batch 614, Loss 0.36093956232070923\n",
      "[Training Epoch 0] Batch 615, Loss 0.3648987412452698\n",
      "[Training Epoch 0] Batch 616, Loss 0.3552926778793335\n",
      "[Training Epoch 0] Batch 617, Loss 0.38015201687812805\n",
      "[Training Epoch 0] Batch 618, Loss 0.3645303249359131\n",
      "[Training Epoch 0] Batch 619, Loss 0.36517608165740967\n",
      "[Training Epoch 0] Batch 620, Loss 0.37250038981437683\n",
      "[Training Epoch 0] Batch 621, Loss 0.33837491273880005\n",
      "[Training Epoch 0] Batch 622, Loss 0.38743358850479126\n",
      "[Training Epoch 0] Batch 623, Loss 0.32654276490211487\n",
      "[Training Epoch 0] Batch 624, Loss 0.3559240698814392\n",
      "[Training Epoch 0] Batch 625, Loss 0.3531833291053772\n",
      "[Training Epoch 0] Batch 626, Loss 0.37781667709350586\n",
      "[Training Epoch 0] Batch 627, Loss 0.3613961637020111\n",
      "[Training Epoch 0] Batch 628, Loss 0.3547547161579132\n",
      "[Training Epoch 0] Batch 629, Loss 0.3486481308937073\n",
      "[Training Epoch 0] Batch 630, Loss 0.36043936014175415\n",
      "[Training Epoch 0] Batch 631, Loss 0.3559713065624237\n",
      "[Training Epoch 0] Batch 632, Loss 0.3533012568950653\n",
      "[Training Epoch 0] Batch 633, Loss 0.37242862582206726\n",
      "[Training Epoch 0] Batch 634, Loss 0.3726778030395508\n",
      "[Training Epoch 0] Batch 635, Loss 0.3983984589576721\n",
      "[Training Epoch 0] Batch 636, Loss 0.3875850439071655\n",
      "[Training Epoch 0] Batch 637, Loss 0.3500608801841736\n",
      "[Training Epoch 0] Batch 638, Loss 0.3855665326118469\n",
      "[Training Epoch 0] Batch 639, Loss 0.3792204260826111\n",
      "[Training Epoch 0] Batch 640, Loss 0.3638631999492645\n",
      "[Training Epoch 0] Batch 641, Loss 0.3754643201828003\n",
      "[Training Epoch 0] Batch 642, Loss 0.363531231880188\n",
      "[Training Epoch 0] Batch 643, Loss 0.3614834249019623\n",
      "[Training Epoch 0] Batch 644, Loss 0.34786224365234375\n",
      "[Training Epoch 0] Batch 645, Loss 0.3506108224391937\n",
      "[Training Epoch 0] Batch 646, Loss 0.351162850856781\n",
      "[Training Epoch 0] Batch 647, Loss 0.3360159695148468\n",
      "[Training Epoch 0] Batch 648, Loss 0.3763319253921509\n",
      "[Training Epoch 0] Batch 649, Loss 0.40061211585998535\n",
      "[Training Epoch 0] Batch 650, Loss 0.3519069254398346\n",
      "[Training Epoch 0] Batch 651, Loss 0.3389737904071808\n",
      "[Training Epoch 0] Batch 652, Loss 0.3365127444267273\n",
      "[Training Epoch 0] Batch 653, Loss 0.3351864516735077\n",
      "[Training Epoch 0] Batch 654, Loss 0.33526235818862915\n",
      "[Training Epoch 0] Batch 655, Loss 0.3634396195411682\n",
      "[Training Epoch 0] Batch 656, Loss 0.36486440896987915\n",
      "[Training Epoch 0] Batch 657, Loss 0.37326759099960327\n",
      "[Training Epoch 0] Batch 658, Loss 0.34892112016677856\n",
      "[Training Epoch 0] Batch 659, Loss 0.3596963584423065\n",
      "[Training Epoch 0] Batch 660, Loss 0.37465178966522217\n",
      "[Training Epoch 0] Batch 661, Loss 0.35751208662986755\n",
      "[Training Epoch 0] Batch 662, Loss 0.38021320104599\n",
      "[Training Epoch 0] Batch 663, Loss 0.3454734683036804\n",
      "[Training Epoch 0] Batch 664, Loss 0.3716813921928406\n",
      "[Training Epoch 0] Batch 665, Loss 0.36421120166778564\n",
      "[Training Epoch 0] Batch 666, Loss 0.3333391547203064\n",
      "[Training Epoch 0] Batch 667, Loss 0.356683611869812\n",
      "[Training Epoch 0] Batch 668, Loss 0.3674851357936859\n",
      "[Training Epoch 0] Batch 669, Loss 0.3525870442390442\n",
      "[Training Epoch 0] Batch 670, Loss 0.33321768045425415\n",
      "[Training Epoch 0] Batch 671, Loss 0.37576958537101746\n",
      "[Training Epoch 0] Batch 672, Loss 0.34160447120666504\n",
      "[Training Epoch 0] Batch 673, Loss 0.3632086515426636\n",
      "[Training Epoch 0] Batch 674, Loss 0.3585912585258484\n",
      "[Training Epoch 0] Batch 675, Loss 0.35085445642471313\n",
      "[Training Epoch 0] Batch 676, Loss 0.37287425994873047\n",
      "[Training Epoch 0] Batch 677, Loss 0.36030125617980957\n",
      "[Training Epoch 0] Batch 678, Loss 0.3948293924331665\n",
      "[Training Epoch 0] Batch 679, Loss 0.3358544111251831\n",
      "[Training Epoch 0] Batch 680, Loss 0.3411959111690521\n",
      "[Training Epoch 0] Batch 681, Loss 0.36663973331451416\n",
      "[Training Epoch 0] Batch 682, Loss 0.37411946058273315\n",
      "[Training Epoch 0] Batch 683, Loss 0.3659709095954895\n",
      "[Training Epoch 0] Batch 684, Loss 0.3480949401855469\n",
      "[Training Epoch 0] Batch 685, Loss 0.3532875180244446\n",
      "[Training Epoch 0] Batch 686, Loss 0.3558083474636078\n",
      "[Training Epoch 0] Batch 687, Loss 0.34897416830062866\n",
      "[Training Epoch 0] Batch 688, Loss 0.3580930829048157\n",
      "[Training Epoch 0] Batch 689, Loss 0.334514856338501\n",
      "[Training Epoch 0] Batch 690, Loss 0.3551774322986603\n",
      "[Training Epoch 0] Batch 691, Loss 0.3415738046169281\n",
      "[Training Epoch 0] Batch 692, Loss 0.36387085914611816\n",
      "[Training Epoch 0] Batch 693, Loss 0.3640044927597046\n",
      "[Training Epoch 0] Batch 694, Loss 0.3404873013496399\n",
      "[Training Epoch 0] Batch 695, Loss 0.38901740312576294\n",
      "[Training Epoch 0] Batch 696, Loss 0.31836599111557007\n",
      "[Training Epoch 0] Batch 697, Loss 0.3492644131183624\n",
      "[Training Epoch 0] Batch 698, Loss 0.3528043031692505\n",
      "[Training Epoch 0] Batch 699, Loss 0.345365047454834\n",
      "[Training Epoch 0] Batch 700, Loss 0.33801162242889404\n",
      "[Training Epoch 0] Batch 701, Loss 0.35606813430786133\n",
      "[Training Epoch 0] Batch 702, Loss 0.3146762251853943\n",
      "[Training Epoch 0] Batch 703, Loss 0.37545526027679443\n",
      "[Training Epoch 0] Batch 704, Loss 0.3514532446861267\n",
      "[Training Epoch 0] Batch 705, Loss 0.3607010245323181\n",
      "[Training Epoch 0] Batch 706, Loss 0.3483680784702301\n",
      "[Training Epoch 0] Batch 707, Loss 0.36628207564353943\n",
      "[Training Epoch 0] Batch 708, Loss 0.37437212467193604\n",
      "[Training Epoch 0] Batch 709, Loss 0.3754810690879822\n",
      "[Training Epoch 0] Batch 710, Loss 0.3616282343864441\n",
      "[Training Epoch 0] Batch 711, Loss 0.37048375606536865\n",
      "[Training Epoch 0] Batch 712, Loss 0.3567178249359131\n",
      "[Training Epoch 0] Batch 713, Loss 0.33386334776878357\n",
      "[Training Epoch 0] Batch 714, Loss 0.3633873164653778\n",
      "[Training Epoch 0] Batch 715, Loss 0.37191900610923767\n",
      "[Training Epoch 0] Batch 716, Loss 0.3563520312309265\n",
      "[Training Epoch 0] Batch 717, Loss 0.35251468420028687\n",
      "[Training Epoch 0] Batch 718, Loss 0.3594212830066681\n",
      "[Training Epoch 0] Batch 719, Loss 0.36672019958496094\n",
      "[Training Epoch 0] Batch 720, Loss 0.36558830738067627\n",
      "[Training Epoch 0] Batch 721, Loss 0.34041768312454224\n",
      "[Training Epoch 0] Batch 722, Loss 0.361575186252594\n",
      "[Training Epoch 0] Batch 723, Loss 0.3515852689743042\n",
      "[Training Epoch 0] Batch 724, Loss 0.34522730112075806\n",
      "[Training Epoch 0] Batch 725, Loss 0.3186102509498596\n",
      "[Training Epoch 0] Batch 726, Loss 0.3561336398124695\n",
      "[Training Epoch 0] Batch 727, Loss 0.35848039388656616\n",
      "[Training Epoch 0] Batch 728, Loss 0.3237702250480652\n",
      "[Training Epoch 0] Batch 729, Loss 0.36723610758781433\n",
      "[Training Epoch 0] Batch 730, Loss 0.34524476528167725\n",
      "[Training Epoch 0] Batch 731, Loss 0.3250391483306885\n",
      "[Training Epoch 0] Batch 732, Loss 0.3919318616390228\n",
      "[Training Epoch 0] Batch 733, Loss 0.34983304142951965\n",
      "[Training Epoch 0] Batch 734, Loss 0.32785290479660034\n",
      "[Training Epoch 0] Batch 735, Loss 0.3969457149505615\n",
      "[Training Epoch 0] Batch 736, Loss 0.360259473323822\n",
      "[Training Epoch 0] Batch 737, Loss 0.35596007108688354\n",
      "[Training Epoch 0] Batch 738, Loss 0.3434549570083618\n",
      "[Training Epoch 0] Batch 739, Loss 0.37711215019226074\n",
      "[Training Epoch 0] Batch 740, Loss 0.365953266620636\n",
      "[Training Epoch 0] Batch 741, Loss 0.36659976840019226\n",
      "[Training Epoch 0] Batch 742, Loss 0.3413214087486267\n",
      "[Training Epoch 0] Batch 743, Loss 0.3683216869831085\n",
      "[Training Epoch 0] Batch 744, Loss 0.3469458222389221\n",
      "[Training Epoch 0] Batch 745, Loss 0.33858802914619446\n",
      "[Training Epoch 0] Batch 746, Loss 0.34555089473724365\n",
      "[Training Epoch 0] Batch 747, Loss 0.3607497215270996\n",
      "[Training Epoch 0] Batch 748, Loss 0.37205344438552856\n",
      "[Training Epoch 0] Batch 749, Loss 0.3674306869506836\n",
      "[Training Epoch 0] Batch 750, Loss 0.3424900472164154\n",
      "[Training Epoch 0] Batch 751, Loss 0.3402554988861084\n",
      "[Training Epoch 0] Batch 752, Loss 0.3575711250305176\n",
      "[Training Epoch 0] Batch 753, Loss 0.3625265955924988\n",
      "[Training Epoch 0] Batch 754, Loss 0.3659895062446594\n",
      "[Training Epoch 0] Batch 755, Loss 0.3547983169555664\n",
      "[Training Epoch 0] Batch 756, Loss 0.37573477625846863\n",
      "[Training Epoch 0] Batch 757, Loss 0.33398520946502686\n",
      "[Training Epoch 0] Batch 758, Loss 0.3534231185913086\n",
      "[Training Epoch 0] Batch 759, Loss 0.36316412687301636\n",
      "[Training Epoch 0] Batch 760, Loss 0.34215328097343445\n",
      "[Training Epoch 0] Batch 761, Loss 0.3634527325630188\n",
      "[Training Epoch 0] Batch 762, Loss 0.35901305079460144\n",
      "[Training Epoch 0] Batch 763, Loss 0.37063169479370117\n",
      "[Training Epoch 0] Batch 764, Loss 0.36883795261383057\n",
      "[Training Epoch 0] Batch 765, Loss 0.339579313993454\n",
      "[Training Epoch 0] Batch 766, Loss 0.3599011301994324\n",
      "[Training Epoch 0] Batch 767, Loss 0.38257843255996704\n",
      "[Training Epoch 0] Batch 768, Loss 0.35960397124290466\n",
      "[Training Epoch 0] Batch 769, Loss 0.34019115567207336\n",
      "[Training Epoch 0] Batch 770, Loss 0.389401912689209\n",
      "[Training Epoch 0] Batch 771, Loss 0.35349977016448975\n",
      "[Training Epoch 0] Batch 772, Loss 0.3763677775859833\n",
      "[Training Epoch 0] Batch 773, Loss 0.33937084674835205\n",
      "[Training Epoch 0] Batch 774, Loss 0.3536202311515808\n",
      "[Training Epoch 0] Batch 775, Loss 0.37552618980407715\n",
      "[Training Epoch 0] Batch 776, Loss 0.36130350828170776\n",
      "[Training Epoch 0] Batch 777, Loss 0.34777966141700745\n",
      "[Training Epoch 0] Batch 778, Loss 0.37224280834198\n",
      "[Training Epoch 0] Batch 779, Loss 0.37960535287857056\n",
      "[Training Epoch 0] Batch 780, Loss 0.3517245054244995\n",
      "[Training Epoch 0] Batch 781, Loss 0.34543707966804504\n",
      "[Training Epoch 0] Batch 782, Loss 0.3629271984100342\n",
      "[Training Epoch 0] Batch 783, Loss 0.3281596004962921\n",
      "[Training Epoch 0] Batch 784, Loss 0.3481829762458801\n",
      "[Training Epoch 0] Batch 785, Loss 0.3277793824672699\n",
      "[Training Epoch 0] Batch 786, Loss 0.3546793758869171\n",
      "[Training Epoch 0] Batch 787, Loss 0.33358874917030334\n",
      "[Training Epoch 0] Batch 788, Loss 0.3547082841396332\n",
      "[Training Epoch 0] Batch 789, Loss 0.3835274875164032\n",
      "[Training Epoch 0] Batch 790, Loss 0.34326791763305664\n",
      "[Training Epoch 0] Batch 791, Loss 0.32718777656555176\n",
      "[Training Epoch 0] Batch 792, Loss 0.3799939751625061\n",
      "[Training Epoch 0] Batch 793, Loss 0.35860270261764526\n",
      "[Training Epoch 0] Batch 794, Loss 0.3341798782348633\n",
      "[Training Epoch 0] Batch 795, Loss 0.37114620208740234\n",
      "[Training Epoch 0] Batch 796, Loss 0.36568817496299744\n",
      "[Training Epoch 0] Batch 797, Loss 0.3587023615837097\n",
      "[Training Epoch 0] Batch 798, Loss 0.35765010118484497\n",
      "[Training Epoch 0] Batch 799, Loss 0.33012646436691284\n",
      "[Training Epoch 0] Batch 800, Loss 0.3683159351348877\n",
      "[Training Epoch 0] Batch 801, Loss 0.3897491693496704\n",
      "[Training Epoch 0] Batch 802, Loss 0.35795488953590393\n",
      "[Training Epoch 0] Batch 803, Loss 0.38250356912612915\n",
      "[Training Epoch 0] Batch 804, Loss 0.3553463816642761\n",
      "[Training Epoch 0] Batch 805, Loss 0.35467880964279175\n",
      "[Training Epoch 0] Batch 806, Loss 0.33948302268981934\n",
      "[Training Epoch 0] Batch 807, Loss 0.34065908193588257\n",
      "[Training Epoch 0] Batch 808, Loss 0.35718005895614624\n",
      "[Training Epoch 0] Batch 809, Loss 0.3517988324165344\n",
      "[Training Epoch 0] Batch 810, Loss 0.35562893748283386\n",
      "[Training Epoch 0] Batch 811, Loss 0.36845162510871887\n",
      "[Training Epoch 0] Batch 812, Loss 0.34175240993499756\n",
      "[Training Epoch 0] Batch 813, Loss 0.3769662380218506\n",
      "[Training Epoch 0] Batch 814, Loss 0.37348783016204834\n",
      "[Training Epoch 0] Batch 815, Loss 0.3488537669181824\n",
      "[Training Epoch 0] Batch 816, Loss 0.359925776720047\n",
      "[Training Epoch 0] Batch 817, Loss 0.36542049050331116\n",
      "[Training Epoch 0] Batch 818, Loss 0.3829253911972046\n",
      "[Training Epoch 0] Batch 819, Loss 0.36011025309562683\n",
      "[Training Epoch 0] Batch 820, Loss 0.3455801010131836\n",
      "[Training Epoch 0] Batch 821, Loss 0.34797203540802\n",
      "[Training Epoch 0] Batch 822, Loss 0.3648890256881714\n",
      "[Training Epoch 0] Batch 823, Loss 0.35035842657089233\n",
      "[Training Epoch 0] Batch 824, Loss 0.34885138273239136\n",
      "[Training Epoch 0] Batch 825, Loss 0.35248857736587524\n",
      "[Training Epoch 0] Batch 826, Loss 0.35607120394706726\n",
      "[Training Epoch 0] Batch 827, Loss 0.3421621322631836\n",
      "[Training Epoch 0] Batch 828, Loss 0.35041189193725586\n",
      "[Training Epoch 0] Batch 829, Loss 0.3409392237663269\n",
      "[Training Epoch 0] Batch 830, Loss 0.33906522393226624\n",
      "[Training Epoch 0] Batch 831, Loss 0.3487301468849182\n",
      "[Training Epoch 0] Batch 832, Loss 0.36073797941207886\n",
      "[Training Epoch 0] Batch 833, Loss 0.37097784876823425\n",
      "[Training Epoch 0] Batch 834, Loss 0.3763604760169983\n",
      "[Training Epoch 0] Batch 835, Loss 0.35123658180236816\n",
      "[Training Epoch 0] Batch 836, Loss 0.363584041595459\n",
      "[Training Epoch 0] Batch 837, Loss 0.3635598421096802\n",
      "[Training Epoch 0] Batch 838, Loss 0.3224334716796875\n",
      "[Training Epoch 0] Batch 839, Loss 0.3329351544380188\n",
      "[Training Epoch 0] Batch 840, Loss 0.37798431515693665\n",
      "[Training Epoch 0] Batch 841, Loss 0.3489817976951599\n",
      "[Training Epoch 0] Batch 842, Loss 0.38863760232925415\n",
      "[Training Epoch 0] Batch 843, Loss 0.3525099754333496\n",
      "[Training Epoch 0] Batch 844, Loss 0.3414601683616638\n",
      "[Training Epoch 0] Batch 845, Loss 0.3619346022605896\n",
      "[Training Epoch 0] Batch 846, Loss 0.3713777959346771\n",
      "[Training Epoch 0] Batch 847, Loss 0.3332189917564392\n",
      "[Training Epoch 0] Batch 848, Loss 0.3465907573699951\n",
      "[Training Epoch 0] Batch 849, Loss 0.34161341190338135\n",
      "[Training Epoch 0] Batch 850, Loss 0.33788442611694336\n",
      "[Training Epoch 0] Batch 851, Loss 0.32183802127838135\n",
      "[Training Epoch 0] Batch 852, Loss 0.36645007133483887\n",
      "[Training Epoch 0] Batch 853, Loss 0.35262879729270935\n",
      "[Training Epoch 0] Batch 854, Loss 0.37386953830718994\n",
      "[Training Epoch 0] Batch 855, Loss 0.3476254940032959\n",
      "[Training Epoch 0] Batch 856, Loss 0.3539237976074219\n",
      "[Training Epoch 0] Batch 857, Loss 0.38528525829315186\n",
      "[Training Epoch 0] Batch 858, Loss 0.3670392632484436\n",
      "[Training Epoch 0] Batch 859, Loss 0.3300953805446625\n",
      "[Training Epoch 0] Batch 860, Loss 0.358140230178833\n",
      "[Training Epoch 0] Batch 861, Loss 0.3458544909954071\n",
      "[Training Epoch 0] Batch 862, Loss 0.37569576501846313\n",
      "[Training Epoch 0] Batch 863, Loss 0.3670070767402649\n",
      "[Training Epoch 0] Batch 864, Loss 0.36638763546943665\n",
      "[Training Epoch 0] Batch 865, Loss 0.36686813831329346\n",
      "[Training Epoch 0] Batch 866, Loss 0.3802138566970825\n",
      "[Training Epoch 0] Batch 867, Loss 0.34124597907066345\n",
      "[Training Epoch 0] Batch 868, Loss 0.3445936143398285\n",
      "[Training Epoch 0] Batch 869, Loss 0.3615233600139618\n",
      "[Training Epoch 0] Batch 870, Loss 0.3531367778778076\n",
      "[Training Epoch 0] Batch 871, Loss 0.31651878356933594\n",
      "[Training Epoch 0] Batch 872, Loss 0.3742597699165344\n",
      "[Training Epoch 0] Batch 873, Loss 0.36968421936035156\n",
      "[Training Epoch 0] Batch 874, Loss 0.3389202952384949\n",
      "[Training Epoch 0] Batch 875, Loss 0.344060480594635\n",
      "[Training Epoch 0] Batch 876, Loss 0.37000772356987\n",
      "[Training Epoch 0] Batch 877, Loss 0.3847387433052063\n",
      "[Training Epoch 0] Batch 878, Loss 0.349953830242157\n",
      "[Training Epoch 0] Batch 879, Loss 0.37532705068588257\n",
      "[Training Epoch 0] Batch 880, Loss 0.3349311351776123\n",
      "[Training Epoch 0] Batch 881, Loss 0.3411821722984314\n",
      "[Training Epoch 0] Batch 882, Loss 0.3573092222213745\n",
      "[Training Epoch 0] Batch 883, Loss 0.3386930525302887\n",
      "[Training Epoch 0] Batch 884, Loss 0.3443547785282135\n",
      "[Training Epoch 0] Batch 885, Loss 0.3331835865974426\n",
      "[Training Epoch 0] Batch 886, Loss 0.360095351934433\n",
      "[Training Epoch 0] Batch 887, Loss 0.36799171566963196\n",
      "[Training Epoch 0] Batch 888, Loss 0.39239302277565\n",
      "[Training Epoch 0] Batch 889, Loss 0.37509796023368835\n",
      "[Training Epoch 0] Batch 890, Loss 0.3637588322162628\n",
      "[Training Epoch 0] Batch 891, Loss 0.3567308187484741\n",
      "[Training Epoch 0] Batch 892, Loss 0.34955576062202454\n",
      "[Training Epoch 0] Batch 893, Loss 0.3698961138725281\n",
      "[Training Epoch 0] Batch 894, Loss 0.3545339107513428\n",
      "[Training Epoch 0] Batch 895, Loss 0.36123988032341003\n",
      "[Training Epoch 0] Batch 896, Loss 0.3449792265892029\n",
      "[Training Epoch 0] Batch 897, Loss 0.3561232089996338\n",
      "[Training Epoch 0] Batch 898, Loss 0.3537418842315674\n",
      "[Training Epoch 0] Batch 899, Loss 0.3467056155204773\n",
      "[Training Epoch 0] Batch 900, Loss 0.3515387773513794\n",
      "[Training Epoch 0] Batch 901, Loss 0.35964739322662354\n",
      "[Training Epoch 0] Batch 902, Loss 0.3515033721923828\n",
      "[Training Epoch 0] Batch 903, Loss 0.34252211451530457\n",
      "[Training Epoch 0] Batch 904, Loss 0.36098456382751465\n",
      "[Training Epoch 0] Batch 905, Loss 0.33643630146980286\n",
      "[Training Epoch 0] Batch 906, Loss 0.3547927737236023\n",
      "[Training Epoch 0] Batch 907, Loss 0.3361195921897888\n",
      "[Training Epoch 0] Batch 908, Loss 0.38313525915145874\n",
      "[Training Epoch 0] Batch 909, Loss 0.36958155035972595\n",
      "[Training Epoch 0] Batch 910, Loss 0.34732455015182495\n",
      "[Training Epoch 0] Batch 911, Loss 0.37220585346221924\n",
      "[Training Epoch 0] Batch 912, Loss 0.3459349274635315\n",
      "[Training Epoch 0] Batch 913, Loss 0.3470892012119293\n",
      "[Training Epoch 0] Batch 914, Loss 0.3512839674949646\n",
      "[Training Epoch 0] Batch 915, Loss 0.33673161268234253\n",
      "[Training Epoch 0] Batch 916, Loss 0.341811865568161\n",
      "[Training Epoch 0] Batch 917, Loss 0.3367469310760498\n",
      "[Training Epoch 0] Batch 918, Loss 0.3502897620201111\n",
      "[Training Epoch 0] Batch 919, Loss 0.33296874165534973\n",
      "[Training Epoch 0] Batch 920, Loss 0.34460657835006714\n",
      "[Training Epoch 0] Batch 921, Loss 0.353467732667923\n",
      "[Training Epoch 0] Batch 922, Loss 0.3530590534210205\n",
      "[Training Epoch 0] Batch 923, Loss 0.33113622665405273\n",
      "[Training Epoch 0] Batch 924, Loss 0.3780919313430786\n",
      "[Training Epoch 0] Batch 925, Loss 0.36127251386642456\n",
      "[Training Epoch 0] Batch 926, Loss 0.3444593548774719\n",
      "[Training Epoch 0] Batch 927, Loss 0.34604936838150024\n",
      "[Training Epoch 0] Batch 928, Loss 0.3224734663963318\n",
      "[Training Epoch 0] Batch 929, Loss 0.3404211401939392\n",
      "[Training Epoch 0] Batch 930, Loss 0.33771735429763794\n",
      "[Training Epoch 0] Batch 931, Loss 0.342110812664032\n",
      "[Training Epoch 0] Batch 932, Loss 0.3427204489707947\n",
      "[Training Epoch 0] Batch 933, Loss 0.38518577814102173\n",
      "[Training Epoch 0] Batch 934, Loss 0.3780398368835449\n",
      "[Training Epoch 0] Batch 935, Loss 0.3821486234664917\n",
      "[Training Epoch 0] Batch 936, Loss 0.3284393548965454\n",
      "[Training Epoch 0] Batch 937, Loss 0.3284270167350769\n",
      "[Training Epoch 0] Batch 938, Loss 0.3408513069152832\n",
      "[Training Epoch 0] Batch 939, Loss 0.371269166469574\n",
      "[Training Epoch 0] Batch 940, Loss 0.3721098303794861\n",
      "[Training Epoch 0] Batch 941, Loss 0.3650376796722412\n",
      "[Training Epoch 0] Batch 942, Loss 0.34788748621940613\n",
      "[Training Epoch 0] Batch 943, Loss 0.324654221534729\n",
      "[Training Epoch 0] Batch 944, Loss 0.34982866048812866\n",
      "[Training Epoch 0] Batch 945, Loss 0.3362674117088318\n",
      "[Training Epoch 0] Batch 946, Loss 0.36784306168556213\n",
      "[Training Epoch 0] Batch 947, Loss 0.35667258501052856\n",
      "[Training Epoch 0] Batch 948, Loss 0.3437201976776123\n",
      "[Training Epoch 0] Batch 949, Loss 0.3673003911972046\n",
      "[Training Epoch 0] Batch 950, Loss 0.3362431526184082\n",
      "[Training Epoch 0] Batch 951, Loss 0.33754846453666687\n",
      "[Training Epoch 0] Batch 952, Loss 0.3507707417011261\n",
      "[Training Epoch 0] Batch 953, Loss 0.34552016854286194\n",
      "[Training Epoch 0] Batch 954, Loss 0.3480190932750702\n",
      "[Training Epoch 0] Batch 955, Loss 0.35849010944366455\n",
      "[Training Epoch 0] Batch 956, Loss 0.3681488633155823\n",
      "[Training Epoch 0] Batch 957, Loss 0.36917510628700256\n",
      "[Training Epoch 0] Batch 958, Loss 0.3630104660987854\n",
      "[Training Epoch 0] Batch 959, Loss 0.33979105949401855\n",
      "[Training Epoch 0] Batch 960, Loss 0.33882755041122437\n",
      "[Training Epoch 0] Batch 961, Loss 0.3773688077926636\n",
      "[Training Epoch 0] Batch 962, Loss 0.3607903718948364\n",
      "[Training Epoch 0] Batch 963, Loss 0.344826340675354\n",
      "[Training Epoch 0] Batch 964, Loss 0.36210498213768005\n",
      "[Training Epoch 0] Batch 965, Loss 0.3205183744430542\n",
      "[Training Epoch 0] Batch 966, Loss 0.3571483790874481\n",
      "[Training Epoch 0] Batch 967, Loss 0.3269802927970886\n",
      "[Training Epoch 0] Batch 968, Loss 0.3771553337574005\n",
      "[Training Epoch 0] Batch 969, Loss 0.3422716557979584\n",
      "[Training Epoch 0] Batch 970, Loss 0.3457658588886261\n",
      "[Training Epoch 0] Batch 971, Loss 0.3592790365219116\n",
      "[Training Epoch 0] Batch 972, Loss 0.3488394021987915\n",
      "[Training Epoch 0] Batch 973, Loss 0.35402461886405945\n",
      "[Training Epoch 0] Batch 974, Loss 0.341292142868042\n",
      "[Training Epoch 0] Batch 975, Loss 0.36660629510879517\n",
      "[Training Epoch 0] Batch 976, Loss 0.3566915690898895\n",
      "[Training Epoch 0] Batch 977, Loss 0.3435279130935669\n",
      "[Training Epoch 0] Batch 978, Loss 0.357773095369339\n",
      "[Training Epoch 0] Batch 979, Loss 0.36401182413101196\n",
      "[Training Epoch 0] Batch 980, Loss 0.35018154978752136\n",
      "[Training Epoch 0] Batch 981, Loss 0.3583572208881378\n",
      "[Training Epoch 0] Batch 982, Loss 0.3498111367225647\n",
      "[Training Epoch 0] Batch 983, Loss 0.3407036364078522\n",
      "[Training Epoch 0] Batch 984, Loss 0.3655267357826233\n",
      "[Training Epoch 0] Batch 985, Loss 0.3647593557834625\n",
      "[Training Epoch 0] Batch 986, Loss 0.33012598752975464\n",
      "[Training Epoch 0] Batch 987, Loss 0.3710925281047821\n",
      "[Training Epoch 0] Batch 988, Loss 0.36291974782943726\n",
      "[Training Epoch 0] Batch 989, Loss 0.3234065771102905\n",
      "[Training Epoch 0] Batch 990, Loss 0.3534931540489197\n",
      "[Training Epoch 0] Batch 991, Loss 0.3535385727882385\n",
      "[Training Epoch 0] Batch 992, Loss 0.3682093024253845\n",
      "[Training Epoch 0] Batch 993, Loss 0.36913758516311646\n",
      "[Training Epoch 0] Batch 994, Loss 0.3579126000404358\n",
      "[Training Epoch 0] Batch 995, Loss 0.34541255235671997\n",
      "[Training Epoch 0] Batch 996, Loss 0.3431764543056488\n",
      "[Training Epoch 0] Batch 997, Loss 0.36415278911590576\n",
      "[Training Epoch 0] Batch 998, Loss 0.31526219844818115\n",
      "[Training Epoch 0] Batch 999, Loss 0.32355284690856934\n",
      "[Training Epoch 0] Batch 1000, Loss 0.3249054253101349\n",
      "[Training Epoch 0] Batch 1001, Loss 0.34154266119003296\n",
      "[Training Epoch 0] Batch 1002, Loss 0.37076646089553833\n",
      "[Training Epoch 0] Batch 1003, Loss 0.3365451395511627\n",
      "[Training Epoch 0] Batch 1004, Loss 0.3503028452396393\n",
      "[Training Epoch 0] Batch 1005, Loss 0.3379252552986145\n",
      "[Training Epoch 0] Batch 1006, Loss 0.37866178154945374\n",
      "[Training Epoch 0] Batch 1007, Loss 0.3696126937866211\n",
      "[Training Epoch 0] Batch 1008, Loss 0.33631592988967896\n",
      "[Training Epoch 0] Batch 1009, Loss 0.35441944003105164\n",
      "[Training Epoch 0] Batch 1010, Loss 0.35530364513397217\n",
      "[Training Epoch 0] Batch 1011, Loss 0.3690968155860901\n",
      "[Training Epoch 0] Batch 1012, Loss 0.3585996627807617\n",
      "[Training Epoch 0] Batch 1013, Loss 0.34260040521621704\n",
      "[Training Epoch 0] Batch 1014, Loss 0.3492967188358307\n",
      "[Training Epoch 0] Batch 1015, Loss 0.34437015652656555\n",
      "[Training Epoch 0] Batch 1016, Loss 0.3422631025314331\n",
      "[Training Epoch 0] Batch 1017, Loss 0.3605201840400696\n",
      "[Training Epoch 0] Batch 1018, Loss 0.35656338930130005\n",
      "[Training Epoch 0] Batch 1019, Loss 0.3648987412452698\n",
      "[Training Epoch 0] Batch 1020, Loss 0.37997788190841675\n",
      "[Training Epoch 0] Batch 1021, Loss 0.3596551716327667\n",
      "[Training Epoch 0] Batch 1022, Loss 0.3419453203678131\n",
      "[Training Epoch 0] Batch 1023, Loss 0.38534826040267944\n",
      "[Training Epoch 0] Batch 1024, Loss 0.36340564489364624\n",
      "[Training Epoch 0] Batch 1025, Loss 0.34694135189056396\n",
      "[Training Epoch 0] Batch 1026, Loss 0.369954913854599\n",
      "[Training Epoch 0] Batch 1027, Loss 0.33035147190093994\n",
      "[Training Epoch 0] Batch 1028, Loss 0.36543500423431396\n",
      "[Training Epoch 0] Batch 1029, Loss 0.3503868579864502\n",
      "[Training Epoch 0] Batch 1030, Loss 0.33358222246170044\n",
      "[Training Epoch 0] Batch 1031, Loss 0.36517763137817383\n",
      "[Training Epoch 0] Batch 1032, Loss 0.34319716691970825\n",
      "[Training Epoch 0] Batch 1033, Loss 0.3424302935600281\n",
      "[Training Epoch 0] Batch 1034, Loss 0.3555165231227875\n",
      "[Training Epoch 0] Batch 1035, Loss 0.3660973012447357\n",
      "[Training Epoch 0] Batch 1036, Loss 0.33249786496162415\n",
      "[Training Epoch 0] Batch 1037, Loss 0.3582500219345093\n",
      "[Training Epoch 0] Batch 1038, Loss 0.34143972396850586\n",
      "[Training Epoch 0] Batch 1039, Loss 0.3401569724082947\n",
      "[Training Epoch 0] Batch 1040, Loss 0.36857491731643677\n",
      "[Training Epoch 0] Batch 1041, Loss 0.33790457248687744\n",
      "[Training Epoch 0] Batch 1042, Loss 0.32786357402801514\n",
      "[Training Epoch 0] Batch 1043, Loss 0.36458122730255127\n",
      "[Training Epoch 0] Batch 1044, Loss 0.3429010808467865\n",
      "[Training Epoch 0] Batch 1045, Loss 0.34696289896965027\n",
      "[Training Epoch 0] Batch 1046, Loss 0.3394477367401123\n",
      "[Training Epoch 0] Batch 1047, Loss 0.39046597480773926\n",
      "[Training Epoch 0] Batch 1048, Loss 0.34851953387260437\n",
      "[Training Epoch 0] Batch 1049, Loss 0.32962554693222046\n",
      "[Training Epoch 0] Batch 1050, Loss 0.34751108288764954\n",
      "[Training Epoch 0] Batch 1051, Loss 0.35564327239990234\n",
      "[Training Epoch 0] Batch 1052, Loss 0.32987844944000244\n",
      "[Training Epoch 0] Batch 1053, Loss 0.36427754163742065\n",
      "[Training Epoch 0] Batch 1054, Loss 0.3407650589942932\n",
      "[Training Epoch 0] Batch 1055, Loss 0.370903879404068\n",
      "[Training Epoch 0] Batch 1056, Loss 0.3533034324645996\n",
      "[Training Epoch 0] Batch 1057, Loss 0.33019372820854187\n",
      "[Training Epoch 0] Batch 1058, Loss 0.3262123465538025\n",
      "[Training Epoch 0] Batch 1059, Loss 0.351881206035614\n",
      "[Training Epoch 0] Batch 1060, Loss 0.37083032727241516\n",
      "[Training Epoch 0] Batch 1061, Loss 0.3407817780971527\n",
      "[Training Epoch 0] Batch 1062, Loss 0.3669253885746002\n",
      "[Training Epoch 0] Batch 1063, Loss 0.3527013957500458\n",
      "[Training Epoch 0] Batch 1064, Loss 0.3342971205711365\n",
      "[Training Epoch 0] Batch 1065, Loss 0.34453752636909485\n",
      "[Training Epoch 0] Batch 1066, Loss 0.3661555051803589\n",
      "[Training Epoch 0] Batch 1067, Loss 0.3493140935897827\n",
      "[Training Epoch 0] Batch 1068, Loss 0.34262266755104065\n",
      "[Training Epoch 0] Batch 1069, Loss 0.33871832489967346\n",
      "[Training Epoch 0] Batch 1070, Loss 0.34196287393569946\n",
      "[Training Epoch 0] Batch 1071, Loss 0.32585906982421875\n",
      "[Training Epoch 0] Batch 1072, Loss 0.33752721548080444\n",
      "[Training Epoch 0] Batch 1073, Loss 0.35149431228637695\n",
      "[Training Epoch 0] Batch 1074, Loss 0.36077237129211426\n",
      "[Training Epoch 0] Batch 1075, Loss 0.32155629992485046\n",
      "[Training Epoch 0] Batch 1076, Loss 0.3677893280982971\n",
      "[Training Epoch 0] Batch 1077, Loss 0.3499268889427185\n",
      "[Training Epoch 0] Batch 1078, Loss 0.3540695905685425\n",
      "[Training Epoch 0] Batch 1079, Loss 0.3526824414730072\n",
      "[Training Epoch 0] Batch 1080, Loss 0.34779372811317444\n",
      "[Training Epoch 0] Batch 1081, Loss 0.36911559104919434\n",
      "[Training Epoch 0] Batch 1082, Loss 0.3452293872833252\n",
      "[Training Epoch 0] Batch 1083, Loss 0.32964587211608887\n",
      "[Training Epoch 0] Batch 1084, Loss 0.34099411964416504\n",
      "[Training Epoch 0] Batch 1085, Loss 0.33985647559165955\n",
      "[Training Epoch 0] Batch 1086, Loss 0.3535707890987396\n",
      "[Training Epoch 0] Batch 1087, Loss 0.3397162854671478\n",
      "[Training Epoch 0] Batch 1088, Loss 0.3602348566055298\n",
      "[Training Epoch 0] Batch 1089, Loss 0.35546648502349854\n",
      "[Training Epoch 0] Batch 1090, Loss 0.35568803548812866\n",
      "[Training Epoch 0] Batch 1091, Loss 0.36326542496681213\n",
      "[Training Epoch 0] Batch 1092, Loss 0.34017956256866455\n",
      "[Training Epoch 0] Batch 1093, Loss 0.3291488289833069\n",
      "[Training Epoch 0] Batch 1094, Loss 0.34302109479904175\n",
      "[Training Epoch 0] Batch 1095, Loss 0.3297908306121826\n",
      "[Training Epoch 0] Batch 1096, Loss 0.3673032224178314\n",
      "[Training Epoch 0] Batch 1097, Loss 0.3422214984893799\n",
      "[Training Epoch 0] Batch 1098, Loss 0.32238203287124634\n",
      "[Training Epoch 0] Batch 1099, Loss 0.33153945207595825\n",
      "[Training Epoch 0] Batch 1100, Loss 0.3337402939796448\n",
      "[Training Epoch 0] Batch 1101, Loss 0.3278828561306\n",
      "[Training Epoch 0] Batch 1102, Loss 0.36487823724746704\n",
      "[Training Epoch 0] Batch 1103, Loss 0.3454464375972748\n",
      "[Training Epoch 0] Batch 1104, Loss 0.3534931540489197\n",
      "[Training Epoch 0] Batch 1105, Loss 0.3714826703071594\n",
      "[Training Epoch 0] Batch 1106, Loss 0.34624767303466797\n",
      "[Training Epoch 0] Batch 1107, Loss 0.3545076549053192\n",
      "[Training Epoch 0] Batch 1108, Loss 0.3347633481025696\n",
      "[Training Epoch 0] Batch 1109, Loss 0.3322599530220032\n",
      "[Training Epoch 0] Batch 1110, Loss 0.3408876657485962\n",
      "[Training Epoch 0] Batch 1111, Loss 0.335125207901001\n",
      "[Training Epoch 0] Batch 1112, Loss 0.3571809232234955\n",
      "[Training Epoch 0] Batch 1113, Loss 0.3371577858924866\n",
      "[Training Epoch 0] Batch 1114, Loss 0.37018635869026184\n",
      "[Training Epoch 0] Batch 1115, Loss 0.33967310190200806\n",
      "[Training Epoch 0] Batch 1116, Loss 0.34716200828552246\n",
      "[Training Epoch 0] Batch 1117, Loss 0.38538193702697754\n",
      "[Training Epoch 0] Batch 1118, Loss 0.3599032461643219\n",
      "[Training Epoch 0] Batch 1119, Loss 0.36288851499557495\n",
      "[Training Epoch 0] Batch 1120, Loss 0.32887017726898193\n",
      "[Training Epoch 0] Batch 1121, Loss 0.33890753984451294\n",
      "[Training Epoch 0] Batch 1122, Loss 0.3780435621738434\n",
      "[Training Epoch 0] Batch 1123, Loss 0.3760440945625305\n",
      "[Training Epoch 0] Batch 1124, Loss 0.3615630269050598\n",
      "[Training Epoch 0] Batch 1125, Loss 0.34210970997810364\n",
      "[Training Epoch 0] Batch 1126, Loss 0.35783594846725464\n",
      "[Training Epoch 0] Batch 1127, Loss 0.3371727168560028\n",
      "[Training Epoch 0] Batch 1128, Loss 0.34530818462371826\n",
      "[Training Epoch 0] Batch 1129, Loss 0.3661523461341858\n",
      "[Training Epoch 0] Batch 1130, Loss 0.31894630193710327\n",
      "[Training Epoch 0] Batch 1131, Loss 0.3517206311225891\n",
      "[Training Epoch 0] Batch 1132, Loss 0.36673611402511597\n",
      "[Training Epoch 0] Batch 1133, Loss 0.3240860104560852\n",
      "[Training Epoch 0] Batch 1134, Loss 0.3606647849082947\n",
      "[Training Epoch 0] Batch 1135, Loss 0.3523646593093872\n",
      "[Training Epoch 0] Batch 1136, Loss 0.35198357701301575\n",
      "[Training Epoch 0] Batch 1137, Loss 0.34460851550102234\n",
      "[Training Epoch 0] Batch 1138, Loss 0.3324512541294098\n",
      "[Training Epoch 0] Batch 1139, Loss 0.3092501163482666\n",
      "[Training Epoch 0] Batch 1140, Loss 0.3459518551826477\n",
      "[Training Epoch 0] Batch 1141, Loss 0.33563584089279175\n",
      "[Training Epoch 0] Batch 1142, Loss 0.3382800221443176\n",
      "[Training Epoch 0] Batch 1143, Loss 0.33649033308029175\n",
      "[Training Epoch 0] Batch 1144, Loss 0.3832134008407593\n",
      "[Training Epoch 0] Batch 1145, Loss 0.3551834225654602\n",
      "[Training Epoch 0] Batch 1146, Loss 0.35615259408950806\n",
      "[Training Epoch 0] Batch 1147, Loss 0.3655306398868561\n",
      "[Training Epoch 0] Batch 1148, Loss 0.3380255401134491\n",
      "[Training Epoch 0] Batch 1149, Loss 0.35390758514404297\n",
      "[Training Epoch 0] Batch 1150, Loss 0.36168360710144043\n",
      "[Training Epoch 0] Batch 1151, Loss 0.3409276008605957\n",
      "[Training Epoch 0] Batch 1152, Loss 0.3351340591907501\n",
      "[Training Epoch 0] Batch 1153, Loss 0.3590620458126068\n",
      "[Training Epoch 0] Batch 1154, Loss 0.31913700699806213\n",
      "[Training Epoch 0] Batch 1155, Loss 0.34292924404144287\n",
      "[Training Epoch 0] Batch 1156, Loss 0.38097450137138367\n",
      "[Training Epoch 0] Batch 1157, Loss 0.35440200567245483\n",
      "[Training Epoch 0] Batch 1158, Loss 0.32727301120758057\n",
      "[Training Epoch 0] Batch 1159, Loss 0.35274186730384827\n",
      "[Training Epoch 0] Batch 1160, Loss 0.34225109219551086\n",
      "[Training Epoch 0] Batch 1161, Loss 0.3512757420539856\n",
      "[Training Epoch 0] Batch 1162, Loss 0.37080466747283936\n",
      "[Training Epoch 0] Batch 1163, Loss 0.330819308757782\n",
      "[Training Epoch 0] Batch 1164, Loss 0.3468681871891022\n",
      "[Training Epoch 0] Batch 1165, Loss 0.34405624866485596\n",
      "[Training Epoch 0] Batch 1166, Loss 0.352169394493103\n",
      "[Training Epoch 0] Batch 1167, Loss 0.3620791435241699\n",
      "[Training Epoch 0] Batch 1168, Loss 0.33928990364074707\n",
      "[Training Epoch 0] Batch 1169, Loss 0.3363785743713379\n",
      "[Training Epoch 0] Batch 1170, Loss 0.3547571897506714\n",
      "[Training Epoch 0] Batch 1171, Loss 0.3352122902870178\n",
      "[Training Epoch 0] Batch 1172, Loss 0.35825884342193604\n",
      "[Training Epoch 0] Batch 1173, Loss 0.3454107642173767\n",
      "[Training Epoch 0] Batch 1174, Loss 0.34005969762802124\n",
      "[Training Epoch 0] Batch 1175, Loss 0.3756176233291626\n",
      "[Training Epoch 0] Batch 1176, Loss 0.35437703132629395\n",
      "[Training Epoch 0] Batch 1177, Loss 0.3331923186779022\n",
      "[Training Epoch 0] Batch 1178, Loss 0.34901899099349976\n",
      "[Training Epoch 0] Batch 1179, Loss 0.3672545254230499\n",
      "[Training Epoch 0] Batch 1180, Loss 0.33759915828704834\n",
      "[Training Epoch 0] Batch 1181, Loss 0.3321031630039215\n",
      "[Training Epoch 0] Batch 1182, Loss 0.35717296600341797\n",
      "[Training Epoch 0] Batch 1183, Loss 0.35667678713798523\n",
      "[Training Epoch 0] Batch 1184, Loss 0.3554965555667877\n",
      "[Training Epoch 0] Batch 1185, Loss 0.34307941794395447\n",
      "[Training Epoch 0] Batch 1186, Loss 0.3595992922782898\n",
      "[Training Epoch 0] Batch 1187, Loss 0.33672815561294556\n",
      "[Training Epoch 0] Batch 1188, Loss 0.34549880027770996\n",
      "[Training Epoch 0] Batch 1189, Loss 0.3470022976398468\n",
      "[Training Epoch 0] Batch 1190, Loss 0.32971975207328796\n",
      "[Training Epoch 0] Batch 1191, Loss 0.31454703211784363\n",
      "[Training Epoch 0] Batch 1192, Loss 0.3146814703941345\n",
      "[Training Epoch 0] Batch 1193, Loss 0.3482002317905426\n",
      "[Training Epoch 0] Batch 1194, Loss 0.35551196336746216\n",
      "[Training Epoch 0] Batch 1195, Loss 0.3274286091327667\n",
      "[Training Epoch 0] Batch 1196, Loss 0.34556102752685547\n",
      "[Training Epoch 0] Batch 1197, Loss 0.367363840341568\n",
      "[Training Epoch 0] Batch 1198, Loss 0.3137610852718353\n",
      "[Training Epoch 0] Batch 1199, Loss 0.35144948959350586\n",
      "[Training Epoch 0] Batch 1200, Loss 0.35821399092674255\n",
      "[Training Epoch 0] Batch 1201, Loss 0.33653104305267334\n",
      "[Training Epoch 0] Batch 1202, Loss 0.3148139715194702\n",
      "[Training Epoch 0] Batch 1203, Loss 0.3712139129638672\n",
      "[Training Epoch 0] Batch 1204, Loss 0.3368006944656372\n",
      "[Training Epoch 0] Batch 1205, Loss 0.35526835918426514\n",
      "[Training Epoch 0] Batch 1206, Loss 0.32778623700141907\n",
      "[Training Epoch 0] Batch 1207, Loss 0.34866106510162354\n",
      "[Training Epoch 0] Batch 1208, Loss 0.3336503803730011\n",
      "[Training Epoch 0] Batch 1209, Loss 0.34270310401916504\n",
      "[Training Epoch 0] Batch 1210, Loss 0.3443657159805298\n",
      "[Training Epoch 0] Batch 1211, Loss 0.35900717973709106\n",
      "[Training Epoch 0] Batch 1212, Loss 0.3774501085281372\n",
      "[Training Epoch 0] Batch 1213, Loss 0.3191464841365814\n",
      "[Training Epoch 0] Batch 1214, Loss 0.3591098189353943\n",
      "[Training Epoch 0] Batch 1215, Loss 0.34904587268829346\n",
      "[Training Epoch 0] Batch 1216, Loss 0.36861076951026917\n",
      "[Training Epoch 0] Batch 1217, Loss 0.3279275894165039\n",
      "[Training Epoch 0] Batch 1218, Loss 0.34293121099472046\n",
      "[Training Epoch 0] Batch 1219, Loss 0.3133118748664856\n",
      "[Training Epoch 0] Batch 1220, Loss 0.35456329584121704\n",
      "[Training Epoch 0] Batch 1221, Loss 0.3413589596748352\n",
      "[Training Epoch 0] Batch 1222, Loss 0.3395330309867859\n",
      "[Training Epoch 0] Batch 1223, Loss 0.35837656259536743\n",
      "[Training Epoch 0] Batch 1224, Loss 0.33365723490715027\n",
      "[Training Epoch 0] Batch 1225, Loss 0.3282352387905121\n",
      "[Training Epoch 0] Batch 1226, Loss 0.32137954235076904\n",
      "[Training Epoch 0] Batch 1227, Loss 0.3578137755393982\n",
      "[Training Epoch 0] Batch 1228, Loss 0.32809582352638245\n",
      "[Training Epoch 0] Batch 1229, Loss 0.3315999507904053\n",
      "[Training Epoch 0] Batch 1230, Loss 0.3460984230041504\n",
      "[Training Epoch 0] Batch 1231, Loss 0.3231167197227478\n",
      "[Training Epoch 0] Batch 1232, Loss 0.3689800500869751\n",
      "[Training Epoch 0] Batch 1233, Loss 0.36950504779815674\n",
      "[Training Epoch 0] Batch 1234, Loss 0.3347432017326355\n",
      "[Training Epoch 0] Batch 1235, Loss 0.34939733147621155\n",
      "[Training Epoch 0] Batch 1236, Loss 0.3629992604255676\n",
      "[Training Epoch 0] Batch 1237, Loss 0.34657809138298035\n",
      "[Training Epoch 0] Batch 1238, Loss 0.3342312276363373\n",
      "[Training Epoch 0] Batch 1239, Loss 0.34149396419525146\n",
      "[Training Epoch 0] Batch 1240, Loss 0.32908856868743896\n",
      "[Training Epoch 0] Batch 1241, Loss 0.35454776883125305\n",
      "[Training Epoch 0] Batch 1242, Loss 0.3234052360057831\n",
      "[Training Epoch 0] Batch 1243, Loss 0.33137235045433044\n",
      "[Training Epoch 0] Batch 1244, Loss 0.33197271823883057\n",
      "[Training Epoch 0] Batch 1245, Loss 0.33775460720062256\n",
      "[Training Epoch 0] Batch 1246, Loss 0.36958110332489014\n",
      "[Training Epoch 0] Batch 1247, Loss 0.3396615982055664\n",
      "[Training Epoch 0] Batch 1248, Loss 0.3313281834125519\n",
      "[Training Epoch 0] Batch 1249, Loss 0.35777437686920166\n",
      "[Training Epoch 0] Batch 1250, Loss 0.3167525827884674\n",
      "[Training Epoch 0] Batch 1251, Loss 0.31870126724243164\n",
      "[Training Epoch 0] Batch 1252, Loss 0.32256531715393066\n",
      "[Training Epoch 0] Batch 1253, Loss 0.3457571268081665\n",
      "[Training Epoch 0] Batch 1254, Loss 0.3521910607814789\n",
      "[Training Epoch 0] Batch 1255, Loss 0.3278485834598541\n",
      "[Training Epoch 0] Batch 1256, Loss 0.35404354333877563\n",
      "[Training Epoch 0] Batch 1257, Loss 0.3664358854293823\n",
      "[Training Epoch 0] Batch 1258, Loss 0.32585686445236206\n",
      "[Training Epoch 0] Batch 1259, Loss 0.35490384697914124\n",
      "[Training Epoch 0] Batch 1260, Loss 0.3653916120529175\n",
      "[Training Epoch 0] Batch 1261, Loss 0.3623396158218384\n",
      "[Training Epoch 0] Batch 1262, Loss 0.3231452703475952\n",
      "[Training Epoch 0] Batch 1263, Loss 0.3358217775821686\n",
      "[Training Epoch 0] Batch 1264, Loss 0.3487531542778015\n",
      "[Training Epoch 0] Batch 1265, Loss 0.3337629735469818\n",
      "[Training Epoch 0] Batch 1266, Loss 0.35933825373649597\n",
      "[Training Epoch 0] Batch 1267, Loss 0.3755682706832886\n",
      "[Training Epoch 0] Batch 1268, Loss 0.3492865562438965\n",
      "[Training Epoch 0] Batch 1269, Loss 0.3263164162635803\n",
      "[Training Epoch 0] Batch 1270, Loss 0.3156639635562897\n",
      "[Training Epoch 0] Batch 1271, Loss 0.33479389548301697\n",
      "[Training Epoch 0] Batch 1272, Loss 0.3314501941204071\n",
      "[Training Epoch 0] Batch 1273, Loss 0.33488255739212036\n",
      "[Training Epoch 0] Batch 1274, Loss 0.3218799829483032\n",
      "[Training Epoch 0] Batch 1275, Loss 0.3410099744796753\n",
      "[Training Epoch 0] Batch 1276, Loss 0.32753705978393555\n",
      "[Training Epoch 0] Batch 1277, Loss 0.3266388177871704\n",
      "[Training Epoch 0] Batch 1278, Loss 0.3482874929904938\n",
      "[Training Epoch 0] Batch 1279, Loss 0.34142085909843445\n",
      "[Training Epoch 0] Batch 1280, Loss 0.34465956687927246\n",
      "[Training Epoch 0] Batch 1281, Loss 0.37688004970550537\n",
      "[Training Epoch 0] Batch 1282, Loss 0.3423358201980591\n",
      "[Training Epoch 0] Batch 1283, Loss 0.36992931365966797\n",
      "[Training Epoch 0] Batch 1284, Loss 0.36961567401885986\n",
      "[Training Epoch 0] Batch 1285, Loss 0.3238074779510498\n",
      "[Training Epoch 0] Batch 1286, Loss 0.3337133228778839\n",
      "[Training Epoch 0] Batch 1287, Loss 0.33434635400772095\n",
      "[Training Epoch 0] Batch 1288, Loss 0.32290923595428467\n",
      "[Training Epoch 0] Batch 1289, Loss 0.37050777673721313\n",
      "[Training Epoch 0] Batch 1290, Loss 0.33369922637939453\n",
      "[Training Epoch 0] Batch 1291, Loss 0.31301403045654297\n",
      "[Training Epoch 0] Batch 1292, Loss 0.3355575203895569\n",
      "[Training Epoch 0] Batch 1293, Loss 0.36442479491233826\n",
      "[Training Epoch 0] Batch 1294, Loss 0.3540707230567932\n",
      "[Training Epoch 0] Batch 1295, Loss 0.35489094257354736\n",
      "[Training Epoch 0] Batch 1296, Loss 0.32713475823402405\n",
      "[Training Epoch 0] Batch 1297, Loss 0.33596301078796387\n",
      "[Training Epoch 0] Batch 1298, Loss 0.3442107141017914\n",
      "[Training Epoch 0] Batch 1299, Loss 0.34281450510025024\n",
      "[Training Epoch 0] Batch 1300, Loss 0.3307197093963623\n",
      "[Training Epoch 0] Batch 1301, Loss 0.3509726822376251\n",
      "[Training Epoch 0] Batch 1302, Loss 0.3341025114059448\n",
      "[Training Epoch 0] Batch 1303, Loss 0.3514915108680725\n",
      "[Training Epoch 0] Batch 1304, Loss 0.3419337868690491\n",
      "[Training Epoch 0] Batch 1305, Loss 0.3337298631668091\n",
      "[Training Epoch 0] Batch 1306, Loss 0.3393198847770691\n",
      "[Training Epoch 0] Batch 1307, Loss 0.3474336862564087\n",
      "[Training Epoch 0] Batch 1308, Loss 0.32403385639190674\n",
      "[Training Epoch 0] Batch 1309, Loss 0.32405614852905273\n",
      "[Training Epoch 0] Batch 1310, Loss 0.34241652488708496\n",
      "[Training Epoch 0] Batch 1311, Loss 0.35186511278152466\n",
      "[Training Epoch 0] Batch 1312, Loss 0.33969777822494507\n",
      "[Training Epoch 0] Batch 1313, Loss 0.33670639991760254\n",
      "[Training Epoch 0] Batch 1314, Loss 0.34467729926109314\n",
      "[Training Epoch 0] Batch 1315, Loss 0.3360059857368469\n",
      "[Training Epoch 0] Batch 1316, Loss 0.3227002024650574\n",
      "[Training Epoch 0] Batch 1317, Loss 0.34993651509284973\n",
      "[Training Epoch 0] Batch 1318, Loss 0.32640278339385986\n",
      "[Training Epoch 0] Batch 1319, Loss 0.33841127157211304\n",
      "[Training Epoch 0] Batch 1320, Loss 0.32452666759490967\n",
      "[Training Epoch 0] Batch 1321, Loss 0.3510841727256775\n",
      "[Training Epoch 0] Batch 1322, Loss 0.31493258476257324\n",
      "[Training Epoch 0] Batch 1323, Loss 0.315041184425354\n",
      "[Training Epoch 0] Batch 1324, Loss 0.35889971256256104\n",
      "[Training Epoch 0] Batch 1325, Loss 0.3671298027038574\n",
      "[Training Epoch 0] Batch 1326, Loss 0.3365963101387024\n",
      "[Training Epoch 0] Batch 1327, Loss 0.35867446660995483\n",
      "[Training Epoch 0] Batch 1328, Loss 0.37884315848350525\n",
      "[Training Epoch 0] Batch 1329, Loss 0.32614666223526\n",
      "[Training Epoch 0] Batch 1330, Loss 0.3190104365348816\n",
      "[Training Epoch 0] Batch 1331, Loss 0.33835741877555847\n",
      "[Training Epoch 0] Batch 1332, Loss 0.3359062373638153\n",
      "[Training Epoch 0] Batch 1333, Loss 0.3089417815208435\n",
      "[Training Epoch 0] Batch 1334, Loss 0.34339219331741333\n",
      "[Training Epoch 0] Batch 1335, Loss 0.3522298336029053\n",
      "[Training Epoch 0] Batch 1336, Loss 0.35785600543022156\n",
      "[Training Epoch 0] Batch 1337, Loss 0.3445364832878113\n",
      "[Training Epoch 0] Batch 1338, Loss 0.32986655831336975\n",
      "[Training Epoch 0] Batch 1339, Loss 0.3455989360809326\n",
      "[Training Epoch 0] Batch 1340, Loss 0.3175172507762909\n",
      "[Training Epoch 0] Batch 1341, Loss 0.3302995562553406\n",
      "[Training Epoch 0] Batch 1342, Loss 0.3531513810157776\n",
      "[Training Epoch 0] Batch 1343, Loss 0.34444594383239746\n",
      "[Training Epoch 0] Batch 1344, Loss 0.32238340377807617\n",
      "[Training Epoch 0] Batch 1345, Loss 0.3400474190711975\n",
      "[Training Epoch 0] Batch 1346, Loss 0.3370405435562134\n",
      "[Training Epoch 0] Batch 1347, Loss 0.35122281312942505\n",
      "[Training Epoch 0] Batch 1348, Loss 0.32233506441116333\n",
      "[Training Epoch 0] Batch 1349, Loss 0.35561609268188477\n",
      "[Training Epoch 0] Batch 1350, Loss 0.33222636580467224\n",
      "[Training Epoch 0] Batch 1351, Loss 0.3175276517868042\n",
      "[Training Epoch 0] Batch 1352, Loss 0.31839561462402344\n",
      "[Training Epoch 0] Batch 1353, Loss 0.34060707688331604\n",
      "[Training Epoch 0] Batch 1354, Loss 0.32509565353393555\n",
      "[Training Epoch 0] Batch 1355, Loss 0.34405383467674255\n",
      "[Training Epoch 0] Batch 1356, Loss 0.3455641567707062\n",
      "[Training Epoch 0] Batch 1357, Loss 0.3342161774635315\n",
      "[Training Epoch 0] Batch 1358, Loss 0.33872079849243164\n",
      "[Training Epoch 0] Batch 1359, Loss 0.33075135946273804\n",
      "[Training Epoch 0] Batch 1360, Loss 0.35005131363868713\n",
      "[Training Epoch 0] Batch 1361, Loss 0.3527153730392456\n",
      "[Training Epoch 0] Batch 1362, Loss 0.3638019859790802\n",
      "[Training Epoch 0] Batch 1363, Loss 0.32954657077789307\n",
      "[Training Epoch 0] Batch 1364, Loss 0.30870866775512695\n",
      "[Training Epoch 0] Batch 1365, Loss 0.33405452966690063\n",
      "[Training Epoch 0] Batch 1366, Loss 0.3586025834083557\n",
      "[Training Epoch 0] Batch 1367, Loss 0.32843589782714844\n",
      "[Training Epoch 0] Batch 1368, Loss 0.34274011850357056\n",
      "[Training Epoch 0] Batch 1369, Loss 0.3424416482448578\n",
      "[Training Epoch 0] Batch 1370, Loss 0.37037619948387146\n",
      "[Training Epoch 0] Batch 1371, Loss 0.3487372398376465\n",
      "[Training Epoch 0] Batch 1372, Loss 0.32957780361175537\n",
      "[Training Epoch 0] Batch 1373, Loss 0.33578383922576904\n",
      "[Training Epoch 0] Batch 1374, Loss 0.370962917804718\n",
      "[Training Epoch 0] Batch 1375, Loss 0.3355214595794678\n",
      "[Training Epoch 0] Batch 1376, Loss 0.3553589880466461\n",
      "[Training Epoch 0] Batch 1377, Loss 0.3371763825416565\n",
      "[Training Epoch 0] Batch 1378, Loss 0.3357556462287903\n",
      "[Training Epoch 0] Batch 1379, Loss 0.3494640588760376\n",
      "[Training Epoch 0] Batch 1380, Loss 0.3480308949947357\n",
      "[Training Epoch 0] Batch 1381, Loss 0.34722715616226196\n",
      "[Training Epoch 0] Batch 1382, Loss 0.34847450256347656\n",
      "[Training Epoch 0] Batch 1383, Loss 0.3200361132621765\n",
      "[Training Epoch 0] Batch 1384, Loss 0.3519170880317688\n",
      "[Training Epoch 0] Batch 1385, Loss 0.3273366689682007\n",
      "[Training Epoch 0] Batch 1386, Loss 0.3072811961174011\n",
      "[Training Epoch 0] Batch 1387, Loss 0.3694489896297455\n",
      "[Training Epoch 0] Batch 1388, Loss 0.3347295820713043\n",
      "[Training Epoch 0] Batch 1389, Loss 0.3466210663318634\n",
      "[Training Epoch 0] Batch 1390, Loss 0.34179434180259705\n",
      "[Training Epoch 0] Batch 1391, Loss 0.3388139009475708\n",
      "[Training Epoch 0] Batch 1392, Loss 0.348894864320755\n",
      "[Training Epoch 0] Batch 1393, Loss 0.3310585021972656\n",
      "[Training Epoch 0] Batch 1394, Loss 0.324886292219162\n",
      "[Training Epoch 0] Batch 1395, Loss 0.3321689963340759\n",
      "[Training Epoch 0] Batch 1396, Loss 0.3422780930995941\n",
      "[Training Epoch 0] Batch 1397, Loss 0.34012100100517273\n",
      "[Training Epoch 0] Batch 1398, Loss 0.338559627532959\n",
      "[Training Epoch 0] Batch 1399, Loss 0.36865514516830444\n",
      "[Training Epoch 0] Batch 1400, Loss 0.34870585799217224\n",
      "[Training Epoch 0] Batch 1401, Loss 0.30801644921302795\n",
      "[Training Epoch 0] Batch 1402, Loss 0.3444133996963501\n",
      "[Training Epoch 0] Batch 1403, Loss 0.33517006039619446\n",
      "[Training Epoch 0] Batch 1404, Loss 0.3806321322917938\n",
      "[Training Epoch 0] Batch 1405, Loss 0.3337832987308502\n",
      "[Training Epoch 0] Batch 1406, Loss 0.32987987995147705\n",
      "[Training Epoch 0] Batch 1407, Loss 0.3389515280723572\n",
      "[Training Epoch 0] Batch 1408, Loss 0.3434762954711914\n",
      "[Training Epoch 0] Batch 1409, Loss 0.30926644802093506\n",
      "[Training Epoch 0] Batch 1410, Loss 0.31421780586242676\n",
      "[Training Epoch 0] Batch 1411, Loss 0.34575992822647095\n",
      "[Training Epoch 0] Batch 1412, Loss 0.33428382873535156\n",
      "[Training Epoch 0] Batch 1413, Loss 0.3222178816795349\n",
      "[Training Epoch 0] Batch 1414, Loss 0.36679965257644653\n",
      "[Training Epoch 0] Batch 1415, Loss 0.3283494710922241\n",
      "[Training Epoch 0] Batch 1416, Loss 0.3218575119972229\n",
      "[Training Epoch 0] Batch 1417, Loss 0.37667757272720337\n",
      "[Training Epoch 0] Batch 1418, Loss 0.33242470026016235\n",
      "[Training Epoch 0] Batch 1419, Loss 0.31807464361190796\n",
      "[Training Epoch 0] Batch 1420, Loss 0.30925244092941284\n",
      "[Training Epoch 0] Batch 1421, Loss 0.32456812262535095\n",
      "[Training Epoch 0] Batch 1422, Loss 0.32409363985061646\n",
      "[Training Epoch 0] Batch 1423, Loss 0.3198736906051636\n",
      "[Training Epoch 0] Batch 1424, Loss 0.3182781934738159\n",
      "[Training Epoch 0] Batch 1425, Loss 0.32437407970428467\n",
      "[Training Epoch 0] Batch 1426, Loss 0.336841344833374\n",
      "[Training Epoch 0] Batch 1427, Loss 0.3338923454284668\n",
      "[Training Epoch 0] Batch 1428, Loss 0.329233318567276\n",
      "[Training Epoch 0] Batch 1429, Loss 0.3568490445613861\n",
      "[Training Epoch 0] Batch 1430, Loss 0.3009864091873169\n",
      "[Training Epoch 0] Batch 1431, Loss 0.36292243003845215\n",
      "[Training Epoch 0] Batch 1432, Loss 0.3441898226737976\n",
      "[Training Epoch 0] Batch 1433, Loss 0.351406455039978\n",
      "[Training Epoch 0] Batch 1434, Loss 0.33722227811813354\n",
      "[Training Epoch 0] Batch 1435, Loss 0.34281349182128906\n",
      "[Training Epoch 0] Batch 1436, Loss 0.33106136322021484\n",
      "[Training Epoch 0] Batch 1437, Loss 0.36401405930519104\n",
      "[Training Epoch 0] Batch 1438, Loss 0.3761446475982666\n",
      "[Training Epoch 0] Batch 1439, Loss 0.33806705474853516\n",
      "[Training Epoch 0] Batch 1440, Loss 0.32221555709838867\n",
      "[Training Epoch 0] Batch 1441, Loss 0.32700520753860474\n",
      "[Training Epoch 0] Batch 1442, Loss 0.34125423431396484\n",
      "[Training Epoch 0] Batch 1443, Loss 0.36243295669555664\n",
      "[Training Epoch 0] Batch 1444, Loss 0.33127421140670776\n",
      "[Training Epoch 0] Batch 1445, Loss 0.33577296137809753\n",
      "[Training Epoch 0] Batch 1446, Loss 0.34426456689834595\n",
      "[Training Epoch 0] Batch 1447, Loss 0.3407803177833557\n",
      "[Training Epoch 0] Batch 1448, Loss 0.3668568730354309\n",
      "[Training Epoch 0] Batch 1449, Loss 0.3676358163356781\n",
      "[Training Epoch 0] Batch 1450, Loss 0.33542853593826294\n",
      "[Training Epoch 0] Batch 1451, Loss 0.3264017701148987\n",
      "[Training Epoch 0] Batch 1452, Loss 0.33362752199172974\n",
      "[Training Epoch 0] Batch 1453, Loss 0.3609965443611145\n",
      "[Training Epoch 0] Batch 1454, Loss 0.32629984617233276\n",
      "[Training Epoch 0] Batch 1455, Loss 0.32078516483306885\n",
      "[Training Epoch 0] Batch 1456, Loss 0.3216507136821747\n",
      "[Training Epoch 0] Batch 1457, Loss 0.36270231008529663\n",
      "[Training Epoch 0] Batch 1458, Loss 0.3308895528316498\n",
      "[Training Epoch 0] Batch 1459, Loss 0.317243367433548\n",
      "[Training Epoch 0] Batch 1460, Loss 0.33922696113586426\n",
      "[Training Epoch 0] Batch 1461, Loss 0.34034907817840576\n",
      "[Training Epoch 0] Batch 1462, Loss 0.3226872980594635\n",
      "[Training Epoch 0] Batch 1463, Loss 0.35379207134246826\n",
      "[Training Epoch 0] Batch 1464, Loss 0.34035998582839966\n",
      "[Training Epoch 0] Batch 1465, Loss 0.3343459665775299\n",
      "[Training Epoch 0] Batch 1466, Loss 0.30772653222084045\n",
      "[Training Epoch 0] Batch 1467, Loss 0.35936838388442993\n",
      "[Training Epoch 0] Batch 1468, Loss 0.3230716586112976\n",
      "[Training Epoch 0] Batch 1469, Loss 0.33657288551330566\n",
      "[Training Epoch 0] Batch 1470, Loss 0.3597344160079956\n",
      "[Training Epoch 0] Batch 1471, Loss 0.36018532514572144\n",
      "[Training Epoch 0] Batch 1472, Loss 0.32834264636039734\n",
      "[Training Epoch 0] Batch 1473, Loss 0.32449066638946533\n",
      "[Training Epoch 0] Batch 1474, Loss 0.3246849775314331\n",
      "[Training Epoch 0] Batch 1475, Loss 0.32091599702835083\n",
      "[Training Epoch 0] Batch 1476, Loss 0.3301590383052826\n",
      "[Training Epoch 0] Batch 1477, Loss 0.3384036719799042\n",
      "[Training Epoch 0] Batch 1478, Loss 0.3522709012031555\n",
      "[Training Epoch 0] Batch 1479, Loss 0.3468514680862427\n",
      "[Training Epoch 0] Batch 1480, Loss 0.3353903293609619\n",
      "[Training Epoch 0] Batch 1481, Loss 0.34009042382240295\n",
      "[Training Epoch 0] Batch 1482, Loss 0.32348665595054626\n",
      "[Training Epoch 0] Batch 1483, Loss 0.32190632820129395\n",
      "[Training Epoch 0] Batch 1484, Loss 0.35706424713134766\n",
      "[Training Epoch 0] Batch 1485, Loss 0.32795190811157227\n",
      "[Training Epoch 0] Batch 1486, Loss 0.36063188314437866\n",
      "[Training Epoch 0] Batch 1487, Loss 0.31334221363067627\n",
      "[Training Epoch 0] Batch 1488, Loss 0.3255408704280853\n",
      "[Training Epoch 0] Batch 1489, Loss 0.31232312321662903\n",
      "[Training Epoch 0] Batch 1490, Loss 0.3457701802253723\n",
      "[Training Epoch 0] Batch 1491, Loss 0.319965660572052\n",
      "[Training Epoch 0] Batch 1492, Loss 0.3214646875858307\n",
      "[Training Epoch 0] Batch 1493, Loss 0.35647425055503845\n",
      "[Training Epoch 0] Batch 1494, Loss 0.3405666947364807\n",
      "[Training Epoch 0] Batch 1495, Loss 0.3272445797920227\n",
      "[Training Epoch 0] Batch 1496, Loss 0.30480316281318665\n",
      "[Training Epoch 0] Batch 1497, Loss 0.3194939196109772\n",
      "[Training Epoch 0] Batch 1498, Loss 0.322600781917572\n",
      "[Training Epoch 0] Batch 1499, Loss 0.31519171595573425\n",
      "[Training Epoch 0] Batch 1500, Loss 0.3608507513999939\n",
      "[Training Epoch 0] Batch 1501, Loss 0.3414320945739746\n",
      "[Training Epoch 0] Batch 1502, Loss 0.320634663105011\n",
      "[Training Epoch 0] Batch 1503, Loss 0.3090212941169739\n",
      "[Training Epoch 0] Batch 1504, Loss 0.31182861328125\n",
      "[Training Epoch 0] Batch 1505, Loss 0.3624657690525055\n",
      "[Training Epoch 0] Batch 1506, Loss 0.34820133447647095\n",
      "[Training Epoch 0] Batch 1507, Loss 0.3296656310558319\n",
      "[Training Epoch 0] Batch 1508, Loss 0.3120478093624115\n",
      "[Training Epoch 0] Batch 1509, Loss 0.3539664149284363\n",
      "[Training Epoch 0] Batch 1510, Loss 0.3057771325111389\n",
      "[Training Epoch 0] Batch 1511, Loss 0.319112628698349\n",
      "[Training Epoch 0] Batch 1512, Loss 0.33733052015304565\n",
      "[Training Epoch 0] Batch 1513, Loss 0.2941293716430664\n",
      "[Training Epoch 0] Batch 1514, Loss 0.3030856251716614\n",
      "[Training Epoch 0] Batch 1515, Loss 0.3724799156188965\n",
      "[Training Epoch 0] Batch 1516, Loss 0.3188015818595886\n",
      "[Training Epoch 0] Batch 1517, Loss 0.32021474838256836\n",
      "[Training Epoch 0] Batch 1518, Loss 0.3491309583187103\n",
      "[Training Epoch 0] Batch 1519, Loss 0.2913700342178345\n",
      "[Training Epoch 0] Batch 1520, Loss 0.35272660851478577\n",
      "[Training Epoch 0] Batch 1521, Loss 0.3592416048049927\n",
      "[Training Epoch 0] Batch 1522, Loss 0.320326030254364\n",
      "[Training Epoch 0] Batch 1523, Loss 0.3128248155117035\n",
      "[Training Epoch 0] Batch 1524, Loss 0.35190173983573914\n",
      "[Training Epoch 0] Batch 1525, Loss 0.3317948579788208\n",
      "[Training Epoch 0] Batch 1526, Loss 0.3160213828086853\n",
      "[Training Epoch 0] Batch 1527, Loss 0.3045646548271179\n",
      "[Training Epoch 0] Batch 1528, Loss 0.3157777786254883\n",
      "[Training Epoch 0] Batch 1529, Loss 0.32630714774131775\n",
      "[Training Epoch 0] Batch 1530, Loss 0.35746070742607117\n",
      "[Training Epoch 0] Batch 1531, Loss 0.36419767141342163\n",
      "[Training Epoch 0] Batch 1532, Loss 0.3385412096977234\n",
      "[Training Epoch 0] Batch 1533, Loss 0.3652072846889496\n",
      "[Training Epoch 0] Batch 1534, Loss 0.3252314031124115\n",
      "[Training Epoch 0] Batch 1535, Loss 0.33183449506759644\n",
      "[Training Epoch 0] Batch 1536, Loss 0.3364284038543701\n",
      "[Training Epoch 0] Batch 1537, Loss 0.3256877064704895\n",
      "[Training Epoch 0] Batch 1538, Loss 0.31685948371887207\n",
      "[Training Epoch 0] Batch 1539, Loss 0.3445234000682831\n",
      "[Training Epoch 0] Batch 1540, Loss 0.335129976272583\n",
      "[Training Epoch 0] Batch 1541, Loss 0.3382372260093689\n",
      "[Training Epoch 0] Batch 1542, Loss 0.3396299481391907\n",
      "[Training Epoch 0] Batch 1543, Loss 0.3283989727497101\n",
      "[Training Epoch 0] Batch 1544, Loss 0.348162978887558\n",
      "[Training Epoch 0] Batch 1545, Loss 0.3087977468967438\n",
      "[Training Epoch 0] Batch 1546, Loss 0.3492172956466675\n",
      "[Training Epoch 0] Batch 1547, Loss 0.3252653181552887\n",
      "[Training Epoch 0] Batch 1548, Loss 0.3150733709335327\n",
      "[Training Epoch 0] Batch 1549, Loss 0.30930453538894653\n",
      "[Training Epoch 0] Batch 1550, Loss 0.32617729902267456\n",
      "[Training Epoch 0] Batch 1551, Loss 0.37949997186660767\n",
      "[Training Epoch 0] Batch 1552, Loss 0.324232816696167\n",
      "[Training Epoch 0] Batch 1553, Loss 0.35524865984916687\n",
      "[Training Epoch 0] Batch 1554, Loss 0.3330017924308777\n",
      "[Training Epoch 0] Batch 1555, Loss 0.3196159601211548\n",
      "[Training Epoch 0] Batch 1556, Loss 0.31819096207618713\n",
      "[Training Epoch 0] Batch 1557, Loss 0.3474813401699066\n",
      "[Training Epoch 0] Batch 1558, Loss 0.31399673223495483\n",
      "[Training Epoch 0] Batch 1559, Loss 0.3733919858932495\n",
      "[Training Epoch 0] Batch 1560, Loss 0.3420286476612091\n",
      "[Training Epoch 0] Batch 1561, Loss 0.30572009086608887\n",
      "[Training Epoch 0] Batch 1562, Loss 0.3358756899833679\n",
      "[Training Epoch 0] Batch 1563, Loss 0.3411821126937866\n",
      "[Training Epoch 0] Batch 1564, Loss 0.30656400322914124\n",
      "[Training Epoch 0] Batch 1565, Loss 0.314355731010437\n",
      "[Training Epoch 0] Batch 1566, Loss 0.32776910066604614\n",
      "[Training Epoch 0] Batch 1567, Loss 0.3122204542160034\n",
      "[Training Epoch 0] Batch 1568, Loss 0.3425098657608032\n",
      "[Training Epoch 0] Batch 1569, Loss 0.3498488664627075\n",
      "[Training Epoch 0] Batch 1570, Loss 0.3231872320175171\n",
      "[Training Epoch 0] Batch 1571, Loss 0.3396393060684204\n",
      "[Training Epoch 0] Batch 1572, Loss 0.34171175956726074\n",
      "[Training Epoch 0] Batch 1573, Loss 0.3481847047805786\n",
      "[Training Epoch 0] Batch 1574, Loss 0.32342278957366943\n",
      "[Training Epoch 0] Batch 1575, Loss 0.3314186930656433\n",
      "[Training Epoch 0] Batch 1576, Loss 0.34784823656082153\n",
      "[Training Epoch 0] Batch 1577, Loss 0.3649054765701294\n",
      "[Training Epoch 0] Batch 1578, Loss 0.317486047744751\n",
      "[Training Epoch 0] Batch 1579, Loss 0.3050259053707123\n",
      "[Training Epoch 0] Batch 1580, Loss 0.3156608045101166\n",
      "[Training Epoch 0] Batch 1581, Loss 0.3235231935977936\n",
      "[Training Epoch 0] Batch 1582, Loss 0.3323521614074707\n",
      "[Training Epoch 0] Batch 1583, Loss 0.29042214155197144\n",
      "[Training Epoch 0] Batch 1584, Loss 0.3204208016395569\n",
      "[Training Epoch 0] Batch 1585, Loss 0.31201833486557007\n",
      "[Training Epoch 0] Batch 1586, Loss 0.33704203367233276\n",
      "[Training Epoch 0] Batch 1587, Loss 0.3130091428756714\n",
      "[Training Epoch 0] Batch 1588, Loss 0.32830220460891724\n",
      "[Training Epoch 0] Batch 1589, Loss 0.3345848321914673\n",
      "[Training Epoch 0] Batch 1590, Loss 0.35875415802001953\n",
      "[Training Epoch 0] Batch 1591, Loss 0.32927411794662476\n",
      "[Training Epoch 0] Batch 1592, Loss 0.34039226174354553\n",
      "[Training Epoch 0] Batch 1593, Loss 0.3140225410461426\n",
      "[Training Epoch 0] Batch 1594, Loss 0.3667137026786804\n",
      "[Training Epoch 0] Batch 1595, Loss 0.288798987865448\n",
      "[Training Epoch 0] Batch 1596, Loss 0.3701411187648773\n",
      "[Training Epoch 0] Batch 1597, Loss 0.323491632938385\n",
      "[Training Epoch 0] Batch 1598, Loss 0.341391384601593\n",
      "[Training Epoch 0] Batch 1599, Loss 0.34994691610336304\n",
      "[Training Epoch 0] Batch 1600, Loss 0.33763572573661804\n",
      "[Training Epoch 0] Batch 1601, Loss 0.33906975388526917\n",
      "[Training Epoch 0] Batch 1602, Loss 0.33343636989593506\n",
      "[Training Epoch 0] Batch 1603, Loss 0.3275720477104187\n",
      "[Training Epoch 0] Batch 1604, Loss 0.3315090537071228\n",
      "[Training Epoch 0] Batch 1605, Loss 0.3285074234008789\n",
      "[Training Epoch 0] Batch 1606, Loss 0.33404287695884705\n",
      "[Training Epoch 0] Batch 1607, Loss 0.3320111632347107\n",
      "[Training Epoch 0] Batch 1608, Loss 0.3486209511756897\n",
      "[Training Epoch 0] Batch 1609, Loss 0.3283648192882538\n",
      "[Training Epoch 0] Batch 1610, Loss 0.31072771549224854\n",
      "[Training Epoch 0] Batch 1611, Loss 0.3232707977294922\n",
      "[Training Epoch 0] Batch 1612, Loss 0.33198362588882446\n",
      "[Training Epoch 0] Batch 1613, Loss 0.34621304273605347\n",
      "[Training Epoch 0] Batch 1614, Loss 0.3093196749687195\n",
      "[Training Epoch 0] Batch 1615, Loss 0.3193792402744293\n",
      "[Training Epoch 0] Batch 1616, Loss 0.3404723107814789\n",
      "[Training Epoch 0] Batch 1617, Loss 0.33543139696121216\n",
      "[Training Epoch 0] Batch 1618, Loss 0.29204902052879333\n",
      "[Training Epoch 0] Batch 1619, Loss 0.3417781591415405\n",
      "[Training Epoch 0] Batch 1620, Loss 0.3223825991153717\n",
      "[Training Epoch 0] Batch 1621, Loss 0.3477018475532532\n",
      "[Training Epoch 0] Batch 1622, Loss 0.30546000599861145\n",
      "[Training Epoch 0] Batch 1623, Loss 0.3250029981136322\n",
      "[Training Epoch 0] Batch 1624, Loss 0.31934016942977905\n",
      "[Training Epoch 0] Batch 1625, Loss 0.30863630771636963\n",
      "[Training Epoch 0] Batch 1626, Loss 0.3160030245780945\n",
      "[Training Epoch 0] Batch 1627, Loss 0.34061843156814575\n",
      "[Training Epoch 0] Batch 1628, Loss 0.3343662619590759\n",
      "[Training Epoch 0] Batch 1629, Loss 0.3146415948867798\n",
      "[Training Epoch 0] Batch 1630, Loss 0.3118199110031128\n",
      "[Training Epoch 0] Batch 1631, Loss 0.2774472236633301\n",
      "[Training Epoch 0] Batch 1632, Loss 0.31800219416618347\n",
      "[Training Epoch 0] Batch 1633, Loss 0.3197125494480133\n",
      "[Training Epoch 0] Batch 1634, Loss 0.3456445336341858\n",
      "[Training Epoch 0] Batch 1635, Loss 0.30239588022232056\n",
      "[Training Epoch 0] Batch 1636, Loss 0.3121463656425476\n",
      "[Training Epoch 0] Batch 1637, Loss 0.3347187042236328\n",
      "[Training Epoch 0] Batch 1638, Loss 0.3428567349910736\n",
      "[Training Epoch 0] Batch 1639, Loss 0.3514564633369446\n",
      "[Training Epoch 0] Batch 1640, Loss 0.310818612575531\n",
      "[Training Epoch 0] Batch 1641, Loss 0.34791797399520874\n",
      "[Training Epoch 0] Batch 1642, Loss 0.33357712626457214\n",
      "[Training Epoch 0] Batch 1643, Loss 0.32278725504875183\n",
      "[Training Epoch 0] Batch 1644, Loss 0.31272923946380615\n",
      "[Training Epoch 0] Batch 1645, Loss 0.33107948303222656\n",
      "[Training Epoch 0] Batch 1646, Loss 0.34583616256713867\n",
      "[Training Epoch 0] Batch 1647, Loss 0.34079813957214355\n",
      "[Training Epoch 0] Batch 1648, Loss 0.33002543449401855\n",
      "[Training Epoch 0] Batch 1649, Loss 0.33788830041885376\n",
      "[Training Epoch 0] Batch 1650, Loss 0.3007230758666992\n",
      "[Training Epoch 0] Batch 1651, Loss 0.3427002429962158\n",
      "[Training Epoch 0] Batch 1652, Loss 0.30577173829078674\n",
      "[Training Epoch 0] Batch 1653, Loss 0.31971168518066406\n",
      "[Training Epoch 0] Batch 1654, Loss 0.324790358543396\n",
      "[Training Epoch 0] Batch 1655, Loss 0.3239637613296509\n",
      "[Training Epoch 0] Batch 1656, Loss 0.3334846496582031\n",
      "[Training Epoch 0] Batch 1657, Loss 0.2946135699748993\n",
      "[Training Epoch 0] Batch 1658, Loss 0.31675297021865845\n",
      "[Training Epoch 0] Batch 1659, Loss 0.32352060079574585\n",
      "[Training Epoch 0] Batch 1660, Loss 0.34201717376708984\n",
      "[Training Epoch 0] Batch 1661, Loss 0.34287410974502563\n",
      "[Training Epoch 0] Batch 1662, Loss 0.32199132442474365\n",
      "[Training Epoch 0] Batch 1663, Loss 0.34286683797836304\n",
      "[Training Epoch 0] Batch 1664, Loss 0.3176839351654053\n",
      "[Training Epoch 0] Batch 1665, Loss 0.293204128742218\n",
      "[Training Epoch 0] Batch 1666, Loss 0.32975712418556213\n",
      "[Training Epoch 0] Batch 1667, Loss 0.33658695220947266\n",
      "[Training Epoch 0] Batch 1668, Loss 0.322209894657135\n",
      "[Training Epoch 0] Batch 1669, Loss 0.32856348156929016\n",
      "[Training Epoch 0] Batch 1670, Loss 0.3305279612541199\n",
      "[Training Epoch 0] Batch 1671, Loss 0.35229161381721497\n",
      "[Training Epoch 0] Batch 1672, Loss 0.36033564805984497\n",
      "[Training Epoch 0] Batch 1673, Loss 0.3227836489677429\n",
      "[Training Epoch 0] Batch 1674, Loss 0.32566243410110474\n",
      "[Training Epoch 0] Batch 1675, Loss 0.33209937810897827\n",
      "[Training Epoch 0] Batch 1676, Loss 0.3376917541027069\n",
      "[Training Epoch 0] Batch 1677, Loss 0.30764153599739075\n",
      "[Training Epoch 0] Batch 1678, Loss 0.34141695499420166\n",
      "[Training Epoch 0] Batch 1679, Loss 0.3152391016483307\n",
      "[Training Epoch 0] Batch 1680, Loss 0.3169514536857605\n",
      "[Training Epoch 0] Batch 1681, Loss 0.3320508897304535\n",
      "[Training Epoch 0] Batch 1682, Loss 0.30328184366226196\n",
      "[Training Epoch 0] Batch 1683, Loss 0.3058925271034241\n",
      "[Training Epoch 0] Batch 1684, Loss 0.3303835988044739\n",
      "[Training Epoch 0] Batch 1685, Loss 0.3184131979942322\n",
      "[Training Epoch 0] Batch 1686, Loss 0.3246051073074341\n",
      "[Training Epoch 0] Batch 1687, Loss 0.3425285518169403\n",
      "[Training Epoch 0] Batch 1688, Loss 0.35909757018089294\n",
      "[Training Epoch 0] Batch 1689, Loss 0.3324892520904541\n",
      "[Training Epoch 0] Batch 1690, Loss 0.3054046928882599\n",
      "[Training Epoch 0] Batch 1691, Loss 0.33781158924102783\n",
      "[Training Epoch 0] Batch 1692, Loss 0.33738845586776733\n",
      "[Training Epoch 0] Batch 1693, Loss 0.3392345905303955\n",
      "[Training Epoch 0] Batch 1694, Loss 0.32425445318222046\n",
      "[Training Epoch 0] Batch 1695, Loss 0.311600923538208\n",
      "[Training Epoch 0] Batch 1696, Loss 0.3187800943851471\n",
      "[Training Epoch 0] Batch 1697, Loss 0.3457150161266327\n",
      "[Training Epoch 0] Batch 1698, Loss 0.3175550699234009\n",
      "[Training Epoch 0] Batch 1699, Loss 0.33568859100341797\n",
      "[Training Epoch 0] Batch 1700, Loss 0.31975388526916504\n",
      "[Training Epoch 0] Batch 1701, Loss 0.32534050941467285\n",
      "[Training Epoch 0] Batch 1702, Loss 0.3066759705543518\n",
      "[Training Epoch 0] Batch 1703, Loss 0.325686514377594\n",
      "[Training Epoch 0] Batch 1704, Loss 0.34489157795906067\n",
      "[Training Epoch 0] Batch 1705, Loss 0.3183485269546509\n",
      "[Training Epoch 0] Batch 1706, Loss 0.33220410346984863\n",
      "[Training Epoch 0] Batch 1707, Loss 0.30025115609169006\n",
      "[Training Epoch 0] Batch 1708, Loss 0.3378526568412781\n",
      "[Training Epoch 0] Batch 1709, Loss 0.3077767789363861\n",
      "[Training Epoch 0] Batch 1710, Loss 0.30964183807373047\n",
      "[Training Epoch 0] Batch 1711, Loss 0.3248942792415619\n",
      "[Training Epoch 0] Batch 1712, Loss 0.3134244680404663\n",
      "[Training Epoch 0] Batch 1713, Loss 0.32098615169525146\n",
      "[Training Epoch 0] Batch 1714, Loss 0.3182189166545868\n",
      "[Training Epoch 0] Batch 1715, Loss 0.3115042448043823\n",
      "[Training Epoch 0] Batch 1716, Loss 0.3155133128166199\n",
      "[Training Epoch 0] Batch 1717, Loss 0.30519548058509827\n",
      "[Training Epoch 0] Batch 1718, Loss 0.3288993835449219\n",
      "[Training Epoch 0] Batch 1719, Loss 0.3528255820274353\n",
      "[Training Epoch 0] Batch 1720, Loss 0.33400648832321167\n",
      "[Training Epoch 0] Batch 1721, Loss 0.30595964193344116\n",
      "[Training Epoch 0] Batch 1722, Loss 0.34208807349205017\n",
      "[Training Epoch 0] Batch 1723, Loss 0.3587704598903656\n",
      "[Training Epoch 0] Batch 1724, Loss 0.33959347009658813\n",
      "[Training Epoch 0] Batch 1725, Loss 0.36180102825164795\n",
      "[Training Epoch 0] Batch 1726, Loss 0.34697195887565613\n",
      "[Training Epoch 0] Batch 1727, Loss 0.34073591232299805\n",
      "[Training Epoch 0] Batch 1728, Loss 0.32805556058883667\n",
      "[Training Epoch 0] Batch 1729, Loss 0.31662240624427795\n",
      "[Training Epoch 0] Batch 1730, Loss 0.3350180387496948\n",
      "[Training Epoch 0] Batch 1731, Loss 0.3376234173774719\n",
      "[Training Epoch 0] Batch 1732, Loss 0.3200528621673584\n",
      "[Training Epoch 0] Batch 1733, Loss 0.30828234553337097\n",
      "[Training Epoch 0] Batch 1734, Loss 0.32270658016204834\n",
      "[Training Epoch 0] Batch 1735, Loss 0.3127703368663788\n",
      "[Training Epoch 0] Batch 1736, Loss 0.3124939501285553\n",
      "[Training Epoch 0] Batch 1737, Loss 0.32803070545196533\n",
      "[Training Epoch 0] Batch 1738, Loss 0.362148642539978\n",
      "[Training Epoch 0] Batch 1739, Loss 0.3204461336135864\n",
      "[Training Epoch 0] Batch 1740, Loss 0.33979207277297974\n",
      "[Training Epoch 0] Batch 1741, Loss 0.34038758277893066\n",
      "[Training Epoch 0] Batch 1742, Loss 0.32608330249786377\n",
      "[Training Epoch 0] Batch 1743, Loss 0.34612172842025757\n",
      "[Training Epoch 0] Batch 1744, Loss 0.34118854999542236\n",
      "[Training Epoch 0] Batch 1745, Loss 0.36447930335998535\n",
      "[Training Epoch 0] Batch 1746, Loss 0.354838490486145\n",
      "[Training Epoch 0] Batch 1747, Loss 0.3099750876426697\n",
      "[Training Epoch 0] Batch 1748, Loss 0.3340635597705841\n",
      "[Training Epoch 0] Batch 1749, Loss 0.31098446249961853\n",
      "[Training Epoch 0] Batch 1750, Loss 0.33600306510925293\n",
      "[Training Epoch 0] Batch 1751, Loss 0.3336597979068756\n",
      "[Training Epoch 0] Batch 1752, Loss 0.34197506308555603\n",
      "[Training Epoch 0] Batch 1753, Loss 0.33956849575042725\n",
      "[Training Epoch 0] Batch 1754, Loss 0.3189271092414856\n",
      "[Training Epoch 0] Batch 1755, Loss 0.3254774808883667\n",
      "[Training Epoch 0] Batch 1756, Loss 0.3208343982696533\n",
      "[Training Epoch 0] Batch 1757, Loss 0.3300750255584717\n",
      "[Training Epoch 0] Batch 1758, Loss 0.31685513257980347\n",
      "[Training Epoch 0] Batch 1759, Loss 0.3107982575893402\n",
      "[Training Epoch 0] Batch 1760, Loss 0.3332042098045349\n",
      "[Training Epoch 0] Batch 1761, Loss 0.3537374436855316\n",
      "[Training Epoch 0] Batch 1762, Loss 0.3411281406879425\n",
      "[Training Epoch 0] Batch 1763, Loss 0.34101298451423645\n",
      "[Training Epoch 0] Batch 1764, Loss 0.3405561149120331\n",
      "[Training Epoch 0] Batch 1765, Loss 0.3264692425727844\n",
      "[Training Epoch 0] Batch 1766, Loss 0.34138187766075134\n",
      "[Training Epoch 0] Batch 1767, Loss 0.34002581238746643\n",
      "[Training Epoch 0] Batch 1768, Loss 0.34667205810546875\n",
      "[Training Epoch 0] Batch 1769, Loss 0.3503543734550476\n",
      "[Training Epoch 0] Batch 1770, Loss 0.3234376907348633\n",
      "[Training Epoch 0] Batch 1771, Loss 0.31771689653396606\n",
      "[Training Epoch 0] Batch 1772, Loss 0.3563271164894104\n",
      "[Training Epoch 0] Batch 1773, Loss 0.32457292079925537\n",
      "[Training Epoch 0] Batch 1774, Loss 0.3297991156578064\n",
      "[Training Epoch 0] Batch 1775, Loss 0.33634766936302185\n",
      "[Training Epoch 0] Batch 1776, Loss 0.3223312497138977\n",
      "[Training Epoch 0] Batch 1777, Loss 0.31378936767578125\n",
      "[Training Epoch 0] Batch 1778, Loss 0.33140793442726135\n",
      "[Training Epoch 0] Batch 1779, Loss 0.32387575507164\n",
      "[Training Epoch 0] Batch 1780, Loss 0.32370370626449585\n",
      "[Training Epoch 0] Batch 1781, Loss 0.3048326373100281\n",
      "[Training Epoch 0] Batch 1782, Loss 0.29006054997444153\n",
      "[Training Epoch 0] Batch 1783, Loss 0.3520323932170868\n",
      "[Training Epoch 0] Batch 1784, Loss 0.3220333755016327\n",
      "[Training Epoch 0] Batch 1785, Loss 0.3021528422832489\n",
      "[Training Epoch 0] Batch 1786, Loss 0.3179474174976349\n",
      "[Training Epoch 0] Batch 1787, Loss 0.30228328704833984\n",
      "[Training Epoch 0] Batch 1788, Loss 0.3289892077445984\n",
      "[Training Epoch 0] Batch 1789, Loss 0.3210662007331848\n",
      "[Training Epoch 0] Batch 1790, Loss 0.31220752000808716\n",
      "[Training Epoch 0] Batch 1791, Loss 0.32527101039886475\n",
      "[Training Epoch 0] Batch 1792, Loss 0.3167908191680908\n",
      "[Training Epoch 0] Batch 1793, Loss 0.35414671897888184\n",
      "[Training Epoch 0] Batch 1794, Loss 0.3119535446166992\n",
      "[Training Epoch 0] Batch 1795, Loss 0.33858346939086914\n",
      "[Training Epoch 0] Batch 1796, Loss 0.32632923126220703\n",
      "[Training Epoch 0] Batch 1797, Loss 0.29942575097084045\n",
      "[Training Epoch 0] Batch 1798, Loss 0.3038839101791382\n",
      "[Training Epoch 0] Batch 1799, Loss 0.3454531729221344\n",
      "[Training Epoch 0] Batch 1800, Loss 0.38367852568626404\n",
      "[Training Epoch 0] Batch 1801, Loss 0.2882166802883148\n",
      "[Training Epoch 0] Batch 1802, Loss 0.3135679364204407\n",
      "[Training Epoch 0] Batch 1803, Loss 0.31480759382247925\n",
      "[Training Epoch 0] Batch 1804, Loss 0.31315475702285767\n",
      "[Training Epoch 0] Batch 1805, Loss 0.3291281461715698\n",
      "[Training Epoch 0] Batch 1806, Loss 0.3163381814956665\n",
      "[Training Epoch 0] Batch 1807, Loss 0.33534717559814453\n",
      "[Training Epoch 0] Batch 1808, Loss 0.3101881742477417\n",
      "[Training Epoch 0] Batch 1809, Loss 0.34045135974884033\n",
      "[Training Epoch 0] Batch 1810, Loss 0.33532431721687317\n",
      "[Training Epoch 0] Batch 1811, Loss 0.2997583746910095\n",
      "[Training Epoch 0] Batch 1812, Loss 0.29760435223579407\n",
      "[Training Epoch 0] Batch 1813, Loss 0.29617053270339966\n",
      "[Training Epoch 0] Batch 1814, Loss 0.3386082053184509\n",
      "[Training Epoch 0] Batch 1815, Loss 0.34128469228744507\n",
      "[Training Epoch 0] Batch 1816, Loss 0.3209822475910187\n",
      "[Training Epoch 0] Batch 1817, Loss 0.34838438034057617\n",
      "[Training Epoch 0] Batch 1818, Loss 0.3565569221973419\n",
      "[Training Epoch 0] Batch 1819, Loss 0.32619568705558777\n",
      "[Training Epoch 0] Batch 1820, Loss 0.3379409611225128\n",
      "[Training Epoch 0] Batch 1821, Loss 0.3045778274536133\n",
      "[Training Epoch 0] Batch 1822, Loss 0.31439530849456787\n",
      "[Training Epoch 0] Batch 1823, Loss 0.32907116413116455\n",
      "[Training Epoch 0] Batch 1824, Loss 0.3105207085609436\n",
      "[Training Epoch 0] Batch 1825, Loss 0.31481772661209106\n",
      "[Training Epoch 0] Batch 1826, Loss 0.3337816596031189\n",
      "[Training Epoch 0] Batch 1827, Loss 0.32763564586639404\n",
      "[Training Epoch 0] Batch 1828, Loss 0.32460930943489075\n",
      "[Training Epoch 0] Batch 1829, Loss 0.3451704978942871\n",
      "[Training Epoch 0] Batch 1830, Loss 0.349966824054718\n",
      "[Training Epoch 0] Batch 1831, Loss 0.3156166672706604\n",
      "[Training Epoch 0] Batch 1832, Loss 0.35681647062301636\n",
      "[Training Epoch 0] Batch 1833, Loss 0.3398466110229492\n",
      "[Training Epoch 0] Batch 1834, Loss 0.35123664140701294\n",
      "[Training Epoch 0] Batch 1835, Loss 0.3393957316875458\n",
      "[Training Epoch 0] Batch 1836, Loss 0.3336455225944519\n",
      "[Training Epoch 0] Batch 1837, Loss 0.3164334297180176\n",
      "[Training Epoch 0] Batch 1838, Loss 0.31343910098075867\n",
      "[Training Epoch 0] Batch 1839, Loss 0.3148577809333801\n",
      "[Training Epoch 0] Batch 1840, Loss 0.2965613603591919\n",
      "[Training Epoch 0] Batch 1841, Loss 0.31541675329208374\n",
      "[Training Epoch 0] Batch 1842, Loss 0.31456390023231506\n",
      "[Training Epoch 0] Batch 1843, Loss 0.2907654643058777\n",
      "[Training Epoch 0] Batch 1844, Loss 0.31686386466026306\n",
      "[Training Epoch 0] Batch 1845, Loss 0.33770087361335754\n",
      "[Training Epoch 0] Batch 1846, Loss 0.3393945097923279\n",
      "[Training Epoch 0] Batch 1847, Loss 0.351780503988266\n",
      "[Training Epoch 0] Batch 1848, Loss 0.3256203830242157\n",
      "[Training Epoch 0] Batch 1849, Loss 0.3291395306587219\n",
      "[Training Epoch 0] Batch 1850, Loss 0.344220906496048\n",
      "[Training Epoch 0] Batch 1851, Loss 0.32828599214553833\n",
      "[Training Epoch 0] Batch 1852, Loss 0.33008962869644165\n",
      "[Training Epoch 0] Batch 1853, Loss 0.3407427668571472\n",
      "[Training Epoch 0] Batch 1854, Loss 0.3048741817474365\n",
      "[Training Epoch 0] Batch 1855, Loss 0.30948540568351746\n",
      "[Training Epoch 0] Batch 1856, Loss 0.33349859714508057\n",
      "[Training Epoch 0] Batch 1857, Loss 0.3374578356742859\n",
      "[Training Epoch 0] Batch 1858, Loss 0.33637821674346924\n",
      "[Training Epoch 0] Batch 1859, Loss 0.3115755319595337\n",
      "[Training Epoch 0] Batch 1860, Loss 0.2984616458415985\n",
      "[Training Epoch 0] Batch 1861, Loss 0.3275695741176605\n",
      "[Training Epoch 0] Batch 1862, Loss 0.2931702435016632\n",
      "[Training Epoch 0] Batch 1863, Loss 0.32673847675323486\n",
      "[Training Epoch 0] Batch 1864, Loss 0.3143858313560486\n",
      "[Training Epoch 0] Batch 1865, Loss 0.32835453748703003\n",
      "[Training Epoch 0] Batch 1866, Loss 0.3327878713607788\n",
      "[Training Epoch 0] Batch 1867, Loss 0.331775426864624\n",
      "[Training Epoch 0] Batch 1868, Loss 0.32230716943740845\n",
      "[Training Epoch 0] Batch 1869, Loss 0.32007694244384766\n",
      "[Training Epoch 0] Batch 1870, Loss 0.35017597675323486\n",
      "[Training Epoch 0] Batch 1871, Loss 0.3183043599128723\n",
      "[Training Epoch 0] Batch 1872, Loss 0.3363381028175354\n",
      "[Training Epoch 0] Batch 1873, Loss 0.3496670722961426\n",
      "[Training Epoch 0] Batch 1874, Loss 0.31946030259132385\n",
      "[Training Epoch 0] Batch 1875, Loss 0.33001285791397095\n",
      "[Training Epoch 0] Batch 1876, Loss 0.3240751028060913\n",
      "[Training Epoch 0] Batch 1877, Loss 0.316286563873291\n",
      "[Training Epoch 0] Batch 1878, Loss 0.3543645739555359\n",
      "[Training Epoch 0] Batch 1879, Loss 0.33141690492630005\n",
      "[Training Epoch 0] Batch 1880, Loss 0.35328400135040283\n",
      "[Training Epoch 0] Batch 1881, Loss 0.3538416922092438\n",
      "[Training Epoch 0] Batch 1882, Loss 0.32089078426361084\n",
      "[Training Epoch 0] Batch 1883, Loss 0.3205050230026245\n",
      "[Training Epoch 0] Batch 1884, Loss 0.31132829189300537\n",
      "[Training Epoch 0] Batch 1885, Loss 0.3328803777694702\n",
      "[Training Epoch 0] Batch 1886, Loss 0.32200098037719727\n",
      "[Training Epoch 0] Batch 1887, Loss 0.3349355459213257\n",
      "[Training Epoch 0] Batch 1888, Loss 0.33442801237106323\n",
      "[Training Epoch 0] Batch 1889, Loss 0.34028637409210205\n",
      "[Training Epoch 0] Batch 1890, Loss 0.3028782606124878\n",
      "[Training Epoch 0] Batch 1891, Loss 0.30476677417755127\n",
      "[Training Epoch 0] Batch 1892, Loss 0.3193309009075165\n",
      "[Training Epoch 0] Batch 1893, Loss 0.3388817310333252\n",
      "[Training Epoch 0] Batch 1894, Loss 0.33287686109542847\n",
      "[Training Epoch 0] Batch 1895, Loss 0.33826711773872375\n",
      "[Training Epoch 0] Batch 1896, Loss 0.33178994059562683\n",
      "[Training Epoch 0] Batch 1897, Loss 0.34933650493621826\n",
      "[Training Epoch 0] Batch 1898, Loss 0.35732635855674744\n",
      "[Training Epoch 0] Batch 1899, Loss 0.32691526412963867\n",
      "[Training Epoch 0] Batch 1900, Loss 0.3021019697189331\n",
      "[Training Epoch 0] Batch 1901, Loss 0.29812389612197876\n",
      "[Training Epoch 0] Batch 1902, Loss 0.35133516788482666\n",
      "[Training Epoch 0] Batch 1903, Loss 0.3116874694824219\n",
      "[Training Epoch 0] Batch 1904, Loss 0.3444325923919678\n",
      "[Training Epoch 0] Batch 1905, Loss 0.33804234862327576\n",
      "[Training Epoch 0] Batch 1906, Loss 0.2933257222175598\n",
      "[Training Epoch 0] Batch 1907, Loss 0.35157254338264465\n",
      "[Training Epoch 0] Batch 1908, Loss 0.33996251225471497\n",
      "[Training Epoch 0] Batch 1909, Loss 0.29623690247535706\n",
      "[Training Epoch 0] Batch 1910, Loss 0.3109881579875946\n",
      "[Training Epoch 0] Batch 1911, Loss 0.3323110044002533\n",
      "[Training Epoch 0] Batch 1912, Loss 0.3117923140525818\n",
      "[Training Epoch 0] Batch 1913, Loss 0.3205357789993286\n",
      "[Training Epoch 0] Batch 1914, Loss 0.3319486379623413\n",
      "[Training Epoch 0] Batch 1915, Loss 0.3366324305534363\n",
      "[Training Epoch 0] Batch 1916, Loss 0.3122260570526123\n",
      "[Training Epoch 0] Batch 1917, Loss 0.32710710167884827\n",
      "[Training Epoch 0] Batch 1918, Loss 0.34392088651657104\n",
      "[Training Epoch 0] Batch 1919, Loss 0.33642667531967163\n",
      "[Training Epoch 0] Batch 1920, Loss 0.3339691460132599\n",
      "[Training Epoch 0] Batch 1921, Loss 0.3322552740573883\n",
      "[Training Epoch 0] Batch 1922, Loss 0.3248273432254791\n",
      "[Training Epoch 0] Batch 1923, Loss 0.34853312373161316\n",
      "[Training Epoch 0] Batch 1924, Loss 0.32609763741493225\n",
      "[Training Epoch 0] Batch 1925, Loss 0.30102285742759705\n",
      "[Training Epoch 0] Batch 1926, Loss 0.32202965021133423\n",
      "[Training Epoch 0] Batch 1927, Loss 0.31241941452026367\n",
      "[Training Epoch 0] Batch 1928, Loss 0.3457668721675873\n",
      "[Training Epoch 0] Batch 1929, Loss 0.311604380607605\n",
      "[Training Epoch 0] Batch 1930, Loss 0.35031652450561523\n",
      "[Training Epoch 0] Batch 1931, Loss 0.30545979738235474\n",
      "[Training Epoch 0] Batch 1932, Loss 0.3240985870361328\n",
      "[Training Epoch 0] Batch 1933, Loss 0.33668726682662964\n",
      "[Training Epoch 0] Batch 1934, Loss 0.32053494453430176\n",
      "[Training Epoch 0] Batch 1935, Loss 0.30448904633522034\n",
      "[Training Epoch 0] Batch 1936, Loss 0.3160141408443451\n",
      "[Training Epoch 0] Batch 1937, Loss 0.32668793201446533\n",
      "[Training Epoch 0] Batch 1938, Loss 0.32929909229278564\n",
      "[Training Epoch 0] Batch 1939, Loss 0.29873812198638916\n",
      "[Training Epoch 0] Batch 1940, Loss 0.3081636428833008\n",
      "[Training Epoch 0] Batch 1941, Loss 0.3224443793296814\n",
      "[Training Epoch 0] Batch 1942, Loss 0.32641053199768066\n",
      "[Training Epoch 0] Batch 1943, Loss 0.33615177869796753\n",
      "[Training Epoch 0] Batch 1944, Loss 0.3047643303871155\n",
      "[Training Epoch 0] Batch 1945, Loss 0.3059934973716736\n",
      "[Training Epoch 0] Batch 1946, Loss 0.3309304118156433\n",
      "[Training Epoch 0] Batch 1947, Loss 0.34841883182525635\n",
      "[Training Epoch 0] Batch 1948, Loss 0.3142215311527252\n",
      "[Training Epoch 0] Batch 1949, Loss 0.29922962188720703\n",
      "[Training Epoch 0] Batch 1950, Loss 0.2991120219230652\n",
      "[Training Epoch 0] Batch 1951, Loss 0.3144721984863281\n",
      "[Training Epoch 0] Batch 1952, Loss 0.3077935576438904\n",
      "[Training Epoch 0] Batch 1953, Loss 0.32416296005249023\n",
      "[Training Epoch 0] Batch 1954, Loss 0.31136882305145264\n",
      "[Training Epoch 0] Batch 1955, Loss 0.31756746768951416\n",
      "[Training Epoch 0] Batch 1956, Loss 0.3216490149497986\n",
      "[Training Epoch 0] Batch 1957, Loss 0.32292452454566956\n",
      "[Training Epoch 0] Batch 1958, Loss 0.3349337577819824\n",
      "[Training Epoch 0] Batch 1959, Loss 0.3336721658706665\n",
      "[Training Epoch 0] Batch 1960, Loss 0.31848639249801636\n",
      "[Training Epoch 0] Batch 1961, Loss 0.3201902210712433\n",
      "[Training Epoch 0] Batch 1962, Loss 0.2915489077568054\n",
      "[Training Epoch 0] Batch 1963, Loss 0.3286641836166382\n",
      "[Training Epoch 0] Batch 1964, Loss 0.313321590423584\n",
      "[Training Epoch 0] Batch 1965, Loss 0.2885882258415222\n",
      "[Training Epoch 0] Batch 1966, Loss 0.3085663914680481\n",
      "[Training Epoch 0] Batch 1967, Loss 0.3298383057117462\n",
      "[Training Epoch 0] Batch 1968, Loss 0.31721439957618713\n",
      "[Training Epoch 0] Batch 1969, Loss 0.3351486027240753\n",
      "[Training Epoch 0] Batch 1970, Loss 0.3405683934688568\n",
      "[Training Epoch 0] Batch 1971, Loss 0.32873377203941345\n",
      "[Training Epoch 0] Batch 1972, Loss 0.30695950984954834\n",
      "[Training Epoch 0] Batch 1973, Loss 0.3164370656013489\n",
      "[Training Epoch 0] Batch 1974, Loss 0.35201770067214966\n",
      "[Training Epoch 0] Batch 1975, Loss 0.3351759612560272\n",
      "[Training Epoch 0] Batch 1976, Loss 0.34573718905448914\n",
      "[Training Epoch 0] Batch 1977, Loss 0.34051990509033203\n",
      "[Training Epoch 0] Batch 1978, Loss 0.30798423290252686\n",
      "[Training Epoch 0] Batch 1979, Loss 0.28757646679878235\n",
      "[Training Epoch 0] Batch 1980, Loss 0.32131287455558777\n",
      "[Training Epoch 0] Batch 1981, Loss 0.32244157791137695\n",
      "[Training Epoch 0] Batch 1982, Loss 0.3218417763710022\n",
      "[Training Epoch 0] Batch 1983, Loss 0.3132351338863373\n",
      "[Training Epoch 0] Batch 1984, Loss 0.33314377069473267\n",
      "[Training Epoch 0] Batch 1985, Loss 0.3241434097290039\n",
      "[Training Epoch 0] Batch 1986, Loss 0.31765681505203247\n",
      "[Training Epoch 0] Batch 1987, Loss 0.35845088958740234\n",
      "[Training Epoch 0] Batch 1988, Loss 0.33049309253692627\n",
      "[Training Epoch 0] Batch 1989, Loss 0.30348604917526245\n",
      "[Training Epoch 0] Batch 1990, Loss 0.35390007495880127\n",
      "[Training Epoch 0] Batch 1991, Loss 0.297640860080719\n",
      "[Training Epoch 0] Batch 1992, Loss 0.33059847354888916\n",
      "[Training Epoch 0] Batch 1993, Loss 0.33247166872024536\n",
      "[Training Epoch 0] Batch 1994, Loss 0.346468985080719\n",
      "[Training Epoch 0] Batch 1995, Loss 0.32968902587890625\n",
      "[Training Epoch 0] Batch 1996, Loss 0.34480077028274536\n",
      "[Training Epoch 0] Batch 1997, Loss 0.3174234628677368\n",
      "[Training Epoch 0] Batch 1998, Loss 0.30013230443000793\n",
      "[Training Epoch 0] Batch 1999, Loss 0.3354005813598633\n",
      "[Training Epoch 0] Batch 2000, Loss 0.3511407971382141\n",
      "[Training Epoch 0] Batch 2001, Loss 0.345313161611557\n",
      "[Training Epoch 0] Batch 2002, Loss 0.30996373295783997\n",
      "[Training Epoch 0] Batch 2003, Loss 0.3248755633831024\n",
      "[Training Epoch 0] Batch 2004, Loss 0.3238662779331207\n",
      "[Training Epoch 0] Batch 2005, Loss 0.33658885955810547\n",
      "[Training Epoch 0] Batch 2006, Loss 0.339558869600296\n",
      "[Training Epoch 0] Batch 2007, Loss 0.3123076558113098\n",
      "[Training Epoch 0] Batch 2008, Loss 0.32538163661956787\n",
      "[Training Epoch 0] Batch 2009, Loss 0.34761545062065125\n",
      "[Training Epoch 0] Batch 2010, Loss 0.317481130361557\n",
      "[Training Epoch 0] Batch 2011, Loss 0.2868911027908325\n",
      "[Training Epoch 0] Batch 2012, Loss 0.3041340112686157\n",
      "[Training Epoch 0] Batch 2013, Loss 0.2903120815753937\n",
      "[Training Epoch 0] Batch 2014, Loss 0.2988741099834442\n",
      "[Training Epoch 0] Batch 2015, Loss 0.345608115196228\n",
      "[Training Epoch 0] Batch 2016, Loss 0.32471537590026855\n",
      "[Training Epoch 0] Batch 2017, Loss 0.3283205032348633\n",
      "[Training Epoch 0] Batch 2018, Loss 0.2798040211200714\n",
      "[Training Epoch 0] Batch 2019, Loss 0.30891332030296326\n",
      "[Training Epoch 0] Batch 2020, Loss 0.2953147292137146\n",
      "[Training Epoch 0] Batch 2021, Loss 0.3235708475112915\n",
      "[Training Epoch 0] Batch 2022, Loss 0.32279613614082336\n",
      "[Training Epoch 0] Batch 2023, Loss 0.3474648594856262\n",
      "[Training Epoch 0] Batch 2024, Loss 0.31898003816604614\n",
      "[Training Epoch 0] Batch 2025, Loss 0.31525084376335144\n",
      "[Training Epoch 0] Batch 2026, Loss 0.33058983087539673\n",
      "[Training Epoch 0] Batch 2027, Loss 0.30126649141311646\n",
      "[Training Epoch 0] Batch 2028, Loss 0.3275125026702881\n",
      "[Training Epoch 0] Batch 2029, Loss 0.31881454586982727\n",
      "[Training Epoch 0] Batch 2030, Loss 0.3407644033432007\n",
      "[Training Epoch 0] Batch 2031, Loss 0.35214656591415405\n",
      "[Training Epoch 0] Batch 2032, Loss 0.3153379559516907\n",
      "[Training Epoch 0] Batch 2033, Loss 0.34447646141052246\n",
      "[Training Epoch 0] Batch 2034, Loss 0.3080836832523346\n",
      "[Training Epoch 0] Batch 2035, Loss 0.3224269449710846\n",
      "[Training Epoch 0] Batch 2036, Loss 0.30128929018974304\n",
      "[Training Epoch 0] Batch 2037, Loss 0.3153702914714813\n",
      "[Training Epoch 0] Batch 2038, Loss 0.3162742257118225\n",
      "[Training Epoch 0] Batch 2039, Loss 0.3685084283351898\n",
      "[Training Epoch 0] Batch 2040, Loss 0.30850914120674133\n",
      "[Training Epoch 0] Batch 2041, Loss 0.3318832516670227\n",
      "[Training Epoch 0] Batch 2042, Loss 0.32821059226989746\n",
      "[Training Epoch 0] Batch 2043, Loss 0.30411168932914734\n",
      "[Training Epoch 0] Batch 2044, Loss 0.3403148651123047\n",
      "[Training Epoch 0] Batch 2045, Loss 0.3311152160167694\n",
      "[Training Epoch 0] Batch 2046, Loss 0.3040124177932739\n",
      "[Training Epoch 0] Batch 2047, Loss 0.35552048683166504\n",
      "[Training Epoch 0] Batch 2048, Loss 0.3239232897758484\n",
      "[Training Epoch 0] Batch 2049, Loss 0.3344138264656067\n",
      "[Training Epoch 0] Batch 2050, Loss 0.3517225384712219\n",
      "[Training Epoch 0] Batch 2051, Loss 0.2988324761390686\n",
      "[Training Epoch 0] Batch 2052, Loss 0.29991865158081055\n",
      "[Training Epoch 0] Batch 2053, Loss 0.33848798274993896\n",
      "[Training Epoch 0] Batch 2054, Loss 0.3287741541862488\n",
      "[Training Epoch 0] Batch 2055, Loss 0.2997704744338989\n",
      "[Training Epoch 0] Batch 2056, Loss 0.3387368619441986\n",
      "[Training Epoch 0] Batch 2057, Loss 0.31220415234565735\n",
      "[Training Epoch 0] Batch 2058, Loss 0.3523595929145813\n",
      "[Training Epoch 0] Batch 2059, Loss 0.33860236406326294\n",
      "[Training Epoch 0] Batch 2060, Loss 0.3192758858203888\n",
      "[Training Epoch 0] Batch 2061, Loss 0.3146076798439026\n",
      "[Training Epoch 0] Batch 2062, Loss 0.32276996970176697\n",
      "[Training Epoch 0] Batch 2063, Loss 0.30843690037727356\n",
      "[Training Epoch 0] Batch 2064, Loss 0.31679126620292664\n",
      "[Training Epoch 0] Batch 2065, Loss 0.3088070750236511\n",
      "[Training Epoch 0] Batch 2066, Loss 0.2889541983604431\n",
      "[Training Epoch 0] Batch 2067, Loss 0.2771452069282532\n",
      "[Training Epoch 0] Batch 2068, Loss 0.3228570222854614\n",
      "[Training Epoch 0] Batch 2069, Loss 0.3140316307544708\n",
      "[Training Epoch 0] Batch 2070, Loss 0.30507180094718933\n",
      "[Training Epoch 0] Batch 2071, Loss 0.3159636855125427\n",
      "[Training Epoch 0] Batch 2072, Loss 0.3084404468536377\n",
      "[Training Epoch 0] Batch 2073, Loss 0.3071404695510864\n",
      "[Training Epoch 0] Batch 2074, Loss 0.3265611529350281\n",
      "[Training Epoch 0] Batch 2075, Loss 0.3283845782279968\n",
      "[Training Epoch 0] Batch 2076, Loss 0.3262069523334503\n",
      "[Training Epoch 0] Batch 2077, Loss 0.337011456489563\n",
      "[Training Epoch 0] Batch 2078, Loss 0.2972445487976074\n",
      "[Training Epoch 0] Batch 2079, Loss 0.3122982382774353\n",
      "[Training Epoch 0] Batch 2080, Loss 0.355536550283432\n",
      "[Training Epoch 0] Batch 2081, Loss 0.3139212727546692\n",
      "[Training Epoch 0] Batch 2082, Loss 0.31438493728637695\n",
      "[Training Epoch 0] Batch 2083, Loss 0.31615614891052246\n",
      "[Training Epoch 0] Batch 2084, Loss 0.3345327377319336\n",
      "[Training Epoch 0] Batch 2085, Loss 0.34984996914863586\n",
      "[Training Epoch 0] Batch 2086, Loss 0.3102066218852997\n",
      "[Training Epoch 0] Batch 2087, Loss 0.3037155270576477\n",
      "[Training Epoch 0] Batch 2088, Loss 0.3549208641052246\n",
      "[Training Epoch 0] Batch 2089, Loss 0.33597081899642944\n",
      "[Training Epoch 0] Batch 2090, Loss 0.2998722195625305\n",
      "[Training Epoch 0] Batch 2091, Loss 0.33495694398880005\n",
      "[Training Epoch 0] Batch 2092, Loss 0.3166695535182953\n",
      "[Training Epoch 0] Batch 2093, Loss 0.342885285615921\n",
      "[Training Epoch 0] Batch 2094, Loss 0.32407790422439575\n",
      "[Training Epoch 0] Batch 2095, Loss 0.30428725481033325\n",
      "[Training Epoch 0] Batch 2096, Loss 0.3319668471813202\n",
      "[Training Epoch 0] Batch 2097, Loss 0.2852044403553009\n",
      "[Training Epoch 0] Batch 2098, Loss 0.3233101963996887\n",
      "[Training Epoch 0] Batch 2099, Loss 0.31505584716796875\n",
      "[Training Epoch 0] Batch 2100, Loss 0.32997316122055054\n",
      "[Training Epoch 0] Batch 2101, Loss 0.3182869553565979\n",
      "[Training Epoch 0] Batch 2102, Loss 0.3348383903503418\n",
      "[Training Epoch 0] Batch 2103, Loss 0.33501338958740234\n",
      "[Training Epoch 0] Batch 2104, Loss 0.3041120171546936\n",
      "[Training Epoch 0] Batch 2105, Loss 0.32566189765930176\n",
      "[Training Epoch 0] Batch 2106, Loss 0.3385446071624756\n",
      "[Training Epoch 0] Batch 2107, Loss 0.3274126648902893\n",
      "[Training Epoch 0] Batch 2108, Loss 0.3629917800426483\n",
      "[Training Epoch 0] Batch 2109, Loss 0.3022382855415344\n",
      "[Training Epoch 0] Batch 2110, Loss 0.3160761594772339\n",
      "[Training Epoch 0] Batch 2111, Loss 0.3172391653060913\n",
      "[Training Epoch 0] Batch 2112, Loss 0.33015260100364685\n",
      "[Training Epoch 0] Batch 2113, Loss 0.3199833929538727\n",
      "[Training Epoch 0] Batch 2114, Loss 0.3159544765949249\n",
      "[Training Epoch 0] Batch 2115, Loss 0.31014886498451233\n",
      "[Training Epoch 0] Batch 2116, Loss 0.31263649463653564\n",
      "[Training Epoch 0] Batch 2117, Loss 0.29152199625968933\n",
      "[Training Epoch 0] Batch 2118, Loss 0.32691115140914917\n",
      "[Training Epoch 0] Batch 2119, Loss 0.3169216811656952\n",
      "[Training Epoch 0] Batch 2120, Loss 0.3385787308216095\n",
      "[Training Epoch 0] Batch 2121, Loss 0.3205803632736206\n",
      "[Training Epoch 0] Batch 2122, Loss 0.30069971084594727\n",
      "[Training Epoch 0] Batch 2123, Loss 0.3283987045288086\n",
      "[Training Epoch 0] Batch 2124, Loss 0.3034082055091858\n",
      "[Training Epoch 0] Batch 2125, Loss 0.3370637893676758\n",
      "[Training Epoch 0] Batch 2126, Loss 0.2893424928188324\n",
      "[Training Epoch 0] Batch 2127, Loss 0.34252023696899414\n",
      "[Training Epoch 0] Batch 2128, Loss 0.3018367886543274\n",
      "[Training Epoch 0] Batch 2129, Loss 0.3116114139556885\n",
      "[Training Epoch 0] Batch 2130, Loss 0.3150984048843384\n",
      "[Training Epoch 0] Batch 2131, Loss 0.29248303174972534\n",
      "[Training Epoch 0] Batch 2132, Loss 0.31898948550224304\n",
      "[Training Epoch 0] Batch 2133, Loss 0.28907835483551025\n",
      "[Training Epoch 0] Batch 2134, Loss 0.3345911502838135\n",
      "[Training Epoch 0] Batch 2135, Loss 0.32896190881729126\n",
      "[Training Epoch 0] Batch 2136, Loss 0.3341183066368103\n",
      "[Training Epoch 0] Batch 2137, Loss 0.3050047755241394\n",
      "[Training Epoch 0] Batch 2138, Loss 0.3062945604324341\n",
      "[Training Epoch 0] Batch 2139, Loss 0.3316066861152649\n",
      "[Training Epoch 0] Batch 2140, Loss 0.3455347418785095\n",
      "[Training Epoch 0] Batch 2141, Loss 0.3359249234199524\n",
      "[Training Epoch 0] Batch 2142, Loss 0.31663694977760315\n",
      "[Training Epoch 0] Batch 2143, Loss 0.3378843665122986\n",
      "[Training Epoch 0] Batch 2144, Loss 0.3091847896575928\n",
      "[Training Epoch 0] Batch 2145, Loss 0.3240087032318115\n",
      "[Training Epoch 0] Batch 2146, Loss 0.32573235034942627\n",
      "[Training Epoch 0] Batch 2147, Loss 0.30557677149772644\n",
      "[Training Epoch 0] Batch 2148, Loss 0.30933067202568054\n",
      "[Training Epoch 0] Batch 2149, Loss 0.35410237312316895\n",
      "[Training Epoch 0] Batch 2150, Loss 0.32509028911590576\n",
      "[Training Epoch 0] Batch 2151, Loss 0.33438917994499207\n",
      "[Training Epoch 0] Batch 2152, Loss 0.3228887617588043\n",
      "[Training Epoch 0] Batch 2153, Loss 0.30319473147392273\n",
      "[Training Epoch 0] Batch 2154, Loss 0.3068261742591858\n",
      "[Training Epoch 0] Batch 2155, Loss 0.30975842475891113\n",
      "[Training Epoch 0] Batch 2156, Loss 0.30090558528900146\n",
      "[Training Epoch 0] Batch 2157, Loss 0.33513063192367554\n",
      "[Training Epoch 0] Batch 2158, Loss 0.3317570686340332\n",
      "[Training Epoch 0] Batch 2159, Loss 0.33170193433761597\n",
      "[Training Epoch 0] Batch 2160, Loss 0.32996609807014465\n",
      "[Training Epoch 0] Batch 2161, Loss 0.33283618092536926\n",
      "[Training Epoch 0] Batch 2162, Loss 0.3195739984512329\n",
      "[Training Epoch 0] Batch 2163, Loss 0.343644380569458\n",
      "[Training Epoch 0] Batch 2164, Loss 0.2770206928253174\n",
      "[Training Epoch 0] Batch 2165, Loss 0.29743897914886475\n",
      "[Training Epoch 0] Batch 2166, Loss 0.31345874071121216\n",
      "[Training Epoch 0] Batch 2167, Loss 0.3347851634025574\n",
      "[Training Epoch 0] Batch 2168, Loss 0.30606263875961304\n",
      "[Training Epoch 0] Batch 2169, Loss 0.3095742464065552\n",
      "[Training Epoch 0] Batch 2170, Loss 0.3141934871673584\n",
      "[Training Epoch 0] Batch 2171, Loss 0.340760201215744\n",
      "[Training Epoch 0] Batch 2172, Loss 0.3111395835876465\n",
      "[Training Epoch 0] Batch 2173, Loss 0.30985796451568604\n",
      "[Training Epoch 0] Batch 2174, Loss 0.3336383104324341\n",
      "[Training Epoch 0] Batch 2175, Loss 0.30714088678359985\n",
      "[Training Epoch 0] Batch 2176, Loss 0.3228505253791809\n",
      "[Training Epoch 0] Batch 2177, Loss 0.31720227003097534\n",
      "[Training Epoch 0] Batch 2178, Loss 0.3381384313106537\n",
      "[Training Epoch 0] Batch 2179, Loss 0.30634674429893494\n",
      "[Training Epoch 0] Batch 2180, Loss 0.2981790006160736\n",
      "[Training Epoch 0] Batch 2181, Loss 0.29805788397789\n",
      "[Training Epoch 0] Batch 2182, Loss 0.3096520006656647\n",
      "[Training Epoch 0] Batch 2183, Loss 0.30368027091026306\n",
      "[Training Epoch 0] Batch 2184, Loss 0.3262096047401428\n",
      "[Training Epoch 0] Batch 2185, Loss 0.32029297947883606\n",
      "[Training Epoch 0] Batch 2186, Loss 0.3098579943180084\n",
      "[Training Epoch 0] Batch 2187, Loss 0.3080480098724365\n",
      "[Training Epoch 0] Batch 2188, Loss 0.31408512592315674\n",
      "[Training Epoch 0] Batch 2189, Loss 0.29097265005111694\n",
      "[Training Epoch 0] Batch 2190, Loss 0.3079417049884796\n",
      "[Training Epoch 0] Batch 2191, Loss 0.334989458322525\n",
      "[Training Epoch 0] Batch 2192, Loss 0.3189314305782318\n",
      "[Training Epoch 0] Batch 2193, Loss 0.29316362738609314\n",
      "[Training Epoch 0] Batch 2194, Loss 0.3284967541694641\n",
      "[Training Epoch 0] Batch 2195, Loss 0.314124196767807\n",
      "[Training Epoch 0] Batch 2196, Loss 0.3062642812728882\n",
      "[Training Epoch 0] Batch 2197, Loss 0.31047412753105164\n",
      "[Training Epoch 0] Batch 2198, Loss 0.30393022298812866\n",
      "[Training Epoch 0] Batch 2199, Loss 0.33050811290740967\n",
      "[Training Epoch 0] Batch 2200, Loss 0.3416612446308136\n",
      "[Training Epoch 0] Batch 2201, Loss 0.3090013563632965\n",
      "[Training Epoch 0] Batch 2202, Loss 0.30334681272506714\n",
      "[Training Epoch 0] Batch 2203, Loss 0.30664411187171936\n",
      "[Training Epoch 0] Batch 2204, Loss 0.3245531916618347\n",
      "[Training Epoch 0] Batch 2205, Loss 0.2886385917663574\n",
      "[Training Epoch 0] Batch 2206, Loss 0.33494919538497925\n",
      "[Training Epoch 0] Batch 2207, Loss 0.33489662408828735\n",
      "[Training Epoch 0] Batch 2208, Loss 0.2925017476081848\n",
      "[Training Epoch 0] Batch 2209, Loss 0.2959434390068054\n",
      "[Training Epoch 0] Batch 2210, Loss 0.32966992259025574\n",
      "[Training Epoch 0] Batch 2211, Loss 0.3058171570301056\n",
      "[Training Epoch 0] Batch 2212, Loss 0.31712043285369873\n",
      "[Training Epoch 0] Batch 2213, Loss 0.3440502882003784\n",
      "[Training Epoch 0] Batch 2214, Loss 0.29652005434036255\n",
      "[Training Epoch 0] Batch 2215, Loss 0.3034980595111847\n",
      "[Training Epoch 0] Batch 2216, Loss 0.31277766823768616\n",
      "[Training Epoch 0] Batch 2217, Loss 0.34444791078567505\n",
      "[Training Epoch 0] Batch 2218, Loss 0.32157963514328003\n",
      "[Training Epoch 0] Batch 2219, Loss 0.30708223581314087\n",
      "[Training Epoch 0] Batch 2220, Loss 0.3202601671218872\n",
      "[Training Epoch 0] Batch 2221, Loss 0.32979682087898254\n",
      "[Training Epoch 0] Batch 2222, Loss 0.29962384700775146\n",
      "[Training Epoch 0] Batch 2223, Loss 0.2975304126739502\n",
      "[Training Epoch 0] Batch 2224, Loss 0.3284269869327545\n",
      "[Training Epoch 0] Batch 2225, Loss 0.32044512033462524\n",
      "[Training Epoch 0] Batch 2226, Loss 0.29769647121429443\n",
      "[Training Epoch 0] Batch 2227, Loss 0.3261605501174927\n",
      "[Training Epoch 0] Batch 2228, Loss 0.30457937717437744\n",
      "[Training Epoch 0] Batch 2229, Loss 0.29573434591293335\n",
      "[Training Epoch 0] Batch 2230, Loss 0.29951849579811096\n",
      "[Training Epoch 0] Batch 2231, Loss 0.34449779987335205\n",
      "[Training Epoch 0] Batch 2232, Loss 0.2945459485054016\n",
      "[Training Epoch 0] Batch 2233, Loss 0.3537212908267975\n",
      "[Training Epoch 0] Batch 2234, Loss 0.3175198435783386\n",
      "[Training Epoch 0] Batch 2235, Loss 0.32960110902786255\n",
      "[Training Epoch 0] Batch 2236, Loss 0.32229992747306824\n",
      "[Training Epoch 0] Batch 2237, Loss 0.3142280876636505\n",
      "[Training Epoch 0] Batch 2238, Loss 0.32262617349624634\n",
      "[Training Epoch 0] Batch 2239, Loss 0.30049753189086914\n",
      "[Training Epoch 0] Batch 2240, Loss 0.32143259048461914\n",
      "[Training Epoch 0] Batch 2241, Loss 0.3465557396411896\n",
      "[Training Epoch 0] Batch 2242, Loss 0.3264003396034241\n",
      "[Training Epoch 0] Batch 2243, Loss 0.30598706007003784\n",
      "[Training Epoch 0] Batch 2244, Loss 0.32574892044067383\n",
      "[Training Epoch 0] Batch 2245, Loss 0.33179330825805664\n",
      "[Training Epoch 0] Batch 2246, Loss 0.318152517080307\n",
      "[Training Epoch 0] Batch 2247, Loss 0.320292592048645\n",
      "[Training Epoch 0] Batch 2248, Loss 0.32843494415283203\n",
      "[Training Epoch 0] Batch 2249, Loss 0.3258787989616394\n",
      "[Training Epoch 0] Batch 2250, Loss 0.32611146569252014\n",
      "[Training Epoch 0] Batch 2251, Loss 0.3253907561302185\n",
      "[Training Epoch 0] Batch 2252, Loss 0.3166300058364868\n",
      "[Training Epoch 0] Batch 2253, Loss 0.35153728723526\n",
      "[Training Epoch 0] Batch 2254, Loss 0.3223429024219513\n",
      "[Training Epoch 0] Batch 2255, Loss 0.31934303045272827\n",
      "[Training Epoch 0] Batch 2256, Loss 0.30099570751190186\n",
      "[Training Epoch 0] Batch 2257, Loss 0.31951504945755005\n",
      "[Training Epoch 0] Batch 2258, Loss 0.2974894642829895\n",
      "[Training Epoch 0] Batch 2259, Loss 0.3191041350364685\n",
      "[Training Epoch 0] Batch 2260, Loss 0.3200916647911072\n",
      "[Training Epoch 0] Batch 2261, Loss 0.32331639528274536\n",
      "[Training Epoch 0] Batch 2262, Loss 0.31252729892730713\n",
      "[Training Epoch 0] Batch 2263, Loss 0.31492239236831665\n",
      "[Training Epoch 0] Batch 2264, Loss 0.3373996317386627\n",
      "[Training Epoch 0] Batch 2265, Loss 0.33374494314193726\n",
      "[Training Epoch 0] Batch 2266, Loss 0.32651156187057495\n",
      "[Training Epoch 0] Batch 2267, Loss 0.3205026388168335\n",
      "[Training Epoch 0] Batch 2268, Loss 0.3036733865737915\n",
      "[Training Epoch 0] Batch 2269, Loss 0.3171383738517761\n",
      "[Training Epoch 0] Batch 2270, Loss 0.2840520143508911\n",
      "[Training Epoch 0] Batch 2271, Loss 0.31531816720962524\n",
      "[Training Epoch 0] Batch 2272, Loss 0.30259448289871216\n",
      "[Training Epoch 0] Batch 2273, Loss 0.3182264566421509\n",
      "[Training Epoch 0] Batch 2274, Loss 0.30446332693099976\n",
      "[Training Epoch 0] Batch 2275, Loss 0.2772170901298523\n",
      "[Training Epoch 0] Batch 2276, Loss 0.3287922143936157\n",
      "[Training Epoch 0] Batch 2277, Loss 0.3214189112186432\n",
      "[Training Epoch 0] Batch 2278, Loss 0.30890876054763794\n",
      "[Training Epoch 0] Batch 2279, Loss 0.3010459542274475\n",
      "[Training Epoch 0] Batch 2280, Loss 0.33472728729248047\n",
      "[Training Epoch 0] Batch 2281, Loss 0.31673645973205566\n",
      "[Training Epoch 0] Batch 2282, Loss 0.3056381344795227\n",
      "[Training Epoch 0] Batch 2283, Loss 0.32481932640075684\n",
      "[Training Epoch 0] Batch 2284, Loss 0.3273962736129761\n",
      "[Training Epoch 0] Batch 2285, Loss 0.31197381019592285\n",
      "[Training Epoch 0] Batch 2286, Loss 0.29926902055740356\n",
      "[Training Epoch 0] Batch 2287, Loss 0.357754111289978\n",
      "[Training Epoch 0] Batch 2288, Loss 0.32173120975494385\n",
      "[Training Epoch 0] Batch 2289, Loss 0.31456273794174194\n",
      "[Training Epoch 0] Batch 2290, Loss 0.318170428276062\n",
      "[Training Epoch 0] Batch 2291, Loss 0.3139367997646332\n",
      "[Training Epoch 0] Batch 2292, Loss 0.3126634657382965\n",
      "[Training Epoch 0] Batch 2293, Loss 0.3150375783443451\n",
      "[Training Epoch 0] Batch 2294, Loss 0.323311984539032\n",
      "[Training Epoch 0] Batch 2295, Loss 0.29640206694602966\n",
      "[Training Epoch 0] Batch 2296, Loss 0.3053539991378784\n",
      "[Training Epoch 0] Batch 2297, Loss 0.31853416562080383\n",
      "[Training Epoch 0] Batch 2298, Loss 0.32068338990211487\n",
      "[Training Epoch 0] Batch 2299, Loss 0.32126888632774353\n",
      "[Training Epoch 0] Batch 2300, Loss 0.31776928901672363\n",
      "[Training Epoch 0] Batch 2301, Loss 0.3224852979183197\n",
      "[Training Epoch 0] Batch 2302, Loss 0.3060898184776306\n",
      "[Training Epoch 0] Batch 2303, Loss 0.3096233010292053\n",
      "[Training Epoch 0] Batch 2304, Loss 0.3330630958080292\n",
      "[Training Epoch 0] Batch 2305, Loss 0.29439568519592285\n",
      "[Training Epoch 0] Batch 2306, Loss 0.33107972145080566\n",
      "[Training Epoch 0] Batch 2307, Loss 0.3085331916809082\n",
      "[Training Epoch 0] Batch 2308, Loss 0.3330680727958679\n",
      "[Training Epoch 0] Batch 2309, Loss 0.2796420156955719\n",
      "[Training Epoch 0] Batch 2310, Loss 0.3271804451942444\n",
      "[Training Epoch 0] Batch 2311, Loss 0.322065532207489\n",
      "[Training Epoch 0] Batch 2312, Loss 0.303680419921875\n",
      "[Training Epoch 0] Batch 2313, Loss 0.3183368742465973\n",
      "[Training Epoch 0] Batch 2314, Loss 0.30411070585250854\n",
      "[Training Epoch 0] Batch 2315, Loss 0.30923956632614136\n",
      "[Training Epoch 0] Batch 2316, Loss 0.3281099200248718\n",
      "[Training Epoch 0] Batch 2317, Loss 0.30850476026535034\n",
      "[Training Epoch 0] Batch 2318, Loss 0.3187964856624603\n",
      "[Training Epoch 0] Batch 2319, Loss 0.2785722613334656\n",
      "[Training Epoch 0] Batch 2320, Loss 0.3369671404361725\n",
      "[Training Epoch 0] Batch 2321, Loss 0.3104718327522278\n",
      "[Training Epoch 0] Batch 2322, Loss 0.3072999119758606\n",
      "[Training Epoch 0] Batch 2323, Loss 0.32096022367477417\n",
      "[Training Epoch 0] Batch 2324, Loss 0.32343655824661255\n",
      "[Training Epoch 0] Batch 2325, Loss 0.28554677963256836\n",
      "[Training Epoch 0] Batch 2326, Loss 0.2967948913574219\n",
      "[Training Epoch 0] Batch 2327, Loss 0.3364983797073364\n",
      "[Training Epoch 0] Batch 2328, Loss 0.3026905655860901\n",
      "[Training Epoch 0] Batch 2329, Loss 0.3481706976890564\n",
      "[Training Epoch 0] Batch 2330, Loss 0.31394487619400024\n",
      "[Training Epoch 0] Batch 2331, Loss 0.29730892181396484\n",
      "[Training Epoch 0] Batch 2332, Loss 0.2993985712528229\n",
      "[Training Epoch 0] Batch 2333, Loss 0.31824737787246704\n",
      "[Training Epoch 0] Batch 2334, Loss 0.3219795823097229\n",
      "[Training Epoch 0] Batch 2335, Loss 0.3291252553462982\n",
      "[Training Epoch 0] Batch 2336, Loss 0.2792220413684845\n",
      "[Training Epoch 0] Batch 2337, Loss 0.2888578474521637\n",
      "[Training Epoch 0] Batch 2338, Loss 0.31326985359191895\n",
      "[Training Epoch 0] Batch 2339, Loss 0.29628893733024597\n",
      "[Training Epoch 0] Batch 2340, Loss 0.34697550535202026\n",
      "[Training Epoch 0] Batch 2341, Loss 0.31075605750083923\n",
      "[Training Epoch 0] Batch 2342, Loss 0.31129148602485657\n",
      "[Training Epoch 0] Batch 2343, Loss 0.34225574135780334\n",
      "[Training Epoch 0] Batch 2344, Loss 0.2826140522956848\n",
      "[Training Epoch 0] Batch 2345, Loss 0.31595301628112793\n",
      "[Training Epoch 0] Batch 2346, Loss 0.3353414535522461\n",
      "[Training Epoch 0] Batch 2347, Loss 0.3417282700538635\n",
      "[Training Epoch 0] Batch 2348, Loss 0.31425970792770386\n",
      "[Training Epoch 0] Batch 2349, Loss 0.30796879529953003\n",
      "[Training Epoch 0] Batch 2350, Loss 0.30413010716438293\n",
      "[Training Epoch 0] Batch 2351, Loss 0.3292970061302185\n",
      "[Training Epoch 0] Batch 2352, Loss 0.32265132665634155\n",
      "[Training Epoch 0] Batch 2353, Loss 0.27415895462036133\n",
      "[Training Epoch 0] Batch 2354, Loss 0.32284843921661377\n",
      "[Training Epoch 0] Batch 2355, Loss 0.30208146572113037\n",
      "[Training Epoch 0] Batch 2356, Loss 0.30720847845077515\n",
      "[Training Epoch 0] Batch 2357, Loss 0.3249407112598419\n",
      "[Training Epoch 0] Batch 2358, Loss 0.29397720098495483\n",
      "[Training Epoch 0] Batch 2359, Loss 0.32421475648880005\n",
      "[Training Epoch 0] Batch 2360, Loss 0.32213443517684937\n",
      "[Training Epoch 0] Batch 2361, Loss 0.3076756000518799\n",
      "[Training Epoch 0] Batch 2362, Loss 0.3282807171344757\n",
      "[Training Epoch 0] Batch 2363, Loss 0.3509625792503357\n",
      "[Training Epoch 0] Batch 2364, Loss 0.3034172058105469\n",
      "[Training Epoch 0] Batch 2365, Loss 0.3147701025009155\n",
      "[Training Epoch 0] Batch 2366, Loss 0.30999332666397095\n",
      "[Training Epoch 0] Batch 2367, Loss 0.3250954747200012\n",
      "[Training Epoch 0] Batch 2368, Loss 0.28938028216362\n",
      "[Training Epoch 0] Batch 2369, Loss 0.305683434009552\n",
      "[Training Epoch 0] Batch 2370, Loss 0.3190962076187134\n",
      "[Training Epoch 0] Batch 2371, Loss 0.29773756861686707\n",
      "[Training Epoch 0] Batch 2372, Loss 0.2844119071960449\n",
      "[Training Epoch 0] Batch 2373, Loss 0.30990850925445557\n",
      "[Training Epoch 0] Batch 2374, Loss 0.3362785279750824\n",
      "[Training Epoch 0] Batch 2375, Loss 0.3042842149734497\n",
      "[Training Epoch 0] Batch 2376, Loss 0.32058024406433105\n",
      "[Training Epoch 0] Batch 2377, Loss 0.3157727122306824\n",
      "[Training Epoch 0] Batch 2378, Loss 0.3352365493774414\n",
      "[Training Epoch 0] Batch 2379, Loss 0.3148218095302582\n",
      "[Training Epoch 0] Batch 2380, Loss 0.29617494344711304\n",
      "[Training Epoch 0] Batch 2381, Loss 0.31348976492881775\n",
      "[Training Epoch 0] Batch 2382, Loss 0.31517043709754944\n",
      "[Training Epoch 0] Batch 2383, Loss 0.2949753403663635\n",
      "[Training Epoch 0] Batch 2384, Loss 0.33118224143981934\n",
      "[Training Epoch 0] Batch 2385, Loss 0.32099664211273193\n",
      "[Training Epoch 0] Batch 2386, Loss 0.3151099383831024\n",
      "[Training Epoch 0] Batch 2387, Loss 0.3135520815849304\n",
      "[Training Epoch 0] Batch 2388, Loss 0.30309486389160156\n",
      "[Training Epoch 0] Batch 2389, Loss 0.30529800057411194\n",
      "[Training Epoch 0] Batch 2390, Loss 0.32177066802978516\n",
      "[Training Epoch 0] Batch 2391, Loss 0.3184686601161957\n",
      "[Training Epoch 0] Batch 2392, Loss 0.33124497532844543\n",
      "[Training Epoch 0] Batch 2393, Loss 0.3156808614730835\n",
      "[Training Epoch 0] Batch 2394, Loss 0.2826420068740845\n",
      "[Training Epoch 0] Batch 2395, Loss 0.3007393479347229\n",
      "[Training Epoch 0] Batch 2396, Loss 0.31852930784225464\n",
      "[Training Epoch 0] Batch 2397, Loss 0.3264344334602356\n",
      "[Training Epoch 0] Batch 2398, Loss 0.32100367546081543\n",
      "[Training Epoch 0] Batch 2399, Loss 0.3104395568370819\n",
      "[Training Epoch 0] Batch 2400, Loss 0.30160635709762573\n",
      "[Training Epoch 0] Batch 2401, Loss 0.2986794114112854\n",
      "[Training Epoch 0] Batch 2402, Loss 0.3344046473503113\n",
      "[Training Epoch 0] Batch 2403, Loss 0.3045637011528015\n",
      "[Training Epoch 0] Batch 2404, Loss 0.34204787015914917\n",
      "[Training Epoch 0] Batch 2405, Loss 0.2859671413898468\n",
      "[Training Epoch 0] Batch 2406, Loss 0.3056167662143707\n",
      "[Training Epoch 0] Batch 2407, Loss 0.3000330626964569\n",
      "[Training Epoch 0] Batch 2408, Loss 0.33259546756744385\n",
      "[Training Epoch 0] Batch 2409, Loss 0.32565629482269287\n",
      "[Training Epoch 0] Batch 2410, Loss 0.35639601945877075\n",
      "[Training Epoch 0] Batch 2411, Loss 0.3270794749259949\n",
      "[Training Epoch 0] Batch 2412, Loss 0.3098522424697876\n",
      "[Training Epoch 0] Batch 2413, Loss 0.30508697032928467\n",
      "[Training Epoch 0] Batch 2414, Loss 0.35030582547187805\n",
      "[Training Epoch 0] Batch 2415, Loss 0.314456582069397\n",
      "[Training Epoch 0] Batch 2416, Loss 0.36152732372283936\n",
      "[Training Epoch 0] Batch 2417, Loss 0.3257244825363159\n",
      "[Training Epoch 0] Batch 2418, Loss 0.31973743438720703\n",
      "[Training Epoch 0] Batch 2419, Loss 0.27918779850006104\n",
      "[Training Epoch 0] Batch 2420, Loss 0.30814307928085327\n",
      "[Training Epoch 0] Batch 2421, Loss 0.32406798005104065\n",
      "[Training Epoch 0] Batch 2422, Loss 0.33209776878356934\n",
      "[Training Epoch 0] Batch 2423, Loss 0.31230542063713074\n",
      "[Training Epoch 0] Batch 2424, Loss 0.31089964509010315\n",
      "[Training Epoch 0] Batch 2425, Loss 0.3259493112564087\n",
      "[Training Epoch 0] Batch 2426, Loss 0.3086583614349365\n",
      "[Training Epoch 0] Batch 2427, Loss 0.33777785301208496\n",
      "[Training Epoch 0] Batch 2428, Loss 0.3097468614578247\n",
      "[Training Epoch 0] Batch 2429, Loss 0.3505350649356842\n",
      "[Training Epoch 0] Batch 2430, Loss 0.3256179094314575\n",
      "[Training Epoch 0] Batch 2431, Loss 0.3089240789413452\n",
      "[Training Epoch 0] Batch 2432, Loss 0.32330936193466187\n",
      "[Training Epoch 0] Batch 2433, Loss 0.3203454613685608\n",
      "[Training Epoch 0] Batch 2434, Loss 0.29078763723373413\n",
      "[Training Epoch 0] Batch 2435, Loss 0.32059234380722046\n",
      "[Training Epoch 0] Batch 2436, Loss 0.2889549136161804\n",
      "[Training Epoch 0] Batch 2437, Loss 0.28704434633255005\n",
      "[Training Epoch 0] Batch 2438, Loss 0.3194867670536041\n",
      "[Training Epoch 0] Batch 2439, Loss 0.30890530347824097\n",
      "[Training Epoch 0] Batch 2440, Loss 0.31716781854629517\n",
      "[Training Epoch 0] Batch 2441, Loss 0.300726056098938\n",
      "[Training Epoch 0] Batch 2442, Loss 0.3417714536190033\n",
      "[Training Epoch 0] Batch 2443, Loss 0.29106655716896057\n",
      "[Training Epoch 0] Batch 2444, Loss 0.3301845192909241\n",
      "[Training Epoch 0] Batch 2445, Loss 0.29415982961654663\n",
      "[Training Epoch 0] Batch 2446, Loss 0.3324885070323944\n",
      "[Training Epoch 0] Batch 2447, Loss 0.3031114637851715\n",
      "[Training Epoch 0] Batch 2448, Loss 0.29870182275772095\n",
      "[Training Epoch 0] Batch 2449, Loss 0.3466038703918457\n",
      "[Training Epoch 0] Batch 2450, Loss 0.318182110786438\n",
      "[Training Epoch 0] Batch 2451, Loss 0.312482088804245\n",
      "[Training Epoch 0] Batch 2452, Loss 0.29717764258384705\n",
      "[Training Epoch 0] Batch 2453, Loss 0.30575841665267944\n",
      "[Training Epoch 0] Batch 2454, Loss 0.346935510635376\n",
      "[Training Epoch 0] Batch 2455, Loss 0.32924139499664307\n",
      "[Training Epoch 0] Batch 2456, Loss 0.3283134996891022\n",
      "[Training Epoch 0] Batch 2457, Loss 0.2855856716632843\n",
      "[Training Epoch 0] Batch 2458, Loss 0.31905481219291687\n",
      "[Training Epoch 0] Batch 2459, Loss 0.33069923520088196\n",
      "[Training Epoch 0] Batch 2460, Loss 0.2911946773529053\n",
      "[Training Epoch 0] Batch 2461, Loss 0.3274051249027252\n",
      "[Training Epoch 0] Batch 2462, Loss 0.33151373267173767\n",
      "[Training Epoch 0] Batch 2463, Loss 0.2934225797653198\n",
      "[Training Epoch 0] Batch 2464, Loss 0.32380086183547974\n",
      "[Training Epoch 0] Batch 2465, Loss 0.3129737973213196\n",
      "[Training Epoch 0] Batch 2466, Loss 0.31863701343536377\n",
      "[Training Epoch 0] Batch 2467, Loss 0.323087215423584\n",
      "[Training Epoch 0] Batch 2468, Loss 0.29868143796920776\n",
      "[Training Epoch 0] Batch 2469, Loss 0.31022512912750244\n",
      "[Training Epoch 0] Batch 2470, Loss 0.2978125810623169\n",
      "[Training Epoch 0] Batch 2471, Loss 0.30860719084739685\n",
      "[Training Epoch 0] Batch 2472, Loss 0.32317596673965454\n",
      "[Training Epoch 0] Batch 2473, Loss 0.30086448788642883\n",
      "[Training Epoch 0] Batch 2474, Loss 0.3010066747665405\n",
      "[Training Epoch 0] Batch 2475, Loss 0.2939111590385437\n",
      "[Training Epoch 0] Batch 2476, Loss 0.28643733263015747\n",
      "[Training Epoch 0] Batch 2477, Loss 0.311107337474823\n",
      "[Training Epoch 0] Batch 2478, Loss 0.30018579959869385\n",
      "[Training Epoch 0] Batch 2479, Loss 0.3165059983730316\n",
      "[Training Epoch 0] Batch 2480, Loss 0.3016830384731293\n",
      "[Training Epoch 0] Batch 2481, Loss 0.33397376537323\n",
      "[Training Epoch 0] Batch 2482, Loss 0.3206802010536194\n",
      "[Training Epoch 0] Batch 2483, Loss 0.29139700531959534\n",
      "[Training Epoch 0] Batch 2484, Loss 0.30442315340042114\n",
      "[Training Epoch 0] Batch 2485, Loss 0.2960782051086426\n",
      "[Training Epoch 0] Batch 2486, Loss 0.3290725648403168\n",
      "[Training Epoch 0] Batch 2487, Loss 0.3180716335773468\n",
      "[Training Epoch 0] Batch 2488, Loss 0.34090301394462585\n",
      "[Training Epoch 0] Batch 2489, Loss 0.2829473316669464\n",
      "[Training Epoch 0] Batch 2490, Loss 0.297733336687088\n",
      "[Training Epoch 0] Batch 2491, Loss 0.3171845078468323\n",
      "[Training Epoch 0] Batch 2492, Loss 0.27745020389556885\n",
      "[Training Epoch 0] Batch 2493, Loss 0.303173303604126\n",
      "[Training Epoch 0] Batch 2494, Loss 0.316680908203125\n",
      "[Training Epoch 0] Batch 2495, Loss 0.3025586009025574\n",
      "[Training Epoch 0] Batch 2496, Loss 0.3008916974067688\n",
      "[Training Epoch 0] Batch 2497, Loss 0.3199141323566437\n",
      "[Training Epoch 0] Batch 2498, Loss 0.29242202639579773\n",
      "[Training Epoch 0] Batch 2499, Loss 0.2875164747238159\n",
      "[Training Epoch 0] Batch 2500, Loss 0.31898200511932373\n",
      "[Training Epoch 0] Batch 2501, Loss 0.32442641258239746\n",
      "[Training Epoch 0] Batch 2502, Loss 0.3122510015964508\n",
      "[Training Epoch 0] Batch 2503, Loss 0.3125985264778137\n",
      "[Training Epoch 0] Batch 2504, Loss 0.3158397078514099\n",
      "[Training Epoch 0] Batch 2505, Loss 0.3044916093349457\n",
      "[Training Epoch 0] Batch 2506, Loss 0.31500154733657837\n",
      "[Training Epoch 0] Batch 2507, Loss 0.3177350163459778\n",
      "[Training Epoch 0] Batch 2508, Loss 0.3308238983154297\n",
      "[Training Epoch 0] Batch 2509, Loss 0.2971424460411072\n",
      "[Training Epoch 0] Batch 2510, Loss 0.2923205494880676\n",
      "[Training Epoch 0] Batch 2511, Loss 0.29197338223457336\n",
      "[Training Epoch 0] Batch 2512, Loss 0.3094712495803833\n",
      "[Training Epoch 0] Batch 2513, Loss 0.3352978825569153\n",
      "[Training Epoch 0] Batch 2514, Loss 0.32358625531196594\n",
      "[Training Epoch 0] Batch 2515, Loss 0.2861412465572357\n",
      "[Training Epoch 0] Batch 2516, Loss 0.3306296765804291\n",
      "[Training Epoch 0] Batch 2517, Loss 0.32033395767211914\n",
      "[Training Epoch 0] Batch 2518, Loss 0.32447493076324463\n",
      "[Training Epoch 0] Batch 2519, Loss 0.3146744668483734\n",
      "[Training Epoch 0] Batch 2520, Loss 0.3082226514816284\n",
      "[Training Epoch 0] Batch 2521, Loss 0.3044666051864624\n",
      "[Training Epoch 0] Batch 2522, Loss 0.29897981882095337\n",
      "[Training Epoch 0] Batch 2523, Loss 0.33320945501327515\n",
      "[Training Epoch 0] Batch 2524, Loss 0.30326229333877563\n",
      "[Training Epoch 0] Batch 2525, Loss 0.29470473527908325\n",
      "[Training Epoch 0] Batch 2526, Loss 0.29942256212234497\n",
      "[Training Epoch 0] Batch 2527, Loss 0.30446112155914307\n",
      "[Training Epoch 0] Batch 2528, Loss 0.29057422280311584\n",
      "[Training Epoch 0] Batch 2529, Loss 0.32967352867126465\n",
      "[Training Epoch 0] Batch 2530, Loss 0.28529417514801025\n",
      "[Training Epoch 0] Batch 2531, Loss 0.3290459215641022\n",
      "[Training Epoch 0] Batch 2532, Loss 0.2877388596534729\n",
      "[Training Epoch 0] Batch 2533, Loss 0.3124018609523773\n",
      "[Training Epoch 0] Batch 2534, Loss 0.29285961389541626\n",
      "[Training Epoch 0] Batch 2535, Loss 0.31354212760925293\n",
      "[Training Epoch 0] Batch 2536, Loss 0.2822013795375824\n",
      "[Training Epoch 0] Batch 2537, Loss 0.30163586139678955\n",
      "[Training Epoch 0] Batch 2538, Loss 0.34349507093429565\n",
      "[Training Epoch 0] Batch 2539, Loss 0.3229652941226959\n",
      "[Training Epoch 0] Batch 2540, Loss 0.294341504573822\n",
      "[Training Epoch 0] Batch 2541, Loss 0.3024604320526123\n",
      "[Training Epoch 0] Batch 2542, Loss 0.3047026991844177\n",
      "[Training Epoch 0] Batch 2543, Loss 0.3213253915309906\n",
      "[Training Epoch 0] Batch 2544, Loss 0.2929805815219879\n",
      "[Training Epoch 0] Batch 2545, Loss 0.32855188846588135\n",
      "[Training Epoch 0] Batch 2546, Loss 0.32140421867370605\n",
      "[Training Epoch 0] Batch 2547, Loss 0.31039732694625854\n",
      "[Training Epoch 0] Batch 2548, Loss 0.32228195667266846\n",
      "[Training Epoch 0] Batch 2549, Loss 0.3253590762615204\n",
      "[Training Epoch 0] Batch 2550, Loss 0.35039544105529785\n",
      "[Training Epoch 0] Batch 2551, Loss 0.29890942573547363\n",
      "[Training Epoch 0] Batch 2552, Loss 0.2760743796825409\n",
      "[Training Epoch 0] Batch 2553, Loss 0.3116722106933594\n",
      "[Training Epoch 0] Batch 2554, Loss 0.3292117416858673\n",
      "[Training Epoch 0] Batch 2555, Loss 0.31090694665908813\n",
      "[Training Epoch 0] Batch 2556, Loss 0.3186805546283722\n",
      "[Training Epoch 0] Batch 2557, Loss 0.29766467213630676\n",
      "[Training Epoch 0] Batch 2558, Loss 0.31160032749176025\n",
      "[Training Epoch 0] Batch 2559, Loss 0.3049675226211548\n",
      "[Training Epoch 0] Batch 2560, Loss 0.3155898451805115\n",
      "[Training Epoch 0] Batch 2561, Loss 0.30031681060791016\n",
      "[Training Epoch 0] Batch 2562, Loss 0.3100391626358032\n",
      "[Training Epoch 0] Batch 2563, Loss 0.29807382822036743\n",
      "[Training Epoch 0] Batch 2564, Loss 0.3204203248023987\n",
      "[Training Epoch 0] Batch 2565, Loss 0.3174716830253601\n",
      "[Training Epoch 0] Batch 2566, Loss 0.33285707235336304\n",
      "[Training Epoch 0] Batch 2567, Loss 0.29347747564315796\n",
      "[Training Epoch 0] Batch 2568, Loss 0.3009971082210541\n",
      "[Training Epoch 0] Batch 2569, Loss 0.3334633409976959\n",
      "[Training Epoch 0] Batch 2570, Loss 0.2986767292022705\n",
      "[Training Epoch 0] Batch 2571, Loss 0.3351212441921234\n",
      "[Training Epoch 0] Batch 2572, Loss 0.31494542956352234\n",
      "[Training Epoch 0] Batch 2573, Loss 0.30597802996635437\n",
      "[Training Epoch 0] Batch 2574, Loss 0.28217220306396484\n",
      "[Training Epoch 0] Batch 2575, Loss 0.3169248104095459\n",
      "[Training Epoch 0] Batch 2576, Loss 0.3134661316871643\n",
      "[Training Epoch 0] Batch 2577, Loss 0.3398987650871277\n",
      "[Training Epoch 0] Batch 2578, Loss 0.3023500442504883\n",
      "[Training Epoch 0] Batch 2579, Loss 0.30051136016845703\n",
      "[Training Epoch 0] Batch 2580, Loss 0.30079829692840576\n",
      "[Training Epoch 0] Batch 2581, Loss 0.3072884678840637\n",
      "[Training Epoch 0] Batch 2582, Loss 0.31662940979003906\n",
      "[Training Epoch 0] Batch 2583, Loss 0.33328545093536377\n",
      "[Training Epoch 0] Batch 2584, Loss 0.3215281367301941\n",
      "[Training Epoch 0] Batch 2585, Loss 0.32291221618652344\n",
      "[Training Epoch 0] Batch 2586, Loss 0.2944399118423462\n",
      "[Training Epoch 0] Batch 2587, Loss 0.31259435415267944\n",
      "[Training Epoch 0] Batch 2588, Loss 0.3217164874076843\n",
      "[Training Epoch 0] Batch 2589, Loss 0.3294501006603241\n",
      "[Training Epoch 0] Batch 2590, Loss 0.3449360430240631\n",
      "[Training Epoch 0] Batch 2591, Loss 0.3097383677959442\n",
      "[Training Epoch 0] Batch 2592, Loss 0.3171961009502411\n",
      "[Training Epoch 0] Batch 2593, Loss 0.2939492464065552\n",
      "[Training Epoch 0] Batch 2594, Loss 0.2996111512184143\n",
      "[Training Epoch 0] Batch 2595, Loss 0.2980189323425293\n",
      "[Training Epoch 0] Batch 2596, Loss 0.3037763833999634\n",
      "[Training Epoch 0] Batch 2597, Loss 0.3046759366989136\n",
      "[Training Epoch 0] Batch 2598, Loss 0.30680322647094727\n",
      "[Training Epoch 0] Batch 2599, Loss 0.3268694579601288\n",
      "[Training Epoch 0] Batch 2600, Loss 0.31393545866012573\n",
      "[Training Epoch 0] Batch 2601, Loss 0.31307587027549744\n",
      "[Training Epoch 0] Batch 2602, Loss 0.28725695610046387\n",
      "[Training Epoch 0] Batch 2603, Loss 0.3013225495815277\n",
      "[Training Epoch 0] Batch 2604, Loss 0.3234493136405945\n",
      "[Training Epoch 0] Batch 2605, Loss 0.32512497901916504\n",
      "[Training Epoch 0] Batch 2606, Loss 0.33659666776657104\n",
      "[Training Epoch 0] Batch 2607, Loss 0.31685546040534973\n",
      "[Training Epoch 0] Batch 2608, Loss 0.30803799629211426\n",
      "[Training Epoch 0] Batch 2609, Loss 0.3259449005126953\n",
      "[Training Epoch 0] Batch 2610, Loss 0.3198188543319702\n",
      "[Training Epoch 0] Batch 2611, Loss 0.2999182939529419\n",
      "[Training Epoch 0] Batch 2612, Loss 0.27661433815956116\n",
      "[Training Epoch 0] Batch 2613, Loss 0.318412721157074\n",
      "[Training Epoch 0] Batch 2614, Loss 0.31638914346694946\n",
      "[Training Epoch 0] Batch 2615, Loss 0.31817203760147095\n",
      "[Training Epoch 0] Batch 2616, Loss 0.2877652049064636\n",
      "[Training Epoch 0] Batch 2617, Loss 0.31475555896759033\n",
      "[Training Epoch 0] Batch 2618, Loss 0.3139280080795288\n",
      "[Training Epoch 0] Batch 2619, Loss 0.3058760166168213\n",
      "[Training Epoch 0] Batch 2620, Loss 0.3185780346393585\n",
      "[Training Epoch 0] Batch 2621, Loss 0.33942049741744995\n",
      "[Training Epoch 0] Batch 2622, Loss 0.2790642976760864\n",
      "[Training Epoch 0] Batch 2623, Loss 0.3100523352622986\n",
      "[Training Epoch 0] Batch 2624, Loss 0.3343621492385864\n",
      "[Training Epoch 0] Batch 2625, Loss 0.32522547245025635\n",
      "[Training Epoch 0] Batch 2626, Loss 0.3109128475189209\n",
      "[Training Epoch 0] Batch 2627, Loss 0.29526299238204956\n",
      "[Training Epoch 0] Batch 2628, Loss 0.2870640754699707\n",
      "[Training Epoch 0] Batch 2629, Loss 0.30756375193595886\n",
      "[Training Epoch 0] Batch 2630, Loss 0.30257219076156616\n",
      "[Training Epoch 0] Batch 2631, Loss 0.3306408226490021\n",
      "[Training Epoch 0] Batch 2632, Loss 0.29197806119918823\n",
      "[Training Epoch 0] Batch 2633, Loss 0.3183424472808838\n",
      "[Training Epoch 0] Batch 2634, Loss 0.2909151315689087\n",
      "[Training Epoch 0] Batch 2635, Loss 0.3324255347251892\n",
      "[Training Epoch 0] Batch 2636, Loss 0.33518606424331665\n",
      "[Training Epoch 0] Batch 2637, Loss 0.31401753425598145\n",
      "[Training Epoch 0] Batch 2638, Loss 0.29038214683532715\n",
      "[Training Epoch 0] Batch 2639, Loss 0.31510183215141296\n",
      "[Training Epoch 0] Batch 2640, Loss 0.3297748565673828\n",
      "[Training Epoch 0] Batch 2641, Loss 0.3132137060165405\n",
      "[Training Epoch 0] Batch 2642, Loss 0.30309391021728516\n",
      "[Training Epoch 0] Batch 2643, Loss 0.3057957887649536\n",
      "[Training Epoch 0] Batch 2644, Loss 0.3164496421813965\n",
      "[Training Epoch 0] Batch 2645, Loss 0.3080228269100189\n",
      "[Training Epoch 0] Batch 2646, Loss 0.32972556352615356\n",
      "[Training Epoch 0] Batch 2647, Loss 0.3162141740322113\n",
      "[Training Epoch 0] Batch 2648, Loss 0.2847602963447571\n",
      "[Training Epoch 0] Batch 2649, Loss 0.3311138451099396\n",
      "[Training Epoch 0] Batch 2650, Loss 0.3166325092315674\n",
      "[Training Epoch 0] Batch 2651, Loss 0.3104802072048187\n",
      "[Training Epoch 0] Batch 2652, Loss 0.33274680376052856\n",
      "[Training Epoch 0] Batch 2653, Loss 0.32100486755371094\n",
      "[Training Epoch 0] Batch 2654, Loss 0.30005306005477905\n",
      "[Training Epoch 0] Batch 2655, Loss 0.3239290714263916\n",
      "[Training Epoch 0] Batch 2656, Loss 0.32917916774749756\n",
      "[Training Epoch 0] Batch 2657, Loss 0.3315027952194214\n",
      "[Training Epoch 0] Batch 2658, Loss 0.2868531346321106\n",
      "[Training Epoch 0] Batch 2659, Loss 0.3040497899055481\n",
      "[Training Epoch 0] Batch 2660, Loss 0.28928500413894653\n",
      "[Training Epoch 0] Batch 2661, Loss 0.30069059133529663\n",
      "[Training Epoch 0] Batch 2662, Loss 0.30896803736686707\n",
      "[Training Epoch 0] Batch 2663, Loss 0.34020447731018066\n",
      "[Training Epoch 0] Batch 2664, Loss 0.27943968772888184\n",
      "[Training Epoch 0] Batch 2665, Loss 0.3024575710296631\n",
      "[Training Epoch 0] Batch 2666, Loss 0.31011250615119934\n",
      "[Training Epoch 0] Batch 2667, Loss 0.29662594199180603\n",
      "[Training Epoch 0] Batch 2668, Loss 0.31826847791671753\n",
      "[Training Epoch 0] Batch 2669, Loss 0.3126819133758545\n",
      "[Training Epoch 0] Batch 2670, Loss 0.3302682340145111\n",
      "[Training Epoch 0] Batch 2671, Loss 0.32801440358161926\n",
      "[Training Epoch 0] Batch 2672, Loss 0.2925356924533844\n",
      "[Training Epoch 0] Batch 2673, Loss 0.30151623487472534\n",
      "[Training Epoch 0] Batch 2674, Loss 0.32554733753204346\n",
      "[Training Epoch 0] Batch 2675, Loss 0.2996913194656372\n",
      "[Training Epoch 0] Batch 2676, Loss 0.3025504946708679\n",
      "[Training Epoch 0] Batch 2677, Loss 0.33053621649742126\n",
      "[Training Epoch 0] Batch 2678, Loss 0.2972530722618103\n",
      "[Training Epoch 0] Batch 2679, Loss 0.307259202003479\n",
      "[Training Epoch 0] Batch 2680, Loss 0.30814021825790405\n",
      "[Training Epoch 0] Batch 2681, Loss 0.3106602430343628\n",
      "[Training Epoch 0] Batch 2682, Loss 0.3060011565685272\n",
      "[Training Epoch 0] Batch 2683, Loss 0.3208838701248169\n",
      "[Training Epoch 0] Batch 2684, Loss 0.30272752046585083\n",
      "[Training Epoch 0] Batch 2685, Loss 0.27299171686172485\n",
      "[Training Epoch 0] Batch 2686, Loss 0.3041284680366516\n",
      "[Training Epoch 0] Batch 2687, Loss 0.33281344175338745\n",
      "[Training Epoch 0] Batch 2688, Loss 0.3019203245639801\n",
      "[Training Epoch 0] Batch 2689, Loss 0.305477499961853\n",
      "[Training Epoch 0] Batch 2690, Loss 0.29875296354293823\n",
      "[Training Epoch 0] Batch 2691, Loss 0.33522501587867737\n",
      "[Training Epoch 0] Batch 2692, Loss 0.2957499027252197\n",
      "[Training Epoch 0] Batch 2693, Loss 0.30670684576034546\n",
      "[Training Epoch 0] Batch 2694, Loss 0.3247973322868347\n",
      "[Training Epoch 0] Batch 2695, Loss 0.33298277854919434\n",
      "[Training Epoch 0] Batch 2696, Loss 0.3275699019432068\n",
      "[Training Epoch 0] Batch 2697, Loss 0.29544928669929504\n",
      "[Training Epoch 0] Batch 2698, Loss 0.3270828425884247\n",
      "[Training Epoch 0] Batch 2699, Loss 0.3257463276386261\n",
      "[Training Epoch 0] Batch 2700, Loss 0.28785812854766846\n",
      "[Training Epoch 0] Batch 2701, Loss 0.3186360001564026\n",
      "[Training Epoch 0] Batch 2702, Loss 0.31041646003723145\n",
      "[Training Epoch 0] Batch 2703, Loss 0.32795286178588867\n",
      "[Training Epoch 0] Batch 2704, Loss 0.30415859818458557\n",
      "[Training Epoch 0] Batch 2705, Loss 0.30721187591552734\n",
      "[Training Epoch 0] Batch 2706, Loss 0.32189619541168213\n",
      "[Training Epoch 0] Batch 2707, Loss 0.28535184264183044\n",
      "[Training Epoch 0] Batch 2708, Loss 0.3130178451538086\n",
      "[Training Epoch 0] Batch 2709, Loss 0.2922137975692749\n",
      "[Training Epoch 0] Batch 2710, Loss 0.31974443793296814\n",
      "[Training Epoch 0] Batch 2711, Loss 0.2850681245326996\n",
      "[Training Epoch 0] Batch 2712, Loss 0.33514660596847534\n",
      "[Training Epoch 0] Batch 2713, Loss 0.31456583738327026\n",
      "[Training Epoch 0] Batch 2714, Loss 0.3289589285850525\n",
      "[Training Epoch 0] Batch 2715, Loss 0.3031563460826874\n",
      "[Training Epoch 0] Batch 2716, Loss 0.3151330053806305\n",
      "[Training Epoch 0] Batch 2717, Loss 0.3262498676776886\n",
      "[Training Epoch 0] Batch 2718, Loss 0.2916621267795563\n",
      "[Training Epoch 0] Batch 2719, Loss 0.29711973667144775\n",
      "[Training Epoch 0] Batch 2720, Loss 0.2812245488166809\n",
      "[Training Epoch 0] Batch 2721, Loss 0.3197648823261261\n",
      "[Training Epoch 0] Batch 2722, Loss 0.3104666471481323\n",
      "[Training Epoch 0] Batch 2723, Loss 0.3306095004081726\n",
      "[Training Epoch 0] Batch 2724, Loss 0.31893089413642883\n",
      "[Training Epoch 0] Batch 2725, Loss 0.2807660698890686\n",
      "[Training Epoch 0] Batch 2726, Loss 0.30794405937194824\n",
      "[Training Epoch 0] Batch 2727, Loss 0.29171955585479736\n",
      "[Training Epoch 0] Batch 2728, Loss 0.325936883687973\n",
      "[Training Epoch 0] Batch 2729, Loss 0.3235778212547302\n",
      "[Training Epoch 0] Batch 2730, Loss 0.33698832988739014\n",
      "[Training Epoch 0] Batch 2731, Loss 0.26506927609443665\n",
      "[Training Epoch 0] Batch 2732, Loss 0.32120662927627563\n",
      "[Training Epoch 0] Batch 2733, Loss 0.30943790078163147\n",
      "[Training Epoch 0] Batch 2734, Loss 0.3282579779624939\n",
      "[Training Epoch 0] Batch 2735, Loss 0.30397191643714905\n",
      "[Training Epoch 0] Batch 2736, Loss 0.3180984556674957\n",
      "[Training Epoch 0] Batch 2737, Loss 0.28460216522216797\n",
      "[Training Epoch 0] Batch 2738, Loss 0.33299243450164795\n",
      "[Training Epoch 0] Batch 2739, Loss 0.31910496950149536\n",
      "[Training Epoch 0] Batch 2740, Loss 0.3173581063747406\n",
      "[Training Epoch 0] Batch 2741, Loss 0.31630855798721313\n",
      "[Training Epoch 0] Batch 2742, Loss 0.31552788615226746\n",
      "[Training Epoch 0] Batch 2743, Loss 0.28913313150405884\n",
      "[Training Epoch 0] Batch 2744, Loss 0.3098903298377991\n",
      "[Training Epoch 0] Batch 2745, Loss 0.3228173851966858\n",
      "[Training Epoch 0] Batch 2746, Loss 0.30995556712150574\n",
      "[Training Epoch 0] Batch 2747, Loss 0.30353760719299316\n",
      "[Training Epoch 0] Batch 2748, Loss 0.2957714796066284\n",
      "[Training Epoch 0] Batch 2749, Loss 0.307344913482666\n",
      "[Training Epoch 0] Batch 2750, Loss 0.3235737085342407\n",
      "[Training Epoch 0] Batch 2751, Loss 0.3226528763771057\n",
      "[Training Epoch 0] Batch 2752, Loss 0.3074871301651001\n",
      "[Training Epoch 0] Batch 2753, Loss 0.29705387353897095\n",
      "[Training Epoch 0] Batch 2754, Loss 0.2986939251422882\n",
      "[Training Epoch 0] Batch 2755, Loss 0.3391869068145752\n",
      "[Training Epoch 0] Batch 2756, Loss 0.31975075602531433\n",
      "[Training Epoch 0] Batch 2757, Loss 0.3009074926376343\n",
      "[Training Epoch 0] Batch 2758, Loss 0.29334354400634766\n",
      "[Training Epoch 0] Batch 2759, Loss 0.3127994239330292\n",
      "[Training Epoch 0] Batch 2760, Loss 0.32157209515571594\n",
      "[Training Epoch 0] Batch 2761, Loss 0.28967908024787903\n",
      "[Training Epoch 0] Batch 2762, Loss 0.29514840245246887\n",
      "[Training Epoch 0] Batch 2763, Loss 0.3071736693382263\n",
      "[Training Epoch 0] Batch 2764, Loss 0.2911390960216522\n",
      "[Training Epoch 0] Batch 2765, Loss 0.2940511107444763\n",
      "[Training Epoch 0] Batch 2766, Loss 0.30582553148269653\n",
      "[Training Epoch 0] Batch 2767, Loss 0.3060911297798157\n",
      "[Training Epoch 0] Batch 2768, Loss 0.29643356800079346\n",
      "[Training Epoch 0] Batch 2769, Loss 0.32917386293411255\n",
      "[Training Epoch 0] Batch 2770, Loss 0.3404068946838379\n",
      "[Training Epoch 0] Batch 2771, Loss 0.33074119687080383\n",
      "[Training Epoch 0] Batch 2772, Loss 0.32954755425453186\n",
      "[Training Epoch 0] Batch 2773, Loss 0.2649950683116913\n",
      "[Training Epoch 0] Batch 2774, Loss 0.31147539615631104\n",
      "[Training Epoch 0] Batch 2775, Loss 0.30935001373291016\n",
      "[Training Epoch 0] Batch 2776, Loss 0.3270300030708313\n",
      "[Training Epoch 0] Batch 2777, Loss 0.2867300510406494\n",
      "[Training Epoch 0] Batch 2778, Loss 0.33097976446151733\n",
      "[Training Epoch 0] Batch 2779, Loss 0.309745192527771\n",
      "[Training Epoch 0] Batch 2780, Loss 0.2792554497718811\n",
      "[Training Epoch 0] Batch 2781, Loss 0.29411277174949646\n",
      "[Training Epoch 0] Batch 2782, Loss 0.3041200041770935\n",
      "[Training Epoch 0] Batch 2783, Loss 0.29498985409736633\n",
      "[Training Epoch 0] Batch 2784, Loss 0.3139161169528961\n",
      "[Training Epoch 0] Batch 2785, Loss 0.31499940156936646\n",
      "[Training Epoch 0] Batch 2786, Loss 0.30041244626045227\n",
      "[Training Epoch 0] Batch 2787, Loss 0.3022332787513733\n",
      "[Training Epoch 0] Batch 2788, Loss 0.3301495909690857\n",
      "[Training Epoch 0] Batch 2789, Loss 0.3096909523010254\n",
      "[Training Epoch 0] Batch 2790, Loss 0.31991830468177795\n",
      "[Training Epoch 0] Batch 2791, Loss 0.3115975856781006\n",
      "[Training Epoch 0] Batch 2792, Loss 0.33557167649269104\n",
      "[Training Epoch 0] Batch 2793, Loss 0.3117977976799011\n",
      "[Training Epoch 0] Batch 2794, Loss 0.3127713203430176\n",
      "[Training Epoch 0] Batch 2795, Loss 0.31360235810279846\n",
      "[Training Epoch 0] Batch 2796, Loss 0.2933788597583771\n",
      "[Training Epoch 0] Batch 2797, Loss 0.2972983419895172\n",
      "[Training Epoch 0] Batch 2798, Loss 0.31556832790374756\n",
      "[Training Epoch 0] Batch 2799, Loss 0.29690712690353394\n",
      "[Training Epoch 0] Batch 2800, Loss 0.3210229277610779\n",
      "[Training Epoch 0] Batch 2801, Loss 0.30347537994384766\n",
      "[Training Epoch 0] Batch 2802, Loss 0.2882382273674011\n",
      "[Training Epoch 0] Batch 2803, Loss 0.3257609009742737\n",
      "[Training Epoch 0] Batch 2804, Loss 0.29191872477531433\n",
      "[Training Epoch 0] Batch 2805, Loss 0.31198713183403015\n",
      "[Training Epoch 0] Batch 2806, Loss 0.3096741735935211\n",
      "[Training Epoch 0] Batch 2807, Loss 0.32071948051452637\n",
      "[Training Epoch 0] Batch 2808, Loss 0.2969692349433899\n",
      "[Training Epoch 0] Batch 2809, Loss 0.2990981340408325\n",
      "[Training Epoch 0] Batch 2810, Loss 0.30448082089424133\n",
      "[Training Epoch 0] Batch 2811, Loss 0.3010931611061096\n",
      "[Training Epoch 0] Batch 2812, Loss 0.301456093788147\n",
      "[Training Epoch 0] Batch 2813, Loss 0.31728172302246094\n",
      "[Training Epoch 0] Batch 2814, Loss 0.28727900981903076\n",
      "[Training Epoch 0] Batch 2815, Loss 0.33017536997795105\n",
      "[Training Epoch 0] Batch 2816, Loss 0.30547288060188293\n",
      "[Training Epoch 0] Batch 2817, Loss 0.3132520318031311\n",
      "[Training Epoch 0] Batch 2818, Loss 0.31901681423187256\n",
      "[Training Epoch 0] Batch 2819, Loss 0.3051723837852478\n",
      "[Training Epoch 0] Batch 2820, Loss 0.31942468881607056\n",
      "[Training Epoch 0] Batch 2821, Loss 0.3085809648036957\n",
      "[Training Epoch 0] Batch 2822, Loss 0.29789602756500244\n",
      "[Training Epoch 0] Batch 2823, Loss 0.33111336827278137\n",
      "[Training Epoch 0] Batch 2824, Loss 0.33534640073776245\n",
      "[Training Epoch 0] Batch 2825, Loss 0.3275982439517975\n",
      "[Training Epoch 0] Batch 2826, Loss 0.3010663390159607\n",
      "[Training Epoch 0] Batch 2827, Loss 0.3302239179611206\n",
      "[Training Epoch 0] Batch 2828, Loss 0.2923924922943115\n",
      "[Training Epoch 0] Batch 2829, Loss 0.2915482819080353\n",
      "[Training Epoch 0] Batch 2830, Loss 0.34744444489479065\n",
      "[Training Epoch 0] Batch 2831, Loss 0.3210887312889099\n",
      "[Training Epoch 0] Batch 2832, Loss 0.2936674654483795\n",
      "[Training Epoch 0] Batch 2833, Loss 0.30982428789138794\n",
      "[Training Epoch 0] Batch 2834, Loss 0.28972458839416504\n",
      "[Training Epoch 0] Batch 2835, Loss 0.30398160219192505\n",
      "[Training Epoch 0] Batch 2836, Loss 0.30653131008148193\n",
      "[Training Epoch 0] Batch 2837, Loss 0.3191533088684082\n",
      "[Training Epoch 0] Batch 2838, Loss 0.28596973419189453\n",
      "[Training Epoch 0] Batch 2839, Loss 0.2938256561756134\n",
      "[Training Epoch 0] Batch 2840, Loss 0.31389179825782776\n",
      "[Training Epoch 0] Batch 2841, Loss 0.32830873131752014\n",
      "[Training Epoch 0] Batch 2842, Loss 0.32487303018569946\n",
      "[Training Epoch 0] Batch 2843, Loss 0.2844487428665161\n",
      "[Training Epoch 0] Batch 2844, Loss 0.3266757130622864\n",
      "[Training Epoch 0] Batch 2845, Loss 0.29652079939842224\n",
      "[Training Epoch 0] Batch 2846, Loss 0.3393511176109314\n",
      "[Training Epoch 0] Batch 2847, Loss 0.31005510687828064\n",
      "[Training Epoch 0] Batch 2848, Loss 0.30375295877456665\n",
      "[Training Epoch 0] Batch 2849, Loss 0.2967652678489685\n",
      "[Training Epoch 0] Batch 2850, Loss 0.2817961275577545\n",
      "[Training Epoch 0] Batch 2851, Loss 0.30435919761657715\n",
      "[Training Epoch 0] Batch 2852, Loss 0.2943737506866455\n",
      "[Training Epoch 0] Batch 2853, Loss 0.3022288978099823\n",
      "[Training Epoch 0] Batch 2854, Loss 0.2968764305114746\n",
      "[Training Epoch 0] Batch 2855, Loss 0.30593764781951904\n",
      "[Training Epoch 0] Batch 2856, Loss 0.305969774723053\n",
      "[Training Epoch 0] Batch 2857, Loss 0.3067014217376709\n",
      "[Training Epoch 0] Batch 2858, Loss 0.31235963106155396\n",
      "[Training Epoch 0] Batch 2859, Loss 0.3550878167152405\n",
      "[Training Epoch 0] Batch 2860, Loss 0.3287406265735626\n",
      "[Training Epoch 0] Batch 2861, Loss 0.3102971315383911\n",
      "[Training Epoch 0] Batch 2862, Loss 0.3064722418785095\n",
      "[Training Epoch 0] Batch 2863, Loss 0.31146562099456787\n",
      "[Training Epoch 0] Batch 2864, Loss 0.2949468195438385\n",
      "[Training Epoch 0] Batch 2865, Loss 0.31450825929641724\n",
      "[Training Epoch 0] Batch 2866, Loss 0.3106343448162079\n",
      "[Training Epoch 0] Batch 2867, Loss 0.33159056305885315\n",
      "[Training Epoch 0] Batch 2868, Loss 0.30170783400535583\n",
      "[Training Epoch 0] Batch 2869, Loss 0.2926860451698303\n",
      "[Training Epoch 0] Batch 2870, Loss 0.2908928096294403\n",
      "[Training Epoch 0] Batch 2871, Loss 0.3002352714538574\n",
      "[Training Epoch 0] Batch 2872, Loss 0.31996551156044006\n",
      "[Training Epoch 0] Batch 2873, Loss 0.3265863358974457\n",
      "[Training Epoch 0] Batch 2874, Loss 0.29181092977523804\n",
      "[Training Epoch 0] Batch 2875, Loss 0.30739110708236694\n",
      "[Training Epoch 0] Batch 2876, Loss 0.2911178767681122\n",
      "[Training Epoch 0] Batch 2877, Loss 0.2988072633743286\n",
      "[Training Epoch 0] Batch 2878, Loss 0.31139928102493286\n",
      "[Training Epoch 0] Batch 2879, Loss 0.3243020474910736\n",
      "[Training Epoch 0] Batch 2880, Loss 0.29823702573776245\n",
      "[Training Epoch 0] Batch 2881, Loss 0.2935444116592407\n",
      "[Training Epoch 0] Batch 2882, Loss 0.3147183656692505\n",
      "[Training Epoch 0] Batch 2883, Loss 0.3072653114795685\n",
      "[Training Epoch 0] Batch 2884, Loss 0.314209520816803\n",
      "[Training Epoch 0] Batch 2885, Loss 0.30546388030052185\n",
      "[Training Epoch 0] Batch 2886, Loss 0.3323010802268982\n",
      "[Training Epoch 0] Batch 2887, Loss 0.30580994486808777\n",
      "[Training Epoch 0] Batch 2888, Loss 0.2975304126739502\n",
      "[Training Epoch 0] Batch 2889, Loss 0.3016953766345978\n",
      "[Training Epoch 0] Batch 2890, Loss 0.29781466722488403\n",
      "[Training Epoch 0] Batch 2891, Loss 0.3331586420536041\n",
      "[Training Epoch 0] Batch 2892, Loss 0.3293445110321045\n",
      "[Training Epoch 0] Batch 2893, Loss 0.29002293944358826\n",
      "[Training Epoch 0] Batch 2894, Loss 0.3023073077201843\n",
      "[Training Epoch 0] Batch 2895, Loss 0.29952001571655273\n",
      "[Training Epoch 0] Batch 2896, Loss 0.30827921628952026\n",
      "[Training Epoch 0] Batch 2897, Loss 0.31484276056289673\n",
      "[Training Epoch 0] Batch 2898, Loss 0.29095256328582764\n",
      "[Training Epoch 0] Batch 2899, Loss 0.3518944978713989\n",
      "[Training Epoch 0] Batch 2900, Loss 0.3105645179748535\n",
      "[Training Epoch 0] Batch 2901, Loss 0.3022245466709137\n",
      "[Training Epoch 0] Batch 2902, Loss 0.3125818073749542\n",
      "[Training Epoch 0] Batch 2903, Loss 0.29846858978271484\n",
      "[Training Epoch 0] Batch 2904, Loss 0.3139806389808655\n",
      "[Training Epoch 0] Batch 2905, Loss 0.29866111278533936\n",
      "[Training Epoch 0] Batch 2906, Loss 0.29749488830566406\n",
      "[Training Epoch 0] Batch 2907, Loss 0.2920251190662384\n",
      "[Training Epoch 0] Batch 2908, Loss 0.3208250105381012\n",
      "[Training Epoch 0] Batch 2909, Loss 0.3149419128894806\n",
      "[Training Epoch 0] Batch 2910, Loss 0.3031441271305084\n",
      "[Training Epoch 0] Batch 2911, Loss 0.3134289085865021\n",
      "[Training Epoch 0] Batch 2912, Loss 0.29783910512924194\n",
      "[Training Epoch 0] Batch 2913, Loss 0.3136613368988037\n",
      "[Training Epoch 0] Batch 2914, Loss 0.3230777978897095\n",
      "[Training Epoch 0] Batch 2915, Loss 0.307412326335907\n",
      "[Training Epoch 0] Batch 2916, Loss 0.28721511363983154\n",
      "[Training Epoch 0] Batch 2917, Loss 0.2923949360847473\n",
      "[Training Epoch 0] Batch 2918, Loss 0.30508655309677124\n",
      "[Training Epoch 0] Batch 2919, Loss 0.32598114013671875\n",
      "[Training Epoch 0] Batch 2920, Loss 0.3096526265144348\n",
      "[Training Epoch 0] Batch 2921, Loss 0.31514787673950195\n",
      "[Training Epoch 0] Batch 2922, Loss 0.3056047558784485\n",
      "[Training Epoch 0] Batch 2923, Loss 0.30747294425964355\n",
      "[Training Epoch 0] Batch 2924, Loss 0.3017367720603943\n",
      "[Training Epoch 0] Batch 2925, Loss 0.3243534564971924\n",
      "[Training Epoch 0] Batch 2926, Loss 0.29982790350914\n",
      "[Training Epoch 0] Batch 2927, Loss 0.3002796471118927\n",
      "[Training Epoch 0] Batch 2928, Loss 0.32417160272598267\n",
      "[Training Epoch 0] Batch 2929, Loss 0.29568853974342346\n",
      "[Training Epoch 0] Batch 2930, Loss 0.33792150020599365\n",
      "[Training Epoch 0] Batch 2931, Loss 0.30663079023361206\n",
      "[Training Epoch 0] Batch 2932, Loss 0.2971704602241516\n",
      "[Training Epoch 0] Batch 2933, Loss 0.33847856521606445\n",
      "[Training Epoch 0] Batch 2934, Loss 0.30897197127342224\n",
      "[Training Epoch 0] Batch 2935, Loss 0.26667672395706177\n",
      "[Training Epoch 0] Batch 2936, Loss 0.3057713210582733\n",
      "[Training Epoch 0] Batch 2937, Loss 0.33807000517845154\n",
      "[Training Epoch 0] Batch 2938, Loss 0.3506183624267578\n",
      "[Training Epoch 0] Batch 2939, Loss 0.29921483993530273\n",
      "[Training Epoch 0] Batch 2940, Loss 0.29162079095840454\n",
      "[Training Epoch 0] Batch 2941, Loss 0.28028160333633423\n",
      "[Training Epoch 0] Batch 2942, Loss 0.33301132917404175\n",
      "[Training Epoch 0] Batch 2943, Loss 0.28951704502105713\n",
      "[Training Epoch 0] Batch 2944, Loss 0.3611866235733032\n",
      "[Training Epoch 0] Batch 2945, Loss 0.2836275100708008\n",
      "[Training Epoch 0] Batch 2946, Loss 0.2860875129699707\n",
      "[Training Epoch 0] Batch 2947, Loss 0.2737358808517456\n",
      "[Training Epoch 0] Batch 2948, Loss 0.3164035379886627\n",
      "[Training Epoch 0] Batch 2949, Loss 0.28040260076522827\n",
      "[Training Epoch 0] Batch 2950, Loss 0.2994351387023926\n",
      "[Training Epoch 0] Batch 2951, Loss 0.2972196936607361\n",
      "[Training Epoch 0] Batch 2952, Loss 0.2872648537158966\n",
      "[Training Epoch 0] Batch 2953, Loss 0.3205191195011139\n",
      "[Training Epoch 0] Batch 2954, Loss 0.31045442819595337\n",
      "[Training Epoch 0] Batch 2955, Loss 0.31191569566726685\n",
      "[Training Epoch 0] Batch 2956, Loss 0.3080495297908783\n",
      "[Training Epoch 0] Batch 2957, Loss 0.29169580340385437\n",
      "[Training Epoch 0] Batch 2958, Loss 0.33204737305641174\n",
      "[Training Epoch 0] Batch 2959, Loss 0.29482799768447876\n",
      "[Training Epoch 0] Batch 2960, Loss 0.32138460874557495\n",
      "[Training Epoch 0] Batch 2961, Loss 0.29417794942855835\n",
      "[Training Epoch 0] Batch 2962, Loss 0.2988024055957794\n",
      "[Training Epoch 0] Batch 2963, Loss 0.29698407649993896\n",
      "[Training Epoch 0] Batch 2964, Loss 0.3091709613800049\n",
      "[Training Epoch 0] Batch 2965, Loss 0.327606201171875\n",
      "[Training Epoch 0] Batch 2966, Loss 0.33386683464050293\n",
      "[Training Epoch 0] Batch 2967, Loss 0.33789336681365967\n",
      "[Training Epoch 0] Batch 2968, Loss 0.2885032296180725\n",
      "[Training Epoch 0] Batch 2969, Loss 0.28168317675590515\n",
      "[Training Epoch 0] Batch 2970, Loss 0.3269481658935547\n",
      "[Training Epoch 0] Batch 2971, Loss 0.325944721698761\n",
      "[Training Epoch 0] Batch 2972, Loss 0.28063511848449707\n",
      "[Training Epoch 0] Batch 2973, Loss 0.29599529504776\n",
      "[Training Epoch 0] Batch 2974, Loss 0.3079230487346649\n",
      "[Training Epoch 0] Batch 2975, Loss 0.29719090461730957\n",
      "[Training Epoch 0] Batch 2976, Loss 0.2779499590396881\n",
      "[Training Epoch 0] Batch 2977, Loss 0.3287162184715271\n",
      "[Training Epoch 0] Batch 2978, Loss 0.319450318813324\n",
      "[Training Epoch 0] Batch 2979, Loss 0.34045547246932983\n",
      "[Training Epoch 0] Batch 2980, Loss 0.31078875064849854\n",
      "[Training Epoch 0] Batch 2981, Loss 0.30261659622192383\n",
      "[Training Epoch 0] Batch 2982, Loss 0.3079652190208435\n",
      "[Training Epoch 0] Batch 2983, Loss 0.2904236912727356\n",
      "[Training Epoch 0] Batch 2984, Loss 0.31885796785354614\n",
      "[Training Epoch 0] Batch 2985, Loss 0.31158849596977234\n",
      "[Training Epoch 0] Batch 2986, Loss 0.3007547855377197\n",
      "[Training Epoch 0] Batch 2987, Loss 0.31441032886505127\n",
      "[Training Epoch 0] Batch 2988, Loss 0.3322782516479492\n",
      "[Training Epoch 0] Batch 2989, Loss 0.29886701703071594\n",
      "[Training Epoch 0] Batch 2990, Loss 0.27825260162353516\n",
      "[Training Epoch 0] Batch 2991, Loss 0.3099525570869446\n",
      "[Training Epoch 0] Batch 2992, Loss 0.3059389591217041\n",
      "[Training Epoch 0] Batch 2993, Loss 0.31660303473472595\n",
      "[Training Epoch 0] Batch 2994, Loss 0.30536505579948425\n",
      "[Training Epoch 0] Batch 2995, Loss 0.28018802404403687\n",
      "[Training Epoch 0] Batch 2996, Loss 0.3077986538410187\n",
      "[Training Epoch 0] Batch 2997, Loss 0.3149459660053253\n",
      "[Training Epoch 0] Batch 2998, Loss 0.301583468914032\n",
      "[Training Epoch 0] Batch 2999, Loss 0.3262092173099518\n",
      "[Training Epoch 0] Batch 3000, Loss 0.2714706063270569\n",
      "[Training Epoch 0] Batch 3001, Loss 0.3164067268371582\n",
      "[Training Epoch 0] Batch 3002, Loss 0.28638210892677307\n",
      "[Training Epoch 0] Batch 3003, Loss 0.3242490887641907\n",
      "[Training Epoch 0] Batch 3004, Loss 0.30849960446357727\n",
      "[Training Epoch 0] Batch 3005, Loss 0.3116150498390198\n",
      "[Training Epoch 0] Batch 3006, Loss 0.3194589614868164\n",
      "[Training Epoch 0] Batch 3007, Loss 0.3273840844631195\n",
      "[Training Epoch 0] Batch 3008, Loss 0.3211103081703186\n",
      "[Training Epoch 0] Batch 3009, Loss 0.30941009521484375\n",
      "[Training Epoch 0] Batch 3010, Loss 0.3236505091190338\n",
      "[Training Epoch 0] Batch 3011, Loss 0.30663129687309265\n",
      "[Training Epoch 0] Batch 3012, Loss 0.26673275232315063\n",
      "[Training Epoch 0] Batch 3013, Loss 0.31243881583213806\n",
      "[Training Epoch 0] Batch 3014, Loss 0.3088086247444153\n",
      "[Training Epoch 0] Batch 3015, Loss 0.3144466280937195\n",
      "[Training Epoch 0] Batch 3016, Loss 0.28683847188949585\n",
      "[Training Epoch 0] Batch 3017, Loss 0.329714834690094\n",
      "[Training Epoch 0] Batch 3018, Loss 0.2831438183784485\n",
      "[Training Epoch 0] Batch 3019, Loss 0.31497928500175476\n",
      "[Training Epoch 0] Batch 3020, Loss 0.295546293258667\n",
      "[Training Epoch 0] Batch 3021, Loss 0.28477805852890015\n",
      "[Training Epoch 0] Batch 3022, Loss 0.3162894546985626\n",
      "[Training Epoch 0] Batch 3023, Loss 0.3094431459903717\n",
      "[Training Epoch 0] Batch 3024, Loss 0.28459876775741577\n",
      "[Training Epoch 0] Batch 3025, Loss 0.3181511461734772\n",
      "[Training Epoch 0] Batch 3026, Loss 0.33671897649765015\n",
      "[Training Epoch 0] Batch 3027, Loss 0.3175774812698364\n",
      "[Training Epoch 0] Batch 3028, Loss 0.2962625026702881\n",
      "[Training Epoch 0] Batch 3029, Loss 0.29615145921707153\n",
      "[Training Epoch 0] Batch 3030, Loss 0.31080716848373413\n",
      "[Training Epoch 0] Batch 3031, Loss 0.29224693775177\n",
      "[Training Epoch 0] Batch 3032, Loss 0.28574085235595703\n",
      "[Training Epoch 0] Batch 3033, Loss 0.3122604787349701\n",
      "[Training Epoch 0] Batch 3034, Loss 0.2811432182788849\n",
      "[Training Epoch 0] Batch 3035, Loss 0.3194572925567627\n",
      "[Training Epoch 0] Batch 3036, Loss 0.31993627548217773\n",
      "[Training Epoch 0] Batch 3037, Loss 0.2685568332672119\n",
      "[Training Epoch 0] Batch 3038, Loss 0.3163982033729553\n",
      "[Training Epoch 0] Batch 3039, Loss 0.28837400674819946\n",
      "[Training Epoch 0] Batch 3040, Loss 0.2972865700721741\n",
      "[Training Epoch 0] Batch 3041, Loss 0.31592825055122375\n",
      "[Training Epoch 0] Batch 3042, Loss 0.2929222583770752\n",
      "[Training Epoch 0] Batch 3043, Loss 0.3085324764251709\n",
      "[Training Epoch 0] Batch 3044, Loss 0.28877198696136475\n",
      "[Training Epoch 0] Batch 3045, Loss 0.3265036642551422\n",
      "[Training Epoch 0] Batch 3046, Loss 0.2712436020374298\n",
      "[Training Epoch 0] Batch 3047, Loss 0.30646535754203796\n",
      "[Training Epoch 0] Batch 3048, Loss 0.2961968183517456\n",
      "[Training Epoch 0] Batch 3049, Loss 0.28609657287597656\n",
      "[Training Epoch 0] Batch 3050, Loss 0.339864045381546\n",
      "[Training Epoch 0] Batch 3051, Loss 0.29154327511787415\n",
      "[Training Epoch 0] Batch 3052, Loss 0.32965123653411865\n",
      "[Training Epoch 0] Batch 3053, Loss 0.3217673897743225\n",
      "[Training Epoch 0] Batch 3054, Loss 0.2980365455150604\n",
      "[Training Epoch 0] Batch 3055, Loss 0.3383249044418335\n",
      "[Training Epoch 0] Batch 3056, Loss 0.30091577768325806\n",
      "[Training Epoch 0] Batch 3057, Loss 0.302530974149704\n",
      "[Training Epoch 0] Batch 3058, Loss 0.33186593651771545\n",
      "[Training Epoch 0] Batch 3059, Loss 0.2899833917617798\n",
      "[Training Epoch 0] Batch 3060, Loss 0.29011088609695435\n",
      "[Training Epoch 0] Batch 3061, Loss 0.30504727363586426\n",
      "[Training Epoch 0] Batch 3062, Loss 0.2864067852497101\n",
      "[Training Epoch 0] Batch 3063, Loss 0.28095543384552\n",
      "[Training Epoch 0] Batch 3064, Loss 0.35123980045318604\n",
      "[Training Epoch 0] Batch 3065, Loss 0.31168317794799805\n",
      "[Training Epoch 0] Batch 3066, Loss 0.28499636054039\n",
      "[Training Epoch 0] Batch 3067, Loss 0.33548492193222046\n",
      "[Training Epoch 0] Batch 3068, Loss 0.2867782711982727\n",
      "[Training Epoch 0] Batch 3069, Loss 0.3025103211402893\n",
      "[Training Epoch 0] Batch 3070, Loss 0.3123280107975006\n",
      "[Training Epoch 0] Batch 3071, Loss 0.3061147928237915\n",
      "[Training Epoch 0] Batch 3072, Loss 0.31751319766044617\n",
      "[Training Epoch 0] Batch 3073, Loss 0.30348435044288635\n",
      "[Training Epoch 0] Batch 3074, Loss 0.32748544216156006\n",
      "[Training Epoch 0] Batch 3075, Loss 0.27964842319488525\n",
      "[Training Epoch 0] Batch 3076, Loss 0.28651243448257446\n",
      "[Training Epoch 0] Batch 3077, Loss 0.3029404282569885\n",
      "[Training Epoch 0] Batch 3078, Loss 0.30337953567504883\n",
      "[Training Epoch 0] Batch 3079, Loss 0.2860431671142578\n",
      "[Training Epoch 0] Batch 3080, Loss 0.3044229745864868\n",
      "[Training Epoch 0] Batch 3081, Loss 0.30844026803970337\n",
      "[Training Epoch 0] Batch 3082, Loss 0.29647254943847656\n",
      "[Training Epoch 0] Batch 3083, Loss 0.2996370196342468\n",
      "[Training Epoch 0] Batch 3084, Loss 0.3402286171913147\n",
      "[Training Epoch 0] Batch 3085, Loss 0.3144482970237732\n",
      "[Training Epoch 0] Batch 3086, Loss 0.2824731469154358\n",
      "[Training Epoch 0] Batch 3087, Loss 0.2892228364944458\n",
      "[Training Epoch 0] Batch 3088, Loss 0.2890129089355469\n",
      "[Training Epoch 0] Batch 3089, Loss 0.32325977087020874\n",
      "[Training Epoch 0] Batch 3090, Loss 0.3079569935798645\n",
      "[Training Epoch 0] Batch 3091, Loss 0.33346882462501526\n",
      "[Training Epoch 0] Batch 3092, Loss 0.2918098568916321\n",
      "[Training Epoch 0] Batch 3093, Loss 0.2921026945114136\n",
      "[Training Epoch 0] Batch 3094, Loss 0.2913404405117035\n",
      "[Training Epoch 0] Batch 3095, Loss 0.3042881488800049\n",
      "[Training Epoch 0] Batch 3096, Loss 0.26656800508499146\n",
      "[Training Epoch 0] Batch 3097, Loss 0.305558443069458\n",
      "[Training Epoch 0] Batch 3098, Loss 0.3035646677017212\n",
      "[Training Epoch 0] Batch 3099, Loss 0.31506508588790894\n",
      "[Training Epoch 0] Batch 3100, Loss 0.2932741045951843\n",
      "[Training Epoch 0] Batch 3101, Loss 0.30366113781929016\n",
      "[Training Epoch 0] Batch 3102, Loss 0.3297293186187744\n",
      "[Training Epoch 0] Batch 3103, Loss 0.34351086616516113\n",
      "[Training Epoch 0] Batch 3104, Loss 0.29849156737327576\n",
      "[Training Epoch 0] Batch 3105, Loss 0.30787792801856995\n",
      "[Training Epoch 0] Batch 3106, Loss 0.3003745377063751\n",
      "[Training Epoch 0] Batch 3107, Loss 0.3314606547355652\n",
      "[Training Epoch 0] Batch 3108, Loss 0.31914716958999634\n",
      "[Training Epoch 0] Batch 3109, Loss 0.2908176779747009\n",
      "[Training Epoch 0] Batch 3110, Loss 0.275323748588562\n",
      "[Training Epoch 0] Batch 3111, Loss 0.2952500283718109\n",
      "[Training Epoch 0] Batch 3112, Loss 0.31850314140319824\n",
      "[Training Epoch 0] Batch 3113, Loss 0.32084599137306213\n",
      "[Training Epoch 0] Batch 3114, Loss 0.2842171788215637\n",
      "[Training Epoch 0] Batch 3115, Loss 0.2845914363861084\n",
      "[Training Epoch 0] Batch 3116, Loss 0.3130045533180237\n",
      "[Training Epoch 0] Batch 3117, Loss 0.2990844249725342\n",
      "[Training Epoch 0] Batch 3118, Loss 0.29400384426116943\n",
      "[Training Epoch 0] Batch 3119, Loss 0.2998674213886261\n",
      "[Training Epoch 0] Batch 3120, Loss 0.3031959533691406\n",
      "[Training Epoch 0] Batch 3121, Loss 0.3046804666519165\n",
      "[Training Epoch 0] Batch 3122, Loss 0.279391348361969\n",
      "[Training Epoch 0] Batch 3123, Loss 0.33169201016426086\n",
      "[Training Epoch 0] Batch 3124, Loss 0.29857003688812256\n",
      "[Training Epoch 0] Batch 3125, Loss 0.2928144931793213\n",
      "[Training Epoch 0] Batch 3126, Loss 0.3130727708339691\n",
      "[Training Epoch 0] Batch 3127, Loss 0.2776540219783783\n",
      "[Training Epoch 0] Batch 3128, Loss 0.31515103578567505\n",
      "[Training Epoch 0] Batch 3129, Loss 0.3022206723690033\n",
      "[Training Epoch 0] Batch 3130, Loss 0.35970208048820496\n",
      "[Training Epoch 0] Batch 3131, Loss 0.28695207834243774\n",
      "[Training Epoch 0] Batch 3132, Loss 0.34725525975227356\n",
      "[Training Epoch 0] Batch 3133, Loss 0.31823331117630005\n",
      "[Training Epoch 0] Batch 3134, Loss 0.3179250955581665\n",
      "[Training Epoch 0] Batch 3135, Loss 0.3249017596244812\n",
      "[Training Epoch 0] Batch 3136, Loss 0.3105016350746155\n",
      "[Training Epoch 0] Batch 3137, Loss 0.31371766328811646\n",
      "[Training Epoch 0] Batch 3138, Loss 0.30732882022857666\n",
      "[Training Epoch 0] Batch 3139, Loss 0.30988356471061707\n",
      "[Training Epoch 0] Batch 3140, Loss 0.30304256081581116\n",
      "[Training Epoch 0] Batch 3141, Loss 0.29542386531829834\n",
      "[Training Epoch 0] Batch 3142, Loss 0.3215574026107788\n",
      "[Training Epoch 0] Batch 3143, Loss 0.31386470794677734\n",
      "[Training Epoch 0] Batch 3144, Loss 0.30541568994522095\n",
      "[Training Epoch 0] Batch 3145, Loss 0.3120088577270508\n",
      "[Training Epoch 0] Batch 3146, Loss 0.3122639060020447\n",
      "[Training Epoch 0] Batch 3147, Loss 0.3288666009902954\n",
      "[Training Epoch 0] Batch 3148, Loss 0.2992977797985077\n",
      "[Training Epoch 0] Batch 3149, Loss 0.28199347853660583\n",
      "[Training Epoch 0] Batch 3150, Loss 0.30823689699172974\n",
      "[Training Epoch 0] Batch 3151, Loss 0.308562695980072\n",
      "[Training Epoch 0] Batch 3152, Loss 0.2882677912712097\n",
      "[Training Epoch 0] Batch 3153, Loss 0.32853755354881287\n",
      "[Training Epoch 0] Batch 3154, Loss 0.3231699764728546\n",
      "[Training Epoch 0] Batch 3155, Loss 0.3154100775718689\n",
      "[Training Epoch 0] Batch 3156, Loss 0.2976011633872986\n",
      "[Training Epoch 0] Batch 3157, Loss 0.30963072180747986\n",
      "[Training Epoch 0] Batch 3158, Loss 0.29844871163368225\n",
      "[Training Epoch 0] Batch 3159, Loss 0.3332839608192444\n",
      "[Training Epoch 0] Batch 3160, Loss 0.3383317291736603\n",
      "[Training Epoch 0] Batch 3161, Loss 0.3377687335014343\n",
      "[Training Epoch 0] Batch 3162, Loss 0.3010702133178711\n",
      "[Training Epoch 0] Batch 3163, Loss 0.32339179515838623\n",
      "[Training Epoch 0] Batch 3164, Loss 0.2947794497013092\n",
      "[Training Epoch 0] Batch 3165, Loss 0.3183962404727936\n",
      "[Training Epoch 0] Batch 3166, Loss 0.31315532326698303\n",
      "[Training Epoch 0] Batch 3167, Loss 0.3047831058502197\n",
      "[Training Epoch 0] Batch 3168, Loss 0.2716514468193054\n",
      "[Training Epoch 0] Batch 3169, Loss 0.30736392736434937\n",
      "[Training Epoch 0] Batch 3170, Loss 0.28150424361228943\n",
      "[Training Epoch 0] Batch 3171, Loss 0.2834756374359131\n",
      "[Training Epoch 0] Batch 3172, Loss 0.29331138730049133\n",
      "[Training Epoch 0] Batch 3173, Loss 0.3175468146800995\n",
      "[Training Epoch 0] Batch 3174, Loss 0.33974504470825195\n",
      "[Training Epoch 0] Batch 3175, Loss 0.3013994097709656\n",
      "[Training Epoch 0] Batch 3176, Loss 0.26143062114715576\n",
      "[Training Epoch 0] Batch 3177, Loss 0.291266530752182\n",
      "[Training Epoch 0] Batch 3178, Loss 0.2944478988647461\n",
      "[Training Epoch 0] Batch 3179, Loss 0.31044501066207886\n",
      "[Training Epoch 0] Batch 3180, Loss 0.31831055879592896\n",
      "[Training Epoch 0] Batch 3181, Loss 0.3209158778190613\n",
      "[Training Epoch 0] Batch 3182, Loss 0.3020992577075958\n",
      "[Training Epoch 0] Batch 3183, Loss 0.31054022908210754\n",
      "[Training Epoch 0] Batch 3184, Loss 0.30810773372650146\n",
      "[Training Epoch 0] Batch 3185, Loss 0.298849493265152\n",
      "[Training Epoch 0] Batch 3186, Loss 0.28065627813339233\n",
      "[Training Epoch 0] Batch 3187, Loss 0.28163284063339233\n",
      "[Training Epoch 0] Batch 3188, Loss 0.3002116084098816\n",
      "[Training Epoch 0] Batch 3189, Loss 0.30039483308792114\n",
      "[Training Epoch 0] Batch 3190, Loss 0.318636953830719\n",
      "[Training Epoch 0] Batch 3191, Loss 0.2999679148197174\n",
      "[Training Epoch 0] Batch 3192, Loss 0.3045499324798584\n",
      "[Training Epoch 0] Batch 3193, Loss 0.3022213280200958\n",
      "[Training Epoch 0] Batch 3194, Loss 0.3050397038459778\n",
      "[Training Epoch 0] Batch 3195, Loss 0.3143368363380432\n",
      "[Training Epoch 0] Batch 3196, Loss 0.3151732385158539\n",
      "[Training Epoch 0] Batch 3197, Loss 0.30445247888565063\n",
      "[Training Epoch 0] Batch 3198, Loss 0.29216134548187256\n",
      "[Training Epoch 0] Batch 3199, Loss 0.3273504674434662\n",
      "[Training Epoch 0] Batch 3200, Loss 0.2778395116329193\n",
      "[Training Epoch 0] Batch 3201, Loss 0.29410141706466675\n",
      "[Training Epoch 0] Batch 3202, Loss 0.2963809669017792\n",
      "[Training Epoch 0] Batch 3203, Loss 0.29518380761146545\n",
      "[Training Epoch 0] Batch 3204, Loss 0.3522994816303253\n",
      "[Training Epoch 0] Batch 3205, Loss 0.33258122205734253\n",
      "[Training Epoch 0] Batch 3206, Loss 0.27723950147628784\n",
      "[Training Epoch 0] Batch 3207, Loss 0.3005240559577942\n",
      "[Training Epoch 0] Batch 3208, Loss 0.26284098625183105\n",
      "[Training Epoch 0] Batch 3209, Loss 0.29451650381088257\n",
      "[Training Epoch 0] Batch 3210, Loss 0.2611023783683777\n",
      "[Training Epoch 0] Batch 3211, Loss 0.3121500015258789\n",
      "[Training Epoch 0] Batch 3212, Loss 0.29100167751312256\n",
      "[Training Epoch 0] Batch 3213, Loss 0.31207501888275146\n",
      "[Training Epoch 0] Batch 3214, Loss 0.3131243586540222\n",
      "[Training Epoch 0] Batch 3215, Loss 0.3045829236507416\n",
      "[Training Epoch 0] Batch 3216, Loss 0.31359240412712097\n",
      "[Training Epoch 0] Batch 3217, Loss 0.2775851786136627\n",
      "[Training Epoch 0] Batch 3218, Loss 0.28246480226516724\n",
      "[Training Epoch 0] Batch 3219, Loss 0.34200841188430786\n",
      "[Training Epoch 0] Batch 3220, Loss 0.31648093461990356\n",
      "[Training Epoch 0] Batch 3221, Loss 0.30370956659317017\n",
      "[Training Epoch 0] Batch 3222, Loss 0.31374919414520264\n",
      "[Training Epoch 0] Batch 3223, Loss 0.3029201328754425\n",
      "[Training Epoch 0] Batch 3224, Loss 0.3126955032348633\n",
      "[Training Epoch 0] Batch 3225, Loss 0.2717882990837097\n",
      "[Training Epoch 0] Batch 3226, Loss 0.3042253255844116\n",
      "[Training Epoch 0] Batch 3227, Loss 0.29472947120666504\n",
      "[Training Epoch 0] Batch 3228, Loss 0.28539007902145386\n",
      "[Training Epoch 0] Batch 3229, Loss 0.32522374391555786\n",
      "[Training Epoch 0] Batch 3230, Loss 0.33449625968933105\n",
      "[Training Epoch 0] Batch 3231, Loss 0.26561880111694336\n",
      "[Training Epoch 0] Batch 3232, Loss 0.3032788038253784\n",
      "[Training Epoch 0] Batch 3233, Loss 0.29789185523986816\n",
      "[Training Epoch 0] Batch 3234, Loss 0.29096394777297974\n",
      "[Training Epoch 0] Batch 3235, Loss 0.2886142134666443\n",
      "[Training Epoch 0] Batch 3236, Loss 0.29290175437927246\n",
      "[Training Epoch 0] Batch 3237, Loss 0.32212144136428833\n",
      "[Training Epoch 0] Batch 3238, Loss 0.2968212068080902\n",
      "[Training Epoch 0] Batch 3239, Loss 0.322442889213562\n",
      "[Training Epoch 0] Batch 3240, Loss 0.2969741225242615\n",
      "[Training Epoch 0] Batch 3241, Loss 0.3321177065372467\n",
      "[Training Epoch 0] Batch 3242, Loss 0.3425880968570709\n",
      "[Training Epoch 0] Batch 3243, Loss 0.30386844277381897\n",
      "[Training Epoch 0] Batch 3244, Loss 0.27812784910202026\n",
      "[Training Epoch 0] Batch 3245, Loss 0.3141537606716156\n",
      "[Training Epoch 0] Batch 3246, Loss 0.31725430488586426\n",
      "[Training Epoch 0] Batch 3247, Loss 0.32126879692077637\n",
      "[Training Epoch 0] Batch 3248, Loss 0.3153178095817566\n",
      "[Training Epoch 0] Batch 3249, Loss 0.30416396260261536\n",
      "[Training Epoch 0] Batch 3250, Loss 0.3134743273258209\n",
      "[Training Epoch 0] Batch 3251, Loss 0.32191866636276245\n",
      "[Training Epoch 0] Batch 3252, Loss 0.3041483759880066\n",
      "[Training Epoch 0] Batch 3253, Loss 0.3102646470069885\n",
      "[Training Epoch 0] Batch 3254, Loss 0.31011784076690674\n",
      "[Training Epoch 0] Batch 3255, Loss 0.3226968050003052\n",
      "[Training Epoch 0] Batch 3256, Loss 0.2861036956310272\n",
      "[Training Epoch 0] Batch 3257, Loss 0.30257484316825867\n",
      "[Training Epoch 0] Batch 3258, Loss 0.30670756101608276\n",
      "[Training Epoch 0] Batch 3259, Loss 0.2800728380680084\n",
      "[Training Epoch 0] Batch 3260, Loss 0.30087682604789734\n",
      "[Training Epoch 0] Batch 3261, Loss 0.31620460748672485\n",
      "[Training Epoch 0] Batch 3262, Loss 0.3302287459373474\n",
      "[Training Epoch 0] Batch 3263, Loss 0.2931582033634186\n",
      "[Training Epoch 0] Batch 3264, Loss 0.32353079319000244\n",
      "[Training Epoch 0] Batch 3265, Loss 0.27251556515693665\n",
      "[Training Epoch 0] Batch 3266, Loss 0.29899513721466064\n",
      "[Training Epoch 0] Batch 3267, Loss 0.30747994780540466\n",
      "[Training Epoch 0] Batch 3268, Loss 0.29168426990509033\n",
      "[Training Epoch 0] Batch 3269, Loss 0.30122071504592896\n",
      "[Training Epoch 0] Batch 3270, Loss 0.34356606006622314\n",
      "[Training Epoch 0] Batch 3271, Loss 0.29796522855758667\n",
      "[Training Epoch 0] Batch 3272, Loss 0.306970477104187\n",
      "[Training Epoch 0] Batch 3273, Loss 0.30972030758857727\n",
      "[Training Epoch 0] Batch 3274, Loss 0.3166202902793884\n",
      "[Training Epoch 0] Batch 3275, Loss 0.31236156821250916\n",
      "[Training Epoch 0] Batch 3276, Loss 0.3014293909072876\n",
      "[Training Epoch 0] Batch 3277, Loss 0.2963269352912903\n",
      "[Training Epoch 0] Batch 3278, Loss 0.3191828429698944\n",
      "[Training Epoch 0] Batch 3279, Loss 0.2962113618850708\n",
      "[Training Epoch 0] Batch 3280, Loss 0.31559646129608154\n",
      "[Training Epoch 0] Batch 3281, Loss 0.30737319588661194\n",
      "[Training Epoch 0] Batch 3282, Loss 0.31203770637512207\n",
      "[Training Epoch 0] Batch 3283, Loss 0.2971801161766052\n",
      "[Training Epoch 0] Batch 3284, Loss 0.3211424946784973\n",
      "[Training Epoch 0] Batch 3285, Loss 0.32529523968696594\n",
      "[Training Epoch 0] Batch 3286, Loss 0.2916145324707031\n",
      "[Training Epoch 0] Batch 3287, Loss 0.30176830291748047\n",
      "[Training Epoch 0] Batch 3288, Loss 0.31671610474586487\n",
      "[Training Epoch 0] Batch 3289, Loss 0.29240095615386963\n",
      "[Training Epoch 0] Batch 3290, Loss 0.33362704515457153\n",
      "[Training Epoch 0] Batch 3291, Loss 0.30232733488082886\n",
      "[Training Epoch 0] Batch 3292, Loss 0.3013245761394501\n",
      "[Training Epoch 0] Batch 3293, Loss 0.3118777573108673\n",
      "[Training Epoch 0] Batch 3294, Loss 0.32669198513031006\n",
      "[Training Epoch 0] Batch 3295, Loss 0.3006696105003357\n",
      "[Training Epoch 0] Batch 3296, Loss 0.28543105721473694\n",
      "[Training Epoch 0] Batch 3297, Loss 0.2965569794178009\n",
      "[Training Epoch 0] Batch 3298, Loss 0.29965639114379883\n",
      "[Training Epoch 0] Batch 3299, Loss 0.29495739936828613\n",
      "[Training Epoch 0] Batch 3300, Loss 0.31413722038269043\n",
      "[Training Epoch 0] Batch 3301, Loss 0.3215574026107788\n",
      "[Training Epoch 0] Batch 3302, Loss 0.29674744606018066\n",
      "[Training Epoch 0] Batch 3303, Loss 0.30330511927604675\n",
      "[Training Epoch 0] Batch 3304, Loss 0.3059726655483246\n",
      "[Training Epoch 0] Batch 3305, Loss 0.30157819390296936\n",
      "[Training Epoch 0] Batch 3306, Loss 0.31862926483154297\n",
      "[Training Epoch 0] Batch 3307, Loss 0.3217927813529968\n",
      "[Training Epoch 0] Batch 3308, Loss 0.30900460481643677\n",
      "[Training Epoch 0] Batch 3309, Loss 0.293320894241333\n",
      "[Training Epoch 0] Batch 3310, Loss 0.28222742676734924\n",
      "[Training Epoch 0] Batch 3311, Loss 0.2823141813278198\n",
      "[Training Epoch 0] Batch 3312, Loss 0.3388999104499817\n",
      "[Training Epoch 0] Batch 3313, Loss 0.30466580390930176\n",
      "[Training Epoch 0] Batch 3314, Loss 0.33588314056396484\n",
      "[Training Epoch 0] Batch 3315, Loss 0.3205416202545166\n",
      "[Training Epoch 0] Batch 3316, Loss 0.30229681730270386\n",
      "[Training Epoch 0] Batch 3317, Loss 0.2964567542076111\n",
      "[Training Epoch 0] Batch 3318, Loss 0.3293907046318054\n",
      "[Training Epoch 0] Batch 3319, Loss 0.287086546421051\n",
      "[Training Epoch 0] Batch 3320, Loss 0.3341667652130127\n",
      "[Training Epoch 0] Batch 3321, Loss 0.27471959590911865\n",
      "[Training Epoch 0] Batch 3322, Loss 0.29585346579551697\n",
      "[Training Epoch 0] Batch 3323, Loss 0.27893781661987305\n",
      "[Training Epoch 0] Batch 3324, Loss 0.305916428565979\n",
      "[Training Epoch 0] Batch 3325, Loss 0.2899012267589569\n",
      "[Training Epoch 0] Batch 3326, Loss 0.2950860857963562\n",
      "[Training Epoch 0] Batch 3327, Loss 0.2782604694366455\n",
      "[Training Epoch 0] Batch 3328, Loss 0.30151277780532837\n",
      "[Training Epoch 0] Batch 3329, Loss 0.30416858196258545\n",
      "[Training Epoch 0] Batch 3330, Loss 0.295482337474823\n",
      "[Training Epoch 0] Batch 3331, Loss 0.31190457940101624\n",
      "[Training Epoch 0] Batch 3332, Loss 0.30068182945251465\n",
      "[Training Epoch 0] Batch 3333, Loss 0.31573379039764404\n",
      "[Training Epoch 0] Batch 3334, Loss 0.29688510298728943\n",
      "[Training Epoch 0] Batch 3335, Loss 0.2883707880973816\n",
      "[Training Epoch 0] Batch 3336, Loss 0.27862441539764404\n",
      "[Training Epoch 0] Batch 3337, Loss 0.30790430307388306\n",
      "[Training Epoch 0] Batch 3338, Loss 0.2985764145851135\n",
      "[Training Epoch 0] Batch 3339, Loss 0.33186110854148865\n",
      "[Training Epoch 0] Batch 3340, Loss 0.29071134328842163\n",
      "[Training Epoch 0] Batch 3341, Loss 0.2859991788864136\n",
      "[Training Epoch 0] Batch 3342, Loss 0.28730857372283936\n",
      "[Training Epoch 0] Batch 3343, Loss 0.2964692711830139\n",
      "[Training Epoch 0] Batch 3344, Loss 0.30871981382369995\n",
      "[Training Epoch 0] Batch 3345, Loss 0.298770010471344\n",
      "[Training Epoch 0] Batch 3346, Loss 0.2815297842025757\n",
      "[Training Epoch 0] Batch 3347, Loss 0.3040481209754944\n",
      "[Training Epoch 0] Batch 3348, Loss 0.3031810224056244\n",
      "[Training Epoch 0] Batch 3349, Loss 0.3009617328643799\n",
      "[Training Epoch 0] Batch 3350, Loss 0.2792741060256958\n",
      "[Training Epoch 0] Batch 3351, Loss 0.3254591226577759\n",
      "[Training Epoch 0] Batch 3352, Loss 0.3032947778701782\n",
      "[Training Epoch 0] Batch 3353, Loss 0.2925341725349426\n",
      "[Training Epoch 0] Batch 3354, Loss 0.3166533410549164\n",
      "[Training Epoch 0] Batch 3355, Loss 0.3102404773235321\n",
      "[Training Epoch 0] Batch 3356, Loss 0.3291585147380829\n",
      "[Training Epoch 0] Batch 3357, Loss 0.3046214282512665\n",
      "[Training Epoch 0] Batch 3358, Loss 0.2987160086631775\n",
      "[Training Epoch 0] Batch 3359, Loss 0.30938422679901123\n",
      "[Training Epoch 0] Batch 3360, Loss 0.3028082549571991\n",
      "[Training Epoch 0] Batch 3361, Loss 0.31639623641967773\n",
      "[Training Epoch 0] Batch 3362, Loss 0.31406134366989136\n",
      "[Training Epoch 0] Batch 3363, Loss 0.29886335134506226\n",
      "[Training Epoch 0] Batch 3364, Loss 0.2812097370624542\n",
      "[Training Epoch 0] Batch 3365, Loss 0.3233779966831207\n",
      "[Training Epoch 0] Batch 3366, Loss 0.3059014081954956\n",
      "[Training Epoch 0] Batch 3367, Loss 0.3123739957809448\n",
      "[Training Epoch 0] Batch 3368, Loss 0.2932140827178955\n",
      "[Training Epoch 0] Batch 3369, Loss 0.294134259223938\n",
      "[Training Epoch 0] Batch 3370, Loss 0.3072913885116577\n",
      "[Training Epoch 0] Batch 3371, Loss 0.3009203374385834\n",
      "[Training Epoch 0] Batch 3372, Loss 0.30166536569595337\n",
      "[Training Epoch 0] Batch 3373, Loss 0.3015700578689575\n",
      "[Training Epoch 0] Batch 3374, Loss 0.2993068993091583\n",
      "[Training Epoch 0] Batch 3375, Loss 0.3089573383331299\n",
      "[Training Epoch 0] Batch 3376, Loss 0.2885018289089203\n",
      "[Training Epoch 0] Batch 3377, Loss 0.27861663699150085\n",
      "[Training Epoch 0] Batch 3378, Loss 0.30466586351394653\n",
      "[Training Epoch 0] Batch 3379, Loss 0.26593708992004395\n",
      "[Training Epoch 0] Batch 3380, Loss 0.31201714277267456\n",
      "[Training Epoch 0] Batch 3381, Loss 0.3140270709991455\n",
      "[Training Epoch 0] Batch 3382, Loss 0.2879258096218109\n",
      "[Training Epoch 0] Batch 3383, Loss 0.30157384276390076\n",
      "[Training Epoch 0] Batch 3384, Loss 0.319429874420166\n",
      "[Training Epoch 0] Batch 3385, Loss 0.33000433444976807\n",
      "[Training Epoch 0] Batch 3386, Loss 0.32835912704467773\n",
      "[Training Epoch 0] Batch 3387, Loss 0.28920862078666687\n",
      "[Training Epoch 0] Batch 3388, Loss 0.30903804302215576\n",
      "[Training Epoch 0] Batch 3389, Loss 0.3039194345474243\n",
      "[Training Epoch 0] Batch 3390, Loss 0.3116537928581238\n",
      "[Training Epoch 0] Batch 3391, Loss 0.31083452701568604\n",
      "[Training Epoch 0] Batch 3392, Loss 0.2994347810745239\n",
      "[Training Epoch 0] Batch 3393, Loss 0.28545957803726196\n",
      "[Training Epoch 0] Batch 3394, Loss 0.2887537479400635\n",
      "[Training Epoch 0] Batch 3395, Loss 0.2873651087284088\n",
      "[Training Epoch 0] Batch 3396, Loss 0.2953967750072479\n",
      "[Training Epoch 0] Batch 3397, Loss 0.30596721172332764\n",
      "[Training Epoch 0] Batch 3398, Loss 0.287702739238739\n",
      "[Training Epoch 0] Batch 3399, Loss 0.28119683265686035\n",
      "[Training Epoch 0] Batch 3400, Loss 0.290896475315094\n",
      "[Training Epoch 0] Batch 3401, Loss 0.3241954743862152\n",
      "[Training Epoch 0] Batch 3402, Loss 0.3027902841567993\n",
      "[Training Epoch 0] Batch 3403, Loss 0.28799429535865784\n",
      "[Training Epoch 0] Batch 3404, Loss 0.2816295027732849\n",
      "[Training Epoch 0] Batch 3405, Loss 0.29690003395080566\n",
      "[Training Epoch 0] Batch 3406, Loss 0.3051263689994812\n",
      "[Training Epoch 0] Batch 3407, Loss 0.30606013536453247\n",
      "[Training Epoch 0] Batch 3408, Loss 0.3124592900276184\n",
      "[Training Epoch 0] Batch 3409, Loss 0.30755531787872314\n",
      "[Training Epoch 0] Batch 3410, Loss 0.3371233344078064\n",
      "[Training Epoch 0] Batch 3411, Loss 0.30468255281448364\n",
      "[Training Epoch 0] Batch 3412, Loss 0.3175470530986786\n",
      "[Training Epoch 0] Batch 3413, Loss 0.30148664116859436\n",
      "[Training Epoch 0] Batch 3414, Loss 0.321965754032135\n",
      "[Training Epoch 0] Batch 3415, Loss 0.28966331481933594\n",
      "[Training Epoch 0] Batch 3416, Loss 0.3210235834121704\n",
      "[Training Epoch 0] Batch 3417, Loss 0.2875674366950989\n",
      "[Training Epoch 0] Batch 3418, Loss 0.2820773720741272\n",
      "[Training Epoch 0] Batch 3419, Loss 0.30166271328926086\n",
      "[Training Epoch 0] Batch 3420, Loss 0.3128277063369751\n",
      "[Training Epoch 0] Batch 3421, Loss 0.3049027919769287\n",
      "[Training Epoch 0] Batch 3422, Loss 0.3014927804470062\n",
      "[Training Epoch 0] Batch 3423, Loss 0.3199778199195862\n",
      "[Training Epoch 0] Batch 3424, Loss 0.2797083854675293\n",
      "[Training Epoch 0] Batch 3425, Loss 0.29101961851119995\n",
      "[Training Epoch 0] Batch 3426, Loss 0.29948920011520386\n",
      "[Training Epoch 0] Batch 3427, Loss 0.3015590310096741\n",
      "[Training Epoch 0] Batch 3428, Loss 0.30037379264831543\n",
      "[Training Epoch 0] Batch 3429, Loss 0.29427778720855713\n",
      "[Training Epoch 0] Batch 3430, Loss 0.3039745092391968\n",
      "[Training Epoch 0] Batch 3431, Loss 0.30771952867507935\n",
      "[Training Epoch 0] Batch 3432, Loss 0.31991514563560486\n",
      "[Training Epoch 0] Batch 3433, Loss 0.2793077230453491\n",
      "[Training Epoch 0] Batch 3434, Loss 0.3118167519569397\n",
      "[Training Epoch 0] Batch 3435, Loss 0.3019201159477234\n",
      "[Training Epoch 0] Batch 3436, Loss 0.26417070627212524\n",
      "[Training Epoch 0] Batch 3437, Loss 0.2996075749397278\n",
      "[Training Epoch 0] Batch 3438, Loss 0.29643192887306213\n",
      "[Training Epoch 0] Batch 3439, Loss 0.3055400848388672\n",
      "[Training Epoch 0] Batch 3440, Loss 0.3255192041397095\n",
      "[Training Epoch 0] Batch 3441, Loss 0.3123410940170288\n",
      "[Training Epoch 0] Batch 3442, Loss 0.3106810450553894\n",
      "[Training Epoch 0] Batch 3443, Loss 0.2957492470741272\n",
      "[Training Epoch 0] Batch 3444, Loss 0.30189698934555054\n",
      "[Training Epoch 0] Batch 3445, Loss 0.3099338710308075\n",
      "[Training Epoch 0] Batch 3446, Loss 0.3112085461616516\n",
      "[Training Epoch 0] Batch 3447, Loss 0.31626129150390625\n",
      "[Training Epoch 0] Batch 3448, Loss 0.3083531856536865\n",
      "[Training Epoch 0] Batch 3449, Loss 0.3329453468322754\n",
      "[Training Epoch 0] Batch 3450, Loss 0.32145988941192627\n",
      "[Training Epoch 0] Batch 3451, Loss 0.2773172855377197\n",
      "[Training Epoch 0] Batch 3452, Loss 0.3249964118003845\n",
      "[Training Epoch 0] Batch 3453, Loss 0.3042888045310974\n",
      "[Training Epoch 0] Batch 3454, Loss 0.3437887132167816\n",
      "[Training Epoch 0] Batch 3455, Loss 0.3195575177669525\n",
      "[Training Epoch 0] Batch 3456, Loss 0.3098178803920746\n",
      "[Training Epoch 0] Batch 3457, Loss 0.30365222692489624\n",
      "[Training Epoch 0] Batch 3458, Loss 0.2734294533729553\n",
      "[Training Epoch 0] Batch 3459, Loss 0.3000369071960449\n",
      "[Training Epoch 0] Batch 3460, Loss 0.30635911226272583\n",
      "[Training Epoch 0] Batch 3461, Loss 0.31045064330101013\n",
      "[Training Epoch 0] Batch 3462, Loss 0.30045217275619507\n",
      "[Training Epoch 0] Batch 3463, Loss 0.30087754130363464\n",
      "[Training Epoch 0] Batch 3464, Loss 0.32018348574638367\n",
      "[Training Epoch 0] Batch 3465, Loss 0.31278514862060547\n",
      "[Training Epoch 0] Batch 3466, Loss 0.304601788520813\n",
      "[Training Epoch 0] Batch 3467, Loss 0.30073994398117065\n",
      "[Training Epoch 0] Batch 3468, Loss 0.340914249420166\n",
      "[Training Epoch 0] Batch 3469, Loss 0.265716016292572\n",
      "[Training Epoch 0] Batch 3470, Loss 0.29924243688583374\n",
      "[Training Epoch 0] Batch 3471, Loss 0.3139737546443939\n",
      "[Training Epoch 0] Batch 3472, Loss 0.28021031618118286\n",
      "[Training Epoch 0] Batch 3473, Loss 0.3121659457683563\n",
      "[Training Epoch 0] Batch 3474, Loss 0.2920764684677124\n",
      "[Training Epoch 0] Batch 3475, Loss 0.31299081444740295\n",
      "[Training Epoch 0] Batch 3476, Loss 0.2991337180137634\n",
      "[Training Epoch 0] Batch 3477, Loss 0.304428368806839\n",
      "[Training Epoch 0] Batch 3478, Loss 0.3018149137496948\n",
      "[Training Epoch 0] Batch 3479, Loss 0.2840087115764618\n",
      "[Training Epoch 0] Batch 3480, Loss 0.3134130537509918\n",
      "[Training Epoch 0] Batch 3481, Loss 0.2924032211303711\n",
      "[Training Epoch 0] Batch 3482, Loss 0.269798219203949\n",
      "[Training Epoch 0] Batch 3483, Loss 0.29156142473220825\n",
      "[Training Epoch 0] Batch 3484, Loss 0.30693942308425903\n",
      "[Training Epoch 0] Batch 3485, Loss 0.33314424753189087\n",
      "[Training Epoch 0] Batch 3486, Loss 0.32454222440719604\n",
      "[Training Epoch 0] Batch 3487, Loss 0.3064586818218231\n",
      "[Training Epoch 0] Batch 3488, Loss 0.2613542079925537\n",
      "[Training Epoch 0] Batch 3489, Loss 0.28562361001968384\n",
      "[Training Epoch 0] Batch 3490, Loss 0.3185848891735077\n",
      "[Training Epoch 0] Batch 3491, Loss 0.2975383400917053\n",
      "[Training Epoch 0] Batch 3492, Loss 0.3423044681549072\n",
      "[Training Epoch 0] Batch 3493, Loss 0.27338919043540955\n",
      "[Training Epoch 0] Batch 3494, Loss 0.30170565843582153\n",
      "[Training Epoch 0] Batch 3495, Loss 0.31626659631729126\n",
      "[Training Epoch 0] Batch 3496, Loss 0.27877432107925415\n",
      "[Training Epoch 0] Batch 3497, Loss 0.30824699997901917\n",
      "[Training Epoch 0] Batch 3498, Loss 0.29022926092147827\n",
      "[Training Epoch 0] Batch 3499, Loss 0.3266676068305969\n",
      "[Training Epoch 0] Batch 3500, Loss 0.3003838360309601\n",
      "[Training Epoch 0] Batch 3501, Loss 0.3163173794746399\n",
      "[Training Epoch 0] Batch 3502, Loss 0.33791130781173706\n",
      "[Training Epoch 0] Batch 3503, Loss 0.29150745272636414\n",
      "[Training Epoch 0] Batch 3504, Loss 0.2860167622566223\n",
      "[Training Epoch 0] Batch 3505, Loss 0.31182020902633667\n",
      "[Training Epoch 0] Batch 3506, Loss 0.3250395953655243\n",
      "[Training Epoch 0] Batch 3507, Loss 0.270551472902298\n",
      "[Training Epoch 0] Batch 3508, Loss 0.30512285232543945\n",
      "[Training Epoch 0] Batch 3509, Loss 0.2929977476596832\n",
      "[Training Epoch 0] Batch 3510, Loss 0.3171781599521637\n",
      "[Training Epoch 0] Batch 3511, Loss 0.31372055411338806\n",
      "[Training Epoch 0] Batch 3512, Loss 0.28434067964553833\n",
      "[Training Epoch 0] Batch 3513, Loss 0.2884305715560913\n",
      "[Training Epoch 0] Batch 3514, Loss 0.284100204706192\n",
      "[Training Epoch 0] Batch 3515, Loss 0.302546888589859\n",
      "[Training Epoch 0] Batch 3516, Loss 0.2878742814064026\n",
      "[Training Epoch 0] Batch 3517, Loss 0.3228367567062378\n",
      "[Training Epoch 0] Batch 3518, Loss 0.2944045960903168\n",
      "[Training Epoch 0] Batch 3519, Loss 0.2903844714164734\n",
      "[Training Epoch 0] Batch 3520, Loss 0.3147178292274475\n",
      "[Training Epoch 0] Batch 3521, Loss 0.31168437004089355\n",
      "[Training Epoch 0] Batch 3522, Loss 0.2572958469390869\n",
      "[Training Epoch 0] Batch 3523, Loss 0.30936986207962036\n",
      "[Training Epoch 0] Batch 3524, Loss 0.3111906051635742\n",
      "[Training Epoch 0] Batch 3525, Loss 0.298297643661499\n",
      "[Training Epoch 0] Batch 3526, Loss 0.2865385115146637\n",
      "[Training Epoch 0] Batch 3527, Loss 0.3080809712409973\n",
      "[Training Epoch 0] Batch 3528, Loss 0.3028407096862793\n",
      "[Training Epoch 0] Batch 3529, Loss 0.3205273151397705\n",
      "[Training Epoch 0] Batch 3530, Loss 0.30508512258529663\n",
      "[Training Epoch 0] Batch 3531, Loss 0.2796320915222168\n",
      "[Training Epoch 0] Batch 3532, Loss 0.31166163086891174\n",
      "[Training Epoch 0] Batch 3533, Loss 0.34244412183761597\n",
      "[Training Epoch 0] Batch 3534, Loss 0.30123162269592285\n",
      "[Training Epoch 0] Batch 3535, Loss 0.32846200466156006\n",
      "[Training Epoch 0] Batch 3536, Loss 0.28600093722343445\n",
      "[Training Epoch 0] Batch 3537, Loss 0.2708388864994049\n",
      "[Training Epoch 0] Batch 3538, Loss 0.3151903748512268\n",
      "[Training Epoch 0] Batch 3539, Loss 0.29946810007095337\n",
      "[Training Epoch 0] Batch 3540, Loss 0.29219457507133484\n",
      "[Training Epoch 0] Batch 3541, Loss 0.30454590916633606\n",
      "[Training Epoch 0] Batch 3542, Loss 0.2730186879634857\n",
      "[Training Epoch 0] Batch 3543, Loss 0.27150243520736694\n",
      "[Training Epoch 0] Batch 3544, Loss 0.2923482060432434\n",
      "[Training Epoch 0] Batch 3545, Loss 0.30651551485061646\n",
      "[Training Epoch 0] Batch 3546, Loss 0.3237214684486389\n",
      "[Training Epoch 0] Batch 3547, Loss 0.28160232305526733\n",
      "[Training Epoch 0] Batch 3548, Loss 0.3023200035095215\n",
      "[Training Epoch 0] Batch 3549, Loss 0.2863239347934723\n",
      "[Training Epoch 0] Batch 3550, Loss 0.3131486773490906\n",
      "[Training Epoch 0] Batch 3551, Loss 0.28615623712539673\n",
      "[Training Epoch 0] Batch 3552, Loss 0.30792170763015747\n",
      "[Training Epoch 0] Batch 3553, Loss 0.2960394024848938\n",
      "[Training Epoch 0] Batch 3554, Loss 0.25259190797805786\n",
      "[Training Epoch 0] Batch 3555, Loss 0.296610951423645\n",
      "[Training Epoch 0] Batch 3556, Loss 0.2976458668708801\n",
      "[Training Epoch 0] Batch 3557, Loss 0.2843968868255615\n",
      "[Training Epoch 0] Batch 3558, Loss 0.3183949589729309\n",
      "[Training Epoch 0] Batch 3559, Loss 0.29688841104507446\n",
      "[Training Epoch 0] Batch 3560, Loss 0.2892026901245117\n",
      "[Training Epoch 0] Batch 3561, Loss 0.31346434354782104\n",
      "[Training Epoch 0] Batch 3562, Loss 0.2875587046146393\n",
      "[Training Epoch 0] Batch 3563, Loss 0.2972503900527954\n",
      "[Training Epoch 0] Batch 3564, Loss 0.3040144145488739\n",
      "[Training Epoch 0] Batch 3565, Loss 0.2798309922218323\n",
      "[Training Epoch 0] Batch 3566, Loss 0.29913243651390076\n",
      "[Training Epoch 0] Batch 3567, Loss 0.3036381006240845\n",
      "[Training Epoch 0] Batch 3568, Loss 0.30967533588409424\n",
      "[Training Epoch 0] Batch 3569, Loss 0.28264135122299194\n",
      "[Training Epoch 0] Batch 3570, Loss 0.3182953894138336\n",
      "[Training Epoch 0] Batch 3571, Loss 0.32105696201324463\n",
      "[Training Epoch 0] Batch 3572, Loss 0.2810271978378296\n",
      "[Training Epoch 0] Batch 3573, Loss 0.2966444492340088\n",
      "[Training Epoch 0] Batch 3574, Loss 0.28976163268089294\n",
      "[Training Epoch 0] Batch 3575, Loss 0.29789936542510986\n",
      "[Training Epoch 0] Batch 3576, Loss 0.29679223895072937\n",
      "[Training Epoch 0] Batch 3577, Loss 0.3220134675502777\n",
      "[Training Epoch 0] Batch 3578, Loss 0.28573983907699585\n",
      "[Training Epoch 0] Batch 3579, Loss 0.3136600852012634\n",
      "[Training Epoch 0] Batch 3580, Loss 0.27536627650260925\n",
      "[Training Epoch 0] Batch 3581, Loss 0.2982589602470398\n",
      "[Training Epoch 0] Batch 3582, Loss 0.3066861927509308\n",
      "[Training Epoch 0] Batch 3583, Loss 0.307523250579834\n",
      "[Training Epoch 0] Batch 3584, Loss 0.2936224341392517\n",
      "[Training Epoch 0] Batch 3585, Loss 0.33371037244796753\n",
      "[Training Epoch 0] Batch 3586, Loss 0.30473822355270386\n",
      "[Training Epoch 0] Batch 3587, Loss 0.30990317463874817\n",
      "[Training Epoch 0] Batch 3588, Loss 0.33118686079978943\n",
      "[Training Epoch 0] Batch 3589, Loss 0.2890079617500305\n",
      "[Training Epoch 0] Batch 3590, Loss 0.3004729747772217\n",
      "[Training Epoch 0] Batch 3591, Loss 0.28879085183143616\n",
      "[Training Epoch 0] Batch 3592, Loss 0.2974345088005066\n",
      "[Training Epoch 0] Batch 3593, Loss 0.3137476444244385\n",
      "[Training Epoch 0] Batch 3594, Loss 0.312726765871048\n",
      "[Training Epoch 0] Batch 3595, Loss 0.3242443799972534\n",
      "[Training Epoch 0] Batch 3596, Loss 0.3022114336490631\n",
      "[Training Epoch 0] Batch 3597, Loss 0.3038221001625061\n",
      "[Training Epoch 0] Batch 3598, Loss 0.3169627785682678\n",
      "[Training Epoch 0] Batch 3599, Loss 0.31684279441833496\n",
      "[Training Epoch 0] Batch 3600, Loss 0.3288966715335846\n",
      "[Training Epoch 0] Batch 3601, Loss 0.32914361357688904\n",
      "[Training Epoch 0] Batch 3602, Loss 0.31439775228500366\n",
      "[Training Epoch 0] Batch 3603, Loss 0.31858235597610474\n",
      "[Training Epoch 0] Batch 3604, Loss 0.3064115345478058\n",
      "[Training Epoch 0] Batch 3605, Loss 0.34596046805381775\n",
      "[Training Epoch 0] Batch 3606, Loss 0.33304619789123535\n",
      "[Training Epoch 0] Batch 3607, Loss 0.27512678503990173\n",
      "[Training Epoch 0] Batch 3608, Loss 0.26859113574028015\n",
      "[Training Epoch 0] Batch 3609, Loss 0.29371893405914307\n",
      "[Training Epoch 0] Batch 3610, Loss 0.29616421461105347\n",
      "[Training Epoch 0] Batch 3611, Loss 0.29360929131507874\n",
      "[Training Epoch 0] Batch 3612, Loss 0.3043444752693176\n",
      "[Training Epoch 0] Batch 3613, Loss 0.2971707880496979\n",
      "[Training Epoch 0] Batch 3614, Loss 0.29111969470977783\n",
      "[Training Epoch 0] Batch 3615, Loss 0.31190603971481323\n",
      "[Training Epoch 0] Batch 3616, Loss 0.2768118381500244\n",
      "[Training Epoch 0] Batch 3617, Loss 0.29655778408050537\n",
      "[Training Epoch 0] Batch 3618, Loss 0.30437690019607544\n",
      "[Training Epoch 0] Batch 3619, Loss 0.3199719190597534\n",
      "[Training Epoch 0] Batch 3620, Loss 0.3149806261062622\n",
      "[Training Epoch 0] Batch 3621, Loss 0.3218625783920288\n",
      "[Training Epoch 0] Batch 3622, Loss 0.31359970569610596\n",
      "[Training Epoch 0] Batch 3623, Loss 0.28642478585243225\n",
      "[Training Epoch 0] Batch 3624, Loss 0.29829490184783936\n",
      "[Training Epoch 0] Batch 3625, Loss 0.3133806586265564\n",
      "[Training Epoch 0] Batch 3626, Loss 0.27571654319763184\n",
      "[Training Epoch 0] Batch 3627, Loss 0.2742384076118469\n",
      "[Training Epoch 0] Batch 3628, Loss 0.2606407403945923\n",
      "[Training Epoch 0] Batch 3629, Loss 0.28094378113746643\n",
      "[Training Epoch 0] Batch 3630, Loss 0.29355281591415405\n",
      "[Training Epoch 0] Batch 3631, Loss 0.31139063835144043\n",
      "[Training Epoch 0] Batch 3632, Loss 0.30498385429382324\n",
      "[Training Epoch 0] Batch 3633, Loss 0.29447802901268005\n",
      "[Training Epoch 0] Batch 3634, Loss 0.32406187057495117\n",
      "[Training Epoch 0] Batch 3635, Loss 0.30882322788238525\n",
      "[Training Epoch 0] Batch 3636, Loss 0.3306463658809662\n",
      "[Training Epoch 0] Batch 3637, Loss 0.309281587600708\n",
      "[Training Epoch 0] Batch 3638, Loss 0.29637467861175537\n",
      "[Training Epoch 0] Batch 3639, Loss 0.2753843069076538\n",
      "[Training Epoch 0] Batch 3640, Loss 0.31228452920913696\n",
      "[Training Epoch 0] Batch 3641, Loss 0.3188930153846741\n",
      "[Training Epoch 0] Batch 3642, Loss 0.28435757756233215\n",
      "[Training Epoch 0] Batch 3643, Loss 0.2957766354084015\n",
      "[Training Epoch 0] Batch 3644, Loss 0.2881001830101013\n",
      "[Training Epoch 0] Batch 3645, Loss 0.30885517597198486\n",
      "[Training Epoch 0] Batch 3646, Loss 0.30831170082092285\n",
      "[Training Epoch 0] Batch 3647, Loss 0.28188958764076233\n",
      "[Training Epoch 0] Batch 3648, Loss 0.297807514667511\n",
      "[Training Epoch 0] Batch 3649, Loss 0.31857284903526306\n",
      "[Training Epoch 0] Batch 3650, Loss 0.30871954560279846\n",
      "[Training Epoch 0] Batch 3651, Loss 0.2951083481311798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 18446/24393 [00:36<00:11, 507.99it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.45 GiB is allocated by PyTorch, and 23.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m sample_generator\u001b[38;5;241m.\u001b[39minstance_a_train_loader(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_negative\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m engine\u001b[38;5;241m.\u001b[39mtrain_an_epoch(train_loader, epoch_id\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m---> 26\u001b[0m precision, recall \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m engine\u001b[38;5;241m.\u001b[39msave(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malias\u001b[39m\u001b[38;5;124m'\u001b[39m], epoch, precision, recall)\n",
      "File \u001b[1;32md:\\Year 3 Semester 1\\SC4020\\Project 1\\Recommendation-System-SC4020\\models_ncf\\engine.py:79\u001b[0m, in \u001b[0;36mEngine.evaluate\u001b[1;34m(self, evaluate_data, epoch_id)\u001b[0m\n\u001b[0;32m     77\u001b[0m     batch_negative_users \u001b[38;5;241m=\u001b[39m negative_users[start_idx:end_idx]\n\u001b[0;32m     78\u001b[0m     batch_negative_items \u001b[38;5;241m=\u001b[39m negative_items[start_idx:end_idx]\n\u001b[1;32m---> 79\u001b[0m     negative_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_negative_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_negative_items\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     80\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcatenate(test_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m     81\u001b[0m negative_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcatenate(negative_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Year 3 Semester 1\\SC4020\\Project 1\\Recommendation-System-SC4020\\models_ncf\\neumf.py:51\u001b[0m, in \u001b[0;36mNeuMF.forward\u001b[1;34m(self, user_indices, item_indices)\u001b[0m\n\u001b[0;32m     48\u001b[0m mf_vector \u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmul(user_embedding_mf, item_embedding_mf)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers))):\n\u001b[1;32m---> 51\u001b[0m     mlp_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     mlp_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU()(mlp_vector)\n\u001b[0;32m     54\u001b[0m vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mlp_vector, mf_vector], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bryti\\miniconda3\\envs\\rec-sys\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.45 GiB is allocated by PyTorch, and 23.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Data\n",
    "\n",
    "ml1m_dir = 'ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "# Reindex\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n",
    "# DataLoader for training \n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "\n",
    "config = neumf_config\n",
    "engine = NeuMFEngine(config)\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    precision, recall = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    engine.save(config['alias'], epoch, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n",
      "Total users: 6040\n",
      "Users in training set: 6040\n",
      "Users in test set: 6040\n",
      "Average test proportion per user: 0.26\n"
     ]
    }
   ],
   "source": [
    "ml1m_dir = 'ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "# Reindex\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n",
    "# DataLoader for training \n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "\n",
    "# Split the data\n",
    "train_ratings, test_ratings = sample_generator._split_loo(sample_generator.preprocess_ratings)\n",
    "\n",
    "# Check the distribution\n",
    "print(f\"Total users: {ml1m_rating['userId'].nunique()}\")\n",
    "print(f\"Users in training set: {train_ratings['userId'].nunique()}\")\n",
    "print(f\"Users in test set: {test_ratings['userId'].nunique()}\")\n",
    "\n",
    "# Verify that approximately 25% of interactions per user are in the test set\n",
    "user_test_counts = test_ratings.groupby('userId').size()\n",
    "user_total_counts = ml1m_rating.groupby('userId').size()\n",
    "test_proportions = (user_test_counts / user_total_counts).mean()\n",
    "print(f\"Average test proportion per user: {test_proportions:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
